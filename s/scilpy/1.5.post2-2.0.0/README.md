# Comparing `tmp/scilpy-1.5.post2.tar.gz` & `tmp/scilpy-2.0.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "scilpy-1.5.post2.tar", last modified: Fri Nov 10 20:53:19 2023, max compression
+gzip compressed data, was "scilpy-2.0.0.tar", last modified: Tue Apr 23 18:30:18 2024, max compression
```

## Comparing `scilpy-1.5.post2.tar` & `scilpy-2.0.0.tar`

### file list

```diff
@@ -1,468 +1,667 @@
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:19.479519 scilpy-1.5.post2/
--rw-r--r--   0 runner    (1001) docker     (127)       16 2023-11-10 20:53:08.000000 scilpy-1.5.post2/.python-version
--rw-r--r--   0 runner    (1001) docker     (127)     1335 2023-11-10 20:53:07.000000 scilpy-1.5.post2/LICENSE
--rw-r--r--   0 runner    (1001) docker     (127)      175 2023-11-10 20:53:08.000000 scilpy-1.5.post2/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (127)     2584 2023-11-10 20:53:19.479519 scilpy-1.5.post2/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (127)     2051 2023-11-10 20:53:07.000000 scilpy-1.5.post2/README.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:19.415518 scilpy-1.5.post2/data/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:19.419518 scilpy-1.5.post2/data/LUT/
--rw-r--r--   0 runner    (1001) docker     (127)      920 2023-11-10 20:53:07.000000 scilpy-1.5.post2/data/LUT/dk_aggregate_structures.json
--rw-r--r--   0 runner    (1001) docker     (127)     2504 2023-11-10 20:53:07.000000 scilpy-1.5.post2/data/LUT/freesurfer_desikan_killiany.json
--rw-r--r--   0 runner    (1001) docker     (127)     1421 2023-11-10 20:53:07.000000 scilpy-1.5.post2/data/LUT/freesurfer_subcortical.json
--rw-r--r--   0 runner    (1001) docker     (127)      701 2023-11-10 20:53:08.000000 scilpy-1.5.post2/requirements.txt
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:19.419518 scilpy-1.5.post2/scilpy/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:19.419518 scilpy-1.5.post2/scilpy/connectivity/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/connectivity/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3213 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/connectivity/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:19.419518 scilpy-1.5.post2/scilpy/denoise/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/denoise/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4441 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/denoise/asym_averaging.py
--rw-r--r--   0 runner    (1001) docker     (127)    15416 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/denoise/bilateral_filtering.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:19.419518 scilpy-1.5.post2/scilpy/direction/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/direction/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    10992 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/direction/peaks.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:19.419518 scilpy-1.5.post2/scilpy/gpuparallel/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/gpuparallel/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7788 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/gpuparallel/opencl_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:19.419518 scilpy-1.5.post2/scilpy/gradientsampling/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/gradientsampling/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1422 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/gradientsampling/gen_gradient_sampling.py
--rw-r--r--   0 runner    (1001) docker     (127)     9181 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/gradientsampling/multiple_shell_energy.py
--rw-r--r--   0 runner    (1001) docker     (127)    11739 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/gradientsampling/optimize_gradient_sampling.py
--rw-r--r--   0 runner    (1001) docker     (127)     1656 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/gradientsampling/save_gradient_sampling.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:19.423518 scilpy-1.5.post2/scilpy/image/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/image/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    12046 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/image/datasets.py
--rw-r--r--   0 runner    (1001) docker     (127)    10256 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/image/labels.py
--rw-r--r--   0 runner    (1001) docker     (127)    19348 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/image/operations.py
--rw-r--r--   0 runner    (1001) docker     (127)     4695 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/image/resample_volume.py
--rw-r--r--   0 runner    (1001) docker     (127)     5079 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/image/reslice.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:19.423518 scilpy-1.5.post2/scilpy/image/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/image/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4820 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/image/tests/test_labels.py
--rw-r--r--   0 runner    (1001) docker     (127)     3606 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/image/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:19.423518 scilpy-1.5.post2/scilpy/io/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/io/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6406 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/io/fetcher.py
--rw-r--r--   0 runner    (1001) docker     (127)     3516 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/io/image.py
--rw-r--r--   0 runner    (1001) docker     (127)     7684 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/io/streamlines.py
--rw-r--r--   0 runner    (1001) docker     (127)    23338 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/io/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    14068 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/io/varian_fdf.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:19.423518 scilpy-1.5.post2/scilpy/preprocessing/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/preprocessing/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4323 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/preprocessing/distortion_correction.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:19.423518 scilpy-1.5.post2/scilpy/reconst/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/reconst/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4632 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/reconst/afd_along_streamlines.py
--rw-r--r--   0 runner    (1001) docker     (127)     7714 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/reconst/b_tensor_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    14995 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/reconst/bingham.py
--rw-r--r--   0 runner    (1001) docker     (127)     8686 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/reconst/divide_fit.py
--rw-r--r--   0 runner    (1001) docker     (127)     2452 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/reconst/dti.py
--rw-r--r--   0 runner    (1001) docker     (127)     2708 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/reconst/fiber_coherence.py
--rw-r--r--   0 runner    (1001) docker     (127)     5241 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/reconst/fodf.py
--rw-r--r--   0 runner    (1001) docker     (127)    12373 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/reconst/frf.py
--rw-r--r--   0 runner    (1001) docker     (127)     4324 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/reconst/lobe_metrics_along_streamlines.py
--rw-r--r--   0 runner    (1001) docker     (127)    25720 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/reconst/multi_processes.py
--rw-r--r--   0 runner    (1001) docker     (127)     4145 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/reconst/raw_signal.py
--rw-r--r--   0 runner    (1001) docker     (127)     2390 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/reconst/sh.py
--rw-r--r--   0 runner    (1001) docker     (127)     2922 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/reconst/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:19.423518 scilpy-1.5.post2/scilpy/segment/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/segment/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1794 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/segment/models.py
--rw-r--r--   0 runner    (1001) docker     (127)    12097 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/segment/recobundlesx.py
--rw-r--r--   0 runner    (1001) docker     (127)    17289 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/segment/streamlines.py
--rw-r--r--   0 runner    (1001) docker     (127)    24002 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/segment/tractogram_from_roi.py
--rw-r--r--   0 runner    (1001) docker     (127)    20841 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/segment/voting_scheme.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:19.427518 scilpy-1.5.post2/scilpy/stats/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/stats/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8021 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/stats/stats.py
--rw-r--r--   0 runner    (1001) docker     (127)    20430 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/stats/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:19.427518 scilpy-1.5.post2/scilpy/tracking/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/tracking/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    20759 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/tracking/propagator.py
--rw-r--r--   0 runner    (1001) docker     (127)     6758 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/tracking/seed.py
--rw-r--r--   0 runner    (1001) docker     (127)     9752 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/tracking/tools.py
--rw-r--r--   0 runner    (1001) docker     (127)    23659 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/tracking/tracker.py
--rw-r--r--   0 runner    (1001) docker     (127)     4134 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/tracking/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:19.427518 scilpy-1.5.post2/scilpy/tractanalysis/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/tractanalysis/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2701 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/tractanalysis/distance_to_centroid.py
--rw-r--r--   0 runner    (1001) docker     (127)    10363 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/tractanalysis/features.py
--rw-r--r--   0 runner    (1001) docker     (127)    11835 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/tractanalysis/grid_intersections.pyx
--rw-r--r--   0 runner    (1001) docker     (127)     1361 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/tractanalysis/quick_tools.pyx
--rwxr-xr-x   0 runner    (1001) docker     (127)    17143 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/tractanalysis/reproducibility_measures.py
--rw-r--r--   0 runner    (1001) docker     (127)    13178 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/tractanalysis/scoring.py
--rw-r--r--   0 runner    (1001) docker     (127)     7495 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/tractanalysis/streamlines_metrics.pyx
--rw-r--r--   0 runner    (1001) docker     (127)    14225 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/tractanalysis/todi.py
--rw-r--r--   0 runner    (1001) docker     (127)     5978 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/tractanalysis/todi_util.py
--rw-r--r--   0 runner    (1001) docker     (127)    12079 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/tractanalysis/tools.py
--rw-r--r--   0 runner    (1001) docker     (127)    14286 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/tractanalysis/uncompress.pyx
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:19.427518 scilpy-1.5.post2/scilpy/tractograms/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/tractograms/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2568 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/tractograms/lazy_tractogram_operations.py
--rw-r--r--   0 runner    (1001) docker     (127)    29107 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/tractograms/tractogram_operations.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:19.427518 scilpy-1.5.post2/scilpy/utils/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    16774 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/utils/bvec_bval_tools.py
--rw-r--r--   0 runner    (1001) docker     (127)     1214 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/utils/filenames.py
--rw-r--r--   0 runner    (1001) docker     (127)     9176 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/utils/image.py
--rw-r--r--   0 runner    (1001) docker     (127)    14398 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/utils/metrics_tools.py
--rw-r--r--   0 runner    (1001) docker     (127)    12626 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/utils/streamlines.py
--rw-r--r--   0 runner    (1001) docker     (127)     2711 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/utils/util.py
--rw-r--r--   0 runner    (1001) docker     (127)     2263 2023-11-10 20:53:08.000000 scilpy-1.5.post2/scilpy/version.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:19.431518 scilpy-1.5.post2/scilpy/viz/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/viz/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8302 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/viz/chord_chart.py
--rw-r--r--   0 runner    (1001) docker     (127)     4957 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/viz/gradient_sampling.py
--rw-r--r--   0 runner    (1001) docker     (127)    30607 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/viz/scene_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     1343 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/viz/screenshot.py
--rw-r--r--   0 runner    (1001) docker     (127)      722 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scilpy/viz/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:19.419518 scilpy-1.5.post2/scilpy.egg-info/
--rw-r--r--   0 runner    (1001) docker     (127)     2584 2023-11-10 20:53:19.000000 scilpy-1.5.post2/scilpy.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (127)    16981 2023-11-10 20:53:19.000000 scilpy-1.5.post2/scilpy.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (127)        1 2023-11-10 20:53:19.000000 scilpy-1.5.post2/scilpy.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (127)    12332 2023-11-10 20:53:19.000000 scilpy-1.5.post2/scilpy.egg-info/entry_points.txt
--rw-r--r--   0 runner    (1001) docker     (127)      680 2023-11-10 20:53:19.000000 scilpy-1.5.post2/scilpy.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (127)       15 2023-11-10 20:53:19.000000 scilpy-1.5.post2/scilpy.egg-info/top_level.txt
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:19.455518 scilpy-1.5.post2/scripts/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     2250 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_add_tracking_mask_to_pft_maps.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     6325 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_analyse_lesions_load.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     2960 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_apply_bias_field_on_dwi.py
--rw-r--r--   0 runner    (1001) docker     (127)     1686 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_apply_transform_to_bvecs.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     6968 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_apply_transform_to_hdf5.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     2499 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_apply_transform_to_image.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     3086 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_apply_transform_to_surface.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     6434 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_apply_transform_to_tractogram.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     9934 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_assign_custom_color_to_tractogram.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     4207 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_assign_uniform_color_to_tractograms.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    12252 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_clean_qbx_clusters.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     5048 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_combine_labels.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     9010 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compare_connectivity.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     2460 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compress_streamlines.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    13746 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_MT_maps.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     6700 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_NODDI.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     6962 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_NODDI_priors.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     8780 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_asym_odf_metrics.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     4198 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_bundle_mean_std.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     4203 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_bundle_mean_std_per_point.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     1850 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_bundle_volume.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     1835 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_bundle_volume_per_label.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    14977 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_bundle_voxel_label_map.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     1799 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_centroid.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    20029 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_connectivity.py
--rw-r--r--   0 runner    (1001) docker     (127)     9067 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_divide.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    18578 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_dti_metrics.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     3574 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_endpoints_map.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     6020 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_fodf_max_in_ventricles.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     8112 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_fodf_metrics.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     7278 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_freewater.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     4049 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_hdf5_average_density_map.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    22033 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_ihMT_maps.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    11906 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_kurtosis_metrics.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     5078 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_lobe_specific_fodf_metrics.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    10183 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_local_tracking.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    11367 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_local_tracking_dev.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     8784 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_local_tracking_gpu.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     3571 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_maps_for_particle_filter_tracking.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     2541 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_mean_fixel_afd_from_bundles.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     5117 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_mean_fixel_afd_from_hdf5.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     3693 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_mean_fixel_lobe_metric_from_bundles.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     1414 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_mean_frf.py
--rw-r--r--   0 runner    (1001) docker     (127)    15406 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_memsmt_fodf.py
--rw-r--r--   0 runner    (1001) docker     (127)    14231 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_memsmt_frf.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     5978 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_metrics_stats_in_ROI.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     9602 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_msmt_fodf.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    11357 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_msmt_frf.py
--rw-r--r--   0 runner    (1001) docker     (127)    13010 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_pca.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    12851 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_pft.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     4867 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_powder_average.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     7275 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_qball_metrics.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     3324 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_qbx.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     2780 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_rish_from_sh.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     2622 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_seed_by_labels.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     3955 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_seed_density_map.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     6304 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_sf_from_sh.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     2729 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_sh_from_signal.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     4582 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_ssst_fodf.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     4824 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_ssst_frf.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     2526 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_streamlines_density_map.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     1406 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_streamlines_length_stats.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     5647 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_compute_todi.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     3283 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_concatenate_dwi.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     5221 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_connectivity_math.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     2439 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_convert_fdf.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     1493 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_convert_gradients_fsl_to_mrtrix.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     1468 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_convert_gradients_mrtrix_to_fsl.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    17177 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_convert_json_to_xlsx.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     2216 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_convert_rgb.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     1614 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_convert_sh_basis.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     1326 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_convert_surface.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     2009 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_convert_tensors.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     1842 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_convert_tractogram.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     2481 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_count_non_zero_voxels.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     1451 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_count_streamlines.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     4516 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_crop_volume.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     4612 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_cut_streamlines.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    20190 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_decompose_connectivity.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     4849 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_detect_dwi_volume_outliers.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     5071 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_detect_streamlines_loops.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     3656 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_dilate_labels.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    12971 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_estimate_bundles_diameter.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     9445 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_evaluate_bundles_binary_classification_measures.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    12576 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_evaluate_bundles_individual_measures.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    14817 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_evaluate_bundles_pairwise_agreement_measures.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     9444 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_evaluate_connectivity_graph_measures.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     3167 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_evaluate_connectivity_pairwise_agreement_measures.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     4465 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_execute_angle_aware_bilateral_filtering.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     3915 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_execute_asymmetric_filtering.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     4483 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_extract_b0.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     3463 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_extract_dwi_shell.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     3727 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_extract_ushape.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     6037 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_filter_connectivity.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     2902 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_filter_streamlines_by_length.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     5290 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_filter_streamlines_by_orientation.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    13847 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_filter_tractogram.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    20192 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_filter_tractogram_anatomically.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     4204 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_fit_bingham_to_fodf.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     8828 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_fix_dsi_studio_trk.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     2555 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_flip_gradients.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     1704 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_flip_streamlines.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     2177 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_flip_surface.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     1473 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_flip_volume.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     7747 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_generate_gradient_sampling.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     6593 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_generate_priors_from_bundle.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    10001 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_group_comparison.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     6056 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_image_math.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     4092 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_merge_json.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     1901 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_merge_sh.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     8145 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_normalize_connectivity.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     3197 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_outlier_rejection.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     4543 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_perform_majority_vote.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     5554 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_plot_mean_std_per_point.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     9738 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_prepare_eddy_command.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     5453 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_prepare_topup_command.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     2148 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_print_connectivity_filenames.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     1553 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_print_header.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     9751 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_project_streamlines_to_map.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     5880 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_recognize_multi_bundles.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     6422 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_recognize_single_bundle.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     3318 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_register_tractogram.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     3909 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_remove_invalid_streamlines.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     1875 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_remove_labels.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     3201 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_remove_outliers_ransac.py
--rwxr-xr-x   0 runner    (1001) docker     (127)      235 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_remove_similar_streamlines.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     5282 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_reorder_connectivity.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     3716 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_reorder_dwi_philips.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     3036 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_resample_bvals.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     1958 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_resample_streamlines.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     7112 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_resample_tractogram.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     3297 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_resample_volume.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     1850 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_reshape_to_reference.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    22751 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_run_commit.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     4287 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_run_nlmeans.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     4434 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_save_connections_from_hdf5.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     8841 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_score_bundles.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    19742 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_score_tractogram.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    10477 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_screenshot_bundle.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     8492 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_screenshot_dti.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     6287 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_screenshot_volume_mosaic_overlap.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     6934 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_search_keywords.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     1747 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_set_response_function.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     1510 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_shuffle_streamlines.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     3992 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_smooth_streamlines.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     2589 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_smooth_surface.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     6508 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_snr_in_roi.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     3981 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_split_image.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     5210 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_split_tractogram.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     3296 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_split_volume_by_ids.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     3243 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_split_volume_by_labels.py
--rw-r--r--   0 runner    (1001) docker     (127)      730 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_streamlines_math.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     2523 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_swap_gradient_axis.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     7583 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_tractogram_math.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     4309 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_uniformize_streamlines_endpoints.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     4618 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_validate_and_correct_bvecs.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     2037 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_validate_and_correct_eddy_gradients.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    16901 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_validate_bids.py
--rwxr-xr-x   0 runner    (1001) docker     (127)      916 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_verify_space_attributes_compatibility.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     4604 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_visualize_bingham_fit.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     8617 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_visualize_bundles.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    14072 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_visualize_bundles_mosaic.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    14239 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_visualize_connectivity.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    14536 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_visualize_fodf.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     6873 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_visualize_gradients.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     2788 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_visualize_histogram.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    10321 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_visualize_scatterplot.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     2374 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_visualize_seeds.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     3782 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/scil_visualize_seeds_3d.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:19.479519 scilpy-1.5.post2/scripts/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1127 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_add_tracking_mask_to_pft_maps.py
--rw-r--r--   0 runner    (1001) docker     (127)      904 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_apply_bias_field_on_dwi.py
--rw-r--r--   0 runner    (1001) docker     (127)     1021 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_apply_transform_to_bvecs.py
--rw-r--r--   0 runner    (1001) docker     (127)     1026 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_apply_transform_to_hdf5.py
--rw-r--r--   0 runner    (1001) docker     (127)     1029 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_apply_transform_to_image.py
--rw-r--r--   0 runner    (1001) docker     (127)      911 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_apply_transform_to_surface.py
--rw-r--r--   0 runner    (1001) docker     (127)     1195 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_apply_transform_to_tractogram.py
--rw-r--r--   0 runner    (1001) docker     (127)     1001 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_assign_custom_color_to_tractogram.py
--rw-r--r--   0 runner    (1001) docker     (127)      918 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_assign_uniform_color_to_tractograms.py
--rw-r--r--   0 runner    (1001) docker     (127)      177 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_clean_qbx_clusters.py
--rw-r--r--   0 runner    (1001) docker     (127)     1781 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_combine_labels.py
--rw-r--r--   0 runner    (1001) docker     (127)     1036 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compare_connectivity.py
--rw-r--r--   0 runner    (1001) docker     (127)      812 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compress_streamlines.py
--rw-r--r--   0 runner    (1001) docker     (127)     8200 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_MT_maps.py
--rw-r--r--   0 runner    (1001) docker     (127)     1323 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_NODDI.py
--rw-r--r--   0 runner    (1001) docker     (127)     1201 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_NODDI_priors.py
--rw-r--r--   0 runner    (1001) docker     (127)     1657 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_asym_odf_metrics.py
--rw-r--r--   0 runner    (1001) docker     (127)      923 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_bundle_mean_std.py
--rw-r--r--   0 runner    (1001) docker     (127)     1166 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_bundle_mean_std_per_point.py
--rw-r--r--   0 runner    (1001) docker     (127)      743 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_bundle_volume.py
--rw-r--r--   0 runner    (1001) docker     (127)      851 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_bundle_volume_per_label.py
--rw-r--r--   0 runner    (1001) docker     (127)     1010 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_bundle_voxel_label_map.py
--rw-r--r--   0 runner    (1001) docker     (127)      784 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_centroid.py
--rw-r--r--   0 runner    (1001) docker     (127)     1466 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_connectivity.py
--rw-r--r--   0 runner    (1001) docker     (127)     5684 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_divide.py
--rw-r--r--   0 runner    (1001) docker     (127)     1122 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_dti_metrics.py
--rw-r--r--   0 runner    (1001) docker     (127)      818 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_endpoints_map.py
--rw-r--r--   0 runner    (1001) docker     (127)     1001 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_fodf_max_in_ventricles.py
--rw-r--r--   0 runner    (1001) docker     (127)     1065 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_fodf_metrics.py
--rw-r--r--   0 runner    (1001) docker     (127)     1426 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_freewater.py
--rw-r--r--   0 runner    (1001) docker     (127)      899 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_hdf5_average_density_map.py
--rw-r--r--   0 runner    (1001) docker     (127)    15549 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_ihMT_maps.py
--rw-r--r--   0 runner    (1001) docker     (127)     1213 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_kurtosis_metrics.py
--rw-r--r--   0 runner    (1001) docker     (127)     1970 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_lobe_specific_fodf_metrics.py
--rw-r--r--   0 runner    (1001) docker     (127)     2360 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_local_tracking.py
--rw-r--r--   0 runner    (1001) docker     (127)     1167 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_local_tracking_dev.py
--rw-r--r--   0 runner    (1001) docker     (127)      184 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_local_tracking_gpu.py
--rw-r--r--   0 runner    (1001) docker     (127)     1022 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_maps_for_particle_filter_tracking.py
--rw-r--r--   0 runner    (1001) docker     (127)     1076 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_mean_fixel_afd_from_bundles.py
--rw-r--r--   0 runner    (1001) docker     (127)     1073 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_mean_fixel_afd_from_hdf5.py
--rw-r--r--   0 runner    (1001) docker     (127)     1079 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_mean_fixel_lobe_metric_from_bundles.py
--rw-r--r--   0 runner    (1001) docker     (127)      730 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_mean_frf.py
--rw-r--r--   0 runner    (1001) docker     (127)     4469 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_memsmt_fodf.py
--rw-r--r--   0 runner    (1001) docker     (127)     8055 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_memsmt_frf.py
--rw-r--r--   0 runner    (1001) docker     (127)      902 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_metrics_stats_in_ROI.py
--rw-r--r--   0 runner    (1001) docker     (127)     1545 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_msmt_fodf.py
--rw-r--r--   0 runner    (1001) docker     (127)     3520 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_msmt_frf.py
--rw-r--r--   0 runner    (1001) docker     (127)     1014 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_pca.py
--rw-r--r--   0 runner    (1001) docker     (127)     1331 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_pft.py
--rw-r--r--   0 runner    (1001) docker     (127)      917 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_powder_average.py
--rw-r--r--   0 runner    (1001) docker     (127)     1458 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_qball_metrics.py
--rw-r--r--   0 runner    (1001) docker     (127)      809 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_qbx.py
--rw-r--r--   0 runner    (1001) docker     (127)      741 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_rish_from_sh.py
--rw-r--r--   0 runner    (1001) docker     (127)      181 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_seed_by_labels.py
--rw-r--r--   0 runner    (1001) docker     (127)      806 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_seed_density_map.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     1109 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_sf_from_sh.py
--rw-r--r--   0 runner    (1001) docker     (127)      993 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_sh_from_signal.py
--rw-r--r--   0 runner    (1001) docker     (127)     1172 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_ssst_fodf.py
--rw-r--r--   0 runner    (1001) docker     (127)     2790 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_ssst_frf.py
--rw-r--r--   0 runner    (1001) docker     (127)     1207 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_streamlines_density_map.py
--rw-r--r--   0 runner    (1001) docker     (127)      818 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_streamlines_length_stats.py
--rw-r--r--   0 runner    (1001) docker     (127)     1567 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_compute_todi.py
--rw-r--r--   0 runner    (1001) docker     (127)     1167 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_concatenate_dwi.py
--rw-r--r--   0 runner    (1001) docker     (127)     1590 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_connectivity_math.py
--rw-r--r--   0 runner    (1001) docker     (127)      170 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_convert_fdf.py
--rw-r--r--   0 runner    (1001) docker     (127)      921 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_convert_gradients_fsl_to_mrtrix.py
--rw-r--r--   0 runner    (1001) docker     (127)      844 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_convert_gradients_mrtrix_to_fsl.py
--rw-r--r--   0 runner    (1001) docker     (127)      795 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_convert_json_to_xlsx.py
--rw-r--r--   0 runner    (1001) docker     (127)      717 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_convert_rgb.py
--rw-r--r--   0 runner    (1001) docker     (127)      843 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_convert_sh_basis.py
--rw-r--r--   0 runner    (1001) docker     (127)      780 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_convert_surface.py
--rw-r--r--   0 runner    (1001) docker     (127)     1292 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_convert_tensors.py
--rw-r--r--   0 runner    (1001) docker     (127)      909 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_convert_tractogram.py
--rw-r--r--   0 runner    (1001) docker     (127)      720 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_count_non_zero_voxels.py
--rw-r--r--   0 runner    (1001) docker     (127)      724 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_count_streamlines.py
--rw-r--r--   0 runner    (1001) docker     (127)      730 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_crop_volume.py
--rw-r--r--   0 runner    (1001) docker     (127)     1003 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_cut_streamlines.py
--rw-r--r--   0 runner    (1001) docker     (127)     1211 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_decompose_connectivity.py
--rw-r--r--   0 runner    (1001) docker     (127)     1045 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_detect_streamlines_loops.py
--rw-r--r--   0 runner    (1001) docker     (127)      886 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_dilate_labels.py
--rw-r--r--   0 runner    (1001) docker     (127)      973 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_estimate_bundles_diameter.py
--rw-r--r--   0 runner    (1001) docker     (127)     1475 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_evaluate_bundles_binary_classification_measures.py
--rw-r--r--   0 runner    (1001) docker     (127)     1114 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_evaluate_bundles_individual_measures.py
--rw-r--r--   0 runner    (1001) docker     (127)     3844 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_evaluate_bundles_pairwise_agreement_measures.py
--rw-r--r--   0 runner    (1001) docker     (127)      969 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_evaluate_connectivity_graph_measures.py
--rw-r--r--   0 runner    (1001) docker     (127)      959 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_evaluate_connectivity_pairwise_agreement_measures.py
--rw-r--r--   0 runner    (1001) docker     (127)     4397 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_execute_angle_aware_bilateral_filtering.py
--rw-r--r--   0 runner    (1001) docker     (127)     1867 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_execute_asymmetric_filtering.py
--rw-r--r--   0 runner    (1001) docker     (127)      986 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_extract_b0.py
--rw-r--r--   0 runner    (1001) docker     (127)     1740 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_extract_dwi_shell.py
--rw-r--r--   0 runner    (1001) docker     (127)      992 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_extract_ushape.py
--rw-r--r--   0 runner    (1001) docker     (127)     1004 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_filter_connectivity.py
--rw-r--r--   0 runner    (1001) docker     (127)      898 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_filter_streamlines_by_length.py
--rw-r--r--   0 runner    (1001) docker     (127)      967 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_filter_streamlines_by_orientation.py
--rw-r--r--   0 runner    (1001) docker     (127)     1205 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_filter_tractogram.py
--rw-r--r--   0 runner    (1001) docker     (127)     1178 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_filter_tractogram_anatomically.py
--rw-r--r--   0 runner    (1001) docker     (127)     1891 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_fit_bingham_to_fodf.py
--rw-r--r--   0 runner    (1001) docker     (127)      281 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_fix_dsi_studio_trk.py
--rw-r--r--   0 runner    (1001) docker     (127)      788 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_flip_gradients.py
--rw-r--r--   0 runner    (1001) docker     (127)      910 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_flip_streamlines.py
--rw-r--r--   0 runner    (1001) docker     (127)      883 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_flip_surface.py
--rw-r--r--   0 runner    (1001) docker     (127)      772 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_flip_volume.py
--rw-r--r--   0 runner    (1001) docker     (127)      834 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_generate_gradient_sampling.py
--rw-r--r--   0 runner    (1001) docker     (127)     1089 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_generate_priors_from_bundle.py
--rw-r--r--   0 runner    (1001) docker     (127)     1018 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_group_comparison.py
--rw-r--r--   0 runner    (1001) docker     (127)     2956 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_image_math.py
--rw-r--r--   0 runner    (1001) docker     (127)      908 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_merge_json.py
--rw-r--r--   0 runner    (1001) docker     (127)      832 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_merge_sh.py
--rw-r--r--   0 runner    (1001) docker     (127)     1185 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_normalize_connectivity.py
--rw-r--r--   0 runner    (1001) docker     (127)      973 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_outlier_rejection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1049 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_perform_majority_vote.py
--rw-r--r--   0 runner    (1001) docker     (127)      813 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_plot_mean_std_per_point.py
--rw-r--r--   0 runner    (1001) docker     (127)      179 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_prepare_eddy_command.py
--rw-r--r--   0 runner    (1001) docker     (127)      180 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_prepare_topup_command.py
--rw-r--r--   0 runner    (1001) docker     (127)      924 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_print_connectivity_filenames.py
--rw-r--r--   0 runner    (1001) docker     (127)      971 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_print_header.py
--rw-r--r--   0 runner    (1001) docker     (127)     1447 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_project_streamlines_to_map.py
--rw-r--r--   0 runner    (1001) docker     (127)     1462 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_recognize_multi_bundles.py
--rw-r--r--   0 runner    (1001) docker     (127)     1346 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_recognize_single_bundle.py
--rw-r--r--   0 runner    (1001) docker     (127)     1068 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_register_tractogram.py
--rw-r--r--   0 runner    (1001) docker     (127)      898 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_remove_invalid_streamlines.py
--rw-r--r--   0 runner    (1001) docker     (127)      848 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_remove_labels.py
--rw-r--r--   0 runner    (1001) docker     (127)      778 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_remove_outliers_ransac.py
--rw-r--r--   0 runner    (1001) docker     (127)     1798 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_reorder_connectivity.py
--rw-r--r--   0 runner    (1001) docker     (127)      178 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_reorder_dwi_philips.py
--rw-r--r--   0 runner    (1001) docker     (127)      785 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_resample_bvals.py
--rw-r--r--   0 runner    (1001) docker     (127)      858 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_resample_streamlines.py
--rw-r--r--   0 runner    (1001) docker     (127)     1825 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_resample_tractogram.py
--rw-r--r--   0 runner    (1001) docker     (127)      778 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_resample_volume.py
--rw-r--r--   0 runner    (1001) docker     (127)     1315 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_reshape_to_reference.py
--rw-r--r--   0 runner    (1001) docker     (127)     1728 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_run_commit.py
--rw-r--r--   0 runner    (1001) docker     (127)      784 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_run_nlmeans.py
--rw-r--r--   0 runner    (1001) docker     (127)      788 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_save_connections_from_hdf5.py
--rw-r--r--   0 runner    (1001) docker     (127)     1551 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_score_bundles.py
--rw-r--r--   0 runner    (1001) docker     (127)     1444 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_score_tractogram.py
--rw-r--r--   0 runner    (1001) docker     (127)      176 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_screenshot_bundle.py
--rw-r--r--   0 runner    (1001) docker     (127)      173 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_screenshot_dti.py
--rw-r--r--   0 runner    (1001) docker     (127)      206 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_screenshot_volume_mosaic_overlap.py
--rw-r--r--   0 runner    (1001) docker     (127)      174 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_search_keywords.py
--rw-r--r--   0 runner    (1001) docker     (127)      778 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_set_response_function.py
--rw-r--r--   0 runner    (1001) docker     (127)      778 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_shuffle_streamlines.py
--rw-r--r--   0 runner    (1001) docker     (127)      861 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_smooth_streamlines.py
--rw-r--r--   0 runner    (1001) docker     (127)      807 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_smooth_surface.py
--rw-r--r--   0 runner    (1001) docker     (127)     1176 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_snr_in_roi.py
--rw-r--r--   0 runner    (1001) docker     (127)     1683 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_split_image.py
--rw-r--r--   0 runner    (1001) docker     (127)     1213 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_split_tractogram.py
--rw-r--r--   0 runner    (1001) docker     (127)      811 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_split_volume_by_ids.py
--rw-r--r--   0 runner    (1001) docker     (127)      957 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_split_volume_by_labels.py
--rw-r--r--   0 runner    (1001) docker     (127)     1045 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_streamlines_math.py
--rw-r--r--   0 runner    (1001) docker     (127)      809 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_swap_gradient_axis.py
--rw-r--r--   0 runner    (1001) docker     (127)     9724 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_tractogram_math.py
--rw-r--r--   0 runner    (1001) docker     (127)      848 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_uniformize_streamlines_endpoints.py
--rw-r--r--   0 runner    (1001) docker     (127)     1301 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_validate_and_correct_bvecs.py
--rw-r--r--   0 runner    (1001) docker     (127)     1448 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_validate_and_correct_eddy_gradients.py
--rw-r--r--   0 runner    (1001) docker     (127)    14647 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_validate_bids.py
--rw-r--r--   0 runner    (1001) docker     (127)      934 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_verify_space_attributes_compatibility.py
--rw-r--r--   0 runner    (1001) docker     (127)      761 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_visualize_bingham_fit.py
--rw-r--r--   0 runner    (1001) docker     (127)      175 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_visualize_bundles.py
--rw-r--r--   0 runner    (1001) docker     (127)      980 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_visualize_bundles_mosaic.py
--rw-r--r--   0 runner    (1001) docker     (127)     1140 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_visualize_connectivity.py
--rw-r--r--   0 runner    (1001) docker     (127)      645 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_visualize_fodf.py
--rw-r--r--   0 runner    (1001) docker     (127)      178 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_visualize_gradients.py
--rw-r--r--   0 runner    (1001) docker     (127)      878 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_visualize_histogram.py
--rw-r--r--   0 runner    (1001) docker     (127)     4295 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_visualize_scatterplot.py
--rw-r--r--   0 runner    (1001) docker     (127)      174 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_visualize_seeds.py
--rw-r--r--   0 runner    (1001) docker     (127)      176 2023-11-10 20:53:07.000000 scilpy-1.5.post2/scripts/tests/test_visualize_seeds_3d.py
--rw-r--r--   0 runner    (1001) docker     (127)       38 2023-11-10 20:53:19.479519 scilpy-1.5.post2/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (127)     3274 2023-11-10 20:53:07.000000 scilpy-1.5.post2/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:18.595846 scilpy-2.0.0/
+-rw-r--r--   0 runner    (1001) docker     (127)       16 2024-04-23 18:30:11.000000 scilpy-2.0.0/.python-version
+-rw-r--r--   0 runner    (1001) docker     (127)     1335 2024-04-23 18:30:11.000000 scilpy-2.0.0/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (127)      204 2024-04-23 18:30:11.000000 scilpy-2.0.0/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (127)     2755 2024-04-23 18:30:18.595846 scilpy-2.0.0/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)     2761 2024-04-23 18:30:11.000000 scilpy-2.0.0/README.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:18.503846 scilpy-2.0.0/data/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:18.507845 scilpy-2.0.0/data/LUT/
+-rw-r--r--   0 runner    (1001) docker     (127)      920 2024-04-23 18:30:11.000000 scilpy-2.0.0/data/LUT/dk_aggregate_structures.json
+-rw-r--r--   0 runner    (1001) docker     (127)     2504 2024-04-23 18:30:11.000000 scilpy-2.0.0/data/LUT/freesurfer_desikan_killiany.json
+-rw-r--r--   0 runner    (1001) docker     (127)     1421 2024-04-23 18:30:11.000000 scilpy-2.0.0/data/LUT/freesurfer_subcortical.json
+-rw-r--r--   0 runner    (1001) docker     (127)      783 2024-04-23 18:30:11.000000 scilpy-2.0.0/requirements.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:18.507845 scilpy-2.0.0/scilpy/
+-rw-r--r--   0 runner    (1001) docker     (127)      452 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:18.507845 scilpy-2.0.0/scilpy/connectivity/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/connectivity/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8483 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/connectivity/connectivity_tools.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:18.511846 scilpy-2.0.0/scilpy/denoise/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/denoise/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    23315 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/denoise/asym_filtering.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:18.511846 scilpy-2.0.0/scilpy/dwi/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/dwi/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13348 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/dwi/operations.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:18.511846 scilpy-2.0.0/scilpy/dwi/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/dwi/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3017 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/dwi/tests/test_operations.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2738 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/dwi/tests/test_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6452 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/dwi/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:18.511846 scilpy-2.0.0/scilpy/gpuparallel/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/gpuparallel/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11313 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/gpuparallel/opencl_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:18.511846 scilpy-2.0.0/scilpy/gradients/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/gradients/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9265 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/gradients/bvec_bval_tools.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9727 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/gradients/gen_gradient_sampling.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12405 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/gradients/optimize_gradient_sampling.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3327 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/gradients/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:18.511846 scilpy-2.0.0/scilpy/image/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/image/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11842 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/image/labels.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5079 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/image/reslice.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:18.511846 scilpy-2.0.0/scilpy/image/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/image/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4981 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/image/tests/test_labels.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20241 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/image/tests/test_volume_math.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5365 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/image/tests/test_volume_operations.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4496 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/image/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3410 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/image/volume_b0_synthesis.py
+-rw-r--r--   0 runner    (1001) docker     (127)    23787 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/image/volume_math.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22213 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/image/volume_operations.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12046 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/image/volume_space_management.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:18.515846 scilpy-2.0.0/scilpy/io/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/io/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9047 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/io/btensor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3086 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/io/deprecator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4415 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/io/dvc.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5368 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/io/fetcher.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3706 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/io/gradients.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10306 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/io/hdf5.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4736 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/io/image.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10072 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/io/mti.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12446 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/io/streamlines.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2452 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/io/tensor.py
+-rw-r--r--   0 runner    (1001) docker     (127)    41740 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/io/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14395 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/io/varian_fdf.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:18.515846 scilpy-2.0.0/scilpy/preprocessing/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/preprocessing/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4556 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/preprocessing/distortion_correction.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:18.515846 scilpy-2.0.0/scilpy/reconst/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/reconst/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2250 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/reconst/aodf.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15013 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/reconst/bingham.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12391 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/reconst/divide.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3870 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/reconst/fiber_coherence.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9620 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/reconst/fodf.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13764 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/reconst/frf.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12998 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/reconst/mti.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26961 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/reconst/sh.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1861 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/reconst/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:18.515846 scilpy-2.0.0/scilpy/segment/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/segment/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1778 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/segment/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11877 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/segment/recobundlesx.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17573 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/segment/streamlines.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24047 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/segment/tractogram_from_roi.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14126 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/segment/voting_scheme.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:18.519846 scilpy-2.0.0/scilpy/stats/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/stats/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6358 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/stats/matrix_stats.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7979 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/stats/stats.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20428 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/stats/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:18.519846 scilpy-2.0.0/scilpy/tracking/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/tracking/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21354 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/tracking/propagator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10514 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/tracking/seed.py
+-rw-r--r--   0 runner    (1001) docker     (127)    25055 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/tracking/tracker.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18035 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/tracking/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:18.519846 scilpy-2.0.0/scilpy/tractanalysis/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/tractanalysis/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4952 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/tractanalysis/afd_along_streamlines.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4224 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/tractanalysis/bingham_metric_along_streamlines.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12383 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/tractanalysis/bundle_operations.py
+-rw-r--r--   0 runner    (1001) docker     (127)      959 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/tractanalysis/distance_to_centroid.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11835 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/tractanalysis/grid_intersections.pyx
+-rw-r--r--   0 runner    (1001) docker     (127)     2228 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/tractanalysis/json_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1361 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/tractanalysis/quick_tools.pyx
+-rwxr-xr-x   0 runner    (1001) docker     (127)    23930 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/tractanalysis/reproducibility_measures.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13145 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/tractanalysis/scoring.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7495 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/tractanalysis/streamlines_metrics.pyx
+-rw-r--r--   0 runner    (1001) docker     (127)    14376 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/tractanalysis/todi.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5978 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/tractanalysis/todi_util.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4160 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/tractanalysis/tools.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:18.523846 scilpy-2.0.0/scilpy/tractograms/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/tractograms/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12945 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/tractograms/dps_and_dpp_management.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2568 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/tractograms/lazy_tractogram_operations.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16446 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/tractograms/streamline_and_mask_operations.py
+-rw-r--r--   0 runner    (1001) docker     (127)    27844 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/tractograms/streamline_operations.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:18.523846 scilpy-2.0.0/scilpy/tractograms/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/tractograms/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7848 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/tractograms/tests/test_dps_and_dpp_management.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7596 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/tractograms/tests/test_streamline_and_mask_operations.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13641 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/tractograms/tests/test_streamline_operations.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7168 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/tractograms/tests/test_tractogram_operations.py
+-rw-r--r--   0 runner    (1001) docker     (127)    32727 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/tractograms/tractogram_operations.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15120 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/tractograms/uncompress.pyx
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:18.523846 scilpy-2.0.0/scilpy/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)     1092 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1214 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/utils/filenames.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13529 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/utils/metrics_tools.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6772 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/utils/spatial.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2227 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/version.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:18.523846 scilpy-2.0.0/scilpy/viz/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/viz/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:18.523846 scilpy-2.0.0/scilpy/viz/backends/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/viz/backends/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14563 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/viz/backends/fury.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11320 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/viz/backends/pil.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5113 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/viz/backends/vtk.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8084 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/viz/color.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4965 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/viz/gradients.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:18.523846 scilpy-2.0.0/scilpy/viz/legacy/
+-rw-r--r--   0 runner    (1001) docker     (127)     2705 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/viz/legacy/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8312 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/viz/legacy/chord_chart.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11343 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/viz/screenshot.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11546 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/viz/slice.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2415 2024-04-23 18:30:11.000000 scilpy-2.0.0/scilpy/viz/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:18.595846 scilpy-2.0.0/scilpy.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (127)     2755 2024-04-23 18:30:18.000000 scilpy-2.0.0/scilpy.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)    25180 2024-04-23 18:30:18.000000 scilpy-2.0.0/scilpy.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        1 2024-04-23 18:30:18.000000 scilpy-2.0.0/scilpy.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (127)    25270 2024-04-23 18:30:18.000000 scilpy-2.0.0/scilpy.egg-info/entry_points.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      780 2024-04-23 18:30:18.000000 scilpy-2.0.0/scilpy.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (127)       15 2024-04-23 18:30:18.000000 scilpy-2.0.0/scilpy.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:18.551846 scilpy-2.0.0/scripts/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:18.571846 scilpy-2.0.0/scripts/legacy/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      475 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_add_tracking_mask_to_pft_maps.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      446 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_analyse_lesions_load.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      447 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_apply_bias_field_on_dwi.py
+-rw-r--r--   0 runner    (1001) docker     (127)      458 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_apply_transform_to_bvecs.py
+-rw-r--r--   0 runner    (1001) docker     (127)      472 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_apply_transform_to_hdf5.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      471 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_apply_transform_to_image.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      475 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_apply_transform_to_surface.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      484 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_apply_transform_to_tractogram.py
+-rw-r--r--   0 runner    (1001) docker     (127)      496 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_assign_custom_color_to_tractogram.py
+-rw-r--r--   0 runner    (1001) docker     (127)      499 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_assign_uniform_color_to_tractograms.py
+-rw-r--r--   0 runner    (1001) docker     (127)      452 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_clean_qbx_clusters.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      489 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_combine_labels.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      546 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compare_connectivity.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      442 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compress_streamlines.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      421 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_MT_maps.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      417 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_NODDI.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      428 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_NODDI_priors.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      432 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_asym_odf_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (127)      437 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_bundle_mean_std.py
+-rw-r--r--   0 runner    (1001) docker     (127)      530 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_bundle_mean_std_per_point.py
+-rw-r--r--   0 runner    (1001) docker     (127)      490 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_bundle_volume.py
+-rw-r--r--   0 runner    (1001) docker     (127)      479 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_bundle_volume_per_label.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      464 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_bundle_voxel_label_map.py
+-rw-r--r--   0 runner    (1001) docker     (127)      446 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_centroid.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      540 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_connectivity.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      428 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_divide.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      425 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_dti_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (127)      461 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_endpoints_map.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      476 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_fodf_max_in_ventricles.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      428 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_fodf_metrics.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      429 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_freewater.py
+-rw-r--r--   0 runner    (1001) docker     (127)      508 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_hdf5_average_density_map.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      427 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_ihMT_maps.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      430 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_kurtosis_metrics.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      466 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_lobe_specific_fodf_metrics.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      434 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_local_tracking.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      464 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_local_tracking_dev.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      477 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_maps_for_particle_filter_tracking.py
+-rw-r--r--   0 runner    (1001) docker     (127)      474 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_mean_fixel_afd_from_bundles.py
+-rw-r--r--   0 runner    (1001) docker     (127)      481 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_mean_fixel_afd_from_hdf5.py
+-rw-r--r--   0 runner    (1001) docker     (127)      509 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_mean_fixel_lobe_metric_from_bundles.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      416 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_mean_frf.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      425 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_memsmt_fodf.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      422 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_memsmt_frf.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      468 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_metrics_stats_in_ROI.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      419 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_msmt_fodf.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      416 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_msmt_frf.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      443 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_pca.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      419 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_pft.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      442 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_powder_average.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      431 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_qball_metrics.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      424 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_qbx.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      424 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_rish_from_sh.py
+-rw-r--r--   0 runner    (1001) docker     (127)      449 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_seed_by_labels.py
+-rw-r--r--   0 runner    (1001) docker     (127)      462 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_seed_density_map.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      418 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_sf_from_sh.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      424 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_sh_from_signal.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      419 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_ssst_fodf.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      416 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_ssst_frf.py
+-rw-r--r--   0 runner    (1001) docker     (127)      493 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_streamlines_density_map.py
+-rw-r--r--   0 runner    (1001) docker     (127)      476 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_streamlines_length_stats.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      440 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_compute_todi.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      429 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_concatenate_dwi.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      425 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_convert_fdf.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      523 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_convert_gradients_fsl_to_mrtrix.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      523 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_convert_gradients_mrtrix_to_fsl.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      478 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_convert_json_to_xlsx.py
+-rw-r--r--   0 runner    (1001) docker     (127)      501 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_convert_rgb.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      420 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_convert_sh_basis.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      447 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_convert_surface.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      437 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_convert_tensors.py
+-rw-r--r--   0 runner    (1001) docker     (127)      509 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_convert_tractogram.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      480 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_count_non_zero_voxels.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      457 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_count_streamlines.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      436 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_crop_volume.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      451 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_cut_streamlines.py
+-rw-r--r--   0 runner    (1001) docker     (127)      478 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_decompose_connectivity.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      480 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_detect_dwi_volume_outliers.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      455 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_detect_streamlines_loops.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      486 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_dilate_labels.py
+-rw-r--r--   0 runner    (1001) docker     (127)      457 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_estimate_bundles_diameter.py
+-rw-r--r--   0 runner    (1001) docker     (127)      541 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_evaluate_bundles_binary_classification_measures.py
+-rw-r--r--   0 runner    (1001) docker     (127)      479 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_evaluate_bundles_individual_measures.py
+-rw-r--r--   0 runner    (1001) docker     (127)      498 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_evaluate_bundles_pairwise_agreement_measures.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      570 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_evaluate_connectivity_graph_measures.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      592 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_evaluate_connectivity_pairwise_agreement_measures.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      506 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_execute_angle_aware_bilateral_filtering.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      506 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_execute_asymmetric_filtering.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      422 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_extract_b0.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      435 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_extract_dwi_shell.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      448 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_extract_ushape.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      519 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_filter_connectivity.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      484 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_filter_streamlines_by_length.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      499 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_filter_streamlines_by_orientation.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      449 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_filter_tractogram.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      489 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_filter_tractogram_anatomically.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      433 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_fit_bingham_to_fodf.py
+-rw-r--r--   0 runner    (1001) docker     (127)      439 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_fix_dsi_studio_trk.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      484 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_flip_gradients.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      430 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_flip_streamlines.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      438 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_flip_surface.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      417 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_flip_volume.py
+-rw-r--r--   0 runner    (1001) docker     (127)      482 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_generate_gradient_sampling.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      536 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_generate_priors_from_bundle.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      462 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_group_comparison.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      456 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_harmonize_json.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      416 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_image_math.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      456 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_merge_json.py
+-rw-r--r--   0 runner    (1001) docker     (127)      410 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_merge_sh.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      528 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_normalize_connectivity.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      445 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_outlier_rejection.py
+-rw-r--r--   0 runner    (1001) docker     (127)      458 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_perform_majority_vote.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      465 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_plot_mean_std_per_point.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      452 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_prepare_eddy_command.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      455 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_prepare_topup_command.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      564 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_print_connectivity_filenames.py
+-rw-r--r--   0 runner    (1001) docker     (127)      430 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_print_header.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      502 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_project_streamlines_to_map.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      459 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_recognize_multi_bundles.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      472 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_recognize_single_bundle.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      442 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_register_tractogram.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      479 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_remove_invalid_streamlines.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      486 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_remove_labels.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      464 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_remove_outliers_ransac.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      532 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_reorder_connectivity.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      441 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_reorder_dwi_philips.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      494 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_resample_bvals.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      463 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_resample_streamlines.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      442 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_resample_tractogram.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      448 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_resample_volume.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      476 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_reshape_to_reference.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      428 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_run_commit.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      429 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_run_nlmeans.py
+-rw-r--r--   0 runner    (1001) docker     (127)      488 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_save_connections_from_hdf5.py
+-rw-r--r--   0 runner    (1001) docker     (127)      483 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_score_bundles.py
+-rw-r--r--   0 runner    (1001) docker     (127)      456 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_score_tractogram.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      450 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_screenshot_bundle.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      433 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_screenshot_dti.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      442 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_screenshot_volume.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      489 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_screenshot_volume_mosaic_overlap.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      447 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_set_response_function.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      439 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_shuffle_streamlines.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      436 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_smooth_streamlines.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      444 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_smooth_surface.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      424 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_snr_in_roi.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      435 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_split_image.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      432 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_split_tractogram.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      518 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_split_volume_by_ids.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      522 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_split_volume_by_labels.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      430 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_streamlines_math.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      488 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_swap_gradient_axis.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      496 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_uniformize_streamlines_endpoints.py
+-rw-r--r--   0 runner    (1001) docker     (127)      480 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_validate_and_correct_bvecs.py
+-rw-r--r--   0 runner    (1001) docker     (127)      499 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_validate_and_correct_eddy_gradients.py
+-rw-r--r--   0 runner    (1001) docker     (127)      441 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_validate_bids.py
+-rw-r--r--   0 runner    (1001) docker     (127)      497 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_verify_space_attributes_compatibility.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      434 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_visualize_bingham_fit.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      420 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_visualize_bundles.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      463 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_visualize_bundles_mosaic.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      437 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_visualize_connectivity.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      413 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_visualize_fodf.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      450 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_visualize_gradients.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      442 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_visualize_histogram.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      448 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_visualize_scatterplot.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      438 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_visualize_seeds.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      447 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/legacy/scil_visualize_seeds_3d.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     7063 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_NODDI_maps.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     8298 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_NODDI_priors.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     8233 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_aodf_metrics.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    18872 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_bids_validate.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     5295 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_bingham_metrics.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    10051 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_btensor_metrics.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    12336 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_bundle_clean_qbx_clusters.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2013 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_bundle_compute_centroid.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3939 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_bundle_compute_endpoints_map.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    14156 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_bundle_diameter.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     5149 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_bundle_filter_by_occurence.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     7046 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_bundle_generate_priors.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    12453 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_bundle_label_map.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3111 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_bundle_mean_fixel_afd.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     4885 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_bundle_mean_fixel_afd_from_hdf5.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3991 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_bundle_mean_fixel_bingham_metric.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     6545 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_bundle_mean_std.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    15129 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_bundle_pairwise_comparison.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3361 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_bundle_reject_outliers.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     9287 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_bundle_score_many_bundles_one_tractogram.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     9955 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_bundle_score_same_bundle_many_segmentations.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    13443 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_bundle_shape_measures.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2218 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_bundle_volume_per_label.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     5771 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_connectivity_compare_populations.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    20482 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_connectivity_compute_matrices.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    13198 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_connectivity_compute_pca.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     6076 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_connectivity_filter.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     5105 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_connectivity_graph_measures.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     4076 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_connectivity_hdf5_average_density_map.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     5213 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_connectivity_math.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     6932 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_connectivity_normalize.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3478 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_connectivity_pairwise_agreement.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2380 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_connectivity_print_filenames.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     6188 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_connectivity_reorder_rois.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     4407 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_denoising_nlmeans.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    12102 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_dki_metrics.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2132 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_dti_convert_tensors.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    18355 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_dti_metrics.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2435 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_dwi_apply_bias_field.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     7061 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_dwi_compute_snr.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3471 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_dwi_concatenate.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2675 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_dwi_convert_FDF.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2687 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_dwi_detect_volume_outliers.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     4645 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_dwi_extract_b0.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3773 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_dwi_extract_shell.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     4752 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_dwi_powder_average.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     9621 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_dwi_prepare_eddy_command.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     5651 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_dwi_prepare_topup_command.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3695 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_dwi_reorder_philips.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     4005 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_dwi_split_by_indices.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3604 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_dwi_to_sh.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     4061 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_fodf_max_in_ventricles.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    11519 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_fodf_memsmt.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     9239 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_fodf_metrics.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     9921 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_fodf_msmt.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     5301 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_fodf_ssst.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     4290 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_fodf_to_bingham.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     7082 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_freewater_maps.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      233 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_freewater_priors.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2044 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_frf_mean.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    13178 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_frf_memsmt.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    10732 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_frf_msmt.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2170 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_frf_set_diffusivities.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     5179 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_frf_ssst.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     4128 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_get_version.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     1719 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_gradients_apply_transform.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2243 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_gradients_convert.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     8202 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_gradients_generate_sampling.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     4673 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_gradients_modify_axes.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2421 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_gradients_round_bvals.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     4838 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_gradients_validate_correct.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2226 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_gradients_validate_correct_eddy.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     1710 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_header_print_info.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     1262 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_header_validate_compatibility.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    17341 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_json_convert_entries_to_xlsx.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2713 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_json_harmonize_entries.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3567 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_json_merge_entries.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     5194 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_labels_combine.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3812 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_labels_dilate.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2002 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_labels_remove.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3435 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_labels_split_volume_by_ids.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3409 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_labels_split_volume_from_lut.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     6747 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_lesions_info.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1657 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_mti_adjust_B1_header.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    12294 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_mti_maps_MT.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    14037 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_mti_maps_ihMT.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     5740 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_plot_stats_per_point.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     7952 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_qball_metrics.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2386 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_rgb_convert.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     6931 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_search_keywords.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2284 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_sh_convert.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2339 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_sh_fusion.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     7934 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_sh_to_aodf.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3337 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_sh_to_rish.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     8271 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_sh_to_sf.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    10017 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_stats_group_comparison.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2609 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_surface_apply_transform.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2753 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_surface_convert.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     1878 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_surface_flip.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2725 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_surface_smooth.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    11939 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tracking_local.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    12907 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tracking_local_dev.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    12588 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tracking_pft.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3629 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tracking_pft_maps.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2551 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tracking_pft_maps_edit.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     6970 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_apply_transform.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     5760 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_apply_transform_to_hdf5.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    10265 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_assign_custom_color.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     5205 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_assign_uniform_color.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    22039 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_commit.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2178 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_compress.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     6288 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_compute_TODI.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2691 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_compute_density_map.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     1956 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_convert.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     5105 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_convert_hdf5_to_trk.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     1674 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_count_streamlines.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     6984 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_cut_streamlines.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     4930 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_detect_loops.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     9616 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_dpp_math.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3362 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_extract_ushape.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    22160 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_filter_by_anatomy.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2678 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_filter_by_length.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     4885 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_filter_by_orientation.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    16674 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_filter_by_roi.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    10789 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_fix_trk.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     1914 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_flip.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     7974 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_math.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5301 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_pairwise_comparison.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2593 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_print_info.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     6369 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_project_map_to_streamlines.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     9746 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_project_streamlines_to_map.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3359 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_qbx.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3545 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_register.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3767 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_remove_invalid.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     6921 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_resample.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2154 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_resample_nb_points.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     4180 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_seed_density_map.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    19762 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_segment_and_score.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     5713 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_segment_bundles.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    19632 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_segment_bundles_for_connectivity.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     6467 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_segment_one_bundle.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     1598 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_shuffle.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3995 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_smooth.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     5101 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_split.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     4698 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_tractogram_uniformize_endpoints.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     5076 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_viz_bingham_fit.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     8780 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_viz_bundle.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    12686 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_viz_bundle_screenshot_mni.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    13215 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_viz_bundle_screenshot_mosaic.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    14636 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_viz_connectivity.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     8551 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_viz_dti_screenshot.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    15553 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_viz_fodf.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     6852 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_viz_gradients_screenshot.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2540 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_viz_tractogram_seeds.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     4150 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_viz_tractogram_seeds_3d.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2918 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_viz_volume_histogram.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    10693 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_viz_volume_scatterplot.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     8048 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_viz_volume_screenshot.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     6474 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_viz_volume_screenshot_mosaic.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2889 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_volume_apply_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3404 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_volume_b0_synthesis.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3135 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_volume_count_non_zero_voxels.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3090 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_volume_crop.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     1545 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_volume_flip.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     5371 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_volume_math.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2802 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_volume_remove_outliers_ransac.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3447 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_volume_resample.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2205 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_volume_reshape_to_reference.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     5355 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_volume_stats_in_ROI.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2020 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/scil_volume_stats_in_labels.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:18.595846 scilpy-2.0.0/scripts/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1362 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_NODDI_maps.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1402 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_NODDI_priors.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2520 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_aodf_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14670 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_bids_validate.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1985 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_bingham_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6300 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_btensor_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (127)      184 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_bundle_clean_qbx_clusters.py
+-rw-r--r--   0 runner    (1001) docker     (127)      812 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_bundle_compute_centroid.py
+-rw-r--r--   0 runner    (1001) docker     (127)      846 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_bundle_compute_endpoints_map.py
+-rw-r--r--   0 runner    (1001) docker     (127)      940 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_bundle_diameter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1087 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_bundle_filter_by_occurence.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1041 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_bundle_generate_priors.py
+-rw-r--r--   0 runner    (1001) docker     (127)      941 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_bundle_label_map.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1006 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_bundle_mean_fixel_afd.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1063 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_bundle_mean_fixel_afd_from_hdf5.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1036 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_bundle_mean_fixel_bingham_metric.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1460 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_bundle_mean_std.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3630 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_bundle_pairwise_comparison.py
+-rw-r--r--   0 runner    (1001) docker     (127)      997 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_bundle_reject_outliers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1650 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_bundle_score_many_bundles_one_tractogram.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1325 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_bundle_score_same_bundle_many_segmentations.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1065 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_bundle_shape_measures.py
+-rw-r--r--   0 runner    (1001) docker     (127)      878 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_bundle_volume_per_label.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1058 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_connectivity_compare_populations.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1453 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_connectivity_compute_matrices.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1003 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_connectivity_compute_pca.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1048 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_connectivity_filter.py
+-rw-r--r--   0 runner    (1001) docker     (127)      944 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_connectivity_graph_measures.py
+-rw-r--r--   0 runner    (1001) docker     (127)      924 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_connectivity_hdf5_average_density_map.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1680 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_connectivity_math.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1180 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_connectivity_normalize.py
+-rw-r--r--   0 runner    (1001) docker     (127)      944 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_connectivity_pairwise_agreement.py
+-rw-r--r--   0 runner    (1001) docker     (127)      968 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_connectivity_print_filenames.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1831 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_connectivity_reorder_rois.py
+-rw-r--r--   0 runner    (1001) docker     (127)      813 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_denoising_nlmeans.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1297 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_dki_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1250 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_dti_convert_tensors.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1756 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_dti_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (127)      942 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_dwi_apply_bias_field.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1233 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_dwi_compute_snr.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1213 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_dwi_concatenate.py
+-rw-r--r--   0 runner    (1001) docker     (127)      174 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_dwi_convert_FDF.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1239 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_dwi_detect_volume_outliers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1274 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_dwi_extract_b0.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2551 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_dwi_extract_shell.py
+-rw-r--r--   0 runner    (1001) docker     (127)      954 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_dwi_powder_average.py
+-rw-r--r--   0 runner    (1001) docker     (127)      183 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_dwi_prepare_eddy_command.py
+-rw-r--r--   0 runner    (1001) docker     (127)      184 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_dwi_prepare_topup_command.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3300 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_dwi_reorder_philips.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1630 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_dwi_split_by_indices.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1270 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_dwi_to_sh.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1031 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_fodf_max_in_ventricles.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4520 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_fodf_memsmt.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1093 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_fodf_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1579 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_fodf_msmt.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1560 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_fodf_ssst.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1947 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_fodf_to_bingham.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1467 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_freewater_maps.py
+-rw-r--r--   0 runner    (1001) docker     (127)      175 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_freewater_priors.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1470 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_frf_mean.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8133 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_frf_memsmt.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3858 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_frf_msmt.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1636 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_frf_set_diffusivities.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3126 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_frf_ssst.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     1038 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_gradients_apply_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3804 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_gradients_convert.py
+-rw-r--r--   0 runner    (1001) docker     (127)      846 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_gradients_generate_sampling.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1069 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_gradients_modify_axes.py
+-rw-r--r--   0 runner    (1001) docker     (127)      878 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_gradients_round_bvals.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1956 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_gradients_validate_correct.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1533 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_gradients_validate_correct_eddy.py
+-rw-r--r--   0 runner    (1001) docker     (127)      998 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_header_print_info.py
+-rw-r--r--   0 runner    (1001) docker     (127)      879 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_header_validate_compatibility.py
+-rw-r--r--   0 runner    (1001) docker     (127)      854 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_json_convert_entries_to_xlsx.py
+-rw-r--r--   0 runner    (1001) docker     (127)      180 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_json_harmonize_entries.py
+-rw-r--r--   0 runner    (1001) docker     (127)      968 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_json_merge_entries.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1850 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_labels_combine.py
+-rw-r--r--   0 runner    (1001) docker     (127)      929 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_labels_dilate.py
+-rw-r--r--   0 runner    (1001) docker     (127)      892 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_labels_remove.py
+-rw-r--r--   0 runner    (1001) docker     (127)      869 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_labels_split_volume_by_ids.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1014 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_labels_split_volume_from_lut.py
+-rw-r--r--   0 runner    (1001) docker     (127)      170 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_lesions_info.py
+-rw-r--r--   0 runner    (1001) docker     (127)      975 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_mti_adjust_B1_header.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11235 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_mti_maps_MT.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13922 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_mti_maps_ihMT.py
+-rw-r--r--   0 runner    (1001) docker     (127)      850 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_plot_stats_per_point.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1779 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_qball_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (127)      788 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_rgb_convert.py
+-rw-r--r--   0 runner    (1001) docker     (127)      552 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_search_keywords.py
+-rw-r--r--   0 runner    (1001) docker     (127)      898 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_sh_convert.py
+-rw-r--r--   0 runner    (1001) docker     (127)      879 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_sh_fusion.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5277 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_sh_to_aodf.py
+-rw-r--r--   0 runner    (1001) docker     (127)      765 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_sh_to_rish.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2801 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_sh_to_sf.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1109 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_stats_group_comparison.py
+-rw-r--r--   0 runner    (1001) docker     (127)      949 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_surface_apply_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1342 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_surface_convert.py
+-rw-r--r--   0 runner    (1001) docker     (127)      926 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_surface_flip.py
+-rw-r--r--   0 runner    (1001) docker     (127)      850 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_surface_smooth.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8097 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tracking_local.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1195 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tracking_local_dev.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1379 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tracking_pft.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1019 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tracking_pft_maps.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1159 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tracking_pft_maps_edit.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1134 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_apply_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1257 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_apply_transform_to_hdf5.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1757 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_assign_custom_color.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1461 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_assign_uniform_color.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2155 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_commit.py
+-rw-r--r--   0 runner    (1001) docker     (127)      853 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_compress.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1674 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_compute_TODI.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1212 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_compute_density_map.py
+-rw-r--r--   0 runner    (1001) docker     (127)      953 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_convert.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2480 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_convert_hdf5_to_trk.py
+-rw-r--r--   0 runner    (1001) docker     (127)      789 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_count_streamlines.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1689 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_cut_streamlines.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1096 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_detect_loops.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2831 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_dpp_math.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1085 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_extract_ushape.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3297 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_filter_by_anatomy.py
+-rw-r--r--   0 runner    (1001) docker     (127)      939 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_filter_by_length.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1008 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_filter_by_orientation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2147 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_filter_by_roi.py
+-rw-r--r--   0 runner    (1001) docker     (127)      177 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_fix_trk.py
+-rw-r--r--   0 runner    (1001) docker     (127)      952 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_flip.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8640 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_math.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1630 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_pairwise_comparison.py
+-rw-r--r--   0 runner    (1001) docker     (127)      754 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_print_info.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3555 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_project_map_to_streamlines.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3316 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_project_streamlines_to_map.py
+-rw-r--r--   0 runner    (1001) docker     (127)      858 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_qbx.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1113 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_register.py
+-rw-r--r--   0 runner    (1001) docker     (127)      939 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_remove_invalid.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2365 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_resample.py
+-rw-r--r--   0 runner    (1001) docker     (127)      919 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_resample_nb_points.py
+-rw-r--r--   0 runner    (1001) docker     (127)      825 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_seed_density_map.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1486 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_segment_and_score.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1358 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_segment_bundles.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1142 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_segment_bundles_for_connectivity.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1432 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_segment_one_bundles.py
+-rw-r--r--   0 runner    (1001) docker     (127)      791 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_shuffle.py
+-rw-r--r--   0 runner    (1001) docker     (127)      911 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_smooth.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1257 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_split.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1335 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_tractogram_uniformize_endpoints.py
+-rw-r--r--   0 runner    (1001) docker     (127)      793 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_viz_bingham_fit.py
+-rw-r--r--   0 runner    (1001) docker     (127)      858 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_viz_bundle.py
+-rw-r--r--   0 runner    (1001) docker     (127)      184 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_viz_bundle_screenshot_mni.py
+-rw-r--r--   0 runner    (1001) docker     (127)      187 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_viz_bundle_screenshot_mosaic.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1172 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_viz_connectivity.py
+-rw-r--r--   0 runner    (1001) docker     (127)      177 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_viz_dti_screenshot.py
+-rw-r--r--   0 runner    (1001) docker     (127)      677 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_viz_fodf.py
+-rw-r--r--   0 runner    (1001) docker     (127)      183 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_viz_gradients_screenshot.py
+-rw-r--r--   0 runner    (1001) docker     (127)      179 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_viz_tractogram_seeds.py
+-rw-r--r--   0 runner    (1001) docker     (127)      181 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_viz_tractogram_seeds_3d.py
+-rw-r--r--   0 runner    (1001) docker     (127)      925 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_viz_volume_histogram.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4534 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_viz_volume_scatterplot.py
+-rw-r--r--   0 runner    (1001) docker     (127)      664 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_viz_volume_screenshot.py
+-rw-r--r--   0 runner    (1001) docker     (127)      187 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_viz_volume_screenshot_mosaic.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1070 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_volume_apply_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1675 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_volume_b0_synthesis.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1280 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_volume_count_non_zero_voxels.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1011 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_volume_crop.py
+-rw-r--r--   0 runner    (1001) docker     (127)      815 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_volume_flip.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3108 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_volume_math.py
+-rw-r--r--   0 runner    (1001) docker     (127)      836 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_volume_remove_outliers_ransac.py
+-rw-r--r--   0 runner    (1001) docker     (127)      821 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_volume_resample.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1404 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_volume_reshape_to_reference.py
+-rw-r--r--   0 runner    (1001) docker     (127)      929 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_volume_stats_in_ROI.py
+-rw-r--r--   0 runner    (1001) docker     (127)      615 2024-04-23 18:30:11.000000 scilpy-2.0.0/scripts/tests/test_volume_stats_in_labels.py
+-rw-r--r--   0 runner    (1001) docker     (127)       38 2024-04-23 18:30:18.595846 scilpy-2.0.0/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (127)     3754 2024-04-23 18:30:11.000000 scilpy-2.0.0/setup.py
```

### Comparing `scilpy-1.5.post2/LICENSE` & `scilpy-2.0.0/LICENSE`

 * *Files identical despite different names*

### Comparing `scilpy-1.5.post2/PKG-INFO` & `scilpy-2.0.0/PKG-INFO`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: scilpy
-Version: 1.5.post2
+Version: 2.0.0
 Summary: Scilpy: diffusion MRI tools and utilities
 Home-page: https://github.com/scilus/scilpy
 Download-URL: 
 Author: The SCIL developers
 Author-email: 
 Maintainer: Arnaud Boré
 Maintainer-email: arnaud.bore@gmail.com
@@ -13,58 +13,63 @@
 Classifier: Development Status :: 3 - Alpha
 Classifier: Environment :: Console
 Classifier: Intended Audience :: Science/Research
 Classifier: License :: OSI Approved :: MIT License
 Classifier: Operating System :: OS Independent
 Classifier: Programming Language :: Python
 Classifier: Topic :: Scientific/Engineering
-Requires-Python: ~=3.8,<3.11
+Requires-Python: >=3.9,<3.11
 License-File: LICENSE
-Requires-Dist: bids-validator==1.9.*
+Requires-Dist: bids-validator==1.11.*
 Requires-Dist: bctpy==0.5.*
 Requires-Dist: bz2file==0.98.*
 Requires-Dist: coloredlogs==15.0.*
-Requires-Dist: cvxpy==1.3.*
+Requires-Dist: cvxpy==1.4.*
 Requires-Dist: cycler==0.11.*
 Requires-Dist: Cython!=0.29.29,==0.29.*
-Requires-Dist: dipy==1.7.*
-Requires-Dist: dmri-amico==1.5.*
-Requires-Dist: dmri-commit==1.6.*
+Requires-Dist: dipy==1.9.*
+Requires-Dist: deepdiff==6.3.0
+Requires-Dist: dmri-amico==2.0.*
+Requires-Dist: dmri-commit==2.1.*
 Requires-Dist: docopt==0.6.*
+Requires-Dist: dvc==3.48.*
+Requires-Dist: dvc-http==2.32.*
 Requires-Dist: formulaic==0.3.*
-Requires-Dist: fury==0.8.*
+Requires-Dist: fury==0.10.*
 Requires-Dist: future==0.18.*
+Requires-Dist: GitPython==3.1.*
 Requires-Dist: h5py==3.7.*
 Requires-Dist: joblib==1.2.*
 Requires-Dist: kiwisolver==1.4.*
 Requires-Dist: matplotlib==3.6.*
-Requires-Dist: nibabel==4.0.*
+Requires-Dist: nibabel==5.2.*
 Requires-Dist: nilearn==0.9.*
 Requires-Dist: numpy==1.23.*
 Requires-Dist: openpyxl==3.0.*
-Requires-Dist: Pillow==9.3.*
-Requires-Dist: pybids==0.15.*
+Requires-Dist: packaging==23.2.*
+Requires-Dist: Pillow==10.2.*
+Requires-Dist: pybids==0.16.*
 Requires-Dist: pyparsing==3.0.*
 Requires-Dist: PySocks==1.7.*
 Requires-Dist: pytest==7.2.*
 Requires-Dist: pytest-console-scripts==1.3.*
+Requires-Dist: pytest-cov==4.1.0
+Requires-Dist: pytest-html==4.1.1
 Requires-Dist: pytest-mock==3.10.*
 Requires-Dist: python-dateutil==2.8.*
 Requires-Dist: pytz==2022.6.*
 Requires-Dist: requests==2.28.*
 Requires-Dist: scikit-learn==1.2.*
+Requires-Dist: scikit-image==0.22.*
 Requires-Dist: scipy==1.9.*
 Requires-Dist: six==1.16.*
 Requires-Dist: spams==2.6.*
 Requires-Dist: statsmodels==0.13.*
-Requires-Dist: trimeshpy==0.0.2
+Requires-Dist: trimeshpy==0.0.3
 Requires-Dist: vtk==9.2.*
-Requires-Dist: h5py>=2.8.0
-Requires-Dist: packaging>=19.0
-Requires-Dist: tqdm>=4.30.0
 
 
 Scilpy
 ========
 Scilpy is a small library mainly containing small tools and utilities
 to quickly work with diffusion MRI. Most of the tools are based
 on or wrapper of the Dipy_ library.
```

### Comparing `scilpy-1.5.post2/README.md` & `scilpy-2.0.0/README.md`

 * *Files 10% similar despite different names*

```diff
@@ -1,12 +1,18 @@
 # Scilpy
 [![GitHub release (latest by date)](https://img.shields.io/github/v/release/scilus/scilpy)](https://github.com/scilus/scilpy/releases)
 [![Build Status](https://travis-ci.com/scilus/scilpy.svg?branch=master)](https://travis-ci.com/scilus/scilpy)
+[![codecov](https://codecov.io/github/scilus/scilpy/graph/badge.svg?token=oXjDog4YZG)](https://codecov.io/github/scilus/scilpy)
 [![Documentation Status](https://readthedocs.org/projects/scilpy/badge/?version=latest)](https://scilpy.readthedocs.io/en/latest/?badge=latest)
 
+[![PyPI version badge](https://img.shields.io/pypi/v/scilpy?logo=pypi&logoColor=white)](https://pypi.org/project/scilpy)
+[![PyPI - Downloads](https://static.pepy.tech/badge/scilpy)](https://pypi.org/project/scilpy)
+
+[![Docker container badge](https://img.shields.io/docker/v/scilus/scilus?label=docker&logo=docker&logoColor=white)](https://hub.docker.com/r/scilus/scilus)
+
 **Scilpy** is the main library supporting research and development at the Sherbrooke Connectivity Imaging Lab
 ([SCIL]).
 
 **Scilpy** mainly comprises tools and utilities to quickly work with diffusion MRI. Most of the tools are based
 on or are wrappers of the [DIPY] library, and most of them will eventually be migrated to [DIPY]. Those tools implement the recommended workflows and parameters used in the lab.
 
 The library is now built for Python 3.10 so be sure to create a virtual environnement for Python 3.10. If this version is not installed on your computer:
@@ -17,19 +23,31 @@
 
 Make sure your pip is up-to-date before trying to install:
 ```
 pip install --upgrade pip
 ```
 
 The library's structure is mostly aligned on that of [DIPY].
+
 The library and scripts can be installed locally by using:
 ```
 pip install -e .
 ```
 
+If you don't want to install legacy scripts:
+```
+export SCILPY_LEGACY='False'
+pip install -e .
+```
+
+(Then, without the legacy scripts, if you want to use pytest, use:)
+```
+pytest --ignore=scripts/legacy
+```
+
 On Linux, most likely you will have to install libraries for COMMIT/AMICO
 ```
 sudo apt install libblas-dev liblapack-dev
 ```
 
 While on MacOS you will have to use (most likely)
 ```
```

### Comparing `scilpy-1.5.post2/data/LUT/dk_aggregate_structures.json` & `scilpy-2.0.0/data/LUT/dk_aggregate_structures.json`

 * *Files identical despite different names*

### Comparing `scilpy-1.5.post2/data/LUT/freesurfer_desikan_killiany.json` & `scilpy-2.0.0/data/LUT/freesurfer_desikan_killiany.json`

 * *Files identical despite different names*

### Comparing `scilpy-1.5.post2/data/LUT/freesurfer_subcortical.json` & `scilpy-2.0.0/data/LUT/freesurfer_subcortical.json`

 * *Files identical despite different names*

### Comparing `scilpy-1.5.post2/scilpy/direction/peaks.py` & `scilpy-2.0.0/scilpy/tractanalysis/bundle_operations.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,322 +1,344 @@
 # -*- coding: utf-8 -*-
+
+from itertools import count, takewhile
+import logging
+
+from dipy.segment.clustering import QuickBundles
+from dipy.segment.featurespeed import ResampleFeature
+from dipy.segment.metric import AveragePointwiseEuclideanMetric
 import numpy as np
+from dipy.tracking.streamlinespeed import set_number_of_points
+from scipy.spatial import cKDTree
+from sklearn.cluster import KMeans
 
+from scilpy.tractograms.streamline_and_mask_operations import \
+    get_endpoints_density_map
+from scilpy.tractograms.streamline_operations import \
+    resample_streamlines_num_points, get_streamlines_bounding_box
 
-def peak_directions_asym(odf, sphere, relative_peak_threshold=.5,
-                         min_separation_angle=25):
-    """Get the directions of odf peaks. Adapted from
-    dipy.direction.peak.peak_directions.
-
-    Peaks are defined as points on the odf that are greater than at least one
-    neighbor and greater than or equal to all neighbors. Peaks are sorted in
-    descending order by their values then filtered based on their relative size
-    and spacing on the sphere. An odf may have 0 peaks, for example if the odf
-    is perfectly isotropic.
 
-    Differs from dipy.direction.peak.peak_directions in that we consider
-    sphere directions v and -v as distinct directions.
+def get_streamlines_centroid(streamlines, nb_points):
+    """
+    Compute centroid from streamlines using QuickBundles.
 
     Parameters
     ----------
-    odf : 1d ndarray
-        The odf function evaluated on the vertices of `sphere`
-    sphere : Sphere
-        The Sphere providing discrete directions for evaluation.
-    relative_peak_threshold : float in [0., 1.]
-        Only peaks greater than ``min + relative_peak_threshold * scale`` are
-        kept, where ``min = max(0, odf.min())`` and
-        ``scale = odf.max() - min``.
-    min_separation_angle : float in [0, 90]
-        The minimum distance between directions. If two peaks are too close
-        only the larger of the two is returned.
+    streamlines: list of ndarray
+        The list of streamlines from which we compute the centroid.
+    nb_points: int
+        Number of points defining the centroid streamline.
 
     Returns
     -------
-    directions : (N, 3) ndarray
-        N vertices for sphere, one for each peak
-    values : (N,) ndarray
-        peak values
-    indices : (N,) ndarray
-        peak indices of the directions on the sphere
-
-    Notes
-    -----
-    If the odf has any negative values, they will be clipped to zeros.
-
-    """
-    values, indices = local_maxima(odf, sphere.edges)
-
-    # If there is only one peak return
-    n = len(values)
-    if n == 0 or (values[0] < 0.):
-        return np.zeros((0, 3)), np.zeros(0), np.zeros(0, dtype=int)
-    elif n == 1:
-        return sphere.vertices[indices], values, indices
-
-    odf_min = np.min(odf)
-    odf_min = odf_min if (odf_min >= 0.) else 0.
-    # because of the relative threshold this algorithm will give the same peaks
-    # as if we divide (values - odf_min) with (odf_max - odf_min) or not so
-    # here we skip the division to increase speed
-    values_norm = (values - odf_min)
-
-    # Remove small peaks
-    n = search_descending(values_norm, relative_peak_threshold)
-    indices = indices[:n]
-    directions = sphere.vertices[indices]
-
-    # Remove peaks too close together
-    directions, uniq = remove_similar_vertices(directions,
-                                               min_separation_angle)
-    values = values[uniq]
-    indices = indices[uniq]
-    return directions, values, indices
-
-
-def remove_similar_vertices(vertices, theta):
-    """Remove vertices that are less than `theta` degrees from any other.
-    Originally taken from dipy.reconst.recspeed.
-
-    Returns vertices that are at least theta degrees from any other vertex.
-    Vertex v and -v are considered the same so if v and -v are both in
-    `vertices` only one is kept. Also if v and w are both in vertices, w must
-    be separated by theta degrees from v to be unique.
+    List of length one, containing a np.ndarray of shape (nb_points, 3)
+    """
+    resample_feature = ResampleFeature(nb_points=nb_points)
+    quick_bundle = QuickBundles(
+        threshold=np.inf,
+        metric=AveragePointwiseEuclideanMetric(resample_feature))
+    clusters = quick_bundle.cluster(streamlines)
+    centroid_streamlines = clusters.centroids
+
+    return centroid_streamlines
+
+
+def uniformize_bundle_sft(sft, axis=None, ref_bundle=None, swap=False):
+    """Uniformize the streamlines in the given tractogram.
 
     Parameters
     ----------
-    vertices : (N, 3) ndarray
-        N unit vectors.
-    theta : float
-        The minimum separation between vertices in degrees.
+    sft: StatefulTractogram
+         The tractogram that contains the list of streamlines to be uniformized
+    axis: int, optional
+        Orient endpoints in the given axis
+    ref_bundle: streamlines
+        Orient endpoints the same way as this bundle (or centroid)
+    swap: boolean, optional
+        Swap the orientation of streamlines
+    """
+    old_space = sft.space
+    old_origin = sft.origin
+    sft.to_vox()
+    sft.to_corner()
+    density = get_endpoints_density_map(sft, point_to_select=3)
+    indices = np.argwhere(density > 0)
+    kmeans = KMeans(n_clusters=2, random_state=0, copy_x=True,
+                    n_init=20).fit(indices)
+
+    labels = np.zeros(density.shape)
+    for i in range(len(kmeans.labels_)):
+        labels[tuple(indices[i])] = kmeans.labels_[i]+1
+
+    k_means_centers = kmeans.cluster_centers_
+    main_dir_barycenter = np.argmax(
+        np.abs(k_means_centers[0] - k_means_centers[-1]))
+
+    if len(sft.streamlines) > 0:
+        axis_name = ['x', 'y', 'z']
+        if axis is None or ref_bundle is not None:
+            if ref_bundle is not None:
+                ref_bundle.to_vox()
+                ref_bundle.to_corner()
+                centroid = get_streamlines_centroid(ref_bundle.streamlines,
+                                                    20)[0]
+            else:
+                centroid = get_streamlines_centroid(sft.streamlines, 20)[0]
+            main_dir_ends = np.argmax(np.abs(centroid[0] - centroid[-1]))
+            main_dir_displacement = np.argmax(
+                np.abs(np.sum(np.gradient(centroid, axis=0), axis=0)))
+
+            if main_dir_displacement != main_dir_ends \
+                    or main_dir_displacement != main_dir_barycenter:
+                logging.info('Ambiguity in orientation, you should use --axis')
+            axis = axis_name[main_dir_displacement]
+        logging.info('Orienting endpoints in the {} axis'.format(axis))
+        axis_pos = axis_name.index(axis)
+
+        if bool(k_means_centers[0][axis_pos] >
+                k_means_centers[1][axis_pos]) ^ bool(swap):
+            labels[labels == 1] = 3
+            labels[labels == 2] = 1
+            labels[labels == 3] = 2
+
+        for i in range(len(sft.streamlines)):
+            if ref_bundle:
+                res_centroid = set_number_of_points(centroid, 20)
+                res_streamlines = set_number_of_points(sft.streamlines[i], 20)
+                norm_direct = np.sum(
+                    np.linalg.norm(res_centroid - res_streamlines, axis=0))
+                norm_flip = np.sum(
+                    np.linalg.norm(res_centroid - res_streamlines[::-1],
+                                   axis=0))
+                if bool(norm_direct > norm_flip) ^ bool(swap):
+                    sft.streamlines[i] = sft.streamlines[i][::-1]
+                    for key in sft.data_per_point[i]:
+                        sft.data_per_point[key][i] = \
+                            sft.data_per_point[key][i][::-1]
+            else:
+                # Bitwise XOR
+                if (bool(labels[tuple(sft.streamlines[i][0].astype(int))] >
+                         labels[tuple(sft.streamlines[i][-1].astype(int))])
+                        ^ bool(swap)):
+                    sft.streamlines[i] = sft.streamlines[i][::-1]
+                    for key in sft.data_per_point[i]:
+                        sft.data_per_point[key][i] = \
+                            sft.data_per_point[key][i][::-1]
+    sft.to_space(old_space)
+    sft.to_origin(old_origin)
+
+
+def uniformize_bundle_sft_using_mask(sft, mask, swap=False):
+    """Uniformize the streamlines in the given tractogram so head is closer to
+    to a region of interest.
+
+    Parameters
+    ----------
+    sft: StatefulTractogram
+         The tractogram that contains the list of streamlines to be uniformized
+    mask: np.ndarray
+        Mask to use as a reference for the ROI.
+    swap: boolean, optional
+        Swap the orientation of streamlines
+    """
+
+    # barycenter = np.average(np.argwhere(mask), axis=0)
+    old_space = sft.space
+    old_origin = sft.origin
+    sft.to_vox()
+    sft.to_corner()
+
+    tree = cKDTree(np.argwhere(mask))
+    for i in range(len(sft.streamlines)):
+        head_dist = tree.query(sft.streamlines[i][0])[0]
+        tail_dist = tree.query(sft.streamlines[i][-1])[0]
+        if bool(head_dist > tail_dist) ^ bool(swap):
+            sft.streamlines[i] = sft.streamlines[i][::-1]
+            for key in sft.data_per_point[i]:
+                sft.data_per_point[key][i] = \
+                    sft.data_per_point[key][i][::-1]
+
+    sft.to_space(old_space)
+    sft.to_origin(old_origin)
+
+
+def detect_ushape(sft, minU, maxU):
+    """
+    Extract streamlines depending on their "u-shapeness".
+
+    Parameters
+    ----------
+    sft: Statefull tractogram
+        Tractogram used to extract streamlines depending on their ushapeness.
+    minU: Float
+        Minimum ufactor of a streamline.
+    maxU: Float
+        Maximum ufactor of a streamline.
 
     Returns
     -------
-    unique_vertices : (M, 3) ndarray
-        Vertices sufficiently separated from one another.
-    mapping : (N,) ndarray
-        For each element ``vertices[i]`` ($i in 0..N-1$), the index $j$ to a
-        vertex in `unique_vertices` that is less than `theta` degrees from
-        ``vertices[i]``.  Only returned if `return_mapping` is True.
-    indices : (N,) ndarray
-        `indices` gives the reverse of `mapping`.  For each element
-        ``unique_vertices[j]`` ($j in 0..M-1$), the index $i$ to a vertex in
-        `vertices` that is less than `theta` degrees from
-        ``unique_vertices[j]``.  If there is more than one element of
-        `vertices` that is less than theta degrees from `unique_vertices[j]`,
-        return the first (lowest index) matching value.  Only return if
-        `return_indices` is True.
-    """
-    if vertices.shape[1] != 3:
-        raise ValueError('Vertices should be 2D with second dim length 3')
-
-    n_unique = 0
-    # Large enough for all possible sizes of vertices
-    n = vertices.shape[0]
-    cos_similarity = np.cos(np.pi/180 * theta)
-    unique_vertices = np.empty((n, 3), dtype=float)
-
-    index = np.empty(n, dtype=np.uint16)
-
-    for i in range(n):
-        pass_all = True
-        # Check all other accepted vertices for similarity to this one
-        for j in range(n_unique):
-            sim = vertices[i].dot(unique_vertices[j])
-            if sim > cos_similarity:  # too similar, drop
-                pass_all = False
-                # This point unique_vertices[j] already has an entry in index,
-                # so we do not need to update.
-                break
-        if pass_all:  # none similar, keep
-            unique_vertices[n_unique] = vertices[i]
-
-            index[n_unique] = i
-            n_unique += 1
-
-    verts = unique_vertices[:n_unique].copy()
-    out = [verts, index[:n_unique].copy()]
-    return out
-
-
-def search_descending(a, relative_threshold):
-    """`i` in descending array `a` so `a[i] < a[0] * relative_threshold`
-    Originally taken from dipy.reconst.recspeed.
-
-    Call ``T = a[0] * relative_threshold``. Return value `i` will be the
-    smallest index in the descending array `a` such that ``a[i] < T``.
-    Equivalently, `i` will be the largest index such that ``all(a[:i] >= T)``.
-    If all values in `a` are >= T, return the length of array `a`.
+    list: the ids of u-shaped streamlines
+        Only the ids are returned so proper filtering can be done afterwards.
+    """
+    ids = []
+    new_sft = resample_streamlines_num_points(sft, 4)
+    for i, s in enumerate(new_sft.streamlines):
+        if len(s) == 4:
+            first_point = s[0]
+            last_point = s[-1]
+            second_point = s[1]
+            third_point = s[2]
+
+            v1 = first_point - second_point
+            v2 = second_point - third_point
+            v3 = third_point - last_point
+
+            v1 = v1 / np.linalg.norm(v1)
+            v2 = v2 / np.linalg.norm(v2)
+            v3 = v3 / np.linalg.norm(v3)
+
+            val = np.dot(np.cross(v1, v2), np.cross(v2, v3))
+
+            if minU <= val <= maxU:
+                ids.append(i)
+
+    return ids
+
 
+def prune(streamlines, threshold, features):
+    """
+    Discriminate streamlines based on a metrics, usually summary from function
+    outliers_removal_using_hierarchical_quickbundles.
     Parameters
     ----------
-    a : ndarray, ndim=1, c-contiguous
-        Array to be searched.  We assume `a` is in descending order.
-    relative_threshold : float
-        Applied threshold will be ``T`` with ``T = a[0] * relative_threshold``.
-
+    streamlines: list of ndarray
+        The list of streamlines from which inliers and outliers are separated.
+    threshold: float
+        Threshold use to discriminate streamlines using the feature.
+    features: ndarray
+        Values that represent a relevant metric to disciminate streamlines.
     Returns
     -------
-    i : np.intp
-        If ``T = a[0] * relative_threshold`` then `i` will be the largest index
-        such that ``all(a[:i] >= T)``.  If all values in `a` are >= T then
-        `i` will be `len(a)`.
-
-    Examples
-    --------
-    >>> a = np.arange(10, 0, -1, dtype=float)
-    >>> a
-    array([ 10.,   9.,   8.,   7.,   6.,   5.,   4.,   3.,   2.,   1.])
-    >>> search_descending(a, 0.5)
-    6
-    >>> a < 10 * 0.5
-    array([False, False, False, False, False, False,
-           True,  True,  True,  True], dtype=bool)
-    >>> search_descending(a, 1)
-    1
-    >>> search_descending(a, 2)
-    0
-    >>> search_descending(a, 0)
-    10
-    """
-    if a.shape[0] == 0:
-        return 0
-
-    threshold = relative_threshold * a[0]
-    indice = np.where(a > threshold)[0][-1] + 1
-    return indice
-
-
-def local_maxima(odf, edges):
-    """Local maxima of a function evaluated on a discrete set of points.
-    Originally taken from dipy.reconst.recspeed.
+    tuple:
+        Indices for outliers (below threshold),
+        indices for inliers (above threshold).
+    """
+    indices = np.arange(len(streamlines))
+
+    outlier_indices = indices[features < threshold]
+    rest_indices = indices[features >= threshold]
 
-    If a function is evaluated on some set of points where each pair of
-    neighboring points is an edge in edges, find the local maxima.
+    return outlier_indices, rest_indices
+
+
+def outliers_removal_using_hierarchical_quickbundles(streamlines,
+                                                     nb_points=12,
+                                                     min_threshold=0.5,
+                                                     nb_samplings_max=30,
+                                                     sampling_seed=1234,
+                                                     fast_approx=False):
+    """
+    Classify inliers and outliers from a list of streamlines.
 
     Parameters
     ----------
-    odf : array, 1d, dtype=double
-        The function evaluated on a set of discrete points.
-    edges : array (N, 2)
-        The set of neighbor relations between the points. Every edge, ie
-        `edges[i, :]`, is a pair of neighboring points.
+    streamlines: list of ndarray
+        The list of streamlines from which inliers and outliers are separated.
+    min_threshold: float
+        Quickbundles distance threshold for the last threshold.
+    nb_samplings_max: int
+        Number of run executed to explore the search space.
+        A different sampling is used each time.
+    sampling_seed: int
+        Random number generation initialization seed.
 
     Returns
     -------
-    peak_values : ndarray
-        Value of odf at a maximum point. Peak values is sorted in descending
-        order.
-    peak_indices : ndarray
-        Indices of maximum points. Sorted in the same order as `peak_values` so
-        `odf[peak_indices[i]] == peak_values[i]`.
-
-    Notes
-    -----
-    A point is a local maximum if it is > at least one neighbor and >= all
-    neighbors. If no points meet the above criteria, 1 maximum is returned such
-    that `odf[maximum] == max(odf)`.
-
-    See Also
-    --------
-    dipy.core.sphere
-
+    ndarray: Float value representing the 0-1 score for each streamline
     """
+    if nb_samplings_max < 2:
+        raise ValueError("'nb_samplings_max' must be >= 2")
+
+    rng = np.random.RandomState(sampling_seed)
+    resample_feature = ResampleFeature(nb_points=nb_points)
+    metric = AveragePointwiseEuclideanMetric(resample_feature)
+
+    box_min, box_max = get_streamlines_bounding_box(streamlines)
+
+    # Half of the bounding box's halved diagonal length.
+    initial_threshold = np.min(np.abs(box_max - box_min)) / 2.
+
+    # Quickbundle's threshold is halved between hierarchical level.
+    if fast_approx:
+        thresholds = np.array([2 / 1.2**i for i in range(25)][1:])
+        thresholds = np.concatenate(([40, 20, 10, 5, 2.5],
+                                     thresholds[thresholds > min_threshold]))
+    else:
+        thresholds = takewhile(lambda t: t >= min_threshold,
+                               (initial_threshold / 1.2**i for i in count()))
+        thresholds = list(thresholds)
+
+    ordering = np.arange(len(streamlines))
+    path_lengths_per_streamline = 0
 
-    wpeak, count = _compare_neighbors(odf, edges)
-    if count == -1:
-        raise IndexError("Values in edges must be < len(odf)")
-    elif count == -2:
-        raise ValueError("odf can not have nans")
-    indices = wpeak[:count].copy()
-    # Get peak values return
-    values = np.take(odf, indices)
-    # Sort both values and indices
-    values, indices = _cosort(values, indices)
-    return values, indices
-
-
-def _cosort(A, B):
-    """Sorts `A` in descending order and applies the same reordering to `B`
-    Originally taken from dipy.reconst.recspeed."""
-    sorted = A.argsort()[::-1]
-    A = A[sorted]
-    B = B[sorted]
-    return A, B
-
-
-def _compare_neighbors(odf, edges):
-    """Compares every pair of points in edges.
-    Taken from dipy.reconst.recspeed.
+    streamlines_path = np.ones((len(streamlines), len(thresholds),
+                                nb_samplings_max), dtype=int) * -1
+
+    for i in range(nb_samplings_max):
+        rng.shuffle(ordering)
+
+        cluster_orderings = [ordering]
+        for j, threshold in enumerate(thresholds):
+            id_cluster = 0
+
+            next_cluster_orderings = []
+            qb = QuickBundles(metric=metric, threshold=threshold)
+            for cluster_ordering in cluster_orderings:
+                clusters = qb.cluster(streamlines, ordering=cluster_ordering)
+
+                for _, cluster in enumerate(clusters):
+                    streamlines_path[cluster.indices, j, i] = id_cluster
+                    id_cluster += 1
+                    if len(cluster) > 10:
+                        next_cluster_orderings.append(cluster.indices)
+
+            cluster_orderings = next_cluster_orderings
+
+        if i <= 1:  # Needs at least two orderings to compute stderror.
+            continue
+
+        path_lengths_per_streamline = np.sum((streamlines_path != -1),
+                                             axis=1)[:, :i]
+
+    summary = np.mean(path_lengths_per_streamline,
+                      axis=1) / np.max(path_lengths_per_streamline)
+    return summary
+
+
+def remove_outliers(streamlines, threshold, nb_points=12, nb_samplings=30,
+                    fast_approx=False):
+    """
+    Wrapper to classify inliers and outliers from a list of streamlines.
 
     Parameters
     ----------
-    odf : array of double
-        values of points on sphere.
-    edges : array of uint16
-        neighbor relationships on sphere. Every set of neighbors on the sphere
-        should be an edge.
-    wpeak_ptr : pointer
-        pointer to a block of memory which will be updated with the result of
-        the comparisons. This block of memory must be large enough to hold
-        len(odf) longs. The first `count` elements of wpeak will be updated
-        with the indices of the peaks.
+    streamlines: list of ndarray
+        The list of streamlines from which inliers and outliers are separated.
+    threshold: float
+        Quickbundles distance threshold for the last threshold.
+    nb_points: int
+    nb_samplings: int
+    fast_approx: bool
 
     Returns
     -------
-    count : long
-        Number of maxima in odf. A value < 0 indicates an error:
-            -1 : value in edges too large, >= than len(odf)
-            -2 : odf contains nans
-    """
-    wpeak = np.zeros((odf.shape[0],), dtype=np.intp)
-    lenedges = edges.shape[0]
-    lenodf = odf.shape[0]
-    count = 0
-
-    for i in range(lenedges):
-
-        find0 = edges[i, 0]
-        find1 = edges[i, 1]
-        if find0 >= lenodf or find1 >= lenodf:
-            count = -1
-            break
-        odf0 = odf[find0]
-        odf1 = odf[find1]
-
-        """
-        Here `wpeak_ptr` is used as an indicator array that can take one of
-        three values.  If `wpeak_ptr[i]` is:
-        * -1 : point i of the sphere is smaller than at least one neighbor.
-        *  0 : point i is equal to all its neighbors.
-        *  1 : point i is > at least one neighbor and >= all its neighbors.
-
-        Each iteration of the loop is a comparison between neighboring points
-        (the two point of an edge). At each iteration we update wpeak_ptr in
-        the following way::
-
-            wpeak_ptr[smaller_point] = -1
-            if wpeak_ptr[larger_point] == 0:
-                wpeak_ptr[larger_point] = 1
-
-        If the two points are equal, wpeak is left unchanged.
-        """
-        if odf0 < odf1:
-            wpeak[find0] = -1
-            wpeak[find1] |= 1
-        elif odf0 > odf1:
-            wpeak[find0] |= 1
-            wpeak[find1] = -1
-        elif (odf0 != odf0) or (odf1 != odf1):
-            count = -2
-            break
-
-    if count < 0:
-        return count
-
-    # Count the number of peaks and use first count elements of wpeak_ptr to
-    # hold indices of those peaks
-    count = np.count_nonzero(wpeak > 0)
-    nonzero = np.nonzero(wpeak > 0)
-    wpeak[:count] = nonzero[0]
+    A tuple containing
+        list: streamlines considered inliers
+        list: streamlines considered outliers
+    """
+    summary = outliers_removal_using_hierarchical_quickbundles(
+        streamlines, nb_points=nb_points, nb_samplings_max=nb_samplings,
+        fast_approx=fast_approx)
+    outliers_ids, inliers_ids = prune(streamlines, threshold, summary)
+
+    return outliers_ids, inliers_ids
 
-    return wpeak, count
```

### Comparing `scilpy-1.5.post2/scilpy/gpuparallel/opencl_utils.py` & `scilpy-2.0.0/scilpy/gpuparallel/opencl_utils.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,49 +1,75 @@
 # -*- coding: utf-8 -*-
 import numpy as np
+import logging
 import inspect
 import os
 import scilpy
 
 from dipy.utils.optpkg import optional_package
 cl, have_opencl, _ = optional_package('pyopencl')
 
 
+def cl_device_type(device_type_str):
+    if device_type_str == 'cpu':
+        return cl.device_type.CPU
+    if device_type_str == 'gpu':
+        return cl.device_type.GPU
+    return -1
+
+
 class CLManager(object):
     """
-    Class for managing an OpenCL GPU program.
+    Class for managing an OpenCL program.
 
     Wraps a subset of pyopencl functions to simplify its
-    integration with python.
+    integration with python. The OpenCL program can be run
+    on the cpu or on the gpu, given the appropriate drivers
+    are installed.
+
+    When multiple cpu or gpu are available, the
+    one that first comes up in the list of available devices
+    is selected.
 
     Parameters
     ----------
     cl_kernel: CLKernel object
         The CLKernel containing the OpenCL program to manage.
-    n_inputs: int
-        Number of input buffers for the kernel.
-    n_outputs: int
-        Number of output buffers for the kernel.
+    device_type: string
+        The device onto which to run the program. One of 'cpu', 'gpu'.
     """
-    def __init__(self, cl_kernel, n_inputs, n_outputs):
+    def __init__(self, cl_kernel, device_type='gpu'):
         if not have_opencl:
             raise RuntimeError('pyopencl is not installed. '
                                'Cannot create CLManager instance.')
 
-        self.input_buffers = [0] * n_inputs
-        self.output_buffers = [0] * n_outputs
+        # Reduce verbose level for pyopencl
+        logging.getLogger('pytools.persistent_dict').setLevel(logging.CRITICAL)
+        logging.getLogger('pyopencl').setLevel(logging.CRITICAL)
+
+        self.input_buffers = []
+        self.output_buffers = []
+
+        # maps key to index in buffers list
+        self.inputs_mapping = {}
+        self.outputs_mapping = {}
 
         # Find the best device for running GPU tasks
         platforms = cl.get_platforms()
         best_device = None
         for p in platforms:
             devices = p.get_devices()
             for d in devices:
-                if best_device is None:
-                    best_device = d
+                d_type = d.get_info(cl.device_info.TYPE)
+                if d_type == cl_device_type(device_type)\
+                   and best_device is None:
+                    best_device = d  # take the first device of right type
+
+        if best_device is None:
+            raise ValueError('No device of type {} found'.format(device_type))
 
         self.context = cl.Context(devices=[best_device])
         self.queue = cl.CommandQueue(self.context)
         program = cl.Program(self.context, cl_kernel.code_string).build()
         self.kernel = cl.Kernel(program, cl_kernel.entry_point)
 
     class OutBuffer(object):
@@ -60,67 +86,128 @@
             Datatype for output.
         """
         def __init__(self, buf, shape, dtype):
             self.buf = buf
             self.shape = shape
             self.dtype = dtype
 
-    def add_input_buffer(self, arg_pos, arr, dtype=np.float32):
+    def add_input_buffer(self, key, arr=None, dtype=np.float32):
         """
         Add an input buffer to the kernel program. Input buffers
         must be added in the same order as they are declared inside
         the kernel code (.cl file).
 
         Parameters
         ----------
-        arg_pos: int
-            Position of the buffer in the input buffers list.
+        key: string
+            Name of the buffer in the input buffers list. Used for
+            referencing when updating buffers.
         arr: numpy ndarray
             Data array.
         dtype: dtype, optional
             Optional type for array data. It is recommended to use float32
             whenever possible to avoid unexpected behaviours.
 
-        Returns
-        -------
-        indice: int
-            Index of the input buffer in the input buffers list.
-
         Note
         ----
         Array is reordered as fortran array and then flattened. This is
         important to keep in mind when writing kernel code.
 
         For example, for a 3-dimensional array of shape (X, Y, Z), the flat
         index for position i, j, k is idx = i + j * X + z * X * Y.
         """
-        # convert to fortran ordered, dtype array
+        buf = None
+        if arr is not None:
+            # convert to fortran ordered, dtype array
+            arr = np.asfortranarray(arr, dtype=dtype)
+            buf = cl.Buffer(self.context, cl.mem_flags.READ_ONLY |
+                            cl.mem_flags.COPY_HOST_PTR, hostbuf=arr)
+
+        if key in self.inputs_mapping.keys():
+            raise ValueError('Invalid key for buffer!')
+
+        self.inputs_mapping[key] = len(self.input_buffers)
+        self.input_buffers.append(buf)
+
+    def update_input_buffer(self, key, arr, dtype=np.float32):
+        """
+        Update an input buffer. Input buffers must first be added
+        to program using `add_input_buffer`.
+
+        Parameters
+        ----------
+        key: string
+            Name of the buffer in the input buffers list.
+        arr: numpy ndarray
+            Data array.
+        dtype: dtype, optional
+            Optional type for array data. It is recommended to use float32
+            whenever possible to avoid unexpected behaviours.
+        """
+        if key not in self.inputs_mapping.keys():
+            raise ValueError('Invalid key for buffer!')
+        argpos = self.inputs_mapping[key]
+
         arr = np.asfortranarray(arr, dtype=dtype)
         buf = cl.Buffer(self.context,
                         cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR,
                         hostbuf=arr)
-        self.input_buffers[arg_pos] = buf
+        self.input_buffers[argpos] = buf
 
-    def add_output_buffer(self, arg_pos, shape, dtype=np.float32):
+    def add_output_buffer(self, key, shape=None, dtype=np.float32):
         """
-        Add an output buffer.
+        Add an output buffer to the kernel program. Output buffers
+        must be added in the same order as they are declared inside
+        the kernel code (.cl file).
 
         Parameters
         ----------
-        arg_pos: int
-            Position of the buffer in the output buffers list.
+        key: string
+            Name of the buffer in the output buffers list. Used for
+            referencing when updating buffers.
         shape: tuple
             Shape of the output array.
         dtype: dtype, optional
-            Data type for the output. It is recommended to keep
-            float32 to avoid unexpected behaviour.
+            Optional type for array data. It is recommended to use float32
+            whenever possible to avoid unexpected behaviours.
+        """
+        if key in self.outputs_mapping.keys():
+            raise ValueError('Invalid key for buffer!')
+
+        buf = None
+        if shape is not None:
+            buf = cl.Buffer(self.context, cl.mem_flags.WRITE_ONLY,
+                            np.prod(shape) * np.dtype(dtype).itemsize)
+
+        self.outputs_mapping[key] = len(self.output_buffers)
+        self.output_buffers.append(self.OutBuffer(buf, shape, dtype))
+
+    def update_output_buffer(self, key, shape, dtype=np.float32):
+        """
+        Update an output buffer. Output buffers must first be added
+        to program using `add_output_buffer`.
+
+        Parameters
+        ----------
+        key: string
+            Name of the buffer in the output buffers list.
+        shape: tuple
+            New shape of the output array.
+        dtype: dtype, optional
+            Optional type for array data. It is recommended to use float32
+            whenever possible to avoid unexpected behaviours.
         """
+        if key not in self.outputs_mapping.keys():
+            raise ValueError('Invalid key for buffer!')
+        argpos = self.outputs_mapping[key]
+
         buf = cl.Buffer(self.context, cl.mem_flags.WRITE_ONLY,
                         np.prod(shape) * np.dtype(dtype).itemsize)
-        self.output_buffers[arg_pos] = self.OutBuffer(buf, shape, dtype)
+        out_buf = self.OutBuffer(buf, shape, dtype)
+        self.output_buffers[argpos] = out_buf
 
     def run(self, global_size, local_size=None):
         """
         Execute the kernel code on the GPU.
 
         Parameters
         ----------
@@ -165,15 +252,19 @@
     module: string
         Scilpy module in which the kernel code is located.
     filename: string
         Name for the file containing the kernel code.
     """
     def __init__(self, entrypoint, module, filename):
         path_to_kernel = self._get_kernel_path(module, filename)
-        f = open(path_to_kernel, 'r')
+        try:
+            f = open(path_to_kernel, 'r')
+        except Exception:
+            raise ValueError('OpenCL file not found in {}'
+                             .format(path_to_kernel))
         self.code = f.readlines()
         self.entrypoint = entrypoint
 
     def _get_kernel_path(self, module, filename):
         """
         Get the full path for the OpenCL kernel located in scilpy
         module `module` with filename `filename`.
```

### Comparing `scilpy-1.5.post2/scilpy/gradientsampling/multiple_shell_energy.py` & `scilpy-2.0.0/scilpy/gradients/gen_gradient_sampling.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,309 +1,300 @@
 # -*- coding: utf-8 -*-
 
-################################################
-# Author: Emmanuel Caruyer <caruyer@gmail.com> #
-#                                              #
-# Code to generate multiple-shell gradient     #
-# sampling, with optimal angular coverage. This #
-# implements the method described in Caruyer   #
-# et al., MRM 69(6), pp. 1534-1540, 2013.      #
-# This software comes with no warranty, etc.   #
-################################################
+"""
+Most of this code was modified from code by Emmanuel Caruyer
+<caruyer@gmail.com>.
 
-import numpy as np
+See his original code on GitHub:
+https://github.com/ecaruyer/qspace/tree/master
+
+The code was reorganized, but general process is kept the same.
+"""
 
+import numpy as np
 from scipy import optimize
 
+from scilpy.gradients.utils import random_uniform_on_sphere
+
 
-def equality_constraints(bvecs, *args):
+def generate_gradient_sampling(nb_samples_per_shell, verbose=1):
     """
-    Spherical equality constraint. Returns 0 if bvecs lies on the unit sphere.
+    Wrapper code to generate gradient sampling from Caruyer's
+    multiple_shell_energy.py
+
+    Code to generate multiple-shell gradient sampling, with optimal angular
+    coverage. This implements the method described in Caruyer et al.,
+    MRM 69(6), pp. 1534-1540, 2013.
+
+    Generates the bvecs of a multiple shell gradient sampling using generalized
+    Jones electrostatic repulsion.
 
     Parameters
     ----------
-    bvecs : array-like shape (N * 3)
+    nb_samples_per_shell: list[int]
+        Number of samples for each shell, starting from lowest.
+    verbose: int
+        0 = silent, 1 = summary upon completion, 2 = print iterations
+        (To be sent to scipy).
 
-    Returns
-    -------
-    array shape (N,) : Difference between squared vector norms and 1.
+    Return
+    ------
+    bvecs: numpy.array of shape [n, 3]
+        bvecs normalized to 1.
+    shell_idx: numpy.array
+        Shell index for each bvec.
     """
-    N = int(bvecs.shape[0] / 3)
-    bvecs = bvecs.reshape((N, 3))
-    return (bvecs ** 2).sum(1) - 1.0
 
+    nb_shells = len(nb_samples_per_shell)
 
-def grad_equality_constraints(bvecs, *args):
-    """
-    Return normals to the surface constraint (wich corresponds to
-    the gradient of the implicit function).
+    # Groups of shells and relative coupling weights
+    shell_groups = ()
+    for i in range(nb_shells):
+        shell_groups += ([i],)
 
-    Parameters
-    ----------
-    bvecs : array-like shape (N * 3)
+    shell_groups += (list(range(nb_shells)),)
+    alphas = list(len(shell_groups) * (1.0,))
+    weights = _compute_weights(nb_shells, nb_samples_per_shell,
+                               shell_groups, alphas)
 
-    Returns
-    -------
-    array shape (N, N * 3). grad[i, j] contains
-    $\\partial f_i / \\partial x_j$
-    """
-    N = bvecs.shape[0] / 3
-    bvecs = bvecs.reshape((N, 3))
-    bvecs = (bvecs.T / np.sqrt((bvecs ** 2).sum(1))).T
-    grad = np.zeros((N, N * 3))
-    for i in range(3):
-        grad[:, i * N:(i+1) * N] = np.diag(bvecs[:, i])
-    return grad
+    # Where the optimized gradient sampling is computed
+    # max_iter hardcoded to fit default Caruyer's value
+    bvecs = _generate_gradient_sampling_with_weights(
+        nb_shells, nb_samples_per_shell, weights, max_iter=100,
+        verbose=verbose)
 
+    shell_idx = np.repeat(range(nb_shells), nb_samples_per_shell)
 
-def electrostatic_repulsion(bvecs, weight_matrix, alpha=1.0):
+    return bvecs, shell_idx
+
+
+def _compute_weights(nb_shells, nb_points_per_shell, shell_groups, alphas):
     """
-    Electrostatic-repulsion objective function. The alpha parameter controls
-    the power repulsion (energy varies as $1 / ralpha$).
+    Computes the weights array from a set of shell groups to couple, and
+    coupling weights.
 
     Parameters
-    ---------
-    bvecs : array-like shape (N * 3,)
-        Vectors.
-    weight_matrix: array-like, shape (N, N)
-        The contribution weight of each pair of points.
-    alpha : float
-        Controls the power of the repulsion. Default is 1.0
+    ----------
+    nb_shells: int
+        Number of shells
+    nb_points_per_shell: list of ints
+        Number of points per shell
+    shell_groups: tuple
+        tuple listing the groups of shells as lists of indices
+    alphas: list
+        list of weights per group of shells
 
     Returns
     -------
-    energy : float
-        sum of all interactions between any two vectors.
+    weights: nd.ndarray
+        weigths for each group of shells
     """
-    epsilon = 1e-9
-    N = bvecs.shape[0] // 3
-    bvecs = bvecs.reshape((N, 3))
-    energy = 0.0
-    for i in range(N):
-        indices = (np.arange(N) > i)
-        diffs = ((bvecs[indices] - bvecs[i]) ** 2).sum(1) ** alpha
-        sums = ((bvecs[indices] + bvecs[i]) ** 2).sum(1) ** alpha
-        energy += (weight_matrix[i, indices] *
-                   (1.0 / (diffs + epsilon) + 1.0 / (sums + epsilon))).sum()
-    return energy
+    weights = np.zeros((nb_shells, nb_shells))
+    for shell_group, alpha in zip(shell_groups, alphas):
+        total_nb_points = 0
+        for shell_id in shell_group:
+            total_nb_points += nb_points_per_shell[shell_id]
+        for i in shell_group:
+            for j in shell_group:
+                weights[i, j] += alpha / total_nb_points**2
+    return weights
 
 
-def grad_electrostatic_repulsion(bvecs, weight_matrix, alpha=1.0):
+def _generate_gradient_sampling_with_weights(
+        nb_shells, nb_points_per_shell, weights, max_iter=100, verbose=2):
     """
-    1st-order derivative of electrostatic-like repulsion energy.
+    Creates a set of sampling directions on the desired number of shells.
 
     Parameters
     ----------
-    bvecs : array-like shape (N * 3,)
-        Vectors.
-    weight_matrix: array-like, shape (N, N)
-        The contribution weight of each pair of points.
-    alpha : floating-point. controls the power of the repulsion. Default is 1.0
+    nb_shells : int
+        The number of shells
+    nb_points_per_shell : list, shape (nb_shells,)
+        A list of integers containing the number of points on each shell.
+    weights : array-like, shape (S, S)
+        weighting parameter, control coupling between shells and how this
+        balances.
+    max_iter: int
+        Maximum number of interations
 
     Returns
     -------
-    grad : numpy.ndarray
-        gradient of the objective function
+    bvecs : array shape (K, 3) where K is the total number of points
+            The points are stored by shell.
     """
-    N = bvecs.shape[0] // 3
-    bvecs = bvecs.reshape((N, 3))
-    grad = np.zeros((N, 3))
-    for i in range(N):
-        indices = (np.arange(N) != i)
-        diffs = ((bvecs[indices] - bvecs[i]) ** 2).sum(1) ** (alpha + 1)
-        sums = ((bvecs[indices] + bvecs[i]) ** 2).sum(1) ** (alpha + 1)
-        grad[i] += (- 2 * alpha * weight_matrix[i, indices] *
-                    (bvecs[i] - bvecs[indices]).T / diffs).sum(1)
-        grad[i] += (- 2 * alpha * weight_matrix[i, indices] *
-                    (bvecs[i] + bvecs[indices]).T / sums).sum(1)
-    grad = grad.reshape(N * 3)
-    return grad
+    # Total number of points
+    nb_point_total = np.sum(nb_points_per_shell)
+
+    # Initialized with random directions
+    bvecs = random_uniform_on_sphere(nb_point_total)
+    bvecs = bvecs.reshape(nb_point_total * 3)
+
+    bvecs = optimize.fmin_slsqp(_multiple_shell_energy, bvecs,
+                                f_eqcons=_constraint_is_bvec_on_sphere,
+                                fprime=_grad_multiple_shell_energy,
+                                iter=max_iter,
+                                acc=1.0e-9,
+                                args=(nb_shells, nb_points_per_shell, weights),
+                                iprint=verbose)
+    bvecs = bvecs.reshape((nb_point_total, 3))
+    bvecs = (bvecs.T / np.sqrt((bvecs ** 2).sum(1))).T
+    return bvecs
 
 
-def cost(bvecs, S, Ks, weights):
+def _multiple_shell_energy(bvecs, nb_shells, nb_points_per_shell, weights):
     """
-    Objective function for multiple-shell energy.
+    Objective function (cost function) for multiple-shell energy.
+
+    This is the main function called during optimization, used as
+    func(x, *args) with args = (nb_shells, nb_points_per_shell, weights)
 
     Parameters
     ----------
     bvecs : array-like shape (N * 3,)
-
-    S: int
+        The bvecs
+    nb_shells: int
         Number of shells.
-    Ks: list of ints, len(Ks) = S. Number of points per shell.
+    nb_points_per_shell: list of ints, len(Ks) = S.
+        Number of points per shell.
     weights : array-like, shape (S, S)
         Weighting parameter, control coupling between shells and how this
         balances.
 
     Returns
     -------
     electrostatic_repulsion: float
-        sum of all interactions between any two vectors.
+        Sum of all interactions between any two vectors.
     """
-    K = np.sum(Ks)
-    indices = np.cumsum(Ks).tolist()
+    nb_points_total = np.sum(nb_points_per_shell)
+    indices = np.cumsum(nb_points_per_shell).tolist()
     indices.insert(0, 0)
-    weight_matrix = np.zeros((K, K))
-    for s1 in range(S):
-        for s2 in range(S):
+    weight_matrix = np.zeros((nb_points_total, nb_points_total))
+    for s1 in range(nb_shells):
+        for s2 in range(nb_shells):
             weight_matrix[indices[s1]:indices[s1 + 1],
                           indices[s2]:indices[s2 + 1]] = weights[s1, s2]
-    return electrostatic_repulsion(bvecs, weight_matrix)
+    return _electrostatic_repulsion_energy(bvecs, weight_matrix)
 
 
-def grad_cost(bvecs, S, Ks, weights):
+def _electrostatic_repulsion_energy(bvecs, weight_matrix, alpha=1.0):
     """
-    gradient of the objective function for multiple shells sampling.
+    Electrostatic-repulsion objective function. The alpha parameter controls
+    the power repulsion (energy varies as $1 / ralpha$).
 
     Parameters
-    ----------
+    ---------
     bvecs : array-like shape (N * 3,)
-    S : int
-        number of shells
-    Ks : list of ints
-        len(Ks) = S. Number of points per shell.
-    weights : array-like, shape (S, S)
-        weighting parameter, control coupling between shells and how this
-        balances.
+        Vectors, flattened.
+    weight_matrix: array-like, shape (N, N)
+        The contribution weight of each pair of points.
+    alpha : float
+        Controls the power of the repulsion. Default is 1.0
 
     Returns
     -------
-    grad_electrostatic_repulsion: float
-        gradient of the objective function
+    energy : float
+        sum of all interactions between any two vectors.
     """
-    K = int(bvecs.shape[0] / 3)
-    indices = np.cumsum(Ks).tolist()
-    indices.insert(0, 0)
-    weight_matrix = np.zeros((K, K))
-    for s1 in range(S):
-        for s2 in range(S):
-            weight_matrix[indices[s1]:indices[s1 + 1],
-                          indices[s2]:indices[s2 + 1]] = weights[s1, s2]
-
-    return grad_electrostatic_repulsion(bvecs, weight_matrix)
+    epsilon = 1e-9
+    nb_bvecs = bvecs.shape[0] // 3
+    bvecs = bvecs.reshape((nb_bvecs, 3))
+    energy = 0.0
+    for i in range(nb_bvecs):
+        indices = (np.arange(nb_bvecs) > i)
+        diffs = ((bvecs[indices] - bvecs[i]) ** 2).sum(1) ** alpha
+        sums = ((bvecs[indices] + bvecs[i]) ** 2).sum(1) ** alpha
+        energy += (weight_matrix[i, indices] *
+                   (1.0 / (diffs + epsilon) + 1.0 / (sums + epsilon))).sum()
+    return energy
 
 
-def multiple_shell(nb_shells, nb_points_per_shell, weights, max_iter=100,
-                   verbose=2):
+def _constraint_is_bvec_on_sphere(bvecs, *args):
     """
-    Creates a set of sampling directions on the desired number of shells.
+    Spherical equality constraint. Returns 0 if bvecs lies on the unit sphere.
+
+    This is used as f_eqcons(x, *args), where
+    args = (nb_shells, nb_points_per_shell, weights)
+
+    (We do not need args here, but it is sent by scipy and must be kept here.)
 
     Parameters
     ----------
-    nb_shells : the number of shells
-    nb_points_per_shell : list, shape (nb_shells,)
-        A list of integers containing the number of points on each shell.
-    weights : array-like, shape (S, S)
-        weighting parameter, control coupling between shells and how this
-        balances.
-    max_iter: int
-        Maximum number of interations
+    bvecs : array-like shape (N * 3)
+        Vectors, flattened.
 
     Returns
     -------
-    bvecs : array shape (K, 3) where K is the total number of points
-            The points are stored by shell.
+    array shape (N,) : Difference between squared vector norms and 1.
     """
-    # Total number of points
-    K = np.sum(nb_points_per_shell)
-
-    # Initialized with random directions
-    bvecs = random_uniform_on_sphere(K)
-    bvecs = bvecs.reshape(K * 3)
-
-    bvecs = optimize.fmin_slsqp(cost, bvecs.reshape(K * 3),
-                                f_eqcons=equality_constraints,
-                                fprime=grad_cost,
-                                iter=max_iter,
-                                acc=1.0e-9,
-                                args=(nb_shells,
-                                      nb_points_per_shell,
-                                      weights),
-                                iprint=verbose)
-    bvecs = bvecs.reshape((K, 3))
-    bvecs = (bvecs.T / np.sqrt((bvecs ** 2).sum(1))).T
-    return bvecs
-
+    nb_bvecs = int(bvecs.shape[0] / 3)
+    bvecs = bvecs.reshape((nb_bvecs, 3))
+    return (bvecs ** 2).sum(1) - 1.0
 
-def write_multiple_shells(bvecs, nb_shells, nb_points_per_shell, filename):
-    """
-    Export multiple shells to text file.
 
-    Parameters
-    ----------
-    bvecs : array-like shape (K, 3)
-        vectors
-    nb_shells: int
-        Number of shells
-    nb_points_per_shell: array-like shape (nb_shells, )
-        A list of integers containing the number of points on each shell.
-    filename : str
-        output filename
+def _grad_multiple_shell_energy(bvecs, nb_shells, nb_points_per_shell,
+                                weights):
     """
-    datafile = open(filename, 'w')
-    datafile.write('#shell-id\tx\ty\tz\n')
-    k = 0
-    for s in range(nb_shells):
-        for n in range(nb_points_per_shell[s]):
-            datafile.write("%d\t%f\t%f\t%f\n" %
-                           (s, bvecs[k, 0], bvecs[k, 1], bvecs[k, 2]))
-            k += 1
-    datafile.close()
-
+    Gradient of the objective function for multiple shells sampling.
 
-def random_uniform_on_sphere(K):
-    """
-    Creates a set of K pseudo-random unit vectors, following a uniform
-    distribution on the sphere.
+    This is called as fprime(x, *args) during optimization, with
+    args = (nb_shells, nb_points_per_shell, weights)
 
     Parameters
     ----------
-    K: int
-        Number of vectors
+    bvecs : array-like shape (N * 3,)
+        The b-vectors, flattened.
+    nb_shells : int
+        Number of shells
+    nb_points_per_shell : list of ints
+        Number of points per shell.
+    weights : array-like, shape (S, S)
+        Weighting parameter, control coupling between shells and how this
+        balances.
 
     Returns
     -------
-    bvecs: nd.array
-        pseudo-random unit vector
+    grad_electrostatic_repulsion: float
+        Gradient of the objective function.
     """
-    phi = 2 * np.pi * np.random.rand(K)
-
-    r = 2 * np.sqrt(np.random.rand(K))
-    theta = 2 * np.arcsin(r / 2)
-
-    bvecs = np.zeros((K, 3))
-    bvecs[:, 0] = np.sin(theta) * np.cos(phi)
-    bvecs[:, 1] = np.sin(theta) * np.sin(phi)
-    bvecs[:, 2] = np.cos(theta)
+    nb_bvecs = int(bvecs.shape[0] / 3)
+    indices = np.cumsum(nb_points_per_shell).tolist()
+    indices.insert(0, 0)
+    weight_matrix = np.zeros((nb_bvecs, nb_bvecs))
+    for s1 in range(nb_shells):
+        for s2 in range(nb_shells):
+            weight_matrix[indices[s1]:indices[s1 + 1],
+                          indices[s2]:indices[s2 + 1]] = weights[s1, s2]
 
-    return bvecs
+    return _grad_electrostatic_repulsion_energy(bvecs, weight_matrix)
 
 
-def compute_weights(nb_shells, nb_points_per_shell, shell_groups, alphas):
+def _grad_electrostatic_repulsion_energy(bvecs, weight_matrix, alpha=1.0):
     """
-    Computes the weights array from a set of shell groups to couple, and
-    coupling weights.
+    1st-order derivative of electrostatic-like repulsion energy.
 
     Parameters
     ----------
-    nb_shells: int
-        Number of shells
-    nb_points_per_shell: list of ints
-        Number of points per shell
-    shell_groups: tuple
-        tuple listing the groups of shells as lists of indices
-    alphas: list
-        list of weights per group of shells
+    bvecs : array-like shape (N * 3,)
+        Vectors.
+    weight_matrix: array-like, shape (N, N)
+        The contribution weight of each pair of bvec.
+    alpha : float
+        Controls the power of the repulsion. Default is 1.0
+
     Returns
     -------
-    weights: nd.ndarray
-        weigths for each group of shells
+    grad : numpy.ndarray
+        gradient of the objective function
     """
-    weights = np.zeros((nb_shells, nb_shells))
-    for shell_group, alpha in zip(shell_groups, alphas):
-        total_nb_points = 0
-        for shell_id in shell_group:
-            total_nb_points += nb_points_per_shell[shell_id]
-        for i in shell_group:
-            for j in shell_group:
-                weights[i, j] += alpha / total_nb_points**2
-    return weights
+    nb_bvecs = bvecs.shape[0] // 3
+    bvecs = bvecs.reshape((nb_bvecs, 3))
+    grad = np.zeros((nb_bvecs, 3))
+    for i in range(nb_bvecs):
+        indices = (np.arange(nb_bvecs) != i)
+        diffs = ((bvecs[indices] - bvecs[i]) ** 2).sum(1) ** (alpha + 1)
+        sums = ((bvecs[indices] + bvecs[i]) ** 2).sum(1) ** (alpha + 1)
+        grad[i] += (- 2 * alpha * weight_matrix[i, indices] *
+                    (bvecs[i] - bvecs[indices]).T / diffs).sum(1)
+        grad[i] += (- 2 * alpha * weight_matrix[i, indices] *
+                    (bvecs[i] + bvecs[indices]).T / sums).sum(1)
+    grad = grad.reshape(nb_bvecs * 3)
+    return grad
```

### Comparing `scilpy-1.5.post2/scilpy/gradientsampling/optimize_gradient_sampling.py` & `scilpy-2.0.0/scilpy/gradients/optimize_gradient_sampling.py`

 * *Files 16% similar despite different names*

```diff
@@ -2,345 +2,365 @@
 
 import logging
 
 import numpy as np
 from scipy.spatial.distance import cdist, pdist, squareform
 
 
-# TODO: make this robust to having b0s
-def swap_sampling_eddy(points, shell_idx, verbose=1):
+def swap_sampling_eddy(bvecs, shell_idx):
     """
-    Optimize the bvecs of fixed multi-shell gradient sampling for eddy
-    currents correction (fsl EDDY).
+    Optimize the bvecs of fixed multi-shell gradient sampling for eddy currents
+    correction (fsl EDDY).
 
-    Bruteforce approach to maximally spread the bvec,
-    shell per shell.
+    Bruteforce approach to maximally spread the bvec, shell per shell.
 
     For each shell:
         For each vector:
             1) find the closest neighbor,
             2) flips it,
             3) if global system energy is better, keep it flipped
 
         repeat until convergence.
 
     Parameters
     ----------
-    points: numpy.array, bvecs normalized to 1.
-    shell_idx: numpy.array, Shell index for bvecs in points.
-    verbose: 0 = silent, 1 = summary upon completion, 2 = print iterations.
+    bvecs: numpy.array
+        bvecs normalized to 1.
+    shell_idx: numpy.array
+        Shell index for bvecs.
 
-    Return
-    ------
-    points: numpy.array, bvecs normalized to 1.
-    shell_idx: numpy.array, Shell index for bvecs in points.
+    Returns
+    -------
+    new_bvecs: numpy.array
+        bvecs normalized to 1.
+    shell_idx: numpy.array
+        Shell index for bvecs.
     """
 
-    new_points = points.copy()
+    new_bvecs = bvecs.copy()
+    nb_points_per_shell = _compute_nb_points_per_shell_from_idx(shell_idx)
+    max_nb_iter = 100
 
-    Ks = compute_ks_from_shell_idx(shell_idx)
-
-    maxIter = 100
-
-    for shell in range(len(Ks)):
+    logging.debug("Verifying shells for eddy current optimization")
+    for shell in range(len(nb_points_per_shell)):
         # Extract points from shell
-        shell_pts = points[shell_idx == shell].copy()
+        this_shell_idx = shell_idx == shell
+        shell_pts = bvecs[this_shell_idx].copy()
 
         logging.debug('Shell = {}'.format(shell))
 
         # System energy matrix
         # TODO: test other energy functions such as electron repulsion
         dist = squareform(pdist(shell_pts, 'Euclidean')) \
             + 2 * np.eye(shell_pts.shape[0])
 
         it = 0
         converged = False
-
-        while (it < maxIter) and not converged:
+        while (it < max_nb_iter) and not converged:
             converged = True
             # For each bvec on the shell
             for pts_idx in range(len(shell_pts)):
                 # Find closest neighbor w.r.t. metric of dist
-                toMove = np.argmin(dist[pts_idx])
-                # Compute new column of system matrix with flipped toMove point
-                new_col = cdist(shell_pts, -shell_pts[None, toMove]).squeeze()
+                to_move = np.argmin(dist[pts_idx])
 
-                old_pts_ener = dist[toMove].sum()
-                new_pts_ener = new_col.sum()
+                # Compute new column of system matrix with flipped to_move
+                # point
+                new_col = cdist(shell_pts, -shell_pts[None, to_move]).squeeze()
 
+                old_pts_ener = dist[to_move].sum()
+                new_pts_ener = new_col.sum()
                 if new_pts_ener > old_pts_ener:
-                    # Swap sign of point toMove
-                    shell_pts[toMove] *= -1
-                    dist[:, toMove] = new_col
-                    dist[toMove, :] = new_col
+                    # Swap sign of point to_move
+                    shell_pts[to_move] *= -1
+                    dist[:, to_move] = new_col
+                    dist[to_move, :] = new_col
 
                     converged = False
 
-                    logging.debug('Swapped {} ({:.2f} -->  \
-                                  {:.2f})'.format(toMove,
-                                                  old_pts_ener,
-                                                  new_pts_ener))
+                    logging.debug('Swapped {} ({:.2f} --> {:.2f})'
+                                  .format(to_move, old_pts_ener, new_pts_ener))
 
             it += 1
 
-        new_points[shell_idx == shell] = shell_pts
+        new_bvecs[this_shell_idx] = shell_pts
 
-    logging.info('Eddy current swap optimization finished.')
+    return new_bvecs, shell_idx
 
-    return new_points, shell_idx
 
-
-def compute_ks_from_shell_idx(shell_idx):
+def _compute_nb_points_per_shell_from_idx(shell_idx):
     """
     Recover number of points per shell from point-wise shell index.
 
     Parameters
     ----------
     shell_idx: numpy.array
         Shell index of gradient sampling.
 
     Return
     ------
     Ks: list
         number of samples for each shell, starting from lowest.
     """
-    K = len(set(shell_idx))
+    nb_shells = len(set(shell_idx))
 
-    Ks = []
-    for idx in range(K):
-        Ks.append(np.sum(shell_idx == idx))
+    nb_points_per_shell = []
+    for idx in range(nb_shells):
+        nb_points_per_shell.append(np.sum(shell_idx == idx))
 
-    return Ks
+    return nb_points_per_shell
 
 
-def add_b0s(points, shell_idx, b0_every=10, finish_b0=False, verbose=1):
+def add_b0s_to_bvecs(bvecs, shell_idx, start_b0=True, b0_every=None,
+                     finish_b0=False):
     """
     Add interleaved b0s to gradient sampling.
 
     Parameters
     ----------
-    points: numpy.array, bvecs normalized to 1.
-    shell_idx: numpy.array, Shell index for bvecs in points.
-    b0_every: integer, final gradient sampling will have a b0 every b0_every
-              samples
-    finish_b0: boolean, Option to add a b0 as last sample.
-    verbose: 0 = silent, 1 = summary upon completion, 2 = print iterations.
+    bvecs: numpy.array,
+        bvecs normalized to 1.
+    shell_idx: numpy.array
+        Shell index for bvecs.
+    start_b0: bool
+        Option to add a b0 at the beginning.
+    b0_every: integer or None
+        Final gradient sampling will have a b0 every b0_every samples.
+        (start_b0 must be true)
+    finish_b0: bool
+        Option to add a b0 as last sample.
 
     Return
     ------
-    points: numpy.array
+    new_bvecs: numpy.array
         bvecs normalized to 1.
     shell_idx: numpy.array
-        Shell index for bvecs in points.
+        Shell index for bvecs. Vectors with shells of value -1 are b0 vectors.
+    nb_new_b0s: int
+        The number of b0s interleaved.
     """
-
-    new_points = []
+    new_bvecs = []
     new_shell_idx = []
 
-    for idx in range(shell_idx.shape[0]):
-        if not idx % (b0_every - 1):
-            # insert b0
-            new_points.append(np.array([0.0, 0.0, 0.0]))
-            new_shell_idx.append(-1)
-
-        new_points.append(points[idx])
-        new_shell_idx.append(shell_idx[idx])
+    # Only a b0 at the beginning.
+    # Same as a b0 every n with n > nb_samples
+    # By default, we do add a b0
+    nb_points_total = len(shell_idx)
+
+    # Prepare b0_every
+    if b0_every is not None:
+        if not start_b0:
+            raise ValueError("Can't add a b0 every {} values with option "
+                             "start_b0 at False.".format(b0_every))
+    elif start_b0:
+        # Setting b0_every to one more than the total number creates the right
+        # result.
+        b0_every = nb_points_total + 1
+
+    if b0_every is not None:
+        for idx in range(nb_points_total):
+            if not idx % (b0_every - 1):
+                # insert b0
+                new_bvecs.append(np.array([0.0, 0.0, 0.0]))
+                new_shell_idx.append(-1)  # Shell -1 ==> means b0.
+
+            # Add pre-defined points.
+            new_bvecs.append(bvecs[idx])
+            new_shell_idx.append(shell_idx[idx])
 
     if finish_b0 and (new_shell_idx[-1] != -1):
         # insert b0
-        new_points.append(np.array([0.0, 0.0, 0.0]))
+        new_bvecs.append(np.array([0.0, 0.0, 0.0]))
         new_shell_idx.append(-1)
 
-    logging.info('Interleaved {} b0s'.format(len(new_shell_idx) -
-                                             shell_idx.shape[0]))
+    nb_new_b0s = len(new_shell_idx) - shell_idx.shape[0]
 
-    return np.array(new_points), np.array(new_shell_idx)
+    return np.asarray(new_bvecs), np.asarray(new_shell_idx), nb_new_b0s
 
 
-def correct_b0s_philips(points, shell_idx, verbose=1):
+def correct_b0s_philips(bvecs, shell_idx):
     """
-    Replace the [0.0, 0.0, 0.0] value of b0s bvecs
-    by existing bvecs in the gradient sampling.
+    Replace the [0.0, 0.0, 0.0] value of b0s bvecs by existing bvecs in the
+    gradient sampling, except possibly the first one.
 
-    This is useful because Recon 1.0 of Philips allocates memory
-    proportional to (total nb. of diff. bvals) x (total nb. diff. bvecs)
-    and we can't leave multiple b0s with b-vector [0.0, 0.0, 0.0] and b-value 0
-    because (b-vector, b-value) pairs have to be unique.
+    This is useful because Recon 1.0 of Philips allocates memory proportional
+    to (total nb. of diff. bvals) x (total nb. diff. bvecs) and we can't leave
+    multiple b0s with b-vector [0.0, 0.0, 0.0] and b-value 0 because
+    (b-vector, b-value) pairs have to be unique.
 
     Parameters
     ----------
-    points: numpy.array
+    bvecs: numpy.array
         bvecs normalized to 1
     shell_idx: numpy.array
-        Shell index for bvecs in points.
-    verbose: 0 = silent, 1 = summary upon completion, 2 = print iterations.
+        Shell index for bvecs. Vectors with shells of value -1 are b0 vectors.
 
     Return
     ------
-    points: numpy.array
-        bvecs normalized to 1
+    new_bvecs: numpy.array
+        bvecs normalized to 1. b0 vectors are now impossible to know as they
+        are replaced by random values from another vector.
     shell_idx: numpy.array
-        Shell index for bvecs in points
+        Shell index for bvecs. b0 vectors still have shells of value -1.
     """
 
-    new_points = points.copy()
+    new_bvecs = bvecs.copy()
+
+    # We could replace by a random value, but (we think that... to verify?)
+    # the machine is more efficient if we copy the previous gradient; the
+    # machine then does have to change its parameters between images.
+
+    # 1. By default, other shells should already be ok (never twice the same
+    # gradients per shell.)
+    # 2. Assume non-collinearity of non-b0s bvecs (i.e. Caruyer sampler type)
+    # between shells. Could be verified?
+    # 3. Assume that we never have two b0s one after the other. This is how we
+    # build them in our scripts.
 
-    # Assume non-collinearity of non-b0s bvecs (i.e. Caruyer sampler type)
-    new_points[np.where(shell_idx == -1)[0][1:]] \
-        = new_points[np.where(shell_idx == -1)[0][1:] - 1]
+    new_bvecs[np.where(shell_idx == -1)[0][1:]] \
+        = new_bvecs[np.where(shell_idx == -1)[0][1:] - 1]
 
     logging.info('Done adapting b0s for Philips scanner.')
 
-    return new_points, shell_idx
+    return new_bvecs, shell_idx
 
 
-def compute_min_duty_cycle_bruteforce(points, shell_idx, bvals, ker_size=10,
-                                      Niter=100000, verbose=1, plotting=False,
-                                      rand_seed=0):
+def compute_min_duty_cycle_bruteforce(bvecs, shell_idx, bvals, ker_size=10,
+                                      nb_iter=100000, rand_seed=0):
     """
-    Optimize the ordering of non-b0s sample to optimize gradient duty-cycle.
+    Optimize the ordering of non-b0 samples to optimize gradient duty-cycle.
 
     Philips scanner (and other) will find the peak power requirements with its
     duty cycle model (this is an approximation) and increase the TR accordingly
-    to the hardware needs. This minimize this effects by:
+    to the hardware needs. This minimizes this effect by:
 
     1) Randomly permuting the non-b0s samples
     2) Finding the peak X, Y, and Z amplitude with a sliding-window
-    3) Compute peak power needed as max(peak_x, peak_y, peak_z)
-    4) Keeps the permutation yielding the lowest peak power
+    3) Computing the peak power needed as max(peak_x, peak_y, peak_z)
+    4) Keeping the permutation yielding the lowest peak power
 
     Parameters
     ----------
-    points: numpy.array
+    bvecs: numpy.array
         bvecs normalized to 1
     shell_idx: numpy.array
-        Shell index for bvecs in points.
+        Shell index for bvecs.
     bvals: list
         increasing bvals, b0 last.
     ker_size: int
         kernel size for the sliding window.
-    Niter: int
+    nb_iter: int
         number of bruteforce iterations.
-    verbose: 0 = silent, 1 = summary upon completion, 2 = print iterations.
-    plotting: bool
-        plot the energy at each iteration.
     rand_seed: int
         seed for the random permutations.
 
     Return
     ------
-    points: numpy.array
+    new_bvecs: numpy.array
         bvecs normalized to 1.
     shell_idx: numpy.array
-        Shell index for bvecs in points.
+        Shell index for bvecs.
     """
 
     logging.debug('Shuffling Data (N_iter = {}, \
-                                   ker_size = {})'.format(Niter, ker_size))
-
-    if plotting:
-        store_best_value = []
+                                   ker_size = {})'.format(nb_iter, ker_size))
 
     non_b0s_mask = shell_idx != -1
     N_dir = non_b0s_mask.sum()
 
-    q_scheme = np.abs(points * np.sqrt(np.array([bvals[idx] for idx in shell_idx]))[:, None])
+    sqrt_val = np.sqrt(np.array([bvals[idx] for idx in shell_idx]))
+    q_scheme = np.abs(bvecs * sqrt_val[:, None])
 
     q_scheme_current = q_scheme.copy()
 
     ordering_best = np.arange(N_dir)
     power_best = compute_peak_power(q_scheme_current, ker_size=ker_size)
-
-    if plotting:
-        store_best_value.append((0, power_best))
+    logging.info("Duty cycle: initial peak power = {}".format(power_best))
 
     np.random.seed(rand_seed)
 
-    for it in range(Niter):
-        if not it % np.ceil(Niter/10.):
-            logging.debug('Iter {} / {}  : {}'.format(it, Niter, power_best))
+    for it in range(nb_iter):
+        if not it % np.ceil(nb_iter / 10.):
+            logging.debug('Iter {} / {}  : {}'.format(it, nb_iter, power_best))
 
         ordering_current = np.random.permutation(N_dir)
         q_scheme_current[non_b0s_mask] \
             = q_scheme[non_b0s_mask][ordering_current]
 
         power_current = compute_peak_power(q_scheme_current, ker_size=ker_size)
 
         if power_current < power_best:
             ordering_best = ordering_current.copy()
             power_best = power_current
 
-            if plotting:
-                store_best_value.append((it+1, power_best))
+    logging.info('Duty cycle optimization finished ({} iterations). '
+                 'Final peak power: {}'.format(nb_iter, power_best))
 
-    logging.debug('Iter {} / {}  : {}'.format(Niter, Niter, power_best))
-
-    logging.info('Duty cycle optimization finished.')
-
-    if plotting:
-        store_best_value = np.array(store_best_value)
-        import pylab as pl
-        pl.plot(store_best_value[:, 0], store_best_value[:, 1], '-o')
-        pl.show()
-
-    new_points = points.copy()
-    new_points[non_b0s_mask] = points[non_b0s_mask][ordering_best]
+    new_bvecs = bvecs.copy()
+    new_bvecs[non_b0s_mask] = bvecs[non_b0s_mask][ordering_best]
 
     new_shell_idx = shell_idx.copy()
     new_shell_idx[non_b0s_mask] = shell_idx[non_b0s_mask][ordering_best]
 
-    return new_points, new_shell_idx
+    return new_bvecs, new_shell_idx
 
 
 def compute_peak_power(q_scheme, ker_size=10):
     """
+    Function suggested by Guillaume Gilbert.
+
+    Optimize the diffusion gradient table by minimizing the maximum gradient
+    load on any of the 3 axes over a preset temporal window (i.e. successive
+    b-vectors).
+
+    In short, we want to avoid using the same gradient axis (x, y, or z)
+    intensely for many successive b-vectors.
 
     Parameters
     ------
     q_scheme: nd.array
         Scheme of acquisition.
     ker_size: int
         Kernel size (default=10).
 
     Return
     ------
         Max peak power from q_scheme.
     """
 
-    # Note: np.convolve inverses the filter
+    # Using a filter of ones = moving average.
     ker = np.ones(ker_size)
 
+    # Note: np.convolve inverses the filter
     pow_x = np.convolve(q_scheme[:, 0], ker, 'full')[:-(ker_size-1)]
     pow_y = np.convolve(q_scheme[:, 1], ker, 'full')[:-(ker_size-1)]
     pow_z = np.convolve(q_scheme[:, 2], ker, 'full')[:-(ker_size-1)]
 
     max_pow_x = np.max(pow_x)
     max_pow_y = np.max(pow_y)
     max_pow_z = np.max(pow_z)
 
     return np.max([max_pow_x, max_pow_y, max_pow_z])
 
 
 def compute_bvalue_lin_q(bmin=0.0, bmax=3000.0, nb_of_b_inside=2,
-                         exclude_bmin=True, verbose=1):
+                         exclude_bmin=True):
     """
-    Compute bvals linearly distributed in q-value in the
-    interval [bmin, bmax].
+    Compute bvals linearly distributed in q-value in the interval [bmin, bmax].
+    This leads to sqrt(b_values) linearly distributed.
 
     Parameters
     ----------
     bmin: float
         Minimum b-value, lower b-value bounds.
     bmax: float
         Maximum b-value, upper b-value bounds.
     nb_of_b_inside: int
         number of b-value excluding bmin and bmax.
     exclude_bmin: bool
         exclude bmin from the interval, useful if bmin = 0.0.
-    verbose: 0 = silent, 1 = summary upon completion, 2 = print iterations.
 
     Return
     ------
     bvals: list
         increasing bvals.
     """
 
@@ -352,59 +372,35 @@
 
     logging.info('bvals linear in q: {}'.format(bvals))
 
     return bvals
 
 
 def compute_bvalue_lin_b(bmin=0.0, bmax=3000.0, nb_of_b_inside=2,
-                         exclude_bmin=True, verbose=1):
+                         exclude_bmin=True):
     """
-    Compute bvals linearly distributed in b-value in the
-    interval [bmin, bmax].
+    Compute bvals linearly distributed in b-value in the interval [bmin, bmax].
 
     Parameters
     ----------
     bmin: float
         Minimum b-value, lower b-value bounds.
     bmax: float
         Maximum b-value, upper b-value bounds.
     nb_of_b_inside: int
         number of b-value excluding bmin and bmax.
     exclude_bmin: boolean
         exclude bmin from the interval, useful if bmin = 0.0.
-    verbose: 0 = silent, 1 = summary upon completion, 2 = print iterations.
 
     Return
     ------
     bvals: list
         increasing bvals.
     """
 
     bvals = list(np.linspace(bmin, bmax, nb_of_b_inside + 2))
     if exclude_bmin:
         bvals = bvals[1:]
 
     logging.info('bvals linear in b: {}'.format(bvals))
 
     return bvals
-
-
-def add_bvalue_b0(bvals, b0_value=0.0):
-    """
-    Add the b0 value to the bvals list.
-
-    Parameters
-    ----------
-    bvals: list
-        bvals of the non-b0 shells.
-    b0_value: float
-        bvals of the b0s
-    verbose: 0 = silent, 1 = summary upon completion, 2 = print iterations.
-
-    Return
-    ------
-    bvals: list
-        bvals of the shells and b0s.
-    """
-
-    bvals.append(b0_value)
-    return bvals
```

### Comparing `scilpy-1.5.post2/scilpy/image/datasets.py` & `scilpy-2.0.0/scilpy/image/volume_space_management.py`

 * *Files identical despite different names*

### Comparing `scilpy-1.5.post2/scilpy/image/labels.py` & `scilpy-2.0.0/scilpy/image/labels.py`

 * *Files 7% similar despite different names*

```diff
@@ -20,15 +20,15 @@
     ------
     data: numpy.ndarray
         Data (dtype: np.uint16).
     """
     curr_type = in_img.get_data_dtype()
 
     if np.issubdtype(curr_type, np.signedinteger) or \
-       np.issubdtype(curr_type, np.unsignedinteger):
+            np.issubdtype(curr_type, np.unsignedinteger):
         return np.asanyarray(in_img.dataobj).astype(np.uint16)
     else:
         basename = os.path.basename(in_img.get_filename())
         raise IOError('The image {} cannot be loaded as label because '
                       'its format {} is not compatible with a label '
                       'image'.format(basename, curr_type))
 
@@ -111,21 +111,26 @@
     ----------
     data_list: list
         List of np.ndarray. Data as labels.
     indices_per_input_volume: list[np.ndarray]
         List of np.ndarray containing the indices to use in each input volume.
     out_labels_choice: tuple(str, any)
         Tuple of a string expressing the choice of output option and the
-        associated necessary value. Choices are:
+        associated necessary value.
+        Choices are:
+
         ('all_labels'): Keeps values from the input volumes, or with
             merge_groups, used the volumes ordering.
+
         ('out_label_ids', list): Out labels will be renamed as given from
             the list.
+
         ('unique'): Out labels will be renamed to range from 1 to
             total_nb_labels (+ the background).
+
         ('group_in_m'): Add (x * 10 000) to each volume labels, where x is the
             input volume order number.
     background_id: int
         Background id, excluded from output. The value is also used as output
         background value. Default : 0.
     merge_groups: bool
         If true, indices from indices_per_input_volume will be merged for each
@@ -291,7 +296,52 @@
 
     # Change values of those background
     data = data.flatten()
     data[id_background.T] = data[id_label.T]
     data = data.reshape(img_shape)
 
     return data
+
+
+def get_stats_in_label(map_data, label_data, label_lut):
+    """
+    Get statistics about a map for each label in an atlas.
+
+    Parameters
+    ----------
+    map_data: np.ndarray
+        The map from which to get statistics.
+    label_data: np.ndarray
+        The loaded atlas.
+    label_lut: dict
+        The loaded label LUT (look-up table).
+
+    Returns
+    -------
+    out_dict: dict
+        A dict with one key per label name, and its values are the computed
+        statistics.
+    """
+    (label_indices, label_names) = zip(*label_lut.items())
+
+    out_dict = {}
+    for label, name in zip(label_indices, label_names):
+        label = int(label)
+        if label != 0:
+            curr_data = (map_data[np.where(label_data == label)])
+            nb_vx_roi = np.count_nonzero(label_data == label)
+            nb_seed_vx = np.count_nonzero(curr_data)
+
+            if nb_seed_vx != 0:
+                mean_seed = np.sum(curr_data) / nb_seed_vx
+                max_seed = np.max(curr_data)
+                std_seed = np.sqrt(np.mean(abs(curr_data[curr_data != 0] -
+                                               mean_seed) ** 2))
+
+                out_dict[name] = {'ROI-idx': label,
+                                  'ROI-name': str(name),
+                                  'nb-vx-roi': int(nb_vx_roi),
+                                  'nb-vx-seed': int(nb_seed_vx),
+                                  'max': int(max_seed),
+                                  'mean': float(mean_seed),
+                                  'std': float(std_seed)}
+    return out_dict
```

### Comparing `scilpy-1.5.post2/scilpy/image/operations.py` & `scilpy-2.0.0/scilpy/image/volume_math.py`

 * *Files 17% similar despite different names*

```diff
@@ -3,36 +3,41 @@
 """
 Utility operations provided for scil_image_math.py
 and scil_connectivity_math.py
 They basically act as wrappers around numpy to avoid installing MRtrix/FSL
 to apply simple operations on nibabel images or numpy arrays.
 """
 
+
+from itertools import combinations
 from collections import OrderedDict
-import logging
 
 import nibabel as nib
 import numpy as np
+from numpy.lib import stride_tricks
 from scipy.ndimage import (binary_closing, binary_dilation,
                            binary_erosion, binary_opening,
                            gaussian_filter)
+from skimage.filters import threshold_otsu
 
-from scilpy.utils.util import is_float
+from scilpy.utils import is_float
 
 
 EPSILON = np.finfo(float).eps
 
 
 def get_array_ops():
     """Get a dictionary of all functions relating to array operations"""
     return OrderedDict([
         ('lower_threshold', lower_threshold),
         ('upper_threshold', upper_threshold),
         ('lower_threshold_eq', lower_threshold_eq),
         ('upper_threshold_eq', upper_threshold_eq),
+        ('lower_threshold_otsu', lower_threshold_otsu),
+        ('upper_threshold_otsu', upper_threshold_otsu),
         ('lower_clip', lower_clip),
         ('upper_clip', upper_clip),
         ('absolute_value', absolute_value),
         ('round', around),
         ('ceil', ceil),
         ('floor', floor),
         ('normalize_sum', normalize_sum),
@@ -43,14 +48,15 @@
         ('invert', invert),
         ('addition', addition),
         ('subtraction', subtraction),
         ('multiplication', multiplication),
         ('division', division),
         ('mean', mean),
         ('std', std),
+        ('correlation', correlation),
         ('union', union),
         ('intersection', intersection),
         ('difference', difference),
     ])
 
 
 def get_image_ops():
@@ -94,37 +100,96 @@
 
 
 def _validate_length(input_list, length, at_least=False):
     """Make sure the the input list has the right number of arguments
     (length)."""
     if at_least:
         if not len(input_list) >= length:
-            logging.error(
-                'This operation requires at least {} operands.'.format(length))
-            raise ValueError
+            raise ValueError('This operation requires at least {}'
+                             ' operands.'.format(length))
+
     else:
         if not len(input_list) == length:
-            logging.error(
-                'This operation requires exactly {} operands.'.format(length))
-            raise ValueError
+            raise ValueError('This operation requires exactly {} '
+                             'operands.'.format(length))
 
 
 def _validate_type(x, dtype):
     """Make sure that the input has the right type."""
     if not isinstance(x, dtype):
-        logging.error(
-            'The input must be of type {} for this operation.'.format(dtype))
-        raise ValueError
+        raise ValueError('The input must be of type {} for this'
+                         ' operation.'.format(dtype))
 
 
 def _validate_float(x):
     """Make sure that the input can be casted to a float."""
     if not is_float(x):
-        logging.error('The input must be float/int for this operation.')
-        raise ValueError
+        raise ValueError('The input must be float/int for this operation.')
+
+
+def cut_up_cube(data, blck):
+    """
+    cut_up_cube: DATA BLOCK STRIDE
+        Cut up a cube of data into patches.
+        - blck is the size of the patches.
+        - strd is the stride between patches.
+        The last cube will be padded with zeros to ensure identical dimensions.
+    """
+    strd = 1
+    pad_size = (blck[0] - 1) // 2
+    data = np.pad(data, (pad_size, pad_size),
+                  'constant', constant_values=(0, 0))
+    sh = np.array(data.shape)
+    blck = np.asanyarray(blck)
+    strd = np.asanyarray(strd)
+    nbl = (sh - blck) // strd + 1
+    strides = np.r_[data.strides * strd, data.strides]
+    dims = np.r_[nbl, blck]
+    data = stride_tricks.as_strided(data, strides=strides, shape=dims)
+
+    return data
+
+
+def lower_threshold_otsu(input_list, ref_img):
+    """
+    lower_threshold_otsu: IMG
+        All values below or equal to the Otsu threshold will be set to zero.
+        All values above the Otsu threshold will be set to one.
+    """
+    _validate_length(input_list, 1)
+    _validate_type(input_list[0], nib.Nifti1Image)
+
+    output_data = np.zeros(ref_img.header.get_data_shape(), dtype=np.float64)
+    data = input_list[0].get_fdata(dtype=np.float64)
+    threshold = threshold_otsu(data)
+
+    output_data[data <= threshold] = 0
+    output_data[data > threshold] = 1
+
+    return output_data
+
+
+def upper_threshold_otsu(input_list, ref_img):
+    """
+    upper_threshold_otsu: IMG
+        All values below the Otsu threshold will be set to one.
+        All values above or equal to the Otsu threshold will be set to zero.
+        Equivalent to lower_threshold_otsu followed by an inversion.
+    """
+    _validate_length(input_list, 1)
+    _validate_type(input_list[0], nib.Nifti1Image)
+
+    output_data = np.zeros(ref_img.header.get_data_shape(), dtype=np.float64)
+    data = input_list[0].get_fdata(dtype=np.float64)
+    threshold = threshold_otsu(data)
+
+    output_data[data < threshold] = 1
+    output_data[data >= threshold] = 0
+
+    return output_data
 
 
 def lower_threshold_eq(input_list, ref_img):
     """
     lower_threshold_eq: IMG THRESHOLD
         All values below the threshold will be set to zero.
         All values above or equal the threshold will be set to one.
@@ -569,14 +634,77 @@
             input_data.append(data)
 
         img.uncache()
 
     return np.rollaxis(np.stack(input_data), axis=0, start=4)
 
 
+def correlation(input_list, ref_img, patch_radius=1):
+    """
+    correlation: IMGs
+        Compute the correlation average of multiple images.
+    """
+    _validate_length(input_list, 2, at_least=True)
+
+    if isinstance(input_list[0], nib.Nifti1Image):
+        _validate_imgs(*input_list, ref_img)
+        data_shape = input_list[0].header.get_data_shape()
+    else:
+        data_shape = input_list[0].shape
+
+    sizes = (patch_radius * 2 + 1, patch_radius * 2 + 1, patch_radius * 2 + 1)
+    combs = list(combinations(range(len(input_list)), r=2))
+    all_corr = np.zeros(data_shape + (len(combs),), dtype=np.float32)
+
+    np.random.seed(0)
+    for i, comb in enumerate(combs):
+        img_1 = input_list[comb[0]]
+        img_2 = input_list[comb[1]]
+
+        if isinstance(img_1, nib.Nifti1Image):
+            data_1 = img_1.get_fdata(dtype=np.float32)
+        else:
+            data_1 = img_1
+        if isinstance(img_2, nib.Nifti1Image):
+            data_2 = img_2.get_fdata(dtype=np.float32)
+        else:
+            data_2 = img_2
+
+        patches_1 = cut_up_cube(data_1, sizes)
+        patches_2 = cut_up_cube(data_2, sizes)
+
+        patches_shape = patches_1.shape
+        nb_patches = np.prod(patches_shape[0:3])
+
+        patches_1 = patches_1.reshape((nb_patches, np.prod(sizes)))
+        patches_2 = patches_2.reshape((nb_patches, np.prod(sizes)))
+        patches = np.concatenate((patches_1, patches_2), axis=-1)
+
+        non_zeros_patches = np.sum(patches, axis=-1)
+        non_zeros_ids = np.where(np.abs(non_zeros_patches) > 0.001)[0]
+
+        def correlate(data):
+            data += np.random.rand(data.shape[0]) * 0.001
+            a, b = np.split(data, 2)
+
+            if np.allclose(a, b):
+                return 1
+
+            corr = np.corrcoef(a, b, dtype=np.float32)[0, 1]
+            return corr
+
+        results = np.zeros((len(patches)), dtype=np.float32)
+        results[non_zeros_ids] = np.apply_along_axis(correlate, 1,
+                                                     patches[non_zeros_ids, :])
+
+        all_corr[..., i] = results.reshape(patches_shape[0:3])
+
+    return np.mean(all_corr, axis=-1)
+
+
 def dilation(input_list, ref_img):
     """
     dilation: IMG, VALUE
         Binary morphological operation to spatially extend the values of an
         image to their neighbors. VALUE is in voxels.
     """
     _validate_length(input_list, 2)
@@ -632,9 +760,11 @@
     blur: IMG, VALUE
         Apply a gaussian blur to a single image.
     """
     _validate_length(input_list, 2)
     _validate_type(input_list[0], nib.Nifti1Image)
     _validate_float(input_list[1])
 
+    # Data is always 3D, using directly scipy. See also
+    # scilpy.image.volume_operations : smooth_to_fwhm.
     return gaussian_filter(input_list[0].get_fdata(dtype=np.float64),
                            sigma=input_list[1])
```

### Comparing `scilpy-1.5.post2/scilpy/image/reslice.py` & `scilpy-2.0.0/scilpy/image/reslice.py`

 * *Files identical despite different names*

### Comparing `scilpy-1.5.post2/scilpy/image/tests/test_labels.py` & `scilpy-2.0.0/scilpy/image/tests/test_labels.py`

 * *Files 1% similar despite different names*

```diff
@@ -78,16 +78,18 @@
 
 
 def test_dilate_labels_with_mask():
     in_labels = deepcopy(ref_in_labels)
     in_mask = deepcopy(ref_in_labels)
     in_mask[in_mask > 0] = 1
     out_labels = dilate_labels(in_labels, 1, 2, 1,
-                               labels_to_dilate=[1, 6], labels_not_to_dilate=[3, 4],
-                               labels_to_fill=[0, 2, 5], mask=in_mask)
+                               labels_to_dilate=[1, 6],
+                               labels_not_to_dilate=[3, 4],
+                               labels_to_fill=[0, 2, 5],
+                               mask=in_mask)
 
     exp_labels = deepcopy(ref_in_labels)
     exp_labels[exp_labels == 2] = 1
     exp_labels[exp_labels == 5] = 6
 
     assert_equal(out_labels, exp_labels)
 
@@ -101,15 +103,15 @@
 
     for i, val in enumerate([544, 156, 36, 36, 36, 36, 156]):
         assert len(out_labels[out_labels == i]) == val
 
 
 def test_get_data_as_labels_int():
     data = np.zeros((2, 2, 2), dtype=np.int64)
-    img = nib.Nifti1Image(data, np.eye(4))
+    img = nib.Nifti1Image(data, np.eye(4), dtype=np.int64)
     img.set_filename('test.nii.gz')
 
     _ = get_data_as_labels(img)
 
     img.set_data_dtype(np.uint8)
     _ = get_data_as_labels(img)
 
@@ -151,7 +153,12 @@
 def test_split_labels():
     in_labels = deepcopy(ref_in_labels)
     out_labels = split_labels(in_labels, [6, 7, 7])
 
     assert len(out_labels) == 3
     assert_equal(np.unique(out_labels[0]), [0, 6])
     assert_equal(np.unique(out_labels[1]), [0])
+
+
+def test_stats_in_labels():
+    # toDO. Will need to create a fake LUT.
+    pass
```

### Comparing `scilpy-1.5.post2/scilpy/io/streamlines.py` & `scilpy-2.0.0/scilpy/io/streamlines.py`

 * *Files 25% similar despite different names*

```diff
@@ -2,18 +2,22 @@
 
 from itertools import islice
 import logging
 import os
 import tempfile
 
 from dipy.io.streamline import load_tractogram
+from dipy.io.streamline import save_tractogram as _save_tractogram
+from dipy.io.utils import is_header_compatible
 import nibabel as nib
 from nibabel.streamlines.array_sequence import ArraySequence
 import numpy as np
 
+from scilpy.io.utils import load_matrix_in_any_format
+
 
 def check_tracts_same_format(parser, tractogram_1, tractogram_2):
     """
     Assert that two filepaths have the same valid extension.
     :param parser: argparse.ArgumentParser object
     :param tractogram_1: Tractogram filename #1
     :param tractogram_2: Tractogram filename #2
@@ -39,15 +43,15 @@
         streamlines
     n: int
         amount of streamlines to load
 
     Return
     ------
 
-    chunck: list
+    chunk: list
         subset of streamlines
     """
 
     sequence = iter(sequence)
     chunk = list(islice(sequence, n))
     while len(chunk) > 0:
         yield chunk
@@ -62,15 +66,15 @@
 def load_tractogram_with_reference(parser, args, filepath, arg_name=None):
     """
     Parameters
     ----------
     parser: Argument Parser
         Used to print errors, if any.
     args: Namespace
-        Parsed arguments. Used to get the 'ref' and 'bbox_check' args.
+        Parsed arguments. Used to get the 'reference' and 'bbox_check' args.
         See scilpy.io.utils to add the arguments to your parser.
     filepath: str
         Path of the tractogram file.
     arg_name: str, optional
         Name of the reference argument. By default the args.ref is used. If
         arg_name is given, then args.arg_name_ref will be used instead.
     """
@@ -83,15 +87,15 @@
     if ext == '.trk':
         if (is_argument_set(args, 'reference') or
                 arg_name and args.__getattribute__(arg_name + '_ref')):
             logging.warning('Reference is discarded for this file format '
                             '{}.'.format(filepath))
         sft = load_tractogram(filepath, 'same',
                               bbox_valid_check=bbox_check)
-        
+
         # Force dtype to int64 instead of float64
         if len(sft.streamlines) == 0:
             sft.streamlines._offsets.dtype = np.dtype(np.int64)
 
     elif ext in ['.tck', '.fib', '.vtk', '.dpy']:
         if arg_name:
             arg_ref = arg_name + '_ref'
@@ -111,30 +115,181 @@
 
     else:
         parser.error('{} is an unsupported file format'.format(filepath))
 
     return sft
 
 
-def streamlines_to_memmap(input_streamlines):
+def save_tractogram(sft, filename, no_empty, bbox_valid_check=True):
+    if len(sft.streamlines) == 0 and no_empty:
+        logging.info("The file {} won't be written (0 streamlines)"
+                     .format(filename))
+    else:
+        if len(sft.streamlines) == 0:
+            logging.info("Writing an empty file (0 streamlines): {} "
+                         .format(filename))
+        _save_tractogram(sft, filename, bbox_valid_check=bbox_valid_check)
+
+
+def verify_compatibility_with_reference_sft(ref_sft, files_to_verify,
+                                            parser, args):
+    """
+    Verifies the compatibility of a reference sft with a list of files.
+
+    Params
+    ------
+    ref_sft: StatefulTractogram
+        A tractogram to be used as reference.
+    files_to_verify: List[str]
+        List of files that should be compatible with the reference sft. Files
+        can be either other tractograms or nifti files (ex: masks).
+    parser: argument parser
+        Will raise an error if a file is not compatible.
+    args: Namespace
+        Should contain a args.reference if any file is a .tck, and possibly a
+        args.bbox_check (set to True by default).
+    """
+    save_ref = args.reference
+
+    for file in files_to_verify:
+        if file is not None:
+            _, ext = os.path.splitext(file)
+            if ext in ['.trk', '.tck', '.fib', '.vtk', '.dpy']:
+                # Cheating ref because it may send a lot of warning if loading
+                # many trk with ref (reference was maybe added only for some
+                # of these files)
+                if ext == '.trk':
+                    args.reference = None
+                else:
+                    args.reference = save_ref
+                mask = load_tractogram_with_reference(parser, args, file)
+            else:  # should be a nifti file.
+                mask = file
+            compatible = is_header_compatible(ref_sft, mask)
+            if not compatible:
+                parser.error("Reference tractogram incompatible with {}"
+                             .format(file))
+
+
+def load_dps_files_as_dps(parser, dps_files, sft, keys=None, overwrite=False):
+    """
+    Load dps information. They must be scalar values.
+
+    Parameters
+    ----------
+    parser: parser
+    dps_files: list[str]
+        Either .npy or .txt files.
+    sft: StatefulTractogram
+    keys: list[str]
+        If None, use the filenames as keys.
+    overwrite: bool
+        If True, allow overwriting an existing dps key.
+
+    Returns
+    -------
+    sft: StatefulTractogram
+        The modified SFT. (Note that it is modified in-place even if the
+        returned variable is not used!)
+    new_keys: list[str]
+        Added keys.
+    """
+    if keys is not None and len(keys) != len(dps_files):
+        parser.error("You must provide one key name per dps file.")
+
+    new_keys = []
+    for i, file in enumerate(dps_files):
+        if keys is None:
+            name = os.path.basename(file)
+            key, ext = os.path.splitext(name)
+        else:
+            key = keys[i]
+
+        if key in sft.data_per_streamline and not overwrite:
+            parser.error("Key {} already exists in your tractogram's dps. "
+                         "You must allow overwriting keys."
+                         .format(key))
+
+        data = np.squeeze(load_matrix_in_any_format(file))
+        if len(data) != len(sft):
+            parser.error('Wrong dps size in file {}. Expected one value per '
+                         'streamline ({}) but got {} values!'
+                         .format(file, len(sft), len(data)))
+
+        new_keys.append(key)
+        sft.data_per_streamline[key] = data
+    return sft, new_keys
+
+
+def load_dpp_files_as_dpp(parser, dpp_files, sft, keys=None, overwrite=False):
+    """
+    Load dpp information. They must be scalar values.
+
+    Parameters
+    ----------
+    parser: parser
+    dpp_files: list[str]
+        Either .npy or .txt files.
+    sft: StatefulTractogram
+    keys: list[str]
+        If None, use the filenames as keys.
+    overwrite: bool
+        If True, allow overwriting an existing dpp key.
+
+    Returns
+    -------
+    sft: StatefulTractogram
+        The modified SFT. (Note that it is modified in-place even if the
+        returned variable is not used!)
+    new_keys: list[str]
+        Added keys.
+    """
+    if keys is not None and len(keys) != len(dpp_files):
+        parser.error("You must provide one key name per dps file.")
+
+    new_keys = []
+    for i, file in enumerate(dpp_files):
+        if keys is None:
+            name = os.path.basename(file)
+            key, ext = os.path.splitext(name)
+        else:
+            key = keys[i]
+
+        if key in sft.data_per_streamline and not overwrite:
+            parser.error("Key {} already exists in your tractogram's dpp. "
+                         "You must allow overwriting keys."
+                         .format(key))
+
+        data = np.squeeze(load_matrix_in_any_format(file))
+        if len(data) != len(sft.streamlines._data):
+            parser.error('Wrong dpp size in file {}. Expected one value per '
+                         'point in your tractogram ({}) but got {}!'
+                         .format(file, len(sft.streamlines._data), len(data)))
+        new_keys.append(key)
+        sft.data_per_point[key] = data
+    return sft, new_keys
+
+
+def streamlines_to_memmap(input_streamlines,
+                          strs_dtype='float32'):
     """
     Function to decompose on disk the array_sequence into its components.
     Parameters
     ----------
     input_streamlines : ArraySequence
         All streamlines of the tractogram to segment.
     Returns
     -------
     tmp_obj : tuple
         Temporary directory and tuple of filenames for the data, offsets
         and lengths.
     """
     tmp_dir = tempfile.TemporaryDirectory()
     data_filename = os.path.join(tmp_dir.name, 'data.dat')
-    data = np.memmap(data_filename, dtype='float32', mode='w+',
+    data = np.memmap(data_filename, dtype=strs_dtype, mode='w+',
                      shape=input_streamlines._data.shape)
     data[:] = input_streamlines._data[:]
 
     offsets_filename = os.path.join(tmp_dir.name, 'offsets.dat')
     offsets = np.memmap(offsets_filename, dtype='int64', mode='w+',
                         shape=input_streamlines._offsets.shape)
     offsets[:] = input_streamlines._offsets[:]
@@ -143,77 +298,46 @@
     lengths = np.memmap(lengths_filename, dtype='int32', mode='w+',
                         shape=input_streamlines._lengths.shape)
     lengths[:] = input_streamlines._lengths[:]
 
     return tmp_dir, (data_filename, offsets_filename, lengths_filename)
 
 
-def reconstruct_streamlines_from_memmap(memmap_filenames, indices=None):
+def reconstruct_streamlines_from_memmap(memmap_filenames, indices=None,
+                                        strs_dtype='float32'):
     """
     Function to reconstruct streamlines from memmaps, mainly to facilitate
     multiprocessing and decrease RAM usage.
 
+    Parameters
     ----------
     memmap_filenames : tuple
         Tuple of 3 filepath to numpy memmap (data, offsets, lengths).
     indices : list
         List of int representing the indices to reconstruct.
 
     Returns
     -------
     streamlines : list of np.ndarray
         List of streamlines.
     """
 
-    data = np.memmap(memmap_filenames[0],  dtype='float32', mode='r')
+    data = np.memmap(memmap_filenames[0],  dtype=strs_dtype, mode='r')
     offsets = np.memmap(memmap_filenames[1],  dtype='int64', mode='r')
     lengths = np.memmap(memmap_filenames[2],  dtype='int32', mode='r')
 
     return reconstruct_streamlines(data, offsets, lengths, indices=indices)
 
 
-def reconstruct_streamlines_from_hdf5(hdf5_filename, key=None):
-    """
-    Function to reconstruct streamlines from hdf5, mainly to facilitate
-    decomposition into thousand of connections and decrease I/O usage.
-    ----------
-    hdf5_filename : str
-        Filepath to the hdf5 file.
-    key : str
-        Key of the connection of interest (LABEL1_LABEL2).
-
-    Returns
-    -------
-    streamlines : list of np.ndarray
-        List of streamlines.
-    """
-
-    hdf5_file = hdf5_filename
-
-    if key is not None:
-        if key not in hdf5_file:
-            return []
-        group = hdf5_file[key]
-        if 'data' not in group:
-            return []
-    else:
-        group = hdf5_file
-
-    data = np.array(group['data']).flatten()
-    offsets = np.array(group['offsets'])
-    lengths = np.array(group['lengths'])
-
-    return reconstruct_streamlines(data, offsets, lengths)
-
-
 def reconstruct_streamlines(data, offsets, lengths, indices=None):
     """
     Function to reconstruct streamlines from its data, offsets and lengths
     (from the nibabel tractogram object).
 
+    Parameters
     ----------
     data : np.ndarray
         Nx3 array representing all points of the streamlines.
     offsets : np.ndarray
         Nx1 array representing the cumsum of length array.
     lengths : np.ndarray
         Nx1 array representing the length of each streamline.
```

### Comparing `scilpy-1.5.post2/scilpy/io/utils.py` & `scilpy-2.0.0/scripts/scil_tractogram_segment_and_score.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,677 +1,457 @@
+#!/usr/bin/env python
 # -*- coding: utf-8 -*-
 
+
+"""
+Scores input tractogram overall and bundlewise.
+
+Outputs
+-------
+
+    - results.json: Contains a full tractometry report.
+    - processing_stats.json: Contains information on the segmentation of
+    bundles (ex: the number of wpc per criteria).
+    - Splits the input tractogram into
+        segmented_VB/*_VS.trk.
+        segmented_IB/*_*_IC.trk   (if args.compute_ic)
+        segmented_WPC/*_wpc.trk  (if args.save_wpc_separately)
+        IS.trk     OR      NC.trk  (if args.compute_ic)
+
+By default, if a streamline fits in many bundles, it will be included in every
+one. This means a streamline may be a VS for a bundle and an IS for
+(potentially many) others. If you want to assign each streamline to at most one
+bundle, use the `--unique` flag.
+
+Config file
+-----------
+
+The config file needs to be a json containing a dict of the ground-truth
+bundles as keys. The value for each bundle is itself a dictionnary with:
+
+Mandatory:
+    - endpoints OR [head AND tail]: filename for the endpoints ROI.
+        If 'enpoints' is used, we will automatically separate the mask into two
+        ROIs, acting as head and tail. Quality check is strongly recommended.
+
+Optional:
+    Concerning metrics:
+    - gt_mask: expected result. OL and OR metrics will be computed from this.*
+
+    Concerning inclusion criteria (other streamlines will be WPC):
+    - all_mask: ROI serving as "all" criteria: to be included in the bundle,
+        ALL points of a streamline must be inside the mask.*
+    - any_mask: ROI serving as "any" criteria: streamlines
+        must touch that mask in at least one point ("any" point) to be included
+        in the bundle.
+    - angle: angle criteria. Streamlines containing loops and sharp turns above
+        given angle will be rejected from the bundle.
+    - length: maximum and minimum lengths per bundle.
+    - length_x / length_x_abs: maximum and minimum total distance in the x
+        direction (i.e. first coordinate).**
+    - length_y / length_y_abs: maximum and minimum total distance in the y
+        direction (i.e. second coordinate).**
+    - length_z / length_z_abs: maximum and minimum total distance in the z
+        direction (i.e. third coordinate).**
+
+* Files must be .tck, .trk, .nii or .nii.gz. If it is a tractogram, a mask will
+be created. If it is a nifti file, it will be considered to be a mask.
+** With absolute values: coming back on yourself will contribute to the total
+distance instead of cancelling it.
+
+Exemple config file:
+{
+  "Ground_truth_bundle_0": {
+    "gt_mask": "PATH/bundle0.nii.gz",
+    "angle": 300,
+    "length": [140, 150],
+    "endpoints": "PATH/file1.nii.gz"
+  }
+}
+"""
 import argparse
+import json
 import itertools
 import logging
+import numpy as np
 import os
-import multiprocessing
-import re
-import shutil
-import xml.etree.ElementTree as ET
 
-import numpy as np
-from dipy.data import SPHERE_FILES
+from dipy.io.streamline import save_tractogram
 from dipy.io.utils import is_header_compatible
-from fury import window
-from PIL import Image
-from scipy.io import loadmat
-import six
-
-from scilpy.io.streamlines import load_tractogram_with_reference
-from scilpy.utils.bvec_bval_tools import DEFAULT_B0_THRESHOLD
-from scilpy.utils.filenames import split_name_with_nii
-
-eddy_options = ["mb", "mb_offs", "slspec", "mporder", "s2v_lambda", "field",
-                "field_mat", "flm", "slm", "fwhm", "niter", "s2v_niter",
-                "cnr_maps", "residuals", "fep", "interp", "s2v_interp",
-                "resamp", "nvoxhp", "ff", "ol_nstd", "ol_nvox", "ol_type",
-                "ol_pos", "ol_sqr", "dont_sep_offs_move", "dont_peas"]
-
-topup_options = ['out', 'fout', 'iout', 'logout', 'warpres', 'subsamp', 'fwhm',
-                 'config', 'miter', 'lambda', 'ssqlambda', 'regmod', 'estmov',
-                 "minmet", 'splineorder', 'numprec', 'interp', 'scale',
-                 'regrid']
-
-axis_name_choices = ["axial", "coronal", "sagittal"]
-
-
-def link_bundles_and_reference(parser, args, input_tractogram_list):
-    """
-    Associate the bundle to their reference (if they require a reference).
-    Parameters
-    ----------
-    parser: argparse.ArgumentParser object
-        Parser as created by argparse.
-    args: argparse namespace
-        Args as created by argparse.
-    input_tractogram_list: list
-        List of tractogram paths.
-    Returns
-    -------
-    list: List of tuples, each matching one tractogram to a reference file.
-    """
-    bundles_references_tuple = []
-    for bundle_filename in input_tractogram_list:
-        _, ext = os.path.splitext(bundle_filename)
-        if ext == '.trk':
-            if args.reference is None:
-                bundles_references_tuple.append(
-                    (bundle_filename, bundle_filename))
-            else:
-                bundles_references_tuple.append(
-                    (bundle_filename, args.reference))
-        elif ext in ['.tck', '.fib', '.vtk', '.dpy']:
-            if args.reference is None:
-                parser.error('--reference is required for this file format '
-                             '{}.'.format(bundle_filename))
-            else:
-                bundles_references_tuple.append(
-                    (bundle_filename, args.reference))
-    return bundles_references_tuple
-
-
-def check_tracts_same_format(parser, filename_list):
-    _, ref_ext = os.path.splitext(filename_list[0])
-
-    for filename in filename_list[1:]:
-        if isinstance(filename, six.string_types) and \
-                not os.path.splitext(filename)[1] == ref_ext:
-            parser.error('All tracts file must use the same format.')
-
-
-def assert_gradients_filenames_valid(parser, filename_list, gradient_format):
-    """
-    Validate if gradients filenames follow BIDS or MRtrix convention
 
-    Parameters
-    ----------
-    parser: parser
-        Parser.
-    filename_list: list
-        list of gradient paths.
-    gradient_format : str
-        Can be either fsl or mrtrix.
+from scilpy.io.streamlines import (load_tractogram_with_reference,
+                                   verify_compatibility_with_reference_sft)
+from scilpy.io.utils import (add_bbox_arg,
+                             add_overwrite_arg,
+                             add_json_args,
+                             add_reference_arg,
+                             add_verbose_arg,
+                             assert_inputs_exist,
+                             assert_output_dirs_exist_and_empty,
+                             assert_outputs_exist)
+from scilpy.segment.tractogram_from_roi import (compute_masks_from_bundles,
+                                                compute_endpoint_masks,
+                                                segment_tractogram_from_roi)
+from scilpy.tractanalysis.scoring import compute_tractometry
+from scilpy.tractanalysis.scoring import __doc__ as tractometry_description
+
+def_len = [0, np.inf]
+
+
+def _build_arg_parser():
+    p = argparse.ArgumentParser(
+        description=__doc__ + tractometry_description,
+        formatter_class=argparse.RawTextHelpFormatter)
+
+    p.add_argument("in_tractogram",
+                   help="Input tractogram to score")
+    p.add_argument("gt_config",
+                   help=".json dict configured as specified above.")
+    p.add_argument("out_dir",
+                   help="Output directory for the resulting segmented "
+                        "bundles.")
+    p.add_argument("--json_prefix", metavar='p', default='',
+                   help="Prefix of the two output json files. Ex: 'study_x_'."
+                        "Files will be saved inside out_dir.\n"
+                        "Suffixes will be 'processing_stats.json' and "
+                        "'results.json'.")
+
+    g = p.add_argument_group("Additions to gt_config")
+    g.add_argument("--gt_dir", metavar='DIR',
+                   help="Root path of the ground truth files listed in the "
+                        "gt_config. \nIf not set, filenames in the config "
+                        "file are considered \nas absolute paths.")
+    g.add_argument("--use_gt_masks_as_all_masks", action='store_true',
+                   help="If set, the gt_config's 'gt_mask' will also be used "
+                        "as\n'all_mask' for each bundle. Note that this "
+                        "means the\nOR will necessarily be 0.")
+
+    g = p.add_argument_group("Preprocessing")
+    g.add_argument("--dilate_endpoints",
+                   metavar="NB_PASS", default=0, type=int,
+                   help="Dilate endpoint masks n-times. Default: 0.")
+    g.add_argument("--remove_invalid", action="store_true",
+                   help="Remove invalid streamlines before scoring.")
+
+    g = p.add_argument_group("Tractometry choices")
+    g.add_argument("--save_wpc_separately", action='store_true',
+                   help="If set, streamlines rejected from VC based on the "
+                        "config\nfile criteria will be saved separately from "
+                        "IS (and IC)\nin one file *_wpc.tck per bundle.")
+    g.add_argument("--compute_ic", action='store_true',
+                   help="If set, IS are split into NC + IC, where IC are "
+                        "computed as one bundle per\npair of ROI not "
+                        "belonging to a true connection, named\n*_*_IC.tck.")
+    g.add_argument("--unique", action='store_true',
+                   help="If set, streamlines are assigned to the first bundle"
+                        " they fit in and not to all.")
+    g.add_argument("--remove_wpc_belonging_to_another_bundle",
+                   action='store_true',
+                   help="If set, WPC actually belonging to any VB (in the \n"
+                        "case of overlapping ROIs) will be removed\n"
+                        "from the WPC classification.")
+
+    p.add_argument("--no_empty", action='store_true',
+                   help='Do not write file if there is no streamline.')
+
+    add_json_args(p)
+    add_bbox_arg(p)
+    add_reference_arg(p)
+    add_verbose_arg(p)
+    add_overwrite_arg(p)
+
+    return p
+
+
+def load_and_verify_everything(parser, args):
+    """
+    - Reads the config file
+    - Loads the masks / sft
+        - If endpoints were given instead of head + tail, separate into two
+          sub-rois.
+    - Verifies compatibility
+    """
+    args.json_prefix = os.path.join(args.out_dir, args.json_prefix)
+    json_outputs = [args.json_prefix + 'processing_stats.json',
+                    args.json_prefix + 'results.json']
+    assert_inputs_exist(parser, args.gt_config, args.reference)
+    assert_output_dirs_exist_and_empty(parser, args, args.out_dir,
+                                       create_dir=True)
+    assert_outputs_exist(parser, args, json_outputs)
+    os.makedirs(os.path.join(args.out_dir, 'segmented_VB'))
+    if args.compute_ic:
+        os.makedirs(os.path.join(args.out_dir, 'segmented_IB'))
+    if args.save_wpc_separately:
+        os.makedirs(os.path.join(args.out_dir, 'segmented_WPC'))
+
+    # Read the config file
+    (bundle_names, gt_masks_files, all_masks_files, any_masks_files,
+     roi_options, lengths, angles, orientation_lengths,
+     abs_orientation_lengths) = read_config_file(args)
+
+    # Find every mandatory mask to be loaded
+    list_masks_files_r = list(itertools.chain(
+        *[list(roi_option.values()) for roi_option in roi_options]))
+    list_masks_files_o = gt_masks_files + all_masks_files + any_masks_files
+    # (This removes duplicates:)
+    list_masks_files_r = list(dict.fromkeys(list_masks_files_r))
+    list_masks_files_o = list(dict.fromkeys(list_masks_files_o))
+
+    # Verify options
+    assert_inputs_exist(parser, list_masks_files_r + [args.in_tractogram],
+                        list_masks_files_o)
+
+    logging.info("Loading tractogram.")
+    sft = load_tractogram_with_reference(parser, args, args.in_tractogram)
+    _, dimensions, _, _ = sft.space_attributes
+
+    if args.remove_invalid:
+        sft.remove_invalid_streamlines()
+
+    logging.info("Verifying compatibility of tractogram with gt_masks, "
+                 "all_masks, any_masks, ROI masks")
+    verify_compatibility_with_reference_sft(
+        sft, list_masks_files_r + list_masks_files_o, parser, args)
+
+    logging.info("Loading and/or computing ground-truth masks, limits "
+                 "masks and any_masks.")
+    gt_masks = compute_masks_from_bundles(gt_masks_files, parser, args)
+    inv_all_masks = compute_masks_from_bundles(all_masks_files, parser, args,
+                                               inverse_mask=True)
+    any_masks = compute_masks_from_bundles(any_masks_files, parser, args)
+
+    logging.info("Extracting ground-truth head and tail masks.")
+    gt_tails, gt_heads = compute_endpoint_masks(roi_options, args.out_dir)
+
+    # Update the list of every ROI, remove duplicates
+    list_rois = gt_tails + gt_heads
+    list_rois = list(dict.fromkeys(list_rois))  # Removes duplicates
+
+    logging.info("Verifying tractogram compatibility with endpoint ROIs.")
+    for file in list_rois:
+        compatible = is_header_compatible(sft, file)
+        if not compatible:
+            parser.error("Input tractogram incompatible with {}".format(file))
+
+    return (gt_tails, gt_heads, sft, bundle_names, list_rois,
+            lengths, angles, orientation_lengths, abs_orientation_lengths,
+            inv_all_masks, gt_masks, any_masks, dimensions, json_outputs)
 
-    Returns
-    -------
-    """
-
-    valid_fsl_extensions = ['.bval', '.bvec']
-    valid_mrtrix_extension = '.b'
-
-    if isinstance(filename_list, str):
-        filename_list = [filename_list]
 
-    if gradient_format == 'fsl':
-        if len(filename_list) == 2:
-            filename_1 = filename_list[0]
-            filename_2 = filename_list[1]
-            basename_1, ext_1 = os.path.splitext(filename_1)
-            basename_2, ext_2 = os.path.splitext(filename_2)
-
-            if ext_1 == '' or ext_2 == '':
-                parser.error('fsl gradients filenames must have extensions: '
-                             '.bval and .bvec.')
-
-            if basename_1 == basename_2:
-                curr_extensions = [ext_1, ext_2]
-                curr_extensions.sort()
-                if curr_extensions != valid_fsl_extensions:
-                    parser.error('Your extensions ({}) doesn\'t follow BIDS '
-                                 'convention.'.format(curr_extensions))
-            else:
-                parser.error('fsl gradients filenames must have the same '
-                             'basename.')
-        else:
-            parser.error('You should have two files for fsl format.')
-
-    elif gradient_format == 'mrtrix':
-        if len(filename_list) == 1:
-            curr_filename = filename_list[0]
-            basename, ext = os.path.splitext(curr_filename)
-            if basename == '' or ext != valid_mrtrix_extension:
-                parser.error('Basename: {} and extension {} are not '
-                             'valid for mrtrix format.'.format(basename, ext))
-        else:
-            parser.error('You should have one file for mrtrix format.')
-    else:
-        parser.error('Gradient file format should be either fsl or mrtrix.')
-
-
-def add_json_args(parser):
-    g1 = parser.add_argument_group(title='Json options')
-    g1.add_argument('--indent',
-                    type=int, default=2,
-                    help='Indent for json pretty print.')
-    g1.add_argument('--sort_keys',
-                    action='store_true',
-                    help='Sort keys in output json.')
-
-
-def add_processes_arg(parser):
-    parser.add_argument('--processes', dest='nbr_processes',
-                        metavar='NBR', type=int, default=1,
-                        help='Number of sub-processes to start. \n'
-                        'Default: [%(default)s]')
-
-
-def add_reference_arg(parser, arg_name=None):
-    if arg_name:
-        parser.add_argument('--'+arg_name+'_ref',
-                            help='Reference anatomy for {} (if tck/vtk/fib/dpy'
-                                 ') file\n'
-                                 'support (.nii or .nii.gz).'.format(arg_name))
-    else:
-        parser.add_argument('--reference',
-                            help='Reference anatomy for tck/vtk/fib/dpy file\n'
-                                 'support (.nii or .nii.gz).')
-
-
-def add_sphere_arg(parser, symmetric_only=False, default='symmetric724'):
-    spheres = sorted(SPHERE_FILES.keys())
-    if symmetric_only:
-        spheres = [s for s in spheres if 'symmetric' in s]
-        if 'symmetric' not in default:
-            raise ValueError("Default cannot be {} if you only accept "
-                             "symmetric spheres.".format(default))
-
-    parser.add_argument('--sphere', choices=spheres,
-                        default=default,
-                        help='Dipy sphere; set of possible directions.\n'
-                             'Default: [%(default)s]')
-
-
-def add_overwrite_arg(parser):
-    parser.add_argument(
-        '-f', dest='overwrite', action='store_true',
-        help='Force overwriting of the output files.')
-
-
-def add_force_b0_arg(parser):
-    parser.add_argument('--force_b0_threshold', action='store_true',
-                        help='If set, the script will continue even if the '
-                             'minimum bvalue is suspiciously high ( > {})'
-                        .format(DEFAULT_B0_THRESHOLD))
-
-
-def add_verbose_arg(parser):
-    parser.add_argument('-v', action='store_true', dest='verbose',
-                        help='If set, produces verbose output.')
-
-
-def add_bbox_arg(parser):
-    parser.add_argument('--no_bbox_check', dest='bbox_check',
-                        action='store_false',
-                        help='Activate to ignore validity of the bounding '
-                             'box during loading / saving of \n'
-                             'tractograms (ignores the presence of invalid '
-                             'streamlines).')
-
-
-def add_sh_basis_args(parser, mandatory=False):
-    """Add spherical harmonics (SH) bases argument.
-
-    Parameters
-    ----------
-    parser: argparse.ArgumentParser object
-        Parser.
-    mandatory: bool
-        Whether this argument is mandatory.
+def read_config_file(args):
     """
-    choices = ['descoteaux07', 'tournier07']
-    def_val = 'descoteaux07'
-    help_msg = 'Spherical harmonics basis used for the SH coefficients.\nMust ' +\
-               'be either \'descoteaux07\' or \'tournier07\' [%(default)s]:\n' +\
-               '    \'descoteaux07\': SH basis from the Descoteaux et al.\n' +\
-               '                      MRM 2007 paper\n' +\
-               '    \'tournier07\'  : SH basis from the Tournier et al.\n' +\
-               '                      NeuroImage 2007 paper.'
-
-    if mandatory:
-        arg_name = 'sh_basis'
-    else:
-        arg_name = '--sh_basis'
-
-    parser.add_argument(arg_name,
-                        choices=choices, default=def_val,
-                        help=help_msg)
-
-
-def validate_nbr_processes(parser, args):
-    """ Check if the passed number of processes arg is valid.
-    Valid values are considered to be in the [0, CPU count] range:
-        - Raises a parser.error if an invalid value is provided.
-        - Returns the maximum number of cores retrieved if no value (or a value
-        of 0) is provided.
-
-    Parameters
-    ----------
-    parser: argparse.ArgumentParser object
-        Parser as created by argparse.
-    args: argparse namespace
-        Args as created by argparse.
+    Reads the gt_config file and returns:
 
     Returns
     -------
-    nbr_cpu: int
-        The number of CPU to be used.
-    """
-
-    if args.nbr_processes:
-        nbr_cpu = args.nbr_processes
-    else:
-        nbr_cpu = multiprocessing.cpu_count()
-
-    if nbr_cpu < 0:
-        parser.error('Number of processes must be > 0.')
-    elif nbr_cpu > multiprocessing.cpu_count():
-        parser.error('Max number of processes is {}. Got {}.'.format(
-            multiprocessing.cpu_count(), nbr_cpu))
-
-    return nbr_cpu
-
-
-def validate_sh_basis_choice(sh_basis):
-    """ Check if the passed sh_basis arg to a fct is right.
-
-    Parameters
-    ----------
-    sh_basis: str
-        Either 'descoteaux08' or 'tournier07'
-
-    Raises
-    ------
-    ValueError
-        If sh_basis is not one of 'descoteaux07' or 'tournier07'
-    """
-    if not (sh_basis == 'descoteaux07' or sh_basis == 'tournier07'):
-        raise ValueError("sh_basis should be either 'descoteaux07' or"
-                         "'tournier07'.")
-
-
-def verify_compression_th(compress_th):
-    """
-    Verify that the compression threshold is between 0.001 and 1. Else,
-    produce a warning.
-
-    Parameters
-    -----------
-    compress_th: float, the compression threshold.
-    """
-    if compress_th:
-        if compress_th < 0.001 or compress_th > 1:
-            logging.warning(
-                'You are using an error rate of {}.\nWe recommend setting it '
-                'between 0.001 and 1.\n0.001 will do almost nothing to the '
-                'tracts while 1 will higly compress/linearize the tracts.'
-                .format(compress_th))
-
-
-def assert_inputs_exist(parser, required, optional=None):
-    """Assert that all inputs exist. If not, print parser's usage and exit.
-
-    Parameters
-    ----------
-    parser: argparse.ArgumentParser object
-        Parser.
-    required: string or list of paths
-        Required paths to be checked.
-    optional: string or list of paths
-        Optional paths to be checked.
-    """
-    def check(path):
-        if not os.path.isfile(path):
-            parser.error('Input file {} does not exist'.format(path))
-
-    if isinstance(required, str):
-        required = [required]
-
-    if isinstance(optional, str):
-        optional = [optional]
-
-    for required_file in required:
-        check(required_file)
-    for optional_file in optional or []:
-        if optional_file is not None:
-            check(optional_file)
-
-
-def assert_outputs_exist(parser, args, required, optional=None,
-                         check_dir_exists=True):
-    """
-    Assert that all outputs don't exist or that if they exist, -f was used.
-    If not, print parser's usage and exit.
-
-    Parameters
-    ----------
-    parser: argparse.ArgumentParser object
-        Parser.
-    args: argparse namespace
-        Argument list.
-    required: string or list of paths to files
-        Required paths to be checked.
-    optional: string or list of paths to files
-        Optional paths to be checked.
-    check_dir_exists: bool
-        Test if output directory exists.
-    """
-    def check(path):
-        if os.path.isfile(path) and not args.overwrite:
-            parser.error('Output file {} exists. Use -f to force '
-                         'overwriting'.format(path))
-
-        if check_dir_exists:
-            path_dir = os.path.dirname(path)
-            if path_dir and not os.path.isdir(path_dir):
-                parser.error('Directory {}/ \n for a given output file '
-                             'does not exists.'.format(path_dir))
-
-    if isinstance(required, str):
-        required = [required]
-
-    if isinstance(optional, str):
-        optional = [optional]
-
-    for required_file in required:
-        check(required_file)
-    for optional_file in optional or []:
-        if optional_file:
-            check(optional_file)
-
-
-def assert_output_dirs_exist_and_empty(parser, args, required,
-                                       optional=None, create_dir=True):
-    """
-    Assert that all output directories exist.
-    If not, print parser's usage and exit.
-    If exists and not empty, and -f used, delete dirs.
-
-    Parameters
-    ----------
-    parser: argparse.ArgumentParser object
-        Parser.
-    args: argparse namespace
-        Argument list.
-    required: string or list of paths to files
-        Required paths to be checked.
-    optional: string or list of paths to files
-        Optional paths to be checked.
-    create_dir: bool
-        If true, create the directory if it does not exist.
-    """
-    def check(path):
-        if not os.path.isdir(path):
-            if not create_dir:
-                parser.error(
-                    'Output directory {} doesn\'t exist.'.format(path))
-            else:
-                os.makedirs(path, exist_ok=True)
-        if os.listdir(path):
-            if not args.overwrite:
-                parser.error(
-                    'Output directory {} isn\'t empty and some files could be '
-                    'overwritten or even deleted. Use -f option if you want '
-                    'to continue.'.format(path))
-            else:
-                for the_file in os.listdir(path):
-                    file_path = os.path.join(path, the_file)
-                    try:
-                        if os.path.isfile(file_path):
-                            os.unlink(file_path)
-                        elif os.path.isdir(file_path):
-                            shutil.rmtree(file_path)
-                    except Exception as e:
-                        print(e)
-
-    if isinstance(required, str):
-        required = [required]
-    if isinstance(optional, str):
-        optional = [optional]
-
-    for cur_dir in required:
-        check(cur_dir)
-    for opt_dir in optional or []:
-        if opt_dir:
-            check(opt_dir)
-
-
-def verify_compatibility_with_reference_sft(ref_sft, files_to_verify,
-                                            parser, args):
-    """
-    Verifies the compatibility of a reference sft with a list of files.
-
-    Params
-    ------
-    ref_sft: StatefulTractogram
-        A tractogram to be used as reference.
-    files_to_verify: List[str]
-        List of files that should be compatible with the reference sft. Files
-        can be either other tractograms or nifti files (ex: masks).
-    parser: argument parser
-        Will raise an error if a file is not compatible.
-    args: Namespace
-        Should contain a args.reference if any file is a .tck, and possibly a
-        args.bbox_check (set to True by default).
-    """
-    save_ref = args.reference
-
-    for file in files_to_verify:
-        if file is not None:
-            _, ext = os.path.splitext(file)
-            if ext in ['.trk', '.tck', '.fib', '.vtk', '.dpy']:
-                # Cheating ref because it may send a lot of warning if loading
-                # many trk with ref (reference was maybe added only for some
-                # of these files)
-                if ext == '.trk':
-                    args.reference = None
+    bundles: List
+        The names of each bundle.
+    gt_masks: List
+        The gt_mask filenames per bundle (None if not set) (used for
+        tractometry statistics).
+    all_masks: List
+        The all_masks filenames per bundles (None if not set).
+    any_masks: List
+        The any_masks filenames per bundles (None if not set).
+    roi_options: List
+        The roi_option dict per bundle. Keys are 'gt_head', 'gt_tail' if
+        they are set, else 'gt_endpoints'.
+    angles: List
+        The maximum angles per bundle (None if not set).
+    lengths: List
+        The [min max] lengths per bundle (None if not set).
+    orientation_length: List
+        The [[min_x, max_x], [min_y, max_y], [min_z, max_z]] per bundle.
+        (None they are all not set).
+    """
+    angles = []
+    lengths = []
+    orientation_lengths = []
+    abs_orientation_lengths = []
+    gt_masks = []
+    all_masks = []
+    any_masks = []
+    roi_options = []
+    show_warning_gt = False
+
+    with open(args.gt_config, "r") as json_file:
+        config = json.load(json_file)
+
+        bundles = list(config.keys())
+        for bundle in bundles:
+            bundle_config = config[bundle]
+
+            if 'gt_mask' not in bundle_config:
+                show_warning_gt = True
+            if 'endpoints' not in bundle_config and \
+                    'head' not in bundle_config:
+                raise ValueError(
+                    "Bundle configuration for bundle {} misses 'endpoints' or "
+                    "'head'/'tail'".format(bundle))
+
+            angle = length = None
+            length_x = length_y = length_z = None
+            length_x_abs = length_y_abs = length_z_abs = None
+            gt_mask = all_mask = any_mask = roi_option = None
+
+            for key in bundle_config.keys():
+                if key == 'angle':
+                    angle = bundle_config['angle']
+                elif key == 'length':
+                    length = bundle_config['length']
+                elif key == 'length_x':
+                    length_x = bundle_config['length_x']
+                elif key == 'length_y':
+                    length_y = bundle_config['length_y']
+                elif key == 'length_z':
+                    length_z = bundle_config['length_z']
+                elif key == 'length_x_abs':
+                    length_x_abs = bundle_config['length_x_abs']
+                elif key == 'length_y_abs':
+                    length_y_abs = bundle_config['length_y_abs']
+                elif key == 'length_z_abs':
+                    length_z_abs = bundle_config['length_z_abs']
+                elif key == 'gt_mask':
+                    if args.gt_dir:
+                        gt_mask = os.path.join(args.gt_dir,
+                                               bundle_config['gt_mask'])
+                    else:
+                        gt_mask = bundle_config['gt_mask']
+
+                    if args.use_gt_masks_as_all_masks:
+                        all_mask = gt_mask
+                elif key == 'all_mask':
+                    if args.use_gt_masks_as_all_masks:
+                        raise ValueError(
+                            "With the option --use_gt_masks_as_all_masks, "
+                            "you should not add any all_mask in the config "
+                            "file.")
+                    if args.gt_dir:
+                        all_mask = os.path.join(args.gt_dir,
+                                                bundle_config['all_mask'])
+                    else:
+                        all_mask = bundle_config['all_mask']
+                elif key == 'endpoints':
+                    if 'head' in bundle_config or 'tail' in bundle_config:
+                        raise ValueError(
+                            "Bundle {} has confusing keywords in the config "
+                            "file. Please choose either endpoints OR "
+                            "head/tail.".format(bundle))
+                    if args.gt_dir:
+                        endpoints = os.path.join(args.gt_dir,
+                                                 bundle_config['endpoints'])
+                    else:
+                        endpoints = bundle_config['endpoints']
+                    roi_option = {'gt_endpoints': endpoints}
+                elif key == 'head':
+                    if 'tail' not in bundle_config:
+                        raise ValueError(
+                            "You have provided the head for bundle {}, but "
+                            "not the tail".format(bundle))
+                    if args.gt_dir:
+                        head = os.path.join(args.gt_dir, bundle_config['head'])
+                        tail = os.path.join(args.gt_dir, bundle_config['tail'])
+                    else:
+                        head = bundle_config['head']
+                        tail = bundle_config['tail']
+                    roi_option = {'gt_head': head, 'gt_tail': tail}
+                elif key == 'tail':
+                    pass  # dealt with at head
+                elif key == 'any_mask':
+                    if args.gt_dir:
+                        any_mask = os.path.join(
+                            args.gt_dir, bundle_config['any_mask'])
+                    else:
+                        any_mask = bundle_config['any_mask']
                 else:
-                    args.reference = save_ref
-                mask = load_tractogram_with_reference(parser, args, file)
-            else:  # should be a nifti file.
-                mask = file
-            compatible = is_header_compatible(ref_sft, mask)
-            if not compatible:
-                parser.error("Reference tractogram incompatible with {}"
-                             .format(file))
-
-
-def is_header_compatible_multiple_files(parser, list_files,
-                                        verbose_all_compatible=False):
-    """
-    Verifies the compatibility between the first item in list_files
-    and the remaining files in list.
-
-    parser: argument parser
-        Will raise an error if a file is not compatible.
-
-    list_files: List
-        List of files to test
+                    raise ValueError("Unrecognized value {} in the config "
+                                     "file for bundle {}".format(key, bundle))
 
-    verbose_all_compatible: bool
-        If true will print a message when everything is okay
-    """
-    all_valid = True
-
-    for filepath in list_files:
-        _, in_extension = split_name_with_nii(filepath)
-        if in_extension not in ['.trk', '.nii', '.nii.gz']:
-            parser.error('{} does not have a supported extension'.format(
-                filepath))
-
-    all_pairs = list(itertools.combinations(list_files, 2))
-    for curr_pair in all_pairs:
-        if not is_header_compatible(curr_pair[0], curr_pair[1]):
-            print('ERROR:\"{}\" and \"{}\" do not have compatible header.'.format(
-                curr_pair[0], curr_pair[1]))
-            all_valid = False
-
-    if all_valid and verbose_all_compatible:
-        print('All input files have compatible headers.')
-    elif not all_valid:
-        parser.error('All input files have not compatible header.')
-
-
-def read_info_from_mb_bdo(filename):
-    tree = ET.parse(filename)
-    root = tree.getroot()
-    geometry = root.attrib['type']
-    center_tag = root.find('origin')
-    flip = [-1, -1, 1]
-    center = [flip[0]*float(center_tag.attrib['x'].replace(',', '.')),
-              flip[1]*float(center_tag.attrib['y'].replace(',', '.')),
-              flip[2]*float(center_tag.attrib['z'].replace(',', '.'))]
-    row_list = tree.iter('Row')
-    radius = [None, None, None]
-    for i, row in enumerate(row_list):
-        for j in range(0, 3):
-            if j == i:
-                key = 'col' + str(j+1)
-                radius[i] = float(row.attrib[key].replace(',', '.'))
+            angles.append(angle)
+            lengths.append(length)
+            if length_x is None and length_y is None and length_z is None:
+                orientation_lengths.append(None)
             else:
-                key = 'col' + str(j+1)
-                value = float(row.attrib[key].replace(',', '.'))
-                if abs(value) > 0.01:
-                    raise ValueError('Does not support rotation, for now \n'
-                                     'only SO aligned on the X,Y,Z axis are '
-                                     'supported.')
-    radius = np.asarray(radius, dtype=np.float32)
-    center = np.asarray(center, dtype=np.float32)
-    return geometry, radius, center
-
-
-def load_matrix_in_any_format(filepath):
-    _, ext = os.path.splitext(filepath)
-    if ext == '.txt':
-        data = np.loadtxt(filepath)
-    elif ext == '.npy':
-        data = np.load(filepath)
-    elif ext == '.mat':
-        # .mat are actually dictionnary. This function support .mat from
-        # antsRegistration that encode a 4x4 transformation matrix.
-        transfo_dict = loadmat(filepath)
-        lps2ras = np.diag([-1, -1, 1])
-        transfo_key = 'AffineTransform_double_3_3'
-        if transfo_key not in transfo_dict:
-            transfo_key = 'AffineTransform_float_3_3'
-
-        rot = transfo_dict[transfo_key][0:9].reshape((3, 3))
-        trans = transfo_dict[transfo_key][9:12]
-        offset = transfo_dict['fixed']
-        r_trans = (np.dot(rot, offset) - offset - trans).T * [1, 1, -1]
-
-        data = np.eye(4)
-        data[0:3, 3] = r_trans
-        data[:3, :3] = np.dot(np.dot(lps2ras, rot), lps2ras)
-    else:
-        raise ValueError('Extension {} is not supported'.format(ext))
-
-    return data
-
-
-def save_matrix_in_any_format(filepath, output_data):
-    _, ext = os.path.splitext(filepath)
-    if ext == '.txt':
-        np.savetxt(filepath, output_data)
-    elif ext == '.npy':
-        np.save(filepath, output_data)
-    elif ext == '':
-        np.save('{}.npy'.format(filepath), output_data)
-    else:
-        raise ValueError('Extension {} is not supported'.format(ext))
-
-
-def assert_fsl_options_exist(parser, options_args, command):
-    """
-    Assert that all options for topup or eddy exist.
-    If not, print parser's usage and exit.
-
-    Parameters
-    ----------
-    parser: argparse.ArgumentParser object
-        Parser.
-    options_args: string
-        Options for fsl command
-    command: string
-        Command used (eddy or topup).
-    """
-    if command == 'eddy':
-        fsl_options = eddy_options
-    elif command == 'topup':
-        fsl_options = topup_options
-    else:
-        parser.error('{} command is not supported as fsl '
-                     'command.'.format(command))
-
-    options = re.split(r'[ =\s]\s*', options_args)
-    res = [i for i in options if "--" in i]
-    res = list(map(lambda x: x.replace('--', ''), res))
-
-    for nOption in res:
-        if nOption not in fsl_options:
-            parser.error('--{} is not a valid option for '
-                         '{} command.'.format(nOption, command))
-
-
-def parser_color_type(arg):
-    """
-    Validate that a color component is between RBG values, else return an error
-    From https://stackoverflow.com/a/55410582
-    """
-
-    MIN_VAL = 0
-    MAX_VAL = 255
-    try:
-        f = float(arg)
-    except ValueError:
-        raise argparse.ArgumentTypeError("Color component must be a floating "
-                                         "point number")
-    if f < MIN_VAL or f > MAX_VAL:
-        raise argparse.ArgumentTypeError(
-            "Argument must be < " + str(MAX_VAL) + "and > " + str(MIN_VAL))
-    return f
-
-
-def snapshot(scene, filename, **kwargs):
-    """ Wrapper around fury.window.snapshot
-    For some reason, fury.window.snapshot flips the image vertically.
-    This image unflips the image and then saves it.
-    """
-    out = window.snapshot(scene, **kwargs)
-    image = Image.fromarray(out[::-1])
-    image.save(filename)
-
-
-def ranged_type(value_type, min_value, max_value):
-    """Return a function handle of an argument type function for ArgumentParser
-    checking a range: `min_value` <= arg <= `max_value`.
-
-    Parameters
-    ----------
-    value_type : Type
-        Value-type to convert the argument.
-    min_value : scalar
-        Minimum acceptable argument value.
-    max_value : scalar
-       Maximum acceptable argument value.
-
-    Returns
-    -------
-    Function handle of an argument type function for ArgumentParser.
-
-    Usage
-    -----
-        ranged_type(float, 0.0, 1.0)
-    """
+                orientation_lengths.append(
+                    [length_x if length_x is not None else def_len,
+                     length_y if length_y is not None else def_len,
+                     length_z if length_z is not None else def_len])
+
+            if length_x_abs is None and length_y_abs is None and \
+                    length_z_abs is None:
+                abs_orientation_lengths.append(None)
+            else:
+                abs_orientation_lengths.append(
+                    [length_x_abs if length_x_abs is not None else def_len,
+                     length_y_abs if length_y_abs is not None else def_len,
+                     length_z_abs if length_z_abs is not None else def_len])
+            gt_masks.append(gt_mask)
+            all_masks.append(all_mask)
+            any_masks.append(any_mask)
+            roi_options.append(roi_option)
+
+    if show_warning_gt:
+        logging.info(
+            "At least one bundle had no gt_mask. Some tractometry metrics "
+            "won't be computed (OR, OL) for these bundles.")
+
+    return (bundles, gt_masks, all_masks, any_masks, roi_options,
+            lengths, angles, orientation_lengths, abs_orientation_lengths)
+
+
+def main():
+    parser = _build_arg_parser()
+    args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
+
+    # Load
+    (gt_tails, gt_heads, sft, bundle_names, list_rois, bundle_lengths, angles,
+     orientation_lengths, abs_orientation_lengths, inv_all_masks, gt_masks,
+     any_masks, dimensions,
+     json_outputs) = load_and_verify_everything(parser, args)
+
+    # Segment VB, WPC, IB
+    (vb_sft_list, wpc_sft_list, ib_sft_list, nc_sft,
+     ib_names, bundle_stats) = segment_tractogram_from_roi(
+        sft, gt_tails, gt_heads, bundle_names, bundle_lengths, angles,
+        orientation_lengths, abs_orientation_lengths, inv_all_masks, any_masks,
+        list_rois, args)
+
+    # Save results
+    with open(json_outputs[0], "w") as f:
+        json.dump(bundle_stats, f, indent=args.indent,
+                  sort_keys=args.sort_keys)
+
+    logging.info("Final segmented bundles will be saved in {}"
+                 .format(args.out_dir))
+    for i in range(len(bundle_names)):
+        if len(vb_sft_list[i]) > 0 or not args.no_empty:
+            filename = "segmented_VB/{}_VS.trk".format(bundle_names[i])
+            save_tractogram(vb_sft_list[i],
+                            os.path.join(args.out_dir, filename),
+                            bbox_valid_check=args.bbox_check)
+        if (args.save_wpc_separately and wpc_sft_list[i] is not None
+                and (len(wpc_sft_list[i]) > 0 or not args.no_empty)):
+            filename = "segmented_WPC/{}_wpc.trk".format(bundle_names[i])
+            save_tractogram(wpc_sft_list[i],
+                            os.path.join(args.out_dir, filename),
+                            bbox_valid_check=args.bbox_check)
+    for i in range(len(ib_sft_list)):
+        if len(ib_sft_list[i]) > 0 or not args.no_empty:
+            file = "segmented_IB/{}_IC.trk".format(ib_names[i])
+            save_tractogram(ib_sft_list[i], os.path.join(args.out_dir, file),
+                            bbox_valid_check=args.bbox_check)
+
+    # Tractometry on bundles
+    final_results = compute_tractometry(
+        vb_sft_list, wpc_sft_list, ib_sft_list, nc_sft,
+        args, bundle_names, gt_masks, dimensions, ib_names)
+
+    logging.info("Final scores will be saved in {}".format(json_outputs[1]))
+    final_results.update({"tractogram_filename": str(args.in_tractogram)})
+    with open(json_outputs[1], "w") as f:
+        json.dump(final_results, f, indent=args.indent,
+                  sort_keys=args.sort_keys)
 
-    def range_checker(arg: str):
-        try:
-            f = value_type(arg)
-        except ValueError:
-            raise argparse.ArgumentTypeError(f"must be a valid {value_type}")
-        if f < min_value or f > max_value:
-            raise argparse.ArgumentTypeError(
-                f"must be within [{min_value}, {max_value}]")
-        return f
 
-    # Return handle to checking function
-    return range_checker
+if __name__ == "__main__":
+    main()
```

### Comparing `scilpy-1.5.post2/scilpy/io/varian_fdf.py` & `scilpy-2.0.0/scilpy/io/varian_fdf.py`

 * *Files 6% similar despite different names*

```diff
@@ -6,15 +6,15 @@
 import re
 import struct
 
 from dipy.io.utils import is_header_compatible
 import nibabel as nib
 import numpy as np
 
-from scilpy.utils.util import str_to_index
+from scilpy.gradients.bvec_bval_tools import str_to_axis_index
 
 
 def load_fdf(file_path):
     """
     Load a Varian FDF file.
 
     Parameters
@@ -82,15 +82,15 @@
 
 def read_file(file_path):
     """
     Read a single fdf file.
 
     Parameters
     ----------
-    path: Path to the fdf file
+    file_path: Path to the fdf file
 
     Return
     ------
     raw_header: Dictionary of header information
     data: Numpy array of the fdf data
     """
     raw_header = dict()
@@ -293,20 +293,24 @@
                bval_path, bvec_path, out_path,
                affine=None, flip=None, swap=None):
     """
     Save a loaded fdf file to nifti.
 
     Parameters
     ----------
-    out_path: Path of the nifti file to be saved
-    data: Raw data to be saved
-    raw_header: Raw header from fdf files
+    dwi_data: Raw data to be saved
+    dwi_header: Raw header from fdf files
+    b0_data: Raw b0
+    b0_header: Raw header for b0
     bval_path: Path to the bval file to be saved
     bvec_path: Path to the bvec file to be saved
+    out_path: Path of the nifti file to be saved
     affine: Affine transformation to save with the data
+    flip:
+    swap:
 
     Return
     ------
     None
     """
     nifti1_dwi_header = format_raw_header(dwi_header)
     nifti1_b0_header = format_raw_header(b0_header)
@@ -348,17 +352,20 @@
                                bval_path=None, bvec_path=None,
                                flip=None, swap=None):
     """
     Write gradient information in present in the header.
 
     Parameters
     ----------
-    header: The header with gradient info.
+    dwi_header: The header with gradient info.
+    b0_header: The header of b0
     bval_path: Path to the bval file to be saved.
     bvec_path: Path to the bvec path to be saved.
+    flip:
+    swap:
 
     Return
     ------
     None
     """
     keys = ['bvalue', 'diff_x', 'diff_y', 'diff_z']
     if all(k in dwi_header for k in keys) and \
@@ -374,32 +381,32 @@
                               len(b0_header['diff_x'] + dwi_header['diff_x'])))
 
             bvecs[0, :] = b0_header['diff_x'] + dwi_header['diff_x']
             bvecs[1, :] = b0_header['diff_y'] + dwi_header['diff_y']
             bvecs[2, :] = b0_header['diff_z'] + dwi_header['diff_z']
 
             if flip:
-                axes = [str_to_index(axis) for axis in list(flip)]
+                axes = [str_to_axis_index(axis) for axis in list(flip)]
                 for axis in axes:
                     bvecs[axis, :] *= -1
 
             if swap:
-                axes = [str_to_index(axis) for axis in list(swap)]
+                axes = [str_to_axis_index(axis) for axis in list(swap)]
                 new_bvecs = np.zeros(bvecs.shape)
                 new_bvecs[axes[0], :] = bvecs[axes[1], :]
                 new_bvecs[axes[1], :] = bvecs[axes[0], :]
                 bvecs = new_bvecs
 
             np.savetxt(bvec_path, bvecs, "%.8f")
     else:
         raise Exception(
             'Could not save gradient. Some keys are missing.')
 
 
-def correct_dwi_intensity(dwi_data, dwi_path, b0_path):
+def correct_procpar_intensity(dwi_data, dwi_path, b0_path):
     """Applies a linear value correction based on gain difference
         found in procpar files. The basic formulae provided by Luc Tremblay
         Luc.Tremblay@usherbrooke.ca is:
 
             Ldb = 20log(P1/P0)
 
         Where:
@@ -413,28 +420,30 @@
             Ldb = 20log(P1)
 
         P1 is the factor we are looking for. Isolating P1 gives us the final
         equation:
 
             P1 = 10**(Ldb/20)
     """
-
+    # Not really an io function: does not load and save! But it is the only
+    # method we have concerning varian_fdf. Kept here, as decided by guru
+    # Arnaud.
     dwi_gain = get_gain(dwi_path)
     b0_gain = get_gain(b0_path)
 
     if dwi_gain is None or b0_gain is None:
         raise Exception(
             'Could not apply gain correction, make sure that your B0 and '
             'dwi procpar files contain the gain tag.')
 
     gain_difference = dwi_gain - b0_gain
     correction_factor = 10 ** (gain_difference / 20.0)
 
     # The dwi intensity is divided by factor instead of multiplying b0's
-    # intensity. This allows the scaling step in compute_dti_metrics to
+    # intensity. This allows the scaling step in dti_metrics to
     # be applyed correclty.
     dwi_data *= 1.0 / correction_factor
 
     return dwi_data
 
 
 def get_gain(procpar_path):
```

### Comparing `scilpy-1.5.post2/scilpy/preprocessing/distortion_correction.py` & `scilpy-2.0.0/scilpy/preprocessing/distortion_correction.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,16 @@
 # -*- coding: utf-8 -*-
 
+import logging
+
 import numpy as np
 
 
-def create_acqparams(readout, encoding_direction, nb_b0s=1, nb_rev_b0s=1):
+def create_acqparams(readout, encoding_direction, synb0=False,
+                     nb_b0s=1, nb_rev_b0s=1):
     """
     Create acqparams for Topup and Eddy
 
     Parameters
     ----------
     readout: float
         Readout time
@@ -19,21 +22,26 @@
         number of reverse b=0 images
 
     Returns
     -------
     acqparams: np.array
         acqparams
     """
+    if synb0:
+        logging.warning('Using SyNb0, untested feature. Be careful.')
+
     acqparams = np.zeros((nb_b0s + nb_rev_b0s, 4))
     acqparams[:, 3] = readout
 
     enum_direction = {'x': 0, 'y': 1, 'z': 2}
     acqparams[0:nb_b0s, enum_direction[encoding_direction]] = 1
     if nb_rev_b0s > 0:
-        acqparams[nb_b0s:, enum_direction[encoding_direction]] = -1
+        val = -1 if not synb0 else 1
+        acqparams[nb_b0s:, enum_direction[encoding_direction]] = val
+        acqparams[nb_b0s:, 3] = readout if not synb0 else 0
 
     return acqparams
 
 
 def create_index(bvals, n_rev=0):
     """
     Create index of bvals for Eddy
```

### Comparing `scilpy-1.5.post2/scilpy/reconst/afd_along_streamlines.py` & `scilpy-2.0.0/scilpy/tractanalysis/afd_along_streamlines.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,18 +1,20 @@
 # -*- coding: utf-8 -*-
 
 from dipy.data import get_sphere
+from dipy.reconst.shm import sh_to_sf_matrix, sph_harm_ind_list
 import numpy as np
 from scipy.special import lpn
 
-from scilpy.reconst.utils import find_order_from_nb_coeff, get_b_matrix
+from scilpy.reconst.utils import find_order_from_nb_coeff
 from scilpy.tractanalysis.grid_intersections import grid_intersections
 
 
-def afd_map_along_streamlines(sft, fodf, fodf_basis, length_weighting):
+def afd_map_along_streamlines(sft, fodf, fodf_basis, length_weighting,
+                              is_legacy=True):
     """
     Compute the mean Apparent Fiber Density (AFD) and mean Radial fODF
     (radfODF) maps along a bundle.
 
     Parameters
     ----------
     sft : StatefulTractogram
@@ -29,29 +31,29 @@
     -------
     afd_sum : np.array
         AFD map (weighted if length_weighting)
     rd_sum : np.array
         rdAFD map (weighted if length_weighting)
     """
 
-
     afd_sum, rd_sum, weights = \
         afd_and_rd_sums_along_streamlines(sft, fodf, fodf_basis,
-                                          length_weighting)
+                                          length_weighting,
+                                          is_legacy=is_legacy)
 
     non_zeros = np.nonzero(afd_sum)
     weights_nz = weights[non_zeros]
     afd_sum[non_zeros] /= weights_nz
     rd_sum[non_zeros] /= weights_nz
 
     return afd_sum, rd_sum
 
 
 def afd_and_rd_sums_along_streamlines(sft, fodf, fodf_basis,
-                                      length_weighting):
+                                      length_weighting, is_legacy=True):
     """
     Compute the mean Apparent Fiber Density (AFD) and mean Radial fODF (radfODF)
     maps along a bundle.
 
     Parameters
     ----------
     sft : StatefulTractogram
@@ -59,14 +61,16 @@
     fodf : nibabel.image
         fODF with shape (X, Y, Z, #coeffs).
         #coeffs depend on the sh_order.
     fodf_basis : string
         Has to be descoteaux07 or tournier07.
     length_weighting : bool
         If set, will weigh the AFD values according to segment lengths.
+    is_legacy : bool, optional
+        Whether or not the SH basis is in its legacy form.
 
     Returns
     -------
     afd_sum_map : np.array
         AFD map.
     rd_sum_map : np.array
         fdAFD map.
@@ -76,15 +80,16 @@
 
     sft.to_vox()
     sft.to_corner()
 
     fodf_data = fodf.get_fdata(dtype=np.float32)
     order = find_order_from_nb_coeff(fodf_data)
     sphere = get_sphere('repulsion724')
-    b_matrix, _, n = get_b_matrix(order, sphere, fodf_basis, return_all=True)
+    b_matrix, _ = sh_to_sf_matrix(sphere, order, fodf_basis, legacy=is_legacy)
+    _, n = sph_harm_ind_list(order)
     legendre0_at_n = lpn(order, 0)[0][n]
     sphere_norm = np.linalg.norm(sphere.vertices)
 
     afd_sum_map = np.zeros(shape=fodf_data.shape[:-1])
     rd_sum_map = np.zeros(shape=fodf_data.shape[:-1])
     weight_map = np.zeros(shape=fodf_data.shape[:-1])
 
@@ -108,27 +113,27 @@
 
         # Those starting points are used for the segment vox_idx computations
         strl_start = crossed_indices[non_zero_lengths]
         vox_indices = (strl_start + (0.5 * segments)).astype(int)
 
         normalization_weights = np.ones_like(seg_lengths)
         if length_weighting:
-            normalization_weights = seg_lengths / np.linalg.norm(fodf.header.get_zooms()[:3])
+            normalization_weights = seg_lengths / \
+                np.linalg.norm(fodf.header.get_zooms()[:3])
 
         for vox_idx, closest_vertex_index, norm_weight in zip(vox_indices,
                                                               closest_vertex_indices,
                                                               normalization_weights):
             vox_idx = tuple(vox_idx)
-            b_at_idx = b_matrix[closest_vertex_index]
+            b_at_idx = b_matrix.T[closest_vertex_index]
             fodf_at_index = fodf_data[vox_idx]
 
             afd_val = np.dot(b_at_idx, fodf_at_index)
             rd_val = np.dot(np.dot(b_at_idx.T, p_matrix),
                             fodf_at_index)
 
             afd_sum_map[vox_idx] += afd_val * norm_weight
             rd_sum_map[vox_idx] += rd_val * norm_weight
             weight_map[vox_idx] += norm_weight
 
     rd_sum_map[rd_sum_map < 0.] = 0.
-
     return afd_sum_map, rd_sum_map, weight_map
```

### Comparing `scilpy-1.5.post2/scilpy/reconst/b_tensor_utils.py` & `scilpy-2.0.0/scilpy/io/btensor.py`

 * *Files 26% similar despite different names*

```diff
@@ -2,46 +2,87 @@
 
 from dipy.core.gradients import (gradient_table,
                                  unique_bvals_tolerance, get_bval_indices)
 from dipy.io.gradients import read_bvals_bvecs
 import nibabel as nib
 import numpy as np
 
-from scilpy.utils.bvec_bval_tools import (normalize_bvecs, is_normalized_bvecs,
-                                          extract_dwi_shell)
+from scilpy.dwi.utils import extract_dwi_shell
+from scilpy.gradients.bvec_bval_tools import (normalize_bvecs,
+                                              is_normalized_bvecs,
+                                              check_b0_threshold)
 
 
 bshapes = {0: "STE", 1: "LTE", -0.5: "PTE", 0.5: "CTE"}
+bdeltas = {"STE": 0, "LTE": 1, "PTE": -0.5, "CTE": 0.5}
 
 
-def generate_btensor_input(in_dwis, in_bvals, in_bvecs,
-                           in_bdeltas, force_b0_threshold,
-                           do_pa_signals=False, tol=20):
+def convert_bshape_to_bdelta(b_shapes):
+    """Convert an array of b_shapes to an array of b_deltas.
+
+    Parameters
+    ----------
+    b_shapes: array of strings
+        b_shapes to convert. Strings can only be LTE, PTE, STE or CTE.
+
+    Returns
+    -------
+    b_deltas: array of floats
+        Converted b_deltas, such that LTE = 1, STE = 0, PTE = -0.5, CTE = 0.5.
+    """
+    b_deltas = np.vectorize(bdeltas.get)(b_shapes)
+    return b_deltas
+
+
+def convert_bdelta_to_bshape(b_deltas):
+    """Convert an array of b_deltas to an array of b_shapes.
+
+    Parameters
+    ----------
+    b_deltas: array of floats
+        b_deltas to convert. Floats can only be 1, 0, -0.5 or 0.5.
+
+    Returns
+    -------
+    b_shapes: array of strings
+        Converted b_shapes, such that LTE = 1, STE = 0, PTE = -0.5, CTE = 0.5.
+    """
+    b_shapes = np.vectorize(bshapes.get)(b_deltas)
+    return b_shapes
+
+
+def generate_btensor_input(in_dwis, in_bvals, in_bvecs, in_bdeltas,
+                           do_pa_signals=False, tol=20, skip_b0_check=False):
     """Generate b-tensor input from an ensemble of data, bvals and bvecs files.
     This generated input is mandatory for all scripts using b-tensor encoding
     data. Also generate the powder-averaged (PA) data if set.
 
+    For the moment, this does not enable the use of a b0_threshold different
+    than the tolerance.
+
     Parameters
     ----------
     in_dwis : list of strings (file paths)
         Diffusion data files for each b-tensor encodings.
     in_bvals : list of strings (file paths)
         All of the bvals files associated.
     in_bvecs : list of strings (file paths)
         All of the bvecs files associated.
     in_bdeltas : list of floats
         All of the b_deltas (describing the type of encoding) files associated.
-    force_b0_threshold : bool, optional
-        If set, will continue even if the minimum bvalue is suspiciously high.
     do_pa_signals : bool, optional
         If set, will compute the powder_averaged input instead of the regular
         one. This means that the signal is averaged over all directions for
         each bvals.
     tol : int
         tolerance gap for b-values clustering. Defaults to 20
+    skip_b0_check: bool
+        (See full explanation in io.utils.add_skip_b0_check_arg.) If true,
+        script will continue even if no b-values are found below the tolerance
+        (no b0s found).
 
     Returns
     -------
     gtab_full : GradientTable
         A single gradient table containing the information of all encodings.
     data_full : np.ndarray (4d)
         All concatenated diffusion data from the different encodings.
@@ -68,14 +109,16 @@
     ubvals_divide = np.empty(0)
     acq_index = 0
     for inputf, bvalsf, bvecsf, b_delta in zip(in_dwis, in_bvals,
                                                in_bvecs, in_bdeltas):
         if inputf:  # verifies if the input file exists
             vol = nib.load(inputf)
             bvals, bvecs = read_bvals_bvecs(bvalsf, bvecsf)
+            _ = check_b0_threshold(bvals.min(), b0_thr=tol,
+                                   skip_b0_check=skip_b0_check)
             if np.sum([bvals > tol]) != 0:
                 bvals = np.round(bvals)
             if not is_normalized_bvecs(bvecs):
                 logging.warning('Your b-vectors do not seem normalized...')
                 bvecs = normalize_bvecs(bvecs)
             ubvals = unique_bvals_tolerance(bvals, tol=tol)
             for ubval in ubvals:  # Loop over all unique bvals
@@ -119,15 +162,15 @@
         gtab_infos = np.ndarray((4, len(ubvals_full)))
         gtab_infos[0] = ubvals_divide
         gtab_infos[1] = ub_deltas_full
         gtab_infos[2] = nb_bvecs_full
         gtab_infos[3] = acq_index_full
         if np.sum([ubvals_full < tol]) < acq_index - 1:
             gtab_infos[3] *= 0
-        return(pa_signals, gtab_infos)
+        return pa_signals, gtab_infos
     # Removing the duplicate b0s from ubvals_full
     duplicate_b0_ind = np.union1d(np.argwhere(ubvals_full == min(ubvals_full)),
                                   np.argwhere(ubvals_full > tol))
     ubvals_full = ubvals_full[duplicate_b0_ind]
     ub_deltas_full = ub_deltas_full[duplicate_b0_ind]
     # Sorting the data by bvals
     sorted_indices = np.argsort(bvals_full, axis=0)
@@ -148,8 +191,8 @@
     ub_deltas_full = np.take_along_axis(np.asarray(ub_deltas_full),
                                         sorted_indices, axis=0)
     # Creating the corresponding gtab
     gtab_full = gradient_table(bvals_full, bvecs_full,
                                b0_threshold=bvals_full.min(),
                                btens=b_shapes)
 
-    return(gtab_full, data_full, ubvals_full, ub_deltas_full)
+    return gtab_full, data_full, ubvals_full, ub_deltas_full
```

### Comparing `scilpy-1.5.post2/scilpy/reconst/bingham.py` & `scilpy-2.0.0/scilpy/reconst/bingham.py`

 * *Files 2% similar despite different names*

```diff
@@ -16,18 +16,18 @@
 NB_PARAMS = 7
 
 
 class BinghamDistribution(object):
     """
     Scaled Bingham distribution, given by:
         B(u) = f0 * e^(- k1 * (mu1 * u)**2 - k2 * (mu2 * u)**2),
-    mu1 and mu2 are unit vectors.
+        mu1 and mu2 are unit vectors.
 
-    Params
-    ------
+    Parameters
+    ----------
     f0: float
         Scaling parameter of the distribution.
     mu_prime1: ndarray (3,)
         Axis with highest concentration scaled by the
         concentration parameter k1.
     mu_prime2: ndarray (3,)
         Axis with lowest concentration scaled by the
@@ -106,15 +106,15 @@
 
 def bingham_to_peak_direction(bingham_volume):
     """
     Compute peak direction for each lobe for a given Bingham volume.
 
     Parameters
     ----------
-    binghams: ndarray (..., max_lobes, 9)
+    bingham_volume: ndarray (..., max_lobes, 9)
         Bingham volume.
 
     Returns
     -------
     peak_dir: ndarray (..., max_lobes, 3)
         Peak direction image.
     """
```

### Comparing `scilpy-1.5.post2/scilpy/reconst/divide_fit.py` & `scilpy-2.0.0/scilpy/reconst/divide.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,13 +1,16 @@
+# -*- coding: utf-8 -*-
+import itertools
+import multiprocessing
 import numpy as np
 from scipy.optimize import curve_fit
 from scipy.special import erf
 
 
-def get_bounds():
+def _get_bounds():
     """Define the lower (lb) and upper (ub) boundaries of the fitting
     parameters, being the signal without diffusion weighting (S0), the mean
     diffusivity (MD), the isotropic variance (V_I) and the anisotropic variance
     (V_A).
 
     Returns
     -------
@@ -21,71 +24,71 @@
     V_I = [1e-24, 5e-18]
     V_A = [1e-24, 5e-18]
     lb = [S0[0], MD[0], V_I[0], V_A[0]]
     ub = [S0[1], MD[1], V_I[1], V_A[1]]
     return lb, ub
 
 
-def random_p0(signal, gtab_infos, lb, ub, weight, n_iter):
+def _random_p0(signal, gtab_infos, lb, ub, weight, n_iter):
     """Produce a guess of initial parameters for the fit, by calculating the
     signals of a given number of random sets of parameters and keeping the one
     closest to the input signal.
 
     Parameters
     ----------
-    signal : np.array
+    signal : np.ndarray
         Diffusion data of a single voxel.
     gtab_infos : np.ndarray
         Contains information about the gtab, such as the unique bvals, the
         encoding types, the number of directions and the acquisition index.
         Obtained as output of the function
-        `reconst.b_tensor_utils.generate_btensor_input`.
-    lb : list of floats
+        `io.btensor.generate_btensor_input`.
+    lb : np.ndarray of floats
         Lower boundaries of the fitting parameters.
-    ub : list of floats
+    ub : np.ndarray of floats
         Upper boundaries of the fitting parameters.
-    weight : np.array
+    weight : np.ndarray
         Gives a different weight to each element of `signal`.
     n_iter : int
         Number of random sets of parameters tested.
 
     Returns
     -------
-    guess : np.array
+    guess : np.ndarray
         Array containing the guessed initial parameters.
     """
     guess = []
     thr = np.inf
 
     for i in range(n_iter):
         params_rand = lb + (ub - lb) * np.random.rand(len(lb))
-        signal_rand = gamma_fit2data(gtab_infos, params_rand)
+        signal_rand = _gamma_fit2data(gtab_infos, params_rand)
         residual_rand = np.sum(((signal - signal_rand) * weight)**2)
 
         if residual_rand < thr:
             thr = residual_rand
             guess = params_rand
 
     return guess
 
 
-def gamma_data2fit(signal, gtab_infos, fit_iters=1, random_iters=50,
-                   do_weight_bvals=False, do_weight_pa=False,
-                   do_multiple_s0=False):
+def _gamma_data2fit(signal, gtab_infos, fit_iters=1, random_iters=50,
+                    do_weight_bvals=False, do_weight_pa=False,
+                    do_multiple_s0=False):
     """Fit the gamma model to data
 
     Parameters
     ----------
     signal : np.array
         Diffusion data of a single voxel.
     gtab_infos : np.ndarray
         Contains information about the gtab, such as the unique bvals, the
         encoding types, the number of directions and the acquisition index.
         Obtained as output of the function
-        `reconst.b_tensor_utils.generate_btensor_input`.
+        `io.btensor.generate_btensor_input`.
     fit_iters : int, optional
         Number of iterations in the gamma fit. Defaults to 1.
     random_iters : int, optional
         Number of random sets of parameters tested to find the initial
         parameters. Defaults to 50.
     do_weight_bvals : bool , optional
         If set, does a weighting on the bvalues in the gamma fit.
@@ -123,18 +126,18 @@
         return weight
 
     def my_gamma_fit2data(gtab_infos, *args):
         """Compute a signal from gtab infomations and fit parameters.
         """
         params_unit = args
         params_SI = params_unit * unit_to_SI
-        signal = gamma_fit2data(gtab_infos, params_SI)
+        signal = _gamma_fit2data(gtab_infos, params_SI)
         return signal * weight
 
-    lb_SI, ub_SI = get_bounds()
+    lb_SI, ub_SI = _get_bounds()
     lb_SI = np.concatenate((lb_SI, 0.5 * np.ones(ns)))
     ub_SI = np.concatenate((ub_SI, 2.0 * np.ones(ns)))
     lb_SI[0] *= np.max(signal)
     ub_SI[0] *= np.max(signal)
 
     lb_unit = lb_SI / unit_to_SI
     ub_unit = ub_SI / unit_to_SI
@@ -146,16 +149,16 @@
     for i in range(fit_iters):
         weight = np.ones(len(signal))
         if do_weight_bvals:
             weight *= weight_bvals(0.07, 1e-9, 2)
         if do_weight_pa:
             weight *= weight_pa()
 
-        p0_SI = random_p0(signal, gtab_infos, lb_SI, ub_SI, weight,
-                          random_iters)
+        p0_SI = _random_p0(signal, gtab_infos, lb_SI, ub_SI, weight,
+                           random_iters)
         p0_unit = p0_SI / unit_to_SI
         params_unit, params_cov = curve_fit(my_gamma_fit2data, gtab_infos,
                                             signal * weight, p0=p0_unit,
                                             bounds=bounds_unit, method="trf",
                                             ftol=1e-8, xtol=1e-8, gtol=1e-8)
 
         if do_weight_bvals:
@@ -165,35 +168,35 @@
 
             params_unit, params_cov = curve_fit(my_gamma_fit2data, gtab_infos,
                                                 signal * weight,
                                                 p0=params_unit,
                                                 bounds=bounds_unit,
                                                 method="trf")
 
-        signal_fit = gamma_fit2data(gtab_infos, params_unit * unit_to_SI)
+        signal_fit = _gamma_fit2data(gtab_infos, params_unit * unit_to_SI)
         residual = np.sum(((signal - signal_fit) * weight) ** 2)
         if residual < res_thr:
             res_thr = residual
             params_best = params_unit
             # params_cov_best = params_cov
 
     params_best[0] = params_best[0] * unit_to_SI[0]
     return params_best[0:4]
 
 
-def gamma_fit2data(gtab_infos, params):
+def _gamma_fit2data(gtab_infos, params):
     """Compute a signal from gtab infomations and fit parameters.
 
     Parameters
     ----------
     gtab_infos : np.ndarray
         Contains information about the gtab, such as the unique bvals, the
         encoding types, the number of directions and the acquisition index.
         Obtained as output of the function
-        `reconst.b_tensor_utils.generate_btensor_input`.
+        `io.btensor.generate_btensor_input`.
     params : np.array
         Array containing the parameters of the fit.
 
     Returns
     -------
     signal : np.array
         Array containing the signal produced by the gamma model.
@@ -253,7 +256,102 @@
     MK_A = 3 * V_A / (MD ** 2)
     MK_T = 3 * V_T / (MD ** 2)
     microFA2 = (3/2.) * (V_L / (V_I + V_L + (MD ** 2)))
     microFA = np.real(np.sqrt(microFA2))
     microFA[np.isnan(microFA)] = 0
 
     return microFA, MK_I, MK_A, MK_T
+
+
+def _fit_gamma_parallel(args):
+    data = args[0]
+    gtab_infos = args[1]
+    fit_iters = args[2]
+    random_iters = args[3]
+    do_weight_bvals = args[4]
+    do_weight_pa = args[5]
+    do_multiple_s0 = args[6]
+    chunk_id = args[7]
+
+    sub_fit_array = np.zeros((data.shape[0], 4))
+    for i in range(data.shape[0]):
+        if data[i].any():
+            sub_fit_array[i] = _gamma_data2fit(data[i], gtab_infos, fit_iters,
+                                               random_iters, do_weight_bvals,
+                                               do_weight_pa, do_multiple_s0)
+
+    return chunk_id, sub_fit_array
+
+
+def fit_gamma(data, gtab_infos, mask=None, fit_iters=1, random_iters=50,
+              do_weight_bvals=False, do_weight_pa=False, do_multiple_s0=False,
+              nbr_processes=None):
+    """Fit the gamma model to data
+
+    Parameters
+    ----------
+    data : np.ndarray (4d)
+        Diffusion data, powder averaged. Obtained as output of the function
+        `reconst.b_tensor_utils.generate_powder_averaged_data`.
+    gtab_infos : np.ndarray
+        Contains information about the gtab, such as the unique bvals, the
+        encoding types, the number of directions and the acquisition index.
+        Obtained as output of the function
+        `reconst.b_tensor_utils.generate_powder_averaged_data`.
+    mask : np.ndarray, optional
+        If `mask` is provided, only the data inside the mask will be
+        used for computations.
+    fit_iters : int, optional
+        Number of iterations in the gamma fit. Defaults to 1.
+    random_iters : int, optional
+        Number of random sets of parameters tested to find the initial
+        parameters. Defaults to 50.
+    do_weight_bvals : bool , optional
+        If set, does a weighting on the bvalues in the gamma fit.
+    do_weight_pa : bool, optional
+        If set, does a powder averaging weighting in the gamma fit.
+    do_multiple_s0 : bool, optional
+        If set, takes into account multiple baseline signals.
+    nbr_processes : int, optional
+        The number of subprocesses to use.
+        Default: multiprocessing.cpu_count()
+
+    Returns
+    -------
+    fit_array : np.ndarray
+        Array containing the fit
+    """
+    data_shape = data.shape
+    if mask is None:
+        mask = np.sum(data, axis=3).astype(bool)
+
+    nbr_processes = multiprocessing.cpu_count() if nbr_processes is None \
+        or nbr_processes <= 0 else nbr_processes
+
+    # Ravel the first 3 dimensions while keeping the 4th intact, like a list of
+    # 1D time series voxels. Then separate it in chunks of len(nbr_processes).
+    data = data[mask].reshape((np.count_nonzero(mask), data_shape[3]))
+    chunks = np.array_split(data, nbr_processes)
+
+    chunk_len = np.cumsum([0] + [len(c) for c in chunks])
+    pool = multiprocessing.Pool(nbr_processes)
+    results = pool.map(_fit_gamma_parallel,
+                       zip(chunks,
+                           itertools.repeat(gtab_infos),
+                           itertools.repeat(fit_iters),
+                           itertools.repeat(random_iters),
+                           itertools.repeat(do_weight_bvals),
+                           itertools.repeat(do_weight_pa),
+                           itertools.repeat(do_multiple_s0),
+                           np.arange(len(chunks))))
+    pool.close()
+    pool.join()
+
+    # Re-assemble the chunk together in the original shape.
+    fit_array = np.zeros((data_shape[0:3])+(4,))
+    tmp_fit_array = np.zeros((np.count_nonzero(mask), 4))
+    for i, fit in results:
+        tmp_fit_array[chunk_len[i]:chunk_len[i+1]] = fit
+
+    fit_array[mask] = tmp_fit_array
+
+    return fit_array
```

### Comparing `scilpy-1.5.post2/scilpy/reconst/dti.py` & `scilpy-2.0.0/scilpy/io/tensor.py`

 * *Files identical despite different names*

### Comparing `scilpy-1.5.post2/scilpy/reconst/fiber_coherence.py` & `scilpy-2.0.0/scilpy/reconst/fiber_coherence.py`

 * *Files 21% similar despite different names*

```diff
@@ -2,18 +2,31 @@
 import itertools
 import numpy as np
 
 
 NB_FLIPS = 4
 ANGLE_TH = np.pi / 6.
 
+# directions to the 26 neighbors.
+# Preparing once rather than in compute_fiber_coherence, possibly called many
+# times.
+ALL_NEIGHBORS = np.indices((3, 3, 3))
+ALL_NEIGHBORS = ALL_NEIGHBORS.T.reshape((27, 3)) - 1
+ALL_NEIGHBORS = np.delete(ALL_NEIGHBORS, 13, axis=0)
 
-def compute_fiber_coherence_table(directions, values):
+
+def compute_coherence_table_for_transforms(directions, values):
     """
-    Compute fiber coherence indexes for all possible axes permutations/flips.
+    Compute fiber coherence indexes for all possible axes permutations/flips
+    (ex, originating from a flip in the gradient table).
+
+    The mathematics are presented in :
+    [1] Schilling et al. A fiber coherence index for quality control of B-table
+    orientation in diffusion MRI scans. Magn Reson Imaging. 2019 May;58:82-89.
+    doi: 10.1016/j.mri.2019.01.018.
 
     Parameters
     ----------
     directions: ndarray (x, y, z, 3)
         Principal fiber orientation for each voxel.
     values: ndarray (x, y, z)
         Anisotropy measure for each voxel (e.g. FA map).
@@ -22,26 +35,27 @@
     -------
     coherence: list
         Fiber coherence value for each permutation/flip.
     transforms: list
         Transform representing each permutation/flip, in the same
         order as `coherence` list.
     """
+    # Generate transforms for 24 possible permutation/flips of
+    # gradient directions. (Reminder. We want to verify if there was possibly
+    # an error in the gradient table).
     permutations = list(itertools.permutations([0, 1, 2]))
     transforms = np.zeros((len(permutations)*NB_FLIPS, 3, 3))
-
-    # Generate transforms for 24 possible permutation/flips of
-    # gradient directions
     for i in range(len(permutations)):
         transforms[i*NB_FLIPS, np.arange(3), permutations[i]] = 1
         for ii in range(3):
             flip = np.eye(3)
             flip[ii, ii] = -1
             transforms[ii+i*NB_FLIPS+1] = transforms[i*NB_FLIPS].dot(flip)
 
+    # Compute the coherence for each one.
     coherence = []
     for t in transforms:
         index = compute_fiber_coherence(directions.dot(t), values)
         coherence.append(index)
     return coherence, list(transforms)
 
 
@@ -57,34 +71,44 @@
         Anisotropy measure for each voxel (e.g. FA map).
 
     Returns
     -------
     coherence: float
         Fiber coherence value.
     """
-    # directions to neighbors
-    all_d = np.indices((3, 3, 3))
-    all_d = all_d.T.reshape((27, 3)) - 1
-    all_d = np.delete(all_d, 13, axis=0)
-
+    # Normalizing peaks
     norm_peaks = np.zeros_like(peaks)
     norms = np.linalg.norm(peaks, axis=-1)
     norm_peaks[norms > 0] = peaks[norms > 0] / norms[norms > 0][..., None]
 
     coherence = 0.0
-    for di in all_d:
+    for di in ALL_NEIGHBORS:
         tx, ty, tz = di.astype(int)
         slice_x = slice(1 + tx, peaks.shape[0] - 1 + tx)
         slice_y = slice(1 + ty, peaks.shape[1] - 1 + ty)
         slice_z = slice(1 + tz, peaks.shape[2] - 1 + tz)
 
         di_norm = di / np.linalg.norm(di)
-        I_u = np.abs(norm_peaks.dot(di_norm)) > np.cos(ANGLE_TH)
+
+        # Spatial coherence between the peak at each voxel and the direction to
+        # the neighbor di.
+        # Ex: if the peak is aligned in x and current di is aligned in x,
+        # returns True (with angle < 30 ; cos angle > 30)
+        cos_angles = np.abs(norm_peaks.dot(di_norm))
+        I_u = cos_angles > np.cos(ANGLE_TH)
+
+        # Doing the same thing with v; results in the same image but translated
+        # from one voxel. (With 1 voxel padding around the border).
         I_v = np.zeros_like(I_u)
         I_v[1:-1, 1:-1, 1:-1] = I_u[slice_x, slice_y, slice_z]
 
+        # Where both conditions are met:
         I_uv = np.logical_and(I_u, I_v)
         u = np.nonzero(I_uv)
+
+        # v = the same voxels as u, but with the neighborhood difference.
         v = tuple(np.array(u) + di.astype(int).reshape(3, 1))
+
+        # Summing the FA of those voxels
         coherence += np.sum(values[u]) + np.sum(values[v])
 
     return coherence
```

### Comparing `scilpy-1.5.post2/scilpy/reconst/fodf.py` & `scilpy-2.0.0/scripts/scil_gradients_validate_correct.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,131 +1,128 @@
+#! /usr/bin/env python3
 # -*- coding: utf-8 -*-
+"""
+Detect sign flips and/or axes swaps in the gradients table from a fiber
+coherence index [1]. The script takes as input the principal direction(s)
+at each voxel, the b-vectors and the fractional anisotropy map and outputs
+a corrected b-vectors file.
+
+A typical pipeline could be:
+>>> scil_dti_metrics.py dwi.nii.gz bval bvec --not_all --fa fa.nii.gz
+    --evecs peaks.nii.gz
+>>> scil_gradients_validate_correct.py bvec peaks_v1.nii.gz fa.nii.gz bvec_corr
+
+Note that peaks_v1.nii.gz is the file containing the direction associated
+to the highest eigenvalue at each voxel.
+
+It is also possible to use a file containing multiple principal directions per
+voxel, given that they are sorted by decreasing amplitude. In that case, the
+first direction (with the highest amplitude) will be chosen for validation.
+Only 4D data is supported, so the directions must be stored in a single
+dimension. For example, peaks.nii.gz from scil_fodf_metrics.py could be used.
 
+Formerly: scil_validate_and_correct_bvecs.py
+"""
+
+import argparse
 import logging
 
-from dipy.core.gradients import gradient_table
-from dipy.data import get_sphere
-from dipy.direction.peaks import peaks_from_model
-from dipy.reconst.csdeconv import ConstrainedSphericalDeconvModel
-
-from scilpy.io.utils import validate_sh_basis_choice
-from scilpy.utils.bvec_bval_tools import (check_b0_threshold, normalize_bvecs,
-                                          is_normalized_bvecs)
-
-
-def compute_fodf(data, bvals, bvecs, full_frf, sh_order=8, nbr_processes=None,
-                 mask=None, sh_basis='descoteaux07', return_sh=True,
-                 n_peaks=5, force_b0_threshold=False):
-    """
-     Script to compute Constrained Spherical Deconvolution (CSD) fiber ODFs.
-
-     By default, will output all possible files, using default names. Specific
-     names can be specified using the file flags specified in the "File flags"
-     section.
-
-    If --not_all is set, only the files specified explicitly by the flags
-    will be output.
-
-    See [Tournier et al. NeuroImage 2007] and [Cote et al Tractometer MedIA 2013]
-    for quantitative comparisons with Sharpening Deconvolution Transform (SDT).
-
-    Parameters
-    ----------
-    data: ndarray
-        4D Input diffusion volume with shape (X, Y, Z, N)
-    bvals: ndarray
-        1D bvals array with shape (N,)
-    bvecs: ndarray
-        2D (normalized) bvecs array with shape (N, 3)
-    full_frf: ndarray
-        frf data, ex, loaded from a frf_file, with shape (4,).
-    sh_order: int, optional
-        SH order used for the CSD. (Default: 8)
-    nbr_processes: int, optional
-        Number of sub processes to start. Default = none, i.e use the cpu count.
-        If 0, use all processes.
-    mask: ndarray, optional
-        3D mask with shape (X,Y,Z)
-        Binary mask. Only the data inside the mask will be used for
-        computations and reconstruction. Useful if no white matter mask is
-        available.
-    sh_basis: str, optional
-        Spherical harmonics basis used for the SH coefficients.Must be either
-        'descoteaux07' or 'tournier07' (default 'descoteaux07')
-        - 'descoteaux07': SH basis from the Descoteaux et al. MRM 2007 paper
-        - 'tournier07': SH basis from the Tournier et al. NeuroImage 2007 paper.
-    return_sh: bool, optional
-        If true, returns the sh.
-    n_peaks: int, optional
-        Nb of peaks for the fodf. Default: copied dipy's default, i.e. 5.
-    force_b0_threshold: bool, optional
-        If True, will continue even if the minimum bvalue is suspiciously high.
-
-    Returns
-    -------
-    peaks_csd: PeaksAndMetrics
-        An object with ``gfa``, ``peak_directions``, ``peak_values``,
-        ``peak_indices``, ``odf``, ``shm_coeffs`` as attributes
-    """
-
-    # Checking data and sh_order
-    b0_thr = check_b0_threshold(force_b0_threshold, bvals.min(), bvals.min())
-    if data.shape[-1] < (sh_order + 1) * (sh_order + 2) / 2:
-        logging.warning(
-            'We recommend having at least {} unique DWI volumes, but you '
-            'currently have {} volumes. Try lowering the parameter sh_order '
-            'in case of non convergence.'.format(
-                (sh_order + 1) * (sh_order + 2) / 2, data.shape[-1]))
-
-    # Checking bvals, bvecs values and loading gtab
-    if not is_normalized_bvecs(bvecs):
-        logging.warning('Your b-vectors do not seem normalized...')
-        bvecs = normalize_bvecs(bvecs)
-    gtab = gradient_table(bvals, bvecs, b0_threshold=b0_thr)
-
-    # Checking full_frf and separating it
-    if not full_frf.shape[0] == 4:
-        raise ValueError('FRF file did not contain 4 elements. '
-                         'Invalid or deprecated FRF format')
-    frf = full_frf[0:3]
-    mean_b0_val = full_frf[3]
-
-    # Checking if we will use parallel processing
-    parallel = True
-    if nbr_processes is not None:
-        if nbr_processes == 0:  # Will use all processed
-            nbr_processes = None
-        elif nbr_processes == 1:
-            parallel = False
-        elif nbr_processes < 0:
-            raise ValueError('nbr_processes should be positive.')
-
-    # Checking sh basis
-    validate_sh_basis_choice(sh_basis)
-
-    # Loading the spheres
-    reg_sphere = get_sphere('symmetric362')
-    peaks_sphere = get_sphere('symmetric724')
-
-    # Computing CSD
-    csd_model = ConstrainedSphericalDeconvModel(
-        gtab, (frf, mean_b0_val),
-        reg_sphere=reg_sphere,
-        sh_order=sh_order)
-
-    # Computing peaks. Run in parallel, using the default number of processes
-    # (default: CPU count)
-    peaks_csd = peaks_from_model(model=csd_model,
-                                 data=data,
-                                 sphere=peaks_sphere,
-                                 relative_peak_threshold=.5,
-                                 min_separation_angle=25,
-                                 mask=mask,
-                                 return_sh=return_sh,
-                                 sh_basis_type=sh_basis,
-                                 sh_order=sh_order,
-                                 normalize_peaks=True,
-                                 npeaks=n_peaks,
-                                 parallel=parallel,
-                                 nbr_processes=nbr_processes)
+from dipy.io.gradients import read_bvals_bvecs
+import numpy as np
+import nibabel as nib
+
+from scilpy.io.utils import (add_overwrite_arg, assert_inputs_exist,
+                             assert_outputs_exist, add_verbose_arg,
+                             assert_headers_compatible)
+from scilpy.io.image import get_data_as_mask
+from scilpy.reconst.fiber_coherence import compute_coherence_table_for_transforms
+
+
+EPILOG = """
+[1] Schilling KG, Yeh FC, Nath V, Hansen C, Williams O, Resnick S, Anderson AW,
+Landman BA. A fiber coherence index for quality control of B-table orientation
+in diffusion MRI scans. Magn Reson Imaging. 2019 May;58:82-89.
+doi: 10.1016/j.mri.2019.01.018.
+"""
+
+
+def _build_arg_parser():
+    p = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter,
+                                description=__doc__, epilog=EPILOG)
+
+    p.add_argument('in_bvec',
+                   help='Path to bvec file.')
+    p.add_argument('in_peaks',
+                   help='Path to peaks file.')
+    p.add_argument('in_FA',
+                   help='Path to the fractional anisotropy file.')
+    p.add_argument('out_bvec',
+                   help='Path to corrected bvec file (FSL format).')
+
+    p.add_argument('--mask',
+                   help='Path to an optional mask. If set, FA and Peaks will '
+                        'only be used inside the mask.')
+    p.add_argument('--fa_threshold', default=0.2, type=float,
+                   help='FA threshold. Only voxels with FA higher '
+                        'than fa_threshold will be considered. [%(default)s]')
+    p.add_argument('--column_wise', action='store_true',
+                   help='Specify if input peaks are column-wise (..., 3, N) '
+                        'instead of row-wise (..., N, 3).')
+
+    add_verbose_arg(p)
+    add_overwrite_arg(p)
+    return p
+
+
+def main():
+    parser = _build_arg_parser()
+    args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
+
+    assert_inputs_exist(parser, [args.in_bvec, args.in_peaks, args.in_FA],
+                        optional=args.mask)
+    assert_outputs_exist(parser, args, args.out_bvec)
+    assert_headers_compatible(parser, [args.in_peaks, args.in_FA],
+                              optional=args.mask)
+
+    _, bvecs = read_bvals_bvecs(None, args.in_bvec)
+    fa = nib.load(args.in_FA).get_fdata()
+    peaks = nib.load(args.in_peaks).get_fdata()
+
+    if peaks.shape[-1] > 3:
+        logging.info('More than one principal direction per voxel was given.')
+        peaks = peaks[..., 0:3]
+        logging.info('The first peak is assumed to be the biggest.')
+
+    # convert peaks to a volume of shape (H, W, D, N, 3)
+    if args.column_wise:
+        peaks = np.reshape(peaks, peaks.shape[:3] + (3, -1))
+        peaks = np.transpose(peaks, axes=(0, 1, 2, 4, 3))
+    else:
+        peaks = np.reshape(peaks, peaks.shape[:3] + (-1, 3))
+
+    peaks = np.squeeze(peaks)
+    if args.mask:
+        mask = get_data_as_mask(nib.load(args.mask), ref_shape=peaks.shape)
+        fa[np.logical_not(mask)] = 0
+        peaks[np.logical_not(mask)] = 0
+
+    peaks[fa < args.fa_threshold] = 0
+    coherence, transform = compute_coherence_table_for_transforms(peaks, fa)
+
+    best_t = transform[np.argmax(coherence)]
+    if (best_t == np.eye(3)).all():
+        logging.info('b-vectors are already correct.')
+        correct_bvecs = bvecs
+    else:
+        logging.info('Applying correction to b-vectors. '
+                     'Transform is: \n{0}.'.format(best_t))
+        correct_bvecs = np.dot(bvecs, best_t)
+
+    logging.info('Saving bvecs to file: {0}.'.format(args.out_bvec))
+
+    # FSL format (3, N)
+    np.savetxt(args.out_bvec, correct_bvecs.T, '%.8f')
+
 
-    return peaks_csd
+if __name__ == "__main__":
+    main()
```

### Comparing `scilpy-1.5.post2/scilpy/reconst/frf.py` & `scilpy-2.0.0/scilpy/reconst/frf.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,38 +1,43 @@
 # -*- coding: utf-8 -*-
 
 import logging
+from ast import literal_eval
 
 from dipy.core.gradients import gradient_table
 from dipy.reconst.csdeconv import (mask_for_response_ssst,
                                    response_from_mask_ssst)
 from dipy.reconst.mcsd import mask_for_response_msmt, response_from_mask_msmt
 from dipy.segment.mask import applymask
 import numpy as np
 
-from scilpy.utils.bvec_bval_tools import (check_b0_threshold,
-                                          is_normalized_bvecs, normalize_bvecs)
+from scilpy.gradients.bvec_bval_tools import (check_b0_threshold,
+                                              is_normalized_bvecs,
+                                              normalize_bvecs,
+                                              DEFAULT_B0_THRESHOLD)
 
 
-def compute_ssst_frf(data, bvals, bvecs, mask=None, mask_wm=None,
-                     fa_thresh=0.7, min_fa_thresh=0.5, min_nvox=300,
-                     roi_radii=10, roi_center=None, force_b0_threshold=False):
+def compute_ssst_frf(data, bvals, bvecs, b0_threshold=DEFAULT_B0_THRESHOLD,
+                     mask=None, mask_wm=None, fa_thresh=0.7, min_fa_thresh=0.5,
+                     min_nvox=300, roi_radii=10, roi_center=None):
     """Compute a single-shell (under b=1500), single-tissue single Fiber
     Response Function from a DWI volume.
     A DTI fit is made, and voxels containing a single fiber population are
     found using a threshold on the FA.
 
     Parameters
     ----------
     data : ndarray
         4D Input diffusion volume with shape (X, Y, Z, N)
     bvals : ndarray
         1D bvals array with shape (N,)
     bvecs : ndarray
         2D bvecs array with shape (N, 3)
+    b0_threshold: float
+        Value under which bvals are considered b0s.
     mask : ndarray, optional
         3D mask with shape (X,Y,Z)
         Binary mask. Only the data inside the mask will be used for
         computations and reconstruction. Useful if no white matter mask is
         available.
     mask_wm : ndarray, optional
         3D mask with shape (X,Y,Z)
@@ -51,16 +56,14 @@
     roi_radii : int or array-like (3,), optional
         Use those radii to select a cuboid roi to estimate the FRF. The roi
         will be a cuboid spanning from the middle of the volume in each
         direction with the different radii. Defaults to 10.
     roi_center : tuple(3), optional
         Use this center to span the roi of size roi_radius (center of the
         3D volume).
-    force_b0_threshold : bool, optional
-        If set, will continue even if the minimum bvalue is suspiciously high.
 
     Returns
     -------
     full_reponse : ndarray
         Fiber Response Function, with shape (4,)
 
     Raises
@@ -71,20 +74,18 @@
     """
     if min_fa_thresh < 0.4:
         logging.warning(
             "Minimal FA threshold ({:.2f}) seems really small. "
             "Make sure it makes sense for this dataset.".format(min_fa_thresh))
 
     if not is_normalized_bvecs(bvecs):
-        logging.warning("Your b-vectors do not seem normalized...")
+        logging.warning("Your b-vectors do not seem normalized... Normalizing")
         bvecs = normalize_bvecs(bvecs)
 
-    b0_thr = check_b0_threshold(force_b0_threshold, bvals.min(), bvals.min())
-
-    gtab = gradient_table(bvals, bvecs, b0_threshold=b0_thr)
+    gtab = gradient_table(bvals, bvecs, b0_threshold=b0_threshold)
 
     if mask is not None:
         data = applymask(data, mask)
 
     if mask_wm is not None:
         data = applymask(data, mask_wm)
     else:
@@ -103,59 +104,70 @@
         mask = mask_for_response_ssst(gtab, data,
                                       roi_center=roi_center,
                                       roi_radii=roi_radii,
                                       fa_thr=fa_thresh)
         nvox = np.sum(mask)
         response, ratio = response_from_mask_ssst(gtab, data, mask)
 
-        logging.debug(
+        logging.info(
             "Number of indices is {:d} with threshold of {:.2f}".format(
                 nvox, fa_thresh))
         fa_thresh -= 0.05
 
     if nvox < min_nvox:
         raise ValueError(
             "Could not find at least {:d} voxels with sufficient FA "
             "to estimate the FRF!".format(min_nvox))
 
-    logging.debug(
+    logging.info(
         "Found {:d} voxels with FA threshold {:.2f} for "
         "FRF estimation".format(nvox, fa_thresh + 0.05))
-    logging.debug("FRF eigenvalues: {}".format(str(response[0])))
-    logging.debug("Ratio for smallest to largest eigen value "
-                  "is {:.3f}".format(ratio))
-    logging.debug("Mean of the b=0 signal for voxels used "
-                  "for FRF: {}".format(response[1]))
+    logging.info("FRF eigenvalues: {}".format(str(response[0])))
+    logging.info("Ratio for smallest to largest eigen value "
+                 "is {:.3f}".format(ratio))
+    logging.info("Mean of the b=0 signal for voxels used "
+                 "for FRF: {}".format(response[1]))
 
     full_response = np.array([response[0][0], response[0][1],
                               response[0][2], response[1]])
 
     return full_response
 
 
 def compute_msmt_frf(data, bvals, bvecs, btens=None, data_dti=None,
                      bvals_dti=None, bvecs_dti=None, btens_dti=None,
                      mask=None, mask_wm=None, mask_gm=None, mask_csf=None,
                      fa_thr_wm=0.7, fa_thr_gm=0.2, fa_thr_csf=0.1,
                      md_thr_gm=0.0007, md_thr_csf=0.003, min_nvox=300,
-                     roi_radii=10, roi_center=None, tol=20,
-                     force_b0_threshold=False):
+                     roi_radii=10, roi_center=None, tol=20):
     """Compute a multi-shell, multi-tissue single Fiber
     Response Function from a DWI volume.
     A DTI fit is made, and voxels containing a single fiber population are
     found using a threshold on the FA and MD.
 
     Parameters
     ----------
     data : ndarray
         4D Input diffusion volume with shape (X, Y, Z, N)
     bvals : ndarray
         1D bvals array with shape (N,)
     bvecs : ndarray
         2D bvecs array with shape (N, 3)
+    btens: ndarray 1D
+        btens array with shape (N,), describing the btensor shape of every
+        pair of bval/bvec.
+    data_dti: ndarray 4D
+        Input diffusion volume with shape (X, Y, Z, M), where M is the number
+        of DTI directions.
+    bvals_dti: ndarray 1D
+        bvals array with shape (M,)
+    bvecs_dti: ndarray 2D
+        bvals array with shape (M, 3)
+    btens_dti: ndarray 1D
+        bvals array with shape (M,)
     mask : ndarray, optional
         3D mask with shape (X,Y,Z)
         Binary mask. Only the data inside the mask will be used for
         computations and reconstruction.
     mask_wm : ndarray, optional
         3D mask with shape (X,Y,Z)
         Binary white matter mask. Only the data inside this mask will be used
@@ -196,16 +208,14 @@
         will be a cuboid spanning from the middle of the volume in each
         direction with the different radii. Defaults to 10.
     roi_center : tuple(3), optional
         Use this center to span the roi of size roi_radius (center of the
         3D volume).
     tol : int
         tolerance gap for b-values clustering. Defaults to 20
-    force_b0_threshold : bool, optional
-        If set, will continue even if the minimum bvalue is suspiciously high.
 
     Returns
     -------
     reponses : list of ndarray
         Fiber Response Function of each (3) tissue type, with shape (4, N).
     frf_masks : list of ndarray
         Mask where the frf was calculated, for each (3) tissue type, with
@@ -217,17 +227,18 @@
         If less than `min_nvox` voxels were found with sufficient FA to
         estimate the FRF.
     """
     if not is_normalized_bvecs(bvecs):
         logging.warning('Your b-vectors do not seem normalized...')
         bvecs = normalize_bvecs(bvecs)
 
-    b0_thr = check_b0_threshold(force_b0_threshold, bvals.min(), bvals.min())
-
-    gtab = gradient_table(bvals, bvecs, btens=btens, b0_threshold=b0_thr)
+    # Note. Using the tolerance here because currently, the gtab.b0s_mask is
+    # not used. Below, we use the tolerance only (in dipy).
+    # An issue has been added in dipy too.
+    gtab = gradient_table(bvals, bvecs, btens=btens, b0_threshold=tol)
 
     if data_dti is None and bvals_dti is None and bvecs_dti is None:
         logging.warning(
             "No data specific to DTI was given. If b-values go over 1200, "
             "this might produce wrong results.")
         wm_frf_mask, gm_frf_mask, csf_frf_mask \
             = mask_for_response_msmt(gtab, data,
@@ -240,15 +251,14 @@
                                      csf_md_thr=md_thr_csf)
     elif (data_dti is not None and bvals_dti is not None
           and bvecs_dti is not None):
         if not is_normalized_bvecs(bvecs_dti):
             logging.warning('Your b-vectors do not seem normalized...')
             bvecs_dti = normalize_bvecs(bvecs_dti)
 
-        check_b0_threshold(force_b0_threshold, bvals_dti.min())
         gtab_dti = gradient_table(bvals_dti, bvecs_dti, btens=btens_dti)
 
         wm_frf_mask, gm_frf_mask, csf_frf_mask \
             = mask_for_response_msmt(gtab_dti, data_dti,
                                      roi_center=roi_center,
                                      roi_radii=roi_radii,
                                      wm_fa_thr=fa_thr_wm,
@@ -288,7 +298,42 @@
     response_wm, response_gm, response_csf \
         = response_from_mask_msmt(gtab, data, wm_frf_mask, gm_frf_mask,
                                   csf_frf_mask, tol=tol)
 
     responses = [response_wm, response_gm, response_csf]
 
     return responses, frf_masks
+
+
+def replace_frf(old_frf, new_frf, no_factor):
+    """
+    Replace old_frf with new_frf
+
+    Parameters
+    ----------
+    old_frf: np.ndarray
+        A loaded frf file, of shape (n, 4).
+    new_frf: tuple
+        The new frf, to be interpreted with a 10**-4 factor. Ex: (15,4,4)
+    no_factor: bool
+        If true, the fiber response function is evaluated without the
+        10**-4 factor.
+    """
+    old_frf = old_frf.T
+    new_frf = np.array(literal_eval(new_frf), dtype=np.float64)
+
+    if not no_factor:
+        new_frf *= 10 ** -4
+    b0_mean = old_frf[3]
+
+    if new_frf.shape[0] % 3 != 0:
+        raise ValueError('Inputed new frf is not valid. There should be '
+                         'three values per shell, and thus the total number '
+                         'of values should be a multiple of three.')
+
+    nb_shells = int(new_frf.shape[0] / 3)
+    new_frf = new_frf.reshape((nb_shells, 3))
+
+    response = np.empty((nb_shells, 4))
+    response[:, 0:3] = new_frf
+    response[:, 3] = b0_mean
+    return response
```

### Comparing `scilpy-1.5.post2/scilpy/reconst/lobe_metrics_along_streamlines.py` & `scilpy-2.0.0/scilpy/tractanalysis/bingham_metric_along_streamlines.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,73 +1,71 @@
 # -*- coding: utf-8 -*-
 
-from dipy.io.stateful_tractogram import StatefulTractogram
 import numpy as np
 from scilpy.reconst.bingham import bingham_to_peak_direction
 from scilpy.tractanalysis.grid_intersections import grid_intersections
 
 
-def lobe_specific_metric_map_along_streamlines(sft, bingham_coeffs,
-                                               metric, max_theta,
-                                               length_weighting):
+def bingham_metric_map_along_streamlines(sft, bingham_coeffs,
+                                         metric, max_theta,
+                                         length_weighting):
     """
-    Compute mean map for a given lobe-specific metric along streamlines.
+    Compute mean map for a given Bingham metric along streamlines.
 
     Parameters
     ----------
     sft : StatefulTractogram
         StatefulTractogram containing the streamlines needed.
     bingham_coeffs : ndarray
         Array of shape (X, Y, Z, N_LOBES, NB_PARAMS) containing
         the Bingham distributions parameters.
     metric : ndarray
-        Array of shape (X, Y, Z) containing the lobe-specific
-        metric of interest.
+        Array of shape (X, Y, Z) containing the Bingham metric of interest.
     max_theta : float
         Maximum angle in degrees between the fiber direction and the
         Bingham peak direction.
     length_weighting : bool
         If True, will weigh the metric values according to segment lengths.
     """
 
     fd_sum, weights = \
-        lobe_metric_sum_along_streamlines(sft, bingham_coeffs,
-                                          metric, max_theta,
-                                          length_weighting)
+        bingham_metric_sum_along_streamlines(sft, bingham_coeffs,
+                                             metric, max_theta,
+                                             length_weighting)
 
     non_zeros = np.nonzero(fd_sum)
     weights_nz = weights[non_zeros]
     fd_sum[non_zeros] /= weights_nz
 
     return fd_sum
 
 
-def lobe_metric_sum_along_streamlines(sft, bingham_coeffs, metric,
-                                      max_theta, length_weighting):
+def bingham_metric_sum_along_streamlines(sft, bingham_coeffs, metric,
+                                         max_theta, length_weighting):
     """
-    Compute a sum map along a bundle for a given lobe-specific metric.
+    Compute a sum map along a bundle for a given Bingham metric.
 
     Parameters
     ----------
     sft : StatefulTractogram
         StatefulTractogram containing the streamlines needed.
     bingham_coeffs : ndarray (X, Y, Z, N_LOBES, NB_PARAMS)
         Bingham distributions parameters volume.
     metric : ndarray (X, Y, Z)
-        The lobe-specific metric of interest.
+        The Bingham metric of interest.
     max_theta : float
         Maximum angle in degrees between the fiber direction and the
         Bingham peak direction.
     length_weighting : bool
         If True, will weight the metric values according to segment lengths.
 
     Returns
     -------
     metric_sum_map : np.array
-        Lobe-specific metric sum map.
+        Bingham metric sum map.
     weight_map : np.array
         Segment lengths.
     """
 
     sft.to_vox()
     sft.to_corner()
```

### Comparing `scilpy-1.5.post2/scilpy/reconst/multi_processes.py` & `scilpy-2.0.0/scilpy/reconst/sh.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,102 +1,190 @@
+# -*- coding: utf-8 -*-
 import itertools
 import logging
 import multiprocessing
+import numpy as np
 
-from scilpy.direction.peaks import peak_directions_asym
-from scipy.sparse.linalg import ArpackNoConvergence
+from dipy.core.sphere import Sphere
 from dipy.direction.peaks import peak_directions
-from dipy.reconst.multi_voxel import MultiVoxelFit
 from dipy.reconst.odf import gfa
-from dipy.reconst.shm import sh_to_sf_matrix, order_from_ncoef
-from dipy.reconst.mcsd import MSDeconvFit
-import numpy as np
+from dipy.reconst.shm import (sh_to_sf_matrix, order_from_ncoef, sf_to_sh,
+                              sph_harm_ind_list)
 
-from scilpy.reconst.divide_fit import gamma_data2fit
+from scilpy.gradients.bvec_bval_tools import (identify_shells,
+                                              is_normalized_bvecs,
+                                              normalize_bvecs,
+                                              DEFAULT_B0_THRESHOLD)
+from scilpy.dwi.operations import compute_dwi_attenuation
 
-from dipy.utils.optpkg import optional_package
-cvx, have_cvxpy, _ = optional_package("cvxpy")
 
+def verify_data_vs_sh_order(data, sh_order):
+    """
+    Raises a warning if the dwi data shape is not enough for the chosen
+    sh_order.
 
-def fit_from_model_parallel(args):
-    model = args[0]
-    data = args[1]
-    chunk_id = args[2]
+    Parameters
+    ----------
+    data: np.ndarray
+        Diffusion signal as weighted images (4D).
+    sh_order: int
+        SH order to fit, by default 4.
+    """
+    if data.shape[-1] < (sh_order + 1) * (sh_order + 2) / 2:
+        logging.warning(
+            'We recommend having at least {} unique DWIs volumes, but you '
+            'currently have {} volumes. Try lowering the parameter --sh_order '
+            'in case of non convergence.'.format(
+                (sh_order + 1) * (sh_order + 2) / 2, data.shape[-1]))
 
-    sub_fit_array = np.zeros((data.shape[0],), dtype='object')
-    for i in range(data.shape[0]):
-        if data[i].any():
-            try:
-                sub_fit_array[i] = model.fit(data[i])
-            except cvx.error.SolverError:
-                coeff = np.full((len(model.n)), np.NaN)
-                sub_fit_array[i] = MSDeconvFit(model, coeff, None)
 
-    return chunk_id, sub_fit_array
+def compute_sh_coefficients(dwi, gradient_table,
+                            b0_threshold=DEFAULT_B0_THRESHOLD, sh_order=4,
+                            basis_type='descoteaux07', smooth=0.006,
+                            use_attenuation=False, mask=None, sphere=None,
+                            is_legacy=True):
+    """Fit a diffusion signal with spherical harmonics coefficients.
+    Data must come from a single shell acquisition.
 
+    Parameters
+    ----------
+    dwi : nib.Nifti1Image object
+        Diffusion signal as weighted images (4D).
+    gradient_table : GradientTable
+        Dipy object that contains all bvals and bvecs.
+    b0_threshold: float
+        Threshold for the b0 values. Used to validate that the data contains
+        single shell signal.
+    sh_order : int, optional
+        SH order to fit, by default 4.
+    basis_type: str
+        Either 'tournier07' or 'descoteaux07'
+    smooth : float, optional
+        Lambda-regularization coefficient in the SH fit, by default 0.006.
+    use_attenuation: bool, optional
+        If true, we will use DWI attenuation. [False]
+    mask: nib.Nifti1Image object, optional
+        Binary mask. Only data inside the mask will be used for computations
+        and reconstruction.
+    sphere: Sphere
+        Dipy object. If not provided, will use Sphere(xyz=bvecs).
+    is_legacy : bool, optional
+        Whether or not the SH basis is in its legacy form.
 
-def fit_from_model(model, data, mask=None, nbr_processes=None):
-    """Fit the model to data
+    Returns
+    -------
+    sh_coeffs : np.ndarray with shape (X, Y, Z, #coeffs)
+        Spherical harmonics coefficients at every voxel. The actual number
+        of coefficients depends on `sh_order`.
+    """
+
+    # Extracting infos
+    b0_mask = gradient_table.b0s_mask
+    bvecs = gradient_table.bvecs
+    bvals = gradient_table.bvals
+
+    # Checks
+    if not is_normalized_bvecs(bvecs):
+        logging.warning("Your b-vectors do not seem normalized...")
+        bvecs = normalize_bvecs(bvecs)
+
+    # Ensure that this is on a single shell.
+    shell_values, _ = identify_shells(bvals)
+    shell_values.sort()
+    if shell_values.shape[0] != 2 or shell_values[0] > b0_threshold:
+        raise ValueError("Can only work on single shell signals.")
+
+    # Keeping b0-based infos
+    bvecs = bvecs[np.logical_not(b0_mask)]
+    weights = dwi[..., np.logical_not(b0_mask)]
+
+    # Compute attenuation using the b0.
+    if use_attenuation:
+        b0 = dwi[..., b0_mask].mean(axis=3)
+        weights = compute_dwi_attenuation(weights, b0)
+
+    # Get cartesian coords from bvecs
+    if sphere is None:
+        sphere = Sphere(xyz=bvecs)
+
+    # Fit SH
+    sh = sf_to_sh(weights, sphere, sh_order, basis_type, smooth=smooth,
+                  legacy=is_legacy)
+
+    # Apply mask
+    if mask is not None:
+        sh *= mask[..., None]
+
+    return sh
+
+
+def compute_rish(sh, mask=None, full_basis=False):
+    """Compute the RISH (Rotationally Invariant Spherical Harmonics) features
+    of the SH signal [1]. Each RISH feature map is the total energy of its
+    associated order. Mathematically, it is the sum of the squared SH
+    coefficients of the SH order.
 
     Parameters
     ----------
-    model : a model instance
-        `model` will be used to fit the data.
-    data : np.ndarray (4d)
-        Diffusion data.
-    mask : np.ndarray, optional
-        If `mask` is provided, only the data inside the mask will be
-        used for computations.
-    nbr_processes : int, optional
-        The number of subprocesses to use.
-        Default: multiprocessing.cpu_count()
+    sh : np.ndarray object
+        Array of the SH coefficients
+    mask: np.ndarray object, optional
+        Binary mask. Only data inside the mask will be used for computation.
+    full_basis: bool, optional
+        True when coefficients are for a full SH basis.
 
     Returns
     -------
-    fit_array : np.ndarray
-        Array containing the fit
+    rish : np.ndarray with shape (x,y,z,n_orders)
+        The RISH features of the input SH, with one channel per SH order.
+    orders : list(int)
+        The SH order of each RISH feature in the last channel of `rish`.
+
+    References
+    ----------
+    [1] Mirzaalian, Hengameh, et al. "Harmonizing diffusion MRI data across
+        multiple sites and scanners." MICCAI 2015.
+        https://scholar.harvard.edu/files/hengameh/files/miccai2015.pdf
     """
-    data_shape = data.shape
-    if mask is None:
-        mask = np.sum(data, axis=3).astype(bool)
-    else:
-        mask_any = np.sum(data, axis=3).astype(bool)
-        mask *= mask_any
+    # Guess SH order
+    sh_order = order_from_ncoef(sh.shape[-1], full_basis=full_basis)
 
-    nbr_processes = multiprocessing.cpu_count() \
-        if nbr_processes is None or nbr_processes <= 0 \
-        else nbr_processes
+    # Get degree / order for all indices
+    degree_ids, order_ids = sph_harm_ind_list(sh_order, full_basis=full_basis)
 
-    # Ravel the first 3 dimensions while keeping the 4th intact, like a list of
-    # 1D time series voxels. Then separate it in chunks of len(nbr_processes).
-    data = data[mask].reshape((np.count_nonzero(mask), data_shape[3]))
-    chunks = np.array_split(data, nbr_processes)
+    # Apply mask to input
+    if mask is not None:
+        sh = sh * mask[..., None]
 
-    chunk_len = np.cumsum([0] + [len(c) for c in chunks])
-    pool = multiprocessing.Pool(nbr_processes)
-    results = pool.map(fit_from_model_parallel,
-                       zip(itertools.repeat(model),
-                           chunks,
-                           np.arange(len(chunks))))
-    pool.close()
-    pool.join()
+    # Get number of indices per order (e.g. for order 6, sym. : [1,5,9,13])
+    step = 1 if full_basis else 2
+    n_indices_per_order = np.bincount(order_ids)[::step]
 
-    # Re-assemble the chunk together in the original shape.
-    fit_array = np.zeros(data_shape[0:3], dtype='object')
-    tmp_fit_array = np.zeros((np.count_nonzero(mask)), dtype='object')
-    for i, fit in results:
-        tmp_fit_array[chunk_len[i]:chunk_len[i+1]] = fit
+    # Get start index of each order (e.g. for order 6 : [0,1,6,15])
+    order_positions = np.concatenate([[0], np.cumsum(n_indices_per_order)])[:-1]
+
+    # Get paired indices for np.add.reduceat, specifying where to reduce.
+    # The last index is omitted, it is automatically replaced by len(array)-1
+    # (e.g. for order 6 : [0,1, 1,6, 6,15, 15,])
+    reduce_indices = np.repeat(order_positions, 2)[1:]
+
+    # Compute the sum of squared coefficients using numpy's `reduceat`
+    squared_sh = np.square(sh)
+    rish = np.add.reduceat(squared_sh, reduce_indices, axis=-1)[..., ::2]
+
+    # Apply mask
+    if mask is not None:
+        rish *= mask[..., None]
 
-    fit_array[mask] = tmp_fit_array
-    fit_array = MultiVoxelFit(model, fit_array, mask)
+    orders = sorted(np.unique(order_ids))
 
-    return fit_array
+    return rish, orders
 
 
-def peaks_from_sh_parallel(args):
+def _peaks_from_sh_parallel(args):
     shm_coeff = args[0]
     B = args[1]
     sphere = args[2]
     relative_peak_threshold = args[3]
     absolute_threshold = args[4]
     min_separation_angle = args[5]
     npeaks = args[6]
@@ -106,24 +194,23 @@
 
     data_shape = shm_coeff.shape[0]
     peak_dirs = np.zeros((data_shape, npeaks, 3))
     peak_values = np.zeros((data_shape, npeaks))
     peak_indices = np.zeros((data_shape, npeaks), dtype='int')
     peak_indices.fill(-1)
 
-    peak_dir_func = peak_directions if is_symmetric else peak_directions_asym
-
     for idx in range(len(shm_coeff)):
         if shm_coeff[idx].any():
             odf = np.dot(shm_coeff[idx], B)
             odf[odf < absolute_threshold] = 0.
 
-            dirs, peaks, ind = peak_dir_func(odf, sphere,
-                                             relative_peak_threshold,
-                                             min_separation_angle)
+            dirs, peaks, ind = peak_directions(odf, sphere,
+                                               relative_peak_threshold,
+                                               min_separation_angle,
+                                               is_symmetric)
 
             if peaks.shape[0] != 0:
                 n = min(npeaks, peaks.shape[0])
 
                 peak_dirs[idx][:n] = dirs[:n]
                 peak_indices[idx][:n] = ind[:n]
                 peak_values[idx][:n] = peaks[:n]
@@ -134,16 +221,16 @@
 
     return chunk_id, peak_dirs, peak_values, peak_indices
 
 
 def peaks_from_sh(shm_coeff, sphere, mask=None, relative_peak_threshold=0.5,
                   absolute_threshold=0, min_separation_angle=25,
                   normalize_peaks=False, npeaks=5,
-                  sh_basis_type='descoteaux07', nbr_processes=None,
-                  full_basis=False, is_symmetric=True):
+                  sh_basis_type='descoteaux07', is_legacy=True,
+                  nbr_processes=None, full_basis=False, is_symmetric=True):
     """Computes peaks from given spherical harmonic coefficients
 
     Parameters
     ----------
     shm_coeff : np.ndarray
         Spherical harmonic coefficients
     sphere : Sphere
@@ -154,29 +241,34 @@
     relative_peak_threshold : float, optional
         Only return peaks greater than ``relative_peak_threshold * m`` where m
         is the largest peak.
         Default: 0.5
     absolute_threshold : float, optional
         Absolute threshold on fODF amplitude. This value should be set to
         approximately 1.5 to 2 times the maximum fODF amplitude in isotropic
-        voxels (ex. ventricles). `scil_compute_fodf_max_in_ventricles.py`
+        voxels (ex. ventricles). `scil_fodf_max_in_ventricles.py`
         can be used to find the maximal value.
         Default: 0
     min_separation_angle : float in [0, 90], optional
         The minimum distance between directions. If two peaks are too close
         only the larger of the two is returned.
         Default: 25
     normalize_peaks : bool, optional
         If true, all peak values are calculated relative to `max(odf)`.
     npeaks : int, optional
         Maximum number of peaks found (default 5 peaks).
     sh_basis_type : str, optional
         Type of spherical harmonic basis used for `shm_coeff`. Either
         `descoteaux07` or `tournier07`.
         Default: `descoteaux07`
+    is_legacy: bool, optional
+        If true, this means that the input SH used a legacy basis definition
+        for backward compatibility with previous ``tournier07`` and
+        ``descoteaux07`` implementations.
+        Default: True
     nbr_processes: int, optional
         The number of subprocesses to use.
         Default: multiprocessing.cpu_count()
     full_basis: bool, optional
         If True, SH coefficients are expressed using a full basis.
         Default: False
     is_symmetric: bool, optional
@@ -185,15 +277,16 @@
 
     Returns
     -------
     tuple of np.ndarray
         peak_dirs, peak_values, peak_indices
     """
     sh_order = order_from_ncoef(shm_coeff.shape[-1], full_basis)
-    B, _ = sh_to_sf_matrix(sphere, sh_order, sh_basis_type, full_basis)
+    B, _ = sh_to_sf_matrix(sphere, sh_order, sh_basis_type, full_basis,
+                           legacy=is_legacy)
 
     data_shape = shm_coeff.shape
     if mask is None:
         mask = np.sum(shm_coeff, axis=3).astype(bool)
 
     nbr_processes = multiprocessing.cpu_count() if nbr_processes is None \
         or nbr_processes < 0 else nbr_processes
@@ -202,15 +295,15 @@
     # 1D time series voxels. Then separate it in chunks of len(nbr_processes).
     shm_coeff = shm_coeff[mask].reshape(
         (np.count_nonzero(mask), data_shape[3]))
     chunks = np.array_split(shm_coeff, nbr_processes)
     chunk_len = np.cumsum([0] + [len(c) for c in chunks])
 
     pool = multiprocessing.Pool(nbr_processes)
-    results = pool.map(peaks_from_sh_parallel,
+    results = pool.map(_peaks_from_sh_parallel,
                        zip(chunks,
                            itertools.repeat(B),
                            itertools.repeat(sphere),
                            itertools.repeat(relative_peak_threshold),
                            itertools.repeat(absolute_threshold),
                            itertools.repeat(min_separation_angle),
                            itertools.repeat(npeaks),
@@ -238,15 +331,15 @@
     peak_dirs_array[mask] = tmp_peak_dirs_array
     peak_values_array[mask] = tmp_peak_values_array
     peak_indices_array[mask] = tmp_peak_indices_array
 
     return peak_dirs_array, peak_values_array, peak_indices_array
 
 
-def maps_from_sh_parallel(args):
+def _maps_from_sh_parallel(args):
     shm_coeff = args[0]
     _ = args[1]
     peak_values = args[2]
     peak_indices = args[3]
     B = args[4]
     sphere = args[5]
     gfa_thr = args[6]
@@ -345,15 +438,15 @@
     shm_coeff_chunks = np.array_split(shm_coeff, nbr_processes)
     peak_dirs_chunks = np.array_split(peak_dirs, nbr_processes)
     peak_values_chunks = np.array_split(peak_values, nbr_processes)
     peak_indices_chunks = np.array_split(peak_indices, nbr_processes)
     chunk_len = np.cumsum([0] + [len(c) for c in shm_coeff_chunks])
 
     pool = multiprocessing.Pool(nbr_processes)
-    results = pool.map(maps_from_sh_parallel,
+    results = pool.map(_maps_from_sh_parallel,
                        zip(shm_coeff_chunks,
                            peak_dirs_chunks,
                            peak_values_chunks,
                            peak_indices_chunks,
                            itertools.repeat(B),
                            itertools.repeat(sphere),
                            itertools.repeat(gfa_thr),
@@ -408,30 +501,32 @@
             or np.array_equal(np.array([1]), afd_unique):
         logging.warning('All AFD_max values are 1. The peaks seem normalized.')
 
     return(nufo_map_array, afd_max_array, afd_sum_array,
            rgb_map_array, gfa_map_array, qa_map_array)
 
 
-def convert_sh_basis_parallel(args):
+def _convert_sh_basis_parallel(args):
     sh = args[0]
     B_in = args[1]
     invB_out = args[2]
     chunk_id = args[3]
 
     for idx in range(sh.shape[0]):
         if sh[idx].any():
             sf = np.dot(sh[idx], B_in)
             sh[idx] = np.dot(sf, invB_out)
 
     return chunk_id, sh
 
 
 def convert_sh_basis(shm_coeff, sphere, mask=None,
-                     input_basis='descoteaux07', nbr_processes=None):
+                     input_basis='descoteaux07', output_basis='tournier07',
+                     is_input_legacy=True, is_output_legacy=False,
+                     nbr_processes=None):
     """Converts spherical harmonic coefficients between two bases
 
     Parameters
     ----------
     shm_coeff : np.ndarray
         Spherical harmonic coefficients
     sphere : Sphere
@@ -439,48 +534,64 @@
     mask : np.ndarray, optional
         If `mask` is provided, only the data inside the mask will be
         used for computations.
     input_basis : str, optional
         Type of spherical harmonic basis used for `shm_coeff`. Either
         `descoteaux07` or `tournier07`.
         Default: `descoteaux07`
+    output_basis : str, optional
+        Type of spherical harmonic basis wanted as output. Either
+        `descoteaux07` or `tournier07`.
+        Default: `tournier07`
+    is_input_legacy: bool, optional
+        If true, this means that the input SH used a legacy basis definition
+        for backward compatibility with previous ``tournier07`` and
+        ``descoteaux07`` implementations.
+        Default: True
+    is_output_legacy: bool, optional
+        If true, this means that the output SH will use a legacy basis
+        definition for backward compatibility with previous ``tournier07`` and
+        ``descoteaux07`` implementations.
+        Default: False
     nbr_processes: int, optional
         The number of subprocesses to use.
         Default: multiprocessing.cpu_count()
 
     Returns
     -------
     shm_coeff_array : np.ndarray
         Spherical harmonic coefficients in the desired basis.
     """
-    output_basis = 'descoteaux07' \
-        if input_basis == 'tournier07' \
-        else 'tournier07'
+    if input_basis == output_basis and is_input_legacy == is_output_legacy:
+        logging.info('Input and output SH basis are equal, no SH basis '
+                     'convertion needed.')
+        return shm_coeff
 
     sh_order = order_from_ncoef(shm_coeff.shape[-1])
-    B_in, _ = sh_to_sf_matrix(sphere, sh_order, input_basis)
-    _, invB_out = sh_to_sf_matrix(sphere, sh_order, output_basis)
+    B_in, _ = sh_to_sf_matrix(sphere, sh_order, input_basis,
+                              legacy=is_input_legacy)
+    _, invB_out = sh_to_sf_matrix(sphere, sh_order, output_basis,
+                                  legacy=is_output_legacy)
 
     data_shape = shm_coeff.shape
     if mask is None:
         mask = np.sum(shm_coeff, axis=3).astype(bool)
 
     nbr_processes = multiprocessing.cpu_count() \
-        if nbr_processes is None or nbr_processes < 0 \
-        else nbr_processes
+        if nbr_processes is None or nbr_processes < 0 else nbr_processes
 
     # Ravel the first 3 dimensions while keeping the 4th intact, like a list of
     # 1D time series voxels. Then separate it in chunks of len(nbr_processes).
     shm_coeff = shm_coeff[mask].reshape(
         (np.count_nonzero(mask), data_shape[3]))
     shm_coeff_chunks = np.array_split(shm_coeff, nbr_processes)
     chunk_len = np.cumsum([0] + [len(c) for c in shm_coeff_chunks])
 
     pool = multiprocessing.Pool(nbr_processes)
-    results = pool.map(convert_sh_basis_parallel,
+    results = pool.map(_convert_sh_basis_parallel,
                        zip(shm_coeff_chunks,
                            itertools.repeat(B_in),
                            itertools.repeat(invB_out),
                            np.arange(len(shm_coeff_chunks))))
     pool.close()
     pool.join()
 
@@ -491,15 +602,15 @@
         tmp_shm_coeff_array[chunk_len[i]:chunk_len[i+1], :] = new_shm_coeff
 
     shm_coeff_array[mask] = tmp_shm_coeff_array
 
     return shm_coeff_array
 
 
-def convert_sh_to_sf_parallel(args):
+def _convert_sh_to_sf_parallel(args):
     sh = args[0]
     B_in = args[1]
     new_output_dim = args[2]
     chunk_id = args[3]
     sf = np.zeros((sh.shape[0], new_output_dim), dtype=np.float32)
 
     for idx in range(sh.shape[0]):
@@ -507,14 +618,15 @@
             sf[idx] = np.dot(sh[idx], B_in)
 
     return chunk_id, sf
 
 
 def convert_sh_to_sf(shm_coeff, sphere, mask=None, dtype="float32",
                      input_basis='descoteaux07', input_full_basis=False,
+                     is_input_legacy=True,
                      nbr_processes=multiprocessing.cpu_count()):
     """Converts spherical harmonic coefficients to an SF sphere
 
     Parameters
     ----------
     shm_coeff : np.ndarray
         Spherical harmonic coefficients
@@ -526,17 +638,19 @@
     dtype : str
         Datatype to use for computation and output array.
         Either `float32` or `float64`. Default: `float32`
     input_basis : str, optional
         Type of spherical harmonic basis used for `shm_coeff`. Either
         `descoteaux07` or `tournier07`.
         Default: `descoteaux07`
-    input_full_basis : bool
+    input_full_basis : bool, optional
         If True, use a full SH basis (even and odd orders) for the input SH
         coefficients.
+    is_input_legacy : bool, optional
+        Whether the input basis is in its legacy form.
     nbr_processes: int, optional
         The number of subprocesses to use.
         Default: multiprocessing.cpu_count()
 
     Returns
     -------
     shm_coeff_array : np.ndarray
@@ -544,30 +658,31 @@
     """
     assert dtype in ["float32", "float64"], "Only `float32` and `float64` " \
                                             "should be used."
 
     sh_order = order_from_ncoef(shm_coeff.shape[-1],
                                 full_basis=input_full_basis)
     B_in, _ = sh_to_sf_matrix(sphere, sh_order, basis_type=input_basis,
-                              full_basis=input_full_basis)
+                              full_basis=input_full_basis,
+                              legacy=is_input_legacy)
     B_in = B_in.astype(dtype)
 
     data_shape = shm_coeff.shape
     if mask is None:
         mask = np.sum(shm_coeff, axis=3).astype(bool)
 
     # Ravel the first 3 dimensions while keeping the 4th intact, like a list of
     # 1D time series voxels. Then separate it in chunks of len(nbr_processes).
     shm_coeff = shm_coeff[mask].reshape(
         (np.count_nonzero(mask), data_shape[3]))
     shm_coeff_chunks = np.array_split(shm_coeff, nbr_processes)
     chunk_len = np.cumsum([0] + [len(c) for c in shm_coeff_chunks])
 
     pool = multiprocessing.Pool(nbr_processes)
-    results = pool.map(convert_sh_to_sf_parallel,
+    results = pool.map(_convert_sh_to_sf_parallel,
                        zip(shm_coeff_chunks,
                            itertools.repeat(B_in),
                            itertools.repeat(len(sphere.vertices)),
                            np.arange(len(shm_coeff_chunks))))
     pool.close()
     pool.join()
 
@@ -578,102 +693,7 @@
                             dtype=dtype)
     for i, new_sf in results:
         tmp_sf_array[chunk_len[i]:chunk_len[i + 1], :] = new_sf
 
     sf_array[mask] = tmp_sf_array
 
     return sf_array
-
-
-def fit_gamma_parallel(args):
-    data = args[0]
-    gtab_infos = args[1]
-    fit_iters = args[2]
-    random_iters = args[3]
-    do_weight_bvals = args[4]
-    do_weight_pa = args[5]
-    do_multiple_s0 = args[6]
-    chunk_id = args[7]
-
-    sub_fit_array = np.zeros((data.shape[0], 4))
-    for i in range(data.shape[0]):
-        if data[i].any():
-            sub_fit_array[i] = gamma_data2fit(data[i], gtab_infos, fit_iters,
-                                              random_iters, do_weight_bvals,
-                                              do_weight_pa, do_multiple_s0)
-
-    return chunk_id, sub_fit_array
-
-
-def fit_gamma(data, gtab_infos, mask=None, fit_iters=1, random_iters=50,
-              do_weight_bvals=False, do_weight_pa=False, do_multiple_s0=False,
-              nbr_processes=None):
-    """Fit the gamma model to data
-
-    Parameters
-    ----------
-    data : np.ndarray (4d)
-        Diffusion data, powder averaged. Obtained as output of the function
-        `reconst.b_tensor_utils.generate_powder_averaged_data`.
-    gtab_infos : np.ndarray
-        Contains information about the gtab, such as the unique bvals, the
-        encoding types, the number of directions and the acquisition index.
-        Obtained as output of the function
-        `reconst.b_tensor_utils.generate_powder_averaged_data`.
-    mask : np.ndarray, optional
-        If `mask` is provided, only the data inside the mask will be
-        used for computations.
-    fit_iters : int, optional
-        Number of iterations in the gamma fit. Defaults to 1.
-    random_iters : int, optional
-        Number of random sets of parameters tested to find the initial
-        parameters. Defaults to 50.
-    do_weight_bvals : bool , optional
-        If set, does a weighting on the bvalues in the gamma fit.
-    do_weight_pa : bool, optional
-        If set, does a powder averaging weighting in the gamma fit.
-    do_multiple_s0 : bool, optional
-        If set, takes into account multiple baseline signals.
-    nbr_processes : int, optional
-        The number of subprocesses to use.
-        Default: multiprocessing.cpu_count()
-
-    Returns
-    -------
-    fit_array : np.ndarray
-        Array containing the fit
-    """
-    data_shape = data.shape
-    if mask is None:
-        mask = np.sum(data, axis=3).astype(bool)
-
-    nbr_processes = multiprocessing.cpu_count() if nbr_processes is None \
-        or nbr_processes <= 0 else nbr_processes
-
-    # Ravel the first 3 dimensions while keeping the 4th intact, like a list of
-    # 1D time series voxels. Then separate it in chunks of len(nbr_processes).
-    data = data[mask].reshape((np.count_nonzero(mask), data_shape[3]))
-    chunks = np.array_split(data, nbr_processes)
-
-    chunk_len = np.cumsum([0] + [len(c) for c in chunks])
-    pool = multiprocessing.Pool(nbr_processes)
-    results = pool.map(fit_gamma_parallel,
-                       zip(chunks,
-                           itertools.repeat(gtab_infos),
-                           itertools.repeat(fit_iters),
-                           itertools.repeat(random_iters),
-                           itertools.repeat(do_weight_bvals),
-                           itertools.repeat(do_weight_pa),
-                           itertools.repeat(do_multiple_s0),
-                           np.arange(len(chunks))))
-    pool.close()
-    pool.join()
-
-    # Re-assemble the chunk together in the original shape.
-    fit_array = np.zeros((data_shape[0:3])+(4,))
-    tmp_fit_array = np.zeros((np.count_nonzero(mask), 4))
-    for i, fit in results:
-        tmp_fit_array[chunk_len[i]:chunk_len[i+1]] = fit
-
-    fit_array[mask] = tmp_fit_array
-
-    return fit_array
```

### Comparing `scilpy-1.5.post2/scilpy/segment/models.py` & `scilpy-2.0.0/scilpy/segment/models.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,11 +1,9 @@
 # encoding: utf-8
 
-import logging
-
 from dipy.align.bundlemin import distance_matrix_mdf
 from dipy.tracking.streamline import set_number_of_points
 import numpy as np
 
 
 def remove_similar_streamlines(streamlines, threshold=5):
     """ Remove similar streamlines, shuffling streamlines will impact the
```

### Comparing `scilpy-1.5.post2/scilpy/segment/recobundlesx.py` & `scilpy-2.0.0/scilpy/segment/recobundlesx.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,18 +1,21 @@
 # -*- coding: utf-8 -*-
 
-from itertools import chain
 import logging
+from time import time
+import warnings
 
 from dipy.align.streamlinear import (BundleMinDistanceMetric,
                                      StreamlineLinearRegistration)
+from dipy.segment.fss import FastStreamlineSearch
 from dipy.segment.clustering import qbx_and_merge
 from dipy.tracking.distances import bundles_distances_mdf
 from dipy.tracking.streamline import (select_random_set_of_streamlines,
                                       transform_streamlines)
+from nibabel.streamlines.array_sequence import ArraySequence
 import numpy as np
 
 from scilpy.io.streamlines import reconstruct_streamlines_from_memmap
 
 
 class RecobundlesX(object):
     """
@@ -26,105 +29,107 @@
     References
     ----------
     .. [Garyfallidis17] Garyfallidis et al. Recognition of white matter
         bundles using local and global streamline-based registration and
         clustering, Neuroimage, 2017.
     """
 
-    def __init__(self, memmap_filenames, wb_clusters_indices, wb_centroids,
-                 nb_points=20, slr_num_thread=1, rng=None):
+    def __init__(self, memmap_filenames, clusters_indices, wb_centroids,
+                 rng=None):
         """
         Parameters
         ----------
         memmap_filenames : tuple
             tuple of filenames for the data, offsets and lengths.
-        wb_clusters_indices : list
-            List of lists containing the indices of streamlines per cluster.
+        clusters_indices: ArraySequence
+            ArraySequence containing the indices of the streamlines per
+            cluster.
         wb_centroids : list of numpy.ndarray
             List contaning the average streamline per cluster as obtained
             from qbx.
-        nb_points : int
-            Number of points used for all resampling of streamlines.
-        slr_num_thread : int
-            Number of threads for SLR.
-            Should remain 1 for nearly all use-case.
         rng : RandomState
             If None then RandomState is initialized internally.
         """
         self.memmap_filenames = memmap_filenames
-        self.wb_clusters_indices = wb_clusters_indices
+        self.wb_clusters_indices = clusters_indices
         self.centroids = wb_centroids
         self.rng = rng
 
-        # Parameters
-        self.nb_points = nb_points
-        self.slr_num_thread = slr_num_thread
-
         # For declaration outside of init
         self.neighb_centroids = None
-        self.neighb_streamlines = None
         self.neighb_indices = None
+        self.models_streamlines = None
         self.model_centroids = None
         self.final_pruned_indices = None
 
     def recognize(self, model_bundle,
-                  model_clust_thr=8, bundle_pruning_thr=8,
+                  model_clust_thr=8, pruning_thr=8,
                   slr_transform_type='similarity', identifier=None):
         """
         Parameters
         ----------
         model_bundle : list or ArraySequence
             Model bundle as loaded by the nibabel API.
         model_clust_thr : obj
             Distance threshold (mm) for model clustering (QBx)
-        bundle_pruning_thr : int
+        pruning_thr : int
             Distance threshold (mm) for the final pruning.
         slr_transform_type : str
             Define the transformation for the local SLR.
             [translation, rigid, similarity, scaling]
         identifier : str
             Identify the current bundle being recognized for the logging.
 
         Returns
         -------
         clusters : list
             Streamlines that were recognized by Recobundles and these
             parameters.
         """
-        self._cluster_model_bundle(model_bundle, model_clust_thr,
+
+        self.model_streamlines = model_bundle
+        self._cluster_model_bundle(model_clust_thr,
                                    identifier=identifier)
-        if not self._reduce_search_space():
+
+        if not self._reduce_search_space(neighbors_reduction_thr=16):
             if identifier:
                 logging.error('{0} did not find any neighbors in '
                               'the tractogram'.format(identifier))
-            return []
+            return np.array([], dtype=np.uint32)
+
+        self._register_model_to_neighb(slr_transform_type=slr_transform_type)
 
-        if self.slr_num_thread > 0:
-            self._register_model_to_neighb(
-                slr_num_thread=self.slr_num_thread,
-                slr_transform_type=slr_transform_type)
+        # self._reduce_search_space(neighbors_reduction_thr=12)
+        if not self._reduce_search_space(neighbors_reduction_thr=12):
+            if identifier:
+                logging.error('{0} did not find any neighbors in '
+                              'the tractogram'.format(identifier))
+            return np.array([], dtype=np.uint32)
 
-        self.pruned_indices = self.prune_far_from_model(
-            bundle_pruning_thr=bundle_pruning_thr)
+        self.prune_far_from_model(pruning_thr=pruning_thr)
 
-        return self.pruned_indices
+        self.cleanup()
+        return self.get_final_pruned_indices()
 
-    def _cluster_model_bundle(self, model, model_clust_thr, identifier=None):
+    def _cluster_model_bundle(self, model_clust_thr, identifier):
         """
-        Wrapper function to compute QBx for the model and logging informations.
-        :param model, list or arraySequence, streamlines to be used as model.
-        :param model_clust_thr, float, distance in mm for clustering.
-        :param identifier, str, name of the bundle for logging.
+        Wrapper function to compute QBx for the model and logging information.
+
+        Parameters
+        ----------
+        model_clust_thr, float, distance in mm for clustering.
+        identifier, str, name of the bundle for logging.
         """
         thresholds = [30, 20, 15, model_clust_thr]
-        model_cluster_map = qbx_and_merge(model, thresholds,
-                                          nb_pts=self.nb_points,
+        model_cluster_map = qbx_and_merge(self.model_streamlines, thresholds,
+                                          nb_pts=12,
                                           rng=self.rng,
                                           verbose=False)
-        self.model_centroids = model_cluster_map.centroids
+
+        self.model_centroids = ArraySequence(model_cluster_map.centroids)
         len_centroids = len(self.model_centroids)
         if len_centroids > 1000:
             logging.warning('Model {0} simplified at threshold '
                             '{1}mm with {2} centroids'.format(identifier,
                                                               str(model_clust_thr),
                                                               str(len_centroids)))
 
@@ -132,43 +137,37 @@
         """
         Wrapper function to discard clusters from the tractogram too far from
         the model and logging informations.
         :param neighbors_reduction_thr, float, distance in mm for thresholding
             to discard distant streamlines.
         """
         centroid_matrix = bundles_distances_mdf(self.model_centroids,
-                                                self.centroids)
+                                                self.centroids).astype(np.float16)
         centroid_matrix[centroid_matrix >
                         neighbors_reduction_thr] = np.inf
 
         mins = np.min(centroid_matrix, axis=0)
         close_clusters_indices = np.array(np.where(mins != np.inf)[0],
-                                          dtype=np.int32)
+                                          dtype=np.uint32)
 
         self.neighb_indices = []
         for i in close_clusters_indices:
             self.neighb_indices.extend(self.wb_clusters_indices[i])
-        self.neighb_indices = np.array(self.neighb_indices, dtype=np.int32)
-        self.neighb_streamlines = reconstruct_streamlines_from_memmap(
-            self.memmap_filenames, self.neighb_indices)
+        self.neighb_indices = np.array(self.neighb_indices, dtype=np.uint32)
 
         self.neighb_centroids = [self.centroids[i]
                                  for i in close_clusters_indices]
 
-        return len(close_clusters_indices) > 0
+        return self.neighb_indices.size
 
-    def _register_model_to_neighb(self, slr_num_thread=1,
-                                  select_model=1000, select_target=1000,
-                                  slr_transform_type='scaling'):
+    def _register_model_to_neighb(self, select_model=1000, select_target=1000,
+                                  slr_transform_type='similarity'):
         """
         Parameters
         ----------
-        slr_num_thread : int
-            Number of threads for SLR.
-            Should remain 1 for nearly all use-case.
         select_model : int
             Maximum number of clusters to select from the model.
         select_target : int
             Maximum number of clusters to select from the neighborhood.
         slr_transform_type : str
             Define the transformation for the local SLR.
             [translation, rigid, similarity, scaling].
@@ -188,89 +187,98 @@
         # Tuple 0,1,2 are the min & max bound in x,y,z for translation
         # Tuple 3,4,5 are the min & max bound in x,y,z for rotation
         # Tuple 6,7,8 are the min & max bound in x,y,z for scaling
         # For uniform scaling (similarity), tuple #6 is enough
         bounds_dof = [(-20, 20), (-20, 20), (-20, 20),
                       (-10, 10), (-10, 10), (-10, 10),
                       (0.8, 1.2), (0.8, 1.2), (0.8, 1.2)]
-        metric = BundleMinDistanceMetric(num_threads=slr_num_thread)
+        metric = BundleMinDistanceMetric(num_threads=1)
         slr_transform_type_id = possible_slr_transform_type[slr_transform_type]
         if slr_transform_type_id >= 0:
             init_transfo_dof = np.zeros(3)
             slr = StreamlineLinearRegistration(metric=metric, method="L-BFGS-B",
                                                x0=init_transfo_dof,
                                                bounds=bounds_dof[:3],
-                                               num_threads=slr_num_thread)
+                                               num_threads=1)
             slm = slr.optimize(static, moving)
 
         if slr_transform_type_id >= 1:
             init_transfo_dof = np.zeros(6)
             init_transfo_dof[:3] = slm.xopt
 
             slr = StreamlineLinearRegistration(metric=metric,
                                                x0=init_transfo_dof,
                                                bounds=bounds_dof[:6],
-                                               num_threads=slr_num_thread)
+                                               num_threads=1)
             slm = slr.optimize(static, moving)
 
         if slr_transform_type_id >= 2:
             if slr_transform_type_id == 2:
                 init_transfo_dof = np.zeros(7)
                 init_transfo_dof[:6] = slm.xopt
                 init_transfo_dof[6] = 1.
 
                 slr = StreamlineLinearRegistration(metric=metric,
                                                    x0=init_transfo_dof,
                                                    bounds=bounds_dof[:7],
-                                                   num_threads=slr_num_thread)
+                                                   num_threads=1)
                 slm = slr.optimize(static, moving)
 
             else:
                 init_transfo_dof = np.zeros(9)
                 init_transfo_dof[:6] = slm.xopt[:6]
                 init_transfo_dof[6:] = np.array((slm.xopt[6],) * 3)
 
                 slr = StreamlineLinearRegistration(metric=metric,
                                                    x0=init_transfo_dof,
                                                    bounds=bounds_dof[:9],
-                                                   num_threads=slr_num_thread)
+                                                   num_threads=1)
                 slm = slr.optimize(static, moving)
         self.model_centroids = transform_streamlines(self.model_centroids,
                                                      np.linalg.inv(slm.matrix))
 
-    def prune_far_from_model(self, bundle_pruning_thr=10,
-                             neighbors_cluster_thr=8):
+    def prune_far_from_model(self, pruning_thr=10):
         """
         Wrapper function to prune clusters from the tractogram too far from
         the model.
-        :param neighbors_to_prune, list or arraySequence, streamlines to prune.
-        :param bundle_pruning_thr, float, distance in mm for pruning.
-        :param neighbors_cluster_thr, float, distance in mm for clustering.
+
+        Parameters
+        ----------
+        pruning_thr: float, distance in
+            thresholds = [32, 16, 24, neighbors_cluster_thr]
         """
         # Neighbors can be refined since the search space is smaller
-        thresholds = [32, 16, 24, neighbors_cluster_thr]
+        t0 = time()
 
-        neighb_cluster_map = qbx_and_merge(self.neighb_streamlines, thresholds,
-                                           nb_pts=self.nb_points,
-                                           rng=self.rng, verbose=False)
-
-        dist_matrix = bundles_distances_mdf(self.model_centroids,
-                                            neighb_cluster_map.centroids)
-        dist_matrix[np.isnan(dist_matrix)] = np.inf
-        dist_matrix[dist_matrix > bundle_pruning_thr] = np.inf
-        mins = np.min(dist_matrix, axis=0)
-
-        pruned_indices = np.fromiter(chain(
-            *[neighb_cluster_map[i].indices
-              for i in np.where(mins != np.inf)[0]]),
-            dtype=np.int32)
+        neighb_streamlines = reconstruct_streamlines_from_memmap(
+            self.memmap_filenames, self.neighb_indices, strs_dtype=np.float16)
+
+        with warnings.catch_warnings(record=True) as _:
+            fss = FastStreamlineSearch(neighb_streamlines,
+                                       pruning_thr, resampling=12)
+            dist_mat = fss.radius_search(self.model_streamlines,
+                                         pruning_thr)
+
+        logging.debug("Fast search took of dimensions {0}: {1} sec.".format(
+            dist_mat.shape, np.round(time() - t0, 2)))
+
+        sparse_dist_mat = np.abs(dist_mat.tocsr())
+        sparse_dist_vec = np.squeeze(np.max(sparse_dist_mat, axis=0).toarray())
+        pruned_indices = np.where(sparse_dist_vec > 1e-6)[0]
 
         # Since the neighbors were clustered, a mapping of indices is neccesary
-        self.final_pruned_indices = self.neighb_indices[pruned_indices]
+        self.final_pruned_indices = self.neighb_indices[pruned_indices].astype(
+            np.uint32)
 
         return self.final_pruned_indices
 
     def get_final_pruned_indices(self):
         """
         Public getter for the final indices recognize by the algorithm.
         """
         return self.final_pruned_indices
+
+    def cleanup(self):
+        del self.neighb_centroids
+        del self.neighb_indices
+        del self.model_streamlines
+        del self.model_centroids
```

### Comparing `scilpy-1.5.post2/scilpy/segment/streamlines.py` & `scilpy-2.0.0/scilpy/segment/streamlines.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,15 +3,16 @@
 import itertools
 
 from dipy.io.stateful_tractogram import StatefulTractogram
 from dipy.tracking.metrics import length
 from dipy.tracking.streamline import set_number_of_points
 from dipy.tracking.vox2track import _streamlines_in_mask
 from nibabel.affines import apply_affine
-from scipy.ndimage import map_coordinates
+from scipy.ndimage import (map_coordinates, generate_binary_structure,
+                           binary_dilation)
 
 import numpy as np
 
 from scilpy.tractanalysis.streamlines_metrics import compute_tract_counts_map
 
 
 def streamlines_in_mask(sft, target_mask, all_in=False):
@@ -100,33 +101,39 @@
         streamlines, sft,
         data_per_streamline=data_per_streamline,
         data_per_point=data_per_point)
 
     return new_sft, line_based_indices
 
 
-def filter_grid_roi(sft, mask, filter_type, is_exclude):
+def filter_grid_roi(sft, mask, filter_type, is_exclude, filter_distance=0):
     """
     Parameters
     ----------
     sft : StatefulTractogram
         StatefulTractogram containing the streamlines to segment.
     mask : numpy.ndarray
         Binary mask in which the streamlines should pass.
     filter_type: str
-        One of the 3 following choices, 'any', 'all', 'either_end', 'both_ends'.
+        One of the 4 following choices, 'any', 'all', 'either_end', 'both_ends'.
     is_exclude: bool
         Value to indicate if the ROI is an AND (false) or a NOT (true).
     Returns
     -------
     new_sft: StatefulTractogram
         Filtered sft.
     ids: list
         Ids of the streamlines passing through the mask.
     """
+
+    if filter_distance != 0:
+        bin_struct = generate_binary_structure(3, 2)
+        mask = binary_dilation(mask, bin_struct,
+                               iterations=filter_distance)
+
     line_based_indices = []
     if filter_type in ['any', 'all']:
         line_based_indices = streamlines_in_mask(sft, mask,
                                                  all_in=filter_type == 'all')
     else:
         sft.to_vox()
         sft.to_corner()
```

### Comparing `scilpy-1.5.post2/scilpy/segment/tractogram_from_roi.py` & `scilpy-2.0.0/scilpy/segment/tractogram_from_roi.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,25 +4,28 @@
 import logging
 
 import nibabel as nib
 import numpy as np
 import os
 from scipy.ndimage import binary_dilation
 
-from dipy.io.stateful_tractogram import StatefulTractogram
 from dipy.io.streamline import save_tractogram
 from dipy.tracking.utils import length as compute_length
 
 from scilpy.io.image import get_data_as_mask
 from scilpy.io.streamlines import load_tractogram_with_reference
 from scilpy.segment.streamlines import filter_grid_roi, filter_grid_roi_both
-from scilpy.tracking.tools import filter_streamlines_by_total_length_per_dim
-from scilpy.tractanalysis.features import remove_loops_and_sharp_turns
-from scilpy.tractanalysis.tools import split_heads_tails_kmeans
+from scilpy.tractograms.streamline_operations import \
+    remove_loops_and_sharp_turns
 from scilpy.tractanalysis.streamlines_metrics import compute_tract_counts_map
+
+from scilpy.tractograms.streamline_and_mask_operations import \
+    split_mask_blobs_kmeans
+from scilpy.tractograms.streamline_operations import \
+    filter_streamlines_by_total_length_per_dim
 from scilpy.utils.filenames import split_name_with_nii
 
 
 def _extract_prefix(filename):
     prefix = os.path.basename(filename)
     prefix, _ = split_name_with_nii(prefix)
 
@@ -119,15 +122,15 @@
         Dimensions of the mask image.
     """
     mask_img = nib.load(gt_endpoints)
     mask = get_data_as_mask(mask_img)
     affine = mask_img.affine
     dimensions = mask.shape
 
-    head, tail = split_heads_tails_kmeans(mask)
+    head, tail = split_mask_blobs_kmeans(mask, nb_clusters=2)
 
     basename = os.path.basename(split_name_with_nii(gt_endpoints)[0])
     tail_filename = os.path.join(out_dir, '{}_tail.nii.gz'.format(basename))
     head_filename = os.path.join(out_dir, '{}_head.nii.gz'.format(basename))
     nib.save(nib.Nifti1Image(head.astype(mask.dtype), affine), head_filename)
     nib.save(nib.Nifti1Image(tail.astype(mask.dtype), affine), tail_filename)
 
@@ -564,22 +567,22 @@
 
 def segment_tractogram_from_roi(
         sft, gt_tails, gt_heads, bundle_names, bundle_lengths, angles,
         orientation_lengths, abs_orientation_lengths, inv_all_masks, any_masks,
         list_rois, args):
     """
     Segments valid bundles (VB). Based on args:
-        - args.compute_ic: computes invalid bundles (IB)
-        - args.save_wpc_separately: compute WPC
+    - args.compute_ic: computes invalid bundles (IB)
+    - args.save_wpc_separately: compute WPC
 
     Returns
     -------
     vb_sft_list: list
         The list of valid bundles discovered. These files are also saved
-        in segmented_VB/*_VS.trk.
+        in segmented_VB/\\*_VS.trk.
     wpc_sft_list: list
         The list of wrong path connections: streamlines connecting the right
         endpoint regions but not included in the ALL mask.
         ** This is only computed if args.save_wpc_separately. Else, this is
         None.
     ib_sft_list: list
         The list of invalid bundles: streamlines connecting regions that should
@@ -619,15 +622,15 @@
         # false connections
         vb_roi_filenames = list(zip(gt_tails, gt_heads))
         for vb_roi_pair in vb_roi_filenames:
             vb_roi_pair = tuple(sorted(vb_roi_pair))
             comb_filename.remove(vb_roi_pair)
         ib_sft_list, ic_ids_list, ib_names = _extract_ib_all_bundles(
             comb_filename, sft[remaining_ids], args)
-        if args.unique:
+        if args.unique and len(ic_ids_list) > 0:
             for i in range(len(ic_ids_list)):
                 # Assign actual ids
                 ic_ids_list[i] = remaining_ids[ic_ids_list[i]]
             detected_vs_wpc_ids = np.concatenate(ic_ids_list)
             remaining_ids = np.setdiff1d(remaining_ids, detected_vs_wpc_ids)
     else:
         ic_ids_list = []
```

### Comparing `scilpy-1.5.post2/scilpy/stats/stats.py` & `scilpy-2.0.0/scilpy/stats/stats.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,68 +1,71 @@
 # -*- coding: utf-8 -*-
 
 import logging
 
 from itertools import combinations
+
 import scipy.stats
 
 
 def verify_normality(data, alpha=0.05):
     """
     Parameters
     ----------
-    data : array_like
+    data: array_like
         Array of sample data to test normality on.
         Should be of 1 dimension.
-    alpha : float
+    alpha: float
         Type 1 error of the normality test.
         Probability of false positive or rejecting null hypothesis
         when it is true.
+
     Returns
     -------
-    normality : bool
+    normality: bool
         Whether or not the sample can be considered normal
-    p_value : float
+    p_value: float
         Probability to obtain an effect at least as extreme as the one
         in the current sample, assuming the null hypothesis.
         We reject the null hypothesis when this value is lower than alpha.
     """
 
     normality = True
     # First, we verify if sample pass Shapiro-Wilk test
     W, p_value = scipy.stats.shapiro(data)
     if p_value < alpha and len(data) < 30:
-        logging.debug('The data sample can not be considered normal')
+        logging.info('The data sample can not be considered normal')
         normality = False
     else:
-        logging.debug('The data sample pass the normality assumption.')
+        logging.info('The data sample pass the normality assumption.')
         normality = True
     return normality, p_value
 
 
 def verify_homoscedasticity(data_by_group, normality=False, alpha=0.05):
     """
     Parameters
     ----------
-    data_by_group : list of array_like
+    data_by_group: list of array_like
         The sample data separated by groups.
         Possibly of different group size.
-    normality : bool
+    normality: bool
         Whether or not the sample data of each groups can be considered normal
-    alpha : float
+    alpha: float
         Type 1 error of the equality of variance test
         Probability of false positive or rejecting null hypothesis
         when it is true.
+
     Returns
     -------
-    test : string
+    test: string
         Name of the test done to verify homoscedasticity
-    homoscedasticity : bool
+    homoscedasticity: bool
         Whether or not the equality of variance across groups can be assumed
-    p_value : float
+    p_value: float
         Probability to obtain an effect at least as extreme as the one
         in the current sample, assuming the null hypothesis.
         We reject the null hypothesis when this value is lower than alpha.
     """
     total_nb = 0
     for group in data_by_group:
         nb_current_group = len(group)
@@ -71,49 +74,49 @@
 
     if normality:
         test = 'Bartlett'
         W, p_value = scipy.stats.bartlett(*data_by_group)
     else:
         test = 'Levene'
         W, p_value = scipy.stats.levene(*data_by_group)
-    logging.debug('Test name: {}'.format(test))
+    logging.info('Test name: {}'.format(test))
     if p_value < alpha and mean_nb < 30:
-        logging.debug('The sample didnt pass the equal variance assumption')
+        logging.info('The sample didnt pass the equal variance assumption')
         homoscedasticity = False
     else:
-        logging.debug('The sample pass the equal variance assumption')
+        logging.info('The sample pass the equal variance assumption')
         homoscedasticity = True
 
     return test, homoscedasticity, p_value
 
 
 def verify_group_difference(data_by_group, normality=False,
                             homoscedasticity=False, alpha=0.05):
     """
     Parameters
     ----------
-    data_by_group : list of array_like
+    data_by_group: list of array_like
         The sample data separated by groups.
         Possibly of different group size.
-    normality : bool
+    normality: bool
         Whether or not the sample data of each groups can be considered normal.
-    homoscedasticity : bool
+    homoscedasticity: bool
         Whether or not the equality of variance across groups can be assumed.
-    alpha : float
+    alpha: float
         Type 1 error of the equality of variance test.
         Probability of false positive or rejecting null hypothesis
         when it is true.
     Returns
     -------
-    test : string
+    test: string
         Name of the test done to verify group difference.
-    difference : bool
+    difference: bool
         Whether or not the variable associated for groups has an effect on
         the current measurement.
-    p_value : float
+    p_value: float
         Probability to obtain an effect at least as extreme as the one
         in the current sample, assuming the null hypothesis.
         We reject the null hypothesis when this value is lower than alpha.
     """
 
     if len(data_by_group) == 2:
         if normality and homoscedasticity:
@@ -140,63 +143,63 @@
         if normality and homoscedasticity:
             test = 'ANOVA'
             T, p_value = scipy.stats.f_oneway(*data_by_group)
         else:
             test = 'Kruskalwallis'
             T, p_value = scipy.stats.kruskal(*data_by_group)
 
-    logging.debug('Test name: {}'.format(test))
+    logging.info('Test name: {}'.format(test))
     if p_value < alpha:
-        logging.debug('There is a difference between groups')
+        logging.info('There is a difference between groups')
         difference = True
     else:
-        logging.debug('We are not able to detect difference between the groups.')
+        logging.info('We are not able to detect difference between'
+                     ' the groups.')
         difference = False
 
     return test, difference, p_value
 
 
 def verify_post_hoc(data_by_group, groups_list, test,
                     correction=True, alpha=0.05):
     """
     Parameters
     ----------
-    data_by_group : list of array_like
+    data_by_group: list of array_like
         The sample data separated by groups.
         Possibly of different lengths group size.
-    groups_list : list of string
+    groups_list: list of string
         The names of each group in the same order as data_by_group.
-    test : string
+    test: string
         The name of the post-hoc analysis test to do.
         Post-hoc analysis is the analysis of pairwise difference a posteriori
         of the fact that there is a difference across groups.
-    correction : bool
+    correction: bool
         Whether or not to do a Bonferroni correction on the alpha threshold.
         Used to have a more stable type 1 error across multiple comparison.
-    alpha : float
+    alpha: float
         Type 1 error of the equality of variance test.
         Probability of false positive or rejecting null hypothesis
         when it is true.
+
     Returns
     -------
-    differences : list of (string, string, bool)
+    differences: list of (string, string, bool)
         The result of the post-hoc for every groups pairwise combinations.
-        1st, 2nd dimension :
-            Names of the groups chosen
-        3rd :
-            Whether or not we detect a pairwise difference on the current
-            measurement.
-        4th :
-            P-value of the pairwise difference test.
-    test : string
+
+        - 1st, 2nd dimension: Names of the groups chosen.
+        - 3rd: Whether or not we detect a pairwise difference on the current
+          measurement.
+        - 4th: P-value of the pairwise difference test.
+    test: string
         Name of the test done to verify group difference
     """
-    logging.debug('We need to do a post-hoc analysis since '
-                  'there is a difference')
-    logging.debug('Post-hoc: {} pairwise'.format(test))
+    logging.info('We need to do a post-hoc analysis since '
+                 'there is a difference')
+    logging.info('Post-hoc: {} pairwise'.format(test))
     differences = []
     nb_group = len(groups_list)
 
     # Bonferroni correction
     if correction:
         nb_combinaison = (nb_group * (nb_group - 1)) / 2
         alpha = alpha / nb_combinaison
@@ -209,11 +212,11 @@
             T, p_value = scipy.stats.mannwhitneyu(
                 data_by_group[x], data_by_group[y])
         elif test == 'Wilcoxon':
             T, p_value = scipy.stats.wilcoxon(
                 data_by_group[x], data_by_group[y])
         differences.append((groups_list[x], groups_list[y],
                             p_value < alpha, p_value))
-    logging.debug('Result:')
-    logging.debug(differences)
+    logging.info('Result:')
+    logging.info(differences)
 
     return test, differences
```

### Comparing `scilpy-1.5.post2/scilpy/stats/utils.py` & `scilpy-2.0.0/scilpy/stats/utils.py`

 * *Files 3% similar despite different names*

```diff
@@ -47,61 +47,63 @@
                     {'bundles': json_info[participant['participant_id']]}})
 
             for variable in participant:
                 if variable != 'participant_id':
                     self.data_dictionnary[participant['participant_id']]\
                         [variable] = participant[variable]
 
-        logging.debug('Data_dictionnary')
-        logging.debug(self.data_dictionnary[self.get_first_participant()])
+        logging.info('Data_dictionnary')
+        logging.info(self.data_dictionnary[self.get_first_participant()])
 
         with open('data.json', 'w') as fp:
             json.dump(self.data_dictionnary, fp, indent=4)
 
     def validation_participant_id(self, json_info, participants_info):
         """
         Verify if the json and tsv file has the same participants id
         """
         # Create the list of participants id from the json dictionnary
 
         participants_from_json = list(json_info.keys())
-        logging.debug('participant list from json dictionnary:')
-        logging.debug(participants_from_json)
+        logging.info('participant list from json dictionnary:')
+        logging.info(participants_from_json)
 
         # Create the list of participants id from the tsv list of dictionnary
         participants_from_tsv = []
         for participant in participants_info:
             participants_from_tsv.append(participant['participant_id'])
-        logging.debug('participant list from tsv file:')
-        logging.debug(participants_from_tsv)
+        logging.info('participant list from tsv file:')
+        logging.info(participants_from_tsv)
 
         # Compare the two list
         participants_from_json.sort()
         participants_from_tsv.sort()
 
         if not participants_from_json == participants_from_tsv:
             if not len(participants_from_json) == len(participants_from_tsv):
-                logging.debug('The number of participants from json file is not the same '
-                              'as the one in the tsv file.')
+                logging.info('The number of participants from json file is '
+                             'not the same as the one in the tsv file.')
             is_in_tsv = np.in1d(participants_from_json, participants_from_tsv)
             is_in_json = np.in1d(participants_from_tsv, participants_from_json)
 
-            logging.debug('participants list from json file missing in tsv file :')
-            logging.debug(np.asarray(participants_from_json)[~is_in_tsv])
-            logging.debug('participants list from tsv file missing in json file :')
-            logging.debug(np.asarray(participants_from_tsv)[~is_in_json])
+            logging.info('participants list from json file missing in tsv '
+                         'file :')
+            logging.info(np.asarray(participants_from_json)[~is_in_tsv])
+            logging.info('participants list from tsv file missing in json '
+                         'file :')
+            logging.info(np.asarray(participants_from_tsv)[~is_in_json])
 
             logging.error('The subjects from the json file does not fit '
                           'with the subjects of the tsv file. '
                           'Impossible to build the data_for_stat object')
             raise BaseException('The subjects from the json file does not fit '
                                 'with the subjects of the tsv file. '
                                 'Impossible to build the data_for_stat object')
         else:
-            logging.debug('The json and the tsv are compatible')
+            logging.info('The json and the tsv are compatible')
 
     def get_participants_list(self):
         # Construct the list of participant_id from the data_dictionnary
 
         return list(self.data_dictionnary.keys())
 
     def get_first_participant(self):
@@ -154,21 +156,21 @@
                                          ['bundles']
                                          [first_bundle][first_metric].keys())
 
     def get_groups_dictionnary(self, group_by):
         """
         Parameters
         ----------
-        groups_by : string
+        groups_by: string
             The attribute with which we generate our groups.
         Returns
         -------
-        group_dict : dictionnary of groups
-            keys : group id generated by group_by.
-            values : dictionnary of participants of that specific group.
+        group_dict: dictionnary of groups
+            keys: group id generated by group_by.
+            values: dictionnary of participants of that specific group.
         """
         group_dict = {}
         # Verify if the group_by exist
         if group_by not in self.get_participant_attributes_list():
             logging.error('Participants doesn\'t contain the attribute '
                           '{} necessary to generate groups.'.format(group_by))
             raise BaseException('Object data_for_stat has no attirbute '
@@ -187,37 +189,37 @@
 
         return group_dict
 
     def get_groups_list(self, group_by):
         """
         Parameters
         ----------
-        groups_by : string
+        groups_by: string
             The attribute with which we generate our groups.
         Returns
         -------
-        group_list : list of string
+        group_list: list of string
             list of group id generated by group_by variable.
         """
         # Generated the list of group generated by group_by variable
         return list(self.get_groups_dictionnary(group_by).keys())
 
     def get_data_sample(self, bundle, metric, value):
         """
         Parameters
         ----------
-        bundle : string
+        bundle: string
             The specific bundle with which we generate our sample.
-        metric : string
+        metric: string
             The specific metric with which we generate our sample.
-        value : string
+        value: string
             The specific value with which we generate our sample.
         Returns
         -------
-        data_sample : array of float
+        data_sample: array of float
             The sample array associate with the parameters.
         """
         data_sample = []
         for participant in self.data_dictionnary:
             data_sample.append(self.data_dictionnary[participant]
                                                     ['bundles']
                                                     [bundle]
@@ -225,28 +227,28 @@
         return data_sample
 
 
 def get_group_data_sample(group_dict, group_id, bundle, metric, value):
     """
     Parameters
     ----------
-    group_dict : dictionnary of groups
-        keys : group id generated by group_by.
-        values : dictionnary of participants of that specific group.
-    group_id : string
+    group_dict: dictionnary of groups
+        keys: group id generated by group_by.
+        values: dictionnary of participants of that specific group.
+    group_id: string
         The name of the group with which we generate our sample.
-    bundle : string
+    bundle: string
         The specific bundle with which we generate our sample.
-    metric : string
+    metric: string
         The specific metric with which we generate our sample.
-    value : string
+    value: string
         The specific value with which we generate our sample.
     Returns
     -------
-    data_sample : array of float
+    data_sample: array of float
         The sample array associate with the parameters.
     """
     sample_size = len(group_dict[group_id].keys())
     data_sample = np.zeros(sample_size)
     for index, participant in enumerate(group_dict[group_id].keys()):
         if bundle in group_dict[group_id][participant]['bundles']:
             # Assure the participants has the bundle in the database
@@ -264,45 +266,49 @@
 
 
 def write_current_dictionnary(metric, normality, variance_equality,
                               diff_result, diff_2_by_2):
     """
     Parameters
     ----------
-    metric : string
-        The name of the metricment in which the group comparison was made on.
-    normality : dictionnary of groups
-        keys : group id
-        values : (result, p-value)
-    variance_equality : (string, bool)
+    metric: string
+        The name of the metric in which the group comparison was made on.
+    normality: dictionnary of groups
+        keys: group id
+        values: (result, p-value)
+    variance_equality: (string, bool)
         The result of the equality of variance test.
-        1st dimension :
+
+        - 1st dimension:
             Name of the equal variance test done.
-        2nd dimension :
+        - 2nd dimension:
             Whether or not it equality of variance can be assumed.
-    diff_result : (string, bool, float)
-        The result of the groups difference analysis on the metricment.
-        1st dimension :
+    diff_result: (string, bool, float)
+        The result of the groups difference analysis on the metric.
+
+        - 1st dimension:
             Name of the test done.
-        2nd dimension :
-            Whether or not we detect a group difference on the metricment.
-        3rd dimension :
+        - 2nd dimension:
+            Whether or not we detect a group difference on the metric.
+        - 3rd dimension:
             p-value result
-    diff_2_by_2 : (list of (string, string, bool, float), string)
+    diff_2_by_2: (list of (string, string, bool, float), string)
         The result of the pairwise groups difference a posteriori analysis.
-        1st dimension :
+
+        - 1st dimension:
             Name of the test done.
-        2nd dimension :
+        - 2nd dimension:
             The result of every pairwise combinations of the groups
             (name of first group, name of second group, result, p-value).
+
     Returns
     -------
-    curr_dict : dictionnary of test
-        keys : The category of test done (Normality, Homoscedascticity,...)
-        values : The result of those test.
+    curr_dict: dictionnary of test
+    keys: The category of test done (Normality, Homoscedascticity,...)
+    values: The result of those test.
     """
     # First, we create the structure
     curr_dict = {
             'Normality': {'Test': 'Shapiro-Wilk',
                           'P-value': {}},
             'Homoscedasticity': {'Test': variance_equality[0]},
             'Group difference': {'Test': diff_result[0]}
@@ -421,30 +427,30 @@
 
 
 def visualise_distribution(data_by_group, participants_id, bundle, metric,
                            value, oFolder, groups_list):
     """
     Parameters
     ----------
-    data_by_group : list of array_like
+    data_by_group: list of array_like
         The sample data separated by groups.
         Possibly of different lengths per group.
-    participants_id : list of string
+    participants_id: list of string
         Names of the participants id "name".
-    metric : string
+    metric: string
         The name of the metricment in which you want to look at the
         across groups.
-    oFolder : path-like object
+    oFolder: path-like object
         Emplacement in which we want to save the graph of the distribution
         the measurement across groups.
-    groups_list : list of string
+    groups_list: list of string
         The names of each group.
     Returns
     -------
-    outliers : list of (string, string)
+    outliers: list of (string, string)
         The list of participants that is considered outlier for their group
         (participant_id, group_id).
     """
     nb_group = len(data_by_group)
     outliers = []
     fig, ax = plt.subplots()
     ls = np.asarray(data_by_group, dtype=object)
@@ -488,10 +494,10 @@
             os.makedirs(os.path.dirname(save_path))
         except OSError as exc:
             if exc.errno != exc.EEXIST:
                 raise
 
     fig.savefig(os.path.join(oFolder, 'Graph', bundle, metric))
 
-    logging.debug('outliers:[(id, group)]')
-    logging.debug(outliers)
+    logging.info('outliers:[(id, group)]')
+    logging.info(outliers)
     return outliers
```

### Comparing `scilpy-1.5.post2/scilpy/tracking/propagator.py` & `scilpy-2.0.0/scilpy/tracking/propagator.py`

 * *Files 4% similar despite different names*

```diff
@@ -6,16 +6,15 @@
 
 import dipy
 from dipy.io.stateful_tractogram import Space, Origin
 from dipy.reconst.shm import sh_to_sf_matrix
 
 from scilpy.reconst.utils import (get_sphere_neighbours,
                                   get_sh_order_and_fullness)
-from scilpy.tracking.tools import sample_distribution
-from scilpy.tracking.utils import TrackingDirection
+from scilpy.tracking.utils import sample_distribution, TrackingDirection
 
 
 class PropagationStatus(Enum):
     ERROR = 1
 
 
 class AbstractPropagator(object):
@@ -25,19 +24,19 @@
     the next direction at current step through Runge-Kutta integration
     (whereas the tracker using this propagator will be responsible for the
     processing parameters, number of streamlines, stopping criteria, etc.).
 
     Propagation depends on the type of data (ex, DTI, fODF) and the way to get
     a direction from it (ex, det, prob).
     """
-    def __init__(self, dataset, step_size, rk_order,  space, origin):
+    def __init__(self, datavolume, step_size, rk_order, space, origin):
         """
         Parameters
         ----------
-        dataset: scilpy.image.datasets.DataVolume
+        datavolume: scilpy.image.volume_space_management.DataVolume
             Trackable Dataset object.
         step_size: float
             The step size for tracking. Important: step size should be in the
             same units as the space of the tracking!
         rk_order: int
             Order for the Runge Kutta integration.
         space: dipy Space
@@ -48,62 +47,67 @@
             in the propagator's methods will be expected to respect that origin.
 
         A note on space and origin: All coordinates received in the
         propagator's methods will be expected to respect those values.
         Tracker will verify that the propagator has the same internal values as
         itself.
         """
-        self.dataset = dataset
+        self.datavolume = datavolume
 
         self.origin = origin
         self.space = space
 
         # Propagation options
         self.step_size = step_size
         if not (rk_order == 1 or rk_order == 2 or rk_order == 4):
             raise ValueError("Invalid runge-kutta order. Is " +
                              str(rk_order) + ". Choices : 1, 2, 4")
         self.rk_order = rk_order
 
         # By default, normalizing directions. Adding option for child classes.
         self.normalize_directions = True
 
+        self.line_rng_generator = None   # Will be reset at each new streamline.
+
     def reset_data(self, new_data=None):
         """
         Reset data before starting a new process. In current implementation,
         we reset the internal data to None before starting a multiprocess, then
         load it back when process has started.
 
-        Params
-        ------
+        Parameters
+        ----------
         new_data: Any
-            Will replace self.dataset.data.
+            Will replace self.datavolume.data.
 
         """
-        self.dataset.data = new_data
+        self.datavolume.data = new_data
 
-    def prepare_forward(self, seeding_pos):
+    def prepare_forward(self, seeding_pos, random_generator):
         """
         Prepare information necessary at the first point of the
         streamline for forward propagation: v_in and any other information
         necessary for the self.propagate method.
 
         Parameters
         ----------
         seeding_pos: tuple(x,y,z)
             The seeding position. Important, position must be in the same space
             and origin as self.space, self.origin!
+        random_generator: numpy Generator.
 
         Returns
         -------
         tracking_info: Any
             Any tracking information necessary for the propagation.
             Return PropagationStatus.ERROR if no good tracking direction can be
             set at current seeding position.
         """
+        # To be defined by child classes.
+        # Should set self.line_rng_generator = random_generator
         raise NotImplementedError
 
     def prepare_backward(self, line, forward_dir):
         """
         Called at the beginning of backward tracking, in case we need to
         reset some parameters
 
@@ -246,34 +250,34 @@
             A valid tracking direction. None if no valid direction is found.
             Direction should be normalized.
         """
         raise NotImplementedError
 
 
 class PropagatorOnSphere(AbstractPropagator):
-    def __init__(self, dataset, step_size, rk_order, dipy_sphere,
+    def __init__(self, datavolume, step_size, rk_order, dipy_sphere,
                  space, origin):
         """
         Parameters
         ----------
-        dataset: scilpy.image.datasets.DataVolume
-            Trackable Dataset object.
+        datavolume: scilpy.image.volume_space_management.DataVolume
+            Trackable DataVolume object.
         step_size: float
             The step size for tracking.
         rk_order: int
             Order for the Runge Kutta integration.
         dipy_sphere: string, optional
             If necessary, name of the DIPY sphere object to use to evaluate
             directions.
         space: dipy Space
             Space of the streamlines during tracking.
         origin: dipy Origin
             Origin of the streamlines during tracking.
         """
-        super().__init__(dataset, step_size, rk_order, space, origin)
+        super().__init__(datavolume, step_size, rk_order, space, origin)
 
         self.sphere = dipy.data.get_sphere(dipy_sphere)
         self.dirs = np.zeros(len(self.sphere.vertices), dtype=np.ndarray)
         for i in range(len(self.sphere.vertices)):
             self.dirs[i] = TrackingDirection(self.sphere.vertices[i], i)
 
     def prepare_backward(self, line, forward_dir):
@@ -308,34 +312,32 @@
         return TrackingDirection(self.sphere.vertices[ind], ind)
 
 
 class ODFPropagator(PropagatorOnSphere):
     """
     Propagator on ODFs/fODFs. Algo can be det or prob.
     """
-    def __init__(self, dataset, step_size,
+    def __init__(self, datavolume, step_size,
                  rk_order, algo, basis, sf_threshold, sf_threshold_init,
                  theta, dipy_sphere='symmetric724',
                  min_separation_angle=np.pi / 16.,
-                 space=Space('vox'), origin=Origin('center')):
+                 space=Space('vox'), origin=Origin('center'),
+                 is_legacy=True):
         """
 
         Parameters
         ----------
-        dataset: scilpy.image.datasets.DataVolume
-            Trackable Dataset object.
+        datavolume: scilpy.image.volume_space_management.DataVolume
+            Trackable DataVolume object.
         step_size: float
             The step size for tracking.
         rk_order: int
             Order for the Runge Kutta integration.
-        theta: float
-            Maximum angle (radians) between two steps.
-        dipy_sphere: string, optional
-            If necessary, name of the DIPY sphere object to use to evaluate
-            directions.
+        algo: string
+            Type of algorithm. Choices are 'det' or 'prob'
         basis: string
             SH basis name. One of 'tournier07' or 'descoteaux07'
         sf_threshold: float
             Threshold on spherical function (SF).
         sf_threshold_init: float
             Threshold on spherical function when initializing a new streamline.
         theta: float
@@ -355,16 +357,18 @@
             dipy. Interpolation of the ODF is done in VOX space (see
             DataVolume.vox_to_value) so this choice implies the less data
             modification.
         origin: dipy Origin
             Origin of the streamlines during tracking. Default: center, like in
             dipy. Interpolation of the ODF is done in center origin so this
             choice implies the less data modification.
+        is_legacy : bool, optional
+            Whether or not the SH basis is in its legacy form.
         """
-        super().__init__(dataset, step_size, rk_order, dipy_sphere,
+        super().__init__(datavolume, step_size, rk_order, dipy_sphere,
                          space, origin)
 
         if self.space == Space.RASMM:
             raise NotImplementedError(
                 "This version of the propagator on ODF is not ready to work "
                 "in RASMM space.")
 
@@ -385,19 +389,20 @@
         self.maxima_neighbours = get_sphere_neighbours(self.sphere,
                                                        min_separation_angle)
 
         # ODF params
         self.sf_threshold = sf_threshold
         self.sf_threshold_init = sf_threshold_init
         sh_order, full_basis =\
-            get_sh_order_and_fullness(self.dataset.data.shape[-1])
+            get_sh_order_and_fullness(self.datavolume.data.shape[-1])
         self.basis = basis
+        self.is_legacy = is_legacy
         self.B = sh_to_sf_matrix(self.sphere, sh_order, self.basis,
                                  smooth=0.006, return_inv=False,
-                                 full_basis=full_basis)
+                                 full_basis=full_basis, legacy=self.is_legacy)
 
     def _get_sf(self, pos):
         """
         Get the spherical function at position pos.
 
         Parameters
         ----------
@@ -408,39 +413,41 @@
         Return
         ------
         sf: ndarray (len(self.sphere.vertices),)
             Spherical function evaluated at pos, normalized by
             its maximum amplitude.
         """
         # Interpolation:
-        sh = self.dataset.get_value_at_coordinate(
+        sh = self.datavolume.get_value_at_coordinate(
             *pos, space=self.space, origin=self.origin)
         sf = np.dot(self.B.T, sh).reshape((-1, 1))
 
         sf_max = np.max(sf)
         if sf_max > 0:
             sf /= sf_max
         return sf
 
-    def prepare_forward(self, seeding_pos):
+    def prepare_forward(self, seeding_pos, random_generator):
         """
         Prepare information necessary at the first point of the
         streamline for forward propagation: v_in and any other information
         necessary for the self.propagate method.
 
-        About v_in: it is used for two things:
+        About **v_in**, it is used for two things:
+
         - To sample the next direction based on _sample_next_direction method.
-         Ex, with fODF, it defines a cone theta of accepable directions.
+            Ex, with fODF, it defines a cone theta of accepable directions.
         - If no valid next dir are found, continue straight.
 
         Parameters
         ----------
         seeding_pos: tuple(x,y,z)
             The seeding position. Important, position must be in the same space
             and origin as self.space, self.origin!
+        random_generator: numpy Generator
 
         Returns
         -------
         v_in: TrackingDirection
             The "fake" previous direction at first step. Could be None if your
             propagator can propagate without knowledge of previous direction.
             Return PropagationStatus.Error if no good tracking direction can be
@@ -448,17 +455,18 @@
         """
         # Sampling on the SF values (no matter if general algo is det or prob)
         # with a different threshold than usual (sf_threshold_init).
         # So the initial step's propagation will be in a cone theta around a
         # "more probable" peak.
         sf = self._get_sf(seeding_pos)
         sf[sf < self.sf_threshold_init] = 0
+        self.line_rng_generator = random_generator
 
         if np.sum(sf) > 0:
-            ind = sample_distribution(sf)
+            ind = sample_distribution(sf, self.line_rng_generator)
             return TrackingDirection(self.dirs[ind], ind)
 
         # Else: sf at current position is smaller than acceptable threshold in
         # all directions.
         return PropagationStatus.ERROR
 
     def _sample_next_direction(self, pos, v_in):
@@ -481,15 +489,16 @@
         """
         if self.algo == 'prob':
             # Tracking field returns the sf and directions
             sf, directions = self._get_possible_next_dirs_prob(pos, v_in)
 
             # Sampling one.
             if np.sum(sf) > 0:
-                v_out = directions[sample_distribution(sf)]
+                v_out = directions[sample_distribution(sf,
+                                                       self.line_rng_generator)]
             else:
                 return None
         elif self.algo == 'det':
             # Tracking field returns the list of possible maxima.
             possible_maxima = self._get_possible_next_dirs_det(pos, v_in)
             # Choosing one.
             cosinus = 0
```

### Comparing `scilpy-1.5.post2/scilpy/tracking/tracker.py` & `scilpy-2.0.0/scilpy/tracking/tracker.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,25 +1,28 @@
 # -*- coding: utf-8 -*-
+from contextlib import nullcontext
 import itertools
 import logging
 import multiprocessing
 import os
 import sys
 from tempfile import TemporaryDirectory
 from time import perf_counter
 import traceback
 from typing import Union
+from tqdm import tqdm
 
 import numpy as np
 from dipy.data import get_sphere
+from dipy.core.sphere import HemiSphere
 from dipy.io.stateful_tractogram import Space
 from dipy.reconst.shm import sh_to_sf_matrix
 from dipy.tracking.streamlinespeed import compress_streamlines
 
-from scilpy.image.datasets import DataVolume
+from scilpy.image.volume_space_management import DataVolume
 from scilpy.tracking.propagator import AbstractPropagator, PropagationStatus
 from scilpy.reconst.utils import find_order_from_nb_coeff
 from scilpy.tracking.seed import SeedGenerator
 from scilpy.gpuparallel.opencl_utils import CLKernel, CLManager, have_opencl
 
 # For the multi-processing:
 # Dictionary. Will contain all parameters necessary for a sub-process
@@ -28,16 +31,17 @@
 
 
 class Tracker(object):
     def __init__(self, propagator: AbstractPropagator, mask: DataVolume,
                  seed_generator: SeedGenerator, nbr_seeds, min_nbr_pts,
                  max_nbr_pts, max_invalid_dirs, compression_th=0.1,
                  nbr_processes=1, save_seeds=False,
-                 mmap_mode: Union[str, None] = None,
-                 rng_seed=1234, track_forward_only=False, skip=0):
+                 mmap_mode: Union[str, None] = None, rng_seed=1234,
+                 track_forward_only=False, skip=0, verbose=False,
+                 append_last_point=True):
         """
         Parameters
         ----------
         propagator : AbstractPropagator
             Tracking object.
             This tracker will use space and origin defined in the
             propagator.
@@ -50,15 +54,15 @@
         min_nbr_pts: int
             Minimum number of points for streamlines.
         max_nbr_pts: int
             Maximum number of points for streamlines.
         max_invalid_dirs: int
             Number of consecutives invalid directions allowed during tracking.
         compression_th : float,
-            Maximal distance threshold for compression. If None or 0, no
+            Maximal distance threshold for compression. If None, no
             compression is applied.
         nbr_processes: int
             Number of sub processes to use.
         save_seeds: bool
             Whether to save the seeds associated to their respective
             streamlines.
         mmap_mode: str
@@ -70,27 +74,36 @@
             If true, only the forward direction is computed.
         skip: int
             Skip the first N seeds created (and thus N rng numbers). Useful if
             you want to create new streamlines to add to a previously created
             tractogram with a fixed rng_seed. Ex: If tractogram_1 was created
             with nbr_seeds=1,000,000, you can create tractogram_2 with
             skip 1,000,000.
+        verbose: bool
+            Display tracking progression.
+        append_last_point: bool
+            Whether to add the last point (once out of the tracking mask) to
+            the streamline or not. Note that points obtained after an invalid
+            direction (based on the propagator's definition of invalid; ex
+            when angle is too sharp of sh_threshold not reached) are never
+            added.
         """
         self.propagator = propagator
         self.mask = mask
         self.seed_generator = seed_generator
         self.nbr_seeds = nbr_seeds
         self.min_nbr_pts = min_nbr_pts
         self.max_nbr_pts = max_nbr_pts
         self.max_invalid_dirs = max_invalid_dirs
         self.compression_th = compression_th
         self.save_seeds = save_seeds
         self.mmap_mode = mmap_mode
         self.rng_seed = rng_seed
         self.track_forward_only = track_forward_only
+        self.append_last_point = append_last_point
         self.skip = skip
 
         self.origin = self.propagator.origin
         self.space = self.propagator.space
         if self.space == Space.RASMM:
             raise NotImplementedError(
                 "This version of the Tracker is not ready to work in RASMM "
@@ -109,39 +122,43 @@
             logging.warning("Memory-mapping mode cannot be {}. Changed to "
                             "None.".format(self.mmap_mode))
             self.mmap_mode = None
 
         self.nbr_processes = self._set_nbr_processes(nbr_processes)
 
         self.printing_frequency = 1000
+        self.verbose = verbose
 
     def track(self):
         """
         Generate a set of streamline from seed, mask and odf files.
 
         Return
         ------
         streamlines: list of numpy.array
             List of streamlines, represented as an array of positions.
         seeds: list of numpy.array
             List of seeding positions, one 3-dimensional position per
             streamline.
         """
         if self.nbr_processes < 2:
-            chunk_id = 1
+            chunk_id = 0
             lines, seeds = self._get_streamlines(chunk_id)
         else:
             # Each process will use get_streamlines_at_seeds
             chunk_ids = np.arange(self.nbr_processes)
             with TemporaryDirectory() as tmpdir:
+                # Lock for logging
+                lock = multiprocessing.Manager().Lock()
+                zipped_chunks = zip(chunk_ids, [lock] * self.nbr_processes)
 
                 pool = self._prepare_multiprocessing_pool(tmpdir)
 
                 lines_per_process, seeds_per_process = zip(*pool.map(
-                    self._get_streamlines_sub, chunk_ids))
+                    self._get_streamlines_sub, zipped_chunks))
                 pool.close()
                 # Make sure all worker processes have exited before leaving
                 # context manager.
                 pool.join()
                 lines = [line for line in itertools.chain(*lines_per_process)]
                 seeds = [seed for seed in itertools.chain(*seeds_per_process)]
 
@@ -158,16 +175,16 @@
             except NotImplementedError:
                 logging.warning("Cannot determine number of cpus: "
                                 "nbr_processes set to 1.")
                 nbr_processes = 1
 
         if nbr_processes > self.nbr_seeds:
             nbr_processes = self.nbr_seeds
-            logging.debug("Setting number of processes to {} since there were "
-                          "less seeds than processes.".format(nbr_processes))
+            logging.info("Setting number of processes to {} since there were "
+                         "less seeds than processes.".format(nbr_processes))
         return nbr_processes
 
     def _prepare_multiprocessing_pool(self, tmpdir):
         """
         Prepare multiprocessing pool.
 
         Data must be carefully managed to avoid corruption with
@@ -187,15 +204,15 @@
         # in the class, which can be heavy, but it is what we would be
         # doing manually with a static class.
         # Be careful however, parameter changes inside the method will
         # not be kept.
 
         # Saving data. We will reload it in each process.
         data_file_name = os.path.join(tmpdir, 'data.npy')
-        np.save(data_file_name, self.propagator.dataset.data)
+        np.save(data_file_name, self.propagator.datavolume.data)
 
         # Clear data from memory
         self.propagator.reset_data(new_data=None)
 
         pool = multiprocessing.Pool(
             self.nbr_processes,
             initializer=self._send_multiprocess_args_to_global,
@@ -212,35 +229,37 @@
         Sends subprocess' initialisation arguments to global for easier access
         by the multiprocessing pool.
         """
         global multiprocess_init_args
         multiprocess_init_args = init_args
         return
 
-    def _get_streamlines_sub(self, chunk_id):
+    def _get_streamlines_sub(self, params):
         """
         multiprocessing.pool.map input function. Calls the main tracking
         method (_get_streamlines) with correct initialization arguments
         (taken from the global variable multiprocess_init_args).
 
         Parameters
         ----------
-        chunk_id: int
-            This processes's id.
+        params: Tuple[chunk_id, Lock]
+            chunk_id: int, this processes's id.
+            Lock: the multiprocessing lock.
 
         Return
         -------
         lines: list
             List of list of 3D positions (streamlines).
         """
+        chunk_id, lock = params
         global multiprocess_init_args
 
         self._reload_data_for_new_process(multiprocess_init_args)
         try:
-            streamlines, seeds = self._get_streamlines(chunk_id)
+            streamlines, seeds = self._get_streamlines(chunk_id, lock)
             return streamlines, seeds
         except Exception as e:
             logging.error("Operation _get_streamlines_sub() failed.")
             traceback.print_exception(*sys.exc_info(), file=sys.stderr)
             raise e
 
     def _reload_data_for_new_process(self, init_args):
@@ -252,25 +271,28 @@
         init_args: Iterable
             Args necessary to reset data. In current implementation: a tuple;
             (file where the data is saved, mmap_mode).
         """
         self.propagator.reset_data(np.load(
             init_args['data_file_name'], mmap_mode=init_args['mmap_mode']))
 
-    def _get_streamlines(self, chunk_id):
+    def _get_streamlines(self, chunk_id, lock=None):
         """
         Tracks the n streamlines associates with current process (identified by
         chunk_id). The number n is the total number of seeds / the number of
         processes. If asked by user, may compress the streamlines and save the
         seeds.
 
         Parameters
         ----------
         chunk_id: int
             This process ID.
+        lock: Lock
+            The multiprocessing lock for verbose printing (optional with
+            single processing).
 
         Returns
         -------
         streamlines: list
             The successful streamlines.
         seeds: list
             The list of seeds for each streamline, if self.save_seeds. Else, an
@@ -285,29 +307,51 @@
         first_seed_of_chunk = chunk_id * chunk_size + self.skip
         random_generator, indices = self.seed_generator.init_generator(
             self.rng_seed, first_seed_of_chunk)
         if chunk_id == self.nbr_processes - 1:
             chunk_size += self.nbr_seeds % self.nbr_processes
 
         # Getting streamlines
-        for s in range(chunk_size):
-            if s % self.printing_frequency == 0:
-                logging.info("Process {} (id {}): {} / {}"
-                             .format(chunk_id, os.getpid(), s, chunk_size))
+        tqdm_text = "#" + "{}".format(chunk_id).zfill(3)
+
+        if self.verbose:
+            if lock is None:
+                lock = nullcontext()
+            with lock:
+                # Note. Option miniters does not work with manual pbar update.
+                # Will verify manually, lower.
+                # Fixed choice of value rather than a percentage of the chunk
+                # size because our tracker is quite slow.
+                miniters = 100
+                p = tqdm(total=chunk_size, desc=tqdm_text, position=chunk_id+1,
+                         leave=False)
 
+        for s in range(chunk_size):
             seed = self.seed_generator.get_next_pos(
                 random_generator, indices, first_seed_of_chunk + s)
 
+            # Setting the random value.
+            # Previous usage (and usage in Dipy) is to set the random seed
+            # based on the (real) seed position. However, in the case where we
+            # like to have exactly the same seed more than once, this will lead
+            # to exactly the same line, even in probabilistic tracking.
+            # Changing to seed position + seed number.
+            # Then in the case of multiprocessing, adding also a fraction based
+            # on current process ID.
+            eps = s + chunk_id / (self.nbr_processes + 1)
+            line_generator = np.random.default_rng(
+                np.uint32(hash((seed + (eps, eps, eps), self.rng_seed))))
+
             # Forward and backward tracking
-            line = self._get_line_both_directions(seed)
+            line = self._get_line_both_directions(seed, line_generator)
 
             if line is not None:
                 streamline = np.array(line, dtype='float32')
 
-                if self.compression_th and self.compression_th > 0:
+                if self.compression_th is not None:
                     # Compressing. Threshold is in mm. Verifying space.
                     if self.space == Space.VOX:
                         # Equivalent of sft.to_voxmm:
                         streamline *= self.seed_generator.voxres
                         compress_streamlines(streamline, self.compression_th)
                         # Equivalent of sft.to_vox:
                         streamline /= self.seed_generator.voxres
@@ -315,41 +359,42 @@
                         compress_streamlines(streamline, self.compression_th)
 
                 streamlines.append(streamline)
 
                 if self.save_seeds:
                     seeds.append(np.asarray(seed, dtype='float32'))
 
+            if self.verbose and (s + 1) % miniters == 0:
+                with lock:
+                    p.update(miniters)
+
+        if self.verbose:
+            with lock:
+                p.close()
         return streamlines, seeds
 
-    def _get_line_both_directions(self, seeding_pos):
+    def _get_line_both_directions(self, seeding_pos, line_generator):
         """
         Generate a streamline from an initial position following the tracking
         parameters.
 
         Parameters
         ----------
         seeding_pos : tuple
             3D position, the seed position.
 
         Returns
         -------
         line: list of 3D positions
             The generated streamline for seeding_pos.
         """
-
-        # toDo See numpy's doc: np.random.seed:
-        #  This is a convenience, legacy function.
-        #  The best practice is to not reseed a BitGenerator, rather to
-        #  recreate a new one. This method is here for legacy reasons.
-        np.random.seed(np.uint32(hash((seeding_pos, self.rng_seed))))
-
         # Forward
         line = [np.asarray(seeding_pos)]
-        tracking_info = self.propagator.prepare_forward(seeding_pos)
+        tracking_info = self.propagator.prepare_forward(seeding_pos,
+                                                        line_generator)
         if tracking_info == PropagationStatus.ERROR:
             # No good tracking direction can be found at seeding position.
             return None
         line = self._propagate_line(line, tracking_info)
 
         # Backward
         if not self.track_forward_only:
@@ -394,41 +439,32 @@
         """
         invalid_direction_count = 0
         propagation_can_continue = True
         while len(line) < self.max_nbr_pts and propagation_can_continue:
             new_pos, new_tracking_info, is_direction_valid = \
                 self.propagator.propagate(line, tracking_info)
 
-            # Verifying and appending
+            # Verifying if direction is valid
+            # If invalid: break. Else, verify tracking mask.
             if is_direction_valid:
                 invalid_direction_count = 0
             else:
                 invalid_direction_count += 1
-            propagation_can_continue = self._verify_stopping_criteria(
-                invalid_direction_count, new_pos)
-            if propagation_can_continue:
+                if invalid_direction_count > self.max_invalid_dirs:
+                    break
+
+            propagation_can_continue = self._verify_stopping_criteria(new_pos)
+            if propagation_can_continue or self.append_last_point:
                 line.append(new_pos)
 
             tracking_info = new_tracking_info
 
-        # Possible last step.
-        final_pos = self.propagator.finalize_streamline(line[-1],
-                                                        tracking_info)
-        if (final_pos is not None and
-                not np.array_equal(final_pos, line[-1]) and
-                self.mask.is_coordinate_in_bound(
-                    *final_pos, space=self.space, origin=self.origin)):
-            line.append(final_pos)
         return line
 
-    def _verify_stopping_criteria(self, invalid_direction_count, last_pos):
-
-        # Checking number of consecutive invalid directions
-        if invalid_direction_count > self.max_invalid_dirs:
-            return False
+    def _verify_stopping_criteria(self, last_pos):
 
         # Checking if out of bound
         if not self.mask.is_coordinate_in_bound(
                 *last_pos, space=self.space, origin=self.origin):
             return False
 
         # Checking if out of mask
@@ -453,168 +489,160 @@
     Parameters
     ----------
     sh : ndarray
         Spherical harmonics volume. Ex: ODF or fODF.
     mask : ndarray
         Tracking mask. Tracking stops outside the mask.
     seeds : ndarray (n_seeds, 3)
-        Seed positions in voxel space with origin `corner`.
+        Seed positions in voxel space with origin `center`.
     step_size : float
         Step size in voxel space.
-    min_nbr_pts : int
-        Minimum length of a streamline in voxel space.
     max_nbr_pts : int
         Maximum length of a streamline in voxel space.
     theta : float or list of float, optional
         Maximum angle (degrees) between 2 steps. If a list, a theta
         is randomly drawn from the list for each streamline.
     sh_basis : str, optional
         Spherical harmonics basis.
+    is_legacy : bool, optional
+        Whether or not the SH basis is in its legacy form.
     batch_size : int, optional
         Approximate size of GPU batches.
     forward_only: bool, optional
         If True, only forward tracking is performed.
     rng_seed : int, optional
         Seed for random number generator.
+    sphere : int, optional
+        Sphere to use for the tracking.
     """
-    def __init__(self, sh, mask, seeds, step_size, min_nbr_pts, max_nbr_pts,
+    def __init__(self, sh, mask, seeds, step_size, max_nbr_pts,
                  theta=20.0, sf_threshold=0.1, sh_interp='trilinear',
-                 sh_basis='descoteaux07', batch_size=100000,
-                 forward_only=False, rng_seed=None):
+                 sh_basis='descoteaux07', is_legacy=True, batch_size=100000,
+                 forward_only=False, rng_seed=None, sphere=None):
         if not have_opencl:
             raise ImportError('pyopencl is not installed. In order to use'
                               'GPU tracker, you need to install it first.')
         self.sh = sh
         if sh_interp not in ['nearest', 'trilinear']:
             raise ValueError('Invalid SH interpolation mode: {}'
                              .format(sh_interp))
         self.sh_interp_nn = sh_interp == 'nearest'
         self.mask = mask
 
-        if (seeds < 0).any():
-            raise ValueError('Invalid seed positions.\nGPUTracker works with'
-                             ' origin \'corner\'.')
         self.n_seeds = len(seeds)
+
         self.seed_batches =\
-            np.array_split(seeds, np.ceil(len(seeds)/batch_size))
+            np.array_split(seeds + 0.5, np.ceil(len(seeds)/batch_size))
+
+        if sphere is None:
+            self.sphere = get_sphere("repulsion724")
+        else:
+            self.sphere = sphere
 
         # tracking step_size and number of points
         self.step_size = step_size
         self.sf_threshold = sf_threshold
-        self.min_strl_points = min_nbr_pts
         self.max_strl_points = max_nbr_pts
 
         # convert theta to array
-        if isinstance(theta, float):
-            theta = np.array([theta])
-        self.theta = theta
+        self.theta = np.atleast_1d(theta)
 
         self.sh_basis = sh_basis
+        self.is_legacy = is_legacy
         self.forward_only = forward_only
 
         # Instantiate random number generator
         self.rng = np.random.default_rng(rng_seed)
 
     def _get_max_amplitudes(self, B_mat):
         fodf_max = np.zeros(self.mask.shape,
                             dtype=np.float32)
         fodf_max[self.mask > 0] = np.max(self.sh[self.mask > 0].dot(B_mat),
                                          axis=-1)
 
         return fodf_max
 
-    def track(self):
+    def __iter__(self):
+        return self._track()
+
+    def _track(self):
         """
         GPU streamlines generator yielding streamlines with corresponding
         seed positions one by one.
         """
-        t0 = perf_counter()
-
-        # Load the sphere
-        sphere = get_sphere('symmetric724')
-
         # Convert theta to cos(theta)
         max_cos_theta = np.cos(np.deg2rad(self.theta))
 
-        cl_kernel = CLKernel('main', 'tracking', 'local_tracking.cl')
+        cl_kernel = CLKernel('tracker', 'tracking', 'local_tracking.cl')
 
         # Set tracking parameters
         cl_kernel.set_define('IM_X_DIM', self.sh.shape[0])
         cl_kernel.set_define('IM_Y_DIM', self.sh.shape[1])
         cl_kernel.set_define('IM_Z_DIM', self.sh.shape[2])
         cl_kernel.set_define('IM_N_COEFFS', self.sh.shape[3])
-        cl_kernel.set_define('N_DIRS', len(sphere.vertices))
+        cl_kernel.set_define('N_DIRS', len(self.sphere.vertices))
 
         cl_kernel.set_define('N_THETAS', len(self.theta))
         cl_kernel.set_define('STEP_SIZE', '{:.8f}f'.format(self.step_size))
         cl_kernel.set_define('MAX_LENGTH', self.max_strl_points)
         cl_kernel.set_define('FORWARD_ONLY',
                              'true' if self.forward_only else 'false')
         cl_kernel.set_define('SF_THRESHOLD',
                              '{:.8f}f'.format(self.sf_threshold))
         cl_kernel.set_define('SH_INTERP_NN',
                              'true' if self.sh_interp_nn else 'false')
 
         # Create CL program
-        n_input_params = 8
-        n_output_params = 2
-        cl_manager = CLManager(cl_kernel, n_input_params, n_output_params)
+        cl_manager = CLManager(cl_kernel)
 
         # Input buffers
         # Constant input buffers
-        cl_manager.add_input_buffer(0, self.sh)
-        cl_manager.add_input_buffer(1, sphere.vertices)
+        cl_manager.add_input_buffer('sh', self.sh)
+        cl_manager.add_input_buffer('vertices', self.sphere.vertices)
 
         sh_order = find_order_from_nb_coeff(self.sh)
-        B_mat = sh_to_sf_matrix(sphere, sh_order, self.sh_basis,
-                                return_inv=False)
-        cl_manager.add_input_buffer(2, B_mat)
+        B_mat = sh_to_sf_matrix(self.sphere, sh_order, self.sh_basis,
+                                return_inv=False, legacy=self.is_legacy)
+        cl_manager.add_input_buffer('b_matrix', B_mat)
 
         fodf_max = self._get_max_amplitudes(B_mat)
-        cl_manager.add_input_buffer(3, fodf_max)
-        cl_manager.add_input_buffer(4, self.mask.astype(np.float32))
+        cl_manager.add_input_buffer('max_amplitudes', fodf_max)
+        cl_manager.add_input_buffer('mask', self.mask.astype(np.float32))
+
+        cl_manager.add_input_buffer('max_cos_theta', max_cos_theta)
 
-        cl_manager.add_input_buffer(5, max_cos_theta)
+        cl_manager.add_input_buffer('seeds')
+        cl_manager.add_input_buffer('randvals')
 
-        logging.debug('Initialized OpenCL program in {:.2f}s.'
-                      .format(perf_counter() - t0))
+        cl_manager.add_output_buffer('out_strl')
+        cl_manager.add_output_buffer('out_lengths')
 
         # Generate streamlines in batches
-        t0 = perf_counter()
-        nb_processed_streamlines = 0
-        nb_valid_streamlines = 0
         for seed_batch in self.seed_batches:
             # Generate random values for sf sampling
             # TODO: Implement random number generator directly
             #       on the GPU to generate values on-the-fly.
             rand_vals = self.rng.uniform(0.0, 1.0,
                                          (len(seed_batch),
                                           self.max_strl_points))
 
             # Update buffers
-            cl_manager.add_input_buffer(6, seed_batch)
-            cl_manager.add_input_buffer(7, rand_vals)
+            cl_manager.update_input_buffer('seeds', seed_batch)
+            cl_manager.update_input_buffer('randvals', rand_vals)
 
             # output streamlines buffer
-            cl_manager.add_output_buffer(0, (len(seed_batch),
+            cl_manager.update_output_buffer('out_strl',
+                                            (len(seed_batch),
                                              self.max_strl_points, 3))
             # output streamlines length buffer
-            cl_manager.add_output_buffer(1, (len(seed_batch), 1))
+            cl_manager.update_output_buffer('out_lengths',
+                                            (len(seed_batch), 1))
 
             # Run the kernel
             tracks, n_points = cl_manager.run((len(seed_batch), 1, 1))
-            n_points = n_points.squeeze().astype(np.int16)
+            n_points = n_points.flatten().astype(np.int16)
             for (strl, seed, n_pts) in zip(tracks, seed_batch, n_points):
-                if n_pts >= self.min_strl_points:
-                    strl = strl[:n_pts]
-                    nb_valid_streamlines += 1
-
-                    # output is yielded so that we can use lazy tractogram.
-                    yield strl, seed
-
-            # per-batch logging information
-            nb_processed_streamlines += len(seed_batch)
-            logging.info('{0:>8}/{1} streamlines generated'
-                         .format(nb_processed_streamlines, self.n_seeds))
+                strl = strl[:n_pts]
 
-        logging.info('Tracked {0} streamlines in {1:.2f}s.'
-                     .format(nb_valid_streamlines, perf_counter() - t0))
+                # output is yielded so that we can use LazyTractogram.
+                # seed and strl with origin center (same as DIPY)
+                yield strl - 0.5, seed - 0.5
```

### Comparing `scilpy-1.5.post2/scilpy/tractanalysis/grid_intersections.pyx` & `scilpy-2.0.0/scilpy/tractanalysis/grid_intersections.pyx`

 * *Files identical despite different names*

### Comparing `scilpy-1.5.post2/scilpy/tractanalysis/quick_tools.pyx` & `scilpy-2.0.0/scilpy/tractanalysis/quick_tools.pyx`

 * *Files identical despite different names*

### Comparing `scilpy-1.5.post2/scilpy/tractanalysis/scoring.py` & `scilpy-2.0.0/scilpy/tractanalysis/scoring.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,54 +1,53 @@
 # -*- coding: utf-8 -*-
 
 """
 Tractometry
 -----------
-
 Global connectivity metrics:
-    Computed by default:
+
+- Computed by default:
     - VS: valid streamlines, belonging to a bundle (i.e. respecting all the
         criteria for that bundle; endpoints, limit_mask, gt_mask.).
     - IS: invalid streamlines. All other streamlines. IS = IC + NC.
 
-    Optional:
+- Optional:
     - WPC: wrong path connections, streamlines connecting correct ROIs but not
         respecting the other criteria for that bundle. Such streamlines always
         exist but they are only saved separately if specified in the options.
         Else, they are merged back with the IS.
-        ** By definition. WPC are only computed if "limits masks" are provided.
+        By definition. WPC are only computed if "limits masks" are provided.
     - IC: invalid connections, streamlines joining an incorrect combination of
         ROIs. Use carefully, quality depends on the quality of your ROIs and no
         analysis is done on the shape of the streamlines.
     - NC: no connections. Invalid streamlines minus invalid connections.
 
-Fidelity metrics:
-    - OL : Overlap. Percentage of ground truth voxels containing streamline(s)
+- Fidelity metrics:
+    - OL: Overlap. Percentage of ground truth voxels containing streamline(s)
         for a given bundle.
     - OR: Overreach. Amount of voxels containing streamline(s) when they
         shouldn't, for a given bundle. We compute two versions :
         OR_pct_vs = divided by the total number of voxel covered by the bundle.
-           (percentage of the voxels touched by VS).
-           Values range between 0 and 100%. Values are not defined when we
-           recovered no streamline for a bundle, but we set the OR_pct_vs to 0
-           in that case.
+        (percentage of the voxels touched by VS).
+        Values range between 0 and 100%. Values are not defined when we
+        recovered no streamline for a bundle, but we set the OR_pct_vs to 0
+        in that case.
         OR_pct_gt = divided by the total size of the ground truth bundle mask.
-           Values could be higher than 100%.
-    - f1 score (which is the same as the Dice score).
+        Values could be higher than 100%.
+    - f1 score: which is the same as the Dice score.
 """
 
 import logging
 
 import numpy as np
 
-from dipy.io.stateful_tractogram import StatefulTractogram
+from scilpy.tractanalysis.streamlines_metrics import compute_tract_counts_map
 
-from scilpy.tractanalysis.reproducibility_measures import \
+from scilpy.tractograms.streamline_and_mask_operations import \
     get_endpoints_density_map
-from scilpy.tractanalysis.streamlines_metrics import compute_tract_counts_map
 
 
 def compute_f1_score(overlap, overreach):
     """
     Compute the F1 score between overlap and overreach (they must be
     percentages).
 
@@ -179,16 +178,15 @@
 
     if len(sft) == 0:
         return np.zeros(dimensions), np.zeros(dimensions)
 
     bundles_voxels = compute_tract_counts_map(sft.streamlines,
                                               dimensions).astype(np.int16)
 
-    endpoints_voxels = get_endpoints_density_map(sft.streamlines,
-                                                 dimensions).astype(np.int16)
+    endpoints_voxels = get_endpoints_density_map(sft).astype(np.int16)
 
     bundles_voxels[bundles_voxels > 0] = 1
     endpoints_voxels[endpoints_voxels > 0] = 1
 
     return bundles_voxels, endpoints_voxels
 
 
@@ -263,14 +261,16 @@
             if current_vb is None or len(current_vb) == 0:
                 logging.debug("   Empty bundle or bundle not found.")
                 bundle_results.update({
                     "TP": 0, "FP": 0, "FN": 0,
                     "OL": 0, "OR_pct_gt": 0, "OR_pct_vs": 0, "f1": 0,
                     "endpoints_OL": 0, "endpoints_OR": 0
                 })
+                nb_bundles_in_stats += 1
+                bundle_wise_dict.update({bundles_names[i]: bundle_results})
                 continue
 
             # Getting the recovered mask
             current_vb_voxels, current_vb_endpoints_voxels = get_binary_maps(
                 current_vb)
 
             (f1, tp_nb_voxels, fp_nb_voxels, fn_nb_voxels,
```

### Comparing `scilpy-1.5.post2/scilpy/tractanalysis/streamlines_metrics.pyx` & `scilpy-2.0.0/scilpy/tractanalysis/streamlines_metrics.pyx`

 * *Files identical despite different names*

### Comparing `scilpy-1.5.post2/scilpy/tractanalysis/todi.py` & `scilpy-2.0.0/scilpy/tractanalysis/todi.py`

 * *Files 2% similar despite different names*

```diff
@@ -226,15 +226,16 @@
         -------
         todi : numpy.ndarray
             Normalized TODI map.
         """
         self.todi = todi_u.p_normalize_vectors(self.todi, p_norm)
         return self.todi
 
-    def get_sh(self, sh_basis, sh_order, smooth=0.006, full_basis=False):
+    def get_sh(self, sh_basis, sh_order, smooth=0.006, full_basis=False,
+               is_legacy=True):
         """Spherical Harmonics (SH) coefficients of the TODI map
 
         Compute the SH representation of the TODI map,
         converting SF to SH with a smoothing factor.
 
         Parameters
         ----------
@@ -245,14 +246,16 @@
             (``None`` defaults to ``descoteaux07``).
         sh_order : int
             Maximum SH order in the SH fit.  For `sh_order`, there will be
             ``(sh_order + 1) * (sh_order_2) / 2`` SH coefficients (default 4).
         smooth : float, optional
             Smoothing factor for the conversion,
             Lambda-regularization in the SH fit (default 0.006).
+        is_legacy : bool, optional
+            Whether or not the SH basis is in its legacy form.
 
         Returns
         -------
         todi_sh : ndarray
             SH representation of the TODI map
 
         References
@@ -261,17 +264,17 @@
                Regularized, Fast, and Robust Analytical Q-ball Imaging.
                Magn. Reson. Med. 2007;58:497-510.
         .. [2] Tournier J.D., Calamante F. and Connelly A.
                Robust determination of the fibre orientation distribution in
                diffusion MRI: Non-negativity constrained super-resolved
                spherical deconvolution. NeuroImage. 2007;35(4):1459-1472.
         """
-        return sf_to_sh(self.todi, self.sphere, sh_order=sh_order,
+        return sf_to_sh(self.todi, self.sphere, sh_order_max=sh_order,
                         basis_type=sh_basis, full_basis=full_basis,
-                        smooth=smooth)
+                        smooth=smooth, legacy=is_legacy)
 
     def reshape_to_3d(self, img_voxelly_masked):
         """Reshape a complex ravelled image to 3D.
 
         Unravel a given unravel mask (1D), image (1D), SH/SF (2D)
         to its original 3D shape (with a 4D for SH/SF).
```

### Comparing `scilpy-1.5.post2/scilpy/tractanalysis/todi_util.py` & `scilpy-2.0.0/scilpy/tractanalysis/todi_util.py`

 * *Files identical despite different names*

### Comparing `scilpy-1.5.post2/scilpy/tractanalysis/uncompress.pyx` & `scilpy-2.0.0/scilpy/tractograms/uncompress.pyx`

 * *Files 3% similar despite different names*

```diff
@@ -33,21 +33,38 @@
 
 @cython.boundscheck(False)
 @cython.wraparound(False)
 @cython.cdivision(True)
 def uncompress(streamlines, return_mapping=False):
     """
     Get the indices of the voxels traversed by each streamline; then returns
-    an ArraySequence of indices. Yes, of *indices*. ArraySequence.get_data() is
-    always of type float32 and contains points except here: it's of type
-    uint16 and contain indices. You can use this object exactly as you would
-    use a normal ArraySequence.
+    an ArraySequence of indices, i.e. [i, j, k] coordinates.
 
-    :param streamlines: nibabel.streamlines.array_sequence.ArraySequence
-        should be in voxel space, aligned to corner.
+    Parameters
+    ----------
+    streamlines: nibabel.streamlines.array_sequence.ArraySequence
+        Should be in voxel space, aligned to corner.
+    return_mapping: bool
+        If true, also returns the points_to_idx.
+
+    Returns
+    -------
+    indices: nibabel.streamlines.array_sequence.ArraySequence
+        An array of length nb_streamlines. Each element is a np.ndarray of
+        shape (nb_voxels, 3) containing indices. All 3D coordinates are unique.
+        Note. ArraySequence.get_data() is always of type float32 and contains
+        points except here: it's of type uint16 and contain indices. You can use
+        this object exactly as you would use a normal ArraySequence.
+    points_to_idx: nibabel.streamlines.array_sequence.ArraySequence (optional)
+        An array of length nb_streamlines. Each element is a np.ndarray of
+        shape (nb_points) containing, for each streamline point, the associated
+        voxel in indices.
+        Note: Some points are associated to the same index, if they were in the
+        same voxel. Some voxels are associated to no point, if they were
+        traversed by a segment but contained no point.
     """
     cdef:
         cnp.npy_intp nb_streamlines = len(streamlines._lengths)
         cnp.npy_intp at_point = 0
 
         # Multiplying by 6 is simply a heuristic to avoiding resizing too many
         # times. In my bundles tests, I had either 0 or 1 resize.
```

### Comparing `scilpy-1.5.post2/scilpy/tractograms/lazy_tractogram_operations.py` & `scilpy-2.0.0/scilpy/tractograms/lazy_tractogram_operations.py`

 * *Files identical despite different names*

### Comparing `scilpy-1.5.post2/scilpy/tractograms/tractogram_operations.py` & `scilpy-2.0.0/scilpy/tractograms/tractogram_operations.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,33 +1,36 @@
 # -*- coding: utf-8 -*-
 
 """
 This module regroups small operations and util functions for tractogram.
 Meaning that these operations are applied on the streamlines as wholes (ex,
 registration, suffling, etc), not on each point of the streamlines separately /
-individually. See scilpy.tractograms.streamline_operations.py for the later.
+individually. See scilpy.tractograms.streamline_operations.py for the latter.
 """
 
-import itertools
-
-import random
 from functools import reduce
+import itertools
 import logging
+import random
 
 from dipy.io.stateful_tractogram import StatefulTractogram, Space
 from dipy.io.utils import get_reference_info, is_header_compatible
 from dipy.segment.clustering import qbx_and_merge
 from dipy.tracking.streamline import transform_streamlines
+from dipy.tracking.streamlinespeed import compress_streamlines
+from nibabel.streamlines import TrkFile, TckFile
 from nibabel.streamlines.array_sequence import ArraySequence
 import numpy as np
+from numpy.polynomial.polynomial import Polynomial
 from scipy.ndimage import map_coordinates
 from scipy.spatial import cKDTree
 
-from scilpy.tracking.tools import smooth_line_gaussian, smooth_line_spline
-from scilpy.utils.streamlines import cut_invalid_streamlines
+from scilpy.tractograms.streamline_operations import smooth_line_gaussian, \
+    resample_streamlines_step_size, parallel_transport_streamline, \
+    cut_invalid_streamlines
 
 MIN_NB_POINTS = 10
 KEY_INDEX = np.concatenate((range(5), range(-1, -6, -1)))
 
 
 def shuffle_streamlines(sft, rng_seed=None):
     indices = np.arange(len(sft.streamlines))
@@ -41,53 +44,83 @@
         streamlines, sft,
         data_per_streamline=data_per_streamline,
         data_per_point=data_per_point)
     return shuffled_sft
 
 
 def get_axis_flip_vector(flip_axes):
+    """
+    Create a flip vector from a list of axes.
+
+    Parameters
+    ----------
+    flip_axes: list
+        List of axes you want to flip
+
+    Returns
+    -------
+    shift_vector: list[3,]
+        Vector with flipped axes
+    """
     flip_vector = np.ones(3)
     if 'x' in flip_axes:
         flip_vector[0] = -1.0
     if 'y' in flip_axes:
         flip_vector[1] = -1.0
     if 'z' in flip_axes:
         flip_vector[2] = -1.0
 
     return flip_vector
 
 
-def get_shift_vector(sft):
+def _get_shift_vector(sft):
     dims = sft.space_attributes[1]
     shift_vector = -1.0 * (np.array(dims) / 2.0)
 
     return shift_vector
 
 
 def flip_sft(sft, flip_axes):
-    flip_vector = get_axis_flip_vector(flip_axes)
-    shift_vector = get_shift_vector(sft)
+    """
+    Parameters
+    ----------
+    sft: StatefulTractogram
+    flip_axes: List[str]
+        The list of axes to flip. Ex: ['x', 'y', 'z']. The axes correspond to
+        the coordinates of the sft as it is stored in memory, not to axes
+        of the image.
 
-    flipped_streamlines = []
+    Returns
+    -------
+    flipped_sft: StatefulTractogram
+    """
+    if len(flip_axes) == 0:
+        # Could return sft. But creating new SFT (or deep copy).
+        flipped_streamlines = sft.streamlines
+    else:
+        flip_vector = get_axis_flip_vector(flip_axes)
+        shift_vector = _get_shift_vector(sft)
+
+        flipped_streamlines = []
 
-    for streamline in sft.streamlines:
-        mod_streamline = streamline + shift_vector
-        mod_streamline *= flip_vector
-        mod_streamline -= shift_vector
-        flipped_streamlines.append(mod_streamline)
+        for streamline in sft.streamlines:
+            mod_streamline = streamline + shift_vector
+            mod_streamline *= flip_vector
+            mod_streamline -= shift_vector
+            flipped_streamlines.append(mod_streamline)
 
     new_sft = StatefulTractogram.from_sft(
         flipped_streamlines, sft,
         data_per_point=sft.data_per_point,
         data_per_streamline=sft.data_per_streamline)
 
     return new_sft
 
 
-def get_streamline_key(streamline, precision=None):
+def _get_streamline_key(streamline, precision=None):
     # Use just a few data points as hash key. I could use all the data of
     # the streamlines, but then the complexity grows with the number of
     # points.
     if len(streamline) < MIN_NB_POINTS:
         key = streamline.copy()
     else:
         key = streamline[KEY_INDEX].copy()
@@ -96,15 +129,15 @@
         key = np.round(key, precision)
 
     key.flags.writeable = False
 
     return key.data.tobytes()
 
 
-def hash_streamlines(streamlines, start_index=0, precision=None):
+def _hash_streamlines(streamlines, start_index=0, precision=None):
     """
     Produces a dict from streamlines by using the points as keys and the
     indices of the streamlines as values.
 
     Parameters
     ----------
     streamlines: list of ndarray
@@ -117,15 +150,16 @@
         rounding is performed.
 
     Returns
     -------
     A dict where the keys are streamline points and the values are indices
     starting at start_index.
     """
-    keys = [get_streamline_key(s, precision) for s in streamlines]
+    keys = [_get_streamline_key(s, precision) for s in streamlines]
+
     return {k: i for i, k in enumerate(keys, start_index)}
 
 
 def intersection(left, right):
     """Intersection of two streamlines dict (see hash_streamlines)"""
     return {k: v for k, v in left.items() if k in right}
 
@@ -136,16 +170,75 @@
 
 
 def union(left, right):
     """Union of two streamlines dict (see hash_streamlines)"""
     return {**left, **right}
 
 
-def perform_tractogram_operation(operation, streamlines, precision=None):
-    """Peforms an operation on a list of list of streamlines
+def perform_tractogram_operation_on_sft(op_name, sft_list, precision,
+                                        no_metadata, fake_metadata):
+    """Peforms an operation on a list of tractograms.
+
+    Parameters
+    ----------
+    op_name: str
+        A callable that takes two streamlines dicts as inputs and preduces a
+        new streamline dict.
+    sft_list: list[StatefulTractogram]
+        The streamlines used in the operation.
+    precision: int, optional
+        The number of decimals to keep when hashing the points of the
+        streamlines. Allows a soft comparison of streamlines. If None, no
+        rounding is performed.
+    no_metadata: bool
+        If true, remove all metadata.
+    fake_metadata: bool
+        If true, fake metadata for SFTs that do not contain the keys available
+        in other SFTs.
+
+    Returns
+    -------
+    sft: StatefulTractogram
+        The final SFT
+    """
+    # Performing operation
+    streamlines_list = [sft.streamlines if sft is not None else []
+                        for sft in sft_list]
+    _, indices = perform_tractogram_operation_on_lines(
+        OPERATIONS[op_name], streamlines_list, precision=precision)
+
+    # Current error in dipy prevents concatenation with empty SFT
+    # (see PR here to fix: https://github.com/dipy/dipy/pull/2864)
+    # Returning empty sft now if that is the case.
+    if len(indices) == 0:
+        empty_sft = sft_list[0]
+        empty_sft.streamlines = []
+        return empty_sft, indices
+
+    # Concatenating only the necessary streamlines, with the metadata
+    indices_per_sft = []
+    streamlines_len_cumsum = [len(sft) for sft in sft_list]
+    start = 0
+    for nb in streamlines_len_cumsum:
+        end = start + nb
+        # Switch to int32 for json
+        indices_per_sft.append([int(i - start)
+                                for i in indices if start <= i < end])
+        start = end
+
+    sft_list = [sft[indices_per_sft[i]] for i, sft in enumerate(sft_list)
+                if len(indices_per_sft[i]) > 0]
+
+    new_sft = concatenate_sft(sft_list, no_metadata, fake_metadata)
+    return new_sft, indices_per_sft
+
+
+def perform_tractogram_operation_on_lines(operation, streamlines,
+                                          precision=None):
+    """Peforms an operation on a list of list of streamlines.
 
     Given a list of list of streamlines, this function applies the operation
     to the first two lists of streamlines. The result in then used recursively
     with the third, fourth, etc. lists of streamlines.
 
     A valid operation is any function that takes two streamlines dict as input
     and produces a new streamlines dict (see hash_streamlines). Union,
@@ -167,57 +260,62 @@
     -------
     streamlines: list of `nib.streamline.Streamlines`
         The streamlines obtained after performing the operation on all the
         input streamlines.
     indices: np.ndarray
         The indices of the streamlines that are used in the output.
     """
-    # Hash the streamlines using the desired precision.
-    indices = np.cumsum([0] + [len(s) for s in streamlines[:-1]])
-    hashes = [hash_streamlines(s, i, precision) for
-              s, i in zip(streamlines, indices)]
-
-    # Perform the operation on the hashes and get the output streamlines.
-    to_keep = reduce(operation, hashes)
-    all_streamlines = list(itertools.chain(*streamlines))
-    indices = np.array(sorted(to_keep.values())).astype(np.uint32)
-    streamlines = [all_streamlines[i] for i in indices]
+    if 'robust' in operation.__name__:
+        if precision is None:
+            precision = 3
+        return operation(streamlines, precision)
+    else:
+        # Hash the streamlines using the desired precision.
+        indices = np.cumsum([0] + [len(s) for s in streamlines[:-1]])
+        hashes = [_hash_streamlines(s, i, precision) for
+                  s, i in zip(streamlines, indices)]
+
+        # Perform the operation on the hashes and get the output streamlines.
+        to_keep = reduce(operation, hashes)
+        all_streamlines = list(itertools.chain(*streamlines))
+        indices = np.array(sorted(to_keep.values())).astype(np.uint32)
+        streamlines = [all_streamlines[i] for i in indices]
     return streamlines, indices
 
 
 def intersection_robust(streamlines_list, precision=3):
     """ Intersection of a list of StatefulTractogram """
     if not isinstance(streamlines_list, list):
         streamlines_list = [streamlines_list]
 
-    streamlines_fused, indices = find_identical_streamlines(
+    streamlines_fused, indices = _find_identical_streamlines(
         streamlines_list, epsilon=10**(-precision))
     return streamlines_fused[indices], indices
 
 
 def difference_robust(streamlines_list, precision=3):
     """ Difference of a list of StatefulTractogram from the first element """
     if not isinstance(streamlines_list, list):
         streamlines_list = [streamlines_list]
-    streamlines_fused, indices = find_identical_streamlines(
+    streamlines_fused, indices = _find_identical_streamlines(
         streamlines_list, epsilon=10**(-precision), difference_mode=True)
     return streamlines_fused[indices], indices
 
 
 def union_robust(streamlines_list, precision=3):
     """ Union of a list of StatefulTractogram """
     if not isinstance(streamlines_list, list):
         streamlines_list = [streamlines_list]
-    streamlines_fused, indices = find_identical_streamlines(
+    streamlines_fused, indices = _find_identical_streamlines(
         streamlines_list, epsilon=10**(-precision), union_mode=True)
     return streamlines_fused[indices], indices
 
 
-def find_identical_streamlines(streamlines_list, epsilon=0.001,
-                               union_mode=False, difference_mode=False):
+def _find_identical_streamlines(streamlines_list, epsilon=0.001,
+                                union_mode=False, difference_mode=False):
     """ Return the intersection/union/difference from a list of list of
     streamlines. Allows for a maximum distance for matching.
 
     Parameters:
     -----------
     streamlines_list: list
         List of lists of streamlines or list of ArraySequences
@@ -235,18 +333,20 @@
     streamlines = ArraySequence(itertools.chain(*streamlines_list))
     nb_streamlines = np.cumsum([len(sft) for sft in streamlines_list])
     nb_streamlines = np.insert(nb_streamlines, 0, 0)
 
     if union_mode and difference_mode:
         raise ValueError('Cannot use union_mode and difference_mode at the '
                          'same time.')
+    intersect_mode = (not union_mode and not difference_mode)
 
     all_tree = {}
     all_tree_mapping = {}
     first_points = np.array(streamlines.get_data()[streamlines._offsets])
+
     # Uses the number of point to speed up the search in the ckdtree
     for point_count in np.unique(streamlines._lengths):
         same_length_ind = np.where(streamlines._lengths == point_count)[0]
         all_tree[point_count] = cKDTree(first_points[same_length_ind])
         all_tree_mapping[point_count] = same_length_ind
 
     inversion_val = 1 if union_mode or difference_mode else 0
@@ -254,92 +354,107 @@
     average_match_distance = []
 
     # Difference by design will never select streamlines that are not from the
     # first set
     if difference_mode:
         streamlines_to_keep[nb_streamlines[1]:] = 0
     for i, streamline in enumerate(streamlines):
-        # Unless do an union, there is no point at looking past the first set
+        # Unless we do a union, there is no point looking past the first set
         if not union_mode and i >= nb_streamlines[1]:
             break
 
         # Find the closest (first) points
         distance_ind = all_tree[len(streamline)].query_ball_point(
             streamline[0], r=2*epsilon)
         actual_ind = np.sort(all_tree_mapping[len(streamline)][distance_ind])
 
-        # Intersection requires finding matches is all sets
-        if not union_mode or not difference_mode:
+        # Intersection requires finding matches in all sets
+        if intersect_mode:
             intersect_test = np.zeros((len(nb_streamlines)-1,))
 
+            # The streamline's set itself is obviously already ok.
+            set_i = np.max(np.where(nb_streamlines <= i)[0])
+            intersect_test[set_i] = True
+
+        # Looking at similar streamlines only:
         for j in actual_ind:
+            # 1) Yourself is never a match.
+            #    (For union : always kept)
+            #    (For difference: if another is found, we will remove i)
+            #    (For intersection: will be kept lower if others are found).
+            if i == j:
+                continue
+
             # Actual check of the whole streamline
             sub_vector = streamline-streamlines[j]
             norm = np.linalg.norm(sub_vector, axis=1)
+            average_match_distance.append(np.average(sub_vector, axis=0))
 
             if union_mode:
                 # 1) Yourself is not a match
                 # 2) If the streamline hasn't been selected (by another match)
                 # 3) The streamline is 'identical'
-                if (i != j and streamlines_to_keep[i] == inversion_val
-                        and (norm < 2*epsilon).all()):
-                    streamlines_to_keep[j] = not inversion_val
-                    average_match_distance.append(
-                        np.average(sub_vector, axis=0))
+                if streamlines_to_keep[i] == 1 and (norm < 2*epsilon).all():
+                    streamlines_to_keep[j] = 0
+
             elif difference_mode:
                 # 1) Yourself is not a match
                 # 2) The streamline is 'identical'
-                if i != j and (norm < 2*epsilon).all():
-                    pos_in_list_j = np.max(np.where(nb_streamlines <= j)[0])
+                if (norm < 2*epsilon).all():
+                    set_j = np.max(np.where(nb_streamlines <= j)[0])
 
-                    # If it is an identical streamline, but from the same set
+                    # If it is an identical streamline, but from the first set,
                     # it needs to be removed, otherwise remove all instances
-                    if pos_in_list_j == 0:
-                        # If it is the first 'encounter' add it
+                    if set_j == 0:
+                        # If it is the first 'encounter', keep it.
+                        # Else, remove it.
                         if streamlines_to_keep[actual_ind].all():
-                            streamlines_to_keep[j] = not inversion_val
-                            average_match_distance.append(
-                                np.average(sub_vector, axis=0))
+                            streamlines_to_keep[j] = 0
                     else:
-                        streamlines_to_keep[actual_ind] = not inversion_val
-                        average_match_distance.append(
-                            np.average(sub_vector, axis=0))
-            else:
-                # 1) The streamline is 'identical'
-                if (norm < 2*epsilon).all():
-                    pos_in_list_i = np.max(np.where(nb_streamlines <= i)[0])
-                    pos_in_list_j = np.max(np.where(nb_streamlines <= j)[0])
+                        streamlines_to_keep[actual_ind] = 0
+            else:  # intersect_mode
+                # 1) Yourself is not a match
+                # 2) An equivalent streamline has not been selected by another
+                #    match.
+                # 3) The streamline is 'identical'
+                if (not np.any(streamlines_to_keep[actual_ind])
+                        and (norm < 2*epsilon).all()):
+                    set_j = np.max(np.where(nb_streamlines <= j)[0])
                     # If it is an identical streamline, but from the same set
-                    # it needs to be removed
-                    if i == j or pos_in_list_i != pos_in_list_j:
-                        intersect_test[pos_in_list_j] = True
-                    if i != j:
-                        average_match_distance.append(
-                            np.average(sub_vector, axis=0))
+                    # it needs to be removed (keeping streamlines_to_keep at 0)
+
+                    # Else: will be added only if it is found in all sets
+                    if set_i != set_j:
+                        intersect_test[set_j] = True
 
         # Verify that you actually found a match in each set
-        if (not union_mode or not difference_mode) and intersect_test.all():
-            streamlines_to_keep[i] = not inversion_val
+        if intersect_mode and intersect_test.all():
+            # Keeping only the first one; i.
+            streamlines_to_keep[i] = 1
 
     # To facilitate debugging and discovering shifts in data
     if average_match_distance:
         logging.info('Average matches distance: {}mm'.format(
             np.round(np.average(average_match_distance, axis=0), 5)))
     else:
         logging.info('No matches found.')
 
     return streamlines, np.where(streamlines_to_keep > 0)[0].astype(np.uint32)
 
 
 def concatenate_sft(sft_list, erase_metadata=False, metadata_fake_init=False):
     """ Concatenate a list of StatefulTractogram together """
+    if erase_metadata and metadata_fake_init:
+        raise ValueError("You cannot choose both erase_metadata and "
+                         "metadata_fake_init")
     if erase_metadata:
         sft_list[0].data_per_point = {}
         sft_list[0].data_per_streamline = {}
 
+    fused_sft = sft_list[0]
     for sft in sft_list[1:]:
         if erase_metadata:
             sft.data_per_point = {}
             sft.data_per_streamline = {}
         elif metadata_fake_init:
             for dps_key in list(sft.data_per_streamline.keys()):
                 if dps_key not in sft_list[0].data_per_streamline.keys():
@@ -368,75 +483,23 @@
         if not metadata_fake_init and \
                 not StatefulTractogram.are_compatible(sft, sft_list[0]):
             raise ValueError('Incompatible SFT, check space attributes and '
                              'data_per_point/streamlines.')
         elif not is_header_compatible(sft, sft_list[0]):
             raise ValueError('Incompatible SFT, check space attributes.')
 
-    total_streamlines = 0
-    total_points = 0
-    lengths = []
-    for sft in sft_list:
-        total_streamlines += len(sft.streamlines._offsets)
-        total_points += len(sft.streamlines._data)
-        lengths.extend(sft.streamlines._lengths)
-    lengths = np.array(lengths, dtype=np.uint32)
-    offsets = np.concatenate(([0], np.cumsum(lengths[:-1]))).astype(np.uint64)
-
-    dpp = {}
-    for dpp_key in sft_list[0].data_per_point.keys():
-        arr_seq_shape = list(sft_list[0].data_per_point[dpp_key]._data.shape)
-        arr_seq_shape[0] = total_points
-        dpp[dpp_key] = ArraySequence()
-        dpp[dpp_key]._data = np.zeros(arr_seq_shape)
-        dpp[dpp_key]._lengths = lengths
-        dpp[dpp_key]._offsets = offsets
-
-    dps = {}
-    for dps_key in sft_list[0].data_per_streamline.keys():
-        arr_seq_shape = list(sft_list[0].data_per_streamline[dps_key].shape)
-        arr_seq_shape[0] = total_streamlines
-        dps[dps_key] = np.zeros(arr_seq_shape)
-
-    streamlines = ArraySequence()
-    streamlines._data = np.zeros((total_points, 3))
-    streamlines._lengths = lengths
-    streamlines._offsets = offsets
-
-    pts_counter = 0
-    strs_counter = 0
-    for sft in sft_list:
-        pts_curr_len = len(sft.streamlines._data)
-        strs_curr_len = len(sft.streamlines._offsets)
-
-        if strs_curr_len == 0 or pts_curr_len == 0:
-            continue
-
-        streamlines._data[pts_counter:pts_counter+pts_curr_len] = \
-            sft.streamlines._data
-
-        for dpp_key in sft_list[0].data_per_point.keys():
-            dpp[dpp_key]._data[pts_counter:pts_counter+pts_curr_len] = \
-                sft.data_per_point[dpp_key]._data
-        for dps_key in sft_list[0].data_per_streamline.keys():
-            dps[dps_key][strs_counter:strs_counter+strs_curr_len] = \
-                sft.data_per_streamline[dps_key]
-        pts_counter += pts_curr_len
-        strs_counter += strs_curr_len
-
-    fused_sft = StatefulTractogram.from_sft(streamlines, sft_list[0],
-                                            data_per_point=dpp,
-                                            data_per_streamline=dps)
+        fused_sft += sft
+
     return fused_sft
 
 
 def transform_warp_sft(sft, linear_transfo, target, inverse=False,
                        reverse_op=False, deformation_data=None,
                        remove_invalid=True, cut_invalid=False):
-    """ Transform tractogram using a affine Subsequently apply a warp from
+    """ Transform tractogram using an affine Subsequently apply a warp from
     antsRegistration (optional).
     Remove/Cut invalid streamlines to preserve sft validity.
 
     Parameters
     ----------
     sft: StatefulTractogram
         Stateful tractogram object containing the streamlines to transform.
@@ -538,75 +601,118 @@
 
     new_sft.to_space(space)
     new_sft.to_origin(origin)
 
     return new_sft
 
 
-def upsample_tractogram(sft, nb, point_wise_std=None, streamline_wise_std=None,
-                        gaussian=None, spline=None, seed=None):
+def compress_streamlines_wrapper(tractogram, error_rate):
+    """
+    Compresses the streamlines of a tractogram.
+    Supports both nibabel.Tractogram dipy.StatefulTractogram
+    or list of streamlines.
+
+    Parameters
+    ----------
+    tractogram: TrkFile, TckFile, ArraySequence, list
+        The tractogram to compress.
+    error_rate: float
+        The maximum distance (in mm) for point displacement during compression.
+
+    Returns
+    -------
+    compressed_streamlines: list of np.ndarray
+        The compressed streamlines.
+    """
+    if isinstance(tractogram, (TrkFile, TckFile)):
+        return lambda: (compress_streamlines(
+            s, error_rate) for s in tractogram.streamlines)
+    else:
+        if hasattr(tractogram, 'streamlines'):
+            tractogram = tractogram.streamlines
+        return [compress_streamlines(
+            s, error_rate) for s in tractogram]
+
+
+def upsample_tractogram(sft, nb, point_wise_std=None, tube_radius=None,
+                        gaussian=None, error_rate=None, seed=None):
     """
     Generates new streamlines by either adding gaussian noise around
     streamlines' points, or by translating copies of existing streamlines
     by a random amount.
 
     Parameters
     ----------
     sft : StatefulTractogram
         The tractogram to upsample
     nb : int
         The target number of streamlines in the tractogram.
     point_wise_std : float
         The standard deviation of the gaussian to use to generate point-wise
         noise on the streamlines.
-    streamline_wise_std : float
-        The standard deviation of the gaussian to use to generate
-        streamline-wise noise on the streamlines.
+    tube_radius : float
+        The radius of the tube used to model the streamlines.
     gaussian: float
         The sigma used for smoothing streamlines.
+    error_rate : float
+        The maximum distance (in mm) to the original position of any point.
     spline: (float, int)
         Pair of sigma and number of control points used to model each
         streamline as a spline and smooth it.
     seed: int
         Seed for RNG.
 
     Returns
     -------
     new_sft : StatefulTractogram
         The upsampled tractogram.
     """
-    assert bool(point_wise_std) ^ bool(streamline_wise_std), \
-        'Can only add either point-wise or streamline-wise noise' + \
-        ', not both nor none.'
-
-    rng = np.random.RandomState(seed)
-
-    # Get the number of streamlines to add
-    nb_new = nb - len(sft.streamlines)
+    rng = np.random.default_rng(seed)
 
     # Get the streamlines that will serve as a base for new ones
-    indices = rng.choice(
-        len(sft.streamlines), nb_new)
-    new_streamlines = sft.streamlines.copy()
+    resampled_sft = resample_streamlines_step_size(sft, 1)
+    new_streamlines = []
+    indices = rng.choice(len(resampled_sft), nb, replace=True)
+    unique_indices, count = np.unique(indices, return_counts=True)
 
     # For all selected streamlines, add noise and smooth
-    for s in sft.streamlines[indices]:
-        if point_wise_std:
-            noise = rng.normal(scale=point_wise_std, size=s.shape)
-        else:  # streamline_wise_std
-            noise = rng.normal(scale=streamline_wise_std, size=s.shape[-1])
-        new_s = s + noise
+    for i, c in zip(unique_indices, count):
+        s = resampled_sft.streamlines[i]
+        if len(s) < 3:
+            new_streamlines.extend(np.repeat(s, c).tolist())
+        new_s = parallel_transport_streamline(s, c, tube_radius)
+
+        # Generate smooth noise_factor
+        noise = rng.normal(loc=0, scale=point_wise_std,
+                           size=len(s))
+
+        # Instead of generating random noise, we fit a polynomial to the
+        # noise and use it to generate a spatially smooth noise along the
+        # streamline (simply to avoid sharp changes in the noise factor).
+        x = np.arange(len(noise))
+        poly_coeffs = np.polyfit(x, noise, 3)
+        polynomial = Polynomial(poly_coeffs[::-1])
+        noise_factor = polynomial(x)
+
+        vec = s - new_s
+        vec /= np.linalg.norm(vec, axis=0)
+        new_s += vec * np.expand_dims(noise_factor, axis=1)
+
         if gaussian:
-            new_s = smooth_line_gaussian(new_s, gaussian)
-        elif spline:
-            new_s = smooth_line_spline(new_s, spline[0], spline[1])
+            new_s = [smooth_line_gaussian(s, gaussian) for s in new_s]
 
-        new_streamlines.append(new_s)
+        new_streamlines.extend(new_s)
 
-    new_sft = StatefulTractogram.from_sft(new_streamlines, sft)
+    if error_rate:
+        compressed_streamlines = compress_streamlines_wrapper(new_streamlines,
+                                                              error_rate)
+    else:
+        compressed_streamlines = new_streamlines
+
+    new_sft = StatefulTractogram.from_sft(compressed_streamlines, sft)
     return new_sft
 
 
 def split_sft_sequentially(orig_sft, chunk_sizes):
     """
     Divides a stateful tractogram into n sub-tractograms of sizes defined by
     chunk_sizes. Streamlines are separated sequentially from the initial
@@ -734,19 +840,20 @@
         raise ValueError("You asked for more streamlines than are available.")
 
     # Percent of streamlines to keep per chunk.
     nb_chunks = len(chunk_sizes)
     percent_kept_per_chunk = [nb / len(orig_sft) for nb in chunk_sizes]
 
     logging.debug("Computing QBx")
+    rng = np.random.RandomState(seed)
     clusters = qbx_and_merge(orig_sft.streamlines, thresholds, nb_pts=20,
-                             verbose=False)
+                             verbose=False, rng=rng)
 
-    logging.debug("Done. Now getting list of indices in each of the {} "
-                  "cluster.".format(len(clusters)))
+    logging.info("Done. Now getting list of indices in each of the {} "
+                 "cluster.".format(len(clusters)))
     total_indices = [[] for _ in range(nb_chunks + 1)]
     for cluster in clusters:
         if len(cluster.indices) > 1:
             cluster_sft = orig_sft[cluster.indices]
             size_cluster = len(cluster.indices)
             chunk_sizes_in_cluster = \
                 [round(p * size_cluster) for p in percent_kept_per_chunk]
@@ -766,7 +873,19 @@
                 chunk_orig_inds = [cluster.indices[ind] for ind in
                                    all_chunks_inds_in_cluster[i]]
                 total_indices[i].extend(chunk_orig_inds)
 
     final_sfts = [orig_sft[inds] for inds in total_indices]
 
     return final_sfts
+
+
+OPERATIONS = {
+    'difference_robust': difference_robust,
+    'intersection_robust': intersection_robust,
+    'union_robust': union_robust,
+    'difference': difference,
+    'intersection': intersection,
+    'union': union,
+    'concatenate': 'concatenate',
+    'lazy_concatenate': 'lazy_concatenate'
+}
```

### Comparing `scilpy-1.5.post2/scilpy/utils/filenames.py` & `scilpy-2.0.0/scilpy/utils/filenames.py`

 * *Files identical despite different names*

### Comparing `scilpy-1.5.post2/scilpy/utils/metrics_tools.py` & `scilpy-2.0.0/scilpy/utils/metrics_tools.py`

 * *Files 6% similar despite different names*

```diff
@@ -153,17 +153,18 @@
 
     Returns
     ---------
     stats : tuple
         a tuple containing the mean and standard deviation of the data
     """
 
-    masked_data = np.ma.masked_array(data, np.isnan(data))
-    mean = np.average(masked_data, weights=weights)
-    variance = np.average((masked_data-mean)**2, weights=weights)
+    masked_data = np.ma.masked_array(data, np.logical_or(np.isnan(data),
+                                                         np.isinf(data)))
+    mean = np.ma.average(masked_data, weights=weights)
+    variance = np.ma.average((masked_data-mean)**2, weights=weights)
 
     return mean, np.sqrt(variance)
 
 
 def get_bundle_metrics_mean_std(streamlines, metrics_files,
                                 distance_values, correlation_values,
                                 density_weighting=True):
@@ -230,17 +231,14 @@
     distance_values: np.ndarray
         List of distances obtained with scil_compute_bundle_voxel_label_map.py
     correlation_values: np.ndarray
         List of correlations obtained with scil_compute_bundle_voxel_label_map.py
     density_weighting: bool
         If true, weight statistics by the number of streamlines passing through
         each voxel. [False]
-    distance_weighting: bool
-        If true, weight statistics by the inverse of the distance between a
-        streamline and the centroid.
 
     Returns
     -------
     stats
     """
     # Computing infos on bundle
     unique_labels = np.unique(labels)[1:]
@@ -310,17 +308,17 @@
                        ylabel=None, figlabel=None, fill_color=None,
                        display_means=False):
     """
     Plots the mean of a metric along n points with the standard deviation.
 
     Parameters
     ----------
-    mean: Numpy 1D (or 2D) array of size n
+    means: Numpy 1D (or 2D) array of size n
         Mean of the metric along n points.
-    std: Numpy 1D (or 2D) array of size n
+    stds: Numpy 1D (or 2D) array of size n
         Standard deviation of the metric along n points.
     title: string
         Title of the figure.
     xlabel: string
         Label of the X axis.
     ylabel: string
         Label of the Y axis (suggestion: the metric name).
@@ -375,34 +373,7 @@
 
     # Plot the std
     plt.fill_between(dim, mean - std, mean + std,
                      facecolor=fill_color, alpha=alpha)
 
     plt.close(fig)
     return fig
-
-
-def get_roi_metrics_mean_std(density_map, metrics_files):
-    """
-    Returns the mean and standard deviation of each metric, using the
-    provided density map. This can be a binary mask,
-    or contain weighted values between 0 and 1.
-
-    Parameters
-    ------------
-    density_map : ndarray
-        3D numpy array containing a density map.
-    metrics_files : sequence
-        list of nibabel objects representing the metrics files.
-
-    Returns
-    ---------
-    stats : list
-        list of tuples where the first element of the tuple is the mean
-        of a metric, and the second element is the standard deviation.
-
-    """
-
-    return map(lambda metric_file:
-               weighted_mean_std(density_map,
-                                 metric_file.get_fdata(dtype=np.float64)),
-               metrics_files)
```

### Comparing `scilpy-1.5.post2/scilpy/utils/streamlines.py` & `scilpy-2.0.0/scilpy/tractograms/dps_and_dpp_management.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,342 +1,368 @@
 # -*- coding: utf-8 -*-
-import copy
-import logging
-
-from dipy.io.stateful_tractogram import StatefulTractogram
-from dipy.tracking.streamline import set_number_of_points
-from dipy.tracking.streamlinespeed import compress_streamlines
 import numpy as np
-from scipy.spatial import cKDTree
-from sklearn.cluster import KMeans
 
-from scilpy.tractanalysis.features import get_streamlines_centroid
+from scilpy.viz.color import clip_and_normalize_data_for_cmap
 
-from scilpy.viz.utils import get_colormap
 
+def add_data_as_color_dpp(sft, cmap, data, clip_outliers=False, min_range=None,
+                          max_range=None, min_cmap=None, max_cmap=None,
+                          log=False, LUT=None):
+    """
+    Normalizes data between 0 and 1 for an easier management with colormaps.
+    The real lower bound and upperbound are returned.
 
-def uniformize_bundle_sft(sft, axis=None, ref_bundle=None, swap=False):
-    """Uniformize the streamlines in the given tractogram.
+    Data can be clipped to (min_range, max_range) before normalization.
+    Alternatively, data can be kept as is, but the colormap be fixed to
+    (min_cmap, max_cmap).
 
     Parameters
     ----------
     sft: StatefulTractogram
-         The tractogram that contains the list of streamlines to be uniformized
-    axis: int, optional
-        Orient endpoints in the given axis
-    ref_bundle: streamlines
-        Orient endpoints the same way as this bundle (or centroid)
-    swap: boolean, optional
-        Swap the orientation of streamlines
-    """
-    from scilpy.tractanalysis.reproducibility_measures import \
-        get_endpoints_density_map
-    old_space = sft.space
-    old_origin = sft.origin
-    sft.to_vox()
-    sft.to_corner()
-    density = get_endpoints_density_map(sft.streamlines, sft.dimensions,
-                                        point_to_select=3)
-    indices = np.argwhere(density > 0)
-    kmeans = KMeans(n_clusters=2, random_state=0, copy_x=True,
-                    n_init=20).fit(indices)
-
-    labels = np.zeros(density.shape)
-    for i in range(len(kmeans.labels_)):
-        labels[tuple(indices[i])] = kmeans.labels_[i]+1
-
-    k_means_centers = kmeans.cluster_centers_
-    main_dir_barycenter = np.argmax(
-        np.abs(k_means_centers[0] - k_means_centers[-1]))
-
-    if len(sft.streamlines) > 0:
-        axis_name = ['x', 'y', 'z']
-        if axis is None or ref_bundle is not None:
-            if ref_bundle is not None:
-                ref_bundle.to_vox()
-                ref_bundle.to_corner()
-                centroid = get_streamlines_centroid(ref_bundle.streamlines,
-                                                    20)[0]
-            else:
-                centroid = get_streamlines_centroid(sft.streamlines, 20)[0]
-            main_dir_ends = np.argmax(np.abs(centroid[0] - centroid[-1]))
-            main_dir_displacement = np.argmax(
-                np.abs(np.sum(np.gradient(centroid, axis=0), axis=0)))
-
-            if main_dir_displacement != main_dir_ends \
-                    or main_dir_displacement != main_dir_barycenter:
-                logging.info('Ambiguity in orientation, you should use --axis')
-            axis = axis_name[main_dir_displacement]
-        logging.info('Orienting endpoints in the {} axis'.format(axis))
-        axis_pos = axis_name.index(axis)
-
-        if bool(k_means_centers[0][axis_pos] >
-                k_means_centers[1][axis_pos]) ^ bool(swap):
-            labels[labels == 1] = 3
-            labels[labels == 2] = 1
-            labels[labels == 3] = 2
-
-        for i in range(len(sft.streamlines)):
-            if ref_bundle:
-                res_centroid = set_number_of_points(centroid, 20)
-                res_streamlines = set_number_of_points(sft.streamlines[i], 20)
-                norm_direct = np.sum(
-                    np.linalg.norm(res_centroid - res_streamlines, axis=0))
-                norm_flip = np.sum(
-                    np.linalg.norm(res_centroid - res_streamlines[::-1], axis=0))
-                if bool(norm_direct > norm_flip) ^ bool(swap):
-                    sft.streamlines[i] = sft.streamlines[i][::-1]
-                    for key in sft.data_per_point[i]:
-                        sft.data_per_point[key][i] = \
-                            sft.data_per_point[key][i][::-1]
-            else:
-                # Bitwise XOR
-                if bool(labels[tuple(sft.streamlines[i][0].astype(int))] >
-                        labels[tuple(sft.streamlines[i][-1].astype(int))]) ^ bool(swap):
-                    sft.streamlines[i] = sft.streamlines[i][::-1]
-                    for key in sft.data_per_point[i]:
-                        sft.data_per_point[key][i] = \
-                            sft.data_per_point[key][i][::-1]
-    sft.to_space(old_space)
-    sft.to_origin(old_origin)
-
-
-def uniformize_bundle_sft_using_mask(sft, mask, swap=False):
-    """Uniformize the streamlines in the given tractogram so head is closer to
-    to a region of interest.
+        The tractogram
+    cmap: plt colormap
+        The colormap. Ex, see scilpy.viz.utils.get_colormap().
+    data: np.ndarray or list[list] or list[np.ndarray]
+        The data to convert to color. Expecting one value per point to add as
+        dpp. If instead data has one value per streamline, setting the same
+        color to all points of the streamline (as dpp).
+        Either a vector numpy array (all streamlines concatenated), or a list
+        of arrays per streamline.
+    clip_outliers: bool
+        See description of the following parameters in
+        clip_and_normalize_data_for_cmap.
+    min_range: float
+        Data values below min_range will be clipped.
+    max_range: float
+        Data values above max_range will be clipped.
+    min_cmap: float
+        Minimum value of the colormap. Most useful when min_range and max_range
+        are not set; to fix the colormap range without modifying the data.
+    max_cmap: float
+        Maximum value of the colormap. Idem.
+    log: bool
+        If True, apply a logarithmic scale to the data.
+    LUT: np.ndarray
+        If set, replaces the data values by the Look-Up Table values. In order,
+        the first value of the LUT is set everywhere where data==1, etc.
+
+    Returns
+    -------
+    sft: StatefulTractogram
+        The tractogram, with dpp 'color' added.
+    lbound: float
+        The lower bound of the associated colormap.
+    ubound: float
+        The upper bound of the associated colormap.
+    """
+    # If data is a list of lists, merge.
+    if isinstance(data[0], list) or isinstance(data[0], np.ndarray):
+        data = np.hstack(data)
+
+    values, lbound, ubound = clip_and_normalize_data_for_cmap(
+        data, clip_outliers, min_range, max_range,
+        min_cmap, max_cmap, log, LUT)
+
+    # Important: values are in float after clip_and_normalize.
+    color = np.asarray(cmap(values)[:, 0:3]) * 255
+    if len(color) == len(sft):
+        tmp = [np.tile([color[i][0], color[i][1], color[i][2]],
+                       (len(sft.streamlines[i]), 1))
+               for i in range(len(sft.streamlines))]
+        sft.data_per_point['color'] = tmp
+    elif len(color) == len(sft.streamlines._data):
+        sft.data_per_point['color'] = sft.streamlines
+        sft.data_per_point['color']._data = color
+    else:
+        raise ValueError("Error in the code... Colors do not have the right "
+                         "shape. Expecting either one color per streamline "
+                         "({}) or one per point ({}) but got {}."
+                         .format(len(sft), len(sft.streamlines._data),
+                                 len(color)))
+    return sft, lbound, ubound
+
+
+def convert_dps_to_dpp(sft, keys, overwrite=False):
+    """
+    Copy the value of the data_per_streamline to each point of the
+    streamline, as data_per_point. The dps key is removed and added as dpp key.
 
     Parameters
     ----------
     sft: StatefulTractogram
-         The tractogram that contains the list of streamlines to be uniformized
-    mask: np.ndarray
-        Mask to use as a reference for the ROI.
-    swap: boolean, optional
-        Swap the orientation of streamlines
+    keys: str or List[str], optional
+        The list of dps keys to convert to dpp.
+    overwrite: bool
+        If true, allow continuing even if the key already existed as dpp.
     """
+    if isinstance(keys, str):
+        keys = [keys]
 
-    # barycenter = np.average(np.argwhere(mask), axis=0)
-    old_space = sft.space
-    old_origin = sft.origin
-    sft.to_vox()
-    sft.to_corner()
+    for key in keys:
+        if key not in sft.data_per_streamline:
+            raise ValueError(
+                "Dps key {} not found! Existing dps keys: {}"
+                .format(key, list(sft.data_per_streamline.keys())))
+        if key in sft.data_per_point and not overwrite:
+            raise ValueError("Dpp key {} already existed. Please allow "
+                             "overwriting.".format(key))
+        sft.data_per_point[key] = [[val]*len(s) for val, s in
+                                   zip(sft.data_per_streamline[key],
+                                       sft.streamlines)]
+        del sft.data_per_streamline[key]
 
-    tree = cKDTree(np.argwhere(mask))
-    for i in range(len(sft.streamlines)):
-        head_dist = tree.query(sft.streamlines[i][0])[0]
-        tail_dist = tree.query(sft.streamlines[i][-1])[0]
-        if bool(head_dist > tail_dist) ^ bool(swap):
-            sft.streamlines[i] = sft.streamlines[i][::-1]
-            for key in sft.data_per_point[i]:
-                sft.data_per_point[key][i] = \
-                    sft.data_per_point[key][i][::-1]
+    return sft
 
-    sft.to_space(old_space)
-    sft.to_origin(old_origin)
 
-def get_color_streamlines_along_length(sft, colormap='jet'):
-    """Color streamlines according to their length.
+def project_map_to_streamlines(sft, map_volume, endpoints_only=False):
+    """
+    Projects a map onto the points of streamlines. The result is a
+    data_per_point.
 
     Parameters
     ----------
     sft: StatefulTractogram
-        The tractogram that contains the list of streamlines to be colored
-    colormap: str
-        The colormap to use.
+        Input tractogram.
+    map_volume: DataVolume
+        Input map.
+    endpoints_only: bool, optional
+        If True, will only project the map_volume onto the endpoints of the
+        streamlines (all values along streamlines set to NaN). If False,
+        will project the map_volume onto all points of the streamlines.
 
     Returns
     -------
-    color: np.ndarray
-        An array of shape (nb_streamlines, 3) containing the RGB values of
-        streamlines
-
+    streamline_data: List[List]
+        The values that could now be associated to a data_per_point key.
+        The map_volume projected to each point of the streamlines.
     """
-    cmap = get_colormap(colormap)
-    color_dpp = copy.deepcopy(sft.streamlines)
-
-    for i in range(len(sft.streamlines)):
-        color_dpp[i] = cmap(np.linspace(0, 1, len(sft.streamlines[i])))[
-            :, 0:3] * 255
-
-    return color_dpp._data
-
+    if len(map_volume.data.shape) == 4:
+        dimension = map_volume.data.shape[3]
+    else:
+        dimension = 1
+
+    streamline_data = []
+    if endpoints_only:
+        for s in sft.streamlines:
+            p1_data = map_volume.get_value_at_coordinate(
+                s[0][0], s[0][1], s[0][2],
+                space=sft.space, origin=sft.origin)
+            p2_data = map_volume.get_value_at_coordinate(
+                s[-1][0], s[-1][1], s[-1][2],
+                space=sft.space, origin=sft.origin)
+
+            thisstreamline_data = np.ones((len(s), dimension)) * np.nan
+
+            thisstreamline_data[0] = p1_data
+            thisstreamline_data[-1] = p2_data
+            thisstreamline_data = np.asarray(thisstreamline_data)
+
+            streamline_data.append(
+                np.reshape(thisstreamline_data,
+                           (len(thisstreamline_data), dimension)))
+    else:
+        # toDo: there is a double loop. Could be faster?
+        for s in sft.streamlines:
+            thisstreamline_data = []
+            for p in s:
+                thisstreamline_data.append(map_volume.get_value_at_coordinate(
+                    p[0], p[1], p[2], space=sft.space, origin=sft.origin))
+
+            streamline_data.append(
+                np.reshape(thisstreamline_data,
+                           (len(thisstreamline_data), dimension)))
 
-def filter_tractogram_data(tractogram, streamline_ids):
-    """ Filter tractogram according to streamline ids and keep the data
+    return streamline_data
 
-    Parameters:
-    -----------
-    tractogram: StatefulTractogram
-        Tractogram containing the data to be filtered
-    streamline_ids: array_like
-        List of streamline ids the data corresponds to
 
-    Returns:
-    --------
-    new_tractogram: Tractogram or StatefulTractogram
-        Returns a new tractogram with only the selected streamlines
-        and data
+def project_dpp_to_map(sft, dpp_key, sum_lines=False, endpoints_only=False):
     """
+    Saves the values of data_per_point keys to the underlying voxels. Averages
+    the values of various streamlines in each voxel. Returns one map per key.
+    The streamlines are not preprocessed here. You should probably first
+    uncompress your streamlines to have smoother maps.
+    Note: If a streamline has two points in the same voxel, it counts twice!
 
-    streamline_ids = np.asarray(streamline_ids, dtype=int)
-
-    assert np.all(
-        np.in1d(streamline_ids, np.arange(len(tractogram.streamlines)))
-    ), "Received ids outside of streamline range"
-
-    new_streamlines = tractogram.streamlines[streamline_ids]
-    new_data_per_streamline = tractogram.data_per_streamline[streamline_ids]
-    new_data_per_point = tractogram.data_per_point[streamline_ids]
+    Parameters
+    ----------
+    sft: StatefulTractogram
+        The tractogram
+    dpp_key: str
+        The data_per_point key to project to a map.
+    sum_lines: bool
+        Do not average values of streamlines that cross a same voxel; sum them
+        instead.
+    endpoints_only: bool
+        If true, only project the streamline's endpoints.
 
-    # Could have been nice to deepcopy the tractogram modify the attributes in
-    # place instead of creating a new one, but tractograms cant be subsampled
-    # if they have data
+    Returns
+    -------
+    the_map: np.ndarray
+        The 3D resulting map.
+    """
+    sft.to_vox()
 
-    return StatefulTractogram.from_sft(
-        new_streamlines,
-        tractogram,
-        data_per_point=new_data_per_point,
-        data_per_streamline=new_data_per_streamline)
+    # Using to_corner, if we simply floor the coordinates of the point, we find
+    # the voxel where it is.
+    sft.to_corner()
 
+    # count: could also use compute_tract_counts_map.
+    count = np.zeros(sft.dimensions)
+    the_map = np.zeros(sft.dimensions)
+    for s in range(len(sft)):
+        if endpoints_only:
+            points = [0, -1]
+        else:
+            points = range(len(sft.streamlines[s]))
 
-def compress_sft(sft, tol_error=0.01):
-    """ Compress a stateful tractogram. Uses Dipy's compress_streamlines, but
-    deals with space better.
+        for p in points:
+            x, y, z = sft.streamlines[s][p, :].astype(int)  # Or floor
+            count[x, y, z] += 1
+            the_map[x, y, z] += sft.data_per_point[dpp_key][s][p]
 
-    Dipy's description:
-    The compression consists in merging consecutive segments that are
-    nearly collinear. The merging is achieved by removing the point the two
-    segments have in common.
+    if not sum_lines:
+        count = np.maximum(count, 1e-6)  # Avoid division by 0
+        the_map /= count
 
-    The linearization process [Presseau15]_ ensures that every point being
-    removed are within a certain margin (in mm) of the resulting streamline.
-    Recommendations for setting this margin can be found in [Presseau15]_
-    (in which they called it tolerance error).
+    return the_map
 
-    The compression also ensures that two consecutive points won't be too far
-    from each other (precisely less or equal than `max_segment_length`mm).
-    This is a tradeoff to speed up the linearization process [Rheault15]_. A
-    low value will result in a faster linearization but low compression,
-    whereas a high value will result in a slower linearization but high
-    compression.
 
-    [Presseau C. et al., A new compression format for fiber tracking datasets,
-    NeuroImage, no 109, 73-83, 2015.]
+def perform_operation_on_dpp(op_name, sft, dpp_name, endpoints_only=False):
+    """
+    Peforms an operation on the data per point for all streamlines (mean, sum,
+    min, max). The operation is applied on each point invidiually, and thus
+    makes sense if the data_per_point at each point is a vector. The result is
+    a new data_per_point.
 
     Parameters
     ----------
+    op_name: str
+        Name of one possible operation (mean, sum, min, max). Will refer to a
+        callable that takes a list of streamline data per point (4D) and
+        returns a list of streamline data per point.
     sft: StatefulTractogram
-        The sft to compress.
-    tol_error: float (optional)
-        Tolerance error in mm (default: 0.01). A rule of thumb is to set it
-        to 0.01mm for deterministic streamlines and 0.1mm for probabilitic
-        streamlines.
+        The streamlines used in the operation.
+    dpp_name: str
+        The name of the data per point to be used in the operation.
+        sft.data_per_point[dpp_name][s] must be a 2D vector: (N, M)
+        with s, a given streamline; N the number of points; M the number of
+        features in the dpp.
+    endpoints_only: bool
+        If True, will only perform operation on endpoints. Values at other
+        points will be set to NaN.
 
     Returns
     -------
-    compressed_sft : StatefulTractogram
+    new_data_per_point: list[np.ndarray]
+        The values that could now be associated to a new data_per_point key.
     """
-    # Go to world space
-    orig_space = sft.space
-    sft.to_rasmm()
+    call_op = OPERATIONS[op_name]
+    if endpoints_only:
+        new_data_per_point = []
+        for s in sft.data_per_point[dpp_name]:
+            this_data_per_point = np.nan * np.ones((len(s), 1))
+            this_data_per_point[0] = call_op(s[0])
+            this_data_per_point[-1] = call_op(s[-1])
+            new_data_per_point.append(np.asarray(this_data_per_point)[:, None])
+    else:
+        new_data_per_point = []
+        for s in sft.data_per_point[dpp_name]:
+            this_data_per_point = []
+            for p in s:
+                this_data_per_point.append(call_op(p))
+            new_data_per_point.append(np.asarray(this_data_per_point)[:, None])
 
-    # Compress streamlines
-    compressed_streamlines = compress_streamlines(sft.streamlines,
-                                                  tol_error=tol_error)
-    if sft.data_per_point is not None and sft.data_per_point.keys():
-        logging.warning("Initial StatefulTractogram contained data_per_point. "
-                        "This information will not be carried in the final "
-                        "tractogram.")
+    return new_data_per_point
 
-    compressed_sft = StatefulTractogram.from_sft(
-        compressed_streamlines, sft,
-        data_per_streamline=sft.data_per_streamline)
 
-    # Return to original space
-    compressed_sft.to_space(orig_space)
+def perform_operation_dpp_to_dps(op_name, sft, dpp_name, endpoints_only=False):
+    """
+    Converts dpp to dps, using a chosen operation.
 
-    return compressed_sft
+    Performs an operation across all data_per_points for each streamline (mean,
+    sum, min, max). The result is a data_per_streamline.
 
+    If the data_per_point at each point is a vector, operation is done on each
+    feature individually. The data_per_streamline will have the same shape.
 
-def cut_invalid_streamlines(sft):
-    """ Cut streamlines so their longest segment are within the bounding box.
-    This function keeps the data_per_point and data_per_streamline.
+    Parameters
+    ----------
+    op_name: str
+        A callable that takes a list of streamline data per streamline and
+        returns a list of data per streamline.
+    sft: StatefulTractogram
+        The streamlines used in the operation.
+    dpp_name: str
+        The name of the data per point to be used in the operation.
+    endpoints_only: bool
+        If True, will only perform operation on endpoints. Other points will be
+        ignored in the operation.
+
+    Returns
+    -------
+    new_data_per_streamline: list
+        The values that could now be associated to a new data_per_streamline
+        key.
+    """
+    call_op = OPERATIONS[op_name]
+    if endpoints_only:
+        new_data_per_streamline = []
+        for s in sft.data_per_point[dpp_name]:
+            start = s[0]
+            end = s[-1]
+            concat = np.concatenate((start[:], end[:]))
+            new_data_per_streamline.append(call_op(concat))
+    else:
+        new_data_per_streamline = []
+        for s in sft.data_per_point[dpp_name]:
+            s_np = np.asarray(s)
+            new_data_per_streamline.append(call_op(s_np))
+
+    return new_data_per_streamline
+
+
+def perform_correlation_on_endpoints(sft, dpp_name='metric'):
+    """Peforms correlation across endpoints for each streamline. The
+    data_per_point at each point must be a vector.
 
     Parameters
     ----------
     sft: StatefulTractogram
-        The sft to remove invalid points from.
+        The streamlines used in the operation.
+    dpp_name: str
+        The name of the data per point to be used in the operation.
 
     Returns
     -------
-    new_sft : StatefulTractogram
-        New object with the invalid points removed from each streamline.
-    cutting_counter : int
-        Number of streamlines that were cut.
-    """
-    if not len(sft):
-        return sft, 0
-
-    # Keep track of the streamlines' original space/origin
-    space = sft.space
-    origin = sft.origin
+    new_data_per_streamline: List
+        The correlation values that could now be associated to a new
+        data_per_streamline key.
+    """
+    new_data_per_streamline = []
+    for s in sft.data_per_point[dpp_name]:
+        new_data_per_streamline.append(np.corrcoef(s[0], s[-1])[0, 1])
 
-    sft.to_vox()
-    sft.to_corner()
+    return new_data_per_streamline
 
-    copy_sft = copy.deepcopy(sft)
-    epsilon = 0.001
-    indices_to_remove, _ = copy_sft.remove_invalid_streamlines()
-
-    new_streamlines = []
-    new_data_per_point = {}
-    new_data_per_streamline = {}
-    for key in sft.data_per_point.keys():
-        new_data_per_point[key] = []
-    for key in sft.data_per_streamline.keys():
-        new_data_per_streamline[key] = []
-
-    cutting_counter = 0
-    for ind in range(len(sft.streamlines)):
-        # No reason to try to cut if all points are within the volume
-        if ind in indices_to_remove:
-            best_pos = [0, 0]
-            cur_pos = [0, 0]
-            for pos, point in enumerate(sft.streamlines[ind]):
-                if (point < epsilon).any() or \
-                        (point >= sft.dimensions - epsilon).any():
-                    cur_pos = [pos+1, pos+1]
-                if cur_pos[1] - cur_pos[0] > best_pos[1] - best_pos[0]:
-                    best_pos = cur_pos
-                cur_pos[1] += 1
-
-            if not best_pos == [0, 0]:
-                new_streamlines.append(
-                    sft.streamlines[ind][best_pos[0]:best_pos[1]-1])
-                cutting_counter += 1
-                for key in sft.data_per_streamline.keys():
-                    new_data_per_streamline[key].append(
-                        sft.data_per_streamline[key][ind])
-                for key in sft.data_per_point.keys():
-                    new_data_per_point[key].append(
-                        sft.data_per_point[key][ind][best_pos[0]:best_pos[1]-1])
-            else:
-                logging.warning('Streamlines entirely out of the volume.')
-        else:
-            new_streamlines.append(sft.streamlines[ind])
-            for key in sft.data_per_streamline.keys():
-                new_data_per_streamline[key].append(
-                    sft.data_per_streamline[key][ind])
-            for key in sft.data_per_point.keys():
-                new_data_per_point[key].append(sft.data_per_point[key][ind])
-    new_sft = StatefulTractogram.from_sft(new_streamlines, sft,
-                                          data_per_streamline=new_data_per_streamline,
-                                          data_per_point=new_data_per_point)
-
-    # Move the streamlines back to the original space/origin
-    sft.to_space(space)
-    sft.to_origin(origin)
 
-    new_sft.to_space(space)
-    new_sft.to_origin(origin)
+def _stream_mean(array):
+    return np.squeeze(np.mean(array, axis=0))
+
+
+def _stream_sum(array):
+    return np.squeeze(np.sum(array, axis=0))
+
+
+def _stream_min(array):
+    return np.squeeze(np.min(array, axis=0))
+
+
+def _stream_max(array):
+    return np.squeeze(np.max(array, axis=0))
+
 
-    return new_sft, cutting_counter
+OPERATIONS = {
+    'mean': _stream_mean,
+    'sum': _stream_sum,
+    'min': _stream_min,
+    'max': _stream_max,
+}
```

### Comparing `scilpy-1.5.post2/scilpy/version.py` & `scilpy-2.0.0/scilpy/version.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,40 +1,35 @@
 # -*- coding: utf-8 -*-
 
+import itertools
 import glob
+import os
 
 # Format expected by setup.py and doc/source/conf.py: string of form "X.Y.Z"
-_version_major = 1
-_version_minor = 5
+_version_major = 2
+_version_minor = 0
 _version_micro = 0
-_version_extra = 'post2'
+_version_extra = ''
 
 # Construct full version string from these.
-_ver = [_version_major, _version_minor]
-if _version_micro:
-    _ver.append(_version_micro)
+_ver = [_version_major, _version_minor, _version_micro]
+
 if _version_extra:
     _ver.append(_version_extra)
 
 __version__ = '.'.join(map(str, _ver))
 
 CLASSIFIERS = ["Development Status :: 3 - Alpha",
                "Environment :: Console",
                "Intended Audience :: Science/Research",
                "License :: OSI Approved :: MIT License",
                "Operating System :: OS Independent",
                "Programming Language :: Python",
                "Topic :: Scientific/Engineering"]
 
-PYTHON_VERSION = ""
-with open('.python-version') as f:
-    _ = f.readline().strip("\n") # Skip best python target, we are
-                                 # interested in the versions range
-    PYTHON_VERSION = f.readline().strip("\n")
-
 # Description should be a one-liner:
 description = "Scilpy: diffusion MRI tools and utilities"
 # Long description will go up on the pypi page
 long_description = """
 Scilpy
 ========
 Scilpy is a small library mainly containing small tools and utilities
@@ -66,10 +61,13 @@
 AUTHOR = "The SCIL developers"
 AUTHOR_EMAIL = ""
 PLATFORMS = "OS Independent"
 MAJOR = _version_major
 MINOR = _version_minor
 MICRO = _version_micro
 VERSION = __version__
-SCRIPTS = glob.glob("scripts/*.py")
+LEGACY_SCRIPTS = filter(lambda s: not os.path.basename(s) == "__init__.py",
+                        glob.glob("scripts/legacy/*.py"))
+SCRIPTS = filter(lambda s: not os.path.basename(s) == "__init__.py",
+                 glob.glob("scripts/*.py"))
 
-PREVIOUS_MAINTAINERS=["Jean-Christophe Houde"]
+PREVIOUS_MAINTAINERS = ["Jean-Christophe Houde"]
```

### Comparing `scilpy-1.5.post2/scilpy/viz/chord_chart.py` & `scilpy-2.0.0/scilpy/viz/legacy/chord_chart.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,21 +1,21 @@
 # -*- coding: utf-8 -*-
 
 """
 Most code here is from https://github.com/fengwangPhysics/matplotlib-chord-diagram.
 It was adapted to our specific needs: size of matrix, order of magnitude of
 max/min values, alpha for visualisation, etc.
-"""
+""" # noqa
 
 import math
 from matplotlib.path import Path
 import matplotlib.patches as patches
 import numpy as np
 
-from scilpy.viz.utils import get_colormap
+from scilpy.viz.color import get_lookup_table
 
 
 def polar2xy(r, theta):
     """
     Plot each shell
 
     Parameters
@@ -183,38 +183,39 @@
         ax.add_patch(patch)
 
 
 def chordDiagram(X, ax, colors=None, width=0.1, pad=2, chordwidth=0.7,
                  angle_threshold=1, alpha=0.1, text_dist=1.1,
                  colormap='plasma'):
     """Plot a chord diagram
+
     Parameters
     ----------
-    X :
+    X:
         flux data, X[i, j] is the flux from i to j
-    ax :
+    ax:
         matplotlib `axes` to show the plot
-    colors : optional
+    colors: optional
         user defined colors in rgb format.
-    width : optional
+    width: optional
         width/thickness of the ideogram arc
-    pad : optional
+    pad: optional
         gap pad between two neighboring ideogram arcs, unit: degree,
         default: 2 degree
-    chordwidth : optional
+    chordwidth: optional
         position of the control points for the chords,
         controlling the shape of the chords
     """
     # X[i, j]:  i -> j
     x = X.sum(axis=1)  # sum over rows
     ax.set_xlim(-text_dist, text_dist)
     ax.set_ylim(-text_dist, text_dist)
 
     if colors is None:
-        cmap = get_colormap(colormap)
+        cmap = get_lookup_table(colormap)
         colors = [cmap(i)[0:3] for i in np.linspace(0, 1, len(x))]
 
     # find position for each start and end
     y = x/np.sum(x).astype(float) * (360 - pad*len(x))
 
     pos = {}
     arc = []
```

### Comparing `scilpy-1.5.post2/scilpy/viz/gradient_sampling.py` & `scilpy-2.0.0/scilpy/viz/gradients.py`

 * *Files 12% similar despite different names*

```diff
@@ -2,28 +2,18 @@
 
 import logging
 import numpy as np
 from tempfile import mkstemp
 
 from dipy.data import get_sphere
 from fury import actor, window
-import fury
 
-from scilpy.io.utils import snapshot
-
-vtkcolors = [window.colors.blue,
-             window.colors.red,
-             window.colors.yellow,
-             window.colors.purple,
-             window.colors.cyan,
-             window.colors.green,
-             window.colors.orange,
-             window.colors.white,
-             window.colors.brown,
-             window.colors.grey]
+from scilpy.viz.color import generate_n_colors
+from scilpy.viz.backends.fury import snapshot_scenes
+from scilpy.viz.screenshot import compose_image
 
 
 def plot_each_shell(ms, centroids, plot_sym_vecs=True, use_sphere=True,
                     same_color=False, rad=0.025, opacity=1.0, ofile=None,
                     ores=(300, 300)):
     """
     Plot each shell
@@ -44,26 +34,22 @@
         radius of each point
     opacity: float
         opacity for the shells
     ofile: str
         output filename
     ores: tuple
         resolution of the output png
-
-    Return
-    ------
     """
-    global vtkcolors
-    if len(ms) > 10:
-        vtkcolors = fury.colormap.distinguishable_colormap(nb_colors=len(ms))
+
+    _colors = generate_n_colors(len(ms))
 
     if use_sphere:
         sphere = get_sphere('symmetric724')
         shape = (1, 1, 1, sphere.vertices.shape[0])
-        fid, fname = mkstemp(suffix='_odf_slicer.mmap')
+        _, fname = mkstemp(suffix='_odf_slicer.mmap')
         odfs = np.memmap(fname, dtype=np.float64, mode='w+', shape=shape)
         odfs[:] = 1
         odfs[..., 0] = 1
         affine = np.eye(4)
 
     for i, shell in enumerate(ms):
         logging.info('Showing shell {}'.format(int(centroids[i])))
@@ -72,24 +58,28 @@
         scene = window.Scene()
         scene.SetBackground(1, 1, 1)
         if use_sphere:
             sphere_actor = actor.odf_slicer(odfs, affine, sphere=sphere,
                                             colormap='winter', scale=1.0,
                                             opacity=opacity)
             scene.add(sphere_actor)
-        pts_actor = actor.point(shell, vtkcolors[i], point_radius=rad)
+        pts_actor = actor.point(shell, _colors[i], point_radius=rad)
         scene.add(pts_actor)
         if plot_sym_vecs:
-            pts_actor = actor.point(-shell, vtkcolors[i], point_radius=rad)
+            pts_actor = actor.point(-shell, _colors[i], point_radius=rad)
             scene.add(pts_actor)
         window.show(scene)
 
         if ofile:
             filename = ofile + '_shell_' + str(int(centroids[i])) + '.png'
-            snapshot(scene, filename, size=ores)
+            # Legacy. When this snapshotting gets updated to align with the
+            # viz module, snapshot_scenes should be called directly
+            snapshot = next(snapshot_scenes([scene], ores))
+            img = compose_image(snapshot, ores, "G")
+            img.save(filename)
 
 
 def plot_proj_shell(ms, use_sym=True, use_sphere=True, same_color=False,
                     rad=0.025, opacity=1.0, ofile=None, ores=(300, 300)):
     """
     Plot each shell
 
@@ -107,50 +97,50 @@
         radius of each point
     opacity: float
         opacity for the shells
     ofile: str
         output filename
     ores: tuple
         resolution of the output png
-
-    Return
-    ------
     """
-    global vtkcolors
-    if len(ms) > 10:
-        vtkcolors = fury.colormap.distinguishable_colormap(nb_colors=len(ms))
+
+    _colors = generate_n_colors(len(ms))
 
     scene = window.Scene()
     scene.SetBackground(1, 1, 1)
     if use_sphere:
         sphere = get_sphere('symmetric724')
         shape = (1, 1, 1, sphere.vertices.shape[0])
-        fid, fname = mkstemp(suffix='_odf_slicer.mmap')
+        _, fname = mkstemp(suffix='_odf_slicer.mmap')
         odfs = np.memmap(fname, dtype=np.float64, mode='w+', shape=shape)
         odfs[:] = 1
         odfs[..., 0] = 1
         affine = np.eye(4)
         sphere_actor = actor.odf_slicer(odfs, affine, sphere=sphere,
                                         colormap='winter', scale=1.0,
                                         opacity=opacity)
 
         scene.add(sphere_actor)
 
     for i, shell in enumerate(ms):
         if same_color:
             i = 0
-        pts_actor = actor.point(shell, vtkcolors[i], point_radius=rad)
+        pts_actor = actor.point(shell, _colors[i], point_radius=rad)
         scene.add(pts_actor)
         if use_sym:
-            pts_actor = actor.point(-shell, vtkcolors[i], point_radius=rad)
+            pts_actor = actor.point(-shell, _colors[i], point_radius=rad)
             scene.add(pts_actor)
     window.show(scene)
     if ofile:
         filename = ofile + '.png'
-        snapshot(scene, filename, size=ores)
+        # Legacy. When this snapshotting gets updated to align with the
+        # viz module, snapshot_scenes should be called directly
+        snapshot = next(snapshot_scenes([scene], ores))
+        img = compose_image(snapshot, ores, "G")
+        img.save(filename)
 
 
 def build_ms_from_shell_idx(bvecs, shell_idx):
     """
     Get bvecs from indexes
 
     Parameters
```

### Comparing `scilpy-1.5.post2/scilpy.egg-info/PKG-INFO` & `scilpy-2.0.0/scilpy.egg-info/PKG-INFO`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: scilpy
-Version: 1.5.post2
+Version: 2.0.0
 Summary: Scilpy: diffusion MRI tools and utilities
 Home-page: https://github.com/scilus/scilpy
 Download-URL: 
 Author: The SCIL developers
 Author-email: 
 Maintainer: Arnaud Boré
 Maintainer-email: arnaud.bore@gmail.com
@@ -13,58 +13,63 @@
 Classifier: Development Status :: 3 - Alpha
 Classifier: Environment :: Console
 Classifier: Intended Audience :: Science/Research
 Classifier: License :: OSI Approved :: MIT License
 Classifier: Operating System :: OS Independent
 Classifier: Programming Language :: Python
 Classifier: Topic :: Scientific/Engineering
-Requires-Python: ~=3.8,<3.11
+Requires-Python: >=3.9,<3.11
 License-File: LICENSE
-Requires-Dist: bids-validator==1.9.*
+Requires-Dist: bids-validator==1.11.*
 Requires-Dist: bctpy==0.5.*
 Requires-Dist: bz2file==0.98.*
 Requires-Dist: coloredlogs==15.0.*
-Requires-Dist: cvxpy==1.3.*
+Requires-Dist: cvxpy==1.4.*
 Requires-Dist: cycler==0.11.*
 Requires-Dist: Cython!=0.29.29,==0.29.*
-Requires-Dist: dipy==1.7.*
-Requires-Dist: dmri-amico==1.5.*
-Requires-Dist: dmri-commit==1.6.*
+Requires-Dist: dipy==1.9.*
+Requires-Dist: deepdiff==6.3.0
+Requires-Dist: dmri-amico==2.0.*
+Requires-Dist: dmri-commit==2.1.*
 Requires-Dist: docopt==0.6.*
+Requires-Dist: dvc==3.48.*
+Requires-Dist: dvc-http==2.32.*
 Requires-Dist: formulaic==0.3.*
-Requires-Dist: fury==0.8.*
+Requires-Dist: fury==0.10.*
 Requires-Dist: future==0.18.*
+Requires-Dist: GitPython==3.1.*
 Requires-Dist: h5py==3.7.*
 Requires-Dist: joblib==1.2.*
 Requires-Dist: kiwisolver==1.4.*
 Requires-Dist: matplotlib==3.6.*
-Requires-Dist: nibabel==4.0.*
+Requires-Dist: nibabel==5.2.*
 Requires-Dist: nilearn==0.9.*
 Requires-Dist: numpy==1.23.*
 Requires-Dist: openpyxl==3.0.*
-Requires-Dist: Pillow==9.3.*
-Requires-Dist: pybids==0.15.*
+Requires-Dist: packaging==23.2.*
+Requires-Dist: Pillow==10.2.*
+Requires-Dist: pybids==0.16.*
 Requires-Dist: pyparsing==3.0.*
 Requires-Dist: PySocks==1.7.*
 Requires-Dist: pytest==7.2.*
 Requires-Dist: pytest-console-scripts==1.3.*
+Requires-Dist: pytest-cov==4.1.0
+Requires-Dist: pytest-html==4.1.1
 Requires-Dist: pytest-mock==3.10.*
 Requires-Dist: python-dateutil==2.8.*
 Requires-Dist: pytz==2022.6.*
 Requires-Dist: requests==2.28.*
 Requires-Dist: scikit-learn==1.2.*
+Requires-Dist: scikit-image==0.22.*
 Requires-Dist: scipy==1.9.*
 Requires-Dist: six==1.16.*
 Requires-Dist: spams==2.6.*
 Requires-Dist: statsmodels==0.13.*
-Requires-Dist: trimeshpy==0.0.2
+Requires-Dist: trimeshpy==0.0.3
 Requires-Dist: vtk==9.2.*
-Requires-Dist: h5py>=2.8.0
-Requires-Dist: packaging>=19.0
-Requires-Dist: tqdm>=4.30.0
 
 
 Scilpy
 ========
 Scilpy is a small library mainly containing small tools and utilities
 to quickly work with diffusion MRI. Most of the tools are based
 on or wrapper of the Dipy_ library.
```

### Comparing `scilpy-1.5.post2/scilpy.egg-info/requires.txt` & `scilpy-2.0.0/scilpy.egg-info/requires.txt`

 * *Files 22% similar despite different names*

```diff
@@ -1,42 +1,47 @@
-bids-validator==1.9.*
+bids-validator==1.11.*
 bctpy==0.5.*
 bz2file==0.98.*
 coloredlogs==15.0.*
-cvxpy==1.3.*
+cvxpy==1.4.*
 cycler==0.11.*
 Cython!=0.29.29,==0.29.*
-dipy==1.7.*
-dmri-amico==1.5.*
-dmri-commit==1.6.*
+dipy==1.9.*
+deepdiff==6.3.0
+dmri-amico==2.0.*
+dmri-commit==2.1.*
 docopt==0.6.*
+dvc==3.48.*
+dvc-http==2.32.*
 formulaic==0.3.*
-fury==0.8.*
+fury==0.10.*
 future==0.18.*
+GitPython==3.1.*
 h5py==3.7.*
 joblib==1.2.*
 kiwisolver==1.4.*
 matplotlib==3.6.*
-nibabel==4.0.*
+nibabel==5.2.*
 nilearn==0.9.*
 numpy==1.23.*
 openpyxl==3.0.*
-Pillow==9.3.*
-pybids==0.15.*
+packaging==23.2.*
+Pillow==10.2.*
+pybids==0.16.*
 pyparsing==3.0.*
 PySocks==1.7.*
 pytest==7.2.*
 pytest-console-scripts==1.3.*
+pytest-cov==4.1.0
+pytest-html==4.1.1
 pytest-mock==3.10.*
 python-dateutil==2.8.*
 pytz==2022.6.*
 requests==2.28.*
 scikit-learn==1.2.*
+scikit-image==0.22.*
 scipy==1.9.*
 six==1.16.*
 spams==2.6.*
 statsmodels==0.13.*
-trimeshpy==0.0.2
+trimeshpy==0.0.3
 vtk==9.2.*
-h5py>=2.8.0
-packaging>=19.0
-tqdm>=4.30.0
```

### Comparing `scilpy-1.5.post2/scripts/scil_add_tracking_mask_to_pft_maps.py` & `scilpy-2.0.0/scripts/scil_tracking_pft_maps_edit.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,23 +1,26 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
 Modify PFT maps to allow PFT tracking in given mask (e.g edema).
+
+Formerly: scil_add_tracking_mask_to_pft_maps.py.
 """
 
 import argparse
+import logging
 
 import nibabel as nib
 import numpy as np
 
 from scilpy.io.image import get_data_as_mask
-from scilpy.io.utils import (add_overwrite_arg,
-                             assert_inputs_exist,
-                             assert_outputs_exist)
+from scilpy.io.utils import (add_overwrite_arg, add_verbose_arg,
+                             assert_inputs_exist, assert_outputs_exist,
+                             assert_headers_compatible)
 
 
 def _build_arg_parser():
     parser = argparse.ArgumentParser(
         description=__doc__,
         formatter_class=argparse.RawTextHelpFormatter)
     parser.add_argument('map_include',
@@ -26,35 +29,40 @@
                         help='PFT map exclude.')
     parser.add_argument('additional_mask',
                         help='Allow PFT tracking in this mask.')
     parser.add_argument('map_include_corr',
                         help='Corrected PFT map include output file name.')
     parser.add_argument('map_exclude_corr',
                         help='Corrected PFT map exclude output file name.')
+
+    add_verbose_arg(parser)
     add_overwrite_arg(parser)
+
     return parser
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, [args.map_include, args.map_exclude,
                                  args.additional_mask])
     assert_outputs_exist(parser, args, [args.map_include_corr,
                                         args.map_exclude_corr])
+    assert_headers_compatible(parser, [args.map_include, args.map_exclude,
+                                 args.additional_mask])
 
     map_inc = nib.load(args.map_include)
     map_inc_data = map_inc.get_fdata(dtype=np.float32)
 
     map_exc = nib.load(args.map_exclude)
     map_exc_data = map_exc.get_fdata(dtype=np.float32)
 
-    additional_mask = nib.load(args.additional_mask)
-    additional_mask_data = get_data_as_mask(additional_mask)
+    additional_mask_data = get_data_as_mask(nib.load(args.additional_mask))
 
     map_inc_data[additional_mask_data > 0] = 0
     map_exc_data[additional_mask_data > 0] = 0
 
     nib.save(
         nib.Nifti1Image(map_inc_data.astype('float32'),
                         map_inc.affine,
```

### Comparing `scilpy-1.5.post2/scripts/scil_analyse_lesions_load.py` & `scilpy-2.0.0/scripts/scil_lesions_info.py`

 * *Files 4% similar despite different names*

```diff
@@ -5,32 +5,34 @@
 This script will output informations about lesion load in bundle(s).
 The input can either be streamlines, binary bundle map, or a bundle voxel
 label map.
 
 To be considered a valid lesion, the lesion volume must be at least
 min_lesion_vol mm3. This avoid the detection of thousand of single voxel
 lesions if an automatic lesion segmentation tool is used.
+
+Formerly: scil_analyse_lesions_load.py
 """
 
 import argparse
 import json
+import logging
 import os
 
 import nibabel as nib
 import numpy as np
 import scipy.ndimage as ndi
 
 from scilpy.image.labels import get_data_as_labels
 from scilpy.io.image import get_data_as_mask
 from scilpy.io.streamlines import load_tractogram_with_reference
-from scilpy.io.utils import (add_overwrite_arg,
-                             assert_inputs_exist,
-                             add_json_args,
-                             assert_outputs_exist,
-                             add_reference_arg)
+from scilpy.io.utils import (add_overwrite_arg, assert_inputs_exist,
+                             add_json_args, assert_outputs_exist,
+                             add_verbose_arg, add_reference_arg,
+                             assert_headers_compatible)
 from scilpy.segment.streamlines import filter_grid_roi
 from scilpy.tractanalysis.streamlines_metrics import compute_tract_counts_map
 from scilpy.utils.filenames import split_name_with_nii
 from scilpy.utils.metrics_tools import compute_lesion_stats
 
 
 def _build_arg_parser():
@@ -55,34 +57,40 @@
                    help='Save the labelized lesion(s) map (.nii.gz).')
     p.add_argument('--out_lesion_stats', metavar='FILE',
                    help='Save the lesion-wise volume measure (.json).')
     p.add_argument('--out_streamlines_stats', metavar='FILE',
                    help='Save the lesion-wise streamline count (.json).')
 
     add_json_args(p)
-    add_overwrite_arg(p)
     add_reference_arg(p)
+    add_verbose_arg(p)
+    add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     if (not args.bundle) and (not args.bundle_mask) \
             and (not args.bundle_labels_map):
         parser.error('One of the option --bundle or --map must be used')
 
-    assert_inputs_exist(parser, [args.in_lesion],
+    assert_inputs_exist(parser, args.in_lesion,
                         optional=[args.bundle, args.bundle_mask,
-                                  args.bundle_labels_map])
+                                  args.bundle_labels_map, args.reference])
     assert_outputs_exist(parser, args, args.out_json,
                          optional=[args.out_lesion_stats,
                                    args.out_streamlines_stats])
+    assert_headers_compatible(parser, args.in_lesion,
+                              optional=[args.bundle, args.bundle_mask,
+                                        args.bundle_labels_map],
+                              reference=args.reference)
 
     lesion_img = nib.load(args.in_lesion)
     lesion_data = get_data_as_mask(lesion_img, dtype=bool)
 
     if args.bundle:
         bundle_name, _ = split_name_with_nii(os.path.basename(args.bundle))
         sft = load_tractogram_with_reference(parser, args, args.bundle)
```

### Comparing `scilpy-1.5.post2/scripts/scil_apply_bias_field_on_dwi.py` & `scilpy-2.0.0/scripts/scil_dwi_apply_bias_field.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,95 +1,76 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
 Apply bias field correction to DWI. This script doesn't compute the bias
-field itself. It ONLY applies an existing bias field. Use the ANTs
-N4BiasFieldCorrection executable to compute the bias field
+field itself. It ONLY applies an existing bias field. Please use the ANTs
+N4BiasFieldCorrection executable to compute the bias field.
+
+Formerly: scil_apply_bias_field_on_dwi.py
 """
 
 import argparse
+import logging
 
 import nibabel as nib
 import numpy as np
 
+from scilpy.dwi.operations import apply_bias_field
 from scilpy.io.image import get_data_as_mask
-from scilpy.io.utils import (add_overwrite_arg,
-                             assert_inputs_exist,
-                             assert_outputs_exist)
+from scilpy.io.utils import (add_overwrite_arg, add_verbose_arg,
+                             assert_inputs_exist, assert_outputs_exist,
+                             assert_headers_compatible)
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__,
         formatter_class=argparse.RawTextHelpFormatter)
     p.add_argument('in_dwi',
                    help='DWI Nifti image.')
     p.add_argument('in_bias_field',
                    help='Bias field Nifti image.')
     p.add_argument('out_name',
                    help='Corrected DWI Nifti image.')
     p.add_argument('--mask',
                    help='Apply bias field correction only in the region '
-                        'defined by the mask.')
+                        'defined by the mask.\n'
+                        'If this is not given, the bias field is still only '
+                        'applied only in non-background data \n(i.e. where '
+                        'the dwi is not 0).')
+    
+    add_verbose_arg(p)
     add_overwrite_arg(p)
+    
     return p
 
 
-def _rescale_intensity(val, slope, in_max, bc_max):
-    return in_max - slope * (bc_max - val)
-
-
-# https://github.com/stnava/ANTs/blob/master/Examples/N4BiasFieldCorrection.cxx
-def _rescale_dwi(in_data, bc_data):
-    in_min = np.amin(in_data)
-    in_max = np.amax(in_data)
-    bc_min = np.amin(bc_data)
-    bc_max = np.amax(bc_data)
-
-    slope = (in_max - in_min) / (bc_max - bc_min)
-
-    chunk = np.arange(0, len(in_data), 100000)
-    chunk = np.append(chunk, len(in_data))
-    for i in range(len(chunk)-1):
-        nz_bc_data = bc_data[chunk[i]:chunk[i+1]]
-        rescale_func = np.vectorize(_rescale_intensity, otypes=[np.float32])
-
-        rescaled_data = rescale_func(nz_bc_data, slope, in_max, bc_max)
-        bc_data[chunk[i]:chunk[i+1]] = rescaled_data
-
-    return bc_data
-
-
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, [args.in_dwi, args.in_bias_field], args.mask)
     assert_outputs_exist(parser, args, args.out_name)
+    assert_headers_compatible(parser, [args.in_dwi, args.in_bias_field],
+                              args.mask)
 
     dwi_img = nib.load(args.in_dwi)
     dwi_data = dwi_img.get_fdata(dtype=np.float32)
 
     bias_field_img = nib.load(args.in_bias_field)
     bias_field_data = bias_field_img.get_fdata(dtype=np.float32)
 
     if args.mask:
-        mask_img = nib.load(args.mask)
-        nz_mask_data = np.nonzero(get_data_as_mask(mask_img))
+        mask_data = get_data_as_mask(nib.load(args.mask))
     else:
-        nz_mask_data = np.nonzero(np.average(dwi_data, axis=-1))
-
-    nuc_dwi_data = np.divide(dwi_data[nz_mask_data],
-                             bias_field_data[nz_mask_data].reshape((len(nz_mask_data[0]), 1)))
+        mask_data = np.average(dwi_data, axis=-1) != 0
 
-    rescaled_nuc_data = _rescale_dwi(dwi_data[nz_mask_data],
-                                     nuc_dwi_data)
+    dwi_data = apply_bias_field(dwi_data, bias_field_data, mask_data)
 
-    dwi_data[nz_mask_data] = rescaled_nuc_data
-    nib.save(nib.Nifti1Image(dwi_data, dwi_img.affine,
-                             dwi_img.header),
+    nib.save(nib.Nifti1Image(dwi_data, dwi_img.affine, dwi_img.header),
              args.out_name)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_apply_transform_to_bvecs.py` & `scilpy-2.0.0/scripts/scil_gradients_apply_transform.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,60 +1,63 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-
-"""
-Transform bvecs using an affine/rigid transformation.
-"""
-
-import argparse
-
-import numpy as np
-
-from dipy.io.gradients import read_bvals_bvecs
-
-from scilpy.io.utils import (add_overwrite_arg, assert_inputs_exist,
-                             assert_outputs_exist, load_matrix_in_any_format)
-from scilpy.utils.filenames import split_name_with_nii
-from scilpy.utils.image import transform_anatomy
-
-
-def _build_arg_parser():
-    p = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter,
-                                description=__doc__)
-
-    p.add_argument('in_bvecs',
-                   help='Path of the bvec file, in FSL format')
-    p.add_argument('in_transfo',
-                   help='Path of the file containing the 4x4 \n'
-                        'transformation, matrix (.txt, .npy or .mat).')
-    p.add_argument('out_bvecs',
-                   help='Output filename of the transformed bvecs.')
-
-    p.add_argument('--inverse', action='store_true',
-                   help='Apply the inverse transformation.')
-
-    add_overwrite_arg(p)
-
-    return p
-
-
-def main():
-    parser = _build_arg_parser()
-    args = parser.parse_args()
-
-    assert_inputs_exist(parser, [args.in_bvecs, args.in_transfo])
-    assert_outputs_exist(parser, args, args.out_bvecs)
-
-    transfo = load_matrix_in_any_format(args.in_transfo)[:3, :3]
-
-    if args.inverse:
-        transfo = np.linalg.inv(transfo)
-
-    _, bvecs = read_bvals_bvecs(None, args.in_bvecs)
-
-    bvecs = bvecs @ transfo
-
-    np.savetxt(str(args.out_bvecs), bvecs.T)
-
-
-if __name__ == "__main__":
-    main()
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+
+"""
+Transform bvecs using an affine/rigid transformation.
+
+Formerly: scil_apply_transform_to_bvecs.py.
+"""
+
+import argparse
+import logging
+
+from dipy.io.gradients import read_bvals_bvecs
+import numpy as np
+
+from scilpy.io.utils import (add_overwrite_arg, assert_inputs_exist,
+                             assert_outputs_exist, add_verbose_arg,
+                             load_matrix_in_any_format)
+
+
+def _build_arg_parser():
+    p = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter,
+                                description=__doc__)
+
+    p.add_argument('in_bvecs',
+                   help='Path of the bvec file, in FSL format')
+    p.add_argument('in_transfo',
+                   help='Path of the file containing the 4x4 \n'
+                        'transformation, matrix (.txt, .npy or .mat).')
+    p.add_argument('out_bvecs',
+                   help='Output filename of the transformed bvecs.')
+
+    p.add_argument('--inverse', action='store_true',
+                   help='Apply the inverse transformation.')
+
+    add_verbose_arg(p)
+    add_overwrite_arg(p)
+
+    return p
+
+
+def main():
+    parser = _build_arg_parser()
+    args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
+
+    assert_inputs_exist(parser, [args.in_bvecs, args.in_transfo])
+    assert_outputs_exist(parser, args, args.out_bvecs)
+
+    transfo = load_matrix_in_any_format(args.in_transfo)[:3, :3]
+
+    if args.inverse:
+        transfo = np.linalg.inv(transfo)
+
+    _, bvecs = read_bvals_bvecs(None, args.in_bvecs)
+
+    bvecs = bvecs @ transfo
+
+    np.savetxt(str(args.out_bvecs), bvecs.T)
+
+
+if __name__ == "__main__":
+    main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_apply_transform_to_image.py` & `scilpy-2.0.0/scripts/scil_volume_apply_transform.py`

 * *Files 8% similar despite different names*

```diff
@@ -2,24 +2,29 @@
 # -*- coding: utf-8 -*-
 
 """
 Transform Nifti (.nii.gz) using an affine/rigid transformation.
 
 For more information on how to use the registration script, follow this link:
 https://scilpy.readthedocs.io/en/latest/documentation/tractogram_registration.html
+
+Formerly: scil_apply_transform_to_image.py.
 """
 
 import argparse
+import logging
 
+import nibabel as nib
 import numpy as np
 
+from scilpy.image.volume_operations import apply_transform
 from scilpy.io.utils import (add_overwrite_arg, assert_inputs_exist,
-                             assert_outputs_exist, load_matrix_in_any_format)
+                             assert_outputs_exist, add_verbose_arg,
+                             load_matrix_in_any_format)
 from scilpy.utils.filenames import split_name_with_nii
-from scilpy.utils.image import transform_anatomy
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter,
                                 description=__doc__)
 
     p.add_argument('in_file',
@@ -27,45 +32,53 @@
     p.add_argument('in_target_file',
                    help='Path of the reference target file (.nii.gz).')
     p.add_argument('in_transfo',
                    help='Path of the file containing the 4x4 \n'
                         'transformation, matrix (.txt, .npy or .mat).')
     p.add_argument('out_name',
                    help='Output filename of the transformed data.')
-
     p.add_argument('--inverse', action='store_true',
                    help='Apply the inverse transformation.')
-
     p.add_argument('--keep_dtype', action='store_true',
                    help='If True, keeps the data_type of the input image '
                         '(in_file) when saving the output image (out_name).')
 
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
+    # Verifications
     assert_inputs_exist(parser, [args.in_file, args.in_target_file,
                                  args.in_transfo])
     assert_outputs_exist(parser, args, args.out_name)
 
-    transfo = load_matrix_in_any_format(args.in_transfo)
-    if args.inverse:
-        transfo = np.linalg.inv(transfo)
-
     _, ref_extension = split_name_with_nii(args.in_target_file)
     _, in_extension = split_name_with_nii(args.in_file)
     if ref_extension not in ['.nii', '.nii.gz']:
-        parser.error('{} is an unsupported format.'.format(args.in_target_file))
+        parser.error('{} is an unsupported format.'.format(
+            args.in_target_file))
     if in_extension not in ['.nii', '.nii.gz']:
         parser.error('{} is an unsupported format.'.format(args.in_file))
 
-    transform_anatomy(transfo, args.in_target_file, args.in_file,
-                      args.out_name, keep_dtype=args.keep_dtype)
+    # Loading
+    transfo = load_matrix_in_any_format(args.in_transfo)
+    if args.inverse:
+        transfo = np.linalg.inv(transfo)
+    moving = nib.load(args.in_file)
+    reference = nib.load(args.in_target_file)
+
+    # Processing, saving
+    warped_img = apply_transform(
+        transfo, reference, moving, keep_dtype=args.keep_dtype)
+
+    nib.save(warped_img, args.out_name)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_apply_transform_to_tractogram.py` & `scilpy-2.0.0/scripts/scil_tractogram_apply_transform.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,148 +1,164 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-Transform tractogram using an affine/rigid transformation and nonlinear
+Transform a tractogram using an affine/rigid transformation and nonlinear
 deformation (optional).
 
 For more information on how to use the registration script, follow this link:
 https://scilpy.readthedocs.io/en/latest/documentation/tractogram_registration.html
 
-Applying transformation to tractogram can lead to invalid streamlines (out of
-the bounding box), three strategies are available:
-1) default, crash at saving if invalid streamlines are present
+Applying transformation to a tractogram can lead to invalid streamlines (out of
+the bounding box), and thus three strategies are available:
+1) Do nothing, may crash at saving if invalid streamlines are present.
+   [This is the default]
 2) --keep_invalid, save invalid streamlines. Leave it to the user to run
-    scil_remove_invalid_streamlines.py if needed.
+    scil_tractogram_remove_invalid.py if needed.
 3) --remove_invalid, automatically remove invalid streamlines before saving.
-    Should not remove more than a few streamlines.
-4) --cut_invalid, automatically cut invalid streamlines before saving.
+    Should not remove more than a few streamlines. Typically, the streamlines
+    that are rejected are the ones reaching the limits of the brain, ex, near
+    the pons.
+4) --cut_invalid, automatically cut invalid streamlines before saving, i.e. the
+   streamlines are kept but the points out of the bounding box are cut.
 
 Example:
-To apply transform from ANTS to tractogram. If the ANTS commands was
-MOVING->REFERENCE, this will bring a tractogram from MOVING->REFERENCE
-scil_apply_transform_to_tractogram.py ${MOVING_FILE} ${REFERENCE_FILE}
-                                        0GenericAffine.mat ${OUTPUT_NAME}
-                                        --inverse
-                                        --in_deformation 1InverseWarp.nii.gz
-
-If the ANTS commands was MOVING->REFERENCE, this will bring a tractogram
-from REFERENCE->MOVING
-scil_apply_transform_to_tractogram.py ${MOVING_FILE} ${REFERENCE_FILE}
-                                        0GenericAffine.mat ${OUTPUT_NAME}
-                                        --in_deformation 1Warp.nii.gz
-                                        --reverse_operation
+To apply a transformation from ANTs to a tractogram, if the ANTs command was
+MOVING->REFERENCE...
+1) To apply the original transformation:
+scil_tractogram_apply_transform.py ${MOVING_FILE} ${REFERENCE_FILE}
+                                   0GenericAffine.mat ${OUTPUT_NAME}
+                                   --inverse
+                                   --in_deformation 1InverseWarp.nii.gz
+
+2) To apply the inverse transformation, i.e. REFERENCE->MOVING:
+scil_tractogram_apply_transform.py ${MOVING_FILE} ${REFERENCE_FILE}
+                                   0GenericAffine.mat ${OUTPUT_NAME}
+                                   --in_deformation 1Warp.nii.gz
+                                   --reverse_operation
+
+Formerly: scil_apply_transform_to_tractogram.py
 """
 
 import argparse
 import logging
 
-from dipy.io.streamline import save_tractogram
 import nibabel as nib
 import numpy as np
 
-from scilpy.io.streamlines import load_tractogram_with_reference
-from scilpy.io.utils import (add_bbox_arg,
-                             add_overwrite_arg,
+from scilpy.io.streamlines import load_tractogram_with_reference, \
+    save_tractogram
+from scilpy.io.utils import (add_overwrite_arg,
                              add_reference_arg,
                              add_verbose_arg,
                              assert_inputs_exist,
                              assert_outputs_exist,
                              load_matrix_in_any_format)
 from scilpy.tractograms.tractogram_operations import transform_warp_sft
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter,
                                 description=__doc__)
 
     p.add_argument('in_moving_tractogram',
                    help='Path of the tractogram to be transformed.\n'
-                        'Bounding box validity will not be checked (could '
+                        'Bounding box validity will not be checked (could \n'
                         'contain invalid streamlines).')
     p.add_argument('in_target_file',
                    help='Path of the reference target file (trk or nii).')
     p.add_argument('in_transfo',
                    help='Path of the file containing the 4x4 \n'
                         'transformation, matrix (.txt, .npy or .mat).')
     p.add_argument('out_tractogram',
                    help='Output tractogram filename (transformed data).')
 
-    p.add_argument('--inverse', action='store_true',
+    g = p.add_argument_group("Transformation options")
+    g.add_argument('--inverse', action='store_true',
                    help='Apply the inverse linear transformation.')
-    p.add_argument('--in_deformation',
+    g.add_argument('--in_deformation', metavar='file',
                    help='Path to the file containing a deformation field.')
-    p.add_argument('--reverse_operation', action='store_true',
-                   help='Apply the transformation in reverse (see doc),'
-                        'warp first, then linear.')
-    invalid = p.add_mutually_exclusive_group()
+    g.add_argument('--reverse_operation', action='store_true',
+                   help='Apply the transformation in reverse (see doc), warp\n'
+                        'first, then linear.')
+
+    g = p.add_argument_group("Management of invalid streamlines")
+    invalid = g.add_mutually_exclusive_group()
     invalid.add_argument('--cut_invalid', action='store_true',
                          help='Cut invalid streamlines rather than removing '
                               'them.\nKeep the longest segment only.')
     invalid.add_argument('--remove_invalid', action='store_true',
                          help='Remove the streamlines landing out of the '
                               'bounding box.')
     invalid.add_argument('--keep_invalid', action='store_true',
                          help='Keep the streamlines landing out of the '
                               'bounding box.')
 
     p.add_argument('--no_empty', action='store_true',
                    help='Do not write file if there is no streamline.\n'
                         'You may save an empty file if you use '
                         'remove_invalid.')
+
     add_reference_arg(p)
-    add_overwrite_arg(p)
     add_verbose_arg(p)
+    add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    if args.verbose:
-        logging.getLogger().setLevel(logging.DEBUG)
-
+    # Verifications
     assert_inputs_exist(parser, [args.in_moving_tractogram,
                                  args.in_target_file,
-                                 args.in_transfo], args.in_deformation)
+                                 args.in_transfo],
+                        [args.in_deformation, args.reference])
     assert_outputs_exist(parser, args, args.out_tractogram)
 
     args.bbox_check = False  # Adding manually bbox_check argument.
+
+    # Loading
     moving_sft = load_tractogram_with_reference(parser, args,
                                                 args.in_moving_tractogram)
 
     transfo = load_matrix_in_any_format(args.in_transfo)
     deformation_data = None
     if args.in_deformation is not None:
         deformation_data = np.squeeze(nib.load(
             args.in_deformation).get_fdata(dtype=np.float32))
 
+    # Processing
     new_sft = transform_warp_sft(moving_sft, transfo,
                                  args.in_target_file,
                                  inverse=args.inverse,
                                  reverse_op=args.reverse_operation,
                                  deformation_data=deformation_data,
                                  remove_invalid=args.remove_invalid,
                                  cut_invalid=args.cut_invalid)
 
-    if len(new_sft.streamlines) == 0:
-        if args.no_empty:
-            logging.debug("The file {} won't be written "
-                          "(0 streamline).".format(args.out_tractogram))
-            return
+    # Saving
 
+    # Default is to crash if invalid.
     if args.keep_invalid:
         if not new_sft.is_bbox_in_vox_valid():
             logging.warning('Saving tractogram with invalid streamlines.')
-        save_tractogram(new_sft, args.out_tractogram, bbox_valid_check=False)
+
+        save_tractogram(new_sft, args.out_tractogram,
+                        args.no_empty,
+                        bbox_valid_check=False)
     else:
+        # Here, there should be no invalid streamlines left. Either option =
+        # to crash, or remove/cut, already managed.
         if not new_sft.is_bbox_in_vox_valid():
-            logging.warning('Removing invalid streamlines before '
-                            'saving tractogram.')
-            new_sft.remove_invalid_streamlines()
-        save_tractogram(new_sft, args.out_tractogram)
+            raise ValueError("The result has invalid streamlines. Please "
+                             "chose --keep_invalid, --cut_invalid or "
+                             "--remove_invalid.")
+        save_tractogram(new_sft, args.out_tractogram,
+                        args.no_empty,
+                        bbox_valid_check=True)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_assign_custom_color_to_tractogram.py` & `scilpy-2.0.0/scripts/scil_tractogram_assign_custom_color.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,244 +1,229 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
 The script uses scalars from an anatomy, data_per_point or data_per_streamline
-(e.g commit_weights) to visualize them on the streamlines.
-Saves the RGB values in the data_per_point (color_x, color_y, color_z).
+(e.g. commit_weights) to visualize them on the streamlines.
+Saves the RGB values in the data_per_point 'color' with 3 values per point:
+(color_x, color_y, color_z).
 
 If called with .tck, the output will always be .trk, because data_per_point has
 no equivalent in tck file.
 
-The usage of --use_dps, --use_dpp and --from_anatomy is more complex. It maps
-the raw values from these sources to RGB using a colormap.
-    --use_dps: total nbr of streamlines of the tractogram = len(streamlines)
-    --use_dpp: total nbr of points of the tractogram = len(streamlines._data)
+If used with a visualization software like MI-Brain
+(https://github.com/imeka/mi-brain), the 'color' dps is applied by default at
+loading time.
+
+COLORING METHOD
+This script maps the raw values from these sources to RGB using a colormap.
+    --use_dpp: The data from each point is converted to a color.
+    --use_dps: The same color is applied to all points of the streamline.
+    --from_anatomy: The voxel's color is used for the points of the streamlines
+    crossing it. See also scil_tractogram_project_map_to_streamlines.py. You
+    can have more options to project maps to dpp, and then use --use_dpp here.
+    --along_profile: The data used here is each point's position in the
+    streamline. To have nice results, you should first uniformize head/tail.
+    See scil_tractogram_uniformize_endpoints.py.
+    --local_angle.
 
+COLORING OPTIONS
 A minimum and a maximum range can be provided to clip values. If the range of
 values is too large for intuitive visualization, a log transform can be
 applied.
 
 If the data provided from --use_dps, --use_dpp and --from_anatomy are integer
 labels, they can be mapped using a LookUp Table (--LUT).
-The file provided as a LUT should be either .txt or .npy and if the
-size is N=20, then the data provided should be between 1-20.
-
-Example: Use --from_anatomy with a voxel labels map (values from 1-20) with a
-text file containing 20 p-values to map p-values to the bundle for
-visualisation.
+The file provided as a LUT should be either .txt or .npy and if the size is
+N=20, then the data provided should be between 1-20.
 
 A custom colormap can be provided using --colormap. It should be a string
 containing a colormap name OR multiple Matplotlib named colors separated by -.
 The colormap used for mapping values to colors can be saved to a png/jpg image
 using the --out_colorbar option.
 
-The script can also be used to color streamlines according to their length
-using the --along_profile option. The streamlines must be uniformized.
+See also: scil_tractogram_assign_uniform_color.py, for simplified options.
+
+Formerly: scil_assign_custom_color_to_tractogram.py
 """
 
 import argparse
 import logging
 
 from dipy.io.streamline import save_tractogram
 import nibabel as nib
 import numpy as np
+import matplotlib.pyplot as plt
 from scipy.ndimage import map_coordinates
 
 from scilpy.io.streamlines import load_tractogram_with_reference
-from scilpy.io.utils import (assert_inputs_exist,
-                             assert_outputs_exist,
-                             add_overwrite_arg,
+from scilpy.io.utils import (add_overwrite_arg,
                              add_reference_arg,
+                             add_verbose_arg,
+                             assert_inputs_exist,
+                             assert_outputs_exist,
                              load_matrix_in_any_format)
-from scilpy.utils.streamlines import get_color_streamlines_along_length
-from scilpy.viz.utils import get_colormap
-
-COLORBAR_NB_VALUES = 255
+from scilpy.tractograms.dps_and_dpp_management import add_data_as_color_dpp
+from scilpy.tractograms.streamline_operations import (get_values_along_length,
+                                                      get_angles)
+from scilpy.viz.color import get_lookup_table
+from scilpy.viz.color import prepare_colorbar_figure
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__,
         formatter_class=argparse.RawTextHelpFormatter)
 
     p.add_argument('in_tractogram',
                    help='Input tractogram (.trk or .tck).')
     p.add_argument('out_tractogram',
                    help='Output tractogram (.trk or .tck).')
 
-    cbar_g = p.add_argument_group('Colorbar Options')
+    cbar_g = p.add_argument_group('Colorbar options')
     cbar_g.add_argument('--out_colorbar',
                         help='Optional output colorbar (.png, .jpg or any '
-                             'format supported by matplotlib).')
+                             'format \nsupported by matplotlib).')
+    cbar_g.add_argument('--show_colorbar', action='store_true',
+                        help="Will show the colorbar. Must be used with "
+                             "--out_colorbar \nto be effective.")
     cbar_g.add_argument('--horizontal_cbar', action='store_true',
                         help='Draw horizontal colorbar (vertical by default).')
 
-    g1 = p.add_argument_group(title='Coloring Methods')
-    p1 = g1.add_mutually_exclusive_group()
+    g1 = p.add_argument_group(title='Coloring method')
+    p1 = g1.add_mutually_exclusive_group(required=True)
     p1.add_argument('--use_dps', metavar='DPS_KEY',
-                    help='Use the data_per_streamline (scalar) for coloring,\n'
-                         'linear scaling from min-max, e.g. commit_weights.')
+                    help='Use the data_per_streamline (scalar) for coloring.')
     p1.add_argument('--use_dpp', metavar='DPP_KEY',
-                    help='Use the data_per_point (scalar) for coloring,\n'
-                         'linear scaling from min-max.')
-    p1.add_argument('--load_dps', metavar='DPS_KEY',
+                    help='Use the data_per_point (scalar) for coloring.')
+    p1.add_argument('--load_dps', metavar='DPS_FILE',
                     help='Load data per streamline (scalar) for coloring')
-    p1.add_argument('--load_dpp', metavar='DPP_KEY',
+    p1.add_argument('--load_dpp', metavar='DPP_FILE',
                     help='Load data per point (scalar) for coloring')
     p1.add_argument('--from_anatomy', metavar='FILE',
                     help='Use the voxel data for coloring,\n'
                          'linear scaling from minmax.')
     p1.add_argument('--along_profile', action='store_true',
                     help='Color streamlines according to each point position'
-                         'along its length.\nMust be uniformized head/tail.')
+                         'along its length.')
+    p1.add_argument('--local_angle', action='store_true',
+                    help="Color streamlines according to the angle between "
+                         "each segment (in degree). \nAngles at first and "
+                         "last points are set to 0.")
 
-    g2 = p.add_argument_group(title='Coloring Options')
+    g2 = p.add_argument_group(title='Coloring options')
     g2.add_argument('--colormap', default='jet',
                     help='Select the colormap for colored trk (dps/dpp) '
                     '[%(default)s].\nUse two Matplotlib named color separeted '
                     'by a - to create your own colormap.')
+    g2.add_argument('--clip_outliers', action='store_true',
+                    help="If set, we will clip the outliers (first and last "
+                         "5%% quantile). Strongly suggested if your data "
+                         "comes from COMMIT!")
     g2.add_argument('--min_range', type=float,
                     help='Set the minimum value when using dps/dpp/anatomy.')
     g2.add_argument('--max_range', type=float,
                     help='Set the maximum value when using dps/dpp/anatomy.')
+    g2.add_argument('--min_cmap', type=float,
+                    help='Set the minimum value of the colormap.')
+    g2.add_argument('--max_cmap', type=float,
+                    help='Set the maximum value of the colormap.')
     g2.add_argument('--log', action='store_true',
-                    help='Apply a base 10 logarithm for colored trk (dps/dpp).')
+                    help='Apply a base 10 logarithm for colored trk (dps/dpp).'
+                    )
     g2.add_argument('--LUT', metavar='FILE',
                     help='If the dps/dpp or anatomy contain integer labels, '
                          'the value will be substituted.\nIf the LUT has 20 '
                          'elements, integers from 1-20 in the data will be\n'
                          'replaced by the value in the file (.npy or .txt)')
 
     add_reference_arg(p)
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
-def transform_data(args, data):
-    if args.LUT:
-        LUT = load_matrix_in_any_format(args.LUT)
-        for i, val in enumerate(LUT):
-            data[data == i+1] = val
-
-    if args.min_range is not None or args.max_range is not None:
-        data = np.clip(data, args.min_range, args.max_range)
-
-    # get data values range
-    lbound = np.min(data)
-    ubound = np.max(data)
-
-    if args.log:
-        data[data > 0] = np.log10(data[data > 0])
-
-    # normalize data between 0 and 1
-    data -= np.min(data)
-    data = data / np.max(data) if np.max(data) > 0 else data
-    return data, lbound, ubound
-
-
-def save_colorbar(cmap, lbound, ubound, args):
-    gradient = cmap(np.linspace(0, 1, COLORBAR_NB_VALUES))[:, 0:3]
-
-    # TODO: Is there a better way to draw a gradient-filled rectangle?
-    width = int(COLORBAR_NB_VALUES * 0.1)
-    gradient = np.tile(gradient, (width, 1, 1))
-    if not args.horizontal_cbar:
-        gradient = np.swapaxes(gradient, 0, 1)
-
-    _, ax = plt.subplots(1, 1)
-    ax.imshow(gradient, origin='lower')
-
-    ticks_labels = ['{0:.3f}'.format(lbound), '{0:.3f}'.format(ubound)]
-
-    if args.log:
-        ticks_labels[0] = 'log(' + ticks_labels[0] + ')'
-        ticks_labels[1] = 'log(' + ticks_labels[1] + ')'
-
-    if not args.horizontal_cbar:
-        ax.set_yticks([0, COLORBAR_NB_VALUES - 1])
-        ax.set_yticklabels(ticks_labels)
-        ax.set_xticks([])
-    else:
-        ax.set_xticks([0, COLORBAR_NB_VALUES - 1])
-        ax.set_xticklabels(ticks_labels)
-        ax.set_yticks([])
-
-    plt.savefig(args.out_colorbar, bbox_inches='tight')
-
-
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
-    logging.getLogger().setLevel(logging.WARNING)
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, args.in_tractogram)
+    # Verifications
+    assert_inputs_exist(parser, args.in_tractogram, args.reference)
     assert_outputs_exist(parser, args, args.out_tractogram,
                          optional=args.out_colorbar)
 
     if args.horizontal_cbar and not args.out_colorbar:
         logging.warning('Colorbar output not supplied. Ignoring '
                         '--horizontal_cbar.')
 
+    if (args.use_dps is not None and
+            args.use_dps in ['commit_weights', 'commit2_weights'] and not
+            args.clip_outliers):
+        logging.warning("You seem to be using commit weights. They typically "
+                        "have outliers values. We suggest using "
+                        "--clip_outliers.")
+
+    # Loading
     sft = load_tractogram_with_reference(parser, args, args.in_tractogram)
+    LUT = load_matrix_in_any_format(args.LUT) if args.LUT else None
 
-    if args.LUT:
-        LUT = load_matrix_in_any_format(args.LUT)
-        if np.any(sft.streamlines._lengths < len(LUT)):
-            logging.warning('Some streamlines have fewer point than the size '
-                            'of the provided LUT.\nConsider using '
-                            'scil_resample_streamlines.py')
-
-    cmap = get_colormap(args.colormap)
-    if args.use_dps or args.use_dpp or args.load_dps or args.load_dpp:
-        if args.use_dps:
-            data = np.squeeze(sft.data_per_streamline[args.use_dps])
-            # I believe it works well for gaussian distribution, but
-            # COMMIT has very weird outliers values
-            if args.use_dps == 'commit_weights' \
-                    or args.use_dps == 'commit2_weights':
-                data = np.clip(data, np.quantile(data, 0.05),
-                               np.quantile(data, 0.95))
-        elif args.use_dpp:
-            data = np.squeeze(sft.data_per_point[args.use_dpp]._data)
-        elif args.load_dps:
-            data = np.squeeze(load_matrix_in_any_format(args.load_dps))
-            if len(data) != len(sft):
-                parser.error('Wrong dps size!')
-        elif args.load_dpp:
-            data = np.squeeze(load_matrix_in_any_format(args.load_dpp))
-            if len(data) != len(sft.streamlines._data):
-                parser.error('Wrong dpp size!')
-        data, lbound, ubound = transform_data(args, data)
-        color = cmap(data)[:, 0:3] * 255
-    elif args.from_anatomy:
-        data = nib.load(args.from_anatomy).get_fdata()
-        data, lbound, ubound = transform_data(args, data)
+    cmap = get_lookup_table(args.colormap)
 
+    # Loading data. Depending on the type of loading, format data now to a 1D
+    # array (one value per point or per streamline)
+    if args.use_dps:
+        if args.use_dps not in sft.data_per_streamline.keys():
+            parser.error("DPS key {} not found in the loaded tractogram's "
+                         "data_per_streamline.".format(args.use_dps))
+        data = np.squeeze(sft.data_per_streamline[args.use_dps])
+    elif args.use_dpp:
+        if args.use_dpp not in sft.data_per_point.keys():
+            parser.error("DPP key {} not found in the loaded tractogram's "
+                         "data_per_point.".format(args.use_dpp))
+        data = np.hstack(
+            [np.squeeze(sft.data_per_point[args.use_dpp][s]) for s in
+             range(len(sft))])
+    elif args.load_dps:
+        data = np.squeeze(load_matrix_in_any_format(args.load_dps))
+        if len(data) != len(sft):
+            parser.error('Wrong dps size! Expected one value per streamline '
+                         '({}) but found {} values.'
+                         .format(len(sft), len(data)))
+    elif args.load_dpp or args.from_anatomy:
         sft.to_vox()
-        values = map_coordinates(data, sft.streamlines._data.T,
-                                 order=0)
-        color = cmap(values)[:, 0:3] * 255
+        concat_points = np.vstack(sft.streamlines).T
+        expected_shape = len(concat_points)
         sft.to_rasmm()
+        if args.load_dpp:
+            data = np.squeeze(load_matrix_in_any_format(args.load_dpp))
+            if len(data) != expected_shape:
+                parser.error('Wrong dpp size! Expected a total of {} points, '
+                             'but got {}'.format(expected_shape, len(data)))
+        else:  # args.from_anatomy:
+            data = nib.load(args.from_anatomy).get_fdata()
+            data = map_coordinates(data, concat_points, order=0)
     elif args.along_profile:
-        color = get_color_streamlines_along_length(sft, args.colormap)
-    else:
-        parser.error('No coloring method specified.')
-
-    if len(color) == len(sft):
-        tmp = [np.tile([color[i][0], color[i][1], color[i][2]],
-                       (len(sft.streamlines[i]), 1))
-               for i in range(len(sft.streamlines))]
-        sft.data_per_point['color'] = tmp
-    elif len(color) == len(sft.streamlines._data):
-        sft.data_per_point['color'] = sft.streamlines
-        sft.data_per_point['color']._data = color
+        data = get_values_along_length(sft)
+    else:  # args.local_angle:
+        data = get_angles(sft)
+
+    # Processing
+    sft, lbound, ubound = add_data_as_color_dpp(
+        sft, cmap, data, args.clip_outliers, args.min_range, args.max_range,
+        args.min_cmap, args.max_cmap, args.log, LUT)
+
+    # Saving
     save_tractogram(sft, args.out_tractogram)
 
-    # output colormap
     if args.out_colorbar:
-        save_colorbar(cmap, lbound, ubound, args)
+        _ = prepare_colorbar_figure(
+            cmap, lbound, ubound,
+            horizontal=args.horizontal_cbar, log=args.log)
+        plt.savefig(args.out_colorbar, bbox_inches='tight')
+        if args.show_colorbar:
+            plt.show()
 
 
 if __name__ == '__main__':
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_assign_uniform_color_to_tractograms.py` & `scilpy-2.0.0/scripts/scil_bundle_mean_fixel_afd_from_hdf5.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,124 +1,142 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-Assign an hexadecimal RGB color to a Trackvis TRK tractogram.
-The hexadecimal RGB color should be formatted as 0xRRGGBB or
-"#RRGGBB".
+Compute the mean Apparent Fiber Density (AFD) and mean Radial fODF (radfODF)
+maps for every connections within a hdf5 (.h5) file.
 
-Saves the RGB values in the data_per_point (color_x, color_y, color_z).
+This is the "real" fixel-based fODF amplitude along every streamline
+of each connection, averaged at every voxel.
 
-If called with .tck, the output will always be .trk, because data_per_point has
-no equivalent in tck file.
+Please use a hdf5 (.h5) file containing decomposed connections
+
+Formerly: scil_compute_fixel_afd_from_hdf5.py
 """
 
 import argparse
-import json
+import itertools
 import logging
+import multiprocessing
 import os
+import shutil
 
-from dipy.io.streamline import save_tractogram
+import h5py
+import nibabel as nib
 import numpy as np
 
-from scilpy.io.streamlines import load_tractogram_with_reference
-from scilpy.io.utils import (assert_inputs_exist,
-                             assert_outputs_exist,
-                             add_overwrite_arg,
-                             add_reference_arg)
+from scilpy.io.hdf5 import (assert_header_compatible_hdf5,
+                            reconstruct_sft_from_hdf5)
+from scilpy.io.utils import (add_overwrite_arg, add_processes_arg,
+                             add_sh_basis_args, add_verbose_arg,
+                             assert_inputs_exist, assert_outputs_exist,
+                             parse_sh_basis_arg, validate_nbr_processes)
+from scilpy.tractanalysis.afd_along_streamlines \
+    import afd_map_along_streamlines
+
+EPILOG = """
+Reference:
+    [1] Raffelt, D., Tournier, JD., Rose, S., Ridgway, GR., Henderson, R.,
+        Crozier, S., Salvado, O., & Connelly, A. (2012).
+        Apparent Fibre Density: a novel measure for the analysis of
+        diffusion-weighted magnetic resonance images. NeuroImage,
+        59(4), 3976--3994.
+"""
 
 
-def _build_arg_parser():
-    p = argparse.ArgumentParser(
-        description=__doc__,
-        formatter_class=argparse.RawTextHelpFormatter)
-
-    p.add_argument('in_tractograms', nargs='+',
-                   help='Input tractograms (.trk or .tck).')
-
-    g1 = p.add_argument_group(title='Coloring Methods')
-    p1 = g1.add_mutually_exclusive_group()
-    p1.add_argument('--fill_color',
-                    help='Can be either hexadecimal (ie. "#RRGGBB" '
-                         'or 0xRRGGBB).')
-    p1.add_argument('--dict_colors',
-                    help='Dictionnary mapping basename to color.\n'
-                         'Same convention as --fill_color.')
-
-    g2 = p.add_argument_group(title='Output options')
-    p2 = g2.add_mutually_exclusive_group()
-    p2.add_argument('--out_suffix', default='colored',
-                    help='Specify suffix to append to input basename.')
-    p2.add_argument('--out_tractogram',
-                    help='Output filename of colored tractogram (.trk).\n'
-                         'Cannot be used with --dict_colors.')
+def _afd_rd_wrapper(args):
+    in_hdf5_filename = args[0]
+    key = args[1]
+    fodf_img = args[2]
+    sh_basis = args[3]
+    length_weighting = args[4]
+    is_legacy = args[5]
+
+    with h5py.File(in_hdf5_filename, 'r') as in_hdf5_file:
+        sft, _ = reconstruct_sft_from_hdf5(in_hdf5_file, key)
+
+    if sft is None:  # No streamlines in group
+        return key, 0
+
+    afd_mean_map, rd_mean_map = afd_map_along_streamlines(sft, fodf_img,
+                                                          sh_basis,
+                                                          length_weighting,
+                                                          is_legacy=is_legacy)
+    afd_mean = np.average(afd_mean_map[afd_mean_map > 0])
 
-    add_reference_arg(p)
-    add_overwrite_arg(p)
+    return key, afd_mean
 
+
+def _build_arg_parser():
+    p = argparse.ArgumentParser(description=__doc__, epilog=EPILOG,
+                                formatter_class=argparse.RawTextHelpFormatter)
+    p.add_argument('in_hdf5',
+                   help='HDF5 filename (.h5) containing decomposed '
+                        'connections.')
+    p.add_argument('in_fodf',
+                   help='Path of the fODF volume in spherical harmonics (SH).')
+    p.add_argument('out_hdf5',
+                   help='Path of the output HDF5 filenames (.h5).')
+
+    p.add_argument('--length_weighting', action='store_true',
+                   help='If set, will weigh the AFD values according to '
+                        'segment lengths. [%(default)s]')
+
+    add_processes_arg(p)
+    add_sh_basis_args(p)
+    add_verbose_arg(p)
+    add_overwrite_arg(p)
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
-    logging.getLogger().setLevel(logging.WARNING)
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    if len(args.in_tractograms) > 1 and args.out_tractogram:
-        parser.error('Using multiple inputs, use --out_suffix.')
+    assert_inputs_exist(parser, [args.in_hdf5, args.in_fodf])
+    assert_outputs_exist(parser, args, [args.out_hdf5])
 
-    assert_inputs_exist(parser, args.in_tractograms)
+    nbr_cpu = validate_nbr_processes(parser, args)
+    sh_basis, is_legacy = parse_sh_basis_arg(args)
 
-    if args.out_suffix:
-        if args.out_tractogram:
-            args.out_suffix = ''
-        elif args.out_suffix[0] != '_':
-            args.out_suffix = '_'+args.out_suffix
-
-    out_filenames = []
-    for filename in args.in_tractograms:
-        base, ext = os.path.splitext(filename) if args.out_tractogram is None \
-            else os.path.splitext(args.out_tractogram)
-        if not ext == '.trk':
-            logging.warning('Input is TCK file, will be converted to TRK.')
-        out_filenames.append('{}{}{}'.format(base, args.out_suffix, '.trk'))
-    assert_outputs_exist(parser, args, out_filenames)
-
-    for i, filename in enumerate(args.in_tractograms):
-        sft = load_tractogram_with_reference(parser, args, filename)
-        base, ext = os.path.splitext(filename)
-        out_filename = out_filenames[i]
-        pos = base.index('__') if '__' in base else -2
-        base = base[pos+2:]
-        color = None
-        if args.dict_colors:
-            with open(args.dict_colors, 'r') as data:
-                dict_colors = json.load(data)
-            # Supports variation from rbx-flow
-            for key in dict_colors.keys():
-                if key in base:
-                    color = dict_colors[key]
-        elif args.fill_color is not None:
-            color = args.fill_color
-
-        if color is None:
-            color = '0x000000'
-
-        if len(color) == 7:
-            args.fill_color = '0x' + args.fill_color.lstrip('#')
-
-        if len(color) == 8:
-            color_int = int(color, 0)
-            red = color_int >> 16
-            green = (color_int & 0x00FF00) >> 8
-            blue = color_int & 0x0000FF
-        else:
-            parser.error('Hexadecimal RGB color should be formatted as '
-                         '"#RRGGBB" or 0xRRGGBB.')
-        tmp = [np.tile([red, green, blue], (len(i), 1))
-               for i in sft.streamlines]
-        sft.data_per_point['color'] = tmp
-        save_tractogram(sft, out_filename)
+    # HDF5 will not overwrite the file
+    if os.path.isfile(args.out_hdf5):
+        os.remove(args.out_hdf5)
+
+    fodf_img = nib.load(args.in_fodf)
+    with h5py.File(args.in_hdf5, 'r') as in_hdf5_file:
+        assert_header_compatible_hdf5(in_hdf5_file, fodf_img)
+        keys = list(in_hdf5_file.keys())
+        in_hdf5_file.close()
+
+    if nbr_cpu == 1:
+        results_list = []
+        for key in keys:
+            results_list.append(_afd_rd_wrapper([args.in_hdf5, key, fodf_img,
+                                                 sh_basis,
+                                                 args.length_weighting,
+                                                 is_legacy]))
+
+    else:
+        pool = multiprocessing.Pool(nbr_cpu)
+        results_list = pool.map(_afd_rd_wrapper,
+                                zip(itertools.repeat(args.in_hdf5),
+                                    keys,
+                                    itertools.repeat(fodf_img),
+                                    itertools.repeat(sh_basis),
+                                    itertools.repeat(args.length_weighting),
+                                    itertools.repeat(is_legacy)))
+        pool.close()
+        pool.join()
+
+    shutil.copy(args.in_hdf5, args.out_hdf5)
+    with h5py.File(args.out_hdf5, 'a') as out_hdf5_file:
+        for key, afd_fixel in results_list:
+            group = out_hdf5_file[key]
+            if 'afd_fixel' in group:
+                del group['afd_fixel']
+            group.create_dataset('afd_fixel', data=afd_fixel)
 
 
 if __name__ == '__main__':
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_clean_qbx_clusters.py` & `scilpy-2.0.0/scripts/scil_bundle_clean_qbx_clusters.py`

 * *Files 1% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 # -*- coding: utf-8 -*-
 
 """
     Render clusters sequentially to either accept or reject them based on
     visual inspection. Useful for cleaning bundles for RBx, BST or for figures.
     The VTK window does not handle well opacity of streamlines, this is a
     normal rendering behavior.
-    Often use in pair with scil_compute_qbx.py.
+    Often use in pair with scil_tractogram_qbx.py.
 
     Key mapping:
     - a/A: accept displayed clusters
     - r/R: reject displayed clusters
     - z/Z: Rewing one element
     - c/C: Stop rendering of the background concatenation of streamlines
     - q/Q: Early window exist, everything remaining will be rejected
@@ -20,24 +20,24 @@
 import argparse
 import os
 import logging
 
 from fury import window, actor, interactor
 from dipy.io.stateful_tractogram import Space, StatefulTractogram
 from dipy.io.streamline import save_tractogram
-from dipy.io.utils import is_header_compatible
 
 from scilpy.io.streamlines import load_tractogram_with_reference
 from scilpy.io.utils import (add_bbox_arg,
                              add_overwrite_arg,
                              add_reference_arg,
                              add_verbose_arg,
                              assert_inputs_exist,
                              assert_outputs_exist,
-                             assert_output_dirs_exist_and_empty)
+                             assert_output_dirs_exist_and_empty,
+                             assert_headers_compatible)
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(description=__doc__,
                                 formatter_class=argparse.RawTextHelpFormatter)
 
     p.add_argument('in_bundles', nargs='+',
@@ -55,22 +55,23 @@
     p.add_argument('--min_cluster_size', type=int, default=1,
                    help='Minimum cluster size for consideration [%(default)s].'
                         'Must be at least 1.')
     p.add_argument('--background_opacity', type=float, default=0.1,
                    help='Opacity of the background streamlines.'
                         'Keep low between 0 and 0.5 [%(default)s].')
     p.add_argument('--background_linewidth', type=float, default=1,
-                   help='Linewidth of the background streamlines [%(default)s].')
+                   help='Linewidth of the background streamlines [%(default)s]'
+                   '.')
     p.add_argument('--clusters_linewidth', type=float, default=1,
                    help='Linewidth of the current cluster [%(default)s].')
 
     add_reference_arg(p)
-    add_overwrite_arg(p)
-    add_verbose_arg(p)
     add_bbox_arg(p)
+    add_verbose_arg(p)
+    add_overwrite_arg(p)
 
     return p
 
 
 def main():
     # Callback required for FURY
     def keypress_callback(obj, _):
@@ -144,30 +145,30 @@
             print('No more cluster, press q to exit')
             renderer.rm(curr_streamlines_actor)
 
         renwin.Render()
 
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, args.in_bundles)
+    assert_inputs_exist(parser, args.in_bundles, args.reference)
     assert_outputs_exist(parser, args, [args.out_accepted, args.out_rejected])
+    assert_headers_compatible(parser, args.in_bundles,
+                              reference=args.reference)
 
     if args.out_accepted_dir:
         assert_output_dirs_exist_and_empty(parser, args,
                                            args.out_accepted_dir,
                                            create_dir=True)
     if args.out_rejected_dir:
         assert_output_dirs_exist_and_empty(parser, args,
                                            args.out_rejected_dir,
                                            create_dir=True)
 
-    if args.verbose:
-        logging.getLogger().setLevel(logging.INFO)
-
     if args.min_cluster_size < 1:
         parser.error('Minimum cluster size must be at least 1.')
 
     clusters_linewidth = args.clusters_linewidth
     background_linewidth = args.background_linewidth
 
     # To accelerate procedure, clusters can be discarded based on size
@@ -178,16 +179,14 @@
 
     ref_bundle = load_tractogram_with_reference(
         parser, args, args.in_bundles[0])
 
     for filename in args.in_bundles:
         basename = os.path.basename(filename)
         sft = load_tractogram_with_reference(parser, args, filename)
-        if not is_header_compatible(ref_bundle, sft):
-            return
         if len(sft) >= args.min_cluster_size:
             sft_accepted_on_size.append(sft)
             filename_accepted_on_size.append(basename)
             concat_streamlines.extend(sft.streamlines)
         else:
             logging.info('File {} has {} streamlines,'
                          'automatically rejected.'.format(filename, len(sft)))
```

### Comparing `scilpy-1.5.post2/scripts/scil_combine_labels.py` & `scilpy-2.0.0/scripts/scil_labels_combine.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,34 +1,36 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-    Script to combine labels from multiple volumes.
-    If there is overlap, it will overwrite them based on the input order.
+Script to combine labels from multiple volumes. If there is overlap, it will
+overwrite them based on the input order.
 
-    >>> scil_combine_labels.py out_labels.nii.gz
+    >>> scil_labels_combine.py out_labels.nii.gz
             --volume_ids animal_labels.nii 20
             --volume_ids DKT_labels.nii.gz 44 53
             --out_labels_indices 20 44 53
-    >>> scil_combine_labels.py slf_labels.nii.gz
+    >>> scil_labels_combine.py slf_labels.nii.gz
             --volume_ids a2009s_aseg.nii.gz all
             --volume_ids clean/s1__DKT.nii.gz 1028 2028
+
+Formerly: scil_combine_labels.py.
 """
 
 
 import argparse
 import logging
 
 from dipy.io.utils import is_header_compatible
 import nibabel as nib
 import numpy as np
 
 from scilpy.image.labels import get_data_as_labels, combine_labels
 from scilpy.io.utils import (add_overwrite_arg, assert_inputs_exist,
-                             assert_outputs_exist)
+                             add_verbose_arg, assert_outputs_exist)
 
 
 EPILOG = """
     References:
         [1] Al-Sharif N.B., St-Onge E., Vogel J.W., Theaud G.,
             Evans A.C. and Descoteaux M. OHBM 2019.
             Surface integration for connectome analysis in age prediction.
@@ -61,21 +63,25 @@
 
     p.add_argument('--background', type=int, default=0,
                    help='Background id, excluded from output [%(default)s],\n'
                         ' the value is used as output background value.')
     p.add_argument('--merge_groups', action='store_true',
                    help='Each group from the --volume_ids option will be '
                         'merged as a single labels.')
+    
+    add_verbose_arg(p)
     add_overwrite_arg(p)
+    
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     image_files = []
     indices_per_input_volume = []
     # Separate argument per volume
     used_indices_all = False
     for v_args in args.volume_ids:
         if len(v_args) < 2:
```

### Comparing `scilpy-1.5.post2/scripts/scil_compress_streamlines.py` & `scilpy-2.0.0/scripts/scil_tractogram_compress.py`

 * *Files 16% similar despite different names*

```diff
@@ -2,70 +2,63 @@
 # -*- coding: utf-8 -*-
 
 """
 Compress tractogram by removing collinear (or almost) points.
 
 The compression threshold represents the maximum distance (in mm) to the
 original position of the point.
+
+Formerly: scil_compress_streamlines.py
 """
 
 import argparse
 import logging
 
-from dipy.tracking.streamlinespeed import compress_streamlines
 import nibabel as nib
 from nibabel.streamlines import LazyTractogram
 import numpy as np
 
 from scilpy.io.streamlines import check_tracts_same_format
-from scilpy.io.utils import (add_overwrite_arg,
-                             assert_inputs_exist,
-                             assert_outputs_exist)
+from scilpy.tractograms.tractogram_operations import \
+    compress_streamlines_wrapper
+from scilpy.io.utils import (add_overwrite_arg, add_verbose_arg,
+                             assert_inputs_exist, assert_outputs_exist,
+                             verify_compression_th)
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__, formatter_class=argparse.RawTextHelpFormatter)
     p.add_argument('in_tractogram',
                    help='Path of the input tractogram file (trk or tck).')
     p.add_argument('out_tractogram',
                    help='Path of the output tractogram file (trk or tck).')
 
     p.add_argument('-e', dest='error_rate', type=float, default=0.1,
                    help='Maximum compression distance in mm [%(default)s].')
+
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
-def compress_streamlines_wrapper(tractogram, error_rate):
-    return lambda: (compress_streamlines(
-        s, error_rate) for s in tractogram.streamlines)
-
-
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, args.in_tractogram)
     assert_outputs_exist(parser, args, args.out_tractogram)
     check_tracts_same_format(parser, args.in_tractogram, args.out_tractogram)
-
-    if args.error_rate < 0.001 or args.error_rate > 1:
-        logging.warning(
-            'You are using an error rate of {}.\n'
-            'We recommend setting it between 0.001 and 1.\n'
-            '0.001 will do almost nothing to the streamlines\n'
-            'while 1 will highly compress/linearize the streamlines'
-            .format(args.error_rate))
+    verify_compression_th(args.error_rate)
 
     in_tractogram = nib.streamlines.load(args.in_tractogram, lazy_load=True)
     compressed_streamlines = compress_streamlines_wrapper(in_tractogram,
                                                           args.error_rate)
-
     out_tractogram = LazyTractogram(compressed_streamlines,
                                     affine_to_rasmm=np.eye(4))
     nib.streamlines.save(out_tractogram, args.out_tractogram,
                          header=in_tractogram.header)
 
 
 if __name__ == "__main__":
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_MT_maps.py` & `scilpy-2.0.0/scripts/scil_mti_maps_ihMT.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,377 +1,297 @@
-#! /usr/bin/env python3
+#!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 """
-This script computes two myelin indices maps from the Magnetization Transfer
-(MT) images.
-Magnetization Transfer is a contrast mechanism in tissue resulting from the
-proton exchange between non-aqueous protons (from macromolecules and their
-closely associated water molecules, the "bound" pool) and protons in the free
-water pool called aqueous protons. This exchange attenuates the MRI signal,
-introducing microstructure-dependent contrast. MT's effect reflects the
-relative density of macromolecules such as proteins and lipids, it has been
-associated with myelin content in white matter of the brain.
-
-Different contrasts can be done with an off-resonance pulse to saturating the
-protons on non-aqueous molecules a frequency irradiation. The MT maps are
-obtained using three contrasts: single frequency irradiation (MT-on, saturated
-images) and an unsaturated contrast (MT-off); and a T1weighted image as
-reference.
-
-The output consist in two types of images:
-        Three contrasts images : MT-off, MT-on and T1weighted images.
-        MT maps corrected or not for an empiric B1 correction maps.
+This script computes four myelin indices maps from the Magnetization Transfer
+(MT) and inhomogeneous Magnetization Transfer (ihMT) images. Magnetization
+Transfer is a contrast mechanism in tissue resulting from the proton exchange
+between non-aqueous protons (from macromolecules and their closely associated
+water molecules, the "bound" pool) and protons in the free water pool called
+aqueous protons. This exchange attenuates the MRI signal, introducing
+microstructure-dependent contrast. MT's effect reflects the relative density
+of macromolecules such as proteins and lipids, it has been associated with
+myelin content in white matter of the brain.
+
+Different contrasts can be done with an off-resonance pulse prior to image
+acquisition (a prepulse), saturating the protons on non-aqueous molecules,
+by applying different frequency irradiation. The two MT maps and two ihMT maps
+are obtained using six contrasts: single positive frequency image, single
+negative frequency image, dual alternating positive/negative frequency image,
+dual alternating negative/positive frequency image (saturated images);
+and two unsaturated contrasts as reference. These two references should be
+acquired with predominant PD (proton density) and T1 weighting at different
+excitation flip angles (a_PD, a_T1) and repetition times (TR_PD, TR_T1).
+
 
 Input Data recommendation:
   - it is recommended to use dcm2niix (v1.0.20200331) to convert data
     https://github.com/rordenlab/dcm2niix/releases/tag/v1.0.20200331
   - dcm2niix conversion will create all echo files for each contrast and
     corresponding json files
-  - all input must have a matching json file with the same filename
   - all contrasts must have a same number of echoes and coregistered
-    between them before running the script.
+    between them before running the script
   - Mask must be coregistered to the echo images
   - ANTs can be used for the registration steps (http://stnava.github.io/ANTs/)
 
 
-The output consist in two types of images in two folders :
-  1. Contrasts_MT_maps which contains the 2 contrast images
-      - MT-off.nii.gz : pulses applied at positive frequency
-      - MT-on.nii.gz : pulses applied at negative frequency
-      - T1w.nii.gz : anatomical T1 reference images
-
-
-  2. MT_native_maps which contains the 4 myelin maps
-      - MTR.nii.gz : Magnetization Transfer Ratio map
-      The MT ratio is a measure reflecting the amount of bound protons.
-
-      - MTsat.nii.gz : Magnetization Transfer saturation map
-      The MT saturation is a pseudo-quantitative maps representing
-      the signal change between the bound and free water pools.
-
->>> scil_compute_ihMT_maps.py path/to/output/directory path/to/mask_bin.nii.gz
-    --in_mtoff path/to/echo*mtoff.nii.gz --in_mton path/to/echo*mton.nii.gz
-    --in_t1w path/to/echo*T1w.nii.gz
-
+The output consists of a ihMT_native_maps folder containing the 4 myelin maps:
+    - MTR.nii.gz : Magnetization Transfer Ratio map
+    - ihMTR.nii.gz : inhomogeneous Magnetization Transfer Ratio map
+    The (ih)MT ratio is a measure reflecting the amount of bound protons.
+    - MTsat.nii.gz : Magnetization Transfer saturation map
+    - ihMTsat.nii.gz : inhomogeneous Magnetization Transfer saturation map
+    The (ih)MT saturation is a pseudo-quantitative maps representing
+    the signal change between the bound and free water pools.
+
+As an option, the Complementary_maps folder contains the following images:
+    - altnp.nii.gz : dual alternating negative and positive frequency image
+    - altpn.nii.gz : dual alternating positive and negative frequency image
+    - positive.nii.gz : single positive frequency image
+    - negative.nii.gz : single negative frequency image
+    - mtoff_PD.nii.gz : unsaturated proton density weighted image
+    - mtoff_T1.nii.gz : unsaturated T1 weighted image
+    - MTsat_d.nii.gz : MTsat computed from the mean dual frequency images
+    - MTsat_sp.nii.gz : MTsat computed from the single positive frequency image
+    - MTsat_sn.nii.gz : MTsat computed from the single negative frequency image
+    - R1app.nii.gz : Apparent R1 map computed for MTsat.
+    - B1_map.nii.gz : B1 map after correction and smoothing (if given).
+
+
+The final maps from ihMT_native_maps can be corrected for B1+ field
+  inhomogeneity, using either an empiric method with
+  --in_B1_map option, suffix *B1_corrected is added for each map.
+  --B1_correction_method empiric
+  or a model-based method with
+  --in_B1_map option, suffix *B1_corrected is added for each map.
+  --B1_correction_method model_based
+  --B1_fitValues 3 .mat files, obtained externally from
+    https://github.com/TardifLab/OptimizeIHMTimaging/tree/master/b1Correction,
+    and given in this order: positive frequency saturation, negative frequency
+    saturation, dual frequency saturation.
+For both methods, the nominal value of the B1 map can be set with
+  --B1_nominal value
+
+
+>>> scil_mti_maps_ihMT.py path/to/output/directory
+    --in_altnp path/to/echo*altnp.nii.gz --in_altpn path/to/echo*altpn.nii.gz
+    --in_mtoff_pd path/to/echo*mtoff.nii.gz
+    --in_negative path/to/echo*neg.nii.gz --in_positive path/to/echo*pos.nii.gz
+    --in_mtoff_t1 path/to/echo*T1w.nii.gz --mask path/to/mask_bin.nii.gz
+    --in_jsons path/to/echo*mtoff.json path/to/echo*T1w.json
+
+By default, the script uses all the echoes available in the input folder.
+If you want to use a single echo, replace the * with the specific number of
+the echo.
 """
 
 import argparse
+import logging
 import os
-import json
 
 import nibabel as nib
 import numpy as np
-import scipy.ndimage
 
-from scilpy.io.image import get_data_as_mask
-from scilpy.io.utils import (add_overwrite_arg, assert_inputs_exist,
+from scilpy.io.mti import add_common_args_mti, load_and_verify_mti
+from scilpy.io.utils import (add_overwrite_arg,
+                             assert_inputs_exist, add_verbose_arg,
                              assert_output_dirs_exist_and_empty)
+from scilpy.reconst.mti import (apply_B1_corr_empiric,
+                                apply_B1_corr_model_based,
+                                compute_ratio_map,
+                                compute_saturation_map,
+                                threshold_map)
 
 EPILOG = """
+Varma G, Girard OM, Prevost VH, Grant AK, Duhamel G, Alsop DC.
+Interpretation of magnetization transfer from inhomogeneously broadened lines
+(ihMT) in tissues as a dipolar order effect within motion restricted molecules.
+Journal of Magnetic Resonance. 1 nov 2015;260:67-76.
+
+Manning AP, Chang KL, MacKay AL, Michal CA. The physical mechanism of
+"inhomogeneous" magnetization transfer MRI. Journal of Magnetic Resonance.
+1 janv 2017;274:125-36.
+
 Helms G, Dathe H, Kallenberg K, Dechent P. High-resolution maps of
 magnetization transfer with inherent correction for RF inhomogeneity
 and T1 relaxation obtained from 3D FLASH MRI. Magnetic Resonance in Medicine.
 2008;60(6):1396-407.
 """
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(description=__doc__,
                                 formatter_class=argparse.RawTextHelpFormatter)
     p.add_argument('out_dir',
                    help='Path to output folder.')
-    p.add_argument('in_mask',
-                   help='Path to the T1 binary brain mask. Must be the sum '
-                        'of the three tissue probability maps from '
-                        'T1 segmentation (GM+WM+CSF).')
     p.add_argument('--out_prefix',
                    help='Prefix to be used for each output image.')
-    p.add_argument('--in_B1_map',
-                   help='Path to B1 coregister map to MT contrasts.')
+    p.add_argument('--mask',
+                   help='Path to the binary brain mask.')
 
-    g = p.add_argument_group(title='MT contrasts', description='Path to '
-                             'echoes corresponding to contrasts images. All '
-                             'constrasts must have the same number of echoes '
-                             'and coregistered between them.'
+    g = p.add_argument_group(title='Contrast maps', description='Path to '
+                             'echoes corresponding to contrast images. All '
+                             'constrasts must have \nthe same number of '
+                             'echoes and coregistered between them. '
                              'Use * to include all echoes.')
-    g.add_argument("--in_mtoff", nargs='+',
+    g.add_argument('--in_altnp', nargs='+', required=True,
                    help='Path to all echoes corresponding to the '
-                        'no frequency saturation pulse (reference image).')
-    g.add_argument("--in_mton", nargs='+',
+                        'alternation of \nnegative and positive '
+                        'frequency saturation pulse.')
+    g.add_argument('--in_altpn', nargs='+', required=True,
                    help='Path to all echoes corresponding to the '
-                        'Positive frequency saturation pulse.')
-    g.add_argument("--in_t1w", nargs='+',
+                        'alternation of \npositive and negative '
+                        'frequency saturation pulse.')
+    g.add_argument("--in_negative", nargs='+', required=True,
                    help='Path to all echoes corresponding to the '
-                        'T1-weigthed.')
+                        'negative frequency \nsaturation pulse.')
+    g.add_argument("--in_positive", nargs='+', required=True,
+                   help='Path to all echoes corresponding to the '
+                        'positive frequency \nsaturation pulse.')
+    g.add_argument("--in_mtoff_pd", nargs='+', required=True,
+                   help='Path to all echoes corresponding to the predominant '
+                        'PD \n(proton density) weighting images with no '
+                        'saturation pulse.')
+    g.add_argument("--in_mtoff_t1", nargs='+',
+                   help='Path to all echoes corresponding to the predominant '
+                        'T1 \nweighting images with no saturation pulse. This '
+                        'one is optional, \nsince it is only needed for the '
+                        'calculation of MTsat and ihMTsat. \nAcquisition '
+                        'parameters should also be set with this image.')
+
+    # Other MTI arguments are gathered here.
+    add_common_args_mti(p)
 
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
-def set_acq_parameters(json_path):
-    """
-    Function to extract Repetition Time and Flip Angle from json file.
-
-    Parameters
-    ----------
-    json_path   Path to the json file
-
-    Returns
-    ----------
-    Return Repetition Time (in second) and Flip Angle (in radians)
-    """
-    with open(json_path) as f:
-        data = json.load(f)
-    TR = data['RepetitionTime']*1000
-    FlipAngle = data['FlipAngle']*np.pi/180
-    return TR, FlipAngle
-
-
-def merge_images(echoes_image):
-    """
-    Function to load each echo in a 3D-array matrix and
-    concatenate each of them along the 4th dimension.
-
-    Parameters
-    ----------
-    echoes_image     List : list of echoes path for each contrasts. Ex.
-                     ['path/to/echo-1_acq-pos',
-                      'path/to/echo-2_acq-pos',
-                      'path/to/echo-3_acq-pos']
-
-    Returns
-    ----------
-    Return a 4D-array matrix of size x, y, z, n where n represented
-    the number of echoes.
-    """
-    merge_array = []
-    for echo in range(len(echoes_image)):
-        load_image = nib.load(echoes_image[echo])
-        merge_array.append(load_image.get_fdata(dtype=np.float32))
-    merge_array = np.stack(merge_array, axis=-1)
-    return merge_array
-
-
-def compute_contrasts_maps(echoes_image):
-    """
-    Load echoes and compute corresponding contrast map.
-
-    Parameters
-    ----------
-    echoes_image    List of file path : list of echoes path for contrast
-    filtering       Apply Gaussian filtering to remove Gibbs ringing
-                    (default is False).
-
-    Returns
-    -------
-    Contrast map in 3D-Array.
-    """
-
-    # Merged the 3 echo images into 4D-array
-    merged_map = merge_images(echoes_image)
-
-    # Compute the sum of contrast map
-    contrast_map = np.sqrt(np.sum(np.squeeze(merged_map**2), 3))
-
-    return contrast_map
-
-
-def compute_MT_maps(contrasts_maps, acq_parameters):
-    """
-    Compute Magnetization transfer ratio and saturation maps.
-    MT ratio is computed as the percentage difference of two images, one
-    acquired with off-resonance saturation (MT-on) and one without (MT-off).
-    MT saturation is computed from apparent longitudinal relaxation rate
-    (R1app) and apparent signal amplitude (Aapp). The estimation of the MT
-    saturation includes correction for the effects of excitation flip angle
-    and longitudinal relaxation rate, and remove the effect of T1-weighted
-    image.
-        cPD : contrast proton density
-            1 : reference proton density (MT-off)
-            2 : mean of positive and negative proton density (MT-on)
-        cT1 : contrast T1-weighted
-        num : numberator
-        den : denumerator
-
-    see Helms et al., 2008
-    https://onlinelibrary.wiley.com/doi/full/10.1002/mrm.21732
-
-    Parameters
-    ----------
-    contrasts_maps:      List of 3D-array constrats matrices : list of all
-                        contrast maps computed with compute_ihMT_contrasts
-    acq_parameters:     List of TR and Flipangle for ihMT and T1w images
-                        [TR, Flipangle]
-    Returns
-    -------
-    MT ratio and MT saturation matrice in 3D-array.
-    """
-    # Compute MT Ratio map
-    MTR = 100*(contrasts_maps[0] - contrasts_maps[1]) / contrasts_maps[0]
-
-    # Compute MT saturation maps: cPD1 = mt-off; cPD2 = mt-on
-    cPD1 = contrasts_maps[0]
-    cPD2 = contrasts_maps[1]
-    cT1 = contrasts_maps[2]
-
-    Aapp_num = ((2*acq_parameters[0][0] / (acq_parameters[0][1]**2)) -
-                (2*acq_parameters[1][0] / (acq_parameters[1][1]**2)))
-    Aapp_den = (((2*acq_parameters[0][0]) / (acq_parameters[0][1]*cPD1)) -
-                ((2*acq_parameters[1][0]) / (acq_parameters[1][1]*cT1)))
-    Aapp = Aapp_num / Aapp_den
-
-    R1app_num = ((cPD1 / acq_parameters[0][1]) - (cT1 / acq_parameters[1][1]))
-    R1app_den = ((cT1*acq_parameters[1][1]) / (2*acq_parameters[1][0]) -
-                 (cPD1*acq_parameters[0][1]) / (2*acq_parameters[0][0]))
-    R1app = R1app_num / R1app_den
-
-    MTsat = 100*(((Aapp*acq_parameters[0][1]*acq_parameters[0][0]/R1app)/cPD2)
-                 - (acq_parameters[0][0]/R1app) - (acq_parameters[0][1]**2)/2)
-
-    return MTR, MTsat
-
-
-def threshold_MT_maps(computed_map, in_mask, lower_threshold, upper_threshold):
-    """
-    Remove NaN and apply different threshold based on
-       - maximum and minimum threshold value
-       - T1 mask
-
-    Parameters
-    ----------
-    computed_map        3D-Array Myelin map.
-    in_mask             Path to binary T1 mask from T1 segmentation.
-                        Must be the sum of GM+WM+CSF.
-    lower_threshold     Value for low thresold <int>
-    upper_thresold      Value for up thresold <int>
-
-    Returns
-    ----------
-    Thresholded matrix in 3D-array.
-    """
-    # Remove NaN and apply thresold based on lower and upper value
-    computed_map[np.isnan(computed_map)] = 0
-    computed_map[np.isinf(computed_map)] = 0
-    computed_map[computed_map < lower_threshold] = 0
-    computed_map[computed_map > upper_threshold] = 0
-
-    # Load and apply sum of T1 probability maps on myelin maps
-    mask_image = nib.load(in_mask)
-    mask_data = get_data_as_mask(mask_image)
-    computed_map[np.where(mask_data == 0)] = 0
-
-    return computed_map
-
-
-def apply_B1_correction(MT_map, B1_map):
-    """
-    Function to apply an empiric B1 correction.
-
-    see Weiskopf et al., 2013
-    https://www.frontiersin.org/articles/10.3389/fnins.2013.00095/full
-
-    Parameters
-    ----------
-    MT_map           3D-Array Myelin map.
-    B1_map           Path to B1 coregister map.
-
-    Returns
-    ----------
-    Corrected MT matrix in 3D-array.
-    """
-    # Load B1 image
-    B1_img = nib.load(B1_map)
-    B1_img_data = B1_img.get_fdata(dtype=np.float32)
-
-    # Apply a light smoothing to the B1 map
-    h = np.ones((5, 5, 1))/25
-    B1_smooth_map = scipy.ndimage.convolve(B1_img_data,
-                                           h).astype(np.float32)
-
-    # Apply an empiric B1 correction via B1 smooth on MT data
-    MT_map_B1_corrected = MT_map*(1.0-0.4)/(1-0.4*(B1_smooth_map/100))
-
-    return MT_map_B1_corrected
-
-
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_output_dirs_exist_and_empty(parser, args,
-                                       os.path.join(args.out_dir,
-                                                    'Contrasts_MT_maps'),
-                                       os.path.join(args.out_dir,
-                                                    'MT_native_maps'),
-                                       create_dir=True)
-
-    # Merge all echos path into a list
-    maps = [args.in_mtoff, args.in_mton, args.in_t1w]
-
-    maps_flat = (args.in_mtoff + args.in_mton + args.in_t1w)
-
-    jsons = [curr_map.replace('.nii.gz', '.json')
-             for curr_map in maps_flat]
-
-    # check data
-    assert_inputs_exist(parser, jsons + maps_flat)
-    for curr_map in maps[1:]:
-        if len(curr_map) != len(maps[0]):
-            parser.error('Not the same number of echoes per contrast')
-
-    # Set TR and FlipAngle parameters for MT (mtoff contrast)
-    # and T1w images
-    parameters = [set_acq_parameters(maps[0][0].replace('.nii.gz', '.json')),
-                  set_acq_parameters(maps[2][0].replace('.nii.gz', '.json'))]
-
-    # Fix issue from the presence of invalide value and division by zero
-    np.seterr(divide='ignore', invalid='ignore')
-
-    # Define reference image for saving maps
-    ref_img = nib.load(maps[0][0])
-
-    # Define contrasts maps names
-    contrasts_name = ['mt_off', 'mt_on', 'T1w']
+    output_dir = os.path.join(args.out_dir, 'ihMT_native_maps')
+    extended_dir = None
+    if args.extended:
+        extended_dir = os.path.join(args.out_dir, 'Complementary_maps')
+        assert_output_dirs_exist_and_empty(parser, args, extended_dir,
+                                           output_dir, create_dir=True)
+    else:
+        assert_output_dirs_exist_and_empty(parser, args, output_dir,
+                                           create_dir=True)
 
     if args.out_prefix:
-        contrasts_name = [args.out_prefix + '_' + curr_name
-                          for curr_name in contrasts_name]
+        out_prefix = args.out_prefix + "_"
+    else:
+        out_prefix = ""
+
+    # Combine all echos path into a list of lists
+    input_maps_lists = [args.in_altnp, args.in_altpn, args.in_negative,
+                        args.in_positive, args.in_mtoff_pd]
+    if args.in_mtoff_t1:
+        input_maps_lists.append(args.in_mtoff_t1)
+
+    input_maps_flat_list = [m for _list in input_maps_lists for m in _list]
+    assert_inputs_exist(parser, input_maps_flat_list,
+                        optional=args.in_mtoff_t1 or [] + [args.mask])
 
-    # Compute contrasts maps
-    computed_contrasts = []
-    for idx, curr_map in enumerate(maps):
-        computed_contrasts.append(compute_contrasts_maps(curr_map))
-
-        nib.save(nib.Nifti1Image(computed_contrasts[idx].astype(np.float32),
-                                 ref_img.affine),
-                 os.path.join(args.out_dir, 'Contrasts_MT_maps',
-                              contrasts_name[idx] + '.nii.gz'))
-
-    # Compute and thresold MT maps
-    MTR, MTsat = compute_MT_maps(computed_contrasts, parameters)
-    for curr_map in MTR, MTsat:
-        curr_map = threshold_MT_maps(curr_map, args.in_mask, 0, 100)
-        if args.in_B1_map:
-            curr_map = apply_B1_correction(curr_map, args.in_B1_map)
+    # Define affine. Uses the first in_mtoff_pd (required).
+    affine = nib.load(input_maps_lists[4][0]).affine
 
-    # Save MT maps
-    img_name = ['MTR', 'MTsat']
+    # Define contrasts maps names
+    contrast_names = ['altnp', 'altpn', 'negative', 'positive', 'mtoff_PD',
+                      'mtoff_T1']
 
+    # Other checks, loading, saving contrast_maps.
+    single_echo, flip_angles, rep_times, B1_map, contrast_maps = \
+        load_and_verify_mti(args, parser, input_maps_lists, extended_dir,
+                            affine, contrast_names)
+
+    # Compute ratio maps
+    MTR, ihMTR = compute_ratio_map((contrast_maps[2] + contrast_maps[3]) / 2,
+                                   contrast_maps[4],
+                                   mt_on_dual=(contrast_maps[0] +
+                                               contrast_maps[1]) / 2)
+    img_name = ['ihMTR', 'MTR']
+    img_data = [ihMTR, MTR]
+
+    # Compute saturation maps
+    if args.in_mtoff_t1:
+        MTsat_sp, T1app = compute_saturation_map(contrast_maps[3],
+                                                 contrast_maps[4],
+                                                 contrast_maps[5],
+                                                 flip_angles, rep_times)
+        MTsat_sn, _ = compute_saturation_map(contrast_maps[2],
+                                             contrast_maps[4],
+                                             contrast_maps[5],
+                                             flip_angles, rep_times)
+        MTsat_d, _ = compute_saturation_map((contrast_maps[0] +
+                                             contrast_maps[1]) / 2,
+                                            contrast_maps[4], contrast_maps[5],
+                                            flip_angles, rep_times)
+        R1app = 1000 / T1app  # convert 1/ms to 1/s
+        if args.extended:
+            nib.save(nib.Nifti1Image(MTsat_sp, affine),
+                     os.path.join(extended_dir,
+                                  out_prefix + "MTsat_single_positive.nii.gz"))
+            nib.save(nib.Nifti1Image(MTsat_sn, affine),
+                     os.path.join(extended_dir,
+                                  out_prefix + "MTsat_single_negative.nii.gz"))
+            nib.save(nib.Nifti1Image(MTsat_d, affine),
+                     os.path.join(extended_dir,
+                                  out_prefix + "MTsat_dual.nii.gz"))
+            nib.save(nib.Nifti1Image(R1app, affine),
+                     os.path.join(extended_dir,
+                                  out_prefix + "apparent_R1.nii.gz"))
+
+        MTsat_maps = [MTsat_sp, MTsat_sn, MTsat_d]
+
+        # Apply model-based B1 correction
+        if args.in_B1_map and args.B1_correction_method == 'model_based':
+            for i, MTsat_map in enumerate(MTsat_maps):
+                MTsat_maps[i] = apply_B1_corr_model_based(MTsat_map, B1_map,
+                                                          R1app,
+                                                          args.B1_fitvalues[i])
+
+        # Compute MTsat and ihMTsat from saturations
+        MTsat = (MTsat_maps[0] + MTsat_maps[1]) / 2
+        ihMTsat = MTsat_maps[2] - MTsat
+
+        # Apply empiric B1 correction
+        if args.in_B1_map and args.B1_correction_method == 'empiric':
+            MTsat = apply_B1_corr_empiric(MTsat, B1_map)
+            ihMTsat = apply_B1_corr_empiric(ihMTsat, B1_map)
+
+        img_name.extend(('ihMTsat', 'MTsat'))
+        img_data.extend((ihMTsat, MTsat))
+
+    # Apply thresholds on maps
+    upper_thresholds = [100, 100, 10, 10]
+    idx_contrast_lists = [[0, 1, 2, 3, 4], [3, 4], [0, 1, 2, 3], [3, 4]]
+    for i, map in enumerate(img_data):
+        img_data[i] = threshold_map(map, args.mask, 0, upper_thresholds[i],
+                                    idx_contrast_list=idx_contrast_lists[i],
+                                    contrast_maps=contrast_maps)
+
+    # Save ihMT and MT images
+    if args.filtering:
+        img_name = [curr_name + '_filter'
+                    for curr_name in img_name]
+    if single_echo:
+        img_name = [curr_name + '_single_echo'
+                    for curr_name in img_name]
     if args.in_B1_map:
         img_name = [curr_name + '_B1_corrected'
                     for curr_name in img_name]
-
     if args.out_prefix:
         img_name = [args.out_prefix + '_' + curr_name
                     for curr_name in img_name]
 
-    img_data = MTR, MTsat
     for img_to_save, name in zip(img_data, img_name):
-        nib.save(nib.Nifti1Image(img_to_save.astype(np.float32),
-                                 ref_img.affine),
-                 os.path.join(args.out_dir, 'MT_native_maps',
-                              name + '.nii.gz'))
+        nib.save(nib.Nifti1Image(img_to_save.astype(np.float32), affine),
+                 os.path.join(output_dir, name + '.nii.gz'))
 
 
 if __name__ == '__main__':
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_NODDI.py` & `scilpy-2.0.0/scripts/scil_NODDI_maps.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,65 +1,71 @@
 #! /usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
 Compute NODDI [1] maps using AMICO.
 Multi-shell DWI necessary.
+
+Formerly: scil_compute_NODDI.py
 """
 
 import argparse
 from contextlib import redirect_stdout
 import io
 import logging
 import os
 import sys
 import tempfile
 
 import amico
 from dipy.io.gradients import read_bvals_bvecs
 import numpy as np
 
+from scilpy.io.gradients import fsl2mrtrix
 from scilpy.io.utils import (add_overwrite_arg,
                              add_processes_arg,
                              add_verbose_arg,
                              assert_inputs_exist,
-                             assert_output_dirs_exist_and_empty)
-from scilpy.utils.bvec_bval_tools import fsl2mrtrix, identify_shells
+                             assert_output_dirs_exist_and_empty,
+                             redirect_stdout_c, add_tolerance_arg,
+                             add_skip_b0_check_arg)
+from scilpy.gradients.bvec_bval_tools import (check_b0_threshold,
+                                              identify_shells)
+
 
 EPILOG = """
 Reference:
     [1] Zhang H, Schneider T, Wheeler-Kingshott CA, Alexander DC.
         NODDI: practical in vivo neurite orientation dispersion
         and density imaging of the human brain.
         NeuroImage. 2012 Jul 16;61:1000-16.
 """
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__, epilog=EPILOG,
-        formatter_class=argparse.RawDescriptionHelpFormatter)
+        formatter_class=argparse.RawTextHelpFormatter)
 
     p.add_argument('in_dwi',
                    help='DWI file acquired with a NODDI compatible protocol '
                         '(single-shell data not suited).')
     p.add_argument('in_bval',
                    help='b-values filename, in FSL format (.bval).')
     p.add_argument('in_bvec',
                    help='b-vectors filename, in FSL format (.bvec).')
 
     p.add_argument('--mask',
                    help='Brain mask filename.')
     p.add_argument('--out_dir', default="results",
                    help='Output directory for the NODDI results. '
                         '[%(default)s]')
-    p.add_argument('--b_thr', type=int, default=40,
-                   help='Limit value to consider that a b-value is on an '
-                        'existing shell. Above this limit, the b-value is '
-                        'placed on a new shell. This includes b0s values.')
+    add_tolerance_arg(p)
+    add_skip_b0_check_arg(p, will_overwrite_with_min=False,
+                          b0_tol_name='--tolerance')
 
     g1 = p.add_argument_group(title='Model options')
     g1.add_argument('--para_diff', type=float, default=1.7e-3,
                     help='Axial diffusivity (AD) in the CC. [%(default)s]')
     g1.add_argument('--iso_diff', type=float, default=3e-3,
                     help='Mean diffusivity (MD) in ventricles. [%(default)s]')
     g1.add_argument('--lambda1', type=float, default=5e-1,
@@ -74,92 +80,93 @@
     kern.add_argument('--load_kernels', metavar='DIRECTORY',
                       help='Input directory where the COMMIT kernels are '
                            'located.')
     g2.add_argument('--compute_only', action='store_true',
                     help='Compute kernels only, --save_kernels must be used.')
 
     add_processes_arg(p)
-    add_overwrite_arg(p)
     add_verbose_arg(p)
-    return p
-
+    add_overwrite_arg(p)
 
-def redirect_stdout_c():
-    sys.stdout.flush()
-    newstdout = os.dup(1)
-    devnull = os.open(os.devnull, os.O_WRONLY)
-    os.dup2(devnull, 1)
-    os.close(devnull)
-    sys.stdout = os.fdopen(newstdout, 'w')
+    return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    # COMMIT has some c-level stdout and non-logging print that cannot
+    # be easily stopped. Manual redirection of all printed output
+    if args.verbose == "WARNING":
+        f = io.StringIO()
+        redirected_stdout = redirect_stdout(f)
+        redirect_stdout_c() 
+    else:
+        logging.getLogger().setLevel(logging.getLevelName(args.verbose))
+        redirected_stdout = redirect_stdout(sys.stdout)
 
+    # Verifications
     if args.compute_only and not args.save_kernels:
         parser.error('--compute_only must be used with --save_kernels.')
 
     assert_inputs_exist(parser, [args.in_dwi, args.in_bval, args.in_bvec],
                         args.mask)
 
-    assert_output_dirs_exist_and_empty(parser, args,
-                                       args.out_dir,
+    assert_output_dirs_exist_and_empty(parser, args, args.out_dir,
                                        optional=args.save_kernels)
 
-    # COMMIT has some c-level stdout and non-logging print that cannot
-    # be easily stopped. Manual redirection of all printed output
-    if args.verbose:
-        logging.getLogger().setLevel(logging.DEBUG)
-        redirected_stdout = redirect_stdout(sys.stdout)
-    else:
-        f = io.StringIO()
-        redirected_stdout = redirect_stdout(f)
-        redirect_stdout_c()
+    # Generate a scheme file from the bvals and bvecs files
+    bvals, _ = read_bvals_bvecs(args.in_bval, args.in_bvec)
+    _ = check_b0_threshold(bvals.min(), b0_thr=args.tolerance,
+                           skip_b0_check=args.skip_b0_check)
+    shells_centroids, indices_shells = identify_shells(bvals, args.tolerance,
+                                                       round_centroids=True)
+
+    nb_shells = len(shells_centroids)
+    if nb_shells <= 1:
+        raise ValueError("Amico's NODDI works with data with more than one "
+                         "shell, but you seem to have single-shell data (we "
+                         "found shells {}). Change tolerance if necessary."
+                         .format(np.sort(shells_centroids)))
+
+    logging.info('Will compute NODDI with AMICO on {} shells at found at {}.'
+                 .format(len(shells_centroids), np.sort(shells_centroids)))
 
-    # Generage a scheme file from the bvals and bvecs files
+    # Save the resulting bvals to a temporary file
     tmp_dir = tempfile.TemporaryDirectory()
-    tmp_scheme_filename = os.path.join(tmp_dir.name, 'gradients.scheme')
+    tmp_scheme_filename = os.path.join(tmp_dir.name, 'gradients.b')
     tmp_bval_filename = os.path.join(tmp_dir.name, 'bval')
-    bvals, _ = read_bvals_bvecs(args.in_bval, args.in_bvec)
-    shells_centroids, indices_shells = identify_shells(bvals,
-                                                       args.b_thr,
-                                                       roundCentroids=True)
     np.savetxt(tmp_bval_filename, shells_centroids[indices_shells],
                newline=' ', fmt='%i')
     fsl2mrtrix(tmp_bval_filename, args.in_bvec, tmp_scheme_filename)
-    logging.debug('Compute NODDI with AMICO on {} shells at found '
-                  'at {}.'.format(len(shells_centroids), shells_centroids))
 
     with redirected_stdout:
         # Load the data
         amico.core.setup()
         ae = amico.Evaluation('.', '.')
-        ae.load_data(args.in_dwi,
-                     tmp_scheme_filename,
-                     mask_filename=args.mask)
+        ae.load_data(args.in_dwi, tmp_scheme_filename, mask_filename=args.mask)
+
         # Compute the response functions
         ae.set_model("NODDI")
 
         intra_vol_frac = np.linspace(0.1, 0.99, 12)
         intra_orient_distr = np.hstack((np.array([0.03, 0.06]),
                                         np.linspace(0.09, 0.99, 10)))
 
-        ae.model.set(args.para_diff, args.iso_diff,
-                     intra_vol_frac, intra_orient_distr,
-                     False)
+        ae.model.set(dPar=args.para_diff, dIso=args.iso_diff,
+                     IC_VFs=intra_vol_frac, IC_ODs=intra_orient_distr,
+                     isExvivo=False)
         ae.set_solver(lambda1=args.lambda1, lambda2=args.lambda2)
 
         # The kernels are, by default, set to be in the current directory
         # Depending on the choice, manually change the saving location
         if args.save_kernels:
-            kernels_dir = os.path.join(args.save_kernels)
+            kernels_dir = args.save_kernels
             regenerate_kernels = True
         elif args.load_kernels:
-            kernels_dir = os.path.join(args.load_kernels)
+            kernels_dir = args.load_kernels
             regenerate_kernels = False
         else:
             kernels_dir = os.path.join(tmp_dir.name, 'kernels', ae.model.id)
             regenerate_kernels = True
 
         ae.set_config('ATOMS_path', kernels_dir)
         ae.set_config('OUTPUT_path', args.out_dir)
@@ -167,14 +174,15 @@
         if args.compute_only:
             return
 
         ae.load_kernels()
 
         # Model fit
         ae.fit()
+
         # Save the results
         ae.save_results()
 
     tmp_dir.cleanup()
 
 
 if __name__ == "__main__":
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_NODDI_priors.py` & `scilpy-2.0.0/scripts/scil_NODDI_priors.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,185 +1,207 @@
 #! /usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-Compute the axial (para_diff) and mean (iso_diff) diffusivity priors for NODDI.
+Compute the axial (para_diff), radial (perp_diff), and mean (iso_diff)
+diffusivity priors for NODDI.
+
+Formerly: scil_compute_NODDI_priors.py
 """
 
 import argparse
 import logging
 
 import nibabel as nib
 import numpy as np
 
 from scilpy.io.image import assert_same_resolution
 from scilpy.io.utils import (assert_inputs_exist,
                              assert_outputs_exist,
                              add_overwrite_arg,
                              add_verbose_arg)
 
-
 EPILOG = """
 Reference:
     [1] Zhang H, Schneider T, Wheeler-Kingshott CA, Alexander DC.
-        NODDI: practical in vivo neurite orientation dispersion
-        and density imaging of the human brain.
-        NeuroImage. 2012 Jul 16;61:1000-16.
+        NODDI: practical in vivo neurite orientation dispersion and density
+        imaging of the human brain. NeuroImage. 2012 Jul 16;61:1000-16.
 """
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__, epilog=EPILOG,
-        formatter_class=argparse.RawDescriptionHelpFormatter)
+        formatter_class=argparse.RawTextHelpFormatter)
     p.add_argument('in_FA',
                    help='Path to the FA volume.')
     p.add_argument('in_AD',
                    help='Path to the axial diffusivity (AD) volume.')
+    p.add_argument('in_RD',
+                   help='Path to the radial diffusivity (RD) volume.')
     p.add_argument('in_MD',
                    help='Path to the mean diffusivity (MD) volume.')
 
     g1 = p.add_argument_group('Metrics options')
     g1.add_argument(
-        '--fa_min', type=float, default='0.7',
+        '--fa_min_single_fiber', type=float, default=0.7,
         help='Minimal threshold of FA (voxels above that threshold are '
-             'considered in the single fiber mask). [%(default)s]')
+             'considered in \nthe single fiber mask). [%(default)s]')
     g1.add_argument(
-        '--fa_max', type=float, default='0.1',
+        '--fa_max_ventricles', type=float, default=0.1,
         help='Maximal threshold of FA (voxels under that threshold are '
-             'considered in the ventricles). [%(default)s]')
+             'considered in \nthe ventricles). [%(default)s]')
     g1.add_argument(
-        '--md_min', dest='md_min',  type=float, default='0.003',
+        '--md_min_ventricles', type=float, default=0.003,
         help='Minimal threshold of MD in mm2/s (voxels above that threshold '
-             'are considered for in the ventricles). [%(default)s]')
+             'are considered \nfor in the ventricles). [%(default)s]')
 
     g2 = p.add_argument_group('Regions options')
     g2.add_argument(
         '--roi_radius', type=int, default=20,
         help='Radius of the region used to estimate the priors. The roi will '
-             'be a cube spanning from ROI_CENTER in each direction. '
+             'be a cube spanning \nfrom ROI_CENTER in each direction. '
              '[%(default)s]')
     g2.add_argument(
-        '--roi_center', metavar='tuple(3)', nargs="+", type=int,
+        '--roi_center', metavar='pos', nargs=3, type=int,
         help='Center of the roi of size roi_radius used to estimate the '
-             'priors. [center of the 3D volume]')
+             'priors; a 3-value coordinate. \nIf not set, uses the center of '
+             'the 3D volume.')
 
     g3 = p.add_argument_group('Outputs')
-    g3.add_argument('--out_txt_1fiber', metavar='FILE',
+    g3.add_argument('--out_txt_1fiber_para', metavar='FILE',
                     help='Output path for the text file containing the single '
-                         'fiber average value of AD.\nIf not set, the file will not '
-                         'be saved.')
+                         'fiber average value of AD.\nIf not set, the file '
+                         'will not be saved.')
+    g3.add_argument('--out_txt_1fiber_perp', metavar='FILE',
+                    help='Output path for the text file containing the single '
+                         'fiber average value of RD.\nIf not set, the file '
+                         'will not be saved.')
     g3.add_argument('--out_mask_1fiber', metavar='FILE',
                     help='Output path for single fiber mask. If not set, the '
                          'mask will not be saved.')
     g3.add_argument('--out_txt_ventricles', metavar='FILE',
                     help='Output path for the text file containing the '
-                         'ventricles average value of MD.\nIf not set, the file '
-                         'will not be saved.')
+                         'ventricles average value of MD.\nIf not set, the '
+                         'file will not be saved.')
     g3.add_argument('--out_mask_ventricles', metavar='FILE',
                     help='Output path for the ventricule mask.\nIf not set, '
                          'the mask will not be saved.')
 
-    add_overwrite_arg(p)
     add_verbose_arg(p)
+    add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, [args.in_AD, args.in_FA, args.in_MD])
+    # Verifications
+    assert_inputs_exist(parser, [args.in_AD, args.in_FA, args.in_MD,
+                                 args.in_RD])
     assert_outputs_exist(parser, args, [],
                          [args.out_mask_1fiber,
                           args.out_mask_ventricles,
                           args.out_txt_ventricles,
-                          args.out_txt_1fiber])
-
-    assert_same_resolution([args.in_AD, args.in_FA, args.in_MD])
+                          args.out_txt_1fiber_para,
+                          args.out_txt_1fiber_perp])
 
-    log_level = logging.DEBUG if args.verbose else logging.INFO
-    logging.getLogger().setLevel(log_level)
+    assert_same_resolution([args.in_AD, args.in_FA, args.in_MD, args.in_RD])
 
+    # Loading
     fa_img = nib.load(args.in_FA)
     fa_data = fa_img.get_fdata(dtype=np.float32)
     affine = fa_img.affine
 
     md_data = nib.load(args.in_MD).get_fdata(dtype=np.float32)
     ad_data = nib.load(args.in_AD).get_fdata(dtype=np.float32)
+    rd_data = nib.load(args.in_RD).get_fdata(dtype=np.float32)
 
-    mask_cc = np.zeros(fa_data.shape, dtype=np.uint8)
-    mask_vent = np.zeros(fa_data.shape, dtype=np.uint8)
-
-    # center
+    # Finding ROI center
     if args.roi_center is None:
+        # Using the center of the image: single-fiber region should be the CC
         ci, cj, ck = np.array(fa_data.shape[:3]) // 2
     else:
         if len(args.roi_center) != 3:
             parser.error("roi_center needs to receive 3 values")
         elif not np.all(np.asarray(args.roi_center) > 0):
             parser.error("roi_center needs to be positive")
         else:
             ci, cj, ck = args.roi_center
 
+    # Get values in the ROI
     w = args.roi_radius
-    fa_shape = fa_data.shape
-    roi_ad = ad_data[max(int(ci - w), 0): min(int(ci + w), fa_shape[0]),
-                     max(int(cj - w), 0): min(int(cj + w), fa_shape[1]),
-                     max(int(ck - w), 0): min(int(ck + w), fa_shape[2])]
-    roi_md = md_data[max(int(ci - w), 0): min(int(ci + w), fa_shape[0]),
-                     max(int(cj - w), 0): min(int(cj + w), fa_shape[1]),
-                     max(int(ck - w), 0): min(int(ck + w), fa_shape[2])]
-    roi_fa = fa_data[max(int(ci - w), 0): min(int(ci + w), fa_shape[0]),
-                     max(int(cj - w), 0): min(int(cj + w), fa_shape[1]),
-                     max(int(ck - w), 0): min(int(ck + w), fa_shape[2])]
-
-    logging.debug('fa_min, fa_max, md_min: {}, {}, {}'.format(
-        args.fa_min, args.fa_max, args.md_min))
-
-    indices = np.where((roi_fa > args.fa_min) & (roi_fa < 0.95))
-    N = roi_ad[indices].shape[0]
-
-    logging.debug('Number of voxels found in single fiber area: {}'.format(N))
-
-    cc_avg = np.mean(roi_ad[indices])
-    cc_std = np.std(roi_ad[indices])
+    roi_posx = slice(max(int(ci - w), 0), min(int(ci + w), fa_data.shape[0]))
+    roi_posy = slice(max(int(cj - w), 0), min(int(cj + w), fa_data.shape[1]))
+    roi_posz = slice(max(int(ck - w), 0), min(int(ck + w), fa_data.shape[2]))
+    roi_ad = ad_data[roi_posx, roi_posy, roi_posz]
+    roi_rd = rd_data[roi_posx, roi_posy, roi_posz]
+    roi_md = md_data[roi_posx, roi_posy, roi_posz]
+    roi_fa = fa_data[roi_posx, roi_posy, roi_posz]
+
+    # Get information in single fiber voxels
+    # Taking voxels with FA < 0.95 just to avoid using weird broken voxels.
+    indices = np.where((roi_fa > args.fa_min_single_fiber) & (roi_fa < 0.95))
+    nb_voxels = roi_fa[indices].shape[0]
+    logging.info('Number of voxels found in single fiber area (FA in range '
+                 '{}-{}]: {}'
+                 .format(args.fa_min_single_fiber, 0.95, nb_voxels))
+    single_fiber_ad_mean = np.mean(roi_ad[indices])
+    single_fiber_ad_std = np.std(roi_ad[indices])
+    single_fiber_rd_mean = np.mean(roi_rd[indices])
+    single_fiber_rd_std = np.std(roi_rd[indices])
 
+    # Create mask of single fiber in ROI
     indices[0][:] += ci - w
     indices[1][:] += cj - w
     indices[2][:] += ck - w
-    mask_cc[indices] = 1
-
-    indices = np.where((roi_md > args.md_min) & (roi_fa < args.fa_max))
-    N = roi_md[indices].shape[0]
+    mask_single_fiber = np.zeros(fa_data.shape, dtype=np.uint8)
+    mask_single_fiber[indices] = 1
 
-    logging.debug('Number of voxels found in ventricles: {}'.format(N))
+    # Get information in ventricles
+    indices = np.where((roi_md > args.md_min_ventricles) &
+                       (roi_fa < args.fa_max_ventricles))
+    nb_voxels = roi_md[indices].shape[0]
+    logging.info('Number of voxels found in ventricles (FA < {} and MD > {}): '
+                 '{}'.format(args.fa_max_ventricles, args.md_min_ventricles,
+                             nb_voxels))
 
     vent_avg = np.mean(roi_md[indices])
     vent_std = np.std(roi_md[indices])
 
+    # Create mask of ventricle in ROI
     indices[0][:] += ci - w
     indices[1][:] += cj - w
     indices[2][:] += ck - w
+    mask_vent = np.zeros(fa_data.shape, dtype=np.uint8)
     mask_vent[indices] = 1
 
+    # Saving
     if args.out_mask_1fiber:
-        nib.save(nib.Nifti1Image(mask_cc, affine), args.out_mask_1fiber)
+        nib.save(nib.Nifti1Image(mask_single_fiber, affine),
+                 args.out_mask_1fiber)
 
     if args.out_mask_ventricles:
         nib.save(nib.Nifti1Image(mask_vent, affine), args.out_mask_ventricles)
 
-    if args.out_txt_1fiber:
-        np.savetxt(args.out_txt_1fiber, [cc_avg], fmt='%f')
+    if args.out_txt_1fiber_para:
+        np.savetxt(args.out_txt_1fiber_para, [single_fiber_ad_mean], fmt='%f')
+
+    if args.out_txt_1fiber_perp:
+        np.savetxt(args.out_txt_1fiber_perp, [single_fiber_rd_mean], fmt='%f')
 
     if args.out_txt_ventricles:
         np.savetxt(args.out_txt_ventricles, [vent_avg], fmt='%f')
 
-    logging.info("Average AD in single fiber areas: {} +- {}".format(cc_avg,
-                                                                     cc_std))
-    logging.info("Average MD in ventricles: {} +- {}".format(vent_avg,
-                                                             vent_std))
+    logging.info("Average AD in single fiber areas: {} +- {}"
+                 .format(single_fiber_ad_mean, single_fiber_ad_std))
+    logging.info("Average RD in single fiber areas: {} +- {}"
+                 .format(single_fiber_rd_mean, single_fiber_rd_std))
+    logging.info("Average MD in ventricles: {} +- {}"
+                 .format(vent_avg, vent_std))
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_asym_odf_metrics.py` & `scilpy-2.0.0/scripts/scil_sh_to_aodf.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,222 +1,187 @@
 #!/usr/bin/env python3
+# -*- coding: utf-8 -*-
 """
-Script to compute various metrics derivated from asymmetric ODF.
+Script to estimate asymmetric ODFs (aODFs) from a spherical harmonics image.
 
-These metrics include an asymmetric peak directions image, a number of peaks
-(nupeaks) map [2], a cosine-similarity-based asymmetry map [1] and an
-odd-power map [2].
-
-The asymmetric peak directions image contains peaks per hemisphere, considering
-antipodal sphere directions as distinct. On a symmetric signal, the number of
-asymmetric peaks extracted is then twice the number of symmetric peaks.
-
-The nupeaks map is the asymmetric alternative to NuFO maps. It counts the
-number of asymmetric peaks extracted and ranges in [0..N] with N the maximum
-number of peaks.
-
-The cosine-based asymmetry map is in the range [0..1], with 0 corresponding
-to a perfectly symmetric signal and 1 to a perfectly asymmetric signal.
-
-The odd-power map is also in the range [0..1], with 0 corresponding
-to a perfectly symmetric signal and 1 to a perfectly anti-symmetric signal. It
-is given by the ratio of the L2-norm of odd SH coefficients on the L2-norm of
-all SH coefficients.
+Two methods are available:
+    * Unified filtering [1] combines four asymmetric filtering methods into
+      a single equation and relies on a combination of four gaussian filters.
+    * Cosine filtering [2] is a simpler implementation using cosine distance
+      for assigning weights to neighbours.
+
+Unified filtering can be accelerated using OpenCL with the option --use_opencl.
+Make sure you have pyopencl installed before using this option. By default, the
+OpenCL program will run on the cpu. To use a gpu instead, also specify the
+option --device gpu.
 """
 
-
 import argparse
+import logging
+import time
 import nibabel as nib
 import numpy as np
 
-from dipy.data import get_sphere, SPHERE_FILES
-from dipy.direction.peaks import reshape_peaks_for_visualization
+from dipy.data import SPHERE_FILES
 from dipy.reconst.shm import sph_harm_ind_list
-
-from scilpy.reconst.multi_processes import peaks_from_sh
 from scilpy.reconst.utils import get_sh_order_and_fullness
-from scilpy.io.utils import (add_processes_arg,
-                             add_sh_basis_args,
-                             assert_inputs_exist,
-                             assert_outputs_exist,
-                             add_overwrite_arg)
-from scilpy.io.image import get_data_as_mask
+from scilpy.io.utils import (add_overwrite_arg, add_verbose_arg,
+                             assert_inputs_exist, add_sh_basis_args,
+                             assert_outputs_exist, parse_sh_basis_arg)
+from scilpy.denoise.asym_filtering import (cosine_filtering, unified_filtering)
 
 
 EPILOG = """
-References:
-[1] S. Cetin Karayumak, E. Özarslan, and G. Unal,
-“Asymmetric Orientation Distribution Functions (AODFs) revealing
-intravoxel geometry in diffusion MRI,” Magnetic Resonance Imaging,
-vol. 49, pp. 145–158, Jun. 2018, doi: 10.1016/j.mri.2018.03.006.
-
-[2] C. Poirier, E. St-Onge, and M. Descoteaux, "Investigating the Occurence of
-Asymmetric Patterns in White Matter Fiber Orientation Distribution Functions"
-[Abstract], In: Proc. Intl. Soc. Mag. Reson. Med. 29 (2021), 2021 May 15-20,
-Vancouver, BC, Abstract number 0865.
+[1] Poirier and Descoteaux, 2024, "A Unified Filtering Method for Estimating
+    Asymmetric Orientation Distribution Functions", Neuroimage, vol. 287,
+    https://doi.org/10.1016/j.neuroimage.2024.120516
+
+[2] Poirier et al, 2021, "Investigating the Occurrence of Asymmetric Patterns
+    in White Matter Fiber Orientation Distribution Functions", ISMRM 2021
+    (abstract 0865)
 """
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(description=__doc__, epilog=EPILOG,
                                 formatter_class=argparse.RawTextHelpFormatter)
-    p.add_argument('in_sh', help='Input SH image.')
+    p.add_argument('in_sh',
+                   help='Path to the input file.')
 
-    p.add_argument('--mask', default='',
-                   help='Optional mask.')
+    p.add_argument('out_sh',
+                   help='File name for averaged signal.')
 
-    # outputs
-    p.add_argument('--cos_asym_map', default='',
-                   help='Output asymmetry map using cos similarity.')
-    p.add_argument('--odd_power_map', default='',
-                   help='Output odd power map.')
-    p.add_argument('--peaks', default='',
-                   help='Output filename for the extracted peaks.')
-    p.add_argument('--peak_values', default='',
-                   help='Output filename for the extracted peaks values.')
-    p.add_argument('--peak_indices', default='',
-                   help='Output filename for the generated peaks indices on '
-                        'the sphere.')
-    p.add_argument('--nupeaks', default='',
-                   help='Output filename for the nupeaks file.')
-    p.add_argument('--not_all', action='store_true',
-                   help='If set, only saves the files specified using the '
-                        'file flags [%(default)s].')
-
-    p.add_argument('--at', dest='a_threshold', type=float, default='0.0',
-                   help='Absolute threshold on fODF amplitude. This '
-                        'value should be set to\napproximately 1.5 to 2 times '
-                        'the maximum fODF amplitude in isotropic voxels\n'
-                        '(ie. ventricles).\n'
-                        'Use compute_fodf_max_in_ventricles.py to find the '
-                        'maximal value.\n'
-                        'See [Dell\'Acqua et al HBM 2013] [%(default)s].')
-    p.add_argument('--rt', dest='r_threshold', type=float, default='0.1',
-                   help='Relative threshold on fODF amplitude in percentage '
-                        '[%(default)s].')
-    p.add_argument('--sphere', default='symmetric724',
-                   choices=sorted(SPHERE_FILES.keys()),
-                   help='Sphere to use for peak directions estimation '
-                        '[%(default)s].')
+    p.add_argument('--out_sym', default=None,
+                   help='Name of optional symmetric output. [%(default)s]')
 
-    add_processes_arg(p)
     add_sh_basis_args(p)
-    add_overwrite_arg(p)
-    return p
-
-
-def compute_cos_asym_map(sh_coeffs, order, mask):
-    _, l_list = sph_harm_ind_list(order, full_basis=True)
-
-    sign = np.power(-1.0, l_list)
-    sign = np.reshape(sign, (1, 1, 1, len(l_list)))
-    sh_squared = sh_coeffs**2
-    mask = np.logical_and(sh_squared.sum(axis=-1) > 0., mask)
-
-    cos_asym_map = np.zeros(sh_coeffs.shape[:-1])
-    cos_asym_map[mask] = np.sum(sh_squared * sign, axis=-1)[mask] / \
-        np.sum(sh_squared, axis=-1)[mask]
 
-    cos_asym_map = np.sqrt(1 - cos_asym_map**2) * mask
-
-    return cos_asym_map
-
-
-def compute_odd_power_map(sh_coeffs, order, mask):
-    _, l_list = sph_harm_ind_list(order, full_basis=True)
-    odd_l_list = (l_list % 2 == 1).reshape((1, 1, 1, -1))
-
-    odd_order_norm = np.linalg.norm(sh_coeffs * odd_l_list,
-                                    ord=2, axis=-1)
-
-    full_order_norm = np.linalg.norm(sh_coeffs, ord=2, axis=-1)
+    p.add_argument('--sphere', default='repulsion200',
+                   choices=sorted(SPHERE_FILES.keys()),
+                   help='Sphere used for the SH to SF projection. '
+                        '[%(default)s]')
 
-    asym_map = np.zeros(sh_coeffs.shape[:-1])
-    mask = np.logical_and(full_order_norm > 0, mask)
-    asym_map[mask] = odd_order_norm[mask] / full_order_norm[mask]
+    p.add_argument('--method', default='unified',
+                   choices=['unified', 'cosine'],
+                   help="Method for estimating asymmetric ODFs "
+                        "[%(default)s].\nOne of:\n"
+                        "    'unified': Unified filtering [1].\n"
+                        "    'cosine' : Cosine-based filtering [2].")
+
+    shared_group = p.add_argument_group('Shared filter arguments')
+    shared_group.add_argument('--sigma_spatial', default=1.0, type=float,
+                              help='Standard deviation for spatial distance.'
+                                   ' [%(default)s]')
+
+    unified_group = p.add_argument_group('Unified filter arguments')
+    unified_group.add_argument('--sigma_align', default=0.8, type=float,
+                               help='Standard deviation for alignment '
+                                    'filter. [%(default)s]')
+    unified_group.add_argument('--sigma_range', default=0.2, type=float,
+                               help='Standard deviation for range filter\n'
+                                    '*relative to SF range of image*. '
+                                    '[%(default)s]')
+    unified_group.add_argument('--sigma_angle', type=float,
+                               help='Standard deviation for angular filter\n'
+                                    '(disabled by default).')
+    unified_group.add_argument('--disable_spatial', action='store_true',
+                               help='Disable spatial filtering.')
+    unified_group.add_argument('--disable_align', action='store_true',
+                               help='Disable alignment filtering.')
+    unified_group.add_argument('--disable_range', action='store_true',
+                               help='Disable range filtering.')
+    unified_group.add_argument('--include_center', action='store_true',
+                               help='Include center voxel in neighourhood.')
+    unified_group.add_argument('--win_hwidth', type=int,
+                               help='Filtering window half-width. Defaults to '
+                                    '3*sigma_spatial.')
+
+    cosine_group = p.add_argument_group('Cosine filter arguments')
+    cosine_group.add_argument('--sharpness', default=1.0, type=float,
+                              help='Specify sharpness factor to use for\n'
+                                   'weighted average. [%(default)s]')
+
+    p.add_argument('--device', choices=['cpu', 'gpu'], default='cpu',
+                   help='Device to use for execution. [%(default)s]')
+    p.add_argument('--use_opencl', action='store_true',
+                   help='Accelerate code using OpenCL (requires pyopencl\n'
+                        'and a working OpenCL implementation).')
+    p.add_argument('--patch_size', type=int, default=40,
+                   help='OpenCL patch size. [%(default)s]')
 
-    return asym_map
+    add_verbose_arg(p)
+    add_overwrite_arg(p)
+    return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    if not args.not_all:
-        args.cos_asym_map = args.cos_asym_map or 'cos_asym_map.nii.gz'
-        args.odd_power_map = args.odd_power_map or 'odd_power_map.nii.gz'
-        args.peaks = args.peaks or 'asym_peaks.nii.gz'
-        args.peak_values = args.peak_values or 'asym_peak_values.nii.gz'
-        args.peak_indices = args.peak_indices or 'asym_peak_indices.nii.gz'
-        args.nupeaks = args.nupeaks or 'nupeaks.nii.gz'
-
-    arglist = [args.cos_asym_map, args.odd_power_map, args.peaks,
-               args.peak_values, args.peak_indices, args.nupeaks]
-    if args.not_all and not any(arglist):
-        parser.error('When using --not_all, you need to specify at least '
-                     'one file to output.')
-
-    inputs = [args.in_sh]
-    if args.mask:
-        inputs.append(args.mask)
-
-    assert_inputs_exist(parser, inputs)
-    assert_outputs_exist(parser, args, arglist)
+    if args.device == 'gpu' and not args.use_opencl:
+        logging.warning("Device 'gpu' chosen but --use_opencl not specified. "
+                        "Proceeding with use_opencl=True.")
+        # force use_opencl if option gpu is chosen
+        args.use_opencl = True
+
+    if args.use_opencl and args.method == 'cosine':
+        parser.error('Option --use_opencl is not supported'
+                     ' for cosine filtering.')
+
+    outputs = [args.out_sh]
+    if args.out_sym:
+        outputs.append(args.out_sym)
+    assert_outputs_exist(parser, args, outputs)
+    assert_inputs_exist(parser, args.in_sh)
 
+    # Prepare data
     sh_img = nib.load(args.in_sh)
-    sh = sh_img.get_fdata()
+    data = sh_img.get_fdata(dtype=np.float32)
 
-    sphere = get_sphere(args.sphere)
+    sh_order, full_basis = get_sh_order_and_fullness(data.shape[-1])
+    sh_basis, is_legacy = parse_sh_basis_arg(args)
 
-    sh_order, full_basis = get_sh_order_and_fullness(sh.shape[-1])
-    if not full_basis:
-        parser.error('Invalid SH image. A full SH basis is expected.')
-
-    if args.mask:
-        mask = get_data_as_mask(nib.load(args.mask), dtype=bool)
-    else:
-        mask = np.sum(np.abs(sh), axis=-1) > 0
-
-    if args.cos_asym_map:
-        cos_asym_map = compute_cos_asym_map(sh, sh_order, mask)
-        nib.save(nib.Nifti1Image(cos_asym_map, sh_img.affine),
-                 args.cos_asym_map)
-
-    if args.odd_power_map:
-        odd_power_map = compute_odd_power_map(sh, sh_order, mask)
-        nib.save(nib.Nifti1Image(odd_power_map, sh_img.affine),
-                 args.odd_power_map)
-
-    if args.peaks or args.peak_values or args.peak_indices or args.nupeaks:
-        peaks, values, indices =\
-            peaks_from_sh(sh, sphere, mask=mask,
-                          relative_peak_threshold=args.r_threshold,
-                          absolute_threshold=args.a_threshold,
-                          min_separation_angle=25,
-                          normalize_peaks=False,
-                          # because v and -v are unique, we want twice
-                          # the usual default value (5) of npeaks
-                          npeaks=10,
-                          sh_basis_type=args.sh_basis,
-                          nbr_processes=args.nbr_processes,
-                          full_basis=True,
-                          is_symmetric=False)
-
-        if args.peaks:
-            nib.save(nib.Nifti1Image(reshape_peaks_for_visualization(peaks),
-                                     sh_img.affine), args.peaks)
-
-        if args.peak_values:
-            nib.save(nib.Nifti1Image(values, sh_img.affine),
-                     args.peak_values)
-
-        if args.peak_indices:
-            nib.save(nib.Nifti1Image(indices.astype(np.uint8), sh_img.affine),
-                     args.peak_indices)
-
-        if args.nupeaks:
-            nupeaks = np.count_nonzero(values, axis=-1).astype(np.uint8)
-            nib.save(nib.Nifti1Image(nupeaks, sh_img.affine), args.nupeaks)
+    t0 = time.perf_counter()
+    logging.info('Filtering SH image.')
+    if args.method == 'unified':
+        sigma_align = None if args.disable_align else args.sigma_align
+        sigma_range = None if args.disable_range else args.sigma_range
+        sigma_spatial = None if args.disable_spatial else args.sigma_spatial
+
+        asym_sh = unified_filtering(
+            data, sh_order=sh_order, sh_basis=sh_basis,
+            is_legacy=is_legacy, full_basis=full_basis,
+            sphere_str=args.sphere,
+            sigma_spatial=sigma_spatial,
+            sigma_align=sigma_align,
+            sigma_angle=args.sigma_angle,
+            rel_sigma_range=sigma_range,
+            win_hwidth=args.win_hwidth,
+            exclude_center=not args.include_center,
+            device_type=args.device,
+            use_opencl=args.use_opencl)
+    else:  # args.method == 'cosine'
+        asym_sh = cosine_filtering(
+            data, sh_order=sh_order,
+            sh_basis=sh_basis,
+            in_full_basis=full_basis,
+            is_legacy=is_legacy,
+            sphere_str=args.sphere,
+            dot_sharpness=args.sharpness,
+            sigma=args.sigma_spatial)
+
+    t1 = time.perf_counter()
+    logging.info('Elapsed time (s): {0}'.format(t1 - t0))
+
+    logging.info('Saving filtered SH to file {0}.'.format(args.out_sh))
+    nib.save(nib.Nifti1Image(asym_sh, sh_img.affine), args.out_sh)
+
+    if args.out_sym:
+        _, orders = sph_harm_ind_list(sh_order, full_basis=True)
+        logging.info('Saving symmetric SH to file {0}.'.format(args.out_sym))
+        nib.save(nib.Nifti1Image(asym_sh[..., orders % 2 == 0], sh_img.affine),
+                 args.out_sym)
 
 
 if __name__ == '__main__':
     main()
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_bundle_mean_std.py` & `scilpy-2.0.0/scripts/scil_connectivity_hdf5_average_density_map.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,116 +1,124 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-Compute mean and std for the whole bundle for each metric. This is achieved by
-averaging the metrics value of all voxels occupied by the bundle.
+Compute a density map for each connection from a hdf5 file.
+Typically use after scil_tractogram_segment_bundles_for_connectivity.py in
+order to obtain the average density map of each connection to allow the use
+of --similarity in scil_connectivity_compute_matrices.py.
+
+This script is parallelized, but will run much slower on non-SSD if too many
+processes are used. The output is a directory containing the thousands of
+connections:
+out_dir/
+    |-- LABEL1_LABEL1.nii.gz
+    |-- LABEL1_LABEL2.nii.gz
+    |-- [...]
+    |-- LABEL90_LABEL90.nii.gz
 
-Density weighting modifies the contribution of voxel with lower/higher
-streamline count to reduce influence of spurious streamlines.
+Formerly: scil_compute_hdf5_average_density_map.py
 """
 
 import argparse
-import json
+import itertools
 import logging
+import multiprocessing
 import os
 
-import nibabel as nib
+import h5py
 import numpy as np
+import nibabel as nib
 
-from scilpy.utils.filenames import split_name_with_nii
-from scilpy.io.image import assert_same_resolution
-from scilpy.io.streamlines import load_tractogram_with_reference
-from scilpy.io.utils import (add_json_args,
-                             add_reference_arg,
-                             assert_inputs_exist)
-from scilpy.utils.metrics_tools import get_bundle_metrics_mean_std
+from scilpy.io.hdf5 import assert_header_compatible_hdf5, \
+    reconstruct_streamlines_from_hdf5
+from scilpy.io.utils import (add_overwrite_arg, add_verbose_arg,
+                             add_processes_arg, assert_inputs_exist,
+                             assert_output_dirs_exist_and_empty,
+                             validate_nbr_processes)
+from scilpy.tractanalysis.streamlines_metrics import compute_tract_counts_map
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
-        description=__doc__,
-        formatter_class=argparse.RawTextHelpFormatter)
+        description=__doc__, formatter_class=argparse.RawTextHelpFormatter)
+    p.add_argument('in_hdf5', nargs='+',
+                   help='List of HDF5 filenames (.h5) from '
+                        'scil_tractogram_segment_bundles_for_connectivity.py.')
+    p.add_argument('out_dir',
+                   help='Path of the output directory.')
+
+    p.add_argument('--binary', action='store_true',
+                   help='Binarize density maps before the population average.')
+
+    add_processes_arg(p)
+    add_verbose_arg(p)
+    add_overwrite_arg(p)
+    return p
 
-    p.add_argument('in_bundle',
-                   help='Fiber bundle file to compute statistics on')
-    p.add_argument('in_metrics', nargs='+',
-                   help='Nifti file to compute statistics on. Probably some '
-                        'tractometry measure(s) such as FA, MD, RD, ...')
-
-    p.add_argument('--density_weighting', action='store_true',
-                   help='If set, weight statistics by the number of '
-                        'fibers passing through each voxel.')
-    p.add_argument('--distance_weighting', metavar='DISTANCE_NII',
-                   help='If set, weight statistics by the inverse of the '
-                        'distance between a streamline and the centroid.')
-    p.add_argument('--correlation_weighting', metavar='CORRELATION_NII',
-                   help='If set, weight statistics by the correlation strength '
-                        'between longitudinal data.')
-    p.add_argument('--include_dps', action='store_true',
-                   help='Save values from data_per_streamline.')
-    add_reference_arg(p)
-    add_json_args(p)
 
-    return p
+def _average_wrapper(args):
+    hdf5_filenames = args[0]
+    key = args[1]
+    binary = args[2]
+    out_dir = args[3]
+
+    with h5py.File(hdf5_filenames[0], 'r') as hdf5_file_ref:
+        affine = hdf5_file_ref.attrs['affine']
+        dimensions = hdf5_file_ref.attrs['dimensions']
+        density_data = np.zeros(dimensions, dtype=np.float32)
+
+    for hdf5_filename in hdf5_filenames:
+        with h5py.File(hdf5_filename, 'r') as hdf5_file:
+            assert_header_compatible_hdf5(hdf5_file, (affine, dimensions))
+
+            # scil_tractogram_segment_bundles_for_connectivity.py saves the
+            # streamlines in VOX/CORNER
+            streamlines = reconstruct_streamlines_from_hdf5(hdf5_file[key])
+            if len(streamlines) == 0:
+                continue
+            density = compute_tract_counts_map(streamlines, dimensions)
+
+        if binary:
+            density_data[density > 0] += 1
+        elif np.max(density) > 0:
+            density_data += density / np.max(density)
+
+    if np.max(density_data) > 0:
+        density_data /= len(hdf5_filenames)
+
+        nib.save(nib.Nifti1Image(density_data, affine),
+                 os.path.join(out_dir, '{}.nii.gz'.format(key)))
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, [args.in_bundle] + args.in_metrics,
-                        optional=args.reference)
-
-    assert_same_resolution(args.in_metrics)
-    metrics = [nib.load(metric) for metric in args.in_metrics]
-
-    sft = load_tractogram_with_reference(parser, args, args.in_bundle)
-    sft.to_vox()
-    sft.to_corner()
-
-    if args.distance_weighting:
-        img = nib.load(args.distance_weighting)
-        distances_map = img.get_fdata(dtype=float)
+    assert_inputs_exist(parser, args.in_hdf5)
+    assert_output_dirs_exist_and_empty(parser, args, args.out_dir,
+                                       create_dir=True)
+
+    keys = []
+    for filename in args.in_hdf5:
+        with h5py.File(filename, 'r') as curr_file:
+            keys.extend(curr_file.keys())
+
+    keys = set(keys)
+    nbr_cpu = validate_nbr_processes(parser, args)
+    if nbr_cpu == 1:
+        for key in keys:
+            _average_wrapper([args.in_hdf5, key, args.binary, args.out_dir])
     else:
-        distances_map = None
-
-    if args.correlation_weighting:
-        img = nib.load(args.correlation_weighting)
-        correlation_map = img.get_fdata(dtype=float)
-    else:
-        correlation_map = None
-
-    for index, metric in enumerate(metrics):
-        if np.any(np.isnan(metric.get_fdata())):
-            logging.warning('Metric \"{}\" contains some NaN.'.format(args.in_metrics[index]) +
-                            ' Ignoring voxels with NaN.')
-
-    bundle_stats = get_bundle_metrics_mean_std(sft.streamlines,
-                                               metrics,
-                                               distances_map,
-                                               correlation_map,
-                                               args.density_weighting)
-
-    bundle_name, _ = os.path.splitext(os.path.basename(args.in_bundle))
-
-    stats = {bundle_name: {}}
-    for metric, (mean, std) in zip(metrics, bundle_stats):
-        metric_name = split_name_with_nii(
-            os.path.basename(metric.get_filename()))[0]
-        stats[bundle_name][metric_name] = {
-            'mean': mean,
-            'std': std
-        }
-    if args.include_dps:
-        for metric in sft.data_per_streamline.keys():
-            mean = float(np.average(sft.data_per_streamline[metric]))
-            std = float(np.std(sft.data_per_streamline[metric]))
-            stats[bundle_name][metric] = {
-                'mean': mean,
-                'std': std
-            }
-    print(json.dumps(stats, indent=args.indent, sort_keys=args.sort_keys))
+        pool = multiprocessing.Pool(nbr_cpu)
+        _ = pool.map(_average_wrapper,
+                     zip(itertools.repeat(args.in_hdf5),
+                         keys,
+                         itertools.repeat(args.binary),
+                         itertools.repeat(args.out_dir)))
+        pool.close()
+        pool.join()
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_bundle_volume.py` & `scilpy-2.0.0/scripts/scil_tractogram_count_streamlines.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,65 +1,61 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-Compute bundle volume in mm3. This script supports anisotropic voxels
-resolution. Volume is estimated by counting the number of voxels occupied by
-the bundle and multiplying it by the volume of a single voxel.
+Return the number of streamlines in a tractogram. Only support trk and tck in
+order to support the lazy loading from nibabel.
 
-This estimation is typically performed at resolution around 1mm3.
+Formerly: scil_count_streamlines.py
 """
 
 import argparse
 import json
+import logging
 import os
 
-import numpy as np
-
-from scilpy.io.streamlines import load_tractogram_with_reference
 from scilpy.io.utils import (add_json_args,
-                             add_reference_arg,
+                             add_verbose_arg,
                              assert_inputs_exist)
-from scilpy.tractanalysis.streamlines_metrics import compute_tract_counts_map
+from scilpy.tractograms.lazy_tractogram_operations import \
+    lazy_streamlines_count
 
 
 def _build_arg_parser():
-    p = argparse.ArgumentParser(
-        description=__doc__,
-        formatter_class=argparse.RawTextHelpFormatter)
-
-    p.add_argument('in_bundle',
-                   help='Fiber bundle file.')
+    p = argparse.ArgumentParser(description=__doc__,
+                                formatter_class=argparse.RawTextHelpFormatter)
+    p.add_argument('in_tractogram',
+                   help='Path of the input tractogram file.')
+    p.add_argument('--print_count_alone', action='store_true',
+                   help="If true, prints the result only. \nElse, prints the "
+                        "bundle name and count formatted as a json dict."
+                        "(default)")
 
-    add_reference_arg(p)
     add_json_args(p)
+    add_verbose_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, args.in_bundle, optional=args.reference)
+    assert_inputs_exist(parser, args.in_tractogram)
 
-    sft = load_tractogram_with_reference(parser, args, args.in_bundle)
-    sft.to_vox()
-    sft.to_corner()
-
-    bundle_name, _ = os.path.splitext(os.path.basename(args.in_bundle))
-    stats = {bundle_name: {}}
-    if len(sft.streamlines) == 0:
-        stats[bundle_name]['volume'] = None
-        print(json.dumps(stats, indent=args.indent, sort_keys=args.sort_keys))
-        return
-
-    tdi = compute_tract_counts_map(sft.streamlines,
-                                   tuple(sft.dimensions))
-    voxel_volume = np.prod(np.prod(sft.voxel_sizes))
-    stats[bundle_name]['volume'] = np.count_nonzero(tdi) * voxel_volume
+    bundle_name, _ = os.path.splitext(os.path.basename(args.in_tractogram))
+    count = int(lazy_streamlines_count(args.in_tractogram))
 
-    print(json.dumps(stats, indent=args.indent, sort_keys=args.sort_keys))
+    if args.print_count_alone:
+        print(count)
+    else:
+        stats = {
+            bundle_name: {
+                'streamline_count': count
+            }
+        }
+        print(json.dumps(stats, indent=args.indent))
 
 
 if __name__ == '__main__':
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_bundle_volume_per_label.py` & `scilpy-2.0.0/scripts/scil_bundle_volume_per_label.py`

 * *Files 8% similar despite different names*

```diff
@@ -2,48 +2,58 @@
 # -*- coding: utf-8 -*-
 
 """
 Compute bundle volume per label in mm3. This script supports anisotropic voxels
 resolution. Volume is estimated by counting the number of voxel occupied by
 each label and multiplying it by the volume of a single voxel.
 
+The labels can be obtained by scil_bundle_label_map.py.
+
 This estimation is typically performed at resolution around 1mm3.
+
+To get the volume and other measures directly from the (whole) bundle, use
+scil_bundle_shape_measures.py.
+
+Formerly: scil_compute_bundle_volume_per_label.py
 """
 
 import argparse
 import json
+import logging
 
 import nibabel as nib
 import numpy as np
 
 from scilpy.image.labels import get_data_as_labels
 from scilpy.io.utils import (add_json_args,
+                             add_verbose_arg,
                              add_overwrite_arg,
                              assert_inputs_exist)
 
 
 def _build_arg_parser():
-    p = argparse.ArgumentParser(
-        description=__doc__,
-        formatter_class=argparse.RawTextHelpFormatter)
+    p = argparse.ArgumentParser(description=__doc__,
+                                formatter_class=argparse.RawTextHelpFormatter)
 
     p.add_argument('voxel_label_map',
                    help='Fiber bundle file.')
     p.add_argument('bundle_name',
                    help='Bundle name.')
 
     add_json_args(p)
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, args.voxel_label_map)
 
     voxel_label_map_img = nib.load(args.voxel_label_map)
     voxel_label_map_data = get_data_as_labels(voxel_label_map_img)
     voxel_size = voxel_label_map_img.header['pixdim'][1:4]
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_bundle_voxel_label_map.py` & `scilpy-2.0.0/scripts/scil_bundle_label_map.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,194 +1,145 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-Compute label image (Nifti) from bundle and centroid.
+Compute the label image (Nifti) from a centroid and tractograms (all
+representing the same bundle). The label image represents the coverage of
+the bundle, segmented into regions labelled from 0 to --nb_pts, starting from
+the head, ending in the tail.
+
 Each voxel will have the label of its nearest centroid point.
 
 The number of labels will be the same as the centroid's number of points.
+
+Formerly: scil_compute_bundle_voxel_label_map.py
 """
 
 import argparse
-import itertools
 import logging
 import os
 
+from dipy.align.streamlinear import StreamlineLinearRegistration
 from dipy.io.streamline import save_tractogram
-from dipy.io.stateful_tractogram import StatefulTractogram, set_sft_logger_level
+from dipy.io.stateful_tractogram import StatefulTractogram
 from dipy.io.utils import is_header_compatible
-from dipy.segment.clustering import qbx_and_merge
 import matplotlib.pyplot as plt
 import nibabel as nib
 from nibabel.streamlines.array_sequence import ArraySequence
 import numpy as np
 import scipy.ndimage as ndi
 from scipy.spatial import cKDTree
 
+from scilpy.image.volume_math import correlation
 from scilpy.io.streamlines import load_tractogram_with_reference
 from scilpy.io.utils import (add_overwrite_arg,
                              add_reference_arg,
+                             add_verbose_arg,
                              assert_inputs_exist,
                              assert_output_dirs_exist_and_empty)
-from scilpy.tracking.tools import resample_streamlines_num_points
+from scilpy.tractanalysis.bundle_operations import uniformize_bundle_sft
 from scilpy.tractanalysis.streamlines_metrics import compute_tract_counts_map
-from scilpy.tractanalysis.tools import cut_outside_of_mask_streamlines
 from scilpy.tractanalysis.distance_to_centroid import min_dist_to_centroid
-from scipy.ndimage import gaussian_filter, map_coordinates
-from scilpy.utils.streamlines import uniformize_bundle_sft
-from scilpy.viz.utils import get_colormap
+from scilpy.tractograms.streamline_and_mask_operations import \
+    cut_outside_of_mask_streamlines
+from scilpy.tractograms.streamline_operations import \
+    resample_streamlines_num_points
+from scilpy.viz.color import get_lookup_table
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__,
         formatter_class=argparse.RawTextHelpFormatter)
 
     p.add_argument('in_bundles', nargs='+',
                    help='Fiber bundle file.')
     p.add_argument('in_centroid',
                    help='Centroid streamline corresponding to bundle.')
     p.add_argument('out_dir',
-                   help='Directory to save all mapping and coloring file.')
+                   help='Directory to save all mapping and coloring files:\n'
+                        '  - correlation_map.nii.gz\n'
+                        '  - session_x/labels_map.nii.gz\n'
+                        '  - session_x/distance_map.nii.gz\n'
+                        '  - session_x/correlation_map.nii.gz\n'
+                        '  - session_x/labels.trk\n'
+                        '  - session_x/distance.trk\n'
+                        '  - session_x/correlation.trk\n'
+                        'Where session_x is numbered with each bundle.')
 
     p.add_argument('--nb_pts', type=int,
                    help='Number of divisions for the bundles.\n'
                         'Default is the number of points of the centroid.')
-    p.add_argument('--new_labeling', action='store_true',
-                   help='Activate the new labeling method based on clusters.')
-    p.add_argument('--min_streamline_count', type=int, default=100000,
-                   help='Minimum number of streamlines for filtering/cutting'
-                        'operation [%(default)s].')
-    p.add_argument('--min_voxel_count', type=int, default=1000000,
-                   help='Minimum number of voxels for filtering/cutting'
-                        'operation [%(default)s].')
     p.add_argument('--colormap', default='jet',
                    help='Select the colormap for colored trk (data_per_point) '
                         '[%(default)s].')
+    p.add_argument('--new_labelling', action='store_true',
+                   help='Use the new labelling method (multi-centroids).')
 
     add_reference_arg(p)
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
-def cube_correlation(density_list, binary_list, size=5):
-    """
-    Compute a local correlation coefficient within a sliding window between
-    all the provided density maps. Computation time grows quickly as the length
-    of density_list increases, since np.corrcoef is pairwise (combinatorial).
-
-    Parameters
-    ----------
-    density_list: list of np.ndarray
-        List of density map from the same bundle across multiple acquisitions.
-    binary_list: list of np.ndarray
-        List of binary map from the same bundle across multiple acquisitions.
-    size: int
-        Total size of the sliding window for the correlation.
-    Returns
-    -------
-    corr_map: np.ndarray
-        Array with local corralation between density maps, same shape as inputs.
-        Between -1 and 1, as floating point values.
-    """
-    elem = np.arange(-(size//2), size//2 + 1).tolist()
-    cube_ind = np.array(list(itertools.product(elem, elem, elem)))
-    union = np.sum(binary_list, axis=0)
-    corr_map = np.zeros(density_list[0].shape)
-    indices = np.array(np.where(union)).T
-    if len(density_list) > 1:
-        for i, ind in enumerate(indices):
-            ind = tuple(ind)
-
-            cube_list = []
-            for density in density_list:
-                cube = map_coordinates(density, (cube_ind+ind).T, order=0)
-
-                if np.count_nonzero(cube) > 1:
-                    cube_list.append(cube.ravel())
-
-            cube_list = np.array(cube_list).T
-            cov_matrix = np.triu(np.corrcoef(cube_list), k=1)
-            corr_map[ind] = np.average(cov_matrix[cov_matrix > 0])
-    else:
-        corr_map = binary_list[0]
-
-    return corr_map
-
-
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
-    set_sft_logger_level('ERROR')
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
+
     assert_inputs_exist(parser, args.in_bundles + [args.in_centroid],
                         optional=args.reference)
     assert_output_dirs_exist_and_empty(parser, args, args.out_dir)
 
     sft_centroid = load_tractogram_with_reference(parser, args,
                                                   args.in_centroid)
 
-    if len(sft_centroid.streamlines) < 1 \
-            or len(sft_centroid.streamlines) > 1:
-        logging.error('Centroid file {} should contain one streamline. '
-                      'Skipping'.format(args.in_centroid))
-        raise ValueError
     sft_centroid.to_vox()
     sft_centroid.to_corner()
 
     sft_list = []
     for filename in args.in_bundles:
         sft = load_tractogram_with_reference(parser, args, filename)
         if not len(sft.streamlines):
-            logging.error('Empty bundle file {}. '
+            raise IOError('Empty bundle file {}. '
                           'Skipping'.format(args.in_bundle))
-            raise ValueError
         sft.to_vox()
         sft.to_corner()
         sft_list.append(sft)
 
         if len(sft_list):
             if not is_header_compatible(sft_list[0], sft_list[-1]):
-                parser.error('ERROR HEADER')
+                parser.error('Header of {} and {} are not compatible'.format(
+                    args.in_bundles[0], filename))
 
     density_list = []
     binary_list = []
     for sft in sft_list:
         density = compute_tract_counts_map(sft.streamlines,
                                            sft.dimensions).astype(float)
         binary = np.zeros(sft.dimensions)
         binary[density > 0] = 1
         binary_list.append(binary)
-
-        density = gaussian_filter(density, 1) * binary
-        density[binary < 1] += np.random.normal(0.0, 1.0,
-                                                binary[binary < 1].shape)
         density_list.append(density)
 
     if not is_header_compatible(sft_centroid, sft_list[0]):
         raise IOError('{} and {}do not have a compatible header'.format(
             args.in_centroid, args.in_bundle))
 
-    corr_map = cube_correlation(density_list, binary_list)
+    if len(density_list) > 1:
+        corr_map = correlation(density_list, None)
+    else:
+        corr_map = density_list[0].astype(float)
+        corr_map[corr_map > 0] = 1
+
     # Slightly cut the bundle at the edgge to clean up single streamline voxels
     # with no neighbor. Remove isolated voxels to keep a single 'blob'
     binary_bundle = np.zeros(corr_map.shape, dtype=bool)
     binary_bundle[corr_map > 0.5] = 1
-    min_streamlines_count = 1e16
-    for sft in sft_list:
-        min_streamlines_count = min(len(sft), min_streamlines_count)
-
-    structure_cross = ndi.generate_binary_structure(3, 1)
-    if np.count_nonzero(binary_bundle) > args.min_voxel_count \
-            and min_streamlines_count > args.min_streamline_count:
-        binary_bundle = ndi.binary_dilation(binary_bundle,
-                                            structure=structure_cross)
-        binary_bundle = ndi.binary_erosion(binary_bundle,
-                                           structure=structure_cross,
-                                           iterations=2)
 
     bundle_disjoint, _ = ndi.label(binary_bundle)
     unique, count = np.unique(bundle_disjoint, return_counts=True)
     val = unique[np.argmax(count[1:])+1]
     binary_bundle[bundle_disjoint != val] = 0
 
     corr_map = corr_map*binary_bundle
@@ -199,110 +150,111 @@
     concat_sft = StatefulTractogram.from_sft([], sft_list[0])
     for i in range(len(sft_list)):
         sft_list[i] = cut_outside_of_mask_streamlines(sft_list[i],
                                                       binary_bundle)
         if len(sft_list[i]):
             concat_sft += sft_list[i]
 
-    if args.nb_pts is not None:
-        sft_centroid = resample_streamlines_num_points(sft_centroid,
-                                                       args.nb_pts)
+    args.nb_pts = len(sft_centroid.streamlines[0]) if args.nb_pts is None \
+        else args.nb_pts
+
+    sft_centroid = resample_streamlines_num_points(sft_centroid, args.nb_pts)
+    tmp_sft = resample_streamlines_num_points(concat_sft, args.nb_pts)
+
+    if not args.new_labelling:
+        new_streamlines = sft_centroid.streamlines.copy()
+        sft_centroid = StatefulTractogram.from_sft([new_streamlines[0]],
+                                                   sft_centroid)
     else:
-        args.nb_pts = len(sft_centroid.streamlines[0])
+        srr = StreamlineLinearRegistration()
+        srm = srr.optimize(static=tmp_sft.streamlines,
+                           moving=sft_centroid.streamlines)
+        sft_centroid.streamlines = srm.transform(sft_centroid.streamlines)
+
+    uniformize_bundle_sft(concat_sft, ref_bundle=sft_centroid[0])
+    labels, dists = min_dist_to_centroid(concat_sft.streamlines._data,
+                                         sft_centroid.streamlines._data,
+                                         args.nb_pts)
+    labels += 1  # 0 means no labels
 
-    thresholds = [24, 18, 12, 6] if args.new_labeling else [200]
-    clusters_map = qbx_and_merge(concat_sft.streamlines, thresholds,
-                                 nb_pts=args.nb_pts, verbose=False,
-                                 rng=np.random.RandomState(1))
+    # It is not allowed that labels jumps labels for consistency
+    # Streamlines should have continous labels
     final_streamlines = []
     final_label = []
-    final_dist = []
-    for _, cluster in enumerate(clusters_map):
-        tmp_sft = StatefulTractogram.from_sft([cluster.centroid], concat_sft)
-        uniformize_bundle_sft(tmp_sft, ref_bundle=sft_centroid)
-        cluster_centroid = tmp_sft.streamlines[0] if args.new_labeling \
-            else sft_centroid.streamlines[0]
-        cluster_streamlines = ArraySequence(cluster[:])
-        min_dist_label, min_dist = min_dist_to_centroid(cluster_streamlines._data,
-                                                        cluster_centroid)
-        min_dist_label += 1  # 0 means no labels
-
-        # It is not allowed that labels jumps labels for consistency
-        # Streamlines should have continous labels
-        curr_ind = 0
-        for i, streamline in enumerate(cluster_streamlines):
-            next_ind = curr_ind + len(streamline)
-            curr_labels = min_dist_label[curr_ind:next_ind]
-            curr_dist = min_dist[curr_ind:next_ind]
-            curr_ind = next_ind
-
-            # Flip streamlines so the labels increase (facilitate if/else)
-            # Should always be ordered in nextflow pipeline
-            gradient = np.gradient(curr_labels)
-            if len(np.argwhere(gradient < 0)) > len(np.argwhere(gradient > 0)):
-                streamline = streamline[::-1]
-                curr_labels = curr_labels[::-1]
-                curr_dist = curr_dist[::-1]
-
-            # Find jumps, cut them and find the longest
-            gradient = np.ediff1d(curr_labels)
-            max_jump = max(args.nb_pts // 5, 1)
-            if len(np.argwhere(np.abs(gradient) > max_jump)) > 0:
-                pos_jump = np.where(np.abs(gradient) > max_jump)[0] + 1
-                split_chunk = np.split(curr_labels,
-                                       pos_jump)
-                max_len = 0
-                max_pos = 0
-                for j, chunk in enumerate(split_chunk):
-                    if len(chunk) > max_len:
-                        max_len = len(chunk)
-                        max_pos = j
-
-                curr_labels = split_chunk[max_pos]
-                gradient_chunk = np.ediff1d(chunk)
-                if len(np.unique(np.sign(gradient_chunk))) > 1:
-                    continue
-                streamline = np.split(streamline,
-                                      pos_jump)[max_pos]
-                curr_dist = np.split(curr_dist,
-                                     pos_jump)[max_pos]
-
-            final_streamlines.append(streamline)
-            final_label.append(curr_labels)
-            final_dist.append(curr_dist)
+    final_dists = []
+    curr_ind = 0
+    for i, streamline in enumerate(concat_sft.streamlines):
+        next_ind = curr_ind + len(streamline)
+        curr_labels = labels[curr_ind:next_ind]
+        curr_dists = dists[curr_ind:next_ind]
+        curr_ind = next_ind
+
+        # Flip streamlines so the labels increase (facilitate if/else)
+        # Should always be ordered in nextflow pipeline
+        gradient = np.gradient(curr_labels)
+        if len(np.argwhere(gradient < 0)) > len(np.argwhere(gradient > 0)):
+            streamline = streamline[::-1]
+            curr_labels = curr_labels[::-1]
+            curr_dists = curr_dists[::-1]
+
+        # # Find jumps, cut them and find the longest
+        gradient = np.ediff1d(curr_labels)
+        max_jump = max(args.nb_pts // 5, 1)
+        if len(np.argwhere(np.abs(gradient) > max_jump)) > 0:
+            pos_jump = np.where(np.abs(gradient) > max_jump)[0] + 1
+            split_chunk = np.split(curr_labels,
+                                   pos_jump)
+
+            max_len = 0
+            max_pos = 0
+            for j, chunk in enumerate(split_chunk):
+                if len(chunk) > max_len:
+                    max_len = len(chunk)
+                    max_pos = j
+
+            curr_labels = split_chunk[max_pos]
+            gradient_chunk = np.ediff1d(chunk)
+            if len(np.unique(np.sign(gradient_chunk))) > 1:
+                continue
+            streamline = np.split(streamline,
+                                  pos_jump)[max_pos]
+            curr_dists = np.split(curr_dists,
+                                  pos_jump)[max_pos]
+
+        final_streamlines.append(streamline)
+        final_label.append(curr_labels)
+        final_dists.append(curr_dists)
 
     final_streamlines = ArraySequence(final_streamlines)
-    labels_array = ArraySequence(final_label)
-    dist_array = ArraySequence(final_dist)
+    final_labels = ArraySequence(final_label)
+    final_dists = ArraySequence(final_dists)
+
     kd_tree = cKDTree(final_streamlines._data)
     labels_map = np.zeros(binary_bundle.shape, dtype=np.int16)
     distance_map = np.zeros(binary_bundle.shape, dtype=float)
-    indices = np.nonzero(binary_bundle)
+    indices = np.array(np.nonzero(binary_bundle), dtype=int).T
+
+    for ind in indices:
+        _, neighbor_ids = kd_tree.query(ind, k=5)
 
-    for i in range(len(indices[0])):
-        ind = np.array([indices[0][i], indices[1][i], indices[2][i]],
-                       dtype=int)
-        neighbor_ids = kd_tree.query_ball_point(ind, 2.0)
-        if not neighbor_ids:
+        if not len(neighbor_ids):
             continue
-        labels_val = labels_array._data[neighbor_ids]
-        dist_centro = dist_array._data[neighbor_ids]
-        dist_vox = np.linalg.norm(final_streamlines._data[neighbor_ids] - ind,
-                                  axis=1)
-
-        if np.sum(dist_centro) > 0:
-            labels_map[tuple(ind)] = np.round(
-                np.average(labels_val, weights=dist_centro*dist_vox))
-            distance_map[tuple(ind)] = np.average(dist_centro*dist_vox)
-        else:
-            labels_map[tuple(ind)] = np.round(
-                np.average(labels_val, weights=dist_vox))
-            distance_map[tuple(ind)] = np.average(dist_vox)
 
-        cmap = get_colormap(args.colormap)
+        labels_val = final_labels._data[neighbor_ids]
+        dists_val = final_dists._data[neighbor_ids]
+        sum_dists_vox = np.sum(dists_val)
+        weights_vox = np.exp(-dists_val / sum_dists_vox)
+
+        vote = np.bincount(labels_val, weights=weights_vox)
+        total = np.arange(np.amax(labels_val+1))
+        winner = total[np.argmax(vote)]
+        labels_map[ind[0], ind[1], ind[2]] = winner
+        distance_map[ind[0], ind[1], ind[2]] = np.average(dists_val)
+
+        cmap = get_lookup_table(args.colormap)
 
     for i, sft in enumerate(sft_list):
         if len(sft_list) > 1:
             sub_out_dir = os.path.join(args.out_dir, 'session_{}'.format(i+1))
         else:
             sub_out_dir = args.out_dir
         new_sft = StatefulTractogram.from_sft(sft.streamlines, sft_list[0])
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_centroid.py` & `scilpy-2.0.0/scripts/scil_bundle_compute_centroid.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,25 +1,29 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
 Compute a single bundle centroid, using an 'infinite' QuickBundles threshold.
+
+Formerly: scil_compute_centroid.py
 """
 
 import argparse
+import logging
 
 from dipy.io.stateful_tractogram import StatefulTractogram
 from dipy.io.streamline import save_tractogram
 
 from scilpy.io.streamlines import load_tractogram_with_reference
 from scilpy.io.utils import (add_overwrite_arg,
                              assert_inputs_exist,
                              assert_outputs_exist,
+                             add_verbose_arg,
                              add_reference_arg)
-from scilpy.tractanalysis.features import get_streamlines_centroid
+from scilpy.tractanalysis.bundle_operations import get_streamlines_centroid
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__,
         formatter_class=argparse.RawTextHelpFormatter)
 
@@ -28,23 +32,25 @@
     p.add_argument('out_centroid',
                    help='Output centroid streamline filename.')
     p.add_argument('--nb_points', type=int, default=20,
                    help='Number of points defining the centroid streamline'
                         '[%(default)s].')
 
     add_reference_arg(p)
+    add_verbose_arg(p)
     add_overwrite_arg(p)
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, args.in_bundle)
+    assert_inputs_exist(parser, args.in_bundle, args.reference)
     assert_outputs_exist(parser, args, args.out_centroid)
 
     if args.nb_points < 2:
         parser.error('--nb_points {} should be >= 2'.format(args.nb_points))
 
     sft = load_tractogram_with_reference(parser, args, args.in_bundle)
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_connectivity.py` & `scilpy-2.0.0/scripts/scil_connectivity_compute_matrices.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
 This script computes a variety of measures in the form of connectivity
-matrices. This script is made to follow scil_decompose_connectivity and
+matrices. This script is made to follow
+scil_tractogram_segment_bundles_for_connectivity.py and
 uses the same labels list as input.
 
 The script expects a folder containing all relevants bundles following the
 naming convention LABEL1_LABEL2.trk and a text file containing the list of
 labels that should be part of the matrices. The ordering of labels in the
 matrices will follow the same order as the list.
 This script only generates matrices in the form of array, does not visualize
@@ -31,40 +32,44 @@
 
 The parameters --lesion_load will compute 3 lesion(s) related matrices:
 lesion_count.npy, lesion_vol.npy, lesion_sc.npy and put it inside of a
 specified folder. They represent the number of lesion, the total volume of
 lesion(s) and the total of streamlines going through the lesion(s) for  of each
 connection. Each connection can be seen as a 'bundle' and then something
 similar to scil_analyse_lesion_load.py is run for each 'bundle'.
+
+Formerly: scil_compute_connectivity.py
 """
 
 import argparse
 import copy
 import itertools
-import multiprocessing
 import logging
+import multiprocessing
 import os
 
 import coloredlogs
 from dipy.io.utils import is_header_compatible, get_reference_info
 from dipy.tracking.streamlinespeed import length
 from dipy.tracking.vox2track import _streamlines_in_mask
 import h5py
 import nibabel as nib
 import numpy as np
 import scipy.ndimage as ndi
 
 from scilpy.image.labels import get_data_as_labels
+from scilpy.io.hdf5 import (assert_header_compatible_hdf5,
+                            reconstruct_streamlines_from_hdf5)
 from scilpy.io.image import get_data_as_mask
-from scilpy.io.streamlines import reconstruct_streamlines_from_hdf5
 from scilpy.io.utils import (add_overwrite_arg, add_processes_arg,
                              add_verbose_arg,
                              assert_inputs_exist, assert_outputs_exist,
                              validate_nbr_processes)
-from scilpy.tractanalysis.reproducibility_measures import compute_bundle_adjacency_voxel
+from scilpy.tractanalysis.reproducibility_measures import \
+    compute_bundle_adjacency_voxel
 from scilpy.tractanalysis.streamlines_metrics import compute_tract_counts_map
 from scilpy.utils.metrics_tools import compute_lesion_stats
 
 
 def load_node_nifti(directory, in_label, out_label, ref_img):
     in_filename = os.path.join(directory,
                                '{}_{}.nii.gz'.format(in_label, out_label))
@@ -89,29 +94,27 @@
     include_dps = args[6]
     min_lesion_vol = args[7]
 
     hdf5_file = h5py.File(hdf5_filename, 'r')
     key = '{}_{}'.format(in_label, out_label)
     if key not in hdf5_file:
         return
-    streamlines = reconstruct_streamlines_from_hdf5(hdf5_file, key)
+    streamlines = reconstruct_streamlines_from_hdf5(hdf5_file[key])
     if len(streamlines) == 0:
         return
 
     affine, dimensions, voxel_sizes, _ = get_reference_info(labels_img)
     measures_to_return = {}
-
-    if not (np.allclose(hdf5_file.attrs['affine'], affine, atol=1e-03)
-            and np.array_equal(hdf5_file.attrs['dimensions'], dimensions)):
-        raise ValueError('Provided hdf5 have incompatible headers.')
+    assert_header_compatible_hdf5(hdf5_file, (affine, dimensions))
 
     # Precompute to save one transformation, insert later
     if 'length' in measures_to_compute:
         streamlines_copy = list(streamlines)
-        # scil_decompose_connectivity.py requires isotropic voxels
+        # scil_tractogram_segment_bundles_for_connectivity.py requires
+        # isotropic voxels
         mean_length = np.average(length(streamlines_copy))*voxel_sizes[0]
 
     # If density is not required, do not compute it
     # Only required for volume, similarity and any metrics
     if not ((len(measures_to_compute) == 1 and
              ('length' in measures_to_compute or
               'streamline_count' in measures_to_compute)) or
@@ -222,15 +225,16 @@
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__,
         formatter_class=argparse.RawTextHelpFormatter,)
     p.add_argument('in_hdf5',
                    help='Input filename for the hdf5 container (.h5).\n'
-                        'Obtained from scil_decompose_connectivity.py.')
+                        'Obtained from '
+                        'scil_tractogram_segment_bundles_for_connectivity.py.')
     p.add_argument('in_labels',
                    help='Labels file name (nifti).\n'
                         'This generates a NxN connectivity matrix.')
     p.add_argument('--volume', metavar='OUT_FILE',
                    help='Output file for the volume weighted matrix (.npy).')
     p.add_argument('--streamline_count', metavar='OUT_FILE',
                    help='Output file for the streamline count weighted matrix '
@@ -253,15 +257,16 @@
     p.add_argument('--lesion_load', nargs=2, metavar=('IN_FILE', 'OUT_DIR'),
                    help='Input binary mask (.nii.gz) and output directory '
                         'for all lesion-related matrices.')
     p.add_argument('--min_lesion_vol', type=float, default=7,
                    help='Minimum lesion volume in mm3 [%(default)s].')
 
     p.add_argument('--density_weighting', action="store_true",
-                   help='Use density-weighting for the metric weighted matrix.')
+                   help='Use density-weighting for the metric weighted'
+                   'matrix.')
     p.add_argument('--no_self_connection', action="store_true",
                    help='Eliminate the diagonal from the matrices.')
     p.add_argument('--include_dps', metavar='OUT_DIR',
                    help='Save matrices from data_per_streamline in the output '
                         'directory.\nCOMMIT-related values will be summed '
                         'instead of averaged.\nWill always overwrite files.')
     p.add_argument('--force_labels_list',
@@ -274,22 +279,21 @@
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
+    coloredlogs.install(level=logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, [args.in_hdf5, args.in_labels],
                         args.force_labels_list)
 
-    log_level = logging.INFO if args.verbose else logging.WARNING
-    logging.getLogger().setLevel(log_level)
-    coloredlogs.install(level=log_level)
-
+    # Summarizing all options chosen by user in measures_to_compute.
     measures_to_compute = []
     measures_output_filename = []
     if args.volume:
         measures_to_compute.append('volume')
         measures_output_filename.append(args.volume)
     if args.streamline_count:
         measures_to_compute.append('streamline_count')
@@ -297,34 +301,37 @@
     if args.length:
         measures_to_compute.append('length')
         measures_output_filename.append(args.length)
     if args.similarity:
         measures_to_compute.append('similarity')
         measures_output_filename.append(args.similarity[1])
 
+    # Adding measures from pre-computed maps.
     dict_maps_out_name = {}
     if args.maps is not None:
         for in_folder, out_name in args.maps:
             measures_to_compute.append(in_folder)
             dict_maps_out_name[in_folder] = out_name
             measures_output_filename.append(out_name)
 
+    # Adding measures from pre-computed metrics.
     dict_metrics_out_name = {}
     if args.metrics is not None:
         for in_name, out_name in args.metrics:
             # Verify that all metrics are compatible with each other
             if not is_header_compatible(args.metrics[0][0], in_name):
-                raise IOError('Metrics {} and  {} do not share a compatible '
+                raise IOError('Metrics {} and {} do not share a compatible '
                               'header'.format(args.metrics[0][0], in_name))
 
             # This is necessary to support more than one map for weighting
             measures_to_compute.append((in_name, nib.load(in_name)))
             dict_metrics_out_name[in_name] = out_name
             measures_output_filename.append(out_name)
 
+    # Adding measures from lesions.
     dict_lesion_out_name = {}
     if args.lesion_load is not None:
         in_name = args.lesion_load[0]
         lesion_img = nib.load(in_name)
         lesion_data = get_data_as_mask(lesion_img, dtype=bool)
         lesion_atlas, _ = ndi.label(lesion_data)
         measures_to_compute.append(((in_name, np.unique(lesion_atlas)[1:]),
@@ -336,63 +343,69 @@
         out_name_3 = os.path.join(args.lesion_load[1], 'lesion_sc.npy')
 
         dict_lesion_out_name[in_name+'vol'] = out_name_1
         dict_lesion_out_name[in_name+'count'] = out_name_2
         dict_lesion_out_name[in_name+'sc'] = out_name_3
         measures_output_filename.extend([out_name_1, out_name_2, out_name_3])
 
+    # Verifying all outputs that will be used for all measures.
     assert_outputs_exist(parser, args, measures_output_filename)
     if not measures_to_compute:
         parser.error('No connectivity measures were selected, nothing '
                      'to compute.')
 
     logging.info('The following measures will be computed and save: {}'.format(
         measures_output_filename))
 
     if args.include_dps:
         if not os.path.isdir(args.include_dps):
             os.makedirs(args.include_dps)
         logging.info('data_per_streamline weighting is activated.')
 
+    # Loading the data
     img_labels = nib.load(args.in_labels)
     data_labels = get_data_as_labels(img_labels)
     if not args.force_labels_list:
         labels_list = np.unique(data_labels)[1:].tolist()
     else:
         labels_list = np.loadtxt(
             args.force_labels_list, dtype=np.int16).tolist()
 
+    # Finding all connectivity combo (start-finish)
     comb_list = list(itertools.combinations(labels_list, r=2))
     if not args.no_self_connection:
         comb_list.extend(zip(labels_list, labels_list))
 
+    # Running everything!
     nbr_cpu = validate_nbr_processes(parser, args)
     measures_dict_list = []
     if nbr_cpu == 1:
         for comb in comb_list:
-            measures_dict_list.append(_processing_wrapper([args.in_hdf5,
-                                                           img_labels, comb,
-                                                           measures_to_compute,
-                                                           args.similarity,
-                                                           args.density_weighting,
-                                                           args.include_dps,
-                                                           args.min_lesion_vol]))
+            measures_dict_list.append(_processing_wrapper(
+                                                [args.in_hdf5,
+                                                 img_labels, comb,
+                                                 measures_to_compute,
+                                                 args.similarity,
+                                                 args.density_weighting,
+                                                 args.include_dps,
+                                                 args.min_lesion_vol]))
     else:
         pool = multiprocessing.Pool(nbr_cpu)
         measures_dict_list = pool.map(_processing_wrapper,
                                       zip(itertools.repeat(args.in_hdf5),
                                           itertools.repeat(img_labels),
                                           comb_list,
                                           itertools.repeat(
                                               measures_to_compute),
                                           itertools.repeat(args.similarity),
                                           itertools.repeat(
                                           args.density_weighting),
                                           itertools.repeat(args.include_dps),
-                                          itertools.repeat(args.min_lesion_vol)))
+                                          itertools.repeat(args.min_lesion_vol)
+                                          ))
         pool.close()
         pool.join()
 
     # Removing None entries (combinaisons that do not exist)
     # Fusing the multiprocessing output into a single dictionary
     measures_dict_list = [it for it in measures_dict_list if it is not None]
     if not measures_dict_list:
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_divide.py` & `scilpy-2.0.0/scripts/scil_btensor_metrics.py`

 * *Files 10% similar despite different names*

```diff
@@ -8,42 +8,56 @@
 tensor encoding (LTE, b_delta = 1), the planar tensor encoding
 (PTE, b_delta = -0.5), the spherical tensor encoding (STE, b_delta = 0) and
 the cigar shape tensor encoding (b_delta = 0.5) are available. Moreover, all
 of `--in_dwis`, `--in_bvals`, `--in_bvecs` and `--in_bdeltas` must have the
 same number of arguments. Be sure to keep the same order of encodings
 throughout all these inputs and to set `--in_bdeltas` accordingly (IMPORTANT).
 
-By default, will output all possible files, using default names.
-Specific names can be specified using the file flags specified in the
-"File flags" section.
+By default, will output all possible files, using default names. Thus, this
+script outputs the results from the DIVIDE fit or direct derivatives:
+mean diffusivity (MD), isotropic mean kurtosis (mk_i), anisotropic mean
+kurtosis (mk_a), total mean kurtosis (mk_t) and finally micro-FA (uFA).
+Specific names can be specified using the
+file flags specified in the "File flags" section.
 
 If --not_all is set, only the files specified explicitly by the flags
-will be output.
+will be output. The order parameter can also be computed from the uFA and a
+precomputed FA, using separate input parameters.
+
+>>> scil_btensor_metrics.py --in_dwis LTE.nii.gz PTE.nii.gz STE.nii.gz
+    --in_bvals LTE.bval PTE.bval STE.bval --in_bvecs LTE.bvec PTE.bvec STE.bvec
+    --in_bdeltas 1 -0.5 0 --mask mask.nii.gz
+
+IMPORTANT: If the script does not converge to a solution, it is probably due to
+noise outside the brain. Thus, it is strongly recommanded to provide a brain
+mask with --mask.
 
 Based on Markus Nilsson, Filip Szczepankiewicz, Björn Lampinen, André Ahlgren,
 João P. de Almeida Martins, Samo Lasic, Carl-Fredrik Westin,
 and Daniel Topgaard. An open-source framework for analysis of multidimensional
 diffusion MRI data implemented in MATLAB.
 Proc. Intl. Soc. Mag. Reson. Med. (26), Paris, France, 2018.
+
+Formerly: scil_compute_divide.py
 """
 
 import argparse
 import logging
 
 import nibabel as nib
 import numpy as np
 
 from scilpy.image.utils import extract_affine
+from scilpy.io.btensor import generate_btensor_input
 from scilpy.io.image import get_data_as_mask
 from scilpy.io.utils import (add_overwrite_arg, assert_inputs_exist,
-                             assert_outputs_exist, add_force_b0_arg,
-                             add_processes_arg, add_verbose_arg)
-from scilpy.reconst.multi_processes import fit_gamma
-from scilpy.reconst.divide_fit import gamma_fit2metrics
-from scilpy.reconst.b_tensor_utils import generate_btensor_input
+                             assert_outputs_exist, add_processes_arg,
+                             add_verbose_arg, add_skip_b0_check_arg,
+                             add_tolerance_arg, assert_headers_compatible)
+from scilpy.reconst.divide import fit_gamma, gamma_fit2metrics
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(description=__doc__,
                                 formatter_class=argparse.RawTextHelpFormatter)
 
     p.add_argument('--in_dwis', nargs='+', required=True,
@@ -60,21 +74,17 @@
                    help='Value of b_delta for each b-tensor encoding type, '
                         'in the same order as dwi, bval and bvec inputs.')
 
     p.add_argument(
         '--mask',
         help='Path to a binary mask. Only the data inside the '
              'mask will be used for computations and reconstruction.')
-    p.add_argument(
-        '--fa',
-        help='Path to a FA map. Needed for calculating the OP.')
-    p.add_argument(
-        '--tolerance', type=int, default=20,
-        help='The tolerated gap between the b-values to '
-             'extract\nand the current b-value. [%(default)s]')
+    add_tolerance_arg(p)
+    add_skip_b0_check_arg(p, will_overwrite_with_min=False,
+                          b0_tol_name='--tolerance')
     p.add_argument(
         '--fit_iters', type=int, default=1,
         help='The number of time the gamma fit will be done [%(default)s]')
     p.add_argument(
         '--random_iters', type=int, default=50,
         help='The number of iterations for the initial parameters search. '
              '[%(default)s]')
@@ -86,18 +96,27 @@
         '--do_weight_pa', action='store_false',
         help='If set, does not do a powder averaging weighting in the gamma '
              'fit.')
     p.add_argument(
         '--do_multiple_s0', action='store_false',
         help='If set, does not take into account multiple baseline signals.')
 
-    add_force_b0_arg(p)
+    g2 = p.add_argument_group(title='Order parameter (OP)')
+    g2.add_argument(
+        '--op',
+        help='Output filename for the order parameter. The OP will not be '
+             'output if this is not given. Computation of the OP also '
+             'requires a precomputed FA map (given using --fa).')
+    g2.add_argument(
+        '--fa',
+        help='Path to a FA map. Needed for calculating the OP.')
+
     add_processes_arg(p)
-    add_overwrite_arg(p)
     add_verbose_arg(p)
+    add_overwrite_arg(p)
 
     p.add_argument(
         '--not_all', action='store_true',
         help='If set, only saves the files specified using the '
              'file flags. (Default: False)')
 
     g = p.add_argument_group(title='File flags')
@@ -105,17 +124,14 @@
     g.add_argument(
         '--md', metavar='file', default='',
         help='Output filename for the MD.')
     g.add_argument(
         '--ufa', metavar='file', default='',
         help='Output filename for the microscopic FA.')
     g.add_argument(
-        '--op', metavar='file', default='',
-        help='Output filename for the order parameter.')
-    g.add_argument(
         '--mk_i', metavar='file', default='',
         help='Output filename for the isotropic mean kurtosis.')
     g.add_argument(
         '--mk_a', metavar='file', default='',
         help='Output filename for the anisotropic mean kurtosis.')
     g.add_argument(
         '--mk_t', metavar='file', default='',
@@ -123,76 +139,78 @@
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    if args.verbose:
-        logging.basicConfig(level=logging.DEBUG)
-    else:
-        logging.basicConfig(level=logging.INFO)
-
+    # Verifications
     if not args.not_all:
         args.md = args.md or 'md.nii.gz'
         args.ufa = args.ufa or 'ufa.nii.gz'
-        args.op = args.op or 'op.nii.gz'
         args.mk_i = args.mk_i or 'mk_i.nii.gz'
         args.mk_a = args.mk_a or 'mk_a.nii.gz'
         args.mk_t = args.mk_t or 'mk_t.nii.gz'
 
-    arglist = [args.md, args.ufa, args.op, args.mk_i, args.mk_a, args.mk_t]
+    arglist = [args.md, args.ufa, args.mk_i, args.mk_a, args.mk_t]
     if args.not_all and not any(arglist):
-        parser.error('When using --not_all, you need to specify at least ' +
+        parser.error('When using --not_all, you need to specify at least '
                      'one file to output.')
 
-    assert_inputs_exist(parser, [],
-                        optional=list(np.concatenate((args.in_dwis,
-                                                      args.in_bvals,
-                                                      args.in_bvecs))))
+    assert_inputs_exist(parser, args.in_dwis + args.in_bvals + args.in_bvecs,
+                        [args.mask, args.fa])
     assert_outputs_exist(parser, args, arglist)
+    assert_headers_compatible(parser, args.in_dwis, [args.mask, args.fa])
+
+    if args.op and not args.fa:
+        parser.error('Computation of the OP requires a precomputed '
+                     'FA map (given using --fa).')
 
     if not (len(args.in_dwis) == len(args.in_bvals)
             == len(args.in_bvecs) == len(args.in_bdeltas)):
         msg = """The number of given dwis, bvals, bvecs and bdeltas must be the
               same. Please verify that all inputs were correctly inserted."""
         raise ValueError(msg)
 
     if len(np.unique(args.in_bdeltas)) < 2:
         msg = """At least two different b-tensor shapes are needed for the
               script to work properly."""
         raise ValueError(msg)
 
+    # Loading
     affine = extract_affine(args.in_dwis)
 
-    tol = args.tolerance
-    force_b0_thr = args.force_b0_threshold
-
+    # Note. This script does not currently allow using a separate b0_threshold
+    # for the b0s. Using the tolerance. To change this, we would have to
+    # change generate_btensor_input.
     data, gtab_infos = generate_btensor_input(args.in_dwis,
                                               args.in_bvals,
                                               args.in_bvecs,
                                               args.in_bdeltas,
-                                              force_b0_thr,
                                               do_pa_signals=True,
-                                              tol=tol)
+                                              tol=args.tolerance,
+                                              skip_b0_check=args.skip_b0_check)
 
     gtab_infos[0] *= 1e6  # getting bvalues to SI units
 
     if args.mask is None:
         mask = None
+        logging.warning(
+            'No mask provided. The fit might not converge due to noise. '
+            'Please provide a mask if it is the case.')
     else:
         mask = get_data_as_mask(nib.load(args.mask), dtype=bool)
-        if mask.shape != data.shape[:-1]:
-            raise ValueError("Mask is not the same shape as data.")
 
     if args.fa is not None:
         vol = nib.load(args.fa)
         FA = vol.get_fdata(dtype=np.float32)
 
+    # Processing
     parameters = fit_gamma(data, gtab_infos, mask=mask,
                            fit_iters=args.fit_iters,
                            random_iters=args.random_iters,
                            do_weight_bvals=args.do_weight_bvals,
                            do_weight_pa=args.do_weight_pa,
                            do_multiple_s0=args.do_multiple_s0,
                            nbr_processes=args.nbr_processes)
@@ -200,24 +218,20 @@
     microFA, MK_I, MK_A, MK_T = gamma_fit2metrics(parameters)
     microFA[np.isnan(microFA)] = 0
     microFA = np.clip(microFA, 0, 1)
 
     if args.md:
         nib.save(nib.Nifti1Image(parameters[..., 1].astype(np.float32),
                                  affine), args.md)
-
     if args.ufa:
         nib.save(nib.Nifti1Image(microFA.astype(np.float32), affine), args.ufa)
     if args.op:
-        if args.fa is not None:
-            OP = np.sqrt((3 * (microFA ** (-2)) - 2) / (3 * (FA ** (-2)) - 2))
-            OP[microFA < FA] = 0
-            nib.save(nib.Nifti1Image(OP.astype(np.float32), affine), args.op)
-        else:
-            logging.warning('The FA must be given in order to compute the OP.')
+        OP = np.sqrt((3 * (microFA ** (-2)) - 2) / (3 * (FA ** (-2)) - 2))
+        OP[microFA < FA] = 0
+        nib.save(nib.Nifti1Image(OP.astype(np.float32), affine), args.op)
     if args.mk_i:
         nib.save(nib.Nifti1Image(MK_I.astype(np.float32), affine), args.mk_i)
     if args.mk_a:
         nib.save(nib.Nifti1Image(MK_A.astype(np.float32), affine), args.mk_a)
     if args.mk_t:
         nib.save(nib.Nifti1Image(MK_T.astype(np.float32), affine), args.mk_t)
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_dti_metrics.py` & `scilpy-2.0.0/scripts/scil_dti_metrics.py`

 * *Files 8% similar despite different names*

```diff
@@ -16,14 +16,16 @@
 FA (rgb), principal tensor e-vector and tensor coefficients (dxx, dxy, dxz,
 dyy, dyz, dzz).
 
 For all the quality control metrics such as residual, physically implausible
 signals, pulsation and misalignment artifacts, see
 [J-D Tournier, S. Mori, A. Leemans. Diffusion Tensor Imaging and Beyond.
 MRM 2011].
+
+Formerly: scil_compute_dti_metrics.py
 """
 
 import argparse
 import logging
 
 import matplotlib.pyplot as plt
 import nibabel as nib
@@ -35,31 +37,32 @@
 from dipy.reconst.dti import (TensorModel, color_fa, fractional_anisotropy,
                               geodesic_anisotropy, mean_diffusivity,
                               axial_diffusivity, norm,
                               radial_diffusivity, lower_triangular)
 # Aliased to avoid clashes with images called mode.
 from dipy.reconst.dti import mode as dipy_mode
 
+from scilpy.dwi.operations import compute_residuals, \
+    compute_residuals_statistics
 from scilpy.io.image import get_data_as_mask
-from scilpy.io.utils import (add_overwrite_arg, assert_inputs_exist,
-                             assert_outputs_exist, add_force_b0_arg)
-from scilpy.reconst.dti import convert_tensor_from_dipy_format, \
+from scilpy.io.utils import (add_b0_thresh_arg, add_overwrite_arg,
+                             add_skip_b0_check_arg, add_verbose_arg,
+                             assert_inputs_exist, assert_outputs_exist,
+                             assert_headers_compatible)
+from scilpy.io.tensor import convert_tensor_from_dipy_format, \
     supported_tensor_formats, tensor_format_description
-from scilpy.utils.bvec_bval_tools import (normalize_bvecs, is_normalized_bvecs,
-                                          check_b0_threshold)
+from scilpy.gradients.bvec_bval_tools import (check_b0_threshold,
+                                              is_normalized_bvecs,
+                                              normalize_bvecs)
 from scilpy.utils.filenames import add_filename_suffix, split_name_with_nii
 
-logger = logging.getLogger("Compute_DTI_Metrics")
+logger = logging.getLogger("DTI_Metrics")
 logger.setLevel(logging.INFO)
 
 
-def _get_min_nonzero_signal(data):
-    return np.min(data[data > 0])
-
-
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__,
         formatter_class=argparse.RawTextHelpFormatter)
     p.add_argument('in_dwi',
                    help='Path of the input diffusion volume.')
     p.add_argument('in_bval',
@@ -135,23 +138,96 @@
         help='Standard deviation map across all diffusion-weighted images '
              'and across b=0 images if more than one is available.\nShows '
              'pulsation and misalignment artifacts.')
     g.add_argument(
         '--residual', dest='residual', metavar='file', default='',
         help='Output filename for the map of the residual of the tensor fit.')
 
-    add_force_b0_arg(p)
+    add_b0_thresh_arg(p)
+    add_skip_b0_check_arg(p, will_overwrite_with_min=True)
+    add_verbose_arg(p)
 
     return p
 
 
+def _plot_residuals(args, data_diff, mask, R_k, q1, q3, iqr, residual_basename):
+    # Showing results in graph
+    # Note that stats will be computed manually and plotted using bxp
+    # but could be computed using stats = cbook.boxplot_stats
+    # or pyplot.boxplot(x)
+    if mask is None:
+        logging.info("Outlier detection will not be performed, since no "
+                     "mask was provided.")
+
+    # Initializing stats as a List[dict]
+    stats = [dict.fromkeys(['label', 'mean', 'iqr', 'cilo', 'cihi',
+                            'whishi', 'whislo', 'fliers', 'q1',
+                            'med', 'q3'], [])
+             for _ in range(data_diff.shape[-1])]
+
+    nb_voxels = np.count_nonzero(mask)
+    percent_outliers = np.zeros(data_diff.shape[-1], dtype=np.float32)
+    for k in range(data_diff.shape[-1]):
+        stats[k]['med'] = (q1[k] + q3[k]) / 2
+        stats[k]['mean'] = R_k[k]
+        stats[k]['q1'] = q1[k]
+        stats[k]['q3'] = q3[k]
+        stats[k]['whislo'] = q1[k] - 1.5 * iqr[k]
+        stats[k]['whishi'] = q3[k] + 1.5 * iqr[k]
+        stats[k]['label'] = k
+
+        # Outliers are observations that fall below Q1 - 1.5(IQR) or
+        # above Q3 + 1.5(IQR) We check if a voxel is an outlier only if
+        # we have a mask, else we are biased.
+        if args.mask is not None:
+            x = data_diff[..., k]
+            outliers = (x < stats[k]['whislo']) | (x > stats[k]['whishi'])
+            percent_outliers[k] = np.sum(outliers) / nb_voxels * 100
+            # What would be our definition of too many outliers?
+            # Maybe mean(all_means)+-3SD?
+            # Or we let people choose based on the figure.
+            # if percent_outliers[k] > ???? :
+            #    logger.warning('   Careful! Diffusion-Weighted Image'
+            #                   ' i=%s has %s %% outlier voxels',
+            #                   k, percent_outliers[k])
+
+    if args.mask is None:
+        fig, axe = plt.subplots(nrows=1, ncols=1, squeeze=False)
+    else:
+        fig, axe = plt.subplots(nrows=1, ncols=2, squeeze=False,
+                                figsize=[10, 4.8])
+        # Default is [6.4, 4.8]. Increasing width to see better.
+
+    medianprops = dict(linestyle='-', linewidth=2.5, color='firebrick')
+    meanprops = dict(linestyle='-', linewidth=2.5, color='green')
+    axe[0, 0].bxp(stats, showmeans=True, meanline=True, showfliers=False,
+                  medianprops=medianprops, meanprops=meanprops)
+    axe[0, 0].set_xlabel('DW image')
+    axe[0, 0].set_ylabel('Residuals per DWI volume. Red is median,\n'
+                         'green is mean. Whiskers are 1.5*interquartile')
+    axe[0, 0].set_title('Residuals')
+    axe[0, 0].set_xticks(range(0, q1.shape[0], 5))
+    axe[0, 0].set_xticklabels(range(0, q1.shape[0], 5))
+
+    if args.mask is not None:
+        axe[0, 1].plot(range(data_diff.shape[-1]), percent_outliers)
+        axe[0, 1].set_xticks(range(0, q1.shape[0], 5))
+        axe[0, 1].set_xticklabels(range(0, q1.shape[0], 5))
+        axe[0, 1].set_xlabel('DW image')
+        axe[0, 1].set_ylabel('Percentage of outlier voxels')
+        axe[0, 1].set_title('Outliers')
+    plt.savefig(residual_basename + '_residuals_stats.png')
+
+
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
+    # Verifications
     if not args.not_all:
         args.fa = args.fa or 'fa.nii.gz'
         args.ga = args.ga or 'ga.nii.gz'
         args.rgb = args.rgb or 'rgb.nii.gz'
         args.md = args.md or 'md.nii.gz'
         args.ad = args.ad or 'ad.nii.gz'
         args.rd = args.rd or 'rd.nii.gz'
@@ -171,298 +247,197 @@
     if args.not_all and not any(outputs):
         parser.error('When using --not_all, you need to specify at least ' +
                      'one metric to output.')
 
     assert_inputs_exist(
         parser, [args.in_dwi, args.in_bval, args.in_bvec], args.mask)
     assert_outputs_exist(parser, args, outputs)
+    assert_headers_compatible(parser, args.in_dwi, args.mask)
 
+    # Loading
     img = nib.load(args.in_dwi)
     data = img.get_fdata(dtype=np.float32)
     affine = img.affine
-    if args.mask is None:
-        mask = None
-    else:
-        mask = get_data_as_mask(nib.load(args.mask), dtype=bool)
+    mask = get_data_as_mask(nib.load(args.mask),
+                            dtype=bool) if args.mask else None
 
-    # Validate bvals and bvecs
     logging.info('Tensor estimation with the {} method...'.format(args.method))
     bvals, bvecs = read_bvals_bvecs(args.in_bval, args.in_bvec)
 
     if not is_normalized_bvecs(bvecs):
         logging.warning('Your b-vectors do not seem normalized...')
         bvecs = normalize_bvecs(bvecs)
 
-    b0_thr = check_b0_threshold(
-        args.force_b0_threshold, bvals.min(), bvals.min())
-    gtab = gradient_table(bvals, bvecs, b0_threshold=b0_thr)
+    # How the b0_threshold is used: gtab.b0s_mask is used
+    # 1) In TensorModel in Dipy:
+    #       - The S0 images used as any other image in the design matrix and in
+    #         method .fit().
+    # 2) But we do use this information below, with options p_i_signal,
+    #    pulsation and residual.
+    args.b0_threshold = check_b0_threshold(bvals.min(),
+                                           b0_thr=args.b0_threshold,
+                                           skip_b0_check=args.skip_b0_check)
+    gtab = gradient_table(bvals, bvecs, b0_threshold=args.b0_threshold)
+
+    # Processing
 
     # Get tensors
     if args.method == 'restore':
         sigma = ne.estimate_sigma(data)
         tenmodel = TensorModel(gtab, fit_method=args.method, sigma=sigma,
-                               min_signal=_get_min_nonzero_signal(data))
+                               min_signal=np.min(data[data > 0]))
     else:
         tenmodel = TensorModel(gtab, fit_method=args.method,
-                               min_signal=_get_min_nonzero_signal(data))
+                               min_signal=np.min(data[data > 0]))
 
     tenfit = tenmodel.fit(data, mask)
 
-    FA = fractional_anisotropy(tenfit.evals)
-    FA[np.isnan(FA)] = 0
-    FA = np.clip(FA, 0, 1)
-
+    # Save all metrics.
     if args.tensor:
         # Get the Tensor values
         # Format them for visualization in various software.
         tensor_vals = lower_triangular(tenfit.quadratic_form)
         tensor_vals_reordered = convert_tensor_from_dipy_format(
             tensor_vals, final_format=args.tensor_format)
 
         fiber_tensors = nib.Nifti1Image(
             tensor_vals_reordered.astype(np.float32), affine)
         nib.save(fiber_tensors, args.tensor)
 
         del tensor_vals, fiber_tensors, tensor_vals_reordered
 
-    if args.fa:
-        fa_img = nib.Nifti1Image(FA.astype(np.float32), affine)
-        nib.save(fa_img, args.fa)
-
-        del fa_img
+    if args.fa or args.RGB:
+        FA = fractional_anisotropy(tenfit.evals)
+        FA[np.isnan(FA)] = 0
+        FA = np.clip(FA, 0, 1)
+        if args.fa:
+            nib.save(nib.Nifti1Image(FA.astype(np.float32), affine), args.fa)
+
+        if args.rgb:
+            RGB = color_fa(FA, tenfit.evecs)
+            nib.save(nib.Nifti1Image(np.array(255 * RGB, 'uint8'), affine),
+                     args.rgb)
 
     if args.ga:
         GA = geodesic_anisotropy(tenfit.evals)
         GA[np.isnan(GA)] = 0
-
-        ga_img = nib.Nifti1Image(GA.astype(np.float32), affine)
-        nib.save(ga_img, args.ga)
-
-        del GA, ga_img
-
-    if args.rgb:
-        RGB = color_fa(FA, tenfit.evecs)
-        rgb_img = nib.Nifti1Image(np.array(255 * RGB, 'uint8'), affine)
-        nib.save(rgb_img, args.rgb)
-
-        del RGB, rgb_img
+        nib.save(nib.Nifti1Image(GA.astype(np.float32), affine), args.ga)
 
     if args.md:
         MD = mean_diffusivity(tenfit.evals)
-        md_img = nib.Nifti1Image(MD.astype(np.float32), affine)
-        nib.save(md_img, args.md)
-
-        del MD, md_img
+        nib.save(nib.Nifti1Image(MD.astype(np.float32), affine), args.md)
 
     if args.ad:
         AD = axial_diffusivity(tenfit.evals)
-        ad_img = nib.Nifti1Image(AD.astype(np.float32), affine)
-        nib.save(ad_img, args.ad)
-
-        del AD, ad_img
+        nib.save(nib.Nifti1Image(AD.astype(np.float32), affine), args.ad)
 
     if args.rd:
         RD = radial_diffusivity(tenfit.evals)
-        rd_img = nib.Nifti1Image(RD.astype(np.float32), affine)
-        nib.save(rd_img, args.rd)
-
-        del RD, rd_img
+        nib.save(nib.Nifti1Image(RD.astype(np.float32), affine), args.rd)
 
     if args.mode:
         # Compute tensor mode
         inter_mode = dipy_mode(tenfit.quadratic_form)
 
         # Since the mode computation can generate NANs when not masked,
         # we need to remove them.
         non_nan_indices = np.isfinite(inter_mode)
         mode = np.zeros(inter_mode.shape)
         mode[non_nan_indices] = inter_mode[non_nan_indices]
-
-        mode_img = nib.Nifti1Image(mode.astype(np.float32), affine)
-        nib.save(mode_img, args.mode)
-
-        del inter_mode, mode_img, mode
+        nib.save(nib.Nifti1Image(mode.astype(np.float32), affine), args.mode)
 
     if args.norm:
         NORM = norm(tenfit.quadratic_form)
-        norm_img = nib.Nifti1Image(NORM.astype(np.float32), affine)
-        nib.save(norm_img, args.norm)
-
-        del NORM, norm_img
+        nib.save(nib.Nifti1Image(NORM.astype(np.float32), affine), args.norm)
 
     if args.evecs:
         evecs = tenfit.evecs.astype(np.float32)
-        evecs_img = nib.Nifti1Image(evecs, affine)
-        nib.save(evecs_img, args.evecs)
+        nib.save(nib.Nifti1Image(evecs, affine), args.evecs)
 
         # save individual e-vectors also
         for i in range(3):
-            e_img = nib.Nifti1Image(evecs[..., i], affine)
-            nib.save(e_img, add_filename_suffix(args.evecs, '_v'+str(i+1)))
-            del e_img
-
-        del evecs, evecs_img
+            nib.save(nib.Nifti1Image(evecs[..., i], affine),
+                     add_filename_suffix(args.evecs, '_v'+str(i+1)))
 
     if args.evals:
         evals = tenfit.evals.astype(np.float32)
-        evals_img = nib.Nifti1Image(evals, affine)
-        nib.save(evals_img, args.evals)
+        nib.save(nib.Nifti1Image(evals, affine), args.evals)
 
         # save individual e-values also
         for i in range(3):
-            e_img = nib.Nifti1Image(evals[..., i], affine)
-            nib.save(e_img, add_filename_suffix(args.evals, '_e' + str(i+1)))
-            del e_img
-
-        del evals, evals_img
+            nib.save(nib.Nifti1Image(evals[..., i], affine),
+                     add_filename_suffix(args.evals, '_e' + str(i+1)))
 
     if args.p_i_signal:
         S0 = np.mean(data[..., gtab.b0s_mask], axis=-1, keepdims=True)
         DWI = data[..., ~gtab.b0s_mask]
         pis_mask = np.max(S0 < DWI, axis=-1)
 
         if args.mask is not None:
             pis_mask *= mask
 
-        pis_img = nib.Nifti1Image(pis_mask.astype(np.int16), affine)
-        nib.save(pis_img, args.p_i_signal)
-
-        del pis_img, S0, DWI
+        nib.save(nib.Nifti1Image(pis_mask.astype(np.int16), affine),
+                 args.p_i_signal)
 
     if args.pulsation:
         STD = np.std(data[..., ~gtab.b0s_mask], axis=-1)
 
         if args.mask is not None:
             STD *= mask
 
-        std_img = nib.Nifti1Image(STD.astype(np.float32), affine)
-        nib.save(std_img, add_filename_suffix(args.pulsation, '_std_dwi'))
+        nib.save(nib.Nifti1Image(STD.astype(np.float32), affine),
+                 add_filename_suffix(args.pulsation, '_std_dwi'))
 
         if np.sum(gtab.b0s_mask) <= 1:
             logger.info('Not enough b=0 images to output standard '
                         'deviation map')
         else:
             if len(np.where(gtab.b0s_mask)) == 2:
                 logger.info('Only two b=0 images. Be careful with the '
                             'interpretation of this std map')
 
             STD = np.std(data[..., gtab.b0s_mask], axis=-1)
 
             if args.mask is not None:
                 STD *= mask
 
-            std_img = nib.Nifti1Image(STD.astype(np.float32), affine)
-            nib.save(std_img, add_filename_suffix(args.pulsation, '_std_b0'))
-
-        del STD, std_img
+            nib.save(nib.Nifti1Image(STD.astype(np.float32), affine),
+                     add_filename_suffix(args.pulsation, '_std_b0'))
 
     if args.residual:
         # Mean residual image
         S0 = np.mean(data[..., gtab.b0s_mask], axis=-1)
-        data_diff = np.zeros(data.shape, dtype=np.float32)
+        tenfit2_predict = np.zeros(data.shape, dtype=np.float32)
 
         for i in range(data.shape[0]):
             if args.mask is not None:
                 tenfit2 = tenmodel.fit(data[i, :, :, :], mask[i, :, :])
             else:
                 tenfit2 = tenmodel.fit(data[i, :, :, :])
 
-            data_diff[i, :, :, :] = np.abs(
-                tenfit2.predict(gtab, S0[i, :, :]).astype(
-                    np.float32) - data[i, :, :])
+            tenfit2_predict[i, :, :, :] = tenfit2.predict(gtab, S0[i, :, :])
 
-        R = np.mean(data_diff[..., ~gtab.b0s_mask], axis=-1, dtype=np.float32)
-
-        if args.mask is not None:
-            R *= mask
-
-        R_img = nib.Nifti1Image(R.astype(np.float32), affine)
-        nib.save(R_img, args.residual)
-
-        del R, R_img, S0
+        R, data_diff = compute_residuals(
+            predicted_data=tenfit2_predict.astype(np.float32),
+            real_data=data, b0s_mask=gtab.b0s_mask, mask=mask)
+        nib.save(nib.Nifti1Image(R.astype(np.float32), affine), args.residual)
 
         # Each volume's residual statistics
-        if args.mask is None:
-            logger.info("Outlier detection will not be performed, since no "
-                        "mask was provided.")
-        stats = [dict.fromkeys(['label', 'mean', 'iqr', 'cilo', 'cihi', 'whishi',
-                                'whislo', 'fliers', 'q1', 'med', 'q3'], [])
-                 for i in range(data.shape[-1])]  # stats with format for boxplots
-        # Note that stats will be computed manually and plotted using bxp
-        # but could be computed using stats = cbook.boxplot_stats
-        # or pyplot.boxplot(x)
-        R_k = np.zeros(data.shape[-1], dtype=np.float32)    # mean residual per DWI
-        std = np.zeros(data.shape[-1], dtype=np.float32)  # std residual per DWI
-        q1 = np.zeros(data.shape[-1], dtype=np.float32)   # first quartile per DWI
-        q3 = np.zeros(data.shape[-1], dtype=np.float32)   # third quartile per DWI
-        iqr = np.zeros(data.shape[-1], dtype=np.float32)  # interquartile per DWI
-        percent_outliers = np.zeros(data.shape[-1], dtype=np.float32)
-        nb_voxels = np.count_nonzero(mask)
-        for k in range(data.shape[-1]):
-            x = data_diff[..., k]
-            R_k[k] = np.mean(x)
-            std[k] = np.std(x)
-            q3[k], q1[k] = np.percentile(x, [75, 25])
-            iqr[k] = q3[k] - q1[k]
-            stats[k]['med'] = (q1[k] + q3[k]) / 2
-            stats[k]['mean'] = R_k[k]
-            stats[k]['q1'] = q1[k]
-            stats[k]['q3'] = q3[k]
-            stats[k]['whislo'] = q1[k] - 1.5 * iqr[k]
-            stats[k]['whishi'] = q3[k] + 1.5 * iqr[k]
-            stats[k]['label'] = k
-
-            # Outliers are observations that fall below Q1 - 1.5(IQR) or
-            # above Q3 + 1.5(IQR) We check if a voxel is an outlier only if
-            # we have a mask, else we are biased.
-            if args.mask is not None:
-                outliers = (x < stats[k]['whislo']) | (x > stats[k]['whishi'])
-                percent_outliers[k] = np.sum(outliers)/nb_voxels*100
-                # What would be our definition of too many outliers?
-                # Maybe mean(all_means)+-3SD?
-                # Or we let people choose based on the figure.
-                # if percent_outliers[k] > ???? :
-                #    logger.warning('   Careful! Diffusion-Weighted Image'
-                #                   ' i=%s has %s %% outlier voxels',
-                #                   k, percent_outliers[k])
+        R_k, q1, q3, iqr, std = compute_residuals_statistics(data_diff)
 
         # Saving all statistics as npy values
         residual_basename, _ = split_name_with_nii(args.residual)
         res_stats_basename = residual_basename + ".npy"
         np.save(add_filename_suffix(
             res_stats_basename, "_mean_residuals"), R_k)
         np.save(add_filename_suffix(res_stats_basename, "_q1_residuals"), q1)
         np.save(add_filename_suffix(res_stats_basename, "_q3_residuals"), q3)
         np.save(add_filename_suffix(res_stats_basename, "_iqr_residuals"), iqr)
         np.save(add_filename_suffix(res_stats_basename, "_std_residuals"), std)
 
-        # Showing results in graph
-        if args.mask is None:
-            fig, axe = plt.subplots(nrows=1, ncols=1, squeeze=False)
-        else:
-            fig, axe = plt.subplots(nrows=1, ncols=2, squeeze=False,
-                                    figsize=[10, 4.8])
-            # Default is [6.4, 4.8]. Increasing width to see better.
-
-        medianprops = dict(linestyle='-', linewidth=2.5, color='firebrick')
-        meanprops = dict(linestyle='-', linewidth=2.5, color='green')
-        axe[0, 0].bxp(stats, showmeans=True, meanline=True, showfliers=False,
-                      medianprops=medianprops, meanprops=meanprops)
-        axe[0, 0].set_xlabel('DW image')
-        axe[0, 0].set_ylabel('Residuals per DWI volume. Red is median,\n'
-                             'green is mean. Whiskers are 1.5*interquartile')
-        axe[0, 0].set_title('Residuals')
-        axe[0, 0].set_xticks(range(0, q1.shape[0], 5))
-        axe[0, 0].set_xticklabels(range(0, q1.shape[0], 5))
-
-        if args.mask is not None:
-            axe[0, 1].plot(range(data.shape[-1]), percent_outliers)
-            axe[0, 1].set_xticks(range(0, q1.shape[0], 5))
-            axe[0, 1].set_xticklabels(range(0, q1.shape[0], 5))
-            axe[0, 1].set_xlabel('DW image')
-            axe[0, 1].set_ylabel('Percentage of outlier voxels')
-            axe[0, 1].set_title('Outliers')
-        plt.savefig(residual_basename + '_residuals_stats.png')
+        # Plotting and saving figure
+        _plot_residuals(args, data_diff, mask, R_k, q1, q3, iqr,
+                        residual_basename)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_endpoints_map.py` & `scilpy-2.0.0/scripts/scil_bundle_compute_endpoints_map.py`

 * *Files 15% similar despite different names*

```diff
@@ -6,32 +6,35 @@
 the number of streamlines that start or end in each voxel.
 
 The idea is to estimate the cortical area affected by the bundle (assuming
 streamlines start/end in the cortex).
 
 Note: If the streamlines are not ordered the head/tail are random and not
 really two coherent groups. Use the following script to order streamlines:
-scil_uniformize_streamlines_endpoints.py
+scil_tractogram_uniformize_endpoints.py
+
+Formerly: scil_compute_endpoints_map.py
 """
 
 import argparse
 import logging
 import json
 import os
 
 import nibabel as nib
 import numpy as np
 
 from scilpy.io.streamlines import load_tractogram_with_reference
 from scilpy.io.utils import (add_json_args,
+                             add_verbose_arg,
                              add_overwrite_arg,
                              add_reference_arg,
                              assert_inputs_exist,
                              assert_outputs_exist)
-from scilpy.tractanalysis.reproducibility_measures import \
+from scilpy.tractograms.streamline_and_mask_operations import \
     get_head_tail_density_maps
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__,
         formatter_class=argparse.RawTextHelpFormatter)
@@ -43,49 +46,54 @@
     p.add_argument('endpoints_map_tail',
                    help='Output endpoints map tail filename.')
     p.add_argument('--swap', action='store_true',
                    help='Swap head<->tail convention. '
                         'Can be useful when the reference is not in RAS.')
     p.add_argument('--binary', action='store_true',
                    help="Save outputs as a binary mask instead of a heat map.")
+    p.add_argument('--nb_points', type=int, default=1,
+                   help="Number of points to consider at the extremities"
+                        " of the streamlines. [%(default)s]")
 
     add_json_args(p)
     add_reference_arg(p)
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
     swap = args.swap
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, args.in_bundle, args.reference)
     assert_outputs_exist(parser, args, [args.endpoints_map_head,
                                         args.endpoints_map_tail])
 
     sft = load_tractogram_with_reference(parser, args, args.in_bundle)
     sft.to_vox()
     sft.to_corner()
     if len(sft.streamlines) == 0:
         logging.warning('Empty bundle file {}. Skipping'.format(args.bundle))
         return
 
-    transfo, dim, _, _ = sft.space_attributes
+    transfo, *_ = sft.space_attributes
 
     head_name = args.endpoints_map_head
     tail_name = args.endpoints_map_tail
 
     if swap:
         head_name = args.endpoints_map_tail
         tail_name = args.endpoints_map_head
 
     endpoints_map_head, endpoints_map_tail = \
-        get_head_tail_density_maps(sft.streamlines, dim)
+        get_head_tail_density_maps(sft, args.nb_points)
 
     if args.binary:
         endpoints_map_head = (endpoints_map_head > 0).astype(np.int16)
         endpoints_map_tail = (endpoints_map_tail > 0).astype(np.int16)
 
     nib.save(nib.Nifti1Image(endpoints_map_head, transfo), head_name)
     nib.save(nib.Nifti1Image(endpoints_map_tail, transfo), tail_name)
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_fodf_metrics.py` & `scilpy-2.0.0/scripts/scil_fodf_metrics.py`

 * *Files 18% similar despite different names*

```diff
@@ -10,79 +10,96 @@
 
 NuFO is the the number of maxima of the fODF with an ABSOLUTE amplitude above
 the threshold set using --at, AND an amplitude above the RELATIVE threshold
 set using --rt.
 
 The --at argument should be set to a value which is 1.5 times the maximal
 value of the fODF in the ventricules. This can be obtained with the
-compute_fodf_max_in_ventricules.py script.
+scil_fodf_max_in_ventricles.py script.
+
+If the --abs_peaks_and_values argument is set, the peaks are all normalized
+and the peak_values are equal to the actual fODF amplitude of the peaks. By
+default, the script max-normalizes the peak_values for each voxel and
+multiplies the peaks by peak_values.
 
 By default, will output all possible files, using default names. Specific names
 can be specified using the file flags specified in the "File flags" section.
 
 If --not_all is set, only the files specified explicitly by the flags will be
 output.
 
 See [Raffelt et al. NeuroImage 2012] and [Dell'Acqua et al HBM 2013] for the
 definitions.
+
+Formerly: scil_compute_fodf_metrics.py
 """
 
 import argparse
+import logging
 import numpy as np
 import nibabel as nib
 
 from dipy.data import get_sphere
 from dipy.direction.peaks import reshape_peaks_for_visualization
 
 from scilpy.io.image import get_data_as_mask
 from scilpy.io.utils import (add_overwrite_arg, add_sh_basis_args,
-                             add_processes_arg,
-                             assert_inputs_exist, assert_outputs_exist)
-from scilpy.reconst.multi_processes import peaks_from_sh, maps_from_sh
+                             add_processes_arg, add_verbose_arg,
+                             assert_inputs_exist, assert_outputs_exist,
+                             parse_sh_basis_arg, assert_headers_compatible)
+from scilpy.reconst.sh import peaks_from_sh, maps_from_sh
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__, formatter_class=argparse.RawTextHelpFormatter)
     p.add_argument('in_fODF',
                    help='Path of the fODF volume in spherical harmonics (SH).')
 
     p.add_argument('--sphere', metavar='string', default='repulsion724',
                    choices=['repulsion100', 'repulsion724'],
                    help='Discrete sphere to use in the processing '
                         '[%(default)s].')
     p.add_argument('--mask', metavar='',
-                   help='Path to a binary mask. Only the data inside the mask\n'
-                        'will beused for computations and reconstruction '
+                   help='Path to a binary mask. Only the data inside the mask'
+                        '\nwill beused for computations and reconstruction '
                         '[%(default)s].')
     p.add_argument('--at', dest='a_threshold', type=float, default='0.0',
                    help='Absolute threshold on fODF amplitude. This '
                         'value should be set to\napproximately 1.5 to 2 times '
                         'the maximum fODF amplitude in isotropic voxels\n'
-                        '(ie. ventricles).\nUse compute_fodf_max_in_ventricles.py '
-                        'to find the maximal value.\n'
+                        '(ie. ventricles).\nUse scil_fodf_max_in_ventricles.py'
+                        ' to find the maximal value.\n'
                         'See [Dell\'Acqua et al HBM 2013] [%(default)s].')
     p.add_argument('--rt', dest='r_threshold', type=float, default='0.1',
                    help='Relative threshold on fODF amplitude in percentage  '
                         '[%(default)s].')
+    p.add_argument('--abs_peaks_and_values', action='store_true',
+                   help='If set, the peak_values are not max-normalized for '
+                        'each voxel, \nbut rather they keep the actual fODF '
+                        'amplitude of the peaks. \nAlso, the peaks are '
+                        'given as unit directions instead of being '
+                        'proportional to peak_values. [%(default)s]')
     add_sh_basis_args(p)
-    add_overwrite_arg(p)
+    add_verbose_arg(p)
     add_processes_arg(p)
+    add_overwrite_arg(p)
     p.add_argument('--not_all', action='store_true',
                    help='If set, only saves the files specified using the '
                         'file flags [%(default)s].')
 
     g = p.add_argument_group(title='File flags')
     g.add_argument('--afd_max', metavar='file', default='',
                    help='Output filename for the AFD_max map.')
     g.add_argument('--afd_total', metavar='file', default='',
-                   help='Output filename for the AFD_total map (SH coeff = 0).')
+                   help='Output filename for the AFD_total map'
+                   '(SH coeff = 0).')
     g.add_argument('--afd_sum', metavar='file', default='',
-                   help='Output filename for the sum of all peak contributions\n'
-                        '(sum of fODF lobes on the sphere).')
+                   help='Output filename for the sum of all peak contributions'
+                        '\n(sum of fODF lobes on the sphere).')
     g.add_argument('--nufo', metavar='file', default='',
                    help='Output filename for the NuFO map.')
     g.add_argument('--rgb', metavar='file', default='',
                    help='Output filename for the RGB map.')
     g.add_argument('--peaks', metavar='file', default='',
                    help='Output filename for the extracted peaks.')
     g.add_argument('--peak_values', metavar='file', default='',
@@ -92,15 +109,17 @@
                         'the sphere.')
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
+    # Verifications
     if not args.not_all:
         args.afd_max = args.afd_max or 'afd_max.nii.gz'
         args.afd_total = args.afd_total or 'afd_total_sh0.nii.gz'
         args.afd_sum = args.afd_sum or 'afd_sum.nii.gz'
         args.nufo = args.nufo or 'nufo.nii.gz'
         args.rgb = args.rgb or 'rgb.nii.gz'
         args.peaks = args.peaks or 'peaks.nii.gz'
@@ -110,80 +129,83 @@
     arglist = [args.afd_max, args.afd_total, args.afd_sum, args.nufo,
                args.rgb, args.peaks, args.peak_values,
                args.peak_indices]
     if args.not_all and not any(arglist):
         parser.error('When using --not_all, you need to specify at least '
                      'one file to output.')
 
-    assert_inputs_exist(parser, args.in_fODF)
+    assert_inputs_exist(parser, args.in_fODF, args.mask)
     assert_outputs_exist(parser, args, arglist)
+    assert_headers_compatible(parser, args.in_fODF, args.mask)
 
+    # Loading
     vol = nib.load(args.in_fODF)
     data = vol.get_fdata(dtype=np.float32)
     affine = vol.affine
-
-    if args.mask is None:
-        mask = None
-    else:
-        mask = get_data_as_mask(nib.load(args.mask), dtype=bool)
-        if mask.shape != data.shape[:-1]:
-            raise ValueError("Mask is not the same shape as data.")
+    mask = get_data_as_mask(nib.load(args.mask),
+                            dtype=bool) if args.mask else None
 
     sphere = get_sphere(args.sphere)
+    sh_basis, is_legacy = parse_sh_basis_arg(args)
 
     # Computing peaks
     peak_dirs, peak_values, \
         peak_indices = peaks_from_sh(data,
                                      sphere,
                                      mask=mask,
                                      relative_peak_threshold=args.r_threshold,
                                      absolute_threshold=args.a_threshold,
                                      min_separation_angle=25,
                                      normalize_peaks=False,
-                                     sh_basis_type=args.sh_basis,
+                                     sh_basis_type=sh_basis,
+                                     is_legacy=is_legacy,
                                      nbr_processes=args.nbr_processes)
 
     # Computing maps
     if args.nufo or args.afd_max or args.afd_total or args.afd_sum or args.rgb:
         nufo_map, afd_max, afd_sum, rgb_map, \
             _, _ = maps_from_sh(data, peak_dirs, peak_values, peak_indices,
                                 sphere, nbr_processes=args.nbr_processes)
 
-    # Save result
-    if args.nufo:
-        nib.save(nib.Nifti1Image(nufo_map.astype(np.float32),
-                                 affine), args.nufo)
-
-    if args.afd_max:
-        nib.save(nib.Nifti1Image(afd_max.astype(np.float32),
-                                 affine), args.afd_max)
-
-    if args.afd_total:
-        # this is the analytical afd total
-        afd_tot = data[:, :, :, 0]
-        nib.save(nib.Nifti1Image(afd_tot.astype(np.float32),
-                                 affine), args.afd_total)
-
-    if args.afd_sum:
-        nib.save(nib.Nifti1Image(afd_sum.astype(np.float32),
-                                 affine), args.afd_sum)
-
-    if args.rgb:
-        nib.save(nib.Nifti1Image(rgb_map.astype('uint8'), affine), args.rgb)
+        # Save result
+        if args.nufo:
+            nib.save(nib.Nifti1Image(nufo_map.astype(np.float32), affine),
+                     args.nufo)
+
+        if args.afd_max:
+            nib.save(nib.Nifti1Image(afd_max.astype(np.float32), affine),
+                     args.afd_max)
+
+        if args.afd_total:
+            # this is the analytical afd total
+            afd_tot = data[:, :, :, 0]
+            nib.save(nib.Nifti1Image(afd_tot.astype(np.float32), affine),
+                     args.afd_total)
+
+        if args.afd_sum:
+            nib.save(nib.Nifti1Image(afd_sum.astype(np.float32), affine),
+                     args.afd_sum)
+
+        if args.rgb:
+            nib.save(nib.Nifti1Image(rgb_map.astype('uint8'), affine),
+                     args.rgb)
 
     if args.peaks or args.peak_values:
-        peak_values = np.divide(peak_values, peak_values[..., 0, None],
-                                out=np.zeros_like(peak_values),
-                                where=peak_values[..., 0, None] != 0)
-        peak_dirs[...] *= peak_values[..., :, None]
+        if not args.abs_peaks_and_values:
+            peak_values = np.divide(peak_values, peak_values[..., 0, None],
+                                    out=np.zeros_like(peak_values),
+                                    where=peak_values[..., 0, None] != 0)
+            peak_dirs[...] *= peak_values[..., :, None]
         if args.peaks:
-            nib.save(nib.Nifti1Image(reshape_peaks_for_visualization(peak_dirs),
-                                     affine), args.peaks)
+            nib.save(nib.Nifti1Image(
+                reshape_peaks_for_visualization(peak_dirs),
+                affine), args.peaks)
         if args.peak_values:
-            nib.save(nib.Nifti1Image(peak_values, vol.affine), args.peak_values)
+            nib.save(nib.Nifti1Image(peak_values, vol.affine),
+                     args.peak_values)
 
     if args.peak_indices:
         nib.save(nib.Nifti1Image(peak_indices, vol.affine), args.peak_indices)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_freewater.py` & `scilpy-2.0.0/scripts/scil_freewater_maps.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,33 +1,37 @@
 #! /usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
 Compute Free Water maps [1] using AMICO.
 This script supports both single and multi-shell data.
+
+Formerly: scil_compute_freewater.py
 """
 
 import argparse
 from contextlib import redirect_stdout
 import io
 import logging
 import os
 import sys
 import tempfile
 
 import amico
 from dipy.io.gradients import read_bvals_bvecs
 import numpy as np
 
+from scilpy.io.gradients import fsl2mrtrix
 from scilpy.io.utils import (add_overwrite_arg,
                              add_processes_arg,
                              add_verbose_arg,
                              assert_inputs_exist,
-                             assert_output_dirs_exist_and_empty)
-from scilpy.utils.bvec_bval_tools import fsl2mrtrix, identify_shells
+                             assert_output_dirs_exist_and_empty,
+                             redirect_stdout_c)
+from scilpy.gradients.bvec_bval_tools import identify_shells
 
 
 EPILOG = """
 Reference:
     [1] Pasternak 0, Sochen N, Gur Y, Intrator N, Assaf Y.
         Free water elimination and mapping from diffusion mri.
         Magn Reson Med. 62 (3) (2009) 717-730.
@@ -81,67 +85,56 @@
     g2.add_argument('--compute_only', action='store_true',
                     help='Compute kernels only, --save_kernels must be used.')
 
     p.add_argument('--mouse', action='store_true',
                    help='If set, use mouse fitting profile.')
 
     add_processes_arg(p)
-    add_overwrite_arg(p)
     add_verbose_arg(p)
+    add_overwrite_arg(p)
 
     return p
 
 
-def redirect_stdout_c():
-    sys.stdout.flush()
-    newstdout = os.dup(1)
-    devnull = os.open(os.devnull, os.O_WRONLY)
-    os.dup2(devnull, 1)
-    os.close(devnull)
-    sys.stdout = os.fdopen(newstdout, 'w')
-
-
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    # COMMIT has some c-level stdout and non-logging print that cannot
+    # be easily stopped. Manual redirection of all printed output
+    if args.verbose == "WARNING":
+        f = io.StringIO()
+        redirected_stdout = redirect_stdout(f)
+        redirect_stdout_c() 
+    else:
+        logging.getLogger().setLevel(logging.getLevelName(args.verbose))
+        redirected_stdout = redirect_stdout(sys.stdout)
 
+    # Verifications
     if args.compute_only and not args.save_kernels:
         parser.error('--compute_only must be used with --save_kernels.')
 
     assert_inputs_exist(parser, [args.in_dwi, args.in_bval, args.in_bvec],
                         args.mask)
 
-    assert_output_dirs_exist_and_empty(parser, args,
-                                       args.out_dir,
+    assert_output_dirs_exist_and_empty(parser, args, args.out_dir,
                                        optional=args.save_kernels)
 
-    # COMMIT has some c-level stdout and non-logging print that cannot
-    # be easily stopped. Manual redirection of all printed output
-    if args.verbose:
-        logging.getLogger().setLevel(logging.DEBUG)
-        redirected_stdout = redirect_stdout(sys.stdout)
-    else:
-        f = io.StringIO()
-        redirected_stdout = redirect_stdout(f)
-        redirect_stdout_c()
-
-    # Generage a scheme file from the bvals and bvecs files
+    # Generate a scheme file from the bvals and bvecs files
     tmp_dir = tempfile.TemporaryDirectory()
-    tmp_scheme_filename = os.path.join(tmp_dir.name, 'gradients.scheme')
+    tmp_scheme_filename = os.path.join(tmp_dir.name, 'gradients.b')
     tmp_bval_filename = os.path.join(tmp_dir.name, 'bval')
     bvals, _ = read_bvals_bvecs(args.in_bval, args.in_bvec)
-    shells_centroids, indices_shells = identify_shells(bvals,
-                                                       args.b_thr,
-                                                       roundCentroids=True)
+    shells_centroids, indices_shells = identify_shells(bvals, args.b_thr,
+                                                       round_centroids=True)
     np.savetxt(tmp_bval_filename, shells_centroids[indices_shells],
                newline=' ', fmt='%i')
     fsl2mrtrix(tmp_bval_filename, args.in_bvec, tmp_scheme_filename)
-    logging.debug('Compute FreeWater with AMICO on {} shells at found at {}.'.format(
-        len(shells_centroids),
-        shells_centroids))
+    logging.info(
+        'Computing FreeWater with AMICO on {} shells at found at {}.'.format(
+            len(shells_centroids), shells_centroids))
 
     with redirected_stdout:
         amico.core.setup()
         # Load the data
         ae = amico.Evaluation('.', '.')
         # Load the data
         ae.load_data(args.in_dwi,
@@ -151,17 +144,15 @@
         # Compute the response functions
         ae.set_model("FreeWater")
         model_type = 'Human'
         if args.mouse:
             model_type = 'Mouse'
 
         ae.model.set(args.para_diff,
-                     np.linspace(args.perp_diff_min,
-                                 args.perp_diff_max,
-                                 10),
+                     np.linspace(args.perp_diff_min, args.perp_diff_max, 10),
                      [args.iso_diff],
                      model_type)
 
         ae.set_solver(lambda1=args.lambda1, lambda2=args.lambda2)
 
         # The kernels are, by default, set to be in the current directory
         # Depending on the choice, manually change the saving location
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_kurtosis_metrics.py` & `scilpy-2.0.0/scripts/scil_dki_metrics.py`

 * *Files 22% similar despite different names*

```diff
@@ -5,17 +5,17 @@
 Script to compute the Diffusion Kurtosis Imaging (DKI) and Mean Signal DKI
 (MSDKI) metrics. DKI is a multi-shell diffusion model. The input DWI needs
 to be multi-shell, i.e. multi-bvalued.
 
 Since the diffusion kurtosis model involves the estimation of a large number
 of parameters and since the non-Gaussian components of the diffusion signal
 are more sensitive to artefacts, you should really denoise your DWI volume
-before using this DKI script (e.g. scil_run_nlmeans.py). Moreover, to remove
-biases due to fiber dispersion, fiber crossings and other mesoscopic properties
-of the underlying tissue, MSDKI does a powder-average of DWI for all
+before using this DKI script (e.g. scil_denoising_nlmeans.py). Moreover, to
+remove biases due to fiber dispersion, fiber crossings and other mesoscopic
+properties of the underlying tissue, MSDKI does a powder-average of DWI for all
 directions, thus removing the orientational dependencies and creating an
 alternative mean kurtosis map.
 
 DKI is also known to be vulnerable to artefacted voxels induced by the
 low radial diffusivities of aligned white matter (CC, CST voxels). Since it is
 very hard to capture non-Gaussian information due to the low decays in radial
 direction, its kurtosis estimates have very low robustness.
@@ -34,81 +34,85 @@
 files flags" section. If --not_all is set, only the metrics specified
 explicitly by the flags will be output.
 
 This script directly comes from the DIPY example gallery and references
 therein.
 [1] examples_built/reconst_dki/#example-reconst-dki
 [2] examples_built/reconst_msdki/#example-reconst-msdki
+
+Formerly: scil_compute_kurtosis_metrics.py
 """
 
 import argparse
 import logging
 
 import nibabel as nib
 import numpy as np
 
 import dipy.reconst.dki as dki
 import dipy.reconst.msdki as msdki
 
 from dipy.io.gradients import read_bvals_bvecs
 from dipy.core.gradients import gradient_table
 
-from scipy.ndimage import gaussian_filter
-
+from scilpy.dwi.operations import compute_residuals
+from scilpy.image.volume_operations import smooth_to_fwhm
 from scilpy.io.image import get_data_as_mask
-from scilpy.io.utils import (add_overwrite_arg, assert_inputs_exist,
-                             assert_outputs_exist, add_force_b0_arg)
-from scilpy.utils.bvec_bval_tools import (normalize_bvecs, is_normalized_bvecs,
-                                          check_b0_threshold,
-                                          identify_shells)
+from scilpy.io.utils import (add_overwrite_arg, add_skip_b0_check_arg,
+                             add_verbose_arg, assert_inputs_exist,
+                             assert_outputs_exist, add_tolerance_arg,
+                             assert_headers_compatible)
+from scilpy.gradients.bvec_bval_tools import (check_b0_threshold,
+                                              is_normalized_bvecs,
+                                              identify_shells,
+                                              normalize_bvecs)
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__,
         formatter_class=argparse.RawTextHelpFormatter)
     p.add_argument('in_dwi',
                    help='Path of the input multi-shell DWI dataset.')
     p.add_argument('in_bval',
                    help='Path of the b-value file, in FSL format.')
     p.add_argument('in_bvec',
                    help='Path of the b-vector file, in FSL format.')
 
     p.add_argument('--mask',
-                   help='Path to a binary mask.' +
-                   '\nOnly data inside the mask will be used '
-                   'for computations and reconstruction. ' +
-                   '\n[Default: None]')
-
-    p.add_argument('--tolerance', '-t',
-                   metavar='INT', type=int, default=20,
-                   help='The tolerated distance between the b-values to '
-                   'extract\nand the actual b-values [Default: %(default)s].')
+                   help='Path to a binary mask.\n'
+                        'Only data inside the mask will be used for '
+                        'computations and reconstruction.\n[Default: None]')
+
+    add_tolerance_arg(p)
+    add_skip_b0_check_arg(p, will_overwrite_with_min=False,
+                          b0_tol_name='--tolerance')
+
     p.add_argument('--min_k', type=float, default=0.0,
-                   help='Minimum kurtosis value in the output maps ' +
-                   '\n(ak, mk, rk). In theory, -3/7 is the min kurtosis ' +
-                   '\nlimit for regions that consist of water confined ' +
-                   '\nto spherical pores (see DIPY example and ' +
+                   help='Minimum kurtosis value in the output maps '
+                   '\n(ak, mk, rk). In theory, -3/7 is the min kurtosis '
+                   '\nlimit for regions that consist of water confined '
+                   '\nto spherical pores (see DIPY example and '
                    '\ndocumentation) [Default: %(default)s].')
     p.add_argument('--max_k', type=float, default=3.0,
-                   help='Maximum kurtosis value in the output maps ' +
-                   '\n(ak, mk, rk). In theory, 10 is the max kurtosis' +
-                   '\nlimit for regions that consist of water confined' +
-                   '\nto spherical pores (see DIPY example and ' +
+                   help='Maximum kurtosis value in the output maps '
+                   '\n(ak, mk, rk). In theory, 10 is the max kurtosis'
+                   '\nlimit for regions that consist of water confined'
+                   '\nto spherical pores (see DIPY example and '
                    '\ndocumentation) [Default: %(default)s].')
     p.add_argument('--smooth', type=float, default=2.5,
-                   help='Smooth input DWI with a 3D Gaussian filter with ' +
-                   '\nfull-width-half-max (fwhm). Kurtosis fitting is ' +
-                   '\nsensitive and outliers occur easily. According to' +
-                   '\ntests on HCP, CB_Brain, Penthera3T, this smoothing' +
-                   '\nis thus turned ON by default with fwhm=2.5. ' +
+                   help='Smooth input DWI with a 3D Gaussian filter with '
+                   '\nfull-width-half-max (fwhm). Kurtosis fitting is '
+                   '\nsensitive and outliers occur easily. According to'
+                   '\ntests on HCP, CB_Brain, Penthera3T, this smoothing'
+                   '\nis thus turned ON by default with fwhm=2.5. '
                    '\n[Default: %(default)s].')
     p.add_argument('--not_all', action='store_true',
-                   help='If set, will only save the metrics explicitly ' +
-                   '\nspecified using the other metrics flags. ' +
+                   help='If set, will only save the metrics explicitly '
+                   '\nspecified using the other metrics flags. '
                    '\n[Default: not set].')
 
     g = p.add_argument_group(title='Metrics files flags')
     g.add_argument('--ak', metavar='file', default='',
                    help='Output filename for the axial kurtosis.')
     g.add_argument('--mk', metavar='file', default='',
                    help='Output filename for the mean kurtosis.')
@@ -125,32 +129,33 @@
                    help='Output filename for the axial diffusivity from DKI.')
     g.add_argument('--dki_rd', metavar='file', default='',
                    help='Output filename for the radial diffusivity from DKI.')
 
     g = p.add_argument_group(title='Quality control files flags')
     g.add_argument('--dki_residual', metavar='file', default='',
                    help='Output filename for the map of the residual ' +
-                   'of the tensor fit.')
+                   'of the tensor fit.\n'
+                   'Note. In previous versions, the resulting map was '
+                   'normalized. \nIt is not anymore.')
     g.add_argument('--msd', metavar='file', default='',
                    help='Output filename for the mean signal diffusion ' +
                    '(powder-average).')
 
-    add_force_b0_arg(p)
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def main():
-    logger = logging.getLogger("Compute_DKI_Metrics")
-    logger.setLevel(logging.INFO)
-
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
+    # Verifications
     if not args.not_all:
         args.dki_fa = args.dki_fa or 'dki_fa.nii.gz'
         args.dki_md = args.dki_md or 'dki_md.nii.gz'
         args.dki_ad = args.dki_ad or 'dki_ad.nii.gz'
         args.dki_rd = args.dki_rd or 'dki_rd.nii.gz'
         args.mk = args.mk or 'mk.nii.gz'
         args.rk = args.rk or 'rk.nii.gz'
@@ -166,129 +171,113 @@
     if args.not_all and not any(outputs):
         parser.error('When using --not_all, you need to specify at least ' +
                      'one metric to output.')
 
     assert_inputs_exist(
         parser, [args.in_dwi, args.in_bval, args.in_bvec], args.mask)
     assert_outputs_exist(parser, args, outputs)
+    assert_headers_compatible(parser, args.in_dwi, args.mask)
 
+    # Loading
     img = nib.load(args.in_dwi)
     data = img.get_fdata(dtype=np.float32)
     affine = img.affine
-    if args.mask is None:
-        mask = None
-    else:
-        mask_img = nib.load(args.mask)
-        mask = get_data_as_mask(mask_img, dtype=bool)
+    mask = get_data_as_mask(nib.load(args.mask),
+                            dtype=bool) if args.mask else None
 
-    # Validate bvals and bvecs
     bvals, bvecs = read_bvals_bvecs(args.in_bval, args.in_bvec)
     if not is_normalized_bvecs(bvecs):
         logging.warning('Your b-vectors do not seem normalized...')
         bvecs = normalize_bvecs(bvecs)
 
+    # Note. This script does not currently allow using a separate b0_threshold
+    # for the b0s. Using the tolerance. To change this, we would have to
+    # change our identify_shells method.
+    # Also, usage of gtab.b0s_mask unclear in dipy. An issue has been added in
+    # dipy to ask them to clarify the usage of gtab.b0s_mask. See here:
+    #  https://github.com/dipy/dipy/issues/3015
+    # b0_threshold option in gradient_table probably unused, except below with
+    # option dki_residual.
+    _ = check_b0_threshold(bvals.min(), b0_thr=args.tolerance,
+                           skip_b0_check=args.skip_b0_check)
+    gtab = gradient_table(bvals, bvecs, b0_threshold=args.tolerance)
+
+    # Processing
+
     # Find the volume indices that correspond to the shells to extract.
-    tol = args.tolerance
-    shells, _ = identify_shells(bvals, tol)
+    shells, _ = identify_shells(bvals, args.tolerance)
     if not len(shells) >= 3:
-        parser.error('Data is not multi-shell. You need at least 2 non-zero' +
+        parser.error('Data is not multi-shell. You need at least 2 non-zero'
                      ' b-values')
 
     if (shells > 2500).any():
-        logging.warning('You seem to be using b > 2500 s/mm2 DWI data. ' +
+        logging.warning('You seem to be using b > 2500 s/mm2 DWI data. '
                         'In theory, this is beyond the optimal range for DKI')
 
-    b0_thr = check_b0_threshold(args, bvals.min(), bvals.min())
-    gtab = gradient_table(bvals, bvecs, b0_threshold=b0_thr)
-
-    fwhm = args.smooth
-    if fwhm > 0:
-        # converting fwhm to Gaussian std
-        gauss_std = fwhm / np.sqrt(8 * np.log(2))
-        data_smooth = np.zeros(data.shape)
-        for v in range(data.shape[-1]):
-            data_smooth[..., v] = gaussian_filter(data[..., v],
-                                                  sigma=gauss_std)
-        data = data_smooth
+    # Smooth to FWHM
+    data = smooth_to_fwhm(data, fwhm=args.smooth)
 
     # Compute DKI
     dkimodel = dki.DiffusionKurtosisModel(gtab)
     dkifit = dkimodel.fit(data, mask=mask)
 
     min_k = args.min_k
     max_k = args.max_k
 
+    # Save all metrics.
     if args.dki_fa:
         FA = dkifit.fa
         FA[np.isnan(FA)] = 0
         FA = np.clip(FA, 0, 1)
-
-        fa_img = nib.Nifti1Image(FA.astype(np.float32), affine)
-        nib.save(fa_img, args.dki_fa)
+        nib.save(nib.Nifti1Image(FA.astype(np.float32), affine), args.dki_fa)
 
     if args.dki_md:
         MD = dkifit.md
-        md_img = nib.Nifti1Image(MD.astype(np.float32), affine)
-        nib.save(md_img, args.dki_md)
+        nib.save(nib.Nifti1Image(MD.astype(np.float32), affine), args.dki_md)
 
     if args.dki_ad:
         AD = dkifit.ad
-        ad_img = nib.Nifti1Image(AD.astype(np.float32), affine)
-        nib.save(ad_img, args.dki_ad)
+        nib.save(nib.Nifti1Image(AD.astype(np.float32), affine), args.dki_ad)
 
     if args.dki_rd:
         RD = dkifit.rd
-        rd_img = nib.Nifti1Image(RD.astype(np.float32), affine)
-        nib.save(rd_img, args.dki_rd)
+        nib.save(nib.Nifti1Image(RD.astype(np.float32), affine), args.dki_rd)
 
     if args.mk:
         MK = dkifit.mk(min_k, max_k)
-        mk_img = nib.Nifti1Image(MK.astype(np.float32), affine)
-        nib.save(mk_img, args.mk)
+        nib.save(nib.Nifti1Image(MK.astype(np.float32), affine), args.mk)
 
     if args.ak:
         AK = dkifit.ak(min_k, max_k)
-        ak_img = nib.Nifti1Image(AK.astype(np.float32), affine)
-        nib.save(ak_img, args.ak)
+        nib.save(nib.Nifti1Image(AK.astype(np.float32), affine), args.ak)
 
     if args.rk:
         RK = dkifit.rk(min_k, max_k)
-        rk_img = nib.Nifti1Image(RK.astype(np.float32), affine)
-        nib.save(rk_img, args.rk)
+        nib.save(nib.Nifti1Image(RK.astype(np.float32), affine), args.rk)
 
     if args.msk or args.msd:
         # Compute MSDKI
         msdki_model = msdki.MeanDiffusionKurtosisModel(gtab)
         msdki_fit = msdki_model.fit(data, mask=mask)
 
         if args.msk:
             MSK = msdki_fit.msk
             MSK[np.isnan(MSK)] = 0
             MSK = np.clip(MSK, min_k, max_k)
-
-            msk_img = nib.Nifti1Image(MSK.astype(np.float32), affine)
-            nib.save(msk_img, args.msk)
+            nib.save(nib.Nifti1Image(MSK.astype(np.float32), affine), args.msk)
 
         if args.msd:
             MSD = msdki_fit.msd
-            msd_img = nib.Nifti1Image(MSD.astype(np.float32), affine)
-            nib.save(msd_img, args.msd)
+            nib.save(nib.Nifti1Image(MSD.astype(np.float32), affine), args.msd)
 
     if args.dki_residual:
         S0 = np.mean(data[..., gtab.b0s_mask], axis=-1)
-        data_p = dkifit.predict(gtab, S0)
-        R = np.mean(np.abs(data_p[..., ~gtab.b0s_mask] -
-                           data[..., ~gtab.b0s_mask]), axis=-1)
-
-        norm = np.linalg.norm(R)
-        if norm != 0:
-            R = R / norm
-
-        if args.mask is not None:
-            R *= mask
+        data_p = dkifit.predict(gtab=gtab, S0=S0)
 
-        R_img = nib.Nifti1Image(R.astype(np.float32), affine)
-        nib.save(R_img, args.dki_residual)
+        R, _ = compute_residuals(data_p, data,
+                                 b0s_mask=gtab.b0s_mask, mask=mask)
+        nib.save(nib.Nifti1Image(R.astype(np.float32), affine),
+                 args.dki_residual)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_lobe_specific_fodf_metrics.py` & `scilpy-2.0.0/scripts/scil_bingham_metrics.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,37 +1,38 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 """
 Script to compute fODF lobe-specific metrics derived from a Bingham
-distribution fitting, as described in [1]. Resulting metrics are fiber density
+distribution fit, as described in [1]. Resulting metrics are fiber density
 (FD), fiber spread (FS) and fiber fraction (FF) [2].
 
-The Bingham coefficients volume comes from scil_fit_bingham_to_fodf.py.
+The Bingham coefficients volume comes from scil_fodf_to_bingham.py.
 
 A lobe's FD is the integral of the Bingham function on the sphere. It
 represents the density of fibers going through a given voxel for a given
 fODF lobe (fixel). A lobe's FS is the ratio of its FD on its maximum AFD. It
 is at its minimum for a sharp lobe and at its maximum for a wide lobe. A lobe's
 FF is the ratio of its FD on the total FD in the voxel.
 
 Using 12 threads, the execution takes 10 minutes for FD estimation for a brain
 with 1mm isotropic resolution. Other metrics take less than a second.
+
+Formerly: scil_compute_lobe_specific_fodf_metrics.py
 """
 
 import nibabel as nib
 import time
 import argparse
 import logging
-from scilpy.io.image import get_data_as_mask
 
-from scilpy.io.utils import (add_overwrite_arg,
-                             add_processes_arg,
-                             add_verbose_arg,
-                             assert_inputs_exist,
-                             assert_outputs_exist, validate_nbr_processes)
+from scilpy.io.image import get_data_as_mask
+from scilpy.io.utils import (add_overwrite_arg, add_processes_arg,
+                             add_verbose_arg, assert_inputs_exist,
+                             assert_outputs_exist, validate_nbr_processes,
+                             assert_headers_compatible)
 from scilpy.reconst.bingham import (compute_fiber_density,
                                     compute_fiber_spread,
                                     compute_fiber_fraction)
 
 
 EPILOG = """
 [1] T. W. Riffert, J. Schreiber, A. Anwander, and T. R. Knösche, “Beyond
@@ -46,61 +47,64 @@
 """
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(description=__doc__,
                                 formatter_class=argparse.RawTextHelpFormatter,
                                 epilog=EPILOG)
-    p.add_argument('in_bingham', help='Input Bingham image.')
+    p.add_argument('in_bingham',
+                   help='Input Bingham nifti image.')
 
-    p.add_argument('--out_fd', default='',
+    p.add_argument('--out_fd',
                    help='Path to output fiber density. [fd.nii.gz]')
-    p.add_argument('--out_fs', default='',
+    p.add_argument('--out_fs',
                    help='Path to output fiber spread. [fs.nii.gz]')
-    p.add_argument('--out_ff', default='',
+    p.add_argument('--out_ff',
                    help='Path to fiber fraction file. [ff.nii.gz]')
     p.add_argument('--not_all', action='store_true',
-                   help='Do not compute all metrics.')
+                   help='Do not compute all metrics. Then, please provide '
+                        'the output paths of the files you need.')
     p.add_argument('--mask',
                    help='Optional mask image. Only voxels inside '
                         'the mask are computed.')
 
     p.add_argument('--nbr_integration_steps', type=int, default=50,
                    help='Number of integration steps along the theta axis for'
                         ' fiber density estimation. [%(default)s]')
 
-    add_overwrite_arg(p)
     add_verbose_arg(p)
     add_processes_arg(p)
+    add_overwrite_arg(p)
+
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
-    if args.verbose:
-        logging.getLogger().setLevel(logging.INFO)
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     if not args.not_all:
         args.out_fd = args.out_fd or 'fd.nii.gz'
         args.out_fs = args.out_fs or 'fs.nii.gz'
         args.out_ff = args.out_ff or 'ff.nii.gz'
 
     arglist = [args.out_fd, args.out_fs, args.out_ff]
     if args.not_all and not any(arglist):
         parser.error('At least one output file must be specified.')
 
     outputs = [args.out_fd, args.out_fs, args.out_ff]
     assert_inputs_exist(parser, args.in_bingham, args.mask)
-    assert_outputs_exist(parser, args, outputs)
+    assert_outputs_exist(parser, args, [], optional=outputs)
+    assert_headers_compatible(parser, args.in_bingham, args.mask)
 
     bingham_im = nib.load(args.in_bingham)
     bingham = bingham_im.get_fdata()
-    mask = get_data_as_mask(nib.load(args.mask), dtype=bool)\
-        if args.mask else None
+    mask = get_data_as_mask(nib.load(args.mask),
+                            dtype=bool) if args.mask else None
 
     nbr_processes = validate_nbr_processes(parser, args)
 
     t0 = time.perf_counter()
     logging.info('Computing fiber density.')
     fd = compute_fiber_density(bingham, m=args.nbr_integration_steps,
                                mask=mask, nbr_processes=nbr_processes)
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_local_tracking.py` & `scilpy-2.0.0/scripts/scil_dwi_prepare_eddy_command.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,262 +1,235 @@
-#! /usr/bin/env python3
+#!/usr/bin/env python3
 # -*- coding: utf-8 -*-
-
 """
-Local streamline HARDI tractography.
-The tracking direction is chosen in the aperture cone defined by the
-previous tracking direction and the angular constraint.
-
-Algo 'eudx': the peak from the spherical function (SF) most closely aligned
-to the previous direction.
-Algo 'det': the maxima of the spherical function (SF) the most closely aligned
-to the previous direction.
-Algo 'prob': a direction drawn from the empirical distribution function defined
-from the SF.
-
-NOTE: eudx can be used with pre-computed peaks from fodf as well as
-evecs_v1.nii.gz from scil_compute_dti_metrics.py (experimental).
+Prepare a typical command for eddy and create the necessary files. When using
+multiple acquisitions and/or opposite phase directions, images, b-values and
+b-vectors should be merged together using scil_dwi_concatenate.py. If using
+topup prior to calling this script, images should be concatenated in the same
+order as the b0s used with prepare_topup.
 
-All the input nifti files must be in isotropic resolution.
+Formerly: scil_prepare_eddy_command.py
 """
 
 import argparse
 import logging
+import os
+import subprocess
 
-from dipy.core.sphere import HemiSphere
-from dipy.data import get_sphere
-from dipy.direction import (DeterministicMaximumDirectionGetter,
-                            ProbabilisticDirectionGetter)
-from dipy.direction.peaks import PeaksAndMetrics
-from dipy.io.utils import (get_reference_info,
-                           create_tractogram_header)
-from dipy.tracking.local_tracking import LocalTracking
-from dipy.tracking.stopping_criterion import BinaryStoppingCriterion
-from dipy.tracking.streamlinespeed import length, compress_streamlines
-from dipy.tracking import utils as track_utils
-import nibabel as nib
-from nibabel.streamlines.tractogram import LazyTractogram
+from dipy.io.gradients import read_bvals_bvecs
 import numpy as np
-
-from scilpy.reconst.utils import (find_order_from_nb_coeff,
-                                  get_b_matrix, get_maximas)
-from scilpy.io.image import get_data_as_mask
-from scilpy.io.utils import (add_sphere_arg, add_verbose_arg,
-                             assert_inputs_exist, assert_outputs_exist,
-                             verify_compression_th)
-from scilpy.tracking.tools import get_theta
-from scilpy.tracking.utils import (add_mandatory_options_tracking,
-                                   add_out_options, add_seeding_options,
-                                   add_tracking_options,
-                                   verify_streamline_length_options,
-                                   verify_seed_options)
+from scilpy.io.utils import (add_overwrite_arg, add_verbose_arg,
+                             assert_fsl_options_exist,
+                             assert_inputs_exist)
+from scilpy.preprocessing.distortion_correction import \
+    (create_acqparams, create_index, create_multi_topup_index,
+     create_non_zero_norm_bvecs)
 
 
 def _build_arg_parser():
-    p = argparse.ArgumentParser(
-        description=__doc__,
-        formatter_class=argparse.RawTextHelpFormatter)
-
-    add_mandatory_options_tracking(p)
-
-    track_g = add_tracking_options(p)
-    track_g.add_argument('--algo', default='prob',
-                         choices=['det', 'prob', 'eudx'],
-                         help='Algorithm to use. [%(default)s]')
-    add_sphere_arg(track_g, symmetric_only=True)
+    p = argparse.ArgumentParser(description=__doc__,
+                                formatter_class=argparse.RawTextHelpFormatter)
 
-    add_seeding_options(p)
-    out_g = add_out_options(p)
+    p.add_argument('in_dwi',
+                   help='Input DWI Nifti image. If using multiple '
+                        'acquisition and/or opposite phase directions, please '
+                        'merge in the same order as for prepare_topup using '
+                        'scil_dwi_concatenate.py.')
+
+    p.add_argument('in_bvals',
+                   help='Input b-values file in FSL format.')
+
+    p.add_argument('in_bvecs',
+                   help='Input b-vectors file in FSL format.')
+
+    p.add_argument('in_mask',
+                   help='Binary brain mask.')
+
+    p.add_argument('--n_reverse', type=int, default=0,
+                   help='Number of reverse phase volumes included '
+                        'in the DWI image [%(default)s].')
+
+    p.add_argument('--topup',
+                   help='Topup output name. ' +
+                        'If given, apply topup during eddy.\n' +
+                        'Should be the same as --out_prefix from ' +
+                        'scil_dwi_prepare_topup_command.py.')
+
+    p.add_argument('--topup_params', default='',
+                   help='Parameters file (typically named acqparams) '
+                        'used to run topup.')
+
+    p.add_argument('--eddy_cmd', default='eddy_openmp',
+                   choices=['eddy_openmp', 'eddy_cuda', 'eddy_cuda8.0',
+                            'eddy_cuda9.1', 'eddy_cuda10.2',
+                            'eddy', 'eddy_cpu'],
+                   help='Eddy command [%(default)s].')
+
+    p.add_argument('--b0_thr', type=float, default=20,
+                   help='All b-values with values less than or equal ' +
+                        'to b0_thr are considered\nas b0s i.e. without ' +
+                        'diffusion weighting [%(default)s].')
+
+    p.add_argument('--encoding_direction', default='y',
+                   choices=['x', 'y', 'z'],
+                   help='Acquisition direction, default is AP-PA '
+                        '[%(default)s].')
+
+    p.add_argument('--readout', type=float, default=0.062,
+                   help='Total readout time from the DICOM metadata '
+                        '[%(default)s].')
+
+    p.add_argument('--slice_drop_correction', action='store_true',
+                   help="If set, will activate eddy's outlier correction,\n"
+                        "which includes slice drop correction.")
+
+    p.add_argument('--lsr_resampling', action='store_true',
+                   help='Perform least-square resampling, allowing eddy to '
+                        'combine forward and reverse phase acquisitions for '
+                        'better reconstruction. Only works if directions and '
+                        'b-values are identical in both phase direction.')
+
+    p.add_argument('--out_directory', default='.',
+                   help='Output directory for eddy files [%(default)s].')
+
+    p.add_argument('--out_prefix', default='dwi_eddy_corrected',
+                   help='Prefix of the eddy-corrected DWI [%(default)s].')
+
+    p.add_argument('--out_script', action='store_true',
+                   help='If set, will output a .sh script (eddy.sh).\n' +
+                        'else, will output the lines to the ' +
+                        'terminal [%(default)s].')
+
+    p.add_argument('--fix_seed', action='store_true',
+                   help='If set, will use the fixed seed strategy for eddy.\n'
+                        'Enhances reproducibility.')
+
+    p.add_argument('--eddy_options',  default='',
+                   help='Additional options you want to use to run eddy.\n'
+                        'Add these options using quotes (i.e. "--ol_nstd=6'
+                        ' --mb=4").')
 
-    out_g.add_argument('--seed', type=int,
-                       help='Random number generator seed.')
-
-    log_g = p.add_argument_group('Logging options')
-    add_verbose_arg(log_g)
+    add_verbose_arg(p)
+    add_overwrite_arg(p)
 
     return p
 
 
-def _get_direction_getter(args):
-    odf_data = nib.load(args.in_odf).get_fdata(dtype=np.float32)
-    sphere = HemiSphere.from_sphere(get_sphere(args.sphere))
-    theta = get_theta(args.theta, args.algo)
-
-    non_zeros_count = np.count_nonzero(np.sum(odf_data, axis=-1))
-    non_first_val_count = np.count_nonzero(np.argmax(odf_data, axis=-1))
-
-    if args.algo in ['det', 'prob']:
-        if non_first_val_count / non_zeros_count > 0.5:
-            logging.warning('Input detected as peaks. Input should be'
-                            'fodf for det/prob, verify input just in case.')
-        if args.algo == 'det':
-            dg_class = DeterministicMaximumDirectionGetter
-        else:
-            dg_class = ProbabilisticDirectionGetter
-        return dg_class.from_shcoeff(
-            shcoeff=odf_data, max_angle=theta, sphere=sphere,
-            basis_type=args.sh_basis,
-            relative_peak_threshold=args.sf_threshold)
-    elif args.algo == 'eudx':
-        # Code for type EUDX. We don't use peaks_from_model
-        # because we want the peaks from the provided sh.
-        odf_shape_3d = odf_data.shape[:-1]
-        dg = PeaksAndMetrics()
-        dg.sphere = sphere
-        dg.ang_thr = theta
-        dg.qa_thr = args.sf_threshold
-
-        # Heuristic to find out if the input are peaks or fodf
-        # fodf are always around 0.15 and peaks around 0.75
-        if non_first_val_count / non_zeros_count > 0.5:
-            logging.info('Input detected as peaks.')
-            nb_peaks = odf_data.shape[-1] // 3
-            slices = np.arange(0, 15+1, 3)
-            peak_values = np.zeros(odf_shape_3d+(nb_peaks,))
-            peak_indices = np.zeros(odf_shape_3d+(nb_peaks,))
-
-            for idx in np.argwhere(np.sum(odf_data, axis=-1)):
-                idx = tuple(idx)
-                for i in range(nb_peaks):
-                    peak_values[idx][i] = np.linalg.norm(
-                        odf_data[idx][slices[i]:slices[i+1]], axis=-1)
-                    peak_indices[idx][i] = sphere.find_closest(
-                        odf_data[idx][slices[i]:slices[i+1]])
-
-            dg.peak_dirs = odf_data
-        else:
-            logging.info('Input detected as fodf.')
-            npeaks = 5
-            peak_dirs = np.zeros((odf_shape_3d + (npeaks, 3)))
-            peak_values = np.zeros((odf_shape_3d + (npeaks, )))
-            peak_indices = np.full((odf_shape_3d + (npeaks, )), -1,
-                                   dtype='int')
-            b_matrix = get_b_matrix(
-                find_order_from_nb_coeff(odf_data), sphere, args.sh_basis)
-
-            for idx in np.argwhere(np.sum(odf_data, axis=-1)):
-                idx = tuple(idx)
-                directions, values, indices = get_maximas(odf_data[idx],
-                                                          sphere, b_matrix,
-                                                          args.sf_threshold, 0)
-                if values.shape[0] != 0:
-                    n = min(npeaks, values.shape[0])
-                    peak_dirs[idx][:n] = directions[:n]
-                    peak_values[idx][:n] = values[:n]
-                    peak_indices[idx][:n] = indices[:n]
-
-            dg.peak_dirs = peak_dirs
-
-        dg.peak_values = peak_values
-        dg.peak_indices = peak_indices
-
-        return dg
-
-
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    if args.verbose:
-        logging.getLogger().setLevel(logging.DEBUG)
+    try:
+        devnull = open(os.devnull)
+        subprocess.call(args.eddy_cmd, stderr=devnull)
+    except:
+        logging.warning(
+            "{} not found. If executing locally, please install "
+            "the command from the FSL library and make sure it is "
+            "available in your path.".format(args.eddy_cmd))
+
+    required_args = [args.in_dwi, args.in_bvals, args.in_bvecs, args.in_mask]
+
+    assert_inputs_exist(parser, required_args)
+    assert_fsl_options_exist(parser, args.eddy_options, 'eddy')
+
+    if os.path.splitext(args.out_prefix)[1] != '':
+        parser.error('The prefix must not contain any extension.')
+
+    if args.n_reverse and not args.lsr_resampling:
+        logging.warning(
+            'Multiple reverse phase images supplied, but least-square '
+            'resampling is disabled. If the reverse phase image was acquired '
+            'with the same sampling as the forward phase image, '
+            'use --lsr_resampling for better results from Eddy.')
+
+    bvals, bvecs = read_bvals_bvecs(args.in_bvals, args.in_bvecs)
+    bvals_min = bvals.min()
+    b0_threshold = args.b0_thr
+    if bvals_min < 0 or bvals_min > b0_threshold:
+        raise ValueError('The minimal b-value is lesser than 0 or greater '
+                         'than {0}. This is highly suspicious. Please check '
+                         'your data to ensure everything is correct. '
+                         'Value found: {1}'.format(b0_threshold, bvals_min))
+
+    n_rev = args.n_reverse
+
+    if args.topup_params:
+        acqparams = np.loadtxt(args.topup_params)
+
+        if acqparams.shape[0] == 2:
+            index = create_index(bvals, n_rev=n_rev)
+        elif acqparams.shape[0] == np.sum(bvals <= b0_threshold):
+            index = create_multi_topup_index(
+                bvals, 'none', n_rev, b0_threshold)
+        else:
+            b_mask = np.ma.array(bvals, mask=[bvals > b0_threshold])
+            n_b0_clusters = len(np.ma.clump_unmasked(b_mask[:n_rev])) + \
+                len(np.ma.clump_unmasked(b_mask[n_rev:]))
+            if acqparams.shape[0] == n_b0_clusters:
+                index = create_multi_topup_index(
+                    bvals, 'cluster', n_rev, b0_threshold)
+            else:
+                raise ValueError('Could not determine a valid index file '
+                                 'from the provided acquisition parameters '
+                                 'file: {}'.format(args.topup_params))
+    else:
+        acqparams = create_acqparams(args.readout, args.encoding_direction,
+                                     nb_rev_b0s=int(n_rev > 0))
 
-    assert_inputs_exist(parser, [args.in_odf, args.in_seed, args.in_mask])
-    assert_outputs_exist(parser, args, args.out_tractogram)
+        index = create_index(bvals, n_rev=n_rev)
 
-    if not nib.streamlines.is_supported(args.out_tractogram):
-        parser.error('Invalid output streamline file format (must be trk or ' +
-                     'tck): {0}'.format(args.out_tractogram))
-
-    verify_streamline_length_options(parser, args)
-    verify_compression_th(args.compress)
-    verify_seed_options(parser, args)
-
-    mask_img = nib.load(args.in_mask)
-    mask_data = get_data_as_mask(mask_img, dtype=bool)
-
-    # Make sure the data is isotropic. Else, the strategy used
-    # when providing information to dipy (i.e. working as if in voxel space)
-    # will not yield correct results.
-    odf_sh_img = nib.load(args.in_odf)
-    if not np.allclose(np.mean(odf_sh_img.header.get_zooms()[:3]),
-                       odf_sh_img.header.get_zooms()[0], atol=1e-03):
-        parser.error(
-            'ODF SH file is not isotropic. Tracking cannot be ran robustly.')
-
-    if args.npv:
-        nb_seeds = args.npv
-        seed_per_vox = True
-    elif args.nt:
-        nb_seeds = args.nt
-        seed_per_vox = False
-    else:
-        nb_seeds = 1
-        seed_per_vox = True
+    bvecs = create_non_zero_norm_bvecs(bvecs)
 
-    voxel_size = odf_sh_img.header.get_zooms()[0]
-    vox_step_size = args.step_size / voxel_size
-    seed_img = nib.load(args.in_seed)
-
-    if np.count_nonzero(seed_img.get_fdata(dtype=np.float32)) == 0:
-        raise IOError('The image {} is empty. '
-                      'It can\'t be loaded as '
-                      'seeding mask.'.format(args.in_seed))
-
-    # Note. Seeds are in voxel world, center origin.
-    # (See the examples in random_seeds_from_mask).
-    seeds = track_utils.random_seeds_from_mask(
-        seed_img.get_fdata(dtype=np.float32),
-        np.eye(4),
-        seeds_count=nb_seeds,
-        seed_count_per_voxel=seed_per_vox,
-        random_seed=args.seed)
-
-    # Tracking is performed in voxel space
-    max_steps = int(args.max_length / args.step_size) + 1
-    streamlines_generator = LocalTracking(
-        _get_direction_getter(args),
-        BinaryStoppingCriterion(mask_data),
-        seeds, np.eye(4),
-        step_size=vox_step_size, max_cross=1,
-        maxlen=max_steps,
-        fixedstep=True, return_all=True,
-        random_seed=args.seed,
-        save_seeds=args.save_seeds)
-
-    scaled_min_length = args.min_length / voxel_size
-    scaled_max_length = args.max_length / voxel_size
-
-    if args.save_seeds:
-        filtered_streamlines, seeds = \
-            zip(*((s, p) for s, p in streamlines_generator
-                  if scaled_min_length <= length(s) <= scaled_max_length))
-        data_per_streamlines = {'seeds': lambda: seeds}
-    else:
-        filtered_streamlines = \
-            (s for s in streamlines_generator
-             if scaled_min_length <= length(s) <= scaled_max_length)
-        data_per_streamlines = {}
-
-    if args.compress:
-        # Compressing. Threshold is in mm, but we are working in voxel space.
-        # Equivalent of sft.to_voxmm:  streamline *= voxres
-        # Equivalent of sft.to_vox: streamline /= voxres
-        voxres = np.asarray(odf_sh_img.header.get_zooms()[0:3])
-        filtered_streamlines = (
-            compress_streamlines(s * voxres,
-                                 args.compress) / voxres
-            for s in filtered_streamlines)
-
-    tractogram = LazyTractogram(lambda: filtered_streamlines,
-                                data_per_streamlines,
-                                affine_to_rasmm=seed_img.affine)
-
-    filetype = nib.streamlines.detect_format(args.out_tractogram)
-    reference = get_reference_info(seed_img)
-    header = create_tractogram_header(filetype, *reference)
+    if not os.path.exists(args.out_directory):
+        os.makedirs(args.out_directory)
+
+    acqparams_path = os.path.join(args.out_directory, 'acqparams.txt')
+    np.savetxt(acqparams_path, acqparams, fmt='%1.4f', delimiter=' ')
+    index_path = os.path.join(args.out_directory, 'index.txt')
+    np.savetxt(index_path, index, fmt='%i', newline=" ")
+    bvecs_path = os.path.join(args.out_directory, 'non_zero_norm.bvecs')
+    np.savetxt(bvecs_path, bvecs.T, fmt="%.8f")
+
+    additional_args = ""
+    if args.topup is not None:
+        additional_args += "--topup={} ".format(args.topup)
+
+    if args.slice_drop_correction:
+        additional_args += "--repol "
+
+    if args.fix_seed:
+        additional_args += "--initrand "
+
+    if args.lsr_resampling:
+        if len(bvals) - n_rev == n_rev:
+            forward_bb = bvals[:n_rev, None] * bvecs[:n_rev, :]
+            reverse_bb = bvals[n_rev:, None] * bvecs[n_rev:, :]
+            if np.allclose(forward_bb, reverse_bb):
+                additional_args += "--resamp=lsr --fep=true "
+            else:
+                logging.warning('Least-square resampling disabled since '
+                                'directions in both phase directions differ.')
+        else:
+            logging.warning('Least-square resampling disabled since number of '
+                            'directions in both phase directions differ.')
 
-    # Use generator to save the streamlines on-the-fly
-    nib.streamlines.save(tractogram, args.out_tractogram, header=header)
+    if args.eddy_options:
+        additional_args += args.eddy_options
+
+    output_path = os.path.join(args.out_directory, args.out_prefix)
+    eddy = '{0} --imain={1} --mask={2} --acqp={3} --index={4}' \
+           ' --bvecs={5} --bvals={6} --out={7} --data_is_shelled {8}' \
+        .format(args.eddy_cmd, args.in_dwi, args.in_mask, acqparams_path,
+                index_path, bvecs_path, args.in_bvals, output_path,
+                additional_args)
+
+    if args.out_script:
+        with open("eddy.sh", 'w') as f:
+            f.write(eddy)
+    else:
+        print(eddy)
 
 
 if __name__ == '__main__':
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_local_tracking_dev.py` & `scilpy-2.0.0/scripts/scil_tracking_local_dev.py`

 * *Files 8% similar despite different names*

```diff
@@ -4,25 +4,27 @@
 """
 Local streamline HARDI tractography using scilpy-only methods -- no dipy (i.e
 no cython). The goal of this is to have a python-only version that can be
 modified more easily by our team when testing new algorithms and parameters,
 and that can be used as parent classes in sub-projects of our lab such as in
 dwi_ml.
 
-As in scil_compute_local_tracking:
+WARNING. MUCH SLOWER THAN scil_tracking_local.py. We recommand using multi-
+processing with option --nb_processes.
 
+Similar to scil_tracking_local:
     The tracking direction is chosen in the aperture cone defined by the
     previous tracking direction and the angular constraint.
     - Algo 'det': the maxima of the spherical function (SF) the most closely
     aligned to the previous direction.
     - Algo 'prob': a direction drawn from the empirical distribution function
     defined from the SF.
-    - Algo 'eudx' is not yet available!
 
-Contrary to scil_compute_local_tracking:
+Contrary to scil_tracking_local:
+    - Algo 'eudx' is not yet available!
     - Input nifti files do not necessarily need to be in isotropic resolution.
     - The script works with asymmetric input ODF.
     - The interpolation for the tracking mask and spherical function can be
       one of 'nearest' or 'trilinear'.
     - Runge-Kutta integration is supported for the step function.
 
 A few notes on Runge-Kutta integration.
@@ -35,86 +37,102 @@
        tracking.
     2. As a rule of thumb, doubling the rk_order will double the computation
        time in the worst case.
 
 References: [1] Girard, G., Whittingstall K., Deriche, R., and
             Descoteaux, M. (2014). Towards quantitative connectivity analysis:
             reducing tractography biases. Neuroimage, 98, 266-278.
+
+Formerly: scil_compute_local_tracking_dev.py
 """
 import argparse
 import logging
-import math
 import time
 
 import dipy.core.geometry as gm
 import nibabel as nib
 import numpy as np
 
-from dipy.io.stateful_tractogram import StatefulTractogram, Space, \
-                                        set_sft_logger_level
+from dipy.io.stateful_tractogram import StatefulTractogram, Space
 from dipy.io.stateful_tractogram import Origin
 from dipy.io.streamline import save_tractogram
+from nibabel.streamlines import detect_format, TrkFile
 
 from scilpy.io.image import assert_same_resolution
 from scilpy.io.utils import (add_processes_arg, add_sphere_arg,
                              add_verbose_arg,
                              assert_inputs_exist, assert_outputs_exist,
-                             verify_compression_th)
-from scilpy.image.datasets import DataVolume
+                             parse_sh_basis_arg, verify_compression_th)
+from scilpy.image.volume_space_management import DataVolume
 from scilpy.tracking.propagator import ODFPropagator
 from scilpy.tracking.seed import SeedGenerator
-from scilpy.tracking.tools import get_theta
 from scilpy.tracking.tracker import Tracker
 from scilpy.tracking.utils import (add_mandatory_options_tracking,
                                    add_out_options, add_seeding_options,
                                    add_tracking_options,
+                                   get_theta,
                                    verify_streamline_length_options,
                                    verify_seed_options)
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         formatter_class=argparse.RawTextHelpFormatter,
         description=__doc__)
 
+    # Options common to both scripts
     add_mandatory_options_tracking(p)
-
     track_g = add_tracking_options(p)
-    track_g.add_argument('--algo', default='prob',
-                         choices=['det', 'prob'],
+    add_seeding_options(p)
+
+    # Options only for here.
+    track_g.add_argument('--algo', default='prob', choices=['det', 'prob'],
                          help='Algorithm to use. [%(default)s]')
     add_sphere_arg(track_g, symmetric_only=False)
     track_g.add_argument('--sfthres_init', metavar='sf_th', type=float,
                          default=0.5, dest='sf_threshold_init',
                          help="Spherical function relative threshold value "
                               "for the \ninitial direction. [%(default)s]")
     track_g.add_argument('--rk_order', metavar="K", type=int, default=1,
                          choices=[1, 2, 4],
                          help="The order of the Runge-Kutta integration used "
                               "for the step function.\n"
                               "For more information, refer to the note in the"
                               " script description. [%(default)s]")
-    track_g.add_argument('--max_invalid_length', metavar='MAX', type=float,
-                         default=1,
-                         help="Maximum length without valid direction, in mm. "
-                              "[%(default)s]")
+    track_g.add_argument('--max_invalid_nb_points', metavar='MAX', type=float,
+                         default=0,
+                         help="Maximum number of steps without valid "
+                              "direction, \nex: if threshold on ODF or max "
+                              "angles are reached.\n"
+                              "Default: 0, i.e. do not add points following "
+                              "an invalid direction.")
     track_g.add_argument('--forward_only', action='store_true',
                          help="If set, tracks in one direction only (forward) "
                               "given the \ninitial seed. The direction is "
                               "randomly drawn from the ODF.")
     track_g.add_argument('--sh_interp', default='trilinear',
                          choices=['nearest', 'trilinear'],
                          help="Spherical harmonic interpolation: "
                               "nearest-neighbor \nor trilinear. [%(default)s]")
     track_g.add_argument('--mask_interp', default='nearest',
                          choices=['nearest', 'trilinear'],
                          help="Mask interpolation: nearest-neighbor or "
                               "trilinear. [%(default)s]")
-
-    add_seeding_options(p)
+    track_g.add_argument(
+        '--keep_last_out_point', action='store_true',
+        help="If set, keep the last point (once out of the tracking mask) "
+             "of \nthe streamline. Default: discard them. This is the default "
+             " in \nDipy too. Note that points obtained after an invalid "
+             "direction \n(ex when angle is too sharp or sh_threshold not "
+             "reached) are \nnever added.")
+    track_g.add_argument(
+        "--n_repeats_per_seed", type=int, default=1,
+        help="By default, each seed position is used only once. This option\n"
+             "allows for tracking from the exact same seed n_repeats_per_seed"
+             "\ntimes. [%(default)s]")
 
     r_g = p.add_argument_group('Random seeding options')
     r_g.add_argument('--rng_seed', type=int, default=0,
                      help='Initial value for the random number generator. '
                           '[%(default)s]')
     r_g.add_argument('--skip', type=int, default=0,
                      help="Skip the first N random number. \n"
@@ -132,127 +150,132 @@
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
-
-    if args.verbose:
-        logging.getLogger().setLevel(logging.DEBUG)
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     if not nib.streamlines.is_supported(args.out_tractogram):
         parser.error('Invalid output streamline file format (must be trk or ' +
                      'tck): {0}'.format(args.out_tractogram))
 
     inputs = [args.in_odf, args.in_seed, args.in_mask]
     assert_inputs_exist(parser, inputs)
     assert_outputs_exist(parser, args, args.out_tractogram)
 
     verify_streamline_length_options(parser, args)
-    verify_compression_th(args.compress)
+    verify_compression_th(args.compress_th)
     verify_seed_options(parser, args)
 
+    tracts_format = detect_format(args.out_tractogram)
+    if tracts_format is not TrkFile:
+        logging.warning("You have selected option --save_seeds but you are "
+                        "not saving your tractogram as a .trk file. \n"
+                        "Data_per_point information CANNOT be saved.\n"
+                        "Ignoring.")
+        args.save_seeds = False
+
     theta = gm.math.radians(get_theta(args.theta, args.algo))
 
     max_nbr_pts = int(args.max_length / args.step_size)
-    min_nbr_pts = int(args.min_length / args.step_size) + 1
-    max_invalid_dirs = int(math.ceil(args.max_invalid_length / args.step_size))
+    min_nbr_pts = max(int(args.min_length / args.step_size), 1)
 
     assert_same_resolution([args.in_mask, args.in_odf, args.in_seed])
 
     # Choosing our space and origin for this tracking
     # If save_seeds, space and origin must be vox, center. Choosing those
     # values.
     our_space = Space.VOX
     our_origin = Origin('center')
 
     # Preparing everything
-    logging.debug("Loading seeding mask.")
+    logging.info("Loading seeding mask.")
     seed_img = nib.load(args.in_seed)
     seed_data = seed_img.get_fdata(caching='unchanged', dtype=float)
     if np.count_nonzero(seed_data) == 0:
         raise IOError('The image {} is empty. '
                       'It can\'t be loaded as '
                       'seeding mask.'.format(args.in_seed))
 
     seed_res = seed_img.header.get_zooms()[:3]
     seed_generator = SeedGenerator(seed_data, seed_res,
-                                   space=our_space, origin=our_origin)
+                                   space=our_space, origin=our_origin,
+                                   n_repeats=args.n_repeats_per_seed)
     if args.npv:
         # toDo. This will not really produce n seeds per voxel, only true
         #  in average.
-        nbr_seeds = len(seed_generator.seeds_vox) * args.npv
+        nbr_seeds = len(seed_generator.seeds_vox_corner) * args.npv
     elif args.nt:
         nbr_seeds = args.nt
     else:
         # Setting npv = 1.
-        nbr_seeds = len(seed_generator.seeds_vox)
-    if len(seed_generator.seeds_vox) == 0:
+        nbr_seeds = len(seed_generator.seeds_vox_corner)
+    if len(seed_generator.seeds_vox_corner) == 0:
         parser.error('Seed mask "{}" does not have any voxel with value > 0.'
                      .format(args.in_seed))
 
-    logging.debug("Loading tracking mask.")
+    logging.info("Loading tracking mask.")
     mask_img = nib.load(args.in_mask)
     mask_data = mask_img.get_fdata(caching='unchanged', dtype=float)
     mask_res = mask_img.header.get_zooms()[:3]
     mask = DataVolume(mask_data, mask_res, args.mask_interp)
 
-    logging.debug("Loading ODF SH data.")
+    logging.info("Loading ODF SH data.")
     odf_sh_img = nib.load(args.in_odf)
     odf_sh_data = odf_sh_img.get_fdata(caching='unchanged', dtype=float)
     odf_sh_res = odf_sh_img.header.get_zooms()[:3]
     dataset = DataVolume(odf_sh_data, odf_sh_res, args.sh_interp)
 
-    logging.debug("Instantiating propagator.")
+    logging.info("Instantiating propagator.")
     # Converting step size to vox space
     # We only support iso vox for now.
     assert odf_sh_res[0] == odf_sh_res[1] == odf_sh_res[2]
     voxel_size = odf_sh_img.header.get_zooms()[0]
     vox_step_size = args.step_size / voxel_size
 
     # Using space and origin in the propagator: vox and center, like
     # in dipy.
+    sh_basis, is_legacy = parse_sh_basis_arg(args)
     propagator = ODFPropagator(
-        dataset, vox_step_size, args.rk_order, args.algo, args.sh_basis,
+        dataset, vox_step_size, args.rk_order, args.algo, sh_basis,
         args.sf_threshold, args.sf_threshold_init, theta, args.sphere,
-        space=our_space, origin=our_origin)
+        space=our_space, origin=our_origin, is_legacy=is_legacy)
 
-    logging.debug("Instantiating tracker.")
+    logging.info("Instantiating tracker.")
     tracker = Tracker(propagator, mask, seed_generator, nbr_seeds, min_nbr_pts,
-                      max_nbr_pts, max_invalid_dirs,
-                      compression_th=args.compress,
+                      max_nbr_pts, args.max_invalid_nb_points,
+                      compression_th=args.compress_th,
                       nbr_processes=args.nbr_processes,
                       save_seeds=args.save_seeds,
                       mmap_mode='r+', rng_seed=args.rng_seed,
                       track_forward_only=args.forward_only,
-                      skip=args.skip)
+                      skip=args.skip,
+                      append_last_point=args.keep_last_out_point,
+                      verbose=args.verbose)
 
     start = time.time()
-    logging.debug("Tracking...")
+    logging.info("Tracking...")
     streamlines, seeds = tracker.track()
 
     str_time = "%.2f" % (time.time() - start)
-    logging.debug("Tracked {} streamlines (out of {} seeds), in {} seconds.\n"
-                  "Now saving..."
-                  .format(len(streamlines), nbr_seeds, str_time))
+    logging.info("Tracked {} streamlines (out of {} seeds), in {} seconds.\n"
+                 "Now saving..."
+                 .format(len(streamlines), nbr_seeds, str_time))
 
     # save seeds if args.save_seeds is given
     # We seeded (and tracked) in vox, center, which is what is expected for
     # seeds.
     if args.save_seeds:
         data_per_streamline = {'seeds': seeds}
     else:
         data_per_streamline = {}
 
-    # Silencing SFT's logger if our logging is in DEBUG mode, because it
-    # typically produces a lot of outputs!
-    set_sft_logger_level('WARNING')
-
-    # Compared with scil_compute_local_tracking, using sft rather than
+    # Compared with scil_tracking_local, using sft rather than
     # LazyTractogram to deal with space.
     # Contrary to scilpy or dipy, where space after tracking is vox, here
     # space after tracking is voxmm.
     # Smallest possible streamline coordinate is (0,0,0), equivalent of
     # corner origin (TrackVis)
     sft = StatefulTractogram(streamlines, mask_img,
                              space=our_space, origin=our_origin,
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_local_tracking_gpu.py` & `scilpy-2.0.0/scripts/scil_viz_bundle.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,213 +1,214 @@
 #!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+
 """
-Perform probabilistic tractography on a ODF field inside a binary mask.
-The tracking is executed on the GPU using the OpenCL API.
+Visualize bundles.
+
+Example usages:
+
+# Visualize streamlines as tubes, each bundle with a different color
+>>> scil_viz_bundle.py path_to_bundles/ --shape tube --random_coloring 1337
 
-Streamlines are filtered by minimum length, but not by maximum length. This
-means that streamlines are stopped and returned as soon as they reach the
-maximum length instead of being discarded if they go above the maximum allowed
-length. This allows for short-tracks tractography, where streamlines are
-prematurely stopped once they reach some target length.
-
-However, for this reason, there may be streamlines ending in the deep white
-matter. In order to use the resulting tractogram for analysis, it should be
-cleaned with scil_filter_tractogram_anatomically.py.
-
-The white matter mask is interpolated using nearest-neighbor interpolation and
-the SH interpolation defaults to trilinear.
-
-The script also incorporates ideas from Ensemble Tractography [1] (ET). Given
-a list of maximum angles, a different angle drawn at random from the set will
-be used for each streamline.
+# Visualize a tractogram with each streamlines drawn as lines, colored with
+# their local orientation, but only load 1 in 10 streamlines
+>>> scil_viz_bundle.py tractogram.trk --shape line --subsample 10
 
-In order to use the script, you must have a OpenCL compatible GPU and install
-the pyopencl package via `pip install pyopencl`.
+# Visualize CSTs as large tubes and color them from a list of colors in a file
+>>> scil_viz_bundle.py path_to_bundles/CST_* --width 0.5
+    --color_dict colors.json
 """
 
 import argparse
+import colorsys
+import glob
+import json
+import itertools
 import logging
-from time import perf_counter
 import nibabel as nib
 import numpy as np
+import os
+import random
 
-from nibabel.streamlines.tractogram import LazyTractogram, TractogramItem
-from scilpy.io.utils import (add_sh_basis_args, add_verbose_arg,
-                             assert_inputs_exist, assert_outputs_exist)
-from scilpy.io.image import get_data_as_mask
-from scilpy.tracking.utils import (add_out_options, add_seeding_options,
-                                   add_mandatory_options_tracking)
-from scilpy.tracking.tracker import GPUTacker
-from dipy.tracking.utils import random_seeds_from_mask
-from dipy.tracking.streamlinespeed import compress_streamlines
-from dipy.io.utils import get_reference_info, create_tractogram_header
-from scilpy.io.utils import verify_compression_th
-
-
-EPILOG = """
-[1] Takemura, H. et al (2016). Ensemble tractography. PLoS Computational
-    Biology, 12(2), e1004692.
-"""
+from dipy.tracking.streamline import set_number_of_points
+from fury import window, actor, colormap
 
+from scilpy.io.utils import (assert_inputs_exist,
+                             add_verbose_arg,
+                             parser_color_type)
 
-def _build_arg_parser():
-    p = argparse.ArgumentParser(description=__doc__, epilog=EPILOG,
-                                formatter_class=argparse.RawTextHelpFormatter)
 
-    # rename `optional arguments` group to `Generic options`
-    p._optionals.title = 'Generic options'
+streamline_actor = {'tube': actor.streamtube,
+                    'line': actor.line}
 
-    # mandatory tracking options
-    add_mandatory_options_tracking(p)
 
-    track_g = p.add_argument_group('Tracking options')
-    track_g.add_argument('--step_size', type=float, default=0.5,
-                         help='Step size in mm. [%(default)s]')
-    track_g.add_argument('--theta', type=float, nargs='+', default=20.0,
-                         help='Maximum angle between 2 steps. If more than one'
-                              ' value\nare given, the maximum angle will be '
-                              'drawn at random\nfrom the distribution for each'
-                              ' streamline. [%(default)s]')
-    track_g.add_argument('--min_length', type=float, default=20.0,
-                         help='Minimum length of the streamline '
-                              'in mm. [%(default)s]')
-    track_g.add_argument('--max_length', type=float, default=300.0,
-                         help='Maximum length of the streamline '
-                              'in mm. [%(default)s]')
-    track_g.add_argument('--sf_threshold', type=float, default=0.1,
-                         help='Relative threshold on sf amplitudes.'
-                              ' [%(default)s]')
-    track_g.add_argument('--sh_interp', default='trilinear',
-                         choices=['nearest', 'trilinear'],
-                         help='SH interpolation mode. [%(default)s]')
-    track_g.add_argument('--mask_interp', default='nearest',
-                         choices=['nearest', 'trilinear'],
-                         help='Mask interpolation. Only nearest-neighbour '
-                              'interpolation \nis available for now. '
-                              '[%(default)s]')
-    track_g.add_argument('--forward_only', action='store_true',
-                         help='Only perform forward tracking.')
-    add_sh_basis_args(track_g)
-
-    # seeding options
-    add_seeding_options(p)
-
-    out_g = add_out_options(p)
-    # random number generator for SF sampling
-    out_g.add_argument('--rng_seed', type=int,
-                       help='Random number generator seed.')
-
-    gpu_g = p.add_argument_group('GPU options')
-    gpu_g.add_argument('--batch_size', type=int, default=100000,
-                       help='Approximate size of GPU batches (number\n'
-                            'of streamlines to track in parallel).'
-                            ' [%(default)s]')
+def _build_arg_parser():
+    p = argparse.ArgumentParser(
+        description=__doc__, formatter_class=argparse.RawTextHelpFormatter)
+    p.add_argument('in_bundles', nargs='+',
+                   help='List of tractography files supported by nibabel.')
+    p2 = p.add_argument_group(title='Colouring options')
+    coloring_group = p2.add_mutually_exclusive_group()
+    coloring_group.add_argument('--random_coloring', metavar='SEED', type=int,
+                                help='Assign a random color to bundles.')
+    coloring_group.add_argument('--uniform_coloring', metavar=('R', 'G', 'B'),
+                                nargs=3, type=int,
+                                help='Assign a uniform color to streamlines.')
+    coloring_group.add_argument('--local_coloring', action='store_true',
+                                help='Assign coloring to streamlines '
+                                'depending on their local orientations.')
+    coloring_group.add_argument('--color_dict', type=str, metavar='JSON',
+                                help='JSON file containing colors for each '
+                                'bundle.\nBundle filenames are indicated as '
+                                ' keys and colors as values.\nA \'default\' '
+                                ' key and value can be included.')
+    coloring_group.add_argument('--color_from_streamlines',
+                                metavar='KEY', type=str,
+                                help='Extract a color per streamline from the '
+                                'data_per_streamline property of the '
+                                'tractogram at the specified key.')
+    coloring_group.add_argument('--color_from_points', metavar='KEY', type=str,
+                                help='Extract a color per point from the '
+                                'data_per_point property of the tractogram '
+                                'at the specified key.')
+    p.add_argument('--shape', type=str,
+                   choices=['line', 'tube'], default='tube',
+                   help='Display streamlines either as lines or tubes.'
+                   '\n[Default: %(default)s]')
+    p.add_argument('--width', type=float, default=0.25,
+                   help='Width of tubes or lines representing streamlines'
+                   '\n[Default: %(default)s]')
+    p.add_argument('--subsample', type=int, default=1,
+                   help='Only load 1 in N streamlines.'
+                   '\n[Default: %(default)s]')
+    p.add_argument('--downsample', type=int, default=None,
+                   help='Downsample streamlines to N points.'
+                   '\n[Default: %(default)s]')
+    p.add_argument('--background', metavar=('R', 'G', 'B'), nargs=3,
+                   default=[0, 0, 0], type=parser_color_type,
+                   help='RBG values [0, 255] of the color of the background.'
+                   '\n[Default: %(default)s]')
+
+    add_verbose_arg(p)
 
-    log_g = p.add_argument_group('Logging options')
-    add_verbose_arg(log_g)
     return p
 
 
+def random_rgb():
+    # Heuristic to get a random "bright" color
+    # From https://stackoverflow.com/a/43437435
+    h, s, li = (random.random(),
+                0.5 + random.random()/2.0,
+                0.4 + random.random()/5.0)
+    r, g, b = [int(256*i) for i in colorsys.hls_to_rgb(h, li, s)]
+    return np.array([r, g, b])
+
+
 def main():
-    t_init = perf_counter()
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    if args.verbose:
-        logging.getLogger().setLevel(logging.INFO)
-
-    assert_inputs_exist(parser, [args.in_odf, args.in_mask, args.in_seed])
-    assert_outputs_exist(parser, args, args.out_tractogram)
-    if args.compress is not None:
-        verify_compression_th(args.compress)
-
-    if args.mask_interp == 'trilinear':
-        parser.error('Trilinear interpolation for tracking mask'
-                     ' is not available yet. Please set to \'nearest\'.')
-
-    odf_sh_img = nib.load(args.in_odf)
-    mask = get_data_as_mask(nib.load(args.in_mask))
-    odf_sh = odf_sh_img.get_fdata(dtype=np.float32)
-
-    seed_img = nib.load(args.in_seed)
-    if np.count_nonzero(seed_img.get_fdata(dtype=np.float32)) == 0:
-        raise IOError('The image {} is empty. '
-                      'It can\'t be loaded as '
-                      'seeding mask.'.format(args.in_seed))
-    else:
-        seed_mask = seed_img.get_fdata(dtype=np.float32)
-
-    t0 = perf_counter()
-    if args.npv:
-        nb_seeds = args.npv
-        seed_per_vox = True
-    elif args.nt:
-        nb_seeds = args.nt
-        seed_per_vox = False
-    else:
-        nb_seeds = 1
-        seed_per_vox = True
-
-    # Seeds are returned with origin `center`.
-    # However, GPUTracker expects origin to be `corner`.
-    # Therefore, we need to shift the seed positions by half voxel.
-    seeds = random_seeds_from_mask(
-        seed_mask, np.eye(4),
-        seeds_count=nb_seeds,
-        seed_count_per_voxel=seed_per_vox,
-        random_seed=args.rng_seed) + 0.5
-    logging.info('Generated {0} seed positions in {1:.2f}s.'
-                 .format(len(seeds), perf_counter() - t0))
-
-    voxel_size = odf_sh_img.header.get_zooms()[0]
-    vox_step_size = args.step_size / voxel_size
-    vox_max_length = args.max_length / voxel_size
-    vox_min_length = args.min_length / voxel_size
-    min_strl_len = int(vox_min_length / vox_step_size) + 1
-    max_strl_len = int(vox_max_length / vox_step_size) + 1
-    if args.compress:
-        compress_th_vox = args.compress / voxel_size
-
-    # initialize tracking
-    tracker = GPUTacker(odf_sh, mask, seeds, vox_step_size, min_strl_len,
-                        max_strl_len, theta=args.theta,
-                        sf_threshold=args.sf_threshold,
-                        sh_interp=args.sh_interp,
-                        sh_basis=args.sh_basis,
-                        batch_size=args.batch_size,
-                        forward_only=args.forward_only,
-                        rng_seed=args.rng_seed)
-
-    # wrapper for tracker.track() yielding one TractogramItem per
-    # streamline for use with the LazyTractogram.
-    def tracks_generator_wrapper():
-        for strl, seed in tracker.track():
-            # seed must be saved in voxel space, with origin `center`.
-            dps = {}
-            if args.save_seeds:
-                dps['seeds'] = seed - 0.5
-
-            # TODO: Investigate why the streamline must NOT be shifted to
-            # origin `center` for LazyTractogram.
-            strl *= voxel_size  # in mm.
-            if args.compress:
-                strl = compress_streamlines(strl, compress_th_vox)
-            yield TractogramItem(strl, dps, {})
-
-    # instantiate tractogram
-    tractogram = LazyTractogram.from_data_func(tracks_generator_wrapper)
-    tractogram.affine_to_rasmm = odf_sh_img.affine
-
-    filetype = nib.streamlines.detect_format(args.out_tractogram)
-    reference = get_reference_info(odf_sh_img)
-    header = create_tractogram_header(filetype, *reference)
-
-    # Use generator to save the streamlines on-the-fly
-    nib.streamlines.save(tractogram, args.out_tractogram, header=header)
-    logging.info('Saved tractogram to {0}.'.format(args.out_tractogram))
-
-    # Total runtime
-    logging.info('Total runtime of {0:.2f}s.'.format(perf_counter() - t_init))
+    # Make sure the colors are consistent between executions
+    if args.random_coloring is not None:
+        random.seed(int(args.random_coloring))
+
+    # Handle bundle filenames. 3 cases are possible:
+    # A list of files was passed as arguments
+    # A directory was passed as argument
+    # A single-file or path containing wildcard was specified
+    bundle_filenames = args.in_bundles
+    if len(args.in_bundles) == 1:
+        # If only one file is specified, it may be a whole folder or
+        # a single file, or a wildcard usage
+        in_path = args.in_bundles[0]
+        if os.path.isdir(in_path):
+            # Load the folder
+            bundle_filenames = [os.path.join(in_path, f)
+                                for f in os.listdir(in_path)]
+        else:
+            # Load the file/wildcard
+            bundle_filenames = glob.glob(in_path)
+
+    assert_inputs_exist(parser, bundle_filenames, args.color_dict)
+
+    scene = window.Scene()
+    scene.background(tuple(map(int, args.background)))
+
+    def subsample(list_obj):
+        """ Lazily subsample a list
+        """
+        return list(
+            itertools.islice(list_obj, 0, None, args.subsample))
+
+    # Load each bundle, subsample and downsample it if needed
+    for filename in bundle_filenames:
+        try:
+            # Lazy-load streamlines to minimize ram usage
+            tractogram_gen = nib.streamlines.load(
+                filename, lazy_load=True).tractogram
+            streamlines_gen = tractogram_gen.streamlines
+        except ValueError:
+            # Not a file loadable by nibabel's streamline API
+            print('Skipping {}'.format(filename))
+            continue
+
+        # Actually load streamlines according to the subsample argument
+        streamlines = subsample(streamlines_gen)
+
+        if args.downsample:
+            streamlines = set_number_of_points(streamlines, args.downsample)
+
+        # Handle bundle colors. Either assign a random bright color to each
+        # bundle, or load a color specific to each bundle, or let the bundles
+        # be colored according to their local orientation
+        if args.random_coloring:
+            color = random_rgb()
+        elif args.color_dict:
+            with open(args.color_dict) as json_file:
+                # Color dictionary
+                color_dict = json.load(json_file)
+
+                # Extract filenames to compare against the color dictionary
+                basename = os.path.splitext(os.path.basename(filename))[0]
+
+                # Load colors
+                color = color_dict[basename] \
+                    if basename in color_dict.keys() \
+                    else color_dict['default']
+        elif args.color_from_streamlines:
+            color = subsample(
+                tractogram_gen.data_per_streamline[args.color_from_streamlines]
+            )
+        elif args.color_from_points:
+            color = subsample(
+                tractogram_gen.data_per_point[args.color_from_points])
+        elif args.uniform_coloring:  # Assign uniform coloring to streamlines
+            color = tuple(np.asarray(args.uniform_coloring) / 255)
+        elif args.local_coloring:  # Compute coloring from local orientations
+            # Compute segment orientation
+            diff = [np.diff(list(s), axis=0) for s in streamlines]
+            # Repeat first segment so that the number of segments matches
+            # the number of points
+            diff = [[d[0]] + list(d) for d in diff]
+            # Flatten the list of segments
+            orientations = np.asarray([o for d in diff for o in d])
+            # Turn the segments into colors
+            color = colormap.orient2rgb(orientations)
+        else:  # Streamline color will depend on the streamlines' endpoints.
+            color = None
+        # TODO: Coloring from a volume of local orientations
+
+        line_actor = streamline_actor[args.shape](
+            streamlines, colors=color, linewidth=args.width)
+        scene.add(line_actor)
+
+    # If there's actually streamlines to display
+    if len(bundle_filenames):
+        # Showtime !
+        showm = window.ShowManager(scene, reset_camera=True)
+        showm.initialize()
+        showm.start()
 
 
 if __name__ == '__main__':
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_maps_for_particle_filter_tracking.py` & `scilpy-2.0.0/scripts/scil_tracking_pft_maps.py`

 * *Files 4% similar despite different names*

```diff
@@ -5,14 +5,16 @@
 Compute include and exclude maps, and the seeding interface mask from partial
 volume estimation (PVE) maps. Maps should have values in [0,1], gm+wm+csf=1 in
 all voxels of the brain, gm+wm+csf=0 elsewhere.
 
 References: Girard, G., Whittingstall K., Deriche, R., and Descoteaux, M.
 (2014). Towards quantitative connectivity analysis: reducing tractography
 biases. Neuroimage.
+
+Formerly: scil_compute_maps_for_particle_filter_tracking.py
 """
 
 import argparse
 import logging
 
 import numpy as np
 import nibabel as nib
@@ -45,25 +47,25 @@
     p.add_argument('--interface', metavar='filename',
                    default='interface.nii.gz',
                    help='Output interface seeding mask (nifti). [%(default)s]')
     p.add_argument('-t', dest='int_thres', metavar='THRESHOLD',
                    type=float, default=0.1,
                    help='Minimum gm and wm PVE values in a voxel to be into '
                         'the interface. [%(default)s]')
+
     add_overwrite_arg(p)
     add_verbose_arg(p)
+
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
-
-    if args.verbose:
-        logging.getLogger().setLevel(logging.DEBUG)
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, [args.in_wm, args.in_gm, args.in_csf])
     assert_outputs_exist(parser, args,
                          [args.include, args.exclude, args.interface])
 
     wm_pve = nib.load(args.in_wm)
     gm_pve = nib.load(args.in_gm)
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_mean_fixel_afd_from_bundles.py` & `scilpy-2.0.0/scripts/scil_bundle_mean_fixel_afd.py`

 * *Files 12% similar despite different names*

```diff
@@ -5,33 +5,39 @@
 Compute the mean Apparent Fiber Density (AFD) and mean Radial fODF (radfODF)
 maps along a bundle.
 
 This is the "real" fixel-based fODF amplitude along every streamline
 of the bundle provided, averaged at every voxel.
 
 Please use a bundle file rather than a whole tractogram.
+
+Formerly: scil_compute_fixel_afd_from_bundles.py
 """
 
 import argparse
+import logging
 
 import nibabel as nib
 import numpy as np
 
 from scilpy.io.streamlines import load_tractogram_with_reference
 from scilpy.io.utils import (add_overwrite_arg, add_sh_basis_args,
-                             add_reference_arg,
-                             assert_inputs_exist, assert_outputs_exist)
-from scilpy.reconst.afd_along_streamlines import afd_map_along_streamlines
+                             add_reference_arg, add_verbose_arg,
+                             assert_inputs_exist, assert_outputs_exist,
+                             parse_sh_basis_arg, assert_headers_compatible)
+from scilpy.tractanalysis.afd_along_streamlines \
+    import afd_map_along_streamlines
 
 EPILOG = """
 Reference:
     [1] Raffelt, D., Tournier, JD., Rose, S., Ridgway, GR., Henderson, R.,
         Crozier, S., Salvado, O., & Connelly, A. (2012).
         Apparent Fibre Density: a novel measure for the analysis of
-        diffusion-weighted magnetic resonance images. NeuroImage, 59(4), 3976--3994.
+        diffusion-weighted magnetic resonance images. NeuroImage, 59(4),
+        3976--3994.
 """
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(description=__doc__, epilog=EPILOG,
                                 formatter_class=argparse.RawTextHelpFormatter)
     p.add_argument('in_bundle',
@@ -43,33 +49,41 @@
 
     p.add_argument('--length_weighting', action='store_true',
                    help='If set, will weigh the AFD values according to '
                         'segment lengths. [%(default)s]')
 
     add_reference_arg(p)
     add_sh_basis_args(p)
+    add_verbose_arg(p)
     add_overwrite_arg(p)
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, [args.in_bundle, args.in_fodf])
+    assert_inputs_exist(parser, [args.in_bundle, args.in_fodf],
+                        args.reference)
     assert_outputs_exist(parser, args, [args.afd_mean_map])
+    assert_headers_compatible(parser, [args.in_bundle, args.in_fodf],
+                              reference=args.reference)
 
     sft = load_tractogram_with_reference(parser, args, args.in_bundle)
     fodf_img = nib.load(args.in_fodf)
 
-    afd_mean_map, rd_mean_map = afd_map_along_streamlines(sft,
-                                                          fodf_img,
-                                                          args.sh_basis,
-                                                          args.length_weighting)
+    sh_basis, is_legacy = parse_sh_basis_arg(args)
+
+    afd_mean_map, rd_mean_map = afd_map_along_streamlines(
+                                                sft,
+                                                fodf_img,
+                                                sh_basis,
+                                                args.length_weighting,
+                                                is_legacy=is_legacy)
 
     nib.Nifti1Image(afd_mean_map.astype(np.float32),
                     fodf_img.affine).to_filename(args.afd_mean_map)
 
 
-
 if __name__ == '__main__':
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_mean_fixel_lobe_metric_from_bundles.py` & `scilpy-2.0.0/scripts/scil_bundle_mean_fixel_bingham_metric.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,94 +1,103 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-Given a bundle and Bingham coefficients, compute the average lobe-specific
+Given a bundle and Bingham coefficients, compute the average Bingham
 metric at each voxel intersected by the bundle. Intersected voxels are
 found by computing the intersection between the voxel grid and each streamline
 in the input tractogram.
 
 This script behaves like scil_compute_mean_fixel_afd_from_bundles.py for fODFs,
-but here for Bingham distributions. These latest distributions add the unique
-possibility to capture fixel-based fiber spread (FS) and fiber fraction (FF).
-FD from the bingham should be "equivalent" to the AFD_fixel we are used to.
-
-Bingham coefficients volume must come from scil_fit_bingham_to_fodf.py and
-lobe-specific metrics comes from scil_compute_lobe_specific_fodf_metrics.py.
-
-Lobe-specific metrics are metrics extracted from Bingham distributions fitted
-to fODF. Their are as many values per voxel as there are lobes extracted. The
-values chosen for a given voxelis the one belonging to the lobe better aligned
-with the current streamline segment.
+but here for Bingham distributions. These add the unique possibility to capture
+fixel-based fiber spread (FS) and fiber fraction (FF). FD from the bingham
+should be "equivalent" to the AFD_fixel we are used to.
+
+Bingham coefficients volume must come from scil_fodf_to_bingham.py
+and Bingham metrics comes from scil_bingham_metrics.py.
+
+Bingham metrics are extracted from Bingham distributions fitted to fODF. There
+are as many values per voxel as there are lobes extracted. The values chosen
+for a given voxelis the one belonging to the lobe better aligned with the
+current streamline segment.
 
 Please use a bundle file rather than a whole tractogram.
+
+Formerly: scil_compute_mean_fixel_obe_metric_from_bundles.py
 """
 
 import argparse
+import logging
 
 import nibabel as nib
 import numpy as np
 
 from scilpy.io.streamlines import load_tractogram_with_reference
-from scilpy.io.utils import (add_overwrite_arg,
-                             add_reference_arg,
-                             assert_inputs_exist, assert_outputs_exist)
-from scilpy.reconst.lobe_metrics_along_streamlines \
-    import lobe_specific_metric_map_along_streamlines
+from scilpy.io.utils import (add_overwrite_arg, add_reference_arg,
+                             add_verbose_arg, assert_inputs_exist,
+                             assert_outputs_exist, assert_headers_compatible)
+from scilpy.tractanalysis.bingham_metric_along_streamlines \
+    import bingham_metric_map_along_streamlines
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(description=__doc__,
                                 formatter_class=argparse.RawTextHelpFormatter)
     p.add_argument('in_bundle',
                    help='Path of the bundle file.')
     p.add_argument('in_bingham',
                    help='Path of the Bingham volume.')
-    p.add_argument('in_lobe_metric',
-                   help='Path of the lobe-specific metric (FD, FS, or FF)'
-                        ' volume.')
+    p.add_argument('in_bingham_metric',
+                   help='Path of the Bingham metric (FD, FS, or FF) '
+                        'volume.')
     p.add_argument('out_mean_map',
                    help='Path of the output mean map.')
 
     p.add_argument('--length_weighting', action='store_true',
                    help='If set, will weigh the FD values according to '
-                        'segment lengths. [%(default)s]')
+                        'segment lengths.')
 
     p.add_argument('--max_theta', default=60, type=float,
                    help='Maximum angle (in degrees) condition on lobe '
                         'alignment. [%(default)s]')
 
     add_reference_arg(p)
+    add_verbose_arg(p)
     add_overwrite_arg(p)
+
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, [args.in_bundle,
-                                 args.in_bingham,
-                                 args.in_lobe_metric])
+    assert_inputs_exist(parser, [args.in_bundle, args.in_bingham,
+                                 args.in_bingham_metric],
+                        args.reference)
     assert_outputs_exist(parser, args, [args.out_mean_map])
+    assert_headers_compatible(parser, [args.in_bundle, args.in_bingham,
+                                       args.in_bingham_metric],
+                              reference=args.reference)
 
     sft = load_tractogram_with_reference(parser, args, args.in_bundle)
     bingham_img = nib.load(args.in_bingham)
-    metric_img = nib.load(args.in_lobe_metric)
+    metric_img = nib.load(args.in_bingham_metric)
 
     if bingham_img.shape[-2] != metric_img.shape[-1]:
         parser.error('Dimension mismatch between Bingham coefficients '
-                     'and lobe-specific metric image.')
+                     'and Bingham metric image.')
 
     metric_mean_map =\
-        lobe_specific_metric_map_along_streamlines(sft,
-                                                   bingham_img.get_fdata(),
-                                                   metric_img.get_fdata(),
-                                                   args.max_theta,
-                                                   args.length_weighting)
+        bingham_metric_map_along_streamlines(sft,
+                                             bingham_img.get_fdata(),
+                                             metric_img.get_fdata(),
+                                             args.max_theta,
+                                             args.length_weighting)
 
     nib.Nifti1Image(metric_mean_map.astype(np.float32),
                     bingham_img.affine).to_filename(args.out_mean_map)
 
 
 if __name__ == '__main__':
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_mean_frf.py` & `scilpy-2.0.0/scripts/scil_frf_mean.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,55 +1,70 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
 Compute the mean Fiber Response Function from a set of individually
 computed Response Functions.
+
+The FRF files are obtained from scil_frf_ssst.py, scil_frf_msmt.py in the
+case of multi-shell data or scil_frf_memsmt.py in the case of multi-encoding
+multi-shell data.
+
+Formerly: scil_compute_mean_frf.py
 """
 
 import argparse
 import logging
 
 import numpy as np
 
-from scilpy.io.utils import (
-    add_overwrite_arg, assert_inputs_exist, assert_outputs_exist)
+from scilpy.io.utils import (add_overwrite_arg,
+                             assert_inputs_exist,
+                             add_verbose_arg,
+                             assert_outputs_exist)
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__,
         formatter_class=argparse.RawTextHelpFormatter)
 
     p.add_argument('frf_files', metavar='list', nargs='+',
                    help='List of FRF filepaths.')
     p.add_argument('mean_frf', metavar='file',
                    help='Path of the output mean FRF file.')
 
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
-    logging.getLogger().setLevel(logging.INFO)
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, args.frf_files)
     assert_outputs_exist(parser, args, args.mean_frf)
 
-    all_frfs = np.zeros((len(args.frf_files), 4))
+    frf_shape = np.loadtxt(args.frf_files[0]).shape
+    all_frfs = np.zeros((len(args.frf_files),) + frf_shape)
 
     for idx, frf_file in enumerate(args.frf_files):
         frf = np.loadtxt(frf_file)
 
-        if not frf.shape[0] == 4:
-            raise ValueError('FRF file {} did not contain 4 elements. Invalid '
-                             'or deprecated FRF format'.format(frf_file))
+        if not frf.shape[-1] == 4:
+            raise ValueError('FRF file {} did not contain 4 elements per '
+                             'line. Invalid or deprecated FRF format.'
+                             .format(frf_file))
+
+        if not frf.shape == frf_shape:
+            raise ValueError('FRF file {} did not match the format of '
+                             'previous files.'.format(frf_file))
 
         all_frfs[idx] = frf
 
     final_frf = np.mean(all_frfs, axis=0)
 
     np.savetxt(args.mean_frf, final_frf)
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_memsmt_fodf.py` & `scilpy-2.0.0/scripts/scil_fodf_memsmt.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,58 +1,69 @@
 #! /usr/bin/env python
 # -*- coding: utf-8 -*-
 
 """
 Script to compute multi-encoding multi-shell multi-tissue (memsmt)
-Constrained Spherical Deconvolution ODFs. In order to operate,
-the script only needs the data from one type of b-tensor encoding. However,
-giving only a spherical one will not produce good fODFs, as it only probes
-spherical shapes. As for planar encoding, it should technically
+Constrained Spherical Deconvolution ODFs.
+
+In order to operate, the script only needs the data from one type of b-tensor
+encoding. However, giving only a spherical one will not produce good fODFs, as
+it only probes spherical shapes. As for planar encoding, it should technically
 work alone, but seems to be very sensitive to noise and is yet to be properly
 documented. We thus suggest to always use at least the linear encoding, which
 will be equivalent to standard multi-shell multi-tissue if used alone, in
 combinaison with other encodings. Note that custom encodings are not yet
 supported, so that only the linear tensor encoding (LTE, b_delta = 1), the
 planar tensor encoding (PTE, b_delta = -0.5), the spherical tensor encoding
 (STE, b_delta = 0) and the cigar shape tensor encoding (b_delta = 0.5) are
-available. Moreover, all of `--in_dwis`, `--in_bvals`, `--in_bvecs` and
-`--in_bdeltas` must have the same number of arguments. Be sure to keep the
-same order of encodings throughout all these inputs and to set `--in_bdeltas`
-accordingly (IMPORTANT).
+available.
+
+All of `--in_dwis`, `--in_bvals`, `--in_bvecs` and `--in_bdeltas` must have the
+same number of arguments. Be sure to keep the same order of encodings
+throughout all these inputs and to set `--in_bdeltas` accordingly (IMPORTANT).
 
 By default, will output all possible files, using default names.
 Specific names can be specified using the file flags specified in the
 "File flags" section.
 
 If --not_all is set, only the files specified explicitly by the flags
 will be output.
 
-Based on P. Karan et al., Bridging the gap between constrained spherical 
-deconvolution and diffusional variance decomposition via tensor-valued 
+>>> scil_fodf_memsmt.py wm_frf.txt gm_frf.txt csf_frf.txt --in_dwis LTE.nii.gz
+    PTE.nii.gz STE.nii.gz --in_bvals LTE.bval PTE.bval STE.bval --in_bvecs
+    LTE.bvec PTE.bvec STE.bvec --in_bdeltas 1 -0.5 0 --mask mask.nii.gz
+
+Based on P. Karan et al., Bridging the gap between constrained spherical
+deconvolution and diffusional variance decomposition via tensor-valued
 diffusion MRI. Medical Image Analysis (2022)
+
+Formerly: scil_compute_memsmt_fodf.py
 """
 
 import argparse
 import logging
 
-from dipy.core.gradients import GradientTable
-from dipy.data import get_sphere, default_sphere
-from dipy.reconst import shm
-from dipy.reconst.mcsd import MultiShellResponse, MultiShellDeconvModel
+from dipy.data import get_sphere
+from dipy.reconst.mcsd import MultiShellDeconvModel, multi_shell_fiber_response
 import nibabel as nib
 import numpy as np
 
 from scilpy.image.utils import extract_affine
+from scilpy.io.btensor import (generate_btensor_input,
+                               convert_bdelta_to_bshape)
 from scilpy.io.image import get_data_as_mask
-from scilpy.io.utils import (add_overwrite_arg, assert_inputs_exist,
-                             assert_outputs_exist, add_force_b0_arg,
-                             add_sh_basis_args, add_processes_arg,
-                             add_verbose_arg)
-from scilpy.reconst.multi_processes import fit_from_model, convert_sh_basis
-from scilpy.reconst.b_tensor_utils import generate_btensor_input
+from scilpy.io.utils import (add_overwrite_arg, add_processes_arg,
+                             add_sh_basis_args, add_skip_b0_check_arg,
+                             add_tolerance_arg, add_verbose_arg,
+                             assert_inputs_exist, assert_outputs_exist,
+                             parse_sh_basis_arg, assert_headers_compatible)
+from scilpy.reconst.fodf import (fit_from_model,
+                                 verify_failed_voxels_shm_coeff,
+                                 verify_frf_files)
+from scilpy.reconst.sh import convert_sh_basis, verify_data_vs_sh_order
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(description=__doc__,
                                 formatter_class=argparse.RawTextHelpFormatter)
 
     p.add_argument('in_wm_frf',
@@ -79,24 +90,22 @@
     p.add_argument(
         '--sh_order', metavar='int', default=8, type=int,
         help='SH order used for the CSD. (Default: 8)')
     p.add_argument(
         '--mask',
         help='Path to a binary mask. Only the data inside the '
              'mask will be used for computations and reconstruction.')
-    p.add_argument(
-        '--tolerance', type=int, default=20,
-        help='The tolerated gap between the b-values to '
-             'extract\nand the current b-value. [%(default)s]')
+    add_tolerance_arg(p)
+    add_skip_b0_check_arg(p, will_overwrite_with_min=False,
+                          b0_tol_name='--tolerance')
 
-    add_force_b0_arg(p)
     add_sh_basis_args(p)
     add_processes_arg(p)
-    add_overwrite_arg(p)
     add_verbose_arg(p)
+    add_overwrite_arg(p)
 
     p.add_argument(
         '--not_all', action='store_true',
         help='If set, only saves the files specified using the '
              'file flags. (Default: False)')
 
     g = p.add_argument_group(title='File flags')
@@ -116,252 +125,135 @@
     g.add_argument(
         '--vf_rgb', metavar='file', default='',
         help='Output filename for the volume fractions map in rgb.')
 
     return p
 
 
-def single_tensor_btensor(gtab, evals, b_delta, S0=1):
-    # This function should be moved to Dipy at some point
-
-    if b_delta > 1 or b_delta < -0.5:
-        msg = """The value of b_delta must be between -0.5 and 1."""
-        raise ValueError(msg)
-
-    out_shape = gtab.bvecs.shape[:gtab.bvecs.ndim - 1]
-    gradients = gtab.bvecs.reshape(-1, 3)
-
-    evals = np.asarray(evals)
-    D_iso = np.sum(evals) / 3.
-    D_para = evals[np.argmax(abs(evals - D_iso))]
-    D_perp = evals[np.argmin(abs(evals - D_iso))]
-    D_delta = (D_para - D_perp) / (3 * D_iso)
-
-    S = np.zeros(len(gradients))
-    for (i, g) in enumerate(gradients):
-        theta = np.arctan2(np.sqrt(g[0] ** 2 + g[1] ** 2), g[2])
-        P_2 = (3 * np.cos(theta) ** 2 - 1) / 2.
-        b = gtab.bvals[i]
-        S[i] = S0 * np.exp(-b * D_iso * (1 + 2 * b_delta * D_delta * P_2))
-
-    return S.reshape(out_shape)
-
-
-def multi_shell_fiber_response(sh_order, bvals, wm_rf, gm_rf, csf_rf,
-                               b_deltas=None, sphere=None, tol=20):
-    # This function should be moved to Dipy at some point
-
-    bvals = np.array(bvals, copy=True)
-
-    n = np.arange(0, sh_order + 1, 2)
-    m = np.zeros_like(n)
-
-    if sphere is None:
-        sphere = default_sphere
-
-    big_sphere = sphere.subdivide()
-    theta, phi = big_sphere.theta, big_sphere.phi
-
-    B = shm.real_sh_descoteaux_from_index(m, n, theta[:, None], phi[:, None])
-    A = shm.real_sh_descoteaux_from_index(0, 0, 0, 0)
-
-    if b_deltas is None:
-        b_deltas = np.ones(len(bvals) - 1)
-
-    response = np.empty([len(bvals), len(n) + 2])
-
-    if bvals[0] < tol:
-        gtab = GradientTable(big_sphere.vertices * 0)
-        wm_response = single_tensor_btensor(gtab, wm_rf[0, :3], 1, wm_rf[0, 3])
-        response[0, 2:] = np.linalg.lstsq(B, wm_response, rcond=None)[0]
-
-        response[0, 1] = gm_rf[0, 3] / A
-        response[0, 0] = csf_rf[0, 3] / A
-        for i, bvalue in enumerate(bvals[1:]):
-            gtab = GradientTable(big_sphere.vertices * bvalue)
-            wm_response = single_tensor_btensor(gtab, wm_rf[i, :3],
-                                                b_deltas[i],
-                                                wm_rf[i, 3])
-            response[i+1, 2:] = np.linalg.lstsq(B, wm_response, rcond=None)[0]
-
-            response[i+1, 1] = gm_rf[i, 3] * np.exp(-bvalue * gm_rf[i, 0]) / A
-            response[i+1, 0] = csf_rf[i, 3] * np.exp(-bvalue
-                                                     * csf_rf[i, 0]) / A
-
-        S0 = [csf_rf[0, 3], gm_rf[0, 3], wm_rf[0, 3]]
-
-    else:
-        logging.warning('No b0 was given. Proceeding either way.')
-        for i, bvalue in enumerate(bvals):
-            gtab = GradientTable(big_sphere.vertices * bvalue)
-            wm_response = single_tensor_btensor(gtab, wm_rf[i, :3],
-                                                b_deltas[i],
-                                                wm_rf[i, 3])
-            response[i, 2:] = np.linalg.lstsq(B, wm_response, rcond=None)[0]
-
-            response[i, 1] = gm_rf[i, 3] * np.exp(-bvalue * gm_rf[i, 0]) / A
-            response[i, 0] = csf_rf[i, 3] * np.exp(-bvalue * csf_rf[i, 0]) / A
-
-        S0 = [csf_rf[0, 3], gm_rf[0, 3], wm_rf[0, 3]]
-
-    return MultiShellResponse(response, sh_order, bvals, S0=S0)
-
-
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    if args.verbose:
-        logging.basicConfig(level=logging.DEBUG)
-    else:
-        logging.basicConfig(level=logging.INFO)
-
+    # Verifications
     if not args.not_all:
         args.wm_out_fODF = args.wm_out_fODF or 'wm_fodf.nii.gz'
         args.gm_out_fODF = args.gm_out_fODF or 'gm_fodf.nii.gz'
         args.csf_out_fODF = args.csf_out_fODF or 'csf_fodf.nii.gz'
         args.vf = args.vf or 'vf.nii.gz'
         args.vf_rgb = args.vf_rgb or 'vf_rgb.nii.gz'
 
     arglist = [args.wm_out_fODF, args.gm_out_fODF, args.csf_out_fODF,
                args.vf, args.vf_rgb]
     if args.not_all and not any(arglist):
-        parser.error('When using --not_all, you need to specify at least ' +
+        parser.error('When using --not_all, you need to specify at least '
                      'one file to output.')
 
-    assert_inputs_exist(parser, [],
-                        optional=list(np.concatenate((args.in_dwis,
-                                                      args.in_bvals,
-                                                      args.in_bvecs))))
+    required = args.in_dwis + args.in_bvals + args.in_bvecs
+    required += [args.in_wm_frf, args.in_gm_frf, args.in_csf_frf]
+    assert_inputs_exist(parser, required, optional=args.mask)
     assert_outputs_exist(parser, args, arglist)
+    assert_headers_compatible(parser, args.in_dwis, args.mask)
 
     if not (len(args.in_dwis) == len(args.in_bvals)
             == len(args.in_bvecs) == len(args.in_bdeltas)):
-        msg = """The number of given dwis, bvals, bvecs and bdeltas must be the
-              same. Please verify that all inputs were correctly inserted."""
-        raise ValueError(msg)
+        parser.error("The number of given dwis, bvals, bvecs and bdeltas must "
+                     "be the same. Please verify that all inputs were "
+                     "correctly inserted.")
 
+    # Loading data
     affine = extract_affine(args.in_dwis)
 
-    tol = args.tolerance
-    force_b0_thr = args.force_b0_threshold
-
     wm_frf = np.loadtxt(args.in_wm_frf)
     gm_frf = np.loadtxt(args.in_gm_frf)
     csf_frf = np.loadtxt(args.in_csf_frf)
 
-    gtab, data, ubvals, ubdeltas = generate_btensor_input(args.in_dwis,
-                                                          args.in_bvals,
-                                                          args.in_bvecs,
-                                                          args.in_bdeltas,
-                                                          force_b0_thr,
-                                                          tol=tol)
+    # Note. This script does not currently allow using a separate b0_threshold
+    # for the b0s. Using the tolerance. To change this, we would have to
+    # change generate_btensor_input. Not doing any verification on the
+    # bvals. Typically, we would use check_b0_threshold(bvals.min(), args)
+    gtab, data, ubvals, ubdeltas = generate_btensor_input(
+        args.in_dwis, args.in_bvals, args.in_bvecs, args.in_bdeltas,
+        tol=args.tolerance, skip_b0_check=args.skip_b0_check)
 
     # Checking mask
-    if args.mask is None:
-        mask = None
-    else:
-        mask = get_data_as_mask(nib.load(args.mask), dtype=bool)
-        if mask.shape != data.shape[:-1]:
-            raise ValueError("Mask is not the same shape as data.")
-
-    sh_order = args.sh_order
+    mask = get_data_as_mask(nib.load(args.mask),
+                            dtype=bool) if args.mask else None
 
     # Checking data and sh_order
-    if data.shape[-1] < (sh_order + 1) * (sh_order + 2) / 2:
-        logging.warning(
-            'We recommend having at least {} unique DWIs volumes, but you '
-            'currently have {} volumes. Try lowering the parameter --sh_order '
-            'in case of non convergence.'.format(
-                (sh_order + 1) * (sh_order + 2) / 2, data.shape[-1]))
-
-    # Checking response functions and computing msmt response function
-    if len(wm_frf.shape) == 1:
-        wm_frf = np.reshape(wm_frf, (1,) + wm_frf.shape)
-    if len(gm_frf.shape) == 1:
-        gm_frf = np.reshape(gm_frf, (1,) + gm_frf.shape)
-    if len(csf_frf.shape) == 1:
-        csf_frf = np.reshape(csf_frf, (1,) + csf_frf.shape)
-
-    if not wm_frf.shape[1] == 4:
-        raise ValueError('WM frf file did not contain 4 elements. '
-                         'Invalid or deprecated FRF format')
-    if not gm_frf.shape[1] == 4:
-        raise ValueError('GM frf file did not contain 4 elements. '
-                         'Invalid or deprecated FRF format')
-    if not csf_frf.shape[1] == 4:
-        raise ValueError('CSF frf file did not contain 4 elements. '
-                         'Invalid or deprecated FRF format')
+    verify_data_vs_sh_order(data, args.sh_order)
+    sh_basis, is_legacy = parse_sh_basis_arg(args)
 
-    memsmt_response = multi_shell_fiber_response(sh_order,
-                                                 ubvals,
-                                                 wm_frf, gm_frf, csf_frf,
-                                                 ubdeltas[1:],
-                                                 tol=tol)
+    # Checking response functions and computing mesmt response function
+    wm_frf, gm_frf, csf_frf = verify_frf_files(wm_frf, gm_frf, csf_frf)
 
+    # Loading spheres
     reg_sphere = get_sphere('symmetric362')
 
+    # Starting main process!
+
+    ubshapes = convert_bdelta_to_bshape(ubdeltas)
+    memsmt_response = multi_shell_fiber_response(args.sh_order, ubvals,
+                                                 wm_frf, gm_frf, csf_frf,
+                                                 tol=args.tolerance,
+                                                 btens=ubshapes)
+
     # Computing memsmt-CSD
     memsmt_model = MultiShellDeconvModel(gtab, memsmt_response,
                                          reg_sphere=reg_sphere,
-                                         sh_order=sh_order)
+                                         sh_order_max=args.sh_order)
 
     # Computing memsmt-CSD fit
     memsmt_fit = fit_from_model(memsmt_model, data,
                                 mask=mask, nbr_processes=args.nbr_processes)
 
+    # memsmt_fit is a MultiVoxelFit.
+    #   - memsmt_fit.array_fit is a 3D np.ndarray, where value in each voxel is
+    #     a dipy.reconst.mcsd.MSDeconvFit object.
+    #   - When accessing memsmt_fit.all_shm_coeff, we get an array of shape
+    #     (x, y, z, n), where n is the number of fitted values.
     shm_coeff = memsmt_fit.all_shm_coeff
-
-    nan_count = len(np.argwhere(np.isnan(shm_coeff[..., 0])))
-    voxel_count = np.prod(shm_coeff.shape[:-1])
-
-    if nan_count / voxel_count >= 0.05:
-        msg = """There are {} voxels out of {} that could not be solved by
-        the solver, reaching a critical amount of voxels. Make sure to tune the
-        response functions properly, as the solving process is very sensitive
-        to it. Proceeding to fill the problematic voxels by 0.
-        """
-        logging.warning(msg.format(nan_count, voxel_count))
-    elif nan_count > 0:
-        msg = """There are {} voxels out of {} that could not be solved by
-        the solver. Make sure to tune the response functions properly, as the
-        solving process is very sensitive to it. Proceeding to fill the
-        problematic voxels by 0.
-        """
-        logging.warning(msg.format(nan_count, voxel_count))
-
-    shm_coeff = np.where(np.isnan(shm_coeff), 0, shm_coeff)
+    shm_coeff = verify_failed_voxels_shm_coeff(shm_coeff)
 
     vf = memsmt_fit.volume_fractions
     vf = np.where(np.isnan(vf), 0, vf)
 
     # Saving results
     if args.wm_out_fODF:
         wm_coeff = shm_coeff[..., 2:]
-        if args.sh_basis == 'tournier07':
-            wm_coeff = convert_sh_basis(wm_coeff, reg_sphere, mask=mask,
-                                        nbr_processes=args.nbr_processes)
+        wm_coeff = convert_sh_basis(wm_coeff, reg_sphere, mask=mask,
+                                    input_basis='descoteaux07',
+                                    output_basis=sh_basis,
+                                    is_input_legacy=True,
+                                    is_output_legacy=is_legacy,
+                                    nbr_processes=args.nbr_processes)
         nib.save(nib.Nifti1Image(wm_coeff.astype(np.float32),
                                  affine), args.wm_out_fODF)
 
     if args.gm_out_fODF:
         gm_coeff = shm_coeff[..., 1]
-        if args.sh_basis == 'tournier07':
-            gm_coeff = gm_coeff.reshape(gm_coeff.shape + (1,))
-            gm_coeff = convert_sh_basis(gm_coeff, reg_sphere, mask=mask,
-                                        nbr_processes=args.nbr_processes)
+        gm_coeff = gm_coeff.reshape(gm_coeff.shape + (1,))
+        gm_coeff = convert_sh_basis(gm_coeff, reg_sphere, mask=mask,
+                                    input_basis='descoteaux07',
+                                    output_basis=sh_basis,
+                                    is_input_legacy=True,
+                                    is_output_legacy=is_legacy,
+                                    nbr_processes=args.nbr_processes)
         nib.save(nib.Nifti1Image(gm_coeff.astype(np.float32),
                                  affine), args.gm_out_fODF)
 
     if args.csf_out_fODF:
         csf_coeff = shm_coeff[..., 0]
-        if args.sh_basis == 'tournier07':
-            csf_coeff = csf_coeff.reshape(csf_coeff.shape + (1,))
-            csf_coeff = convert_sh_basis(csf_coeff, reg_sphere, mask=mask,
-                                         nbr_processes=args.nbr_processes)
+        csf_coeff = csf_coeff.reshape(csf_coeff.shape + (1,))
+        csf_coeff = convert_sh_basis(csf_coeff, reg_sphere, mask=mask,
+                                     input_basis='descoteaux07',
+                                     output_basis=sh_basis,
+                                     is_input_legacy=True,
+                                     is_output_legacy=is_legacy,
+                                     nbr_processes=args.nbr_processes)
         nib.save(nib.Nifti1Image(csf_coeff.astype(np.float32),
                                  affine), args.csf_out_fODF)
 
     if args.vf:
         nib.save(nib.Nifti1Image(vf.astype(np.float32), affine), args.vf)
 
     if args.vf_rgb:
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_memsmt_frf.py` & `scilpy-2.0.0/scripts/scil_frf_memsmt.py`

 * *Files 6% similar despite different names*

```diff
@@ -25,36 +25,44 @@
 In the wm, we compute the response function in each voxels where
 the FA is superior at threshold_fa_wm.
 
 In the gm (or csf), we compute the response function in each voxels where
 the FA is below at threshold_fa_gm (or threshold_fa_csf) and where
 the MD is below threshold_md_gm (or threshold_md_csf).
 
+>>> scil_frf_memsmt.py wm_frf.txt gm_frf.txt csf_frf.txt --in_dwis LTE.nii.gz
+    PTE.nii.gz STE.nii.gz --in_bvals LTE.bval PTE.bval STE.bval --in_bvecs
+    LTE.bvec PTE.bvec STE.bvec --in_bdeltas 1 -0.5 0 --mask mask.nii.gz
+
 Based on P. Karan et al., Bridging the gap between constrained spherical
 deconvolution and diffusional variance decomposition via tensor-valued
 diffusion MRI. Medical Image Analysis (2022)
+
+Formerly: scil_compute_memsmt_frf.py
 """
 
 import argparse
 import logging
 
 import nibabel as nib
 import numpy as np
 
+from scilpy.dwi.utils import extract_dwi_shell
 from scilpy.image.utils import extract_affine
+from scilpy.io.btensor import generate_btensor_input
 from scilpy.io.image import get_data_as_mask
-from scilpy.io.utils import (add_force_b0_arg,
-                             add_overwrite_arg, add_verbose_arg,
-                             assert_inputs_exist, assert_outputs_exist)
+from scilpy.io.utils import (add_overwrite_arg, add_verbose_arg,
+                             assert_inputs_exist, assert_outputs_exist,
+                             assert_roi_radii_format, add_skip_b0_check_arg,
+                             add_tolerance_arg,
+                             assert_headers_compatible)
 from scilpy.reconst.frf import compute_msmt_frf
-from scilpy.utils.bvec_bval_tools import extract_dwi_shell
-from scilpy.reconst.b_tensor_utils import generate_btensor_input
 
 
-def buildArgsParser():
+def _build_arg_parser():
 
     p = argparse.ArgumentParser(description=__doc__,
                                 formatter_class=argparse.RawTextHelpFormatter)
 
     p.add_argument('out_wm_frf',
                    help='Path to the output WM frf file, in .txt format.')
     p.add_argument('out_gm_frf',
@@ -122,18 +130,17 @@
                         'selected. [%(default)s]')
 
     p.add_argument('--min_nvox',
                    default=100, type=int,
                    help='Minimal number of voxels needed for each tissue masks'
                         ' in order to \nproceed to frf estimation. '
                         '[%(default)s]')
-    p.add_argument('--tolerance',
-                   type=int, default=20,
-                   help='The tolerated gap between the b-values to '
-                        'extract and the current b-value. [%(default)s]')
+    add_tolerance_arg(p)
+    add_skip_b0_check_arg(p, will_overwrite_with_min=False,
+                          b0_tol_name='--tolerance')
     p.add_argument('--dti_bval_limit',
                    type=int, default=1200,
                    help='The highest b-value taken for the DTI model. '
                         '[%(default)s]')
     p.add_argument('--roi_radii',
                    default=[20], nargs='+', type=int,
                    help='If supplied, use those radii to select a cuboid roi '
@@ -158,107 +165,81 @@
                    help='Path to the output GM frf mask file, the voxels used '
                         'to compute the GM frf.')
     p.add_argument('--csf_frf_mask',
                    metavar='file', default='',
                    help='Path to the output CSF frf mask file, the voxels '
                         'used to compute the CSF frf.')
 
-    p.add_argument('--frf_table',
-                   metavar='file', default='',
-                   help='Path to the output frf table file. Saves the frf for '
-                        'each b-value, in .txt format.')
-
-    add_force_b0_arg(p)
-    add_overwrite_arg(p)
     add_verbose_arg(p)
+    add_overwrite_arg(p)
 
     return p
 
 
 def main():
-    parser = buildArgsParser()
+    parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    if args.verbose:
-        logging.basicConfig(level=logging.DEBUG)
-    else:
-        logging.basicConfig(level=logging.INFO)
-
-    assert_inputs_exist(parser, [],
-                        optional=list(np.concatenate((args.in_dwis,
-                                                      args.in_bvals,
-                                                      args.in_bvecs))))
+    masks = [args.mask, args.mask_wm, args.mask_gm, args.mask_csf]
+    assert_inputs_exist(parser, args.in_dwis + args.in_bvals + args.in_bvecs,
+                        optional=masks)
     assert_outputs_exist(parser, args, [args.out_wm_frf, args.out_gm_frf,
                                         args.out_csf_frf])
+    assert_headers_compatible(parser, args.in_dwis, masks)
 
     if not (len(args.in_dwis) == len(args.in_bvals)
             == len(args.in_bvecs) == len(args.in_bdeltas)):
         msg = """The number of given dwis, bvals, bvecs and bdeltas must be the
               same. Please verify that all inputs were correctly inserted."""
         raise ValueError(msg)
 
     affine = extract_affine(args.in_dwis)
 
-    if len(args.roi_radii) == 1:
-        roi_radii = args.roi_radii[0]
-    elif len(args.roi_radii) == 2:
-        parser.error('--roi_radii cannot be of size (2,).')
-    else:
-        roi_radii = args.roi_radii
-    roi_center = args.roi_center
+    roi_radii = assert_roi_radii_format(parser)
 
-    tol = args.tolerance
-    dti_lim = args.dti_bval_limit
-    force_b0_thr = args.force_b0_threshold
-
-    gtab, data, ubvals, ubdeltas = generate_btensor_input(args.in_dwis,
-                                                          args.in_bvals,
-                                                          args.in_bvecs,
-                                                          args.in_bdeltas,
-                                                          force_b0_thr,
-                                                          tol=tol)
+    # Note. This script does not currently allow using a separate b0_threshold
+    # for the b0s. Using the tolerance. To change this, we would have to
+    # change generate_btensor_input. Not doing any verification on the
+    # bvals. Typically, we would use check_b0_threshold(bvals.min(), args)
+    gtab, data, ubvals, ubdeltas = generate_btensor_input(
+        args.in_dwis, args.in_bvals, args.in_bvecs, args.in_bdeltas,
+        tol=args.tolerance, skip_b0_check=args.skip_b0_check)
 
-    if not np.all(ubvals <= dti_lim):
+    if not np.all(ubvals <= args.dti_bval_limit):
         if np.sum(ubdeltas == 1) > 0:
             dti_ubvals = ubvals[ubdeltas == 1]
         elif np.sum(ubdeltas == -0.5) > 0:
             dti_ubvals = ubvals[ubdeltas == -0.5]
         elif np.sum(ubdeltas == args.in_bdelta_custom) > 0:
             dti_ubvals = ubvals[ubdeltas == args.in_bdelta_custom]
         else:
             raise ValueError("No encoding available for DTI.")
         vol = nib.Nifti1Image(data, affine)
-        outputs = extract_dwi_shell(vol, gtab.bvals, gtab.bvecs,
-                                    dti_ubvals[dti_ubvals <= dti_lim],
-                                    tol=1)
-        indices_dti, data_dti, bvals_dti, bvecs_dti = outputs
-        # gtab_dti = gradient_table(np.squeeze(bvals_dti), bvecs_dti,
-        #                           btens=gtab.btens[indices_dti])
+        bvals_to_extract = dti_ubvals[dti_ubvals <= args.dti_bval_limit]
+        indices_dti, data_dti, bvals_dti, bvecs_dti = \
+            extract_dwi_shell(vol, gtab.bvals, gtab.bvecs,
+                              bvals_to_extract, tol=1)
+
         bvals_dti = np.squeeze(bvals_dti)
         btens_dti = gtab.btens[indices_dti]
     else:
         data_dti = None
         bvals_dti = None
         bvecs_dti = None
         btens_dti = None
 
-    mask = None
-    if args.mask is not None:
-        mask = get_data_as_mask(nib.load(args.mask), dtype=bool)
-        if mask.shape != data.shape[:-1]:
-            raise ValueError("Mask is not the same shape as data.")
-    mask_wm = None
-    mask_gm = None
-    mask_csf = None
-    if args.mask_wm:
-        mask_wm = get_data_as_mask(nib.load(args.mask_wm), dtype=bool)
-    if args.mask_gm:
-        mask_gm = get_data_as_mask(nib.load(args.mask_gm), dtype=bool)
-    if args.mask_csf:
-        mask_csf = get_data_as_mask(nib.load(args.mask_csf), dtype=bool)
+    mask = get_data_as_mask(nib.load(args.mask),
+                            dtype=bool) if args.mask else None
+    mask_wm = get_data_as_mask(nib.load(args.mask_wm),
+                               dtype=bool) if args.mask_wm else None
+    mask_gm = get_data_as_mask(nib.load(args.mask_gm),
+                               dtype=bool) if args.mask_gm else None
+    mask_csf = get_data_as_mask(nib.load(args.mask_csf),
+                                dtype=bool) if args.mask_csf else None
 
     responses, frf_masks = compute_msmt_frf(data, gtab.bvals, gtab.bvecs,
                                             btens=gtab.btens,
                                             data_dti=data_dti,
                                             bvals_dti=bvals_dti,
                                             bvecs_dti=bvecs_dti,
                                             btens_dti=btens_dti,
@@ -267,39 +248,24 @@
                                             fa_thr_wm=args.fa_thr_wm,
                                             fa_thr_gm=args.fa_thr_gm,
                                             fa_thr_csf=args.fa_thr_csf,
                                             md_thr_gm=args.md_thr_gm,
                                             md_thr_csf=args.md_thr_csf,
                                             min_nvox=args.min_nvox,
                                             roi_radii=roi_radii,
-                                            roi_center=roi_center,
-                                            tol=0,
-                                            force_b0_threshold=force_b0_thr)
+                                            roi_center=args.roi_center,
+                                            tol=0)
 
     masks_files = [args.wm_frf_mask, args.gm_frf_mask, args.csf_frf_mask]
     for mask, mask_file in zip(frf_masks, masks_files):
         if mask_file:
             nib.save(nib.Nifti1Image(mask.astype(np.uint8), vol.affine),
                      mask_file)
 
     frf_out = [args.out_wm_frf, args.out_gm_frf, args.out_csf_frf]
 
     for frf, response in zip(frf_out, responses):
         np.savetxt(frf, response)
 
-    if args.frf_table:
-        if ubvals[0] < tol:
-            bvals = ubvals[1:]
-        else:
-            bvals = ubvals
-        response_csf = responses[2]
-        response_gm = responses[1]
-        response_wm = responses[0]
-        iso_responses = np.concatenate((response_csf[:, :3],
-                                        response_gm[:, :3]), axis=1)
-        responses = np.concatenate((iso_responses, response_wm[:, :3]), axis=1)
-        frf_table = np.vstack((bvals, responses.T)).T
-        np.savetxt(args.frf_table, frf_table)
-
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_msmt_fodf.py` & `scilpy-2.0.0/scripts/scil_fodf_msmt.py`

 * *Files 11% similar despite different names*

```diff
@@ -11,34 +11,41 @@
 
 If --not_all is set, only the files specified explicitly by the flags
 will be output.
 
 Based on B. Jeurissen et al., Multi-tissue constrained spherical
 deconvolution for improved analysis of multi-shell diffusion
 MRI data. Neuroimage (2014)
+
+Formerly: scil_compute_msmt_fodf.py
 """
 
 import argparse
 import logging
 
 from dipy.core.gradients import gradient_table, unique_bvals_tolerance
 from dipy.data import get_sphere
 from dipy.io.gradients import read_bvals_bvecs
 from dipy.reconst.mcsd import MultiShellDeconvModel, multi_shell_fiber_response
 import nibabel as nib
 import numpy as np
 
+from scilpy.gradients.bvec_bval_tools import (check_b0_threshold,
+                                              normalize_bvecs,
+                                              is_normalized_bvecs)
 from scilpy.io.image import get_data_as_mask
-from scilpy.io.utils import (add_overwrite_arg, assert_inputs_exist,
-                             assert_outputs_exist, add_force_b0_arg,
-                             add_sh_basis_args, add_processes_arg,
-                             add_verbose_arg)
-from scilpy.reconst.multi_processes import fit_from_model, convert_sh_basis
-from scilpy.utils.bvec_bval_tools import (check_b0_threshold, normalize_bvecs,
-                                          is_normalized_bvecs)
+from scilpy.io.utils import (add_overwrite_arg, add_processes_arg,
+                             assert_inputs_exist, assert_outputs_exist,
+                             add_sh_basis_args, add_skip_b0_check_arg,
+                             add_verbose_arg, add_tolerance_arg,
+                             parse_sh_basis_arg, assert_headers_compatible)
+from scilpy.reconst.fodf import (fit_from_model,
+                                 verify_failed_voxels_shm_coeff,
+                                 verify_frf_files)
+from scilpy.reconst.sh import convert_sh_basis, verify_data_vs_sh_order
 
 
 def _build_arg_parser():
 
     p = argparse.ArgumentParser(description=__doc__,
                                 formatter_class=argparse.RawTextHelpFormatter)
 
@@ -58,20 +65,19 @@
     p.add_argument(
         '--sh_order', metavar='int', default=8, type=int,
         help='SH order used for the CSD. (Default: 8)')
     p.add_argument(
         '--mask', metavar='',
         help='Path to a binary mask. Only the data inside the '
              'mask will be used for computations and reconstruction.')
-
-    add_force_b0_arg(p)
+    add_tolerance_arg(p)
+    add_skip_b0_check_arg(p, will_overwrite_with_min=False,
+                          b0_tol_name='--tolerance')
     add_sh_basis_args(p)
     add_processes_arg(p)
-    add_overwrite_arg(p)
-    add_verbose_arg(p)
 
     p.add_argument(
         '--not_all', action='store_true',
         help='If set, only saves the files specified using the '
              'file flags. (Default: False)')
 
     g = p.add_argument_group(title='File flags')
@@ -88,150 +94,141 @@
     g.add_argument(
         '--vf', metavar='file', default='',
         help='Output filename for the volume fractions map.')
     g.add_argument(
         '--vf_rgb', metavar='file', default='',
         help='Output filename for the volume fractions map in rgb.')
 
+    add_verbose_arg(p)
+    add_overwrite_arg(p)
+
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
-    logging.getLogger().setLevel(logging.INFO)
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
+    # Verifications
     if not args.not_all:
         args.wm_out_fODF = args.wm_out_fODF or 'wm_fodf.nii.gz'
         args.gm_out_fODF = args.gm_out_fODF or 'gm_fodf.nii.gz'
         args.csf_out_fODF = args.csf_out_fODF or 'csf_fodf.nii.gz'
         args.vf = args.vf or 'vf.nii.gz'
         args.vf_rgb = args.vf_rgb or 'vf_rgb.nii.gz'
 
     arglist = [args.wm_out_fODF, args.gm_out_fODF, args.csf_out_fODF,
                args.vf, args.vf_rgb]
     if args.not_all and not any(arglist):
-        parser.error('When using --not_all, you need to specify at least ' +
+        parser.error('When using --not_all, you need to specify at least '
                      'one file to output.')
 
     assert_inputs_exist(parser, [args.in_dwi, args.in_bval, args.in_bvec,
                                  args.in_wm_frf, args.in_gm_frf,
-                                 args.in_csf_frf])
+                                 args.in_csf_frf], args.mask)
     assert_outputs_exist(parser, args, arglist)
+    assert_headers_compatible(parser, args.in_dwi, args.mask)
 
     # Loading data
     wm_frf = np.loadtxt(args.in_wm_frf)
     gm_frf = np.loadtxt(args.in_gm_frf)
     csf_frf = np.loadtxt(args.in_csf_frf)
     vol = nib.load(args.in_dwi)
     data = vol.get_fdata(dtype=np.float32)
     bvals, bvecs = read_bvals_bvecs(args.in_bval, args.in_bvec)
 
-    # Checking mask
-    if args.mask is None:
-        mask = None
-    else:
-        mask = get_data_as_mask(nib.load(args.mask), dtype=bool)
-        if mask.shape != data.shape[:-1]:
-            raise ValueError("Mask is not the same shape as data.")
-
-    sh_order = args.sh_order
-
     # Checking data and sh_order
-    b0_thr = check_b0_threshold(
-        args.force_b0_threshold, bvals.min(), bvals.min())
+    wm_frf, gm_frf, csf_frf = verify_frf_files(wm_frf, gm_frf, csf_frf)
+    verify_data_vs_sh_order(data, args.sh_order)
+    sh_basis, is_legacy = parse_sh_basis_arg(args)
 
-    if data.shape[-1] < (sh_order + 1) * (sh_order + 2) / 2:
-        logging.warning(
-            'We recommend having at least {} unique DWIs volumes, but you '
-            'currently have {} volumes. Try lowering the parameter --sh_order '
-            'in case of non convergence.'.format(
-                (sh_order + 1) * (sh_order + 2) / 2, data.shape[-1]))
+    # Checking mask
+    mask = get_data_as_mask(nib.load(args.mask),
+                            dtype=bool) if args.mask else None
 
     # Checking bvals, bvecs values and loading gtab
     if not is_normalized_bvecs(bvecs):
         logging.warning('Your b-vectors do not seem normalized...')
         bvecs = normalize_bvecs(bvecs)
-    gtab = gradient_table(bvals, bvecs, b0_threshold=b0_thr)
 
-    # Checking response functions and computing msmt response function
-    if not wm_frf.shape[1] == 4:
-        raise ValueError('WM frf file did not contain 4 elements. '
-                         'Invalid or deprecated FRF format')
-    if not gm_frf.shape[1] == 4:
-        raise ValueError('GM frf file did not contain 4 elements. '
-                         'Invalid or deprecated FRF format')
-    if not csf_frf.shape[1] == 4:
-        raise ValueError('CSF frf file did not contain 4 elements. '
-                         'Invalid or deprecated FRF format')
-    ubvals = unique_bvals_tolerance(bvals, tol=20)
-    msmt_response = multi_shell_fiber_response(sh_order, ubvals,
-                                               wm_frf, gm_frf, csf_frf)
+    # Note. This script does not currently allow using a separate b0_threshold
+    # for the b0s. Using the tolerance. To change this, we would have to
+    # change many things in dipy. An issue has been added in dipy to
+    # ask them to clarify the usage of gtab.b0s_mask. See here:
+    #  https://github.com/dipy/dipy/issues/3015
+    # b0_threshold option in gradient_table probably unused.
+    _ = check_b0_threshold(bvals.min(), b0_thr=args.tolerance,
+                           skip_b0_check=args.skip_b0_check)
+    gtab = gradient_table(bvals, bvecs, b0_threshold=args.tolerance)
 
     # Loading spheres
     reg_sphere = get_sphere('symmetric362')
 
+    # Starting main process!
+
+    # Checking response functions and computing msmt response function
+    ubvals = unique_bvals_tolerance(bvals, tol=args.tolerance)
+    msmt_response = multi_shell_fiber_response(args.sh_order, ubvals,
+                                               wm_frf, gm_frf, csf_frf,
+                                               tol=args.tolerance)
+
     # Computing msmt-CSD
     msmt_model = MultiShellDeconvModel(gtab, msmt_response,
                                        reg_sphere=reg_sphere,
-                                       sh_order=sh_order)
+                                       sh_order_max=args.sh_order)
 
     # Computing msmt-CSD fit
     msmt_fit = fit_from_model(msmt_model, data,
                               mask=mask, nbr_processes=args.nbr_processes)
 
+    # mmsmt_fit is a MultiVoxelFit.
+    #   - memsmt_fit.array_fit is a 3D np.ndarray, where value in each voxel is
+    #     a dipy.reconst.mcsd.MSDeconvFit object.
+    #   - When accessing memsmt_fit.all_shm_coeff, we get an array of shape
+    #     (x, y, z, n), where n is the number of fitted values.
     shm_coeff = msmt_fit.all_shm_coeff
-
-    nan_count = len(np.argwhere(np.isnan(shm_coeff[..., 0])))
-    voxel_count = np.prod(shm_coeff.shape[:-1])
-
-    if nan_count / voxel_count >= 0.05:
-        msg = """There are {} voxels out of {} that could not be solved by
-        the solver, reaching a critical amount of voxels. Make sure to tune the
-        response functions properly, as the solving process is very sensitive
-        to it. Proceeding to fill the problematic voxels by 0.
-        """
-        logging.warning(msg.format(nan_count, voxel_count))
-    elif nan_count > 0:
-        msg = """There are {} voxels out of {} that could not be solved by
-        the solver. Make sure to tune the response functions properly, as the
-        solving process is very sensitive to it. Proceeding to fill the
-        problematic voxels by 0.
-        """
-        logging.warning(msg.format(nan_count, voxel_count))
-
-    shm_coeff = np.where(np.isnan(shm_coeff), 0, shm_coeff)
+    shm_coeff = verify_failed_voxels_shm_coeff(shm_coeff)
 
     vf = msmt_fit.volume_fractions
     vf = np.where(np.isnan(vf), 0, vf)
 
     # Saving results
     if args.wm_out_fODF:
         wm_coeff = shm_coeff[..., 2:]
-        if args.sh_basis == 'tournier07':
-            wm_coeff = convert_sh_basis(wm_coeff, reg_sphere, mask=mask,
-                                        nbr_processes=args.nbr_processes)
+        wm_coeff = convert_sh_basis(wm_coeff, reg_sphere, mask=mask,
+                                    input_basis='descoteaux07',
+                                    output_basis=sh_basis,
+                                    is_input_legacy=True,
+                                    is_output_legacy=is_legacy,
+                                    nbr_processes=args.nbr_processes)
         nib.save(nib.Nifti1Image(wm_coeff.astype(np.float32),
                                  vol.affine), args.wm_out_fODF)
 
     if args.gm_out_fODF:
         gm_coeff = shm_coeff[..., 1]
-        if args.sh_basis == 'tournier07':
-            gm_coeff = gm_coeff.reshape(gm_coeff.shape + (1,))
-            gm_coeff = convert_sh_basis(gm_coeff, reg_sphere, mask=mask,
-                                        nbr_processes=args.nbr_processes)
+        gm_coeff = gm_coeff.reshape(gm_coeff.shape + (1,))
+        gm_coeff = convert_sh_basis(gm_coeff, reg_sphere, mask=mask,
+                                    input_basis='descoteaux07',
+                                    output_basis=sh_basis,
+                                    is_input_legacy=True,
+                                    is_output_legacy=is_legacy,
+                                    nbr_processes=args.nbr_processes)
         nib.save(nib.Nifti1Image(gm_coeff.astype(np.float32),
                                  vol.affine), args.gm_out_fODF)
 
     if args.csf_out_fODF:
         csf_coeff = shm_coeff[..., 0]
-        if args.sh_basis == 'tournier07':
-            csf_coeff = csf_coeff.reshape(csf_coeff.shape + (1,))
-            csf_coeff = convert_sh_basis(csf_coeff, reg_sphere, mask=mask,
-                                         nbr_processes=args.nbr_processes)
+        csf_coeff = csf_coeff.reshape(csf_coeff.shape + (1,))
+        csf_coeff = convert_sh_basis(csf_coeff, reg_sphere, mask=mask,
+                                     input_basis='descoteaux07',
+                                     output_basis=sh_basis,
+                                     is_input_legacy=True,
+                                     is_output_legacy=is_legacy,
+                                     nbr_processes=args.nbr_processes)
         nib.save(nib.Nifti1Image(csf_coeff.astype(np.float32),
                                  vol.affine), args.csf_out_fODF)
 
     if args.vf:
         nib.save(nib.Nifti1Image(vf.astype(np.float32),
                                  vol.affine), args.vf)
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_msmt_frf.py` & `scilpy-2.0.0/scripts/scil_frf_msmt.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,52 +1,53 @@
 #! /usr/bin/env python
 # -*- coding: utf-8 -*-
 
 """
-Compute response functions for multi-shell multi-tissue (MSMT)
-constrained spherical deconvolution from DWI data.
+Compute response functions for multi-shell multi-tissue (MSMT) constrained
+spherical deconvolution from DWI data.
 
 The script computes a response function for white-matter (wm),
 gray-matter (gm), csf and the mean b=0.
-
-In the wm, we compute the response function in each voxels where
-the FA is superior at threshold_fa_wm.
-
-In the gm (or csf), we compute the response function in each voxels where
-the FA is below at threshold_fa_gm (or threshold_fa_csf) and where
-the MD is below threshold_md_gm (or threshold_md_csf).
+    - In the wm, we compute the response function in each voxel where the FA is
+      superior at threshold_fa_wm.
+    - In the gm (or csf), we compute the response function in each voxel where
+      the FA is below at threshold_fa_gm (or threshold_fa_csf) and where the MD
+      is below threshold_md_gm (or threshold_md_csf).
 
 We output one response function file for each tissue, containing the response
 function for each b-value (arranged by lines). These are saved as the diagonal
 of the axis-symmetric diffusion tensor (3 e-values) and a mean b0 value.
-For example, a typical wm_frf is 15e-4 4e-4 4e-4 700, where the tensor e-values
-are (15,4,4)x10^-4 mm^2/s and the mean b0 is 700.
+For example, a typical wm_frf is [15e-4, 4e-4, 4e-4, 700], where the tensor
+e-values are (15,4,4)x10^-4 mm^2/s and the mean b0 is 700.
+
+Based on B. Jeurissen et al., Multi-tissue constrained spherical deconvolution
+for improved analysis of multi-shell diffusion MRI data. Neuroimage (2014)
 
-Based on B. Jeurissen et al., Multi-tissue constrained spherical
-deconvolution for improved analysis of multi-shell diffusion
-MRI data. Neuroimage (2014)
+Formerly: scil_compute_msmt_frf.py
 """
 
 import argparse
 import logging
 
 from dipy.core.gradients import unique_bvals_tolerance
 from dipy.io.gradients import read_bvals_bvecs
 import nibabel as nib
 import numpy as np
 
+from scilpy.dwi.utils import extract_dwi_shell
+from scilpy.gradients.bvec_bval_tools import check_b0_threshold
 from scilpy.io.image import get_data_as_mask
-from scilpy.io.utils import (add_force_b0_arg,
-                             add_overwrite_arg, add_verbose_arg,
-                             assert_inputs_exist, assert_outputs_exist)
+from scilpy.io.utils import (add_overwrite_arg, add_skip_b0_check_arg,
+                             add_verbose_arg, assert_inputs_exist,
+                             assert_outputs_exist, assert_roi_radii_format,
+                             assert_headers_compatible)
 from scilpy.reconst.frf import compute_msmt_frf
-from scilpy.utils.bvec_bval_tools import extract_dwi_shell
 
 
-def buildArgsParser():
+def _build_arg_parser():
 
     p = argparse.ArgumentParser(
         description=__doc__,
         formatter_class=argparse.RawDescriptionHelpFormatter)
 
     p.add_argument('in_dwi',
                    help='Path to the input diffusion volume.')
@@ -71,190 +72,152 @@
     p.add_argument('--mask_gm',
                    help='Path to the input GM mask file, used to improve the '
                         'final GM frf mask.')
     p.add_argument('--mask_csf',
                    help='Path to the input CSF mask file, used to improve the'
                         ' final CSF frf mask.')
 
-    p.add_argument('--fa_thr_wm',
-                   default=0.7, type=float,
+    p.add_argument('--fa_thr_wm', default=0.7, type=float,
                    help='If supplied, use this threshold to select single WM '
                         'fiber voxels from the FA inside the WM mask defined '
                         ' by mask_wm. Each voxel above this threshold will '
                         'be selected. [%(default)s]')
-    p.add_argument('--fa_thr_gm',
-                   default=0.2, type=float,
+    p.add_argument('--fa_thr_gm', default=0.2, type=float,
                    help='If supplied, use this threshold to select GM voxels '
                         'from the FA inside the GM mask defined by mask_gm. '
                         'Each voxel below this threshold will be selected.'
                         ' [%(default)s]')
-    p.add_argument('--fa_thr_csf',
-                   default=0.1, type=float,
+    p.add_argument('--fa_thr_csf', default=0.1, type=float,
                    help='If supplied, use this threshold to select CSF voxels '
                         'from the FA inside the CSF mask defined by mask_csf. '
                         'Each voxel below this threshold will be selected. '
                         '[%(default)s]')
-    p.add_argument('--md_thr_gm',
-                   default=0.0007, type=float,
+    p.add_argument('--md_thr_gm', default=0.0007, type=float,
                    help='If supplied, use this threshold to select GM voxels '
                         'from the MD inside the GM mask defined by mask_gm. '
                         'Each voxel below this threshold will be selected. '
                         '[%(default)s]')
-    p.add_argument('--md_thr_csf',
-                   default=0.003, type=float,
+    p.add_argument('--md_thr_csf', default=0.003, type=float,
                    help='If supplied, use this threshold to select CSF '
                         'voxels from the MD inside the CSF mask defined by '
                         'mask_csf. Each voxel below this threshold will be'
                         ' selected. [%(default)s]')
 
-    p.add_argument('--min_nvox',
-                   default=100, type=int,
+    p.add_argument('--min_nvox', default=100, type=int,
                    help='Minimal number of voxels needed for each tissue masks'
                         ' in order to proceed to frf estimation. '
                         '[%(default)s]')
-    p.add_argument('--tolerance',
-                   type=int, default=20,
+    p.add_argument('--tolerance', type=int, default=20,
                    help='The tolerated gap between the b-values to '
                         'extract and the current b-value. [%(default)s]')
-    p.add_argument('--dti_bval_limit',
-                   type=int, default=1200,
+    add_skip_b0_check_arg(p, will_overwrite_with_min=False,
+                          b0_tol_name='--tolerance')
+    p.add_argument('--dti_bval_limit', type=int, default=1200,
                    help='The highest b-value taken for the DTI model. '
                         '[%(default)s]')
-    p.add_argument('--roi_radii',
-                   default=[20], nargs='+', type=int,
+    p.add_argument('--roi_radii', default=[20], nargs='+', type=int,
                    help='If supplied, use those radii to select a cuboid roi '
                         'to estimate the response functions. The roi will be '
                         'a cuboid spanning from the middle of the volume in '
                         'each direction with the different radii. The type is '
                         'either an int (e.g. --roi_radii 10) or an array-like '
                         '(3,) (e.g. --roi_radii 20 30 10). [%(default)s]')
-    p.add_argument('--roi_center',
-                   metavar='tuple(3)', nargs=3, type=int,
+    p.add_argument('--roi_center', metavar='tuple(3)', nargs=3, type=int,
                    help='If supplied, use this center to span the cuboid roi '
                         'using roi_radii. [center of the 3D volume] '
                         '(e.g. --roi_center 66 79 79)')
 
-    p.add_argument('--wm_frf_mask',
-                   metavar='file', default='',
+    p.add_argument('--wm_frf_mask', metavar='file', default='',
                    help='Path to the output WM frf mask file, the voxels used '
                         'to compute the WM frf.')
-    p.add_argument('--gm_frf_mask',
-                   metavar='file', default='',
+    p.add_argument('--gm_frf_mask', metavar='file', default='',
                    help='Path to the output GM frf mask file, the voxels used '
                         'to compute the GM frf.')
-    p.add_argument('--csf_frf_mask',
-                   metavar='file', default='',
+    p.add_argument('--csf_frf_mask', metavar='file', default='',
                    help='Path to the output CSF frf mask file, the voxels '
                         'used to compute the CSF frf.')
 
-    p.add_argument('--frf_table',
-                   metavar='file', default='',
-                   help='Path to the output frf table file. Saves the frf for '
-                        'each b-value, in .txt format.')
-
-    add_force_b0_arg(p)
-    add_overwrite_arg(p)
     add_verbose_arg(p)
+    add_overwrite_arg(p)
 
     return p
 
 
 def main():
-
-    parser = buildArgsParser()
+    parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    log_level = logging.DEBUG if args.verbose else logging.INFO
-    logging.getLogger().setLevel(log_level)
-
-    assert_inputs_exist(parser, [args.in_dwi, args.in_bval, args.in_bvec])
+    # Verifications
+    masks = [args.mask, args.mask_wm, args.mask_gm, args.mask_csf]
+    assert_inputs_exist(parser, [args.in_dwi, args.in_bval, args.in_bvec],
+                        optional=masks)
     assert_outputs_exist(parser, args, [args.out_wm_frf, args.out_gm_frf,
                                         args.out_csf_frf])
+    assert_headers_compatible(parser, args.in_dwi, optional=masks)
 
-    if len(args.roi_radii) == 1:
-        roi_radii = args.roi_radii[0]
-    elif len(args.roi_radii) == 2:
-        parser.error('--roi_radii cannot be of size (2,).')
-    else:
-        roi_radii = args.roi_radii
-    roi_center = args.roi_center
+    roi_radii = assert_roi_radii_format(parser)
 
+    # Loading
     vol = nib.load(args.in_dwi)
     data = vol.get_fdata(dtype=np.float32)
     bvals, bvecs = read_bvals_bvecs(args.in_bval, args.in_bvec)
 
-    tol = args.tolerance
     dti_lim = args.dti_bval_limit
 
-    list_bvals = unique_bvals_tolerance(bvals, tol=tol)
+    # Note. This script does not currently allow using a separate b0_threshold
+    # for the b0s. Using the tolerance. To fix this, we would need to change
+    # the unique_bvals_tolerance and extract_dwi_shell methods.
+    _ = check_b0_threshold(bvals.min(), b0_thr=args.tolerance,
+                           skip_b0_check=args.skip_b0_check)
+    list_bvals = unique_bvals_tolerance(bvals, tol=args.tolerance)
     if not np.all(list_bvals <= dti_lim):
-        outputs = extract_dwi_shell(vol, bvals, bvecs,
-                                    list_bvals[list_bvals <= dti_lim],
-                                    tol=tol)
-        _, data_dti, bvals_dti, bvecs_dti = outputs
+        _, data_dti, bvals_dti, bvecs_dti = extract_dwi_shell(
+            vol, bvals, bvecs, list_bvals[list_bvals <= dti_lim],
+            tol=args.tolerance)
         bvals_dti = np.squeeze(bvals_dti)
     else:
         data_dti = None
         bvals_dti = None
         bvecs_dti = None
 
-    mask = None
-    if args.mask is not None:
-        mask = get_data_as_mask(nib.load(args.mask), dtype=bool)
-        if mask.shape != data.shape[:-1]:
-            raise ValueError("Mask is not the same shape as data.")
-    mask_wm = None
-    mask_gm = None
-    mask_csf = None
-    if args.mask_wm:
-        mask_wm = get_data_as_mask(nib.load(args.mask_wm), dtype=bool)
-    if args.mask_gm:
-        mask_gm = get_data_as_mask(nib.load(args.mask_gm), dtype=bool)
-    if args.mask_csf:
-        mask_csf = get_data_as_mask(nib.load(args.mask_csf), dtype=bool)
+    mask = get_data_as_mask(nib.load(args.mask),
+                            dtype=bool) if args.mask else None
+    mask_wm = get_data_as_mask(nib.load(args.mask_wm),
+                               dtype=bool) if args.mask_wm else None
+    mask_gm = get_data_as_mask(nib.load(args.mask_gm),
+                               dtype=bool) if args.mask_gm else None
+    mask_csf = get_data_as_mask(nib.load(args.mask_csf),
+                                dtype=bool) if args.mask_csf else None
 
-    force_b0_thr = args.force_b0_threshold
+    # Processing
     responses, frf_masks = compute_msmt_frf(data, bvals, bvecs,
                                             data_dti=data_dti,
                                             bvals_dti=bvals_dti,
                                             bvecs_dti=bvecs_dti,
                                             mask=mask, mask_wm=mask_wm,
                                             mask_gm=mask_gm, mask_csf=mask_csf,
                                             fa_thr_wm=args.fa_thr_wm,
                                             fa_thr_gm=args.fa_thr_gm,
                                             fa_thr_csf=args.fa_thr_csf,
                                             md_thr_gm=args.md_thr_gm,
                                             md_thr_csf=args.md_thr_csf,
                                             min_nvox=args.min_nvox,
                                             roi_radii=roi_radii,
-                                            roi_center=roi_center,
-                                            tol=tol,
-                                            force_b0_threshold=force_b0_thr)
+                                            roi_center=args.roi_center,
+                                            tol=args.tolerance)
 
+    # Saving
     masks_files = [args.wm_frf_mask, args.gm_frf_mask, args.csf_frf_mask]
     for mask, mask_file in zip(frf_masks, masks_files):
         if mask_file:
             nib.save(nib.Nifti1Image(mask.astype(np.uint8), vol.affine),
                      mask_file)
 
     frf_out = [args.out_wm_frf, args.out_gm_frf, args.out_csf_frf]
 
     for frf, response in zip(frf_out, responses):
         np.savetxt(frf, response)
 
-    if args.frf_table:
-        if list_bvals[0] < tol:
-            bvals = list_bvals[1:]
-        else:
-            bvals = list_bvals
-        response_csf = responses[2]
-        response_gm = responses[1]
-        response_wm = responses[0]
-        iso_responses = np.concatenate((response_csf[:, :3],
-                                        response_gm[:, :3]), axis=1)
-        responses = np.concatenate((iso_responses, response_wm[:, :3]), axis=1)
-        frf_table = np.vstack((bvals, responses.T)).T
-        np.savetxt(args.frf_table, frf_table)
-
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_pca.py` & `scilpy-2.0.0/scripts/scil_connectivity_compute_pca.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,39 +1,48 @@
 #!/usr/bin/env python
 # -*- coding: utf-8 -*-
 
 """
-Script to compute PCA analysis on diffusion metrics. Output returned is all significant principal components
-(e.g. presenting eigenvalues > 1) in a connectivity matrix format. This script can take into account all
-edges from every subject in a population or only non-zero edges across all subjects.
-
-The script can take directly as input a connectoflow output folder. Simply use the --input_connectoflow flag.
-For other type of folder input, the script expects a single folder containing all matrices for all subjects.
+Script to compute PCA analysis on diffusion metrics. Output returned is all
+significant principal components (e.g. presenting eigenvalues > 1) in a
+connectivity matrix format. This script can take into account all edges from
+every subject in a population or only non-zero edges across all subjects.
+
+The script can take directly as input a connectoflow output folder. Simply use
+the --input_connectoflow flag. For other type of folder input, the script
+expects a single folder containing all matrices for all subjects.
 Example:
         [in_folder]
         |--- sub-01_ad.npy
         |--- sub-01_md.npy
         |--- sub-02_ad.npy
         |--- sub-02_md.npy
         |--- ...
 
-The plots, tables and principal components matrices will be outputted in the designated folder from the
-<out_folder> argument. If you want to move back your principal components matrices in your connectoflow
-output, you can use a similar bash command for all principal components:
-for sub in `cat list_id.txt`; do cp out_folder/${sub}_PC1.npy connectoflow_output/$sub/Compute_Connectivity/; done
-
-Interpretation of resulting principal components can be done by evaluating the loadings values for each metrics.
-A value near 0 means that this metric doesn't contribute to this specific component whereas high positive or
-negative values mean a larger contribution. Components can then be labeled based on which metric contributes
-the highest. For example, a principal component showing a high loading for afd_fixel and near 0 loading for all
-other metrics can be interpreted as axonal density (see Gagnon et al. 2022 for this specific example or
-ref [3] for an introduction to PCA).
+The plots, tables and principal components matrices will be outputted in the
+designated folder from the <out_folder> argument. If you want to move back your
+principal components matrices in your connectoflow output, you can use a
+similar bash command for all principal components:
+for sub in `cat list_id.txt`;
+do
+    cp out_folder/${sub}_PC1.npy connectoflow_output/$sub/Compute_Connectivity/
+done
+
+Interpretation of resulting principal components can be done by evaluating the
+loadings values for each metrics. A value near 0 means that this metric doesn't
+contribute to this specific component whereas high positive or negative values
+mean a larger contribution. Components can then be labeled based on which
+metric contributes the highest. For example, a principal component showing a
+high loading for afd_fixel and near 0 loading for all other metrics can be
+interpreted as axonal density (see Gagnon et al. 2022 for this specific example
+or ref [3] for an introduction to PCA).
 
 EXAMPLE USAGE:
-scil_compute_pca.py input_folder/ output_folder/ --metrics ad fa md rd [...] --list_ids list_ids.txt
+scil_connectivity_compute_pca.py input_folder/ output_folder/
+    --metrics ad fa md rd [...] --list_ids list_ids.txt
 """
 
 # Import required libraries.
 import argparse
 import logging
 
 import matplotlib.pyplot as plt
@@ -46,44 +55,52 @@
                              save_matrix_in_any_format,
                              add_verbose_arg,
                              add_overwrite_arg,
                              assert_output_dirs_exist_and_empty)
 
 
 EPILOG = """
-[1] Chamberland M, Raven EP, Genc S, Duffy K, Descoteaux M, Parker GD, Tax CMW, Jones DK. Dimensionality 
-    reduction of diffusion MRI measures for improved tractometry of the human brain. Neuroimage. 2019 Oct 
-    15;200:89-100. doi: 10.1016/j.neuroimage.2019.06.020. Epub 2019 Jun 20. PMID: 31228638; PMCID: PMC6711466.
-[2] Gagnon A., Grenier G., Bocti C., Gillet V., Lepage J.-F., Baccarelli A. A., Posner J., Descoteaux M., 
-    & Takser L. (2022). White matter microstructural variability linked to differential attentional skills 
-    and impulsive behavior in a pediatric population. Cerebral Cortex. https://doi.org/10.1093/cercor/bhac180
+[1] Chamberland M, Raven EP, Genc S, Duffy K, Descoteaux M, Parker GD, Tax CMW,
+ Jones DK. Dimensionality reduction of diffusion MRI measures for improved
+ tractometry of the human brain. Neuroimage. 2019 Oct 15;200:89-100.
+ doi: 10.1016/j.neuroimage.2019.06.020. Epub 2019 Jun 20. PMID: 31228638;
+ PMCID: PMC6711466.
+[2] Gagnon A., Grenier G., Bocti C., Gillet V., Lepage J.-F., Baccarelli A. A.,
+ Posner J., Descoteaux M., Takser L. (2022). White matter microstructural
+ variability linked to differential attentional skills and impulsive behavior
+ in a pediatric population. Cerebral Cortex.
+ https://doi.org/10.1093/cercor/bhac180
 [3] https://towardsdatascience.com/what-are-pca-loadings-and-biplots-9a7897f2e559
     """
 
 
 # Build argument parser.
 def _build_arg_parser():
     p = argparse.ArgumentParser(
-        description=__doc__,
-        formatter_class=argparse.RawTextHelpFormatter,
+        description=__doc__, formatter_class=argparse.RawTextHelpFormatter,
         epilog=EPILOG)
 
     p.add_argument('in_folder',
-                   help='Path to the input folder.')
+                   help='Path to the input folder. See explanation above for '
+                        'its expected organization.')
     p.add_argument('out_folder',
-                   help='Path to the output folder to export graphs, tables and principal components matrices.')
+                   help='Path to the output folder to export graphs, tables '
+                        'and principal \ncomponents matrices.')
     p.add_argument('--metrics', nargs='+', required=True,
-                   help='Suffixes of all metrics to include in PCA analysis (ex: ad md fa rd). They must be '
-                        'immediately followed by the .npy extension.')
+                   help='Suffixes of all metrics to include in PCA analysis '
+                        '(ex: ad md fa rd). \nThey must be immediately '
+                        'followed by the .npy extension.')
     p.add_argument('--list_ids', required=True, metavar='FILE',
                    help='Path to a .txt file containing a list of all ids.')
     p.add_argument('--not_only_common', action='store_true',
-                   help='If true, will include all edges from all subjects and not only common edges (Not recommended)')
+                   help='If true, will include all edges from all subjects '
+                        'and not only \ncommon edges (Not recommended)')
     p.add_argument('--input_connectoflow', action='store_true',
-                   help='If true, script will assume the input folder is a Connectoflow output.')
+                   help='If true, script will assume the input folder is a '
+                        'Connectoflow output.')
 
     add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
@@ -176,17 +193,15 @@
 
     return dictionary
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
-
-    if args.verbose:
-        logging.getLogger().setLevel(logging.INFO)
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_output_dirs_exist_and_empty(parser, args, args.out_folder, create_dir=True)
 
     subjects = open(args.list_ids).read().split()
 
     if args.input_connectoflow:
         # Loading all matrix.
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_pft.py` & `scilpy-2.0.0/scripts/scil_tracking_pft.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,17 +1,18 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
 Local streamline HARDI tractography including Particle Filtering tracking.
 
+WARNING: This script DOES NOT support asymetric FODF input (aFODF).
+
 The tracking is done inside partial volume estimation maps and uses the
 particle filtering tractography (PFT) algorithm. See
-scil_compute_maps_for_particle_filter_tracking.py
-to generate PFT required maps.
+scil_tracking_pft_maps.py to generate PFT required maps.
 
 Streamlines longer than min_length and shorter than max_length are kept.
 The tracking direction is chosen in the aperture cone defined by the
 previous tracking direction and the angular constraint.
 Default parameters as suggested in [1].
 
 Algo 'det': the maxima of the spherical function (SF) the most closely aligned
@@ -19,14 +20,16 @@
 Algo 'prob': a direction drawn from the empirical distribution function defined
 from the SF.
 
 For streamline compression, a rule of thumb is to set it to 0.1mm for the
 deterministic algorithm and 0.2mm for probabilitic algorithm.
 
 All the input nifti files must be in isotropic resolution.
+
+Formerly: scil_compute_pft.py
 """
 
 import argparse
 import logging
 
 from dipy.data import get_sphere, HemiSphere
 from dipy.direction import (ProbabilisticDirectionGetter,
@@ -40,17 +43,19 @@
 from dipy.tracking.streamlinespeed import length, compress_streamlines
 import nibabel as nib
 from nibabel.streamlines import LazyTractogram
 import numpy as np
 
 from scilpy.io.image import get_data_as_mask
 from scilpy.io.utils import (add_overwrite_arg, add_sh_basis_args,
-                             add_verbose_arg,
-                             assert_inputs_exist, assert_outputs_exist)
-from scilpy.tracking.tools import get_theta
+                             add_verbose_arg, assert_inputs_exist,
+                             assert_outputs_exist, parse_sh_basis_arg,
+                             assert_headers_compatible, add_compression_arg,
+                             verify_compression_th)
+from scilpy.tracking.utils import get_theta
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__, formatter_class=argparse.RawTextHelpFormatter,
         epilog='References: [1] Girard, G., Whittingstall K., Deriche, R., '
                'and Descoteaux, M. (2014). Towards quantitative connectivity '
@@ -59,19 +64,21 @@
     p._optionals.title = 'Generic options'
 
     p.add_argument('in_sh',
                    help='Spherical harmonic file (.nii.gz).')
     p.add_argument('in_seed',
                    help='Seeding mask (.nii.gz).')
     p.add_argument('in_map_include',
-                   help='The probability map (.nii.gz) of ending the streamline\n'
-                        'and including it in the output (CMC, PFT [1])')
+                   help='The probability map (.nii.gz) of ending the\n'
+                        'streamline and including it in the output (CMC, PFT '
+                        '[1])')
     p.add_argument('map_exclude_file',
-                   help='The probability map (.nii.gz) of ending the streamline\n'
-                        'and excluding it in the output (CMC, PFT [1]).')
+                   help='The probability map (.nii.gz) of ending the\n'
+                        'streamline and excluding it in the output (CMC, PFT '
+                        '[1]).')
     p.add_argument('out_tractogram',
                    help='Tractogram output file (must be .trk or .tck).')
 
     track_g = p.add_argument_group('Tracking options')
     track_g.add_argument('--algo', default='prob', choices=['det', 'prob'],
                          help='Algorithm to use (must be "det" or "prob"). '
                               '[%(default)s]')
@@ -81,15 +88,15 @@
                          help='Minimum length of a streamline in mm. '
                               '[%(default)s]')
     track_g.add_argument('--max_length', type=float, default=300.,
                          help='Maximum length of a streamline in mm. '
                               '[%(default)s]')
     track_g.add_argument('--theta', type=float,
                          help='Maximum angle between 2 steps. '
-                         '["det"=45, "prob"=20]')
+                              '["det"=45, "prob"=20]')
     track_g.add_argument('--act', action='store_true',
                          help='If set, uses anatomically-constrained '
                               'tractography (ACT) \ninstead of continuous map '
                               'criterion (CMC).')
     track_g.add_argument('--sfthres', dest='sf_threshold',
                          type=float, default=0.1,
                          help='Spherical function relative threshold. '
@@ -107,72 +114,64 @@
     seed_sub_exclusive.add_argument('--npv', type=int,
                                     help='Number of seeds per voxel.')
     seed_sub_exclusive.add_argument('--nt', type=int,
                                     help='Total number of seeds to use.')
 
     pft_g = p.add_argument_group('PFT options')
     pft_g.add_argument('--particles', type=int, default=15,
-                       help='Number of particles to use for PFT. [%(default)s]')
+                       help='Number of particles to use for PFT. [%(default)s]'
+                       )
     pft_g.add_argument('--back', dest='back_tracking', type=float, default=2.,
                        help='Length of PFT back tracking (mm). [%(default)s]')
     pft_g.add_argument('--forward', dest='forward_tracking',
                        type=float, default=1.,
-                       help='Length of PFT forward tracking (mm). [%(default)s]')
+                       help='Length of PFT forward tracking (mm). '
+                            '[%(default)s]')
 
     out_g = p.add_argument_group('Output options')
-    out_g.add_argument('--compress', type=float,
-                       help='If set, will compress streamlines.\n'
-                            'The parameter value is the distance threshold.')
     out_g.add_argument('--all', dest='keep_all', action='store_true',
                        help='If set, keeps "excluded" streamlines.\n'
                             'NOT RECOMMENDED, except for debugging.')
     out_g.add_argument('--seed', type=int,
                        help='Random number generator seed.')
     add_overwrite_arg(out_g)
 
     out_g.add_argument('--save_seeds', action='store_true',
                        help='If set, save the seeds used for the tracking \n '
                             'in the data_per_streamline property.')
 
-    log_g = p.add_argument_group('Logging options')
-    add_verbose_arg(log_g)
+    add_compression_arg(out_g)
+    add_verbose_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    if args.verbose:
-        logging.getLogger().setLevel(logging.DEBUG)
-
-    assert_inputs_exist(parser, [args.in_sh, args.in_seed,
-                                 args.in_map_include,
-                                 args.map_exclude_file])
+    required = [args.in_sh, args.in_seed,
+                args.in_map_include, args.map_exclude_file]
+    assert_inputs_exist(parser, required)
     assert_outputs_exist(parser, args, args.out_tractogram)
+    assert_headers_compatible(parser, required)
 
     if not nib.streamlines.is_supported(args.out_tractogram):
         parser.error('Invalid output streamline file format (must be trk or ' +
                      'tck): {0}'.format(args.out_tractogram))
 
     if not args.min_length > 0:
         parser.error('minL must be > 0, {}mm was provided.'
                      .format(args.min_length))
     if args.max_length < args.min_length:
         parser.error('maxL must be > than minL, (minL={}mm, maxL={}mm).'
                      .format(args.min_length, args.max_length))
 
-    if args.compress:
-        if args.compress < 0.001 or args.compress > 1:
-            logging.warning(
-                'You are using an error rate of {}.\nWe recommend setting it '
-                'between 0.001 and 1.\n0.001 will do almost nothing to the '
-                'tracts while 1 will higly compress/linearize the tracts'
-                .format(args.compress))
+    verify_compression_th(args.compress_th)
 
     if args.particles <= 0:
         parser.error('--particles must be >= 1.')
 
     if args.back_tracking <= 0:
         parser.error('PFT backtracking distance must be > 0.')
 
@@ -193,15 +192,15 @@
 
     tracking_sphere = HemiSphere.from_sphere(get_sphere('repulsion724'))
 
     # Check if sphere is unit, since we couldn't find such check in Dipy.
     if not np.allclose(np.linalg.norm(tracking_sphere.vertices, axis=1), 1.):
         raise RuntimeError('Tracking sphere should be unit normed.')
 
-    sh_basis = args.sh_basis
+    sh_basis, is_legacy = parse_sh_basis_arg(args)
 
     if args.algo == 'det':
         dgklass = DeterministicMaximumDirectionGetter
     else:
         dgklass = ProbabilisticDirectionGetter
 
     theta = get_theta(args.theta, args.algo)
@@ -211,29 +210,32 @@
     # relative_peak_threshold is for initial directions filtering
     # min_separation_angle is the initial separation angle for peak extraction
     dg = dgklass.from_shcoeff(
         fodf_sh_img.get_fdata(dtype=np.float32),
         max_angle=theta,
         sphere=tracking_sphere,
         basis_type=sh_basis,
+        legacy=is_legacy,
         pmf_threshold=args.sf_threshold,
         relative_peak_threshold=args.sf_threshold_init)
 
     map_include_img = nib.load(args.in_map_include)
     map_exclude_img = nib.load(args.map_exclude_file)
     voxel_size = np.average(map_include_img.header['pixdim'][1:4])
 
     if not args.act:
-        tissue_classifier = CmcStoppingCriterion(map_include_img.get_fdata(dtype=np.float32),
-                                                 map_exclude_img.get_fdata(dtype=np.float32),
-                                                 step_size=args.step_size,
-                                                 average_voxel_size=voxel_size)
+        tissue_classifier = CmcStoppingCriterion(
+            map_include_img.get_fdata(dtype=np.float32),
+            map_exclude_img.get_fdata(dtype=np.float32),
+            step_size=args.step_size,
+            average_voxel_size=voxel_size)
     else:
-        tissue_classifier = ActStoppingCriterion(map_include_img.get_fdata(dtype=np.float32),
-                                                 map_exclude_img.get_fdata(dtype=np.float32))
+        tissue_classifier = ActStoppingCriterion(
+            map_include_img.get_fdata(dtype=np.float32),
+            map_exclude_img.get_fdata(dtype=np.float32))
 
     if args.npv:
         nb_seeds = args.npv
         seed_per_vox = True
     elif args.nt:
         nb_seeds = args.nt
         seed_per_vox = False
@@ -280,17 +282,17 @@
         data_per_streamlines = {'seeds': lambda: seeds}
     else:
         filtered_streamlines = \
             (s for s in pft_streamlines
              if scaled_min_length <= length(s) <= scaled_max_length)
         data_per_streamlines = {}
 
-    if args.compress:
+    if args.compress_th:
         filtered_streamlines = (
-            compress_streamlines(s, args.compress)
+            compress_streamlines(s, args.compress_th)
             for s in filtered_streamlines)
 
     tractogram = LazyTractogram(lambda: filtered_streamlines,
                                 data_per_streamlines,
                                 affine_to_rasmm=seed_img.affine)
 
     filetype = nib.streamlines.detect_format(args.out_tractogram)
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_powder_average.py` & `scilpy-2.0.0/scripts/scil_dwi_powder_average.py`

 * *Files 7% similar despite different names*

```diff
@@ -4,37 +4,36 @@
 """
 Script to compute powder average (mean diffusion weighted image) from set of
 diffusion images.
 
 By default will output an average image calculated from all images with
 non-zero bvalue.
 
-specify --bvalue to output an image for a single shell
+Specify --bvalue to output an image for a single shell
 
 Script currently does not take into account the diffusion gradient directions
 being averaged.
+
+Formerly: scil_compute_powder_average.py
 """
 
 import argparse
 import logging
 
 import nibabel as nib
 
 import numpy as np
 
+from dipy.core.gradients import get_bval_indices
 from dipy.io.gradients import read_bvals_bvecs
 
-from scilpy.io.image import (get_data_as_mask, assert_same_resolution)
+from scilpy.io.image import get_data_as_mask
 from scilpy.io.utils import (add_overwrite_arg, assert_inputs_exist,
-                             assert_outputs_exist, add_verbose_arg)
-
-from scilpy.utils.bvec_bval_tools import get_shell_indices
-
-logger = logging.getLogger("Compute_Powder_Average")
-logger.setLevel(logging.INFO)
+                             assert_outputs_exist, add_verbose_arg,
+                             assert_headers_compatible)
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__,
         formatter_class=argparse.RawTextHelpFormatter)
     p.add_argument('in_dwi',
@@ -70,33 +69,25 @@
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
-    inputs = [args.in_dwi, args.in_bval]
-    if args.mask:
-        inputs.append(args.mask)
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, inputs)
+    assert_inputs_exist(parser, [args.in_dwi, args.in_bval], args.mask)
     assert_outputs_exist(parser, args, args.out_avg)
-
-    if args.verbose:
-        logging.getLogger().setLevel(logging.DEBUG)
+    assert_headers_compatible(parser, args.in_dwi, args.mask)
 
     img = nib.load(args.in_dwi)
     data = img.get_fdata(dtype=np.float32)
     affine = img.affine
-    if args.mask is None:
-        mask = None
-    else:
-        mask_img = nib.load(args.mask)
-        assert_same_resolution((img, mask_img))
-        mask = get_data_as_mask(mask_img, dtype='uint8')
+    mask = get_data_as_mask(nib.load(args.mask),
+                            dtype='uint8') if args.mask else None
 
     # Read bvals (bvecs not needed at this point)
     logging.info('Performing powder average')
     bvals, _ = read_bvals_bvecs(args.in_bval, None)
 
     # Select diffusion volumes to average
     if not (args.shells):
@@ -109,22 +100,22 @@
         pwd_avg_idx = []
         logging.debug('Calculating powder average from {} '
                       'shells {}'.format(len(args.shells), args.shells))
 
         for shell in args.shells:
             pwd_avg_idx = np.int64(
                 np.concatenate((pwd_avg_idx,
-                                get_shell_indices(bvals,
-                                                  shell,
-                                                  tol=args.shell_thr))))
+                                get_bval_indices(bvals,
+                                                 shell,
+                                                 tol=args.shell_thr))))
             logging.debug('{} b{} volumes detected and included'.format(
                 len(pwd_avg_idx), shell))
 
         # remove b0 indices
-        b0_idx = get_shell_indices(bvals, 0, args.b0_thr)
+        b0_idx = get_bval_indices(bvals, 0, args.b0_thr)
         logging.debug('{} b0 volumes detected and not included'.format(
             len(b0_idx)))
         for val in b0_idx:
             pwd_avg_idx = pwd_avg_idx[pwd_avg_idx != val]
 
     if len(pwd_avg_idx) == 0:
         raise ValueError('No shells selected for powder average, ensure '
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_qball_metrics.py` & `scilpy-2.0.0/scripts/scil_qball_metrics.py`

 * *Files 14% similar despite different names*

```diff
@@ -4,39 +4,45 @@
 """
 Script to compute the Constant Solid Angle (CSA) or Analytical Q-ball model,
 the generalized fractional anisotropy (GFA) and the peaks of the model.
 
 By default, will output all possible files, using default names. Specific names
 can be specified using the file flags specified in the "File flags" section.
 
-If --not_all is set, only the files specified explicitly by the flags
-will be output.
+If --not_all is set, only the files specified explicitly by the flags will be
+output.
 
 See [Descoteaux et al MRM 2007, Aganj et al MRM 2009] for details and
 [Cote et al MEDIA 2013] for quantitative comparisons.
+
+Formerly: scil_compute_qball_metrics.py
 """
 import argparse
 import logging
 
 import nibabel as nib
 import numpy as np
 
 from dipy.core.gradients import gradient_table
 from dipy.data import get_sphere
 from dipy.io import read_bvals_bvecs
 from dipy.direction.peaks import (peaks_from_model,
                                   reshape_peaks_for_visualization)
 from dipy.reconst.shm import QballModel, CsaOdfModel, anisotropic_power
-from scilpy.io.utils import (add_overwrite_arg, add_processes_arg,
-                             add_sh_basis_args, assert_inputs_exist,
-                             assert_outputs_exist, add_force_b0_arg,
-                             validate_nbr_processes)
+
+from scilpy.gradients.bvec_bval_tools import (check_b0_threshold,
+                                              is_normalized_bvecs,
+                                              normalize_bvecs)
 from scilpy.io.image import get_data_as_mask
-from scilpy.utils.bvec_bval_tools import (normalize_bvecs, is_normalized_bvecs,
-                                          check_b0_threshold)
+from scilpy.io.utils import (add_b0_thresh_arg, add_overwrite_arg,
+                             add_processes_arg, add_sh_basis_args,
+                             add_skip_b0_check_arg, add_verbose_arg,
+                             assert_inputs_exist, assert_outputs_exist,
+                             parse_sh_basis_arg, validate_nbr_processes,
+                             assert_headers_compatible)
 
 
 DEFAULT_SMOOTH = 0.006
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
@@ -60,105 +66,110 @@
                    help='If set, qball will be used as the odf reconstruction'
                         ' model instead of CSA.')
     p.add_argument('--not_all', action='store_true',
                    help='If set, will only save the files specified using the '
                         'following flags.')
 
     g = p.add_argument_group(title='File flags')
-    g.add_argument('--gfa', default='',
+    g.add_argument('--gfa',
                    help='Output filename for the generalized fractional '
                         'anisotropy [gfa.nii.gz].')
-    g.add_argument('--peaks', default='',
+    g.add_argument('--peaks',
                    help='Output filename for the extracted peaks '
                         '[peaks.nii.gz].')
-    g.add_argument('--peak_indices', default='',
+    g.add_argument('--peak_indices',
                    help='Output filename for the generated peaks '
                         'indices on the sphere [peaks_indices.nii.gz].')
-    g.add_argument('--sh', default='',
+    g.add_argument('--sh',
                    help='Output filename for the spherical harmonics '
                         'coefficients [sh.nii.gz].')
-    g.add_argument('--nufo', default='',
+    g.add_argument('--nufo',
                    help='Output filename for the NUFO map [nufo.nii.gz].')
-    g.add_argument('--a_power', default='',
+    g.add_argument('--a_power',
                    help='Output filename for the anisotropic power map'
                         '[anisotropic_power.nii.gz].')
 
-    add_force_b0_arg(p)
+    add_b0_thresh_arg(p)
+    add_skip_b0_check_arg(p, will_overwrite_with_min=True)
     add_sh_basis_args(p)
     add_processes_arg(p)
+    add_verbose_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     if not args.not_all:
         args.gfa = args.gfa or 'gfa.nii.gz'
         args.peaks = args.peaks or 'peaks.nii.gz'
         args.peak_indices = args.peak_indices or 'peaks_indices.nii.gz'
         args.sh = args.sh or 'sh.nii.gz'
         args.nufo = args.nufo or 'nufo.nii.gz'
         args.a_power = args.a_power or 'anisotropic_power.nii.gz'
 
     arglist = [args.gfa, args.peaks, args.peak_indices, args.sh, args.nufo,
                args.a_power]
     if args.not_all and not any(arglist):
-        parser.error('When using --not_all, you need to specify at least ' +
-                     'one file to output.')
+        parser.error('When using --not_all, you need to specify at least one '
+                     'file to output.')
 
-    assert_inputs_exist(parser, [args.in_dwi, args.in_bval, args.in_bvec])
-    assert_outputs_exist(parser, args, arglist)
-    validate_nbr_processes(parser, args)
+    assert_inputs_exist(parser, [args.in_dwi, args.in_bval, args.in_bvec],
+                        args.mask)
+    assert_outputs_exist(parser, args, [], optional=arglist)
+    assert_headers_compatible(parser, args.in_dwi, args.mask)
 
-    nbr_processes = args.nbr_processes
+    nbr_processes = validate_nbr_processes(parser, args)
     parallel = nbr_processes > 1
 
     # Load data
     img = nib.load(args.in_dwi)
     data = img.get_fdata(dtype=np.float32)
 
     bvals, bvecs = read_bvals_bvecs(args.in_bval, args.in_bvec)
 
     if not is_normalized_bvecs(bvecs):
-        logging.warning('Your b-vectors do not seem normalized...')
+        logging.warning('Your b-vectors do not seem normalized... Normalizing '
+                        'now.')
         bvecs = normalize_bvecs(bvecs)
 
-    check_b0_threshold(args, bvals.min())
-    gtab = gradient_table(bvals, bvecs, b0_threshold=bvals.min())
+    # Usage of gtab.b0s_mask in dipy's models is not very well documented, but
+    # we can see that it is indeed used.
+    args.b0_threshold = check_b0_threshold(bvals.min(),
+                                           b0_thr=args.b0_threshold,
+                                           skip_b0_check=args.skip_b0_check)
+    gtab = gradient_table(bvals, bvecs, b0_threshold=args.b0_threshold)
 
     sphere = get_sphere('symmetric724')
+    sh_basis, is_legacy = parse_sh_basis_arg(args)
 
-    mask = None
-    if args.mask:
-        mask = get_data_as_mask(nib.load(args.mask))
-
-        # Sanity check on shape of mask
-        if mask.shape != data.shape[:-1]:
-            raise ValueError('Mask shape does not match data shape.')
+    mask = get_data_as_mask(nib.load(args.mask)) if args.mask else None
 
     if args.use_qball:
-        model = QballModel(gtab, sh_order=args.sh_order,
+        model = QballModel(gtab, sh_order_max=args.sh_order,
                            smooth=DEFAULT_SMOOTH)
     else:
-        model = CsaOdfModel(gtab, sh_order=args.sh_order,
+        model = CsaOdfModel(gtab, sh_order_max=args.sh_order,
                             smooth=DEFAULT_SMOOTH)
 
     odfpeaks = peaks_from_model(model=model,
                                 data=data,
                                 sphere=sphere,
                                 relative_peak_threshold=.5,
                                 min_separation_angle=25,
                                 mask=mask,
                                 return_odf=False,
                                 normalize_peaks=True,
                                 return_sh=True,
-                                sh_order=int(args.sh_order),
-                                sh_basis_type=args.sh_basis,
+                                sh_order_max=int(args.sh_order),
+                                sh_basis_type=sh_basis,
+                                legacy=is_legacy,
                                 npeaks=5,
                                 parallel=parallel,
                                 num_processes=nbr_processes)
 
     if args.gfa:
         nib.save(nib.Nifti1Image(odfpeaks.gfa.astype(np.float32), img.affine),
                  args.gfa)
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_qbx.py` & `scilpy-2.0.0/scripts/scil_tractogram_qbx.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-    Compute clusters using QuickBundlesX and save them separately.
-    We cannot know the number of clusters in advance.
+Compute clusters using QuickBundlesX and save them separately.
+We cannot know the number of clusters in advance.
+
+Formerly: scil_compute_qbx.py
 """
 
 import argparse
 import logging
 from operator import itemgetter
 import os
 
@@ -41,33 +43,31 @@
                    help='Streamlines will be resampled to have this '
                         'number of points [%(default)s].')
     p.add_argument('--out_centroids',
                    help='Output tractogram filename.\n'
                         'Format must be readable by the Nibabel API.')
 
     add_reference_arg(p)
-    add_overwrite_arg(p)
     add_verbose_arg(p)
+    add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, args.in_tractogram)
+    assert_inputs_exist(parser, args.in_tractogram, args.reference)
     assert_outputs_exist(parser, args, [], optional=args.out_centroids)
     assert_output_dirs_exist_and_empty(parser, args,
                                        args.out_clusters_dir,
                                        create_dir=True)
 
-    if args.verbose:
-        logging.getLogger().setLevel(logging.INFO)
-
     sft = load_tractogram_with_reference(parser, args, args.in_tractogram)
     streamlines = sft.streamlines
     thresholds = [40, 30, 20, args.dist_thresh]
     clusters = qbx_and_merge(streamlines, thresholds,
                              nb_pts=args.nb_points, verbose=False)
 
     if args.verbose:
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_rish_from_sh.py` & `scilpy-2.0.0/scripts/scil_sh_to_rish.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,69 +1,77 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-Compute the RISH (Rotationally Invariant Spherical Harmonics) features
-of an SH signal [1].
+Compute the RISH (Rotationally Invariant Spherical Harmonics) features of an SH
+signal [1].
 
-Each RISH feature map is the total energy of its
-associated order. Mathematically, it is the sum of the squared SH
-coefficients of the SH order.
+Each RISH feature map is the total energy of its associated order.
+Mathematically, it is the sum of the squared SH coefficients of the SH order.
 
 This script supports both symmetrical and asymmetrical SH images as input, of
 any SH order.
 
 Each RISH feature will be saved as a separate file.
 
 [1] Mirzaalian, Hengameh, et al. "Harmonizing diffusion MRI data across
 multiple sites and scanners." MICCAI 2015.
 https://scholar.harvard.edu/files/hengameh/files/miccai2015.pdf
+
+Formerly: scil_compute_rish_from_sh.py
 """
 import argparse
+import logging
 
 from dipy.reconst.shm import order_from_ncoef, sph_harm_ind_list
 import nibabel as nib
 import numpy as np
 
 from scilpy.io.image import get_data_as_mask
 from scilpy.io.utils import (add_overwrite_arg, assert_inputs_exist,
-                             assert_outputs_exist)
+                             assert_outputs_exist, add_verbose_arg,
+                             assert_headers_compatible)
 from scilpy.reconst.sh import compute_rish
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__, formatter_class=argparse.RawTextHelpFormatter)
     p.add_argument('in_sh',
-                   help='Path of the sh image. Must be a symmetric SH file.')
+                   help='Path of the sh image. They can be formatted in any '
+                        'sh basis, but we \nexpect it to be a symmetrical '
+                        'one. Else, provide --full_basis.')
     p.add_argument('out_prefix',
-                   help='Prefix of the output RISH files to save.')
+                   help='Prefix of the output RISH files to save. Suffixes '
+                        'will be \nbased on the sh orders.')
     p.add_argument('--full_basis', action="store_true",
                    help="Input SH image uses a full SH basis (asymmetrical).")
     p.add_argument('--mask',
                    help='Path to a binary mask.\nOnly data inside the mask '
                         'will be used for computation.')
 
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, args.in_sh, optional=args.mask)
+    assert_headers_compatible(parser, args.in_sh, optional=args.mask)
 
     # Load data
     sh_img = nib.load(args.in_sh)
     sh = sh_img.get_fdata(dtype=np.float32)
-    mask = None
-    if args.mask:
-        mask = get_data_as_mask(nib.load(args.mask), dtype=bool)
+    mask = get_data_as_mask(nib.load(args.mask),
+                            dtype=bool) if args.mask else None
 
     # Precompute output filenames to check if they exist
     sh_order = order_from_ncoef(sh.shape[-1], full_basis=args.full_basis)
     _, order_ids = sph_harm_ind_list(sh_order, full_basis=args.full_basis)
     orders = sorted(np.unique(order_ids))
     output_fnames = ["{}{}.nii.gz".format(args.out_prefix, i) for i in orders]
     assert_outputs_exist(parser, args, output_fnames)
@@ -72,12 +80,13 @@
     rish, final_orders = compute_rish(sh, mask, full_basis=args.full_basis)
 
     # Make sure the precomputed orders match the orders returned
     assert np.all(orders == np.array(final_orders))
 
     # Save each RISH feature as a separate file
     for i, fname in enumerate(output_fnames):
+        logging.info("Saving {}".format(fname))
         nib.save(nib.Nifti1Image(rish[..., i], sh_img.affine), fname)
 
 
 if __name__ == '__main__':
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_seed_by_labels.py` & `scilpy-2.0.0/scripts/scil_volume_stats_in_labels.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,80 +1,66 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 """
-Computes the information from the seeding map for each cortical region
-(corresponding to an atlas) associated with a specific bundle.
-Here we want to estimate the seeding attribution to cortical area
-affected by the bundle
+Computes the information from the input map for each cortical region
+(corresponding to an atlas).
+
+Hint: For instance, this script could be useful if you have a seed map from a
+specific bundle, to know from which regions it originated.
+
+Formerly: scil_compute_seed_by_labels.py
 """
 
 import argparse
 import json
+import logging
 
 import nibabel as nib
 import numpy as np
 
-from scilpy.image.labels import get_data_as_labels
-from scilpy.io.utils import add_overwrite_arg, assert_inputs_exist
+from scilpy.image.labels import get_data_as_labels, get_stats_in_label
+from scilpy.io.utils import (add_overwrite_arg, add_verbose_arg,
+                             assert_inputs_exist, assert_headers_compatible)
 
 
 def _build_arg_parser():
-    p = argparse.ArgumentParser(
-        description=__doc__,
-        formatter_class=argparse.RawTextHelpFormatter)
+    p = argparse.ArgumentParser(description=__doc__,
+                                formatter_class=argparse.RawTextHelpFormatter)
     p.add_argument('in_labels',
                    help='Path of the input label file.')
     p.add_argument('in_labels_lut',
                    help='Path of the LUT file corresponding to labels,'
                         'used to name the regions of interest.')
-    p.add_argument('in_seed_maps',
-                   help='Path of the input seed map file.')
+    p.add_argument('in_map',
+                   help='Path of the input map file. Expecting a 3D file.')
 
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    required = args.in_labels, args.in_seed_maps, args.in_labels_lut
-    assert_inputs_exist(parser, required)
+    # Verifications
+    assert_inputs_exist(parser,
+                        [args.in_labels, args.in_map, args.in_labels_lut])
+    assert_headers_compatible(parser, [args.in_labels, args.in_map])
 
-    # Load atlas image
-    label_img = nib.load(args.in_labels)
-    label_img_data = get_data_as_labels(label_img)
-
-    # Load atlas lut
+    # Loading
+    label_data = get_data_as_labels(nib.load(args.in_labels))
     with open(args.in_labels_lut) as f:
         label_dict = json.load(f)
-    (label_indices, label_names) = zip(*label_dict.items())
-
-    # Load seed image
-    seed_img = nib.load(args.in_seed_maps)
-    seed_img_data = seed_img.get_fdata(dtype=np.float32)
-
-    for label, name in zip(label_indices, label_names):
-        label = int(label)
-        if label != 0:
-            curr_data = (seed_img_data[np.where(label_img_data == label)])
-            nb_vx_roi = np.count_nonzero(label_img_data == label)
-            nb_seed_vx = np.count_nonzero(curr_data)
-
-            if nb_seed_vx != 0:
-                mean_seed = np.sum(curr_data)/nb_seed_vx
-                max_seed = np.max(curr_data)
-                std_seed = np.sqrt(np.mean(abs(curr_data[curr_data != 0] -
-                                               mean_seed)**2))
-
-                print(json.dumps({'ROI-idx': label,
-                                  'ROI-name': str(name),
-                                  'nb-vx-roi': int(nb_vx_roi),
-                                  'nb-vx-seed': int(nb_seed_vx),
-                                  'max': int(max_seed),
-                                  'mean': float(mean_seed),
-                                  'std': float(std_seed)}))
+    map_data = nib.load(args.in_map).get_fdata(dtype=np.float32)
+    if len(map_data.shape) > 3:
+        parser.error('Mask should be a 3D image.')
+
+    # Process
+    out_dict = get_stats_in_label(map_data, label_data, label_dict)
+    print(json.dumps(out_dict))
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_seed_density_map.py` & `scilpy-2.0.0/scripts/scil_tractogram_seed_density_map.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,22 +1,26 @@
 #! /usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
 Compute a density map of seeds saved in .trk file.
+
+Formerly: scil_compute_seed_density_map.py
 """
 
 import argparse
+import logging
 
 from dipy.io.streamline import load_tractogram
 from nibabel import Nifti1Image
 from nibabel.streamlines import detect_format, TrkFile
 import numpy as np
 
 from scilpy.io.utils import (add_bbox_arg,
+                             add_verbose_arg,
                              add_overwrite_arg,
                              assert_inputs_exist,
                              assert_outputs_exist)
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(description=__doc__,
@@ -28,25 +32,29 @@
                         'These seeds must be in space: voxel, origin: corner.')
     p.add_argument('seed_density_filename',
                    help='Output seed density filename. Format must be Nifti.')
     p.add_argument('--binary',
                    metavar='FIXED_VALUE', type=int, nargs='?', const=1,
                    help='If set, will store the same value for all intersected'
                         ' voxels, creating a binary map.\n'
-                        'When set without a value, 1 is used (and dtype uint8).\n'
-                        'If a value is given, will be used as the stored value.')
-    add_overwrite_arg(p)
+                        'When set without a value, 1 is used (and dtype '
+                        'uint8).\nIf a value is given, will be used as the '
+                        'stored value.')
+
     add_bbox_arg(p)
+    add_verbose_arg(p)
+    add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, [args.tractogram_filename])
     assert_outputs_exist(parser, args, [args.seed_density_filename])
 
     tracts_format = detect_format(args.tractogram_filename)
     if tracts_format is not TrkFile:
         raise ValueError("Invalid input streamline file format " +
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_sf_from_sh.py` & `scilpy-2.0.0/scripts/scil_sh_to_sf.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,34 +1,38 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
 Script to sample SF values from a Spherical Harmonics signal. Outputs a Nifti
-file with the SF values and an associated .bvec file with the chosen directions.
+file with the SF values and an associated .bvec file with the chosen
+directions.
 
 If converting from SH to a DWI-like SF volume, --in_bval and --in_b0 need
 to be provided to concatenate the b0 image to the SF, and to generate the new
 bvals file. Otherwise, no .bval file will be created.
+
+Formerly: scil_compute_sf_from_sh.py
 """
 
 import argparse
+import logging
 
 import nibabel as nib
 import numpy as np
 from dipy.core.gradients import gradient_table
 from dipy.core.sphere import Sphere
 from dipy.data import SPHERE_FILES, get_sphere
 from dipy.io import read_bvals_bvecs
 
-from scilpy.io.utils import (add_force_b0_arg, add_overwrite_arg,
-                             add_processes_arg, add_sh_basis_args,
-                             assert_inputs_exist,
-                             assert_outputs_exist, validate_nbr_processes)
-from scilpy.reconst.multi_processes import convert_sh_to_sf
-from scilpy.utils.bvec_bval_tools import (check_b0_threshold)
+from scilpy.gradients.bvec_bval_tools import DEFAULT_B0_THRESHOLD
+from scilpy.io.utils import (add_overwrite_arg, add_processes_arg,
+                             add_sh_basis_args, add_verbose_arg,
+                             assert_inputs_exist, assert_outputs_exist,
+                             parse_sh_basis_arg, validate_nbr_processes)
+from scilpy.reconst.sh import convert_sh_to_sf
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__, formatter_class=argparse.RawTextHelpFormatter)
     p.add_argument('in_sh',
                    help='Path of the SH volume.')
@@ -38,118 +42,152 @@
 
     # Sphere vs bvecs choice for SF
     directions = p.add_mutually_exclusive_group(required=True)
     directions.add_argument('--sphere',
                             choices=sorted(SPHERE_FILES.keys()),
                             help='Sphere used for the SH to SF projection. ')
     directions.add_argument('--in_bvec',
-                            help="Directions used for the SH to SF projection.")
+                            help="Directions used for the SH to SF "
+                            "projection. \nIf given, --in_bval must also be "
+                            "provided.")
 
     p.add_argument('--dtype', default="float32",
                    choices=["float32", "float64"],
                    help="Datatype to use for SF computation and output array."
                         "'[%(default)s]'")
 
     # Optional args for a DWI-like volume
     p.add_argument('--in_bval',
-                   help='b-value file, in FSL format, '
-                        'used to assign a b-value to the '
-                        'output SF and generate a `.bval` file.')
+                   help='b-value file, in FSL format, used to assign a '
+                        'b-value to the \noutput SF and generate a `.bval` '
+                        'file.\n'
+                        '- If used, --out_bval is required.\n'
+                        '- The output bval will contain one b-value per point '
+                        'in the SF \n  output (i.e. one per point on the '
+                        '--sphere or one per --in_bvec.)\n'
+                        '- The values of the output bval will all be set to '
+                        'the same b-value:\n  the average of your in_bval. '
+                        '(Any b0 found in this file, i.e \n  b-values under '
+                        '--b0_threshold, will be removed beforehand.)\n'
+                        '- To add b0s to both the SF volume and the '
+                        '--out_bval file, use --in_b0.')
     p.add_argument('--in_b0',
-                   help='b0 volume to concatenate to the '
-                        'final SF volume.')
+                   help='b0 volume to concatenate to the final SF volume.')
     p.add_argument('--out_bval',
                    help="Optional output bval file.")
     p.add_argument('--out_bvec',
                    help="Optional output bvec file.")
 
     p.add_argument('--b0_scaling', action="store_true",
-                   help="Scale resulting SF by the b0 image.")
+                   help="Scale resulting SF by the b0 image (--in_b0 must"
+                        "be given).")
 
     add_sh_basis_args(p)
     p.add_argument('--full_basis', action="store_true",
                    help="If true, use a full basis for the input SH "
                         "coefficients.")
 
-    add_processes_arg(p)
+    # Could use add_b0_thresh_arg(p), but adding manually to add more
+    # explanation in the text.
+    p.add_argument(
+        '--b0_threshold', type=float, default=DEFAULT_B0_THRESHOLD,
+        metavar='thr',
+        help='Threshold under which b-values are considered to be b0s.\n'
+             'Default if not set is {}.\n'
+             'This value is used with option --in_bval only: any b0 found in '
+             'the in_bval will be removed.'
+             .format(DEFAULT_B0_THRESHOLD))
 
+    add_processes_arg(p)
+    add_verbose_arg(p)
     add_overwrite_arg(p)
-    add_force_b0_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, args.in_sh,
                         optional=[args.in_bvec, args.in_bval, args.in_b0])
     assert_outputs_exist(parser, args, args.out_sf,
                          optional=[args.out_bvec, args.out_bval])
 
+    if args.in_bval:
+        if args.b0_threshold > DEFAULT_B0_THRESHOLD:
+            logging.warning(
+                'Your defined b0 threshold is {}. This is suspicious. '
+                'Typical b0_thresholds are no higher than {}.'
+                .format(args.b0_threshold, DEFAULT_B0_THRESHOLD))
+
     if (args.in_bval and not args.out_bval) or (
             args.out_bval and not args.in_bval):
         parser.error("--out_bval is required if --in_bval is provided, "
                      "and vice-versa.")
 
-    if args.in_bvec and not args.in_bval:
-        parser.error(
-            "--in_bval is required when using --in_bvec, in order to remove "
-            "bvecs corresponding to b0 images.")
-
     if args.b0_scaling and not args.in_b0:
         parser.error("--in_b0 is required when using --b0_scaling.")
 
     nbr_processes = validate_nbr_processes(parser, args)
+    sh_basis, is_legacy = parse_sh_basis_arg(args)
+
+    # Load bvecs / bvals, verify options.
+    bvals = None
+    bvecs = None
+    if args.in_bvec:
+        if not args.in_bval:
+            parser.error(
+                "--in_bval is required when using --in_bvec, in order to "
+                "remove bvecs corresponding to b0 images.")
+        bvals, bvecs = read_bvals_bvecs(args.in_bval, args.in_bvec)
+    elif args.in_bval:
+        bvals, _ = read_bvals_bvecs(args.in_bval, None)
 
     # Load SH
     vol_sh = nib.load(args.in_sh)
     data_sh = vol_sh.get_fdata(dtype=np.float32)
 
     # Sample SF from SH
     if args.sphere:
         sphere = get_sphere(args.sphere)
-    elif args.in_bvec:
-        bvals, bvecs = read_bvals_bvecs(args.in_bval, args.in_bvec)
-        gtab = gradient_table(bvals, bvecs, b0_threshold=bvals.min())
+    else:  # args.in_bvec is set.
+        gtab = gradient_table(bvals, bvecs, b0_threshold=args.b0_threshold)
         # Remove bvecs corresponding to b0 images
         bvecs = bvecs[np.logical_not(gtab.b0s_mask)]
         sphere = Sphere(xyz=bvecs)
 
     sf = convert_sh_to_sf(data_sh, sphere,
-                          input_basis=args.sh_basis,
+                          input_basis=sh_basis,
                           input_full_basis=args.full_basis,
+                          is_input_legacy=is_legacy,
                           dtype=args.dtype,
                           nbr_processes=nbr_processes)
     new_bvecs = sphere.vertices.astype(np.float32)
 
     # Assign bval to SF if --in_bval was provided
     new_bvals = []
     if args.in_bval:
-        # Load bvals
-        bvals, _ = read_bvals_bvecs(args.in_bval, None)
-
-        # Compute average bval
-        b0_thr = check_b0_threshold(
-            args.force_b0_threshold, bvals.min(), bvals.min())
-        b0s_mask = bvals <= b0_thr
+        # Compute average bval (except b0s), and create n out_bvals.
+        b0s_mask = bvals <= args.b0_threshold
         avg_bval = np.mean(bvals[np.logical_not(b0s_mask)])
 
         new_bvals = ([avg_bval] * len(sphere.theta))
 
     # Add b0 images to SF (and bvals if necessary) if --in_b0 was provided
     if args.in_b0:
         # Load b0
         vol_b0 = nib.load(args.in_b0)
         data_b0 = vol_b0.get_fdata(dtype=args.dtype)
         if data_b0.ndim == 3:
             data_b0 = data_b0[..., np.newaxis]
 
-        new_bvals = ([0] * data_b0.shape[-1]) + new_bvals
+        if args.in_bval:
+            new_bvals = ([0] * data_b0.shape[-1]) + new_bvals
 
         # Append zeros to bvecs
         new_bvecs = np.concatenate(
             (np.zeros((data_b0.shape[-1], 3)), new_bvecs), axis=0)
 
         # Scale SF by b0
         if args.b0_scaling:
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_sh_from_signal.py` & `scilpy-2.0.0/scripts/scil_volume_remove_outliers_ransac.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,78 +1,88 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-Script to compute the SH coefficient directly on the raw DWI signal.
+Remove outliers from image using the RANSAC algorithm.
+The RANSAC algorithm parameters are sensitive to the input data.
+
+NOTE: Current default parameters are tuned for ad/md/rd images only.
+
+Formerly: scil_remove_outliers_ransac.py
 """
 
 import argparse
+import logging
 
-from dipy.core.gradients import gradient_table
-from dipy.io.gradients import read_bvals_bvecs
 import nibabel as nib
 import numpy as np
 
-from scilpy.io.image import get_data_as_mask
-from scilpy.io.utils import (add_force_b0_arg, add_overwrite_arg,
-                             add_sh_basis_args, assert_inputs_exist,
+from scilpy.image.volume_operations import remove_outliers_ransac
+from scilpy.io.utils import (add_overwrite_arg,
+                             add_verbose_arg,
+                             assert_inputs_exist,
                              assert_outputs_exist)
-from scilpy.reconst.raw_signal import compute_sh_coefficients
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__, formatter_class=argparse.RawTextHelpFormatter)
-    p.add_argument('in_dwi',
-                   help='Path of the dwi volume.')
-    p.add_argument('in_bval',
-                   help='Path of the b-value file, in FSL format.')
-    p.add_argument('in_bvec',
-                   help='Path of the b-vector file, in FSL format.')
-    p.add_argument('out_sh',
-                   help='Name of the output SH file to save.')
-
-    p.add_argument('--sh_order', type=int, default=4,
-                   help='SH order to fit (int). [%(default)s]')
-    add_sh_basis_args(p)
-    p.add_argument('--smooth', type=float, default=0.006,
-                   help='Lambda-regularization coefficient in the SH fit '
-                        '(float). [%(default)s]')
-    p.add_argument('--use_attenuation', action='store_true',
-                   help='If set, will use signal attenuation before fitting '
-                        'the SH (i.e. divide by the b0).')
-    add_force_b0_arg(p)
-    p.add_argument('--mask',
-                   help='Path to a binary mask.\nOnly data inside the mask '
-                        'will be used for computations and reconstruction ')
+
+    p.add_argument('in_image',
+                   help='Nifti image.')
+    p.add_argument('out_image',
+                   help='Corrected Nifti image.')
+
+    p.add_argument('--min_fit', type=int, default=50,
+                   help='The minimum number of data values required to fit '
+                        'the model. [%(default)s]')
+    p.add_argument('--max_iter', type=int, default=1000,
+                   help='The maximum number of iterations allowed in the '
+                        'algorithm. [%(default)s]')
+    p.add_argument('--fit_thr', type=float, default=1e-2,
+                   help='Threshold value for determining when a data point '
+                        'fits a model. [%(default)s]')
+
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, [args.in_dwi, args.in_bval, args.in_bvec])
-    assert_outputs_exist(parser, args, args.out_sh)
-
-    vol = nib.load(args.in_dwi)
-    dwi = vol.get_fdata(dtype=np.float32)
-
-    bvals, bvecs = read_bvals_bvecs(args.in_bval, args.in_bvec)
-    gtab = gradient_table(args.in_bval, args.in_bvec, b0_threshold=bvals.min())
-
-    mask = None
-    if args.mask:
-        mask = get_data_as_mask(nib.load(args.mask), dtype=bool)
-
-    sh = compute_sh_coefficients(dwi, gtab, args.sh_order, args.sh_basis,
-                                 args.smooth,
-                                 use_attenuation=args.use_attenuation,
-                                 mask=mask)
-
-    nib.save(nib.Nifti1Image(sh.astype(np.float32), vol.affine), args.out_sh)
+    # Verifications
+    assert_inputs_exist(parser, args.in_image)
+    assert_outputs_exist(parser, args, args.out_image)
+
+    if args.min_fit < 2:
+        parser.error('--min_fit should be at least 2. Current value: {}'
+                     .format(args.min_fit))
+    if args.max_iter < 1:
+        parser.error('--max_iter should be at least 1. Current value: {}'
+                     .format(args.max_iter))
+    if args.fit_thr <= 0:
+        parser.error('--fit_thr should be greater than 0. Current value: {}'
+                     .format(args.fit_thr))
+
+    # Loading
+    in_img = nib.load(args.in_image)
+    in_data = in_img.get_fdata(dtype=np.float32)
+
+    if np.average(in_data[in_data > 0]) > 0.1:
+        logging.warning('Be careful, your image doesn\'t seem to be an ad, '
+                        'md or rd.')
+
+    # Processing
+    out_data = remove_outliers_ransac(in_data, args.min_fit, args.fit_thr,
+                                      args.max_iter)
+
+    # Saving
+    nib.save(nib.Nifti1Image(out_data, in_img.affine, in_img.header),
+             args.out_image)
 
 
-if __name__ == "__main__":
+if __name__ == '__main__':
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_ssst_fodf.py` & `scilpy-2.0.0/scripts/scil_fodf_ssst.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,34 +1,39 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
 Script to compute Constrained Spherical Deconvolution (CSD) fiber ODFs.
 
 See [Tournier et al. NeuroImage 2007]
+
+Formerly: scil_compute_ssst_fodf.py
 """
 
 import argparse
 import logging
 
 from dipy.core.gradients import gradient_table
 from dipy.data import get_sphere
 from dipy.io.gradients import read_bvals_bvecs
 from dipy.reconst.csdeconv import ConstrainedSphericalDeconvModel
 import nibabel as nib
 import numpy as np
 
+from scilpy.gradients.bvec_bval_tools import (check_b0_threshold,
+                                              normalize_bvecs,
+                                              is_normalized_bvecs)
 from scilpy.io.image import get_data_as_mask
-from scilpy.io.utils import (add_overwrite_arg, add_verbose_arg,
-                             assert_inputs_exist, add_verbose_arg,
-                             assert_outputs_exist, add_force_b0_arg,
-                             add_sh_basis_args, add_processes_arg)
-from scilpy.reconst.multi_processes import fit_from_model, convert_sh_basis
-from scilpy.utils.bvec_bval_tools import (check_b0_threshold, normalize_bvecs,
-                                          is_normalized_bvecs)
+from scilpy.io.utils import (add_b0_thresh_arg, add_overwrite_arg,
+                             add_processes_arg, add_sh_basis_args,
+                             add_skip_b0_check_arg, add_verbose_arg,
+                             assert_inputs_exist, assert_outputs_exist,
+                             parse_sh_basis_arg, assert_headers_compatible)
+from scilpy.reconst.fodf import fit_from_model
+from scilpy.reconst.sh import convert_sh_basis
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__, formatter_class=argparse.RawTextHelpFormatter)
 
     p.add_argument('in_dwi',
@@ -44,90 +49,94 @@
 
     p.add_argument(
         '--sh_order', metavar='int', default=8, type=int,
         help='SH order used for the CSD. (Default: 8)')
     p.add_argument(
         '--mask', metavar='',
         help='Path to a binary mask. Only the data inside the mask will be '
-             'used for computations and reconstruction.')
+             'used \nfor computations and reconstruction.')
 
-    add_force_b0_arg(p)
+    add_b0_thresh_arg(p)
+    add_skip_b0_check_arg(p, will_overwrite_with_min=True)
     add_sh_basis_args(p)
     add_processes_arg(p)
-    add_overwrite_arg(p)
     add_verbose_arg(p)
+    add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
-    logging.getLogger().setLevel(logging.INFO)
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, [args.in_dwi, args.in_bval, args.in_bvec,
-                                 args.frf_file])
+                                 args.frf_file], args.mask)
     assert_outputs_exist(parser, args, args.out_fODF)
+    assert_headers_compatible(parser, args.in_dwi, args.mask)
 
     # Loading data
     full_frf = np.loadtxt(args.frf_file)
     vol = nib.load(args.in_dwi)
     data = vol.get_fdata(dtype=np.float32)
     bvals, bvecs = read_bvals_bvecs(args.in_bval, args.in_bvec)
 
     # Checking mask
-    if args.mask is None:
-        mask = None
-    else:
-        mask = get_data_as_mask(nib.load(args.mask), dtype=bool)
-        if mask.shape != data.shape[:-1]:
-            raise ValueError("Mask is not the same shape as data.")
+    mask = get_data_as_mask(nib.load(args.mask),
+                            dtype=bool) if args.mask else None
 
     sh_order = args.sh_order
+    sh_basis, is_legacy = parse_sh_basis_arg(args)
 
     # Checking data and sh_order
-    b0_thr = check_b0_threshold(
-        args.force_b0_threshold, bvals.min(), bvals.min())
     if data.shape[-1] < (sh_order + 1) * (sh_order + 2) / 2:
         logging.warning(
             'We recommend having at least {} unique DWI volumes, but you '
             'currently have {} volumes. Try lowering the parameter sh_order '
             'in case of non convergence.'.format(
                 (sh_order + 1) * (sh_order + 2) / 2, data.shape[-1]))
 
     # Checking bvals, bvecs values and loading gtab
     if not is_normalized_bvecs(bvecs):
         logging.warning('Your b-vectors do not seem normalized...')
         bvecs = normalize_bvecs(bvecs)
-    gtab = gradient_table(bvals, bvecs, b0_threshold=b0_thr)
+
+    # gtab.b0s_mask is used in dipy's csdeconv class.
+    args.b0_threshold = check_b0_threshold(bvals.min(),
+                                           b0_thr=args.b0_threshold,
+                                           skip_b0_check=args.skip_b0_check)
+    gtab = gradient_table(bvals, bvecs, b0_threshold=args.b0_threshold)
 
     # Checking full_frf and separating it
     if not full_frf.shape[0] == 4:
         raise ValueError('FRF file did not contain 4 elements. '
                          'Invalid or deprecated FRF format')
     frf = full_frf[0:3]
     mean_b0_val = full_frf[3]
 
     # Loading the sphere
     reg_sphere = get_sphere('symmetric362')
 
     # Computing CSD
-    csd_model = ConstrainedSphericalDeconvModel(
-        gtab, (frf, mean_b0_val),
-        reg_sphere=reg_sphere,
-        sh_order=sh_order)
+    csd_model = ConstrainedSphericalDeconvModel(gtab, (frf, mean_b0_val),
+                                                reg_sphere=reg_sphere,
+                                                sh_order_max=sh_order)
 
     # Computing CSD fit
     csd_fit = fit_from_model(csd_model, data,
                              mask=mask, nbr_processes=args.nbr_processes)
 
     # Saving results
     shm_coeff = csd_fit.shm_coeff
-    if args.sh_basis == 'tournier07':
-        shm_coeff = convert_sh_basis(shm_coeff, reg_sphere, mask=mask,
-                                     nbr_processes=args.nbr_processes)
+    shm_coeff = convert_sh_basis(shm_coeff, reg_sphere, mask=mask,
+                                 input_basis='descoteaux07',
+                                 output_basis=sh_basis,
+                                 is_input_legacy=True,
+                                 is_output_legacy=is_legacy,
+                                 nbr_processes=args.nbr_processes)
     nib.save(nib.Nifti1Image(shm_coeff.astype(np.float32),
                              vol.affine), args.out_fODF)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_ssst_frf.py` & `scilpy-2.0.0/scripts/scil_frf_ssst.py`

 * *Files 14% similar despite different names*

```diff
@@ -2,129 +2,123 @@
 # -*- coding: utf-8 -*-
 
 """
 Compute a single Fiber Response Function from a DWI.
 
 A DTI fit is made, and voxels containing a single fiber population are
 found using a threshold on the FA.
+
+Formerly: scil_compute_ssst_frf.py
 """
 
 import argparse
 import logging
 
 from dipy.io.gradients import read_bvals_bvecs
 import nibabel as nib
 import numpy as np
 
+from scilpy.gradients.bvec_bval_tools import check_b0_threshold
 from scilpy.io.image import get_data_as_mask
-from scilpy.io.utils import (add_force_b0_arg,
-                             add_overwrite_arg, add_verbose_arg,
-                             assert_inputs_exist, assert_outputs_exist)
+from scilpy.io.utils import (add_b0_thresh_arg, add_overwrite_arg,
+                             add_skip_b0_check_arg, add_verbose_arg,
+                             assert_inputs_exist, assert_outputs_exist,
+                             assert_roi_radii_format,
+                             assert_headers_compatible)
 from scilpy.reconst.frf import compute_ssst_frf
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__,
-        formatter_class=argparse.RawDescriptionHelpFormatter,
+        formatter_class=argparse.RawTextHelpFormatter,
         epilog="References: [1] Tournier et al. NeuroImage 2007")
 
     p.add_argument('in_dwi',
                    help='Path of the input diffusion volume.')
     p.add_argument('in_bval',
                    help='Path of the bvals file, in FSL format.')
     p.add_argument('in_bvec',
                    help='Path of the bvecs file, in FSL format.')
     p.add_argument('frf_file',
                    help='Path to the output FRF file, in .txt format, '
                         'saved by Numpy.')
 
-    add_force_b0_arg(p)
-
     p.add_argument('--mask',
                    help='Path to a binary mask. Only the data inside the '
-                        'mask will be used for computations and '
-                        'reconstruction. Useful if no white matter mask '
+                        'mask will be used \nfor computations and '
+                        'reconstruction. Useful if no white matter mask \n'
                         'is available.')
     p.add_argument('--mask_wm',
                    help='Path to a binary white matter mask. Only the data '
-                        'inside this mask and above the threshold defined '
-                        'by --fa will be used to estimate the fiber response '
-                        'function.')
-    p.add_argument('--fa', dest='fa_thresh',
-                   default=0.7, type=float,
+                        'inside this mask \nand above the threshold defined '
+                        'by --fa_thresh will be used to estimate the \nfiber '
+                        'response function.')
+    p.add_argument('--fa_thresh', default=0.7, type=float,
                    help='If supplied, use this threshold as the initial '
-                        'threshold to select single fiber voxels. '
+                        'threshold to select \nsingle fiber voxels. '
                         '[%(default)s]')
-    p.add_argument('--min_fa', dest='min_fa_thresh',
-                   default=0.5, type=float,
+    p.add_argument('--min_fa_thresh', default=0.5, type=float,
                    help='If supplied, this is the minimal value that will be '
-                        'tried when looking for single fiber '
+                        'tried when looking \nfor single fiber '
                         'voxels. [%(default)s]')
-    p.add_argument('--min_nvox',
-                   default=300, type=int,
+    p.add_argument('--min_nvox', default=300, type=int,
                    help='Minimal number of voxels needing to be identified '
-                        'as single fiber voxels in the automatic '
+                        'as single fiber voxels \nin the automatic '
                         'estimation. [%(default)s]')
 
-    p.add_argument('--roi_radii', nargs='+',
-                   default=[20],  type=int,
+    p.add_argument('--roi_radii', default=[20], nargs='+', type=int,
                    help='If supplied, use those radii to select a cuboid roi '
-                        'to estimate the response functions. The roi will be '
-                        'a cuboid spanning from the middle of the volume in '
-                        'each direction with the different radii. The type is '
-                        'either an int or an array-like (3,). [%(default)s]')
+                        'to estimate the \nresponse functions. The roi will '
+                        'be a cuboid spanning from the middle of \nthe volume '
+                        'in each direction with the different radii. The type '
+                        'is either \nan int (e.g. --roi_radii 10) or an '
+                        'array-like (3,) (e.g. --roi_radii 20 30 10). '
+                        '[%(default)s]')
     p.add_argument('--roi_center', metavar='tuple(3)', nargs=3, type=int,
                    help='If supplied, use this center to span the roi of size '
                         'roi_radius. [center of the 3D volume]')
 
-    add_overwrite_arg(p)
+    add_b0_thresh_arg(p)
+    add_skip_b0_check_arg(p, will_overwrite_with_min=True)
     add_verbose_arg(p)
+    add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    log_level = logging.DEBUG if args.verbose else logging.INFO
-    logging.getLogger().setLevel(log_level)
-
-    assert_inputs_exist(parser, [args.in_dwi, args.in_bval, args.in_bvec])
+    assert_inputs_exist(parser, [args.in_dwi, args.in_bval, args.in_bvec],
+                        [args.mask, args.mask_wm])
     assert_outputs_exist(parser, args, args.frf_file)
+    assert_headers_compatible(parser, args.in_dwi, [args.mask, args.mask_wm])
 
-    if len(args.roi_radii) == 1:
-        roi_radii = args.roi_radii[0]
-    elif len(args.roi_radii) == 3:
-        roi_radii = args.roi_radii
-    else:
-        parser.error('Wrong size for --roi_radii, can only be a scalar' +
-                     'or an array of size (3,)')
+    roi_radii = assert_roi_radii_format(parser)
 
     vol = nib.load(args.in_dwi)
     data = vol.get_fdata(dtype=np.float32)
 
     bvals, bvecs = read_bvals_bvecs(args.in_bval, args.in_bvec)
-
-    mask = None
-    if args.mask:
-        mask = get_data_as_mask(nib.load(args.mask), dtype=bool)
-
-    mask_wm = None
-    if args.mask_wm:
-        mask_wm = get_data_as_mask(nib.load(args.mask_wm), dtype=bool)
+    args.b0_threshold = check_b0_threshold(bvals.min(),
+                                           b0_thr=args.b0_threshold,
+                                           skip_b0_check=args.skip_b0_check)
+
+    mask = get_data_as_mask(nib.load(args.mask),
+                            dtype=bool) if args.mask else None
+    mask_wm = get_data_as_mask(nib.load(args.mask_wm),
+                               dtype=bool) if args.mask_wm else None
 
     full_response = compute_ssst_frf(
-        data, bvals, bvecs, mask=mask,
+        data, bvals, bvecs, args.b0_threshold, mask=mask,
         mask_wm=mask_wm, fa_thresh=args.fa_thresh,
-        min_fa_thresh=args.min_fa_thresh,
-        min_nvox=args.min_nvox,
-        roi_radii=roi_radii,
-        roi_center=args.roi_center,
-        force_b0_threshold=args.force_b0_threshold)
+        min_fa_thresh=args.min_fa_thresh, min_nvox=args.min_nvox,
+        roi_radii=roi_radii, roi_center=args.roi_center)
 
     np.savetxt(args.frf_file, full_response)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_streamlines_density_map.py` & `scilpy-2.0.0/scripts/scil_tractogram_compute_density_map.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,74 +1,82 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-Compute a density map from a streamlines file.
-
-A specific value can be assigned instead of using the tract count.
+Compute a density map from a streamlines file. Can be binary.
 
 This script correctly handles compressed streamlines.
+
+Formerly: scil_compute_streamlines_density_map.py
 """
 import argparse
+import logging
 
 import numpy as np
 import nibabel as nib
 
 from scilpy.io.streamlines import load_tractogram_with_reference
 from scilpy.io.utils import (add_overwrite_arg, add_reference_arg,
-                             assert_inputs_exist, assert_outputs_exist)
+                             assert_inputs_exist, add_verbose_arg, 
+                             assert_outputs_exist)
 from scilpy.tractanalysis.streamlines_metrics import compute_tract_counts_map
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__, formatter_class=argparse.RawTextHelpFormatter)
     p.add_argument('in_bundle',
-                   help='Tractogram filename. Format must be one of \n'
-                        'trk, tck, vtk, fib, dpy.')
+                   help='Tractogram filename.')
     p.add_argument('out_img',
                    help='path of the output image file.')
 
     p.add_argument('--binary', metavar='FIXED_VALUE', type=int,
                    nargs='?', const=1,
-                   help='If set, will store the same value for all intersected'
-                        ' voxels, creating a binary map.\n'
-                        'When set without a value, 1 is used (and dtype uint8).\n'
-                        'If a value is given, will be used as the stored value.')
+                   help='If set, will store the same value for all '
+                        'intersected voxels, \ncreating a binary map.'
+                        'When set without a value, 1 is used (and dtype \n'
+                        'uint8). If a value is given, will be used as the '
+                        'stored value.')
     add_reference_arg(p)
+    add_verbose_arg(p)
     add_overwrite_arg(p)
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
+    # Verifications
     assert_inputs_exist(parser, args.in_bundle, optional=args.reference)
     assert_outputs_exist(parser, args, args.out_img)
 
     max_ = np.iinfo(np.int16).max
     if args.binary is not None and (args.binary <= 0 or args.binary > max_):
         parser.error('The value of --binary ({}) '
                      'must be greater than 0 and smaller or equal to {}'
                      .format(args.binary, max_))
 
+    # Loading
     sft = load_tractogram_with_reference(parser, args, args.in_bundle)
     sft.to_vox()
     sft.to_corner()
-    streamlines = sft.streamlines
     transformation, dimensions, _, _ = sft.space_attributes
 
-    streamline_count = compute_tract_counts_map(streamlines, dimensions)
+    # Processing
+    streamline_count = compute_tract_counts_map(sft.streamlines, dimensions)
 
+    # Saving
     dtype_to_use = np.int32
     if args.binary is not None:
         if args.binary == 1:
             dtype_to_use = np.uint8
         streamline_count[streamline_count > 0] = args.binary
 
-    nib.save(nib.Nifti1Image(streamline_count.astype(dtype_to_use), transformation),
-             args.out_img)
+    img = nib.Nifti1Image(streamline_count.astype(dtype_to_use),
+                          transformation)
+    nib.save(img, args.out_img)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_streamlines_length_stats.py` & `scilpy-2.0.0/scripts/scil_tractogram_shuffle.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,55 +1,55 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-Compute streamlines min, mean and max length, as well as
-standard deviation of length in mm.
+Shuffle the ordering of streamlines.
+
+Formerly: scil_shuffle_streamlines.py
 """
 
 import argparse
-import json
+import logging
 
-from dipy.tracking.streamlinespeed import length
-import numpy as np
+from dipy.io.streamline import save_tractogram
 
 from scilpy.io.streamlines import load_tractogram_with_reference
-from scilpy.io.utils import (add_json_args,
-                             add_reference_arg,
-                             assert_inputs_exist)
+from scilpy.io.utils import (add_overwrite_arg, add_reference_arg,
+                             assert_inputs_exist, add_verbose_arg,
+                             assert_outputs_exist)
+from scilpy.tractograms.tractogram_operations import shuffle_streamlines
 
 
 def _build_arg_parser():
-    p = argparse.ArgumentParser(
-        description=__doc__,
-        formatter_class=argparse.RawTextHelpFormatter)
 
-    p.add_argument('in_bundle',
-                   help='Fiber bundle file.')
+    p = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter,
+                                description=__doc__)
+
+    p.add_argument('in_tractogram',
+                   help='Input tractography file.')
+    p.add_argument('out_tractogram',
+                   help='Output tractography file.')
+    p.add_argument('--seed', type=int, default=None,
+                   help='Random number generator seed [%(default)s].')
 
     add_reference_arg(p)
-    add_json_args(p)
+    add_verbose_arg(p)
+    add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, args.in_bundle)
+    assert_inputs_exist(parser, args.in_tractogram, args.reference)
+    assert_outputs_exist(parser, args, args.out_tractogram)
 
-    sft = load_tractogram_with_reference(parser, args, args.in_bundle)
-    streamlines = sft.streamlines
-    lengths = [0]
-    if streamlines:
-        lengths = list(length(streamlines))
-
-    print(json.dumps({'min_length': float(np.min(lengths)),
-                      'mean_length': float(np.mean(lengths)),
-                      'max_length': float(np.max(lengths)),
-                      'std_length': float(np.std(lengths))},
-                     indent=args.indent, sort_keys=args.sort_keys))
+    sft = load_tractogram_with_reference(parser, args, args.in_tractogram)
+    shuffled_sft = shuffle_streamlines(sft, rng_seed=args.seed)
+    save_tractogram(shuffled_sft, args.out_tractogram)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_compute_todi.py` & `scilpy-2.0.0/scripts/scil_tractogram_compute_TODI.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,153 +1,153 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 """
 Compute a Track Orientation Density Image (TODI).
-Each segment of the streamlines is weighted by its length
-(to support compressed streamlines).
-This script can afterwards output a Track Density Image (TDI)
-or a TODI with SF or SH representation, based on streamlines' segments.\n\n
+
+Each segment of the streamlines is weighted by its length (to support
+compressed streamlines).
+
+This script can afterwards output a Track Density Image (TDI) or a TODI with SF
+or SH representation, based on streamlines' segments.
+
+Formerly: scil_compute_todi.py
 """
 
 import argparse
 import logging
 
 import nibabel as nib
 import numpy as np
 
 from scilpy.io.image import get_data_as_mask
 from scilpy.io.streamlines import load_tractogram_with_reference
 from scilpy.io.utils import (add_overwrite_arg, add_reference_arg,
-                             add_sh_basis_args,
-                             assert_inputs_exist, assert_outputs_exist)
+                             add_sh_basis_args, add_verbose_arg,
+                             assert_inputs_exist, assert_outputs_exist,
+                             parse_sh_basis_arg, assert_headers_compatible)
 from scilpy.tractanalysis.todi import TrackOrientationDensityImaging
 
 
 EPILOG = """
-    References:
-        [1] Dhollander T, Emsell L, Van Hecke W, Maes F, Sunaert S, Suetens P.
-            Track orientation density imaging (TODI) and
-            track orientation distribution (TOD) based tractography.
-            NeuroImage. 2014 Jul 1;94:312-36.
-    """
+References:
+    [1] Dhollander T, Emsell L, Van Hecke W, Maes F, Sunaert S, Suetens P.
+        Track orientation density imaging (TODI) and
+        track orientation distribution (TOD) based tractography.
+        NeuroImage. 2014 Jul 1;94:312-36.
+"""
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(description=__doc__, epilog=EPILOG,
                                 formatter_class=argparse.RawTextHelpFormatter)
 
     p.add_argument('in_tractogram',
                    help='Input streamlines file.')
 
-    add_reference_arg(p)
-
-    p.add_argument('--sphere', default='repulsion724',
-                   help='sphere used for the angular discretization. '
+    g = p.add_argument_group("Computing options")
+    g.add_argument('--sphere', default='repulsion724',
+                   help='Sphere used for the angular discretization. '
                         '[%(default)s]')
+    g.add_argument('--mask',
+                   help='If set, use the given mask.')
+    g.add_argument('--sh_order', type=int, default=8,
+                   help='Order of the original SH. [%(default)s]')
+    g.add_argument('--normalize_per_voxel', action='store_true',
+                   help='If set, normalize each SF/SH at each voxel.')
+    gg = g.add_mutually_exclusive_group()
+    gg.add_argument('--smooth_todi', action='store_true',
+                    help='If set, smooth TODI (angular and spatial).')
+    gg.add_argument('--asymmetric', action='store_true',
+                    help='If set, compute asymmetric TODI.\n'
+                         'Cannot be used with --smooth_todi.')
+    g.add_argument('--n_steps', default=1, type=int,
+                   help='Number of steps for streamline segments '
+                        'subdivision prior to binning [%(default)s].')
 
-    p.add_argument('--mask',
-                   help='Use the given mask.')
-
-    p.add_argument('--out_mask',
+    g = p.add_argument_group("Output files. Saves only when filename is set")
+    g.add_argument('--out_mask',
                    help='Mask showing where TDI > 0.')
-
-    p.add_argument('--out_tdi',
+    g.add_argument('--out_tdi',
                    help='Output Track Density Image (TDI).')
-
-    p.add_argument('--out_todi_sf',
+    g.add_argument('--out_todi_sf',
                    help='Output TODI, with SF (each directions\n'
                         'on the sphere, requires a lot of memory)')
-
-    p.add_argument('--out_todi_sh',
+    g.add_argument('--out_todi_sh',
                    help='Output TODI, with SH coefficients.')
 
-    p.add_argument('--sh_order', type=int, default=8,
-                   help='Order of the original SH. [%(default)s]')
-
-    p.add_argument('--normalize_per_voxel', action='store_true',
-                   help='Normalize each SF/SH at each voxel [%(default)s].')
-
-    p.add_argument('--smooth_todi', action='store_true',
-                   help='Smooth TODI (angular and spatial) [%(default)s].')
-
-    p.add_argument('--asymmetric', action='store_true',
-                   help='Compute asymmetric TODI [%(default)s].')
-
-    p.add_argument('--n_steps', default=1, type=int,
-                   help='Number of steps for streamline segments '
-                        'subdivision prior to binning [%(default)s].')
-
+    add_reference_arg(p)
     add_sh_basis_args(p)
+    add_verbose_arg(p)
     add_overwrite_arg(p)
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
-    logging.getLogger().setLevel(logging.INFO)
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
+    # Verifications
     assert_inputs_exist(parser, args.in_tractogram,
                         [args.mask, args.reference])
+    assert_headers_compatible(parser, args.in_tractogram, args.mask,
+                              reference=args.reference)
+    outputs = [args.out_mask, args.out_tdi, args.out_todi_sf, args.out_todi_sh]
+    assert_outputs_exist(parser, args, [], outputs)
+
+    if np.all([f is None for f in outputs]):
+        parser.error('No output selected. Choose at least one output option.')
+
+    if args.normalize_per_voxel and not (args.out_todi_sh or args.out_todi_sf):
+        logging.warning("Option --normalize_per_voxel is only useful when "
+                        "saving output --out_todi_sh or --out_todi_sf. "
+                        "Ignoring.")
 
-    output_file_list = []
-    if args.out_mask:
-        output_file_list.append(args.out_mask)
-    if args.out_tdi:
-        output_file_list.append(args.out_tdi)
-    if args.out_todi_sf:
-        output_file_list.append(args.out_todi_sf)
-    if args.out_todi_sh:
-        output_file_list.append(args.out_todi_sh)
-
-    if not output_file_list:
-        parser.error('No output to be done')
-
-    if args.smooth_todi and args.asymmetric:
-        parser.error('Invalid arguments combination. '
-                     'Cannot smooth asymmetric TODI.')
-
-    assert_outputs_exist(parser, args, output_file_list)
-
+    # Loading
     sft = load_tractogram_with_reference(parser, args, args.in_tractogram)
     affine, data_shape, _, _ = sft.space_attributes
 
     sft.to_vox()
     # Because compute_todi expects streamline points (in voxel coordinates)
-    # to be in the range (0..size) rather than (-0.5..size - 0.5), we shift
-    # the voxel origin to corner (will only be done if it's not already the
-    # case).
+    # to be in the range [0, size] rather than [-0.5, size - 0.5], we shift
+    # the voxel origin to corner.
     sft.to_corner()
 
+    # Processing
     logging.info('Computing length-weighted TODI ...')
     todi_obj = TrackOrientationDensityImaging(tuple(data_shape), args.sphere)
     todi_obj.compute_todi(sft.streamlines, length_weights=True,
                           n_steps=args.n_steps, asymmetric=args.asymmetric)
 
     if args.smooth_todi:
         logging.info('Smoothing ...')
         todi_obj.smooth_todi_dir()
         todi_obj.smooth_todi_spatial()
 
     if args.mask:
         mask = get_data_as_mask(nib.load(args.mask))
         todi_obj.mask_todi(mask)
 
+    if args.normalize_per_voxel:
+        # Normalization is done on the SH, but, indirectly, it may change the
+        # SF values. So, applying even if we don't have --out_todi_sh.
+        todi_obj.normalize_todi_per_voxel()
+
+    # Saving
     logging.info('Saving Outputs ...')
     if args.out_mask:
-        data = todi_obj.get_mask()
-        img = todi_obj.reshape_to_3d(data)
+        img = todi_obj.reshape_to_3d(todi_obj.get_mask())
         img = nib.Nifti1Image(img.astype(np.int16), affine)
         img.to_filename(args.out_mask)
 
     if args.out_todi_sh:
-        if args.normalize_per_voxel:
-            todi_obj.normalize_todi_per_voxel()
-        img = todi_obj.get_sh(args.sh_basis, args.sh_order,
-                              full_basis=args.asymmetric)
+        sh_basis, is_legacy = parse_sh_basis_arg(args)
+        img = todi_obj.get_sh(sh_basis, args.sh_order,
+                              full_basis=args.asymmetric,
+                              is_legacy=is_legacy)
         img = todi_obj.reshape_to_3d(img)
         img = nib.Nifti1Image(img.astype(np.float32), affine)
         img.to_filename(args.out_todi_sh)
 
     if args.out_tdi:
         img = todi_obj.get_tdi()
         img = todi_obj.reshape_to_3d(img)
```

### Comparing `scilpy-1.5.post2/scripts/scil_concatenate_dwi.py` & `scilpy-2.0.0/scripts/scil_dwi_concatenate.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,23 +1,27 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
 Concatenate DWI, bval and bvecs together. File must be specified in matching
 order. Default data type will be the same as the first input DWI.
+
+Formerly: scil_concatenate_dwi.py
 """
 
 import argparse
+import logging
 
 from dipy.io.gradients import read_bvals_bvecs
 from dipy.io.utils import is_header_compatible
 import nibabel as nib
 import numpy as np
 
 from scilpy.io.utils import (add_overwrite_arg,
+                             add_verbose_arg,
                              assert_inputs_exist,
                              assert_outputs_exist)
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__,
@@ -37,22 +41,24 @@
     p.add_argument('--in_bvecs', nargs='+',
                    help='The b-vectors files in FSL format (.bvec).')
 
     p.add_argument('--data_type',
                    help='Data type of the output image. Use the format: '
                         'uint8, int16, int/float32, int/float64.')
 
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     if len(args.in_dwis) != len(args.in_bvals) \
             or len(args.in_dwis) != len(args.in_bvecs):
         parser.error('DWI, bvals and bvecs must have the same length')
 
     assert_inputs_exist(parser, args.in_dwis + args.in_bvals + args.in_bvecs)
     assert_outputs_exist(parser, args, [args.out_dwi, args.out_bval,
```

### Comparing `scilpy-1.5.post2/scripts/scil_connectivity_math.py` & `scilpy-2.0.0/scripts/scil_connectivity_math.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,21 +13,21 @@
 import argparse
 import logging
 import os
 
 import nibabel as nib
 import numpy as np
 
-from scilpy.image.operations import (get_array_ops, get_operations_doc)
+from scilpy.image.volume_math import (get_array_ops, get_operations_doc)
 from scilpy.io.utils import (add_overwrite_arg,
                              add_verbose_arg,
                              assert_outputs_exist,
                              load_matrix_in_any_format,
                              save_matrix_in_any_format)
-from scilpy.utils.util import is_float
+from scilpy.utils import is_float
 
 OPERATIONS = get_array_ops()
 
 ADDED_DOC = get_operations_doc(OPERATIONS).replace('images', 'matrices')
 ADDED_DOC = ADDED_DOC.replace('image', 'matrix')
 ADDED_DOC = ADDED_DOC.replace('IMG', 'MAT')
 __doc__ += ADDED_DOC
@@ -50,16 +50,16 @@
     p.add_argument('--data_type',
                    help='Data type of the output image. Use the format: '
                         'uint8, float16, int32.')
     p.add_argument('--exclude_background', action='store_true',
                    help='Does not affect the background of the original '
                         'matrices.')
 
-    add_overwrite_arg(p)
     add_verbose_arg(p)
+    add_overwrite_arg(p)
 
     return p
 
 
 def load_matrix(arg):
     if is_float(arg):
         matrix = float(arg)
@@ -81,17 +81,15 @@
 
     return matrix
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
-
-    if args.verbose:
-        logging.getLogger().setLevel(logging.INFO)
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_outputs_exist(parser, args, args.out_matrix)
 
     # Binary operations require specific verifications
     binary_op = ['union', 'intersection', 'difference', 'invert']
 
     if args.operation not in OPERATIONS.keys():
```

### Comparing `scilpy-1.5.post2/scripts/scil_convert_fdf.py` & `scilpy-2.0.0/scripts/scil_dwi_convert_FDF.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,26 +1,32 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-   Converts a Varian FDF file or directory to a nifti file.
-   If the procpar contains diffusion information, it will be saved as bval and
-   bvec in the same folder as the output file.
+Converts a Varian FDF file or directory to a nifti file.
+If the procpar contains diffusion information, it will be saved as bval and
+bvec in the same folder as the output file.
 
-   ex: scil_convert_fdf.py semsdw/b0_folder/ semsdw/dwi_folder/ dwi.nii.gz --bval dwi.bval --bvec dwi.bvec -f
+ex: scil_dwi_convert_FDF.py semsdw/b0_folder/ semsdw/dwi_folder/ \
+        dwi.nii.gz --bval dwi.bval --bvec dwi.bvec -f
+
+Formerly: scil_convert_fdf.py
 """
 
 import argparse
+import logging
 
-from scilpy.io.varian_fdf import load_fdf, save_babel, correct_dwi_intensity
+from scilpy.io.varian_fdf import (correct_procpar_intensity, load_fdf,
+                                  save_babel)
 from scilpy.io.utils import (add_overwrite_arg,
+                             add_verbose_arg,
                              assert_outputs_exist)
 
 
-def build_arg_parser():
+def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__,
         formatter_class=argparse.RawTextHelpFormatter,)
 
     p.add_argument('in_b0_path',
                    help='Path to the b0 FDF file or folder to convert.')
     p.add_argument('in_dwi_path',
@@ -36,30 +42,32 @@
                    help='The axes you want to flip. eg: to flip the x '
                         'and y axes use: x y. [%(default)s]')
     p.add_argument('--swap', metavar='dimension', default=None,
                    choices=['x', 'y', 'z'], nargs='+',
                    help='The axes you want to swap. eg: to swap the x '
                         'and y axes use: x y. [%(default)s]')
 
+    add_verbose_arg(p)
     add_overwrite_arg(p)
     return p
 
 
 def main():
-    parser = build_arg_parser()
+    parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_outputs_exist(parser, args, args.out_path, optional=[args.bval,
                                                                 args.bvec])
 
     data_dwi, header_dwi = load_fdf(args.in_dwi_path)
     data_b0, header_b0 = load_fdf(args.in_b0_path)
 
-    data_dwi = correct_dwi_intensity(data_dwi, args.in_dwi_path,
-                                     args.in_b0_path)
+    data_dwi = correct_procpar_intensity(data_dwi, args.in_dwi_path,
+                                         args.in_b0_path)
 
     save_babel(data_dwi, header_dwi,
                data_b0, header_b0,
                args.bval, args.bvec,
                args.out_path,
                flip=args.flip,
                swap=args.swap)
```

### Comparing `scilpy-1.5.post2/scripts/scil_convert_gradients_mrtrix_to_fsl.py` & `scilpy-2.0.0/scripts/scil_frf_set_diffusivities.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,50 +1,66 @@
-#! /usr/bin/env python3
+#!/usr/bin/env python3
 # -*- coding: utf-8 -*-
+
 """
-Script to convert bval/bvec MRtrix style to FSL style.
+Replace the fiber response function in the FRF file.
+Use this script when you want to use a fixed response function
+and keep the mean b0.
+
+The FRF file is obtained from scil_frf_ssst.py or scil_frf_msmt.py in the case
+of multi-shell data.
+
+Formerly: scil_set_response_function.py
 """
 
 import argparse
 import logging
+import numpy as np
 
 from scilpy.io.utils import (add_overwrite_arg,
+                             assert_inputs_exist,
                              add_verbose_arg,
-                             assert_gradients_filenames_valid,
                              assert_outputs_exist)
-from scilpy.utils.bvec_bval_tools import mrtrix2fsl
+from scilpy.reconst.frf import replace_frf
 
 
 def _build_arg_parser():
-    p = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter,
-                                description=__doc__)
+    p = argparse.ArgumentParser(
+        description=__doc__, formatter_class=argparse.RawTextHelpFormatter)
 
-    p.add_argument('mrtrix_enc',
-                   help='Path to the gradient directions encoding file. (.b)')
-    p.add_argument('fsl_bval',
-                   help='Path to output FSL b-value file (.bval).')
-    p.add_argument('fsl_bvec',
-                   help='Path to output FSL gradient directions file (.bvec).')
+    p.add_argument('frf_file', metavar='input',
+                   help='Path of the FRF file.')
+    p.add_argument('new_frf',
+                   help='New response function given as a tuple. We will '
+                        'replace the \nresponse function in frf_file with '
+                        'this fiber response \nfunction x 10**-4 (e.g. '
+                        '15,4,4). \nIf multi-shell, write the first shell,'
+                        'then the second shell, \nand the third, etc. '
+                        '(e.g. 15,4,4,13,5,5,12,5,5).')
+    p.add_argument('output_frf_file', metavar='output',
+                   help='Path of the new FRF file.')
+    p.add_argument('--no_factor', action='store_true',
+                   help='If supplied, the fiber response function is\n'
+                        'evaluated without the x 10**-4 factor. [%(default)s].'
+                   )
 
-    add_overwrite_arg(p)
     add_verbose_arg(p)
+    add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    if args.verbose:
-        logging.getLogger().setLevel(logging.INFO)
-
-    assert_gradients_filenames_valid(parser, args.mrtrix_enc, 'mrtrix')
-    assert_gradients_filenames_valid(parser, [args.fsl_bval, args.fsl_bvec],
-                                     'fsl')
-    assert_outputs_exist(parser, args, [args.fsl_bval, args.fsl_bvec])
+    assert_inputs_exist(parser, args.frf_file)
+    assert_outputs_exist(parser, args, args.output_frf_file)
 
-    mrtrix2fsl(args.mrtrix_enc, args.fsl_bval, args.fsl_bvec)
+    frf_file = np.loadtxt(args.frf_file)
+    response = replace_frf(frf_file, args.new_frf, args.no_factor)
+    np.savetxt(args.output_frf_file, response)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_convert_json_to_xlsx.py` & `scilpy-2.0.0/scripts/scil_json_convert_entries_to_xlsx.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,21 +1,25 @@
 #!/usr/bin/env python
 # -*- coding: utf-8 -*-
 
-""" Convert a final aggregated json file to an Excel spreadsheet. Typically
-used during the tractometry pipeline.
+"""
+Convert a final aggregated json file to an Excel spreadsheet.
+Typically used during the tractometry pipeline.
+
+Formerly: scil_convert_json_to_xlsx.py
 """
 
 import argparse
 import json
+import logging
 
 import numpy as np
 import pandas as pd
 
-from scilpy.io.utils import (add_overwrite_arg,
+from scilpy.io.utils import (add_overwrite_arg, add_verbose_arg,
                              assert_inputs_exist, assert_outputs_exist)
 
 
 def _get_all_bundle_names(stats):
     bnames = set()
 
     for bundles in iter(stats.values()):
@@ -459,22 +463,24 @@
                    help='Path to a text file containing a list of bundles '
                         'to ignore (.txt).\nOne bundle, corresponding to keys '
                         'in the json, per line.')
     p.add_argument('--stats_over_population', action='store_true',
                    help='If set, consider the input stats to be over an '
                         'entire population and not subject-based.')
 
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, args.in_json)
     assert_outputs_exist(parser, args, args.out_xlsx)
 
     _create_xlsx_from_json(args.in_json, args.out_xlsx,
                            sort_subs=args.no_sort_subs,
                            sort_bundles=args.no_sort_bundles,
```

### Comparing `scilpy-1.5.post2/scripts/scil_convert_rgb.py` & `scilpy-2.0.0/scripts/scil_rgb_convert.py`

 * *Files 5% similar despite different names*

```diff
@@ -13,45 +13,51 @@
 -Case 2: 3D image, in Trackvis format where each voxel contains a
          tuple of 3 elements, one for each value.
 
 Output
 -Case 1: 3D image, in Trackvis format where each voxel contains a
          tuple of 3 elements, one for each value (uint8).
 -Case 2: 4D image where the 4th dimension contains 3 values (uint8).
+
+Formerly: scil_convert_rgb.py
 """
 
 import argparse
+import logging
 
 from dipy.io.utils import decfa, decfa_to_float
 import nibabel as nib
 import numpy as np
 
 from scilpy.io.utils import (add_overwrite_arg, assert_inputs_exist,
-                             assert_outputs_exist)
+                             assert_outputs_exist, add_verbose_arg)
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         formatter_class=argparse.RawTextHelpFormatter,
         description=__doc__)
 
     p.add_argument('in_image',
                    help='name of input RGB image.\n' +
                         'Either 4D or 3D image.')
     p.add_argument('out_image',
                    help='name of output RGB image.\n' +
                         'Either 3D or 4D image.')
+
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, args.in_image)
     assert_outputs_exist(parser, args, args.out_image)
 
     original_im = nib.load(args.in_image)
 
     if original_im.ndim == 4:
@@ -63,15 +69,16 @@
         converted_im = decfa(original_im, scale=scale)
 
         nib.save(converted_im, args.out_image)
 
     elif original_im.ndim == 3:
         converted_im_float = decfa_to_float(original_im)
 
-        converted_data_int = np.asanyarray(converted_im_float.dataobj).astype(np.uint8)
+        converted_data_int = \
+            np.asanyarray(converted_im_float.dataobj).astype(np.uint8)
         converted_im = nib.Nifti1Image(converted_data_int,
                                        converted_im_float.affine)
 
         nib.save(converted_im, args.out_image)
 
 
 if __name__ == "__main__":
```

### Comparing `scilpy-1.5.post2/scripts/scil_convert_surface.py` & `scilpy-2.0.0/scripts/scil_volume_flip.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,52 +1,58 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
+#! /usr/bin/env python3
 
 """
-Script to convert a surface (FreeSurfer or VTK supported).
-    ".vtk", ".vtp", ".ply", ".stl", ".xml", ".obj"
+Flip the volume according to the specified axis.
 
-> scil_convert_surface.py surf.vtk converted_surf.ply
+Formerly: scil_flip_volume.py
 """
 
 import argparse
+import logging
 
-from trimeshpy.io import load_mesh_from_file
+import nibabel as nib
+import numpy as np
 
-from scilpy.io.utils import (add_overwrite_arg,
-                             assert_inputs_exist,
-                             assert_outputs_exist)
-
-EPILOG = """
-References:
-[1] St-Onge, E., Daducci, A., Girard, G. and Descoteaux, M. 2018.
-    Surface-enhanced tractography (SET). NeuroImage.
-"""
+from scilpy.image.volume_operations import flip_volume
+from scilpy.io.utils import (add_overwrite_arg, assert_inputs_exist,
+                             add_verbose_arg, assert_outputs_exist)
 
 
 def _build_arg_parser():
-    p = argparse.ArgumentParser(description=__doc__, epilog=EPILOG,
-                                formatter_class=argparse.RawTextHelpFormatter)
-
-    p.add_argument('in_surface',
-                   help='Input a surface (FreeSurfer or supported by VTK).')
 
-    p.add_argument('out_surface',
-                   help='Output flipped surface (formats supported by VTK).')
+    p = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter,
+                                description=__doc__)
+    p.add_argument('in_image',
+                   help='Path of the input volume (nifti).')
+    p.add_argument('out_image',
+                   help='Path of the output volume (nifti).')
+    p.add_argument('axes', metavar='dimension',
+                   choices=['x', 'y', 'z'], nargs='+',
+                   help='The axes you want to flip. eg: to flip the x '
+                        'and y axes use: x y.')
 
+    add_verbose_arg(p)
     add_overwrite_arg(p)
+
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
+
+    assert_inputs_exist(parser, args.in_image)
+    assert_outputs_exist(parser, args, args.out_image)
+
+    vol = nib.load(args.in_image)
+    data = vol.get_fdata(dtype=np.float32)
+    affine = vol.affine
+    header = vol.header
 
-    assert_inputs_exist(parser, args.in_surface)
-    assert_outputs_exist(parser, args, args.out_surface)
+    data = flip_volume(data, args.axes)
 
-    mesh = load_mesh_from_file(args.in_surface)
-    mesh.save(args.out_surface)
+    nib.save(nib.Nifti1Image(data, affine, header=header), args.out_image)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_convert_tensors.py` & `scilpy-2.0.0/scripts/scil_dti_convert_tensors.py`

 * *Files 14% similar despite different names*

```diff
@@ -3,23 +3,24 @@
 """
 Conversion of tensors (the 6 values from the triangular matrix) between various
 software standards. We cannot discover the input format type, user must know
 how the tensors were created.
 """
 
 import argparse
+import logging
 
 import nibabel as nib
 import numpy as np
 
-from scilpy.io.utils import (add_overwrite_arg,
+from scilpy.io.utils import (add_overwrite_arg, add_verbose_arg,
                              assert_inputs_exist, assert_outputs_exist)
-from scilpy.reconst.dti import (supported_tensor_formats,
-                                tensor_format_description,
-                                convert_tensor_format)
+from scilpy.io.tensor import (supported_tensor_formats,
+                              tensor_format_description,
+                              convert_tensor_format)
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(description=__doc__ + tensor_format_description,
                                 formatter_class=argparse.RawTextHelpFormatter)
 
     p.add_argument('in_file',
@@ -30,22 +31,25 @@
                    choices=supported_tensor_formats,
                    help='Input format. Choices: {}'
                    .format(supported_tensor_formats))
     p.add_argument('out_format', metavar='out_format',
                    choices=supported_tensor_formats,
                    help='Output format. Choices: {}'
                    .format(supported_tensor_formats))
+    
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, args.in_file)
     assert_outputs_exist(parser, args, args.out_file)
 
     in_tensors_img = nib.load(args.in_file)
     in_tensors = in_tensors_img.get_fdata(dtype=np.float32)
```

### Comparing `scilpy-1.5.post2/scripts/scil_convert_tractogram.py` & `scilpy-2.0.0/scripts/scil_tractogram_convert.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,48 +1,53 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 """
 Conversion of '.tck', '.trk', '.fib', '.vtk' and 'dpy' files using updated file
 format standard. TRK file always needs a reference file, a NIFTI, for
 conversion. The FIB file format is in fact a VTK, MITK Diffusion supports it.
+
+Formerly: scil_convert_tractogram.py
 """
 
 import argparse
+import logging
 import os
 
 from dipy.io.streamline import save_tractogram
 
 from scilpy.io.streamlines import load_tractogram_with_reference
 from scilpy.io.utils import (add_bbox_arg, add_overwrite_arg,
                              add_reference_arg, assert_inputs_exist,
-                             assert_outputs_exist)
+                             assert_outputs_exist, add_verbose_arg)
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(description=__doc__,
                                 formatter_class=argparse.RawTextHelpFormatter)
 
-    p.add_argument('in_tractogram', metavar='IN_TRACTOGRAM',
+    p.add_argument('in_tractogram',
                    help='Tractogram filename. Format must be one of \n'
                         'trk, tck, vtk, fib, dpy')
 
-    p.add_argument('output_name', metavar='OUTPUT_NAME',
+    p.add_argument('output_name',
                    help='Output filename. Format must be one of \n'
                         'trk, tck, vtk, fib, dpy')
 
+    add_bbox_arg(p)
     add_reference_arg(p)
+    add_verbose_arg(p)
     add_overwrite_arg(p)
-    add_bbox_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, args.in_tractogram, args.reference)
 
     in_extension = os.path.splitext(args.in_tractogram)[1]
     out_extension = os.path.splitext(args.output_name)[1]
 
     if in_extension == out_extension:
```

### Comparing `scilpy-1.5.post2/scripts/scil_count_non_zero_voxels.py` & `scilpy-2.0.0/scripts/scil_volume_count_non_zero_voxels.py`

 * *Files 22% similar despite different names*

```diff
@@ -4,55 +4,75 @@
 """
 Count the number of non-zero voxels in an image file.
 
 If you give it an image with more than 3 dimensions, it will summarize the 4th
 (or more) dimension to one voxel, and then find non-zero voxels over this.
 This means that if there is at least one non-zero voxel in the 4th dimension,
 this voxel of the 3D volume will be considered as non-zero.
+
+Formerly: scil_count_non_zero_voxels.py
 """
 
 import argparse
+import logging
 import os
 
-from scilpy.image.utils import count_non_zero_voxels
-from scilpy.io.utils import assert_inputs_exist
+import nibabel as nib
+import numpy as np
+
+from scilpy.image.volume_operations import count_non_zero_voxels
+from scilpy.io.utils import assert_inputs_exist, add_verbose_arg, \
+    assert_outputs_exist, add_overwrite_arg
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__, formatter_class=argparse.RawTextHelpFormatter)
     p.add_argument('in_image', metavar='IN_FILE',
                    help='Input file name, in nifti format.')
 
     p.add_argument(
         '--out', metavar='OUT_FILE', dest='out_filename',
-        help='name of the output file, which will be saved as a text file.')
+        help='Name of the output file, which will be saved as a text file.')
     p.add_argument(
         '--stats', action='store_true', dest='stats_format',
-        help='output the value using a stats format. Using this syntax will '
-             'append\na line to the output file, instead of creating a file '
-             'with only one line.\nThis is useful to create a file to be used '
-             'as the source of data for a graph.\nCan be combined with --id')
+        help='If set, output the value using a stats format. Using this '
+             'synthax will append\na line to the output file, instead of '
+             'creating a file with only one line.\nThis is useful to create '
+             'a file to be used as the source of data for a graph.\nCan be '
+             'combined with --id')
     p.add_argument(
         '--id', dest='value_id',
         help='Id of the current count. If used, the value of this argument '
              'will be\noutput (followed by a ":") before the count value.\n'
              'Mostly useful with --stats.')
+
+    add_verbose_arg(p)
+    add_overwrite_arg(p)
+
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, args.in_image)
-    # out_filename can exist or not
+    if not args.stats_format:
+        assert_outputs_exist(parser, args, [], args.out_filename)
+    elif args.overwrite:
+        parser.error("Using -f together with --stats is unclear. Please "
+                     "either remove --stats to start anew, or remove -f to "
+                     "append to existing file.")
 
     # Load image file
-    nb_voxels = count_non_zero_voxels(args.in_image)
+    im = nib.load(args.in_image).get_fdata(dtype=np.float32)
+
+    nb_voxels = count_non_zero_voxels(im)
 
     if args.out_filename is not None:
         open_mode = 'w'
         add_newline = False
         if args.stats_format and os.path.exists(args.out_filename):
             open_mode = 'a'
```

### Comparing `scilpy-1.5.post2/scripts/scil_count_streamlines.py` & `scilpy-2.0.0/scripts/scil_tractogram_flip.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,52 +1,66 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-Return the number of streamlines in a tractogram. Only support trk and tck in
-order to support the lazy loading from nibabel.
+Flip streamlines locally around specific axes.
+
+IMPORTANT: this script should only be used in case of absolute necessity.
+It's better to fix the real tools than to force flipping streamlines to
+have them fit in the tools.
+
+Formerly: scil_flip_streamlines.py
 """
 
 import argparse
-import json
-import os
+import logging
+
+from dipy.io.streamline import save_tractogram
 
-from scilpy.io.utils import add_json_args, assert_inputs_exist
-from scilpy.tractograms.lazy_tractogram_operations import \
-    lazy_streamlines_count
+from scilpy.io.streamlines import load_tractogram_with_reference
+from scilpy.io.utils import (add_reference_arg,
+                             add_verbose_arg,
+                             add_overwrite_arg,
+                             assert_inputs_exist,
+                             assert_outputs_exist)
+from scilpy.tractograms.tractogram_operations import flip_sft
 
 
 def _build_arg_parser():
-    p = argparse.ArgumentParser(description=__doc__,
-                                formatter_class=argparse.RawTextHelpFormatter)
+    p = argparse.ArgumentParser(
+        description=__doc__, formatter_class=argparse.RawTextHelpFormatter)
+
     p.add_argument('in_tractogram',
                    help='Path of the input tractogram file.')
-    p.add_argument('--print_count_alone', action='store_true',
-                   help="If true, prints the result only. \nElse, prints the "
-                        "bundle name and count formatted as a json dict."
-                        "(default)")
-    add_json_args(p)
+    p.add_argument('out_tractogram',
+                   help='Path of the output tractogram file.')
+
+    p.add_argument('axes',
+                   choices=['x', 'y', 'z'], nargs='+',
+                   help='The axes you want to flip. eg: to flip the x '
+                        'and y axes use: x y.')
+
+    add_reference_arg(p)
+    add_verbose_arg(p)
+    add_overwrite_arg(p)
+    
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, args.in_tractogram)
+    assert_inputs_exist(parser, args.in_tractogram, args.reference)
+    assert_outputs_exist(parser, args, args.out_tractogram)
 
-    bundle_name, _ = os.path.splitext(os.path.basename(args.in_tractogram))
-    count = int(lazy_streamlines_count(args.in_tractogram))
+    sft = load_tractogram_with_reference(parser, args, args.in_tractogram)
+    sft.to_vox()
+    sft.to_corner()
 
-    if args.print_count_alone:
-        print(count)
-    else:
-        stats = {
-            bundle_name: {
-                'streamline_count': count
-            }
-        }
-        print(json.dumps(stats, indent=args.indent))
+    new_sft = flip_sft(sft, args.axes)
+    save_tractogram(new_sft, args.out_tractogram)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_cut_streamlines.py` & `scilpy-2.0.0/scripts/scil_tractogram_cut_streamlines.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,116 +1,180 @@
 #!/usr/bin/env python
 # -*- coding: utf-8 -*-
 
 """
 Filters streamlines and only keeps the parts of streamlines within or
-between the ROIs. The script accepts a single input mask, the mask has either
-1 entity/blob or 2 entities/blobs (does not support disconnected voxels).
+between the ROIs. Two options are available.
+
+Input mask:
+
+The mask has either 1 entity/blob or
+2 entities/blobs (does not support disconnected voxels).
 The option --biggest_blob can help if you have such a scenario.
 
 The 1 entity scenario will 'trim' the streamlines so their longest segment is
 within the bounding box or a binary mask.
 
 The 2 entities scenario will cut streamlines so their segment are within the
 bounding box or going from binary mask #1 to binary mask #2.
 
-Both scenarios will erase data_per_point and data_per_streamline.
+Input label:
+
+The label MUST contain 2 labels different from zero.
+Label values could be anything.
+The script will cut streamlines going from label 1 to label 2.
+
+Both inputs and scenarios will erase data_per_point and data_per_streamline.
+
+Formerly: scil_cut_streamlines.py
 """
 
 import argparse
 import logging
 
 from dipy.io.streamline import save_tractogram
-from dipy.io.utils import is_header_compatible
 from dipy.io.stateful_tractogram import StatefulTractogram
 from dipy.tracking.streamlinespeed import compress_streamlines
 import nibabel as nib
 import numpy as np
 import scipy.ndimage as ndi
 
+from scilpy.image.labels import get_data_as_labels
 from scilpy.io.image import get_data_as_mask
 from scilpy.io.streamlines import load_tractogram_with_reference
-from scilpy.io.utils import (add_overwrite_arg, add_verbose_arg,
-                             assert_inputs_exist, assert_outputs_exist)
-from scilpy.tractanalysis.tools import (cut_outside_of_mask_streamlines,
-                                        cut_between_masks_streamlines)
-from scilpy.tracking.tools import resample_streamlines_step_size
+from scilpy.io.utils import (add_overwrite_arg, add_reference_arg,
+                             add_verbose_arg, assert_inputs_exist,
+                             assert_outputs_exist, assert_headers_compatible,
+                             add_compression_arg)
+from scilpy.tractograms.streamline_and_mask_operations import \
+    cut_outside_of_mask_streamlines, cut_between_mask_two_blobs_streamlines
+from scilpy.tractograms.streamline_operations import \
+    resample_streamlines_step_size
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__,
         formatter_class=argparse.RawTextHelpFormatter)
     p.add_argument('in_tractogram',
                    help='Input tractogram file.')
-    p.add_argument('in_mask',
-                   help='Binary mask containing either 1 or 2 blobs.')
+
+    g1 = p.add_argument_group('Mandatory mask options',
+                              'Choose between mask or label input.')
+    g2 = g1.add_mutually_exclusive_group(required=True)
+    g2.add_argument('--mask',
+                    help='Binary mask containing either 1 or 2 blobs.')
+    g2.add_argument('--label',
+                    help='Label containing 2 blobs.')
+
     p.add_argument('out_tractogram',
-                   help='Output tractogram file.')
+                   help='Output tractogram file. Note: data_per_point will be '
+                        'discarded, if any!')
 
+    p.add_argument('--label_ids', nargs=2, type=int,
+                   help='List of labels indices to use to cut '
+                        'streamlines (2 values).')
     p.add_argument('--resample', dest='step_size', type=float, default=None,
                    help='Resample streamlines to a specific step-size in mm '
                         '[%(default)s].')
-    p.add_argument('--compress', dest='error_rate', type=float, default=None,
-                   help='Maximum compression distance in mm [%(default)s].')
     p.add_argument('--biggest_blob', action='store_true',
                    help='Use the biggest entity and force the 1 ROI scenario.')
-    add_overwrite_arg(p)
+
+    add_compression_arg(p)
+    add_reference_arg(p)
     add_verbose_arg(p)
+    add_overwrite_arg(p)
+
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    if args.verbose:
-        logging.getLogger().setLevel(logging.INFO)
-
-    assert_inputs_exist(parser, [args.in_tractogram, args.in_mask])
+    assert_inputs_exist(parser, args.in_tractogram, optional=[args.mask,
+                                                              args.label,
+                                                              args.reference])
     assert_outputs_exist(parser, args, args.out_tractogram)
+    assert_headers_compatible(parser, args.in_tractogram,
+                              optional=[args.mask,
+                                        args.label],
+                              reference=args.reference)
 
+    # Loading
     sft = load_tractogram_with_reference(parser, args, args.in_tractogram)
+    # Streamlines must be in voxel space to deal correctly with bounding box.
+    sft.to_vox()
+    sft.to_corner()
+
     if args.step_size is not None:
         sft = resample_streamlines_step_size(sft, args.step_size)
 
-    mask_img = nib.load(args.in_mask)
-    binary_mask = get_data_as_mask(mask_img)
+    if len(sft.streamlines) == 0:
+        parser.error('Input tractogram is empty.')
 
-    if not is_header_compatible(sft, mask_img):
-        parser.error('Incompatible header between the tractogram and mask.')
+    if args.mask:
+        mask_img = nib.load(args.mask)
+        binary_mask = get_data_as_mask(mask_img)
+
+        bundle_disjoint, _ = ndi.label(binary_mask)
+        unique, count = np.unique(bundle_disjoint, return_counts=True)
+        if args.biggest_blob:
+            val = unique[np.argmax(count[1:])+1]
+            binary_mask[bundle_disjoint != val] = 0
+            unique = [0, val]
+        if len(unique) == 2:
+            logging.info('The provided mask has 1 entity '
+                         'cut_outside_of_mask_streamlines function selected.')
+            new_sft = cut_outside_of_mask_streamlines(sft, binary_mask)
+        elif len(unique) == 3:
+            logging.info('The provided mask has 2 entity '
+                         'cut_between_mask_two_blobs_streamlines '
+                         'function selected.')
+            new_sft = cut_between_mask_two_blobs_streamlines(sft, binary_mask)
+        else:
+            logging.warning('The provided mask has MORE THAN 2 entity '
+                            'cut_between_mask_two_blobs_streamlines function '
+                            'selected. This may cause problems with '
+                            'the outputed streamlines.'
+                            ' Please inspect the output carefully.')
+            new_sft = cut_between_mask_two_blobs_streamlines(sft, binary_mask)
+    else:
+        label_img = nib.load(args.label)
+        label_data = get_data_as_labels(label_img)
 
-    bundle_disjoint, _ = ndi.label(binary_mask)
-    unique, count = np.unique(bundle_disjoint, return_counts=True)
-    if args.biggest_blob:
-        val = unique[np.argmax(count[1:])+1]
-        binary_mask[bundle_disjoint != val] = 0
-        unique = [0, val]
-    if len(unique) == 2:
-        logging.info('The provided mask has 1 entity '
-                     'cut_outside_of_mask_streamlines function selected.')
-        new_sft = cut_outside_of_mask_streamlines(sft, binary_mask)
-    elif len(unique) == 3:
-        logging.info('The provided mask has 2 entity '
-                     'cut_between_masks_streamlines function selected.')
-        new_sft = cut_between_masks_streamlines(sft, binary_mask)
+        if args.label_ids:
+            unique_vals = args.label_ids
+        else:
+            unique_vals = np.unique(label_data[label_data != 0])
+            if len(unique_vals) != 2:
+                parser.error('More than two values in the label file, '
+                             'please use --label_ids to select '
+                             'specific label ids.')
+
+        label_data_1 = np.copy(label_data)
+        mask = label_data_1 != unique_vals[0]
+        label_data_1[mask] = 0
+
+        label_data_2 = np.copy(label_data)
+        mask = label_data_2 != unique_vals[1]
+        label_data_2[mask] = 0
 
-    else:
-        logging.warning('The provided mask has MORE THAN 2 entity '
-                        'cut_between_masks_streamlines function selected. '
-                        'This may cause problems with the outputed '
-                        'streamlines. Please inspect the output carefully.')
-        new_sft = cut_between_masks_streamlines(sft, binary_mask)
+        new_sft = cut_between_mask_two_blobs_streamlines(sft, label_data_1,
+                                                         binary_mask_2=label_data_2)
 
+    # Saving
     if len(new_sft) == 0:
         logging.warning('No streamline intersected the provided mask. '
                         'Saving empty tractogram.')
-    elif args.error_rate is not None:
+    elif args.compress_th:
         compressed_strs = [compress_streamlines(
-            s, args.error_rate) for s in new_sft.streamlines]
-        new_sft = StatefulTractogram.from_sft(compressed_strs, sft)
+            s, args.compress_th) for s in new_sft.streamlines]
+        new_sft = StatefulTractogram.from_sft(
+            compressed_strs, sft, data_per_streamline=sft.data_per_streamline)
 
     save_tractogram(new_sft, args.out_tractogram)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_decompose_connectivity.py` & `scilpy-2.0.0/scripts/scil_tractogram_segment_bundles_for_connectivity.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,70 +1,74 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
 Compute a connectivity matrix from a tractogram and a parcellation.
 
-Current strategy is to keep the longest streamline segment connecting
-2 regions. If the streamline crosses other gray matter regions before
-reaching its final connected region, the kept connection is still the
-longest. This is robust to compressed streamlines.
+Current strategy is to keep the longest streamline segment connecting 2
+regions. If the streamline crosses other gray matter regions before reaching
+its final connected region, the kept connection is still the longest. This is
+robust to compressed streamlines.
 
 The output file is a hdf5 (.h5) where the keys are 'LABEL1_LABEL2' and each
 group is composed of 'data', 'offsets' and 'lengths' from the array_sequence.
-The 'data' is stored in VOX/CORNER for simplicity and efficiency.
+The 'data' is stored in VOX/CORNER for simplicity and efficiency. See script
+scil_tractogram_convert_hdf5_to_trk.py to convert to a list of .trk bundles.
 
 For the --outlier_threshold option the default is a recommended good trade-off
 for a freesurfer parcellation. With smaller parcels (brainnetome, glasser) the
 threshold should most likely be reduced.
+
 Good candidate connections to QC are the brainstem to precentral gyrus
 connection and precentral left to precentral right connection, or equivalent
-in your parcellation."
+in your parcellation.
 
 NOTE: this script can take a while to run. Please be patient.
 Example: on a tractogram with 1.8M streamlines, running on a SSD:
 - 15 minutes without post-processing, only saving final bundles.
 - 30 minutes with full post-processing, only saving final bundles.
 - 60 minutes with full post-processing, saving all possible files.
+
+Formerly: scil_decompose_connectivity.py
 """
 
 import argparse
 import itertools
 import logging
 import os
 import time
 
 import coloredlogs
-from dipy.io.stateful_tractogram import (StatefulTractogram,
-                                         set_sft_logger_level)
+from dipy.io.stateful_tractogram import StatefulTractogram
 from dipy.io.streamline import save_tractogram
 from dipy.io.utils import get_reference_info, is_header_compatible
 from dipy.tracking.streamlinespeed import length
 import h5py
 import nibabel as nib
 from nibabel.streamlines.array_sequence import ArraySequence
 import numpy as np
 
 from scilpy.image.labels import get_data_as_labels
+from scilpy.io.hdf5 import (construct_hdf5_header,
+                            construct_hdf5_group_from_streamlines)
 from scilpy.io.streamlines import load_tractogram_with_reference
-from scilpy.io.utils import (add_bbox_arg,
-                             add_overwrite_arg,
-                             add_processes_arg,
-                             add_verbose_arg,
-                             add_reference_arg,
-                             assert_inputs_exist,
+from scilpy.io.utils import (add_bbox_arg, add_overwrite_arg,
+                             add_processes_arg, add_verbose_arg,
+                             add_reference_arg, assert_inputs_exist,
                              assert_outputs_exist,
                              assert_output_dirs_exist_and_empty,
                              validate_nbr_processes)
-from scilpy.tractanalysis.features import (remove_outliers,
-                                           remove_loops_and_sharp_turns)
+from scilpy.tractanalysis.bundle_operations import remove_outliers
 from scilpy.tractanalysis.tools import (compute_connectivity,
-                                        compute_streamline_segment,
                                         extract_longest_segments_from_profile)
-from scilpy.tractanalysis.uncompress import uncompress
+from scilpy.tractograms.uncompress import uncompress
+from scilpy.tractograms.streamline_operations import \
+    remove_loops_and_sharp_turns
+from scilpy.tractograms.streamline_and_mask_operations import \
+    compute_streamline_segment
 
 
 def _get_output_paths(args):
     root_dir = args.out_dir
     paths = {'raw': os.path.join(root_dir, 'raw_connections/'),
              'final': os.path.join(root_dir, 'final_connections/'),
              'invalid_length': os.path.join(root_dir, 'invalid_length/'),
@@ -120,23 +124,16 @@
             if (norm < 0.001).any():  # or len(sft.streamlines[i]) <= 1:
                 indices.append(i)
 
         indices = np.setdiff1d(range(len(sft)), indices).astype(np.uint32)
         sft = sft[indices]
 
         group = hdf5_file.create_group('{}_{}'.format(in_label, out_label))
-        group.create_dataset('data', data=sft.streamlines._data,
-                             dtype=np.float32)
-        group.create_dataset('offsets', data=sft.streamlines._offsets,
-                             dtype=np.int64)
-        group.create_dataset('lengths', data=sft.streamlines._lengths,
-                             dtype=np.int32)
-        for key in sft.data_per_streamline.keys():
-            group.create_dataset(key, data=sft.data_per_streamline[key],
-                                 dtype=np.float32)
+        construct_hdf5_group_from_streamlines(group, sft.streamlines,
+                                              dps=sft.data_per_streamline)
 
     if args.out_dir:
         saving_options = _get_saving_options(args)
         out_paths = _get_output_paths(args)
 
         if saving_options[save_type] and len(sft):
             out_name = os.path.join(out_paths[step_type],
@@ -159,16 +156,16 @@
     return valid, invalid
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         formatter_class=argparse.RawTextHelpFormatter,
         description=__doc__)
-    p.add_argument('in_tractogram',
-                   help='Tractogram filename. Format must be one of \n'
+    p.add_argument('in_tractograms', nargs='+',
+                   help='Tractogram filenames. Format must be one of \n'
                         'trk, tck, vtk, fib, dpy.')
     p.add_argument('in_labels',
                    help='Labels file name (nifti). Labels must have 0 as '
                         'background.')
     p.add_argument('out_hdf5',
                    help='Output hdf5 file (.h5).')
 
@@ -219,30 +216,33 @@
     s.add_argument('--save_discarded', action='store_true',
                    help='If set, will save discarded streamlines in '
                         'subdirectories.\n'
                         'Includes loops, outliers and qb_loops.')
 
     p.add_argument('--out_labels_list', metavar='OUT_FILE',
                    help='Save the labels list as text file.\n'
-                        'Needed for scil_compute_connectivity.py and others.')
+                        'Needed for scil_connectivity_compute_matrices.py and '
+                        'others.')
 
     add_reference_arg(p)
+    add_bbox_arg(p)
     add_processes_arg(p)
     add_verbose_arg(p)
     add_overwrite_arg(p)
-    add_bbox_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
+    coloredlogs.install(level=logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, [args.in_tractogram, args.in_labels],
+    assert_inputs_exist(parser, args.in_tractograms+[args.in_labels],
                         args.reference)
     assert_outputs_exist(parser, args, args.out_hdf5)
     nbr_cpu = validate_nbr_processes(parser, args)
 
     # HDF5 will not overwrite the file
     if os.path.isfile(args.out_hdf5):
         os.remove(args.out_hdf5)
@@ -254,55 +254,49 @@
 
     if args.out_dir:
         if os.path.abspath(args.out_dir) == os.getcwd():
             parser.error('Do not use the current path as output directory.')
         assert_output_dirs_exist_and_empty(parser, args, args.out_dir,
                                            create_dir=True)
 
-    log_level = logging.INFO if args.verbose else logging.WARNING
-    logging.getLogger().setLevel(log_level)
-    coloredlogs.install(level=log_level)
-    set_sft_logger_level('WARNING')
-
+    # Load everything
     img_labels = nib.load(args.in_labels)
     data_labels = get_data_as_labels(img_labels)
-    real_labels = np.unique(data_labels)[1:]
+    real_labels = np.unique(data_labels)[1:]   # Removing the background 0.
     if args.out_labels_list:
         np.savetxt(args.out_labels_list, real_labels, fmt='%i')
 
     # Voxel size must be isotropic, for speed/performance considerations
     vox_sizes = img_labels.header.get_zooms()
     if not np.allclose(np.mean(vox_sizes), vox_sizes, atol=1e-01):
         parser.error('Labels must be isotropic')
 
     logging.info('*** Loading streamlines ***')
     time1 = time.time()
-    sft = load_tractogram_with_reference(parser, args, args.in_tractogram)
+    sft = None
+    for in_tractogram in args.in_tractograms:
+        if sft is None:
+            sft = load_tractogram_with_reference(parser, args, in_tractogram)
+            if not is_header_compatible(sft, img_labels):
+                raise IOError('{} and {} do not have a compatible '
+                              'header'.format(in_tractogram, args.in_labels))
+        else:
+            sft += load_tractogram_with_reference(parser, args, in_tractogram)
 
-    # If loaded with invalid (bbox_check False), remove invalid streamlines
-    # before continuing.
-    if not args.bbox_check:
-        sft.remove_invalid_streamlines()
-        
     time2 = time.time()
     logging.info('    Loading {} streamlines took {} sec.'.format(
         len(sft), round(time2 - time1, 2)))
 
-    if not is_header_compatible(sft, img_labels):
-        raise IOError('{} and {}do not have a compatible header'.format(
-            args.in_tractogram, args.in_labels))
-
     sft.to_vox()
     sft.to_corner()
-    # Get all streamlines intersection indices
-    logging.info('*** Computing streamlines intersection ***')
-    time1 = time.time()
 
+    # Get the indices of the voxels traversed by each streamline
+    logging.info('*** Computing voxels traversed by each streamline ***')
+    time1 = time.time()
     indices, points_to_idx = uncompress(sft.streamlines, return_mapping=True)
-
     time2 = time.time()
     logging.info('    Streamlines intersection took {} sec.'.format(
         round(time2 - time1, 2)))
 
     # Compute the connectivity mapping
     logging.info('*** Computing connectivity information ***')
     time1 = time.time()
@@ -322,26 +316,22 @@
 
     # Saving will be done from streamlines already in the right space
     comb_list = list(itertools.combinations(real_labels, r=2))
     comb_list.extend(zip(real_labels, real_labels))
 
     iteration_counter = 0
     with h5py.File(args.out_hdf5, 'w') as hdf5_file:
-        affine, dimensions, voxel_sizes, voxel_order = get_reference_info(sft)
-        hdf5_file.attrs['affine'] = affine
-        hdf5_file.attrs['dimensions'] = dimensions
-        hdf5_file.attrs['voxel_sizes'] = voxel_sizes
-        hdf5_file.attrs['voxel_order'] = voxel_order
+        construct_hdf5_header(hdf5_file, sft)
 
-        # Each connections is processed independently. Multiprocessing would be
+        # Each connection is processed independently. Multiprocessing would be
         # a burden on the I/O of most SSD/HD
         for in_label, out_label in comb_list:
             if iteration_counter > 0 and iteration_counter % 100 == 0:
-                logging.info('Split {} nodes out of {}'.format(iteration_counter,
-                                                               len(comb_list)))
+                logging.info('Split {} nodes out of {}'.format(
+                    iteration_counter, len(comb_list)))
             iteration_counter += 1
 
             pair_info = []
             if in_label not in con_info:
                 continue
             elif out_label in con_info[in_label]:
                 pair_info.extend(con_info[in_label][out_label])
@@ -399,20 +389,22 @@
                 valid_length_ids = range(len(connecting_streamlines))
 
             if not len(valid_length):
                 continue
 
             valid_length_sft = raw_sft[valid_length_ids]
             _save_if_needed(valid_length_sft, hdf5_file, args,
-                            'intermediate', 'valid_length', in_label, out_label)
+                            'intermediate', 'valid_length', in_label,
+                            out_label)
 
             if not args.no_remove_loops:
-                no_loop_ids = remove_loops_and_sharp_turns(valid_length,
-                                                           args.loop_max_angle,
-                                                           num_processes=nbr_cpu)
+                no_loop_ids = remove_loops_and_sharp_turns(
+                    valid_length,
+                    args.loop_max_angle,
+                    num_processes=nbr_cpu)
                 loop_ids = np.setdiff1d(np.arange(len(valid_length)),
                                         no_loop_ids)
 
                 loops_sft = valid_length_sft[loop_ids]
                 no_loops = valid_length[no_loop_ids]
                 _save_if_needed(loops_sft, hdf5_file, args,
                                 'discarded', 'loops', in_label, out_label)
@@ -423,18 +415,19 @@
             if not len(no_loops):
                 continue
             no_loops_sft = valid_length_sft[no_loop_ids]
             _save_if_needed(no_loops_sft, hdf5_file, args,
                             'intermediate', 'no_loops', in_label, out_label)
 
             if not args.no_remove_outliers:
-                outliers_ids, inliers_ids = remove_outliers(no_loops,
-                                                            args.outlier_threshold,
-                                                            nb_samplings=10,
-                                                            fast_approx=True)
+                outliers_ids, inliers_ids = remove_outliers(
+                    no_loops,
+                    args.outlier_threshold,
+                    nb_samplings=10,
+                    fast_approx=True)
 
                 outliers_sft = no_loops_sft[outliers_ids]
                 inliers = no_loops[inliers_ids]
                 _save_if_needed(outliers_sft, hdf5_file, args,
                                 'discarded', 'outliers', in_label, out_label)
             else:
                 inliers = no_loops
@@ -447,15 +440,14 @@
             _save_if_needed(inliers_sft, hdf5_file, args,
                             'intermediate', 'inliers', in_label, out_label)
 
             if not args.no_remove_curv_dev:
                 no_qb_curv_ids = remove_loops_and_sharp_turns(
                     inliers,
                     args.loop_max_angle,
-                    use_qb=True,
                     qb_threshold=args.curv_qb_distance,
                     num_processes=nbr_cpu)
                 qb_curv_ids = np.setdiff1d(np.arange(len(inliers)),
                                            no_qb_curv_ids)
 
                 qb_curv_sft = inliers_sft[qb_curv_ids]
                 _save_if_needed(qb_curv_sft, hdf5_file, args,
```

### Comparing `scilpy-1.5.post2/scripts/scil_detect_streamlines_loops.py` & `scilpy-2.0.0/scripts/scil_tractogram_detect_loops.py`

 * *Files 14% similar despite different names*

```diff
@@ -2,133 +2,126 @@
 # -*- coding: utf-8 -*-
 
 """
 This script can be used to remove loops in two types of streamline datasets:
 
   - Whole brain: For this type, the script removes streamlines if they
     make a loop with an angle of more than 360 degrees. It's possible to change
-    this angle with the -a option. Warning: Don't use --qb option for a
+    this angle with the --angle option. Warning: Don't use --qb option for a
     whole brain tractography.
 
   - Bundle dataset: For this type, it is possible to remove loops and
-    streamlines outside of the bundle. For the sharp angle turn, use --qb
-    option.
+    streamlines outside the bundle. For the sharp angle turn, use --qb option.
 
-----------------------------------------------------------------------------
-Reference:
-QuickBundles based on [Garyfallidis12] Frontiers in Neuroscience, 2012.
-----------------------------------------------------------------------------
+Formerly: scil_detect_streamlines_loops.py
 """
 
 import argparse
 import json
 import logging
 
-from dipy.io.streamline import save_tractogram
 import numpy as np
 
-from scilpy.io.streamlines import load_tractogram_with_reference
+from scilpy.io.streamlines import load_tractogram_with_reference, \
+    save_tractogram
 from scilpy.io.utils import (add_json_args,
+                             add_verbose_arg,
                              add_overwrite_arg,
                              add_processes_arg,
                              add_reference_arg,
                              assert_inputs_exist,
                              assert_outputs_exist,
                              check_tracts_same_format,
-                             validate_nbr_processes)
-from scilpy.utils.streamlines import filter_tractogram_data
-from scilpy.tractanalysis.features import remove_loops_and_sharp_turns
+                             validate_nbr_processes, ranged_type)
+from scilpy.tractograms.streamline_operations import \
+    remove_loops_and_sharp_turns
+
+
+EPILOG = """
+References:
+    QuickBundles, based on [Garyfallidis12] Frontiers in Neuroscience, 2012.
+"""
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter,
-                                description=__doc__)
+                                description=__doc__, epilog=EPILOG)
     p.add_argument('in_tractogram',
                    help='Tractogram input file name.')
     p.add_argument('out_tractogram',
                    help='Output tractogram without loops.')
-    p.add_argument('--looping_tractogram',
+    p.add_argument('--looping_tractogram', metavar='out_filename',
                    help='If set, saves detected looping streamlines.')
-    p.add_argument('--qb', action='store_true',
-                   help='If set, uses QuickBundles to detect\n' +
-                        'outliers (loops, sharp angle turns).\n' +
-                        'Should mainly be used with bundles. '
-                        '[%(default)s]')
-    p.add_argument('--threshold', default=8., type=float,
-                   help='Maximal streamline to bundle distance\n' +
-                        'for a streamline to be considered as\n' +
-                        'a tracking error. [%(default)s]')
-    p.add_argument('-a', dest='angle', default=360, type=float,
+    p.add_argument('--qb', nargs='?', metavar='threshold', dest='qb_threshold',
+                   const=8., type=ranged_type(float, 0.0, None),
+                   help='If set, uses QuickBundles to detect outliers (loops, '
+                        'sharp angle \nturns). Given threshold is the maximal '
+                        'streamline to bundle \ndistance for a streamline to '
+                        'be considered as a tracking error.\nDefault if '
+                        'set: [%(const)s]')
+    p.add_argument('--angle', default=360, type=ranged_type(float, 0.0, 360.0),
                    help='Maximum looping (or turning) angle of\n' +
                         'a streamline in degrees. [%(default)s]')
     p.add_argument('--display_counts', action='store_true',
                    help='Print streamline count before and after filtering')
+    p.add_argument('--no_empty', action='store_true',
+                   help="If set, will not save outputs if they are empty.")
 
+    add_json_args(p)
     add_processes_arg(p)
-    add_overwrite_arg(p)
     add_reference_arg(p)
-    add_json_args(p)
+    add_verbose_arg(p)
+    add_overwrite_arg(p)
+
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, args.in_tractogram)
+    # Verifications
+    assert_inputs_exist(parser, args.in_tractogram, args.reference)
     assert_outputs_exist(parser, args, args.out_tractogram,
                          optional=args.looping_tractogram)
     check_tracts_same_format(parser, [args.in_tractogram, args.out_tractogram,
                                       args.looping_tractogram])
     nbr_cpu = validate_nbr_processes(parser, args)
 
-    if args.threshold <= 0:
-        parser.error('Threshold "{}" '.format(args.threshold) +
-                     'must be greater than 0')
-
-    if args.angle <= 0:
-        parser.error('Angle "{}" '.format(args.angle) +
-                     'must be greater than 0')
-
-    tractogram = load_tractogram_with_reference(
-        parser, args, args.in_tractogram)
-
-    streamlines = tractogram.streamlines
-
-    ids_c = []
-
-    ids_l = []
-
-    if len(streamlines) > 1:
-        ids_c = remove_loops_and_sharp_turns(
-            streamlines, args.angle, use_qb=args.qb,
-            qb_threshold=args.threshold,
-            num_processes=nbr_cpu)
-        ids_l = np.setdiff1d(np.arange(len(streamlines)), ids_c)
-    else:
+    # Loading
+    sft = load_tractogram_with_reference(parser, args, args.in_tractogram)
+    nb_streamlines = len(sft.streamlines)
+
+    if nb_streamlines < 1:
         parser.error(
-            'Zero or one streamline in {}'.format(args.in_tractogram) +
-            '. The file must have more than one streamline.')
+            'Zero or one streamline in {}. The file must have more than one '
+            'streamline.'.format(args.in_tractogram))
 
-    if len(ids_c) > 0:
-        sft_c = filter_tractogram_data(tractogram, ids_c)
-        save_tractogram(sft_c, args.out_tractogram)
-    else:
-        logging.warning(
-            'No clean streamlines in {}'.format(args.in_tractogram))
+    # Processing
+    ids_clean = remove_loops_and_sharp_turns(
+        sft.streamlines, args.angle, qb_threshold=args.qb_threshold,
+        num_processes=nbr_cpu)
+    if len(ids_clean) == 0:
+        logging.warning('No clean streamlines in {}. They are all looping '
+                        'streamlines? Check your parameters.'
+                        .format(args.in_tractogram))
+    sft_clean = sft[ids_clean]
 
     if args.display_counts:
-        sc_bf = len(tractogram.streamlines)
-        sc_af = len(sft_c.streamlines)
-        print(json.dumps({'streamline_count_before_filtering': int(sc_bf),
+        sc_af = len(sft_clean.streamlines)
+        print(json.dumps({'streamline_count_before_filtering': nb_streamlines,
                          'streamline_count_after_filtering': int(sc_af)},
                          indent=args.indent))
 
-    if len(ids_l) == 0:
-        logging.warning('No loops in {}'.format(args.in_tractogram))
-    elif args.looping_tractogram:
-        sft_l = filter_tractogram_data(tractogram, ids_l)
-        save_tractogram(sft_l, args.looping_tractogram)
+    # Saving
+    save_tractogram(sft_clean, args.out_tractogram,
+                    args.no_empty)
+    if args.looping_tractogram:
+        ids_removed = np.setdiff1d(np.arange(nb_streamlines), ids_clean)
+        sft_l = sft[ids_removed]
+        save_tractogram(sft_l, args.looping_tractogram,
+                        args.no_empty)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_dilate_labels.py` & `scilpy-2.0.0/scripts/scil_labels_dilate.py`

 * *Files 11% similar despite different names*

```diff
@@ -6,29 +6,32 @@
     "label_to_fill" if close enough to it ("distance").
 - "label_to_dilate", by default (None) will be all
         non-"label_to_fill" and non-"label_not_to_dilate".
 - "label_not_to_dilate" will not be changed, but will not dilate.
 - "mask" is where the dilation is allowed (constrained)
     in addition to "background_label" (logical AND)
 
->>> scil_dilate_labels.py wmparc_t1.nii.gz wmparc_dil.nii.gz \\
+>>> scil_labels_dilate.py wmparc_t1.nii.gz wmparc_dil.nii.gz \\
     --label_to_fill 0 5001 5002 \\
     --label_not_to_dilate 4 43 10 11 12 49 50 51
+
+Formerly: scil_dilate_labels.py
 """
 
 import argparse
 import logging
 
 import nibabel as nib
 import numpy as np
 
 from scilpy.image.labels import get_data_as_labels, dilate_labels
 from scilpy.io.image import get_data_as_mask
 from scilpy.io.utils import (add_overwrite_arg, add_processes_arg,
-                             assert_inputs_exist, assert_outputs_exist)
+                             assert_inputs_exist, add_verbose_arg,
+                             assert_outputs_exist, assert_headers_compatible)
 
 EPILOG = """
     References:
         [1] Al-Sharif N.B., St-Onge E., Vogel J.W., Theaud G.,
             Evans A.C. and Descoteaux M. OHBM 2019.
             Surface integration for connectome analysis in age prediction.
     """
@@ -36,63 +39,56 @@
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(description=__doc__, epilog=EPILOG,
                                 formatter_class=argparse.RawTextHelpFormatter)
 
     p.add_argument('in_file',
                    help='Path of the volume (nii or nii.gz).')
-
     p.add_argument('out_file',
                    help='Output filename of the dilated labels.')
 
     p.add_argument('--distance', type=float, default=2.0,
                    help='Maximal distance to dilate (in mm) [%(default)s].')
-
     p.add_argument('--labels_to_dilate', type=int, nargs='+', default=None,
                    help='Label list to dilate. By default it dilates all \n'
                         'labels not in labels_to_fill nor in '
                         'labels_not_to_dilate.')
-
     p.add_argument('--labels_to_fill', type=int, nargs='+', default=[0],
                    help='Background id / labels to be filled [%(default)s],\n'
                         ' the first one is given as output background value.')
-
     p.add_argument('--labels_not_to_dilate', type=int, nargs='+', default=[],
                    help='Label list not to dilate.')
-
     p.add_argument('--mask',
                    help='Only dilate values inside the mask.')
 
     add_processes_arg(p)
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
-    logging.getLogger().setLevel(logging.INFO)
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, args.in_file, optional=args.mask)
     assert_outputs_exist(parser, args, args.out_file)
+    assert_headers_compatible(parser, args.in_file, optional=args.mask)
 
     if args.nbr_processes is None:
         args.nbr_processes = -1
 
     # load volume
     volume_nib = nib.load(args.in_file)
     data = get_data_as_labels(volume_nib)
     vox_size = np.reshape(volume_nib.header.get_zooms(), (1, 3))
 
-    if args.mask:
-        mask_nib = nib.load(args.mask)
-        mask_data = get_data_as_mask(mask_nib)
-    else:
-        mask_data = None
+    mask_data = get_data_as_mask(nib.load(args.mask)) if args.mask else None
 
     data = dilate_labels(data, vox_size, args.distance, args.nbr_processes,
                          labels_to_dilate=args.labels_to_dilate,
                          labels_not_to_dilate=args.labels_not_to_dilate,
                          labels_to_fill=args.labels_to_fill,
                          mask=mask_data)
```

### Comparing `scilpy-1.5.post2/scripts/scil_estimate_bundles_diameter.py` & `scilpy-2.0.0/scripts/scil_bundle_diameter.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,65 +1,76 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
 Script to estimate the diameter of bundle(s) along their length.
+See also scil_bundle_shape_measures.py, which prints a quick estimate of
+the diameter (volume / length). The computation here is more complex and done
+for each section of the bundle.
+
 The script expects:
-- bundles with coherent endpoints from scil_uniformize_streamlines_endpoints.py
-- labels maps with around 5-50 points scil_compute_bundle_voxel_label_map.py
+- bundles with coherent endpoints from scil_tractogram_uniformize_endpoints.py
+- labels maps with around 5-50 points scil_bundle_label_map.py
     <5 is not enough, high risk of bad fit
     >50 is too much, high risk of bad fit
 - bundles that are close to a tube
     without major fanning in a single axis
     fanning is in 2 directions (uniform dispersion) good approximation
 
 The scripts prints a JSON file with mean/std to be compatible with tractometry.
 WARNING: STD is in fact an ERROR measure from the fit and NOT an STD.
 
 Since the estimation and fit quality is not always intuitive for some bundles
 and the tube with varying diameter is not easy to color/visualize,
 the script comes with its own VTK rendering to allow exploration of the data.
 (optional).
+
+Formerly: scil_estimate_bundles_diameter.py
 """
 
 import argparse
 import json
+import logging
 import os
 
-from dipy.io.utils import is_header_compatible
-from fury import window, actor
+from fury import actor
 import nibabel as nib
 import numpy as np
 from scipy.linalg import svd
 from scipy.ndimage import map_coordinates, gaussian_filter
 
 from scilpy.io.streamlines import load_tractogram_with_reference
-from scilpy.io.utils import (add_overwrite_arg,
+from scilpy.io.utils import (add_json_args,
+                             add_overwrite_arg,
                              add_reference_arg,
-                             add_json_args,
+                             add_verbose_arg,
+                             assert_headers_compatible,
                              assert_inputs_exist,
                              assert_output_dirs_exist_and_empty,
-                             parser_color_type,
-                             snapshot)
-from scilpy.viz.scene_utils import create_tube_with_radii
-from scilpy.viz.utils import get_colormap
+                             parser_color_type)
+from scilpy.viz.backends.fury import (create_interactive_window,
+                                      create_scene,
+                                      snapshot_scenes)
+from scilpy.viz.backends.vtk import create_tube_with_radii
+from scilpy.viz.color import get_lookup_table
+from scilpy.viz.screenshot import compose_image
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__, formatter_class=argparse.RawTextHelpFormatter)
     p.add_argument('in_bundles', nargs='+',
-                   help='List of tractography files supported by nibabel.')
+                   help='List of tractography files.')
     p.add_argument('in_labels', nargs='+',
-                   help='List of labels maps that matches the bundles.')
+                   help='List of labels maps that match the bundles.')
 
     p.add_argument('--fitting_func', choices=['lin_up', 'lin_down', 'exp',
-                                              'inv', 'log'], default=None,
+                                              'inv', 'log'],
                    help='Function to weigh points using their distance.'
-                   '\n[Default: %(default)s]')
+                        '\n[Default: %(default)s]')
 
     p2 = p.add_argument_group(title='Visualization options')
     p3 = p2.add_mutually_exclusive_group()
     p3.add_argument('--show_rendering', action='store_true',
                     help='Display VTK window (optional).')
     p3.add_argument('--save_rendering', metavar='OUT_FOLDER',
                     help='Save VTK render in the specified folder (optional)')
@@ -69,21 +80,25 @@
                     help='Use the fitting error to color the tube.')
     p2.add_argument('--width', type=float, default=0.2,
                     help='Width of tubes or lines representing streamlines'
                     '\n[Default: %(default)s]')
     p2.add_argument('--opacity', type=float, default=0.2,
                     help='Opacity for the streamlines rendered with the tube.'
                     '\n[Default: %(default)s]')
+    p2.add_argument("--win_dims", nargs=2, metavar=("WIDTH", "HEIGHT"),
+                    default=(1920, 1080), type=int,
+                    help="The dimensions for the vtk window. [%(default)s]")
     p2.add_argument('--background', metavar=('R', 'G', 'B'), nargs=3,
                     default=[1, 1, 1], type=parser_color_type,
                     help='RBG values [0, 255] of the color of the background.'
                     '\n[Default: %(default)s]')
 
     add_reference_arg(p)
     add_json_args(p)
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def fit_circle_2d(x, y, dist_w):
     """
@@ -191,41 +206,50 @@
     center, radius = fit_circle_planar(proj_positions, dist_w)
     dist = np.linalg.norm(proj_positions - center, axis=1)
     error = np.average(np.sqrt((dist - radius)**2))
 
     return center, radius, error
 
 
+def snapshot(scene, win_dims, output_filename):
+    # Legacy. When this snapshotting gets updated to align with the
+    # viz module, snapshot_scenes should be called directly
+    snapshot = next(snapshot_scenes([scene], win_dims))
+    img = compose_image(snapshot, win_dims, "NONE")
+    img.save(output_filename)
+
+
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     # The number of labels maps must be equal to the number of bundles
     tmp = args.in_bundles + args.in_labels
     args.in_labels = args.in_bundles[(len(tmp) // 2):] + args.in_labels
     args.in_bundles = args.in_bundles[0:len(tmp) // 2]
-    assert_inputs_exist(parser, args.in_bundles+args.in_labels)
+    assert_inputs_exist(parser, args.in_bundles + args.in_labels,
+                        args.reference)
     assert_output_dirs_exist_and_empty(parser, args, [],
                                        optional=args.save_rendering)
 
+    # same subject: same header or coregistered subjects: same header
+    assert_headers_compatible(parser, args.in_bundles + args.in_labels,
+                              reference=args.reference)
+
     stats = {}
     num_digits_labels = 3
-    scene = window.Scene()
-    scene.background(tuple(map(int, args.background)))
+    actor_list = []
+    spatial_shape = nib.load(args.in_labels[0]).shape[:3]
     for i, filename in enumerate(args.in_bundles):
         sft = load_tractogram_with_reference(parser, args, filename)
         sft.to_vox()
         sft.to_corner()
         img_labels = nib.load(args.in_labels[i])
 
-        # same subject: same header or coregistered subjects: same header
-        if not is_header_compatible(sft, args.in_bundles[0]) \
-                or not is_header_compatible(img_labels, args.in_bundles[0]):
-            parser.error('All headers must be identical.')
-
         data_labels = img_labels.get_fdata()
         bundle_name, _ = os.path.splitext(os.path.basename(filename))
         unique_labels = np.unique(data_labels)[1:].astype(int)
 
         # Empty bundle should at least return a json
         if not len(sft):
             tmp_dict = {}
@@ -274,64 +298,71 @@
         for label in unique_labels:
             tmp_dict['{}'.format(label).zfill(num_digits_labels)] \
                 = {'mean': float(radius[label-1])*2,
                    'std': float(error[label-1])}
         stats[bundle_name] = {'diameter': tmp_dict}
 
         if args.show_rendering or args.save_rendering:
-            tube_actor = create_tube_with_radii(centroid_smooth, radius, error,
-                                                wireframe=args.wireframe,
-                                                error_coloring=args.error_coloring)
-            scene.add(tube_actor)
-            cmap = get_colormap('jet')
+            tube_actor = create_tube_with_radii(
+                centroid_smooth, radius, error,
+                wireframe=args.wireframe,
+                error_coloring=args.error_coloring)
+            actor_list.append(tube_actor)
+            # TODO : move streamline actor to fury backend
+            cmap = get_lookup_table('jet')
             coloring = cmap(pts_labels / np.max(pts_labels))[:, 0:3]
             streamlines_actor = actor.streamtube(sft.streamlines,
                                                  linewidth=args.width,
                                                  opacity=args.opacity,
                                                  colors=coloring)
-            scene.add(streamlines_actor)
+            actor_list.append(streamlines_actor)
 
             slice_actor = actor.slicer(data_labels, np.eye(4))
             slice_actor.opacity(0.0)
-            scene.add(slice_actor)
+            actor_list.append(slice_actor)
+
+    scene = create_scene(actor_list, "axial",
+                         spatial_shape[2] // 2, spatial_shape,
+                         args.win_dims[0] / args.win_dims[1],
+                         bg_color=tuple(map(int, args.background)))
 
     # If there's actually streamlines to display
     if args.show_rendering:
-        showm = window.ShowManager(scene, reset_camera=True)
-        showm.initialize()
-        showm.start()
+        create_interactive_window(scene, args.win_dims, "image")
     elif args.save_rendering:
+        # TODO : transform screenshotting to abide with viz module
         scene.reset_camera()
-        snapshot(scene, os.path.join(args.save_rendering, 'superior.png'),
-                 size=(1920, 1080), offscreen=True)
+        snapshot(scene, args.win_dims,
+                 os.path.join(args.save_rendering, 'superior.png'))
 
         scene.pitch(180)
         scene.reset_camera()
-        snapshot(scene, os.path.join(args.save_rendering, 'inferior.png'),
-                 size=(1920, 1080), offscreen=True)
+        snapshot(scene, args.win_dims,
+                 os.path.join(args.save_rendering, 'inferior.png'))
 
         scene.pitch(90)
         scene.set_camera(view_up=(0, 0, 1))
         scene.reset_camera()
-        snapshot(scene, os.path.join(args.save_rendering, 'posterior.png'),
-                 size=(1920, 1080), offscreen=True)
+        snapshot(scene, args.win_dims,
+                 os.path.join(args.save_rendering, 'posterior.png'))
 
         scene.pitch(180)
         scene.set_camera(view_up=(0, 0, 1))
         scene.reset_camera()
-        snapshot(scene, os.path.join(args.save_rendering, 'anterior.png'),
-                 size=(1920, 1080), offscreen=True)
+        snapshot(scene, args.win_dims,
+                 os.path.join(args.save_rendering, 'anterior.png'))
 
         scene.yaw(90)
         scene.reset_camera()
-        snapshot(scene, os.path.join(args.save_rendering, 'right.png'),
-                 size=(1920, 1080), offscreen=True)
+        snapshot(scene, args.win_dims,
+                 os.path.join(args.save_rendering, 'right.png'))
 
         scene.yaw(180)
         scene.reset_camera()
-        snapshot(scene, os.path.join(args.save_rendering, 'left.png'),
-                 size=(1920, 1080), offscreen=True)
+        snapshot(scene, args.win_dims,
+                 os.path.join(args.save_rendering, 'left.png'))
+
     print(json.dumps(stats, indent=args.indent, sort_keys=args.sort_keys))
 
 
 if __name__ == '__main__':
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_evaluate_bundles_binary_classification_measures.py` & `scilpy-2.0.0/scripts/scil_bundle_score_same_bundle_many_segmentations.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,19 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-Evaluate binary classification measures between gold standard and bundles.
-All tractograms must be in the same space (aligned to one reference)
-The measures can be applied to voxel-wise or streamline-wise representation.
+This script is intended to score many versions of a same bundle, compared to
+ONE ground truth / gold standard.
+
+See also scil_bundle_score_many_bundles_one_tractogram.py to score all bundles
+from a single tractogram by comparing each valid bundle to its ground truth.
+
+All tractograms must be in the same space (aligned to one reference).
+The measures can be applied to a voxel-wise or streamline-wise representation.
 
 A gold standard must be provided for the desired representation.
 A gold standard would be a segmentation from an expert or a group of experts.
 If only the streamline-wise representation is provided without a voxel-wise
 gold standard, it will be computed from the provided streamlines.
 At least one of the two representations is required.
 
@@ -16,14 +21,16 @@
 which the segmentation is performed.
 The gold standard tracking mask is the tracking mask used by the tractography
 algorighm to generate the gold standard tractogram.
 
 The computed binary classification measures are:
 sensitivity, specificity, precision, accuracy, dice, kappa, youden for both
 the streamline and voxel representation (if provided).
+
+Formerly: scil_evaluate_bundles_binary_classification_measures.py
 """
 
 import argparse
 import itertools
 import json
 import logging
 import multiprocessing
@@ -38,15 +45,15 @@
                              add_overwrite_arg,
                              add_processes_arg,
                              add_reference_arg,
                              add_verbose_arg,
                              assert_inputs_exist,
                              assert_outputs_exist,
                              link_bundles_and_reference,
-                             validate_nbr_processes)
+                             validate_nbr_processes, assert_headers_compatible)
 from scilpy.io.streamlines import load_tractogram_with_reference
 from scilpy.tractanalysis.streamlines_metrics import compute_tract_counts_map
 from scilpy.tractanalysis.reproducibility_measures import binary_classification
 from scilpy.tractograms.tractogram_operations import intersection_robust
 
 
 def _build_arg_parser():
@@ -54,23 +61,25 @@
                                 formatter_class=argparse.RawTextHelpFormatter)
     p.add_argument('in_bundles', nargs='+',
                    help='Path of the input bundles.')
     p.add_argument('out_json',
                    help='Path of the output json.')
     p.add_argument('--streamlines_measures', nargs=2,
                    metavar=('GOLD_STANDARD_STREAMLINES', 'TRACTOGRAM'),
-                   help='The gold standard bundle and the original tractogram.')
+                   help='The gold standard bundle and the original '
+                   'tractogram.')
     p.add_argument('--voxels_measures', nargs=2,
                    metavar=('GOLD_STANDARD_MASK', 'TRACKING MASK'),
-                   help='The gold standard mask and the original tracking mask.')
+                   help='The gold standard mask and the original tracking '
+                   'mask.')
 
     add_processes_arg(p)
     add_reference_arg(p)
-    add_verbose_arg(p)
     add_json_args(p)
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def compute_voxel_measures(args):
     bundle_filename, bundle_reference = args[0]
@@ -89,18 +98,19 @@
 
     binary_3d = compute_tract_counts_map(bundle_streamlines, bundle_dimensions)
     binary_3d[binary_3d > 0] = 1
 
     binary_3d_indices = np.where(binary_3d.flatten() > 0)[0]
     gs_binary_3d_indices = np.where(gs_binary_3d.flatten() > 0)[0]
 
-    voxels_binary = binary_classification(binary_3d_indices,
-                                          gs_binary_3d_indices,
-                                          int(np.prod(tracking_mask.shape)),
-                                          mask_count=np.count_nonzero(tracking_mask))
+    voxels_binary = binary_classification(
+                            binary_3d_indices,
+                            gs_binary_3d_indices,
+                            int(np.prod(tracking_mask.shape)),
+                            mask_count=np.count_nonzero(tracking_mask))
 
     return dict(zip(['sensitivity_voxels',
                      'specificity_voxels',
                      'precision_voxels',
                      'accuracy_voxels',
                      'dice_voxels',
                      'kappa_voxels',
@@ -123,15 +133,16 @@
     bundle_streamlines = bundle_sft.streamlines
     _, bundle_dimensions, _, _ = bundle_sft.space_attributes
 
     if not bundle_streamlines:
         logging.info('{} is empty'.format(bundle_filename))
         return None
 
-    _, streamlines_indices = intersection_robust([wb_streamlines, bundle_streamlines])
+    _, streamlines_indices = intersection_robust([wb_streamlines,
+                                                  bundle_streamlines])
 
     streamlines_binary = binary_classification(streamlines_indices,
                                                gs_streamlines_indices,
                                                len(wb_streamlines))
 
     return dict(zip(['sensitivity_streamlines',
                      'specificity_streamlines',
@@ -142,20 +153,20 @@
                      'youden_streamlines'],
                     streamlines_binary))
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    if args.verbose:
-        logging.getLogger().setLevel(logging.INFO)
-
-    assert_inputs_exist(parser, args.in_bundles)
+    assert_inputs_exist(parser, args.in_bundles, args.reference)
     assert_outputs_exist(parser, args, args.out_json)
+    assert_headers_compatible(parser, args.in_bundles,
+                              reference=args.reference)
 
     if (not args.streamlines_measures) and (not args.voxels_measures):
         parser.error('At least one of the two modes is needed')
 
     nbr_cpu = validate_nbr_processes(parser, args)
 
     all_binary_metrics = []
@@ -174,15 +185,16 @@
                                                 args.streamlines_measures[0])
         gs_sft.to_vox()
         gs_sft.to_corner()
         gs_streamlines = gs_sft.streamlines
         _, gs_dimensions, _, _ = gs_sft.space_attributes
 
         # Prepare the gold standard only once
-        _, gs_streamlines_indices = intersection_robust([wb_streamlines, gs_streamlines])
+        _, gs_streamlines_indices = intersection_robust([wb_streamlines,
+                                                         gs_streamlines])
 
         if nbr_cpu == 1:
             streamlines_dict = []
             for i in bundles_references_tuple_extended:
                 streamlines_dict.append(compute_streamlines_measures(
                     [i, wb_streamlines, gs_streamlines_indices]))
         else:
@@ -203,16 +215,16 @@
 
         tracking_mask_data = compute_tract_counts_map(wb_streamlines,
                                                       gs_dimensions)
         tracking_mask_data[tracking_mask_data > 0] = 1
     else:
         gs_binary_3d = get_data_as_mask(nib.load(args.voxels_measures[0]))
         gs_binary_3d[gs_binary_3d > 0] = 1
-        tracking_mask_data = get_data_as_mask(nib.load(args.voxels_measures[1]))
-        tracking_mask_data[tracking_mask_data > 0] = 1
+        tracking_mask_data = get_data_as_mask(
+            nib.load(args.voxels_measures[1]))
 
     if nbr_cpu == 1:
         voxels_dict = []
         for i in bundles_references_tuple_extended:
             voxels_dict.append(compute_voxel_measures(
                 [i, tracking_mask_data, gs_binary_3d]))
     else:
```

### Comparing `scilpy-1.5.post2/scripts/scil_evaluate_bundles_individual_measures.py` & `scilpy-2.0.0/scripts/scil_bundle_shape_measures.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,85 +1,93 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-Evaluate basic measurements of bundles, all at once.
-All tractograms must be in the same space (aligned to one reference)
+Evaluate basic measurements of bundle(s).
+
 The computed measures are:
-volume, volume_endpoints, streamlines_count, avg_length, std_length,
-min_length, max_length, span, curl, diameter, elongation, surface area,
-irregularity, end surface area, radius, end surface irregularity,
-mean_curvature, fractal dimension.
+    - volume_info: volume, volume_endpoints
+    - streamlines_info: streamlines_count, avg_length (in mm or in number of
+      point), average step size, min_length, max_length.
+      ** You may also get this information with scil_tractogram_print_info.py.
+    - shape_info: span, curl, diameter, elongation, surface area,
+      irregularity, end surface area, radius, end surface irregularity,
+      mean_curvature, fractal dimension.
+      ** The diameter, here, is a simple estimation using volume / length.
+      For a more complex calculation, see scil_bundle_diameter.py.
+
+With more than one bundle, the measures are averaged over bundles. All
+tractograms must be in the same space.
 
 The set average contains the average measures of all input bundles. The
 measures that are dependent on the streamline count are weighted by the number
 of streamlines of each bundle. Each of these average measure is computed by
 first summing the multiple of a measure and the streamline count of each
 bundle and divide the sum by the total number of streamlines. Thus, measures
 including length and span are essentially averages of all the streamlines.
 Other streamline-related set measure are computed with other set averages.
-Whereas, bundle-related measures are computed as an average of all bundles.
+Whereas bundle-related measures are computed as an average of all bundles.
 These measures include volume and surface area.
 
 The fractal dimension is dependent on the voxel size and the number of voxels.
 If data comparison is performed, the bundles MUST be in same resolution.
+
+Formerly: scil_compute_bundle_volume.py or
+scil_evaluate_bundles_individual_measures.py
 """
 
 import argparse
 import itertools
 import json
 import logging
 import multiprocessing
 
 from dipy.io.streamline import load_tractogram
 from dipy.tracking.metrics import mean_curvature
 from dipy.tracking.utils import length
 import numpy as np
 
-from scilpy.io.utils import (add_json_args,
-                             add_overwrite_arg,
-                             add_processes_arg,
-                             add_reference_arg,
-                             assert_inputs_exist,
-                             assert_outputs_exist,
-                             link_bundles_and_reference,
-                             validate_nbr_processes)
-
+from scilpy.io.utils import (add_json_args, add_verbose_arg,
+                             add_overwrite_arg, add_processes_arg,
+                             add_reference_arg, assert_inputs_exist,
+                             assert_outputs_exist, link_bundles_and_reference,
+                             validate_nbr_processes, assert_headers_compatible)
+from scilpy.tractanalysis.bundle_operations import uniformize_bundle_sft
 from scilpy.tractanalysis.reproducibility_measures \
-    import (get_endpoints_density_map,
-            get_head_tail_density_maps,
-            approximate_surface_node,
+    import (approximate_surface_node,
             compute_fractal_dimension)
 from scilpy.tractanalysis.streamlines_metrics import compute_tract_counts_map
-from scilpy.utils.streamlines import uniformize_bundle_sft
+from scilpy.tractograms.streamline_and_mask_operations import \
+    get_endpoints_density_map, get_head_tail_density_maps
 
 EPILOG = """
 References:
 [1] Fang-Cheng Yeh. 2020.
     Shape analysis of the human association pathways. NeuroImage.
 """
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(description=__doc__, epilog=EPILOG,
                                 formatter_class=argparse.RawTextHelpFormatter)
     p.add_argument('in_bundles', nargs='+',
                    help='Path of the input bundles.')
-    p.add_argument('out_json',
-                   help='Path of the output file.')
+    p.add_argument('--out_json',
+                   help='Path of the output file. If not given, the output '
+                        'is simply printed on screen.')
     p.add_argument('--group_statistics', action='store_true',
-                   help='Show average measures \n'
-                        '[%(default)s].')
+                   help='Show average measures [%(default)s].')
 
     p.add_argument('--no_uniformize', action='store_true',
                    help='Do NOT automatically uniformize endpoints for the'
                         'endpoints related metrics.')
     add_reference_arg(p)
     add_processes_arg(p)
     add_json_args(p)
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def compute_measures(args):
     filename_tuple, no_uniformize = args
@@ -108,29 +116,29 @@
     length_min = float(np.min(length_list))
     length_max = float(np.max(length_list))
 
     sft.to_vox()
     sft.to_corner()
     streamlines = sft.streamlines
     density = compute_tract_counts_map(streamlines, dimensions)
-    endpoints_density = get_endpoints_density_map(streamlines, dimensions)
+    endpoints_density = get_endpoints_density_map(sft)
 
     span_list = list(map(compute_span, streamline_cords))
     span = float(np.average(span_list))
     curl = length_avg / span
     volume = np.count_nonzero(density) * np.product(voxel_size)
     diameter = 2 * np.sqrt(volume / (np.pi * length_avg))
     elon = length_avg / diameter
 
     roi = np.where(density != 0, 1, density)
     surf_area = approximate_surface_node(roi) * (voxel_size[0] ** 2)
     irregularity = surf_area / (np.pi * diameter * length_avg)
 
     endpoints_map_head, endpoints_map_tail = \
-        get_head_tail_density_maps(sft.streamlines, dimensions)
+        get_head_tail_density_maps(sft)
     endpoints_map_head_roi = \
         np.where(endpoints_map_head != 0, 1, endpoints_map_head)
     endpoints_map_tail_roi = \
         np.where(endpoints_map_tail != 0, 1, endpoints_map_tail)
     end_sur_area_head = \
         approximate_surface_node(endpoints_map_head_roi) * (voxel_size[0] ** 2)
     end_sur_area_tail = \
@@ -179,16 +187,20 @@
     dists = np.sqrt((np.diff([xyz[0], xyz[-1]], axis=0) ** 2).sum(axis=1))
     return np.sum(dists)
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
-    assert_inputs_exist(parser, args.in_bundles)
-    assert_outputs_exist(parser, args, args.out_json)
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
+
+    assert_inputs_exist(parser, args.in_bundles, args.reference)
+    assert_outputs_exist(parser, args, [], args.out_json)
+    assert_headers_compatible(parser, args.in_bundles,
+                              reference=args.reference)
 
     nbr_cpu = validate_nbr_processes(parser, args)
     bundles_references_tuple_extended = link_bundles_and_reference(
         parser, args, args.in_bundles)
 
     if nbr_cpu == 1:
         all_measures_dict = []
@@ -263,14 +275,19 @@
             np.average(
                 output_measures_dict['irregularity_of_end_surface_head'])
         output_measures_dict['group_stats']['avg_irregularity_tail'] = \
             np.average(
                 output_measures_dict['irregularity_of_end_surface_tail'])
         output_measures_dict['group_stats']['avg_fractal_dimension'] = \
             np.average(output_measures_dict['fractal_dimension'])
-    with open(args.out_json, 'w') as outfile:
-        json.dump(output_measures_dict, outfile,
-                  indent=args.indent, sort_keys=args.sort_keys)
+
+    if args.out_json:
+        with open(args.out_json, 'w') as outfile:
+            json.dump(output_measures_dict, outfile,
+                      indent=args.indent, sort_keys=args.sort_keys)
+    else:
+        print(json.dumps(output_measures_dict,
+                         indent=args.indent, sort_keys=args.sort_keys))
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_evaluate_bundles_pairwise_agreement_measures.py` & `scilpy-2.0.0/scripts/scil_bundle_pairwise_comparison.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,22 +1,24 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
 Evaluate pair-wise similarity measures of bundles.
-All tractograms must be in the same space (aligned to one reference)
+All tractograms must be in the same space (aligned to one reference).
 
-For the voxel representation the computed similarity measures are:
-bundle_adjacency_voxels, dice_voxels, w_dice_voxels, density_correlation
-volume_overlap, volume_overreach
+For the voxel representation, the computed similarity measures are:
+    bundle_adjacency_voxels, dice_voxels, w_dice_voxels, density_correlation
+    volume_overlap, volume_overreach
 The same measures are also evluated for the endpoints.
 
-For the streamline representation the computed similarity measures are:
-bundle_adjacency_streamlines, dice_streamlines, streamlines_count_overlap,
-streamlines_count_overreach
+For the streamline representation, the computed similarity measures are:
+    bundle_adjacency_streamlines, dice_streamlines, streamlines_count_overlap,
+    streamlines_count_overreach
+
+Formerly: scil_evaluate_bundles_pairwise_agreement_measures.py
 """
 
 import argparse
 import copy
 import hashlib
 import itertools
 import json
@@ -33,27 +35,30 @@
 import numpy as np
 from numpy.random import RandomState
 
 from scilpy.io.utils import (add_json_args,
                              add_overwrite_arg,
                              add_processes_arg,
                              add_reference_arg,
+                             add_verbose_arg,
                              assert_inputs_exist,
                              assert_outputs_exist,
                              link_bundles_and_reference,
-                             validate_nbr_processes)
+                             validate_nbr_processes, assert_headers_compatible)
 from scilpy.tractanalysis.reproducibility_measures \
     import (compute_dice_voxel,
             compute_bundle_adjacency_streamlines,
             compute_bundle_adjacency_voxel,
             compute_correlation,
-            compute_dice_streamlines,
-            get_endpoints_density_map)
+            compute_dice_streamlines)
 from scilpy.tractanalysis.streamlines_metrics import compute_tract_counts_map
 
+from scilpy.tractograms.streamline_and_mask_operations import \
+    get_endpoints_density_map
+
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__,
         formatter_class=argparse.RawTextHelpFormatter)
     p.add_argument('in_bundles', nargs='+',
                    help='Path of the input bundles.')
@@ -76,32 +81,35 @@
                         'reference tractogram in a Tractometer-style way.\n'
                         'Can only be used if also using the `single_compare` '
                         'option.')
 
     add_processes_arg(p)
     add_reference_arg(p)
     add_json_args(p)
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def load_data_tmp_saving(args):
     filename = args[0]
     reference = args[1]
     init_only = args[2]
     disable_centroids = args[3]
 
-    # Since data is often re-use when comparing multiple bundles, anything
-    # that can be computed once is saved temporarily and simply loaded on demand
+    # Since data is often re-used when comparing multiple bundles, anything
+    # that can be computed once is saved temporarily and simply loaded on
+    # demand
     hash_tmp = hashlib.md5(filename.encode()).hexdigest()
     tmp_density_filename = os.path.join('tmp_measures/',
                                         '{}_density.nii.gz'.format(hash_tmp))
     tmp_endpoints_filename = os.path.join('tmp_measures/',
-                                          '{}_endpoints.nii.gz'.format(hash_tmp))
+                                          '{}_endpoints.nii.gz'.format(
+                                                                    hash_tmp))
     tmp_centroids_filename = os.path.join('tmp_measures/',
                                           '{}_centroids.trk'.format(hash_tmp))
 
     sft = load_tractogram(filename, reference)
     sft.to_vox()
     sft.to_corner()
     streamlines = sft.get_streamlines_copy()
@@ -122,16 +130,15 @@
         sft_centroids = load_tractogram(tmp_centroids_filename, reference)
         sft_centroids.to_vox()
         sft_centroids.to_corner()
         centroids = sft_centroids.get_streamlines_copy()
     else:
         transformation, dimensions, _, _ = sft.space_attributes
         density = compute_tract_counts_map(streamlines, dimensions)
-        endpoints_density = get_endpoints_density_map(streamlines, dimensions,
-                                                      point_to_select=3)
+        endpoints_density = get_endpoints_density_map(sft, point_to_select=3)
         thresholds = [32, 24, 12, 6]
         if disable_centroids:
             centroids = []
         else:
             centroids = qbx_and_merge(streamlines, thresholds,
                                       rng=RandomState(0),
                                       verbose=False).centroids
@@ -155,18 +162,14 @@
     filename_1, reference_1 = tuple_1
     filename_2, reference_2 = tuple_2
     streamline_dice = args[1]
     bundle_adjency_no_overlap = args[2]
     disable_streamline_distance = args[3]
     ratio = args[4]
 
-    if not is_header_compatible(reference_1, reference_2):
-        raise ValueError('{} and {} have incompatible headers'.format(
-            filename_1, filename_2))
-
     data_tuple_1 = load_data_tmp_saving([filename_1, reference_1, False,
                                          disable_streamline_distance])
     if data_tuple_1 is None:
         return None
 
     density_1, endpoints_density_1, bundle_1, \
         centroids_1 = data_tuple_1
@@ -273,17 +276,20 @@
 
     return dict(zip(measures_name, measures))
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, args.in_bundles)
+    assert_inputs_exist(parser, args.in_bundles, args.reference)
     assert_outputs_exist(parser, args, [args.out_json])
+    assert_headers_compatible(parser, args.in_bundles,
+                              reference=args.reference)
 
     if args.ratio and not args.single_compare:
         parser.error('Can only compute ratio if also using `single_compare`')
 
     nbr_cpu = validate_nbr_processes(parser, args)
     if nbr_cpu > 1:
         pool = multiprocessing.Pool(nbr_cpu)
@@ -295,17 +301,19 @@
         # Move the single_compare only once, at the end.
         if args.single_compare in args.in_bundles:
             args.in_bundles.remove(args.single_compare)
         bundles_list = args.in_bundles + [args.single_compare]
         bundles_references_tuple_extended = link_bundles_and_reference(
             parser, args, bundles_list)
 
-        single_compare_reference_tuple = bundles_references_tuple_extended.pop()
-        comb_dict_keys = list(itertools.product(bundles_references_tuple_extended,
-                                                [single_compare_reference_tuple]))
+        single_compare_reference_tuple = \
+            bundles_references_tuple_extended.pop()
+        comb_dict_keys = list(itertools.product(
+                                        bundles_references_tuple_extended,
+                                        [single_compare_reference_tuple]))
     else:
         bundles_list = args.in_bundles
         # Pre-compute the needed files, to avoid conflict when the number
         # of cpu is higher than the number of bundle
         bundles_references_tuple = link_bundles_and_reference(parser,
                                                               args,
                                                               bundles_list)
```

### Comparing `scilpy-1.5.post2/scripts/scil_evaluate_connectivity_pairwise_agreement_measures.py` & `scilpy-2.0.0/scripts/scil_connectivity_pairwise_agreement.py`

 * *Files 13% similar despite different names*

```diff
@@ -2,23 +2,27 @@
 # -*- coding: utf-8 -*-
 
 """
 Evaluate pair-wise similarity measures of connectivity matrix.
 
 The computed similarity measures are:
 sum of square difference and pearson correlation coefficent
+
+Formerly: scil_evaluate_connectivity_pairwaise_agreement_measures.py
 """
 
 import argparse
 import itertools
 import json
+import logging
 
 import numpy as np
 
 from scilpy.io.utils import (add_json_args,
+                             add_verbose_arg,
                              add_overwrite_arg,
                              assert_inputs_exist,
                              assert_outputs_exist,
                              load_matrix_in_any_format)
 from scilpy.tractanalysis.reproducibility_measures import compute_dice_voxel
 
 
@@ -26,29 +30,32 @@
     p = argparse.ArgumentParser(
         description=__doc__,
         formatter_class=argparse.RawTextHelpFormatter)
     p.add_argument('in_matrices', nargs='+',
                    help='Path of the input matricies.')
     p.add_argument('out_json',
                    help='Path of the output json file.')
-    p.add_argument('--single_compare',
-                   help='Compare inputs to this single file.')
+    p.add_argument('--single_compare', metavar='matrix',
+                   help='Compare inputs to this single file.\n'
+                        '(Else, compute all pairs in in_matrices).')
     p.add_argument('--normalize', action='store_true',
-                   help='If set, will normalize all matrices '
-                        'from zero to one.')
+                   help='If set, will normalize all matrices from zero to '
+                        'one.')
 
     add_json_args(p)
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, args.in_matrices)
     assert_outputs_exist(parser, args, args.out_json)
 
     all_matrices = []
     for filename in args.in_matrices:
         tmp_mat = load_matrix_in_any_format(filename)
```

### Comparing `scilpy-1.5.post2/scripts/scil_extract_b0.py` & `scilpy-2.0.0/scripts/scil_dwi_extract_b0.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,30 +1,34 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-Extract B0s from DWI.
+Extract B0s from DWI, based on the bval and bvec information.
 
 The default behavior is to save the first b0 of the series.
+
+Formerly: scil_extract_b0.py
 """
 
 import argparse
 import logging
 import os
 
 from dipy.core.gradients import gradient_table
 from dipy.io.gradients import read_bvals_bvecs
 
 import nibabel as nib
 import numpy as np
 
-from scilpy.io.utils import (assert_inputs_exist, add_force_b0_arg,
-                             add_verbose_arg)
-from scilpy.utils.bvec_bval_tools import (check_b0_threshold, extract_b0,
-                                          B0ExtractionStrategy)
+from scilpy.dwi.utils import extract_b0
+from scilpy.io.utils import (add_b0_thresh_arg, add_overwrite_arg,
+                             add_skip_b0_check_arg, add_verbose_arg,
+                             assert_inputs_exist)
+from scilpy.gradients.bvec_bval_tools import (check_b0_threshold,
+                                              B0ExtractionStrategy)
 from scilpy.utils.filenames import split_name_with_nii
 
 logger = logging.getLogger(__file__)
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(description=__doc__,
@@ -33,43 +37,41 @@
                    help='DWI Nifti image.')
     p.add_argument('in_bval',
                    help='b-values filename, in FSL format (.bval).')
     p.add_argument('in_bvec',
                    help='b-values filename, in FSL format (.bvec).')
     p.add_argument('out_b0',
                    help='Output b0 file(s).')
-    p.add_argument('--b0_thr', type=float, default=0.0,
-                   help='All b-values with values less than or equal '
-                        'to b0_thr are considered as b0s i.e. without '
-                        'diffusion weighting. [%(default)s]')
 
-    group = p.add_mutually_exclusive_group()
+    group_ = p.add_argument_group("Options in the case of multiple b0s.")
+    group = group_.add_mutually_exclusive_group()
     group.add_argument('--all', action='store_true',
-                       help='Extract all b0. Index number will be appended to '
-                            'the output file.')
+                       help='Extract all b0s. Index number will be appended '
+                            'to the output file.')
     group.add_argument('--mean', action='store_true', help='Extract mean b0.')
     group.add_argument('--cluster-mean', action='store_true',
                        help='Extract mean of each continuous cluster of b0s.')
     group.add_argument('--cluster-first', action='store_true',
-                       help='Extract first b0 of each '
-                            'continuous cluster of b0s.')
+                       help='Extract first b0 of each continuous cluster of '
+                            'b0s.')
 
     p.add_argument('--block-size', '-s',
                    metavar='INT', type=int,
-                   help='Load the data using this block size. '
-                        'Useful\nwhen the data is too large to be '
-                        'loaded in memory.')
+                   help='Load the data using this block size. Useful\nwhen '
+                        'the data is too large to be loaded in memory.')
 
     p.add_argument('--single-image', action='store_true',
                    help='If output b0 volume has multiple time points, only '
-                        'outputs a single image instead of a numbered series '
-                        'of images.')
+                        'outputs a single \nimage instead of a numbered '
+                        'series of images.')
 
-    add_force_b0_arg(p)
+    add_b0_thresh_arg(p)
+    add_skip_b0_check_arg(p, will_overwrite_with_min=True)
     add_verbose_arg(p)
+    add_overwrite_arg(p)
 
     return p
 
 
 def _split_time_steps(b0, affine, header, output):
     fname, fext = split_name_with_nii(os.path.basename(output))
 
@@ -80,28 +82,27 @@
             '{}_{}{}'.format(fname, t, fext)) if multiple_b0 else output
         nib.save(nib.Nifti1Image(b0[..., t], affine, header), out_name)
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
-    if args.verbose:
-        logging.getLogger().setLevel(logging.INFO)
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, [args.in_dwi, args.in_bval, args.in_bvec])
 
     # Outputs are not checked, since multiple use cases
     # are possible and hard to check
 
     bvals, bvecs = read_bvals_bvecs(args.in_bval, args.in_bvec)
 
-    b0_threshold = check_b0_threshold(
-        args.force_b0_threshold, bvals.min(), args.b0_thr)
-
-    gtab = gradient_table(bvals, bvecs, b0_threshold=b0_threshold)
+    args.b0_threshold = check_b0_threshold(bvals.min(),
+                                           b0_thr=args.b0_threshold,
+                                           skip_b0_check=args.skip_b0_check)
+    gtab = gradient_table(bvals, bvecs, b0_threshold=args.b0_threshold)
     b0_idx = np.where(gtab.b0s_mask)[0]
 
     logger.info('Number of b0 images in the data: {}'.format(len(b0_idx)))
 
     strategy, extract_in_cluster = B0ExtractionStrategy.FIRST, False
     if args.mean or args.cluster_mean:
         strategy = B0ExtractionStrategy.MEAN
```

### Comparing `scilpy-1.5.post2/scripts/scil_extract_dwi_shell.py` & `scilpy-2.0.0/scripts/scil_dwi_extract_shell.py`

 * *Files 8% similar despite different names*

```diff
@@ -11,98 +11,93 @@
 and a tolerance of 20 will extract all volumes with a b-values from 1980 to
 2020.
 
 Files that are too large to be loaded in memory can still be processed by
 setting the --block-size argument. A block size of X means that X DWI volumes
 are loaded at a time for processing.
 
+Formerly: scil_extract_dwi_shell.py
 """
 
 import argparse
 import logging
 
 from dipy.io import read_bvals_bvecs
 import nibabel as nib
 import numpy as np
 
+from scilpy.dwi.utils import extract_dwi_shell
+from scilpy.io.gradients import save_gradient_sampling_fsl
 from scilpy.io.utils import (add_overwrite_arg, add_verbose_arg,
                              assert_inputs_exist, assert_outputs_exist)
-from scilpy.utils.bvec_bval_tools import extract_dwi_shell
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__,
         formatter_class=argparse.RawTextHelpFormatter)
 
     p.add_argument('in_dwi',
                    help='The DW image file to split.')
-
     p.add_argument('in_bval',
                    help='The b-values file in FSL format (.bval).')
-
     p.add_argument('in_bvec',
                    help='The b-vectors file in FSL format (.bvec).')
-
-    p.add_argument('in_bvals_to_extract', nargs='+',
-                   type=int,
+    p.add_argument('in_bvals_to_extract', nargs='+', type=int,
                    help='The list of b-values to extract. For example 0 2000.')
-
     p.add_argument('out_dwi',
                    help='The name of the output DWI file.')
-
     p.add_argument('out_bval',
                    help='The name of the output b-value file (.bval).')
-
     p.add_argument('out_bvec',
                    help='The name of the output b-vector file (.bvec).')
 
-    p.add_argument('--block-size', '-s',
-                   metavar='INT', type=int,
-                   help='Loads the data using this block size. '
-                        'Useful\nwhen the data is too large to be '
-                        'loaded in memory.')
-
+    p.add_argument('--out_indices',
+                   help='Optional filename for valid indices in input dwi '
+                        'volume')
+    p.add_argument('--block-size', '-s', metavar='INT', type=int,
+                   help='Loads the data using this block size. Useful\n'
+                        'when the data is too large to be loaded in memory.')
     p.add_argument('--tolerance', '-t',
                    metavar='INT', type=int, default=20,
-                   help='The tolerated gap between the b-values to '
-                        'extract\nand the actual b-values.')
+                   help='The tolerated gap between the b-values to  extract\n'
+                        'and the actual b-values. [%(default)s]')
 
     add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
-
-    if args.verbose:
-        logging.getLogger().setLevel(logging.INFO)
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, [args.in_dwi, args.in_bval, args.in_bvec])
     assert_outputs_exist(parser, args, [args.out_dwi, args.out_bval,
-                                        args.out_bvec])
+                                        args.out_bvec], args.out_indices)
 
     bvals, bvecs = read_bvals_bvecs(args.in_bval, args.in_bvec)
 
     # Find the volume indices that correspond to the shells to extract.
-    tol = args.tolerance
-
     img = nib.load(args.in_dwi)
 
-    outputs = extract_dwi_shell(img, bvals, bvecs, args.in_bvals_to_extract,
-                                tol, args.block_size)
-
-    indices, shell_data, new_bvals, new_bvecs = outputs
+    indices, shell_data, new_bvals, new_bvecs = extract_dwi_shell(
+        img, bvals, bvecs, args.in_bvals_to_extract,
+        args.tolerance, args.block_size)
 
     logging.info("Selected indices: {}".format(indices))
 
+    # toDo Could we use: scilpy.io.gradients.save_gradient_sampling_fsl?
     np.savetxt(args.out_bval, new_bvals, '%d')
     np.savetxt(args.out_bvec, new_bvecs.T, '%0.15f')
     nib.save(nib.Nifti1Image(shell_data, img.affine, header=img.header),
              args.out_dwi)
 
+    # output indices file
+    if args.out_indices:
+        np.savetxt(args.out_indices, indices, '%u')
+
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_extract_ushape.py` & `scilpy-2.0.0/scripts/scil_tractogram_extract_ushape.py`

 * *Files 9% similar despite different names*

```diff
@@ -5,105 +5,94 @@
 This script extracts streamlines depending on their U-shapeness.
 This script is a replica of Trackvis method.
 
 When ufactor is close to:
 *  0 it defines straight streamlines
 *  1 it defines U-fibers
 * -1 it defines S-fibers
+
+Formerly: scil_extract_ushape.py
 """
 
 import argparse
 import json
 import logging
 
-from dipy.io.streamline import save_tractogram
 import numpy as np
 
-from scilpy.io.streamlines import load_tractogram_with_reference
+from scilpy.io.streamlines import load_tractogram_with_reference, \
+    save_tractogram
 from scilpy.io.utils import (add_json_args,
+                             add_verbose_arg,
                              add_overwrite_arg,
                              add_reference_arg,
                              assert_inputs_exist,
                              assert_outputs_exist,
-                             check_tracts_same_format)
-from scilpy.tractanalysis.features import detect_ushape
+                             check_tracts_same_format, ranged_type)
+from scilpy.tractanalysis.bundle_operations import detect_ushape
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter,
                                 description=__doc__)
     p.add_argument('in_tractogram',
                    help='Tractogram input file name.')
     p.add_argument('out_tractogram',
                    help='Output tractogram file name.')
-    p.add_argument('--minU',
-                   default=0.5, type=float,
+    p.add_argument('--minU', default=0.5, type=ranged_type(float, -1.0, 1.0),
                    help='Min ufactor value. [%(default)s]')
-    p.add_argument('--maxU',
-                   default=1.0, type=float,
+    p.add_argument('--maxU', default=1.0, type=ranged_type(float, -1.0, 1.0),
                    help='Max ufactor value. [%(default)s]')
 
-    p.add_argument('--remaining_tractogram',
+    p.add_argument('--remaining_tractogram', metavar='filename',
                    help='If set, saves remaining streamlines.')
     p.add_argument('--no_empty', action='store_true',
                    help='Do not write file if there is no streamline.')
     p.add_argument('--display_counts', action='store_true',
                    help='Print streamline count before and after filtering.')
 
-    add_overwrite_arg(p)
-    add_reference_arg(p)
     add_json_args(p)
+    add_reference_arg(p)
+    add_verbose_arg(p)
+    add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, args.in_tractogram)
+    # Verifications
+    assert_inputs_exist(parser, args.in_tractogram, args.reference)
     assert_outputs_exist(parser, args, args.out_tractogram,
                          optional=args.remaining_tractogram)
     check_tracts_same_format(parser, [args.in_tractogram, args.out_tractogram,
                                       args.remaining_tractogram])
 
-    if not(-1 <= args.minU <= 1 and -1 <= args.maxU <= 1):
-        parser.error('Min-Max ufactor "{},{}" '.format(args.minU, args.maxU) +
-                     'must be between -1 and 1.')
-
+    # Loading
     sft = load_tractogram_with_reference(parser, args, args.in_tractogram)
 
-    ids_c = detect_ushape(sft, args.minU, args.maxU)
-    ids_l = np.setdiff1d(np.arange(len(sft.streamlines)), ids_c)
-
-    if len(ids_c) == 0:
-        if args.no_empty:
-            logging.debug("The file {} won't be written "
-                          "(0 streamline).".format(args.out_tractogram))
-            return
-
-        logging.debug('The file {} contains 0 streamline.'.format(
-            args.out_tractogram))
-
-    save_tractogram(sft[ids_c], args.out_tractogram)
+    # Processing
+    ids_ushaped = detect_ushape(sft, args.minU, args.maxU)
+    ids_others = np.setdiff1d(np.arange(len(sft.streamlines)), ids_ushaped)
 
     if args.display_counts:
         sc_bf = len(sft.streamlines)
-        sc_af = len(ids_c)
+        sc_af = len(ids_ushaped)
         print(json.dumps({'streamline_count_before_filtering': int(sc_bf),
                          'streamline_count_after_filtering': int(sc_af)},
                          indent=args.indent))
 
-    if args.remaining_tractogram:
-        if len(ids_l) == 0:
-            if args.no_empty:
-                logging.debug("The file {} won't be written (0 streamline"
-                              ").".format(args.remaining_tractogram))
-                return
-
-            logging.warning('No remaining streamlines.')
+    # Saving
+    save_tractogram(sft[ids_ushaped], args.out_tractogram,
+                    args.no_empty)
 
-        save_tractogram(sft[ids_l], args.remaining_tractogram)
+    if args.remaining_tractogram:
+        save_tractogram(sft[ids_others],
+                        args.remaining_tractogram,
+                        args.no_empty)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_filter_connectivity.py` & `scilpy-2.0.0/scripts/scil_connectivity_filter.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,18 +1,20 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
 Script to facilitate filtering of connectivity matrices.
-The same could be achieved through a complex sequence of scil_connectivity_math.py.
+The same could be achieved through a complex sequence of
+scil_connectivity_math.py.
 
-Can be used with any connectivity matrix from scil_compute_connectivity.py.
+Can be used with any connectivity matrix from
+scil_connectivity_compute_matrices.py.
 
 For example, a simple filtering (Jasmeen style) would be:
-scil_filter_connectivity.py out_mask.npy
+scil_connectivity_filter.py out_mask.npy
     --greater_than */sc.npy 1 0.90
     --lower_than */sim.npy 2 0.90
     --greater_than */len.npy 40 0.90 -v;
 
 This will result in a binary mask where each node with a value of 1 represents
 a node with at least 90% of the population having at least 1 streamline,
 90% of the population is similar to the average (2mm) and 90% of the
@@ -25,22 +27,24 @@
 It is strongly recommended (but not enforced) that the same number of
 connectivity matrices is used for each condition.
 
 This script performs an intersection of all conditions, meaning that all
 conditions must be met in order not to be filtered.
 If the user wants to manually handle the requirements, --keep_condition_count
 can be used and manually binarized using scil_connectivity_math.py
+
+Formerly: scil_filter_connectivity.py
 """
 
 import argparse
 import logging
 
 import numpy as np
 
-from scilpy.image.operations import invert
+from scilpy.image.volume_math import invert
 from scilpy.io.utils import (add_overwrite_arg, add_verbose_arg,
                              assert_outputs_exist,
                              load_matrix_in_any_format,
                              save_matrix_in_any_format)
 
 
 def _build_arg_parser():
@@ -73,23 +77,21 @@
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_outputs_exist(parser, args, args.out_matrix_mask)
 
     if not args.lower_than and not args.greater_than:
         parser.error('At least one of the two options is required.')
 
-    if args.verbose:
-        logging.getLogger().setLevel(logging.DEBUG)
-
     conditions_list = []
     if args.lower_than:
         for input_list in args.lower_than:
             conditions_list.append(('lower', input_list))
     if args.greater_than:
         for input_list in args.greater_than:
             conditions_list.append(('greater', input_list))
@@ -112,16 +114,16 @@
         if condition == 'lower':
             empty_matrices[matrices < value_threshold] = 1
         else:
             empty_matrices[matrices > value_threshold] = 1
 
         population_score = np.sum(empty_matrices, axis=2)
 
-        logging.debug('Condition {}_than (#{}) resulted in {} filtered '
-                      'elements out of {}.'.format(
+        logging.info('Condition {}_than (#{}) resulted in {} filtered '
+                     'elements out of {}.'.format(
                           condition,
                           condition_counter,
                           len(np.where(population_score <
                                        population_threshold)[0]),
                           np.prod(shape)))
 
         output_mask[population_score > population_threshold] += 1
@@ -141,17 +143,17 @@
     # To prevent mis-usage, --keep_condition_count should not be used for
     # masking without binarization first
     if args.keep_condition_count:
         logging.warning('Keeping the condition count is not recommanded for '
                         'filtering.\nApply threshold manually to binarize the '
                         'output matrix.')
     else:
-        logging.debug('All condition resulted in {} filtered '
-                      'elements out of {}.'.format(filtered_elem,
-                                                   np.prod(shape)))
+        logging.info('All condition resulted in {} filtered '
+                     'elements out of {}.'.format(filtered_elem,
+                                                  np.prod(shape)))
 
     save_matrix_in_any_format(args.out_matrix_mask,
                               output_mask.astype(np.uint8))
 
 
 if __name__ == '__main__':
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_filter_streamlines_by_length.py` & `scilpy-2.0.0/scripts/scil_tractogram_resample_nb_points.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,90 +1,67 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-Script to filter streamlines based on their lengths.
-"""
+Script to resample a set of streamlines to either a new number of points per
+streamline or to a fixed step size. WARNING: data_per_point is not carried.
 
+Formerly: scil_resample_streamlines.py
+"""
 import argparse
-import json
 import logging
 
 from dipy.io.streamline import save_tractogram
-import numpy as np
 
-from scilpy.tracking.tools import filter_streamlines_by_length
 from scilpy.io.streamlines import load_tractogram_with_reference
-from scilpy.io.utils import (add_json_args,
-                             add_overwrite_arg,
+from scilpy.io.utils import (add_overwrite_arg,
                              add_reference_arg,
                              add_verbose_arg,
                              assert_inputs_exist,
                              assert_outputs_exist)
+from scilpy.tractograms.streamline_operations import \
+    resample_streamlines_num_points, resample_streamlines_step_size
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         formatter_class=argparse.RawTextHelpFormatter, description=__doc__)
 
     p.add_argument('in_tractogram',
                    help='Streamlines input file name.')
     p.add_argument('out_tractogram',
                    help='Streamlines output file name.')
-    p.add_argument('--minL', default=0., type=float,
-                   help='Minimum length of streamlines, in mm. [%(default)s]')
-    p.add_argument('--maxL', default=np.inf, type=float,
-                   help='Maximum length of streamlines, in mm. [%(default)s]')
-    p.add_argument('--no_empty', action='store_true',
-                   help='Do not write file if there is no streamline.')
-    p.add_argument('--display_counts', action='store_true',
-                   help='Print streamline count before and after filtering')
+
+    g = p.add_mutually_exclusive_group(required=True)
+    g.add_argument('--nb_pts_per_streamline', type=int,
+                   help='Number of points per streamline in the output.')
+    g.add_argument('--step_size', type=float,
+                   help='Step size in the output (in mm).')
 
     add_reference_arg(p)
-    add_overwrite_arg(p)
     add_verbose_arg(p)
-    add_json_args(p)
+    add_overwrite_arg(p)
 
     return p
 
 
 def main():
-
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, args.in_tractogram)
+    assert_inputs_exist(parser, args.in_tractogram, args.reference)
     assert_outputs_exist(parser, args, args.out_tractogram)
 
-    if args.verbose:
-        logging.getLogger().setLevel(logging.DEBUG)
-
-    if args.minL == 0 and np.isinf(args.maxL):
-        logging.debug("You have not specified minL nor maxL. Output will "
-                      "simply be a copy of your input!")
-
     sft = load_tractogram_with_reference(parser, args, args.in_tractogram)
 
-    new_sft = filter_streamlines_by_length(sft, args.minL, args.maxL)
-
-    if args.display_counts:
-        sc_bf = len(sft.streamlines)
-        sc_af = len(new_sft.streamlines)
-        print(json.dumps({'streamline_count_before_filtering': int(sc_bf),
-                         'streamline_count_after_filtering': int(sc_af)},
-                         indent=args.indent))
-
-    if len(new_sft.streamlines) == 0:
-        if args.no_empty:
-            logging.debug("The file {} won't be written "
-                          "(0 streamline).".format(args.out_tractogram))
-
-            return
-
-        logging.debug('The file {} contains 0 streamline'.format(
-            args.out_tractogram))
+    if args.nb_pts_per_streamline:
+        new_sft = resample_streamlines_num_points(sft,
+                                                  args.nb_pts_per_streamline)
+    else:
+        new_sft = resample_streamlines_step_size(sft, args.step_size)
 
     save_tractogram(new_sft, args.out_tractogram)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_filter_streamlines_by_orientation.py` & `scilpy-2.0.0/scripts/scil_tractogram_filter_by_orientation.py`

 * *Files 8% similar despite different names*

```diff
@@ -10,32 +10,34 @@
 Examples: In a brain aligned with x coordinates in left - right axis and y
 coordinates in anterior-posterior axis, a streamline from the ...
     - corpus callosum will likely travel a very short distance in the y axis.
     - cingulum will likely travel a very short distance in the x axis.
 
 Note: we consider that x, y, z are the coordinates of the streamlines; we
 do not verify if they are aligned with the brain's orientation.
+
+Formerly: scil_filter_streamlines_by_orientation.py
 """
 
 import argparse
 import json
 import logging
 
-from dipy.io.stateful_tractogram import set_sft_logger_level
-from dipy.io.streamline import save_tractogram
 import numpy as np
 
-from scilpy.tracking.tools import filter_streamlines_by_total_length_per_dim
-from scilpy.io.streamlines import load_tractogram_with_reference
+from scilpy.io.streamlines import load_tractogram_with_reference, \
+    save_tractogram
 from scilpy.io.utils import (add_json_args,
                              add_overwrite_arg,
                              add_reference_arg,
                              add_verbose_arg,
                              assert_inputs_exist,
                              assert_outputs_exist)
+from scilpy.tractograms.streamline_operations import \
+    filter_streamlines_by_total_length_per_dim
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         formatter_class=argparse.RawTextHelpFormatter, description=__doc__)
 
     p.add_argument('in_tractogram',
@@ -69,36 +71,30 @@
     p.add_argument('--no_empty', action='store_true',
                    help='Do not write file if there is no streamline.')
     p.add_argument('--display_counts', action='store_true',
                    help='Print streamline count before and after filtering.')
     p.add_argument('--save_rejected', metavar='filename',
                    help="Save the SFT of rejected streamlines.")
 
+    add_json_args(p)
     add_reference_arg(p)
-    add_overwrite_arg(p)
     add_verbose_arg(p)
-    add_json_args(p)
+    add_overwrite_arg(p)
 
     return p
 
 
 def main():
-
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, args.in_tractogram)
+    assert_inputs_exist(parser, args.in_tractogram, args.reference)
     assert_outputs_exist(parser, args, args.out_tractogram, args.save_rejected)
 
-    if args.verbose:
-        logging.getLogger().setLevel(logging.DEBUG)
-        # Silencing SFT's logger if our logging is in DEBUG mode, because it
-        # typically produces a lot of outputs!
-        set_sft_logger_level('WARNING')
-
     if args.min_x == 0 and np.isinf(args.max_x) and \
        args.min_y == 0 and np.isinf(args.max_y) and \
        args.min_z == 0 and np.isinf(args.max_z):
         logging.warning("You have not specified min or max in any direction. "
                         "Output will simply be a copy of your input!")
 
     sft = load_tractogram_with_reference(parser, args, args.in_tractogram)
@@ -111,23 +107,17 @@
     if args.display_counts:
         sc_bf = len(sft.streamlines)
         sc_af = len(new_sft.streamlines)
         print(json.dumps({'streamline_count_before_filtering': int(sc_bf),
                          'streamline_count_after_filtering': int(sc_af)},
                          indent=args.indent))
 
-    if len(new_sft.streamlines) == 0:
-        if args.no_empty:
-            logging.debug("The file {} won't be written "
-                          "(0 streamline).".format(args.out_tractogram))
-        else:
-            logging.debug('The file {} contains 0 streamline'.format(
-                args.out_tractogram))
-
-    save_tractogram(new_sft, args.out_tractogram)
+    save_tractogram(new_sft, args.out_tractogram,
+                    args.no_empty)
 
     if computed_rejected_sft:
-        save_tractogram(rejected_sft, args.save_rejected)
+        save_tractogram(rejected_sft, args.save_rejected,
+                        args.no_empty)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_filter_tractogram.py` & `scilpy-2.0.0/scripts/scil_tractogram_filter_by_roi.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,20 +1,21 @@
 #!/usr/bin/env python3
 #  -*- coding: utf-8 -*-
 
 """
 Now supports sequential filtering condition and mixed filtering object.
-For example, --atlas_roi ROI_NAME ID MODE CRITERIA
+For example, --atlas_roi ROI_NAME ID MODE CRITERIA DISTANCE
 - ROI_NAME is the filename of a Nifti
 - ID is one or multiple integer values in the atlas. If multiple values,
     ID needs to be between quotes.
     Example: "1:6 9 10:15" will use values between 1 and 6 and values
                            between 10 and 15 included as well as value 9.
 - MODE must be one of these values: ['any', 'all', 'either_end', 'both_ends']
 - CRITERIA must be one of these values: ['include', 'exclude']
+- DISTANCE must be a int and is optional
 
 If any meant any part of the streamline must be in the mask, all means that
 all part of the streamline must be in the mask.
 
 When used with exclude, it means that a streamline entirely in the mask will
 be excluded. Using all it with x/y/z plane works but makes very little sense.
 
@@ -24,111 +25,116 @@
 one command (without manually inverting the mask first) or
 to remove any streamlines staying in GM without getting out.
 
 Multiple filtering tuples can be used and options mixed.
 A logical AND is the only behavior available. All theses filtering
 conditions will be sequentially applied.
 
-WARNING: --soft_distance should be used carefully with large voxel size
-(e.g > 2.5mm).
+WARNING: DISTANCE is optional and it should be used carefully with large
+voxel size (e.g > 2.5mm). The value is in voxel for ROIs and in mm for
+bounding box. Anisotropic data will affect each direction differently
+
+Formerly: scil_filter_tractogram.py
 """
 
 import argparse
 import json
 import logging
 import os
 from copy import deepcopy
 
-from dipy.io.stateful_tractogram import set_sft_logger_level
-from dipy.io.streamline import save_tractogram
 from dipy.io.utils import is_header_compatible
 import nibabel as nib
 import numpy as np
-from scipy import ndimage
 
 from scilpy.io.image import (get_data_as_mask,
                              merge_labels_into_mask)
 from scilpy.image.labels import get_data_as_labels
-from scilpy.io.streamlines import load_tractogram_with_reference
+from scilpy.io.streamlines import load_tractogram_with_reference, \
+    save_tractogram
 from scilpy.io.utils import (add_json_args,
                              add_overwrite_arg,
                              add_reference_arg,
                              add_verbose_arg,
                              assert_inputs_exist,
                              assert_outputs_exist,
                              read_info_from_mb_bdo)
 from scilpy.segment.streamlines import (filter_cuboid, filter_ellipsoid,
                                         filter_grid_roi)
 
 
+MODES = ['any', 'all', 'either_end', 'both_ends']
+CRITERIA = ['include', 'exclude']
+
+
 def _build_arg_parser():
     p = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter,
                                 description=__doc__)
 
     p.add_argument('in_tractogram',
                    help='Path of the input tractogram file.')
     p.add_argument('out_tractogram',
                    help='Path of the output tractogram file.')
 
-    p.add_argument('--drawn_roi', nargs=3, action='append',
-                   metavar=('ROI_NAME', 'MODE', 'CRITERIA'),
-                   help='Filename of a hand drawn ROI (.nii or .nii.gz).')
-    p.add_argument('--atlas_roi', nargs=4, action='append',
-                   metavar=('ROI_NAME', 'ID', 'MODE', 'CRITERIA'),
-                   help='Filename of an atlas (.nii or .nii.gz).')
-    p.add_argument('--bdo', nargs=3, action='append',
-                   metavar=('BDO_NAME', 'MODE', 'CRITERIA'),
-                   help='Filename of a bounding box (bdo) file from MI-Brain.')
-
-    p.add_argument('--x_plane', nargs=3, action='append',
-                   metavar=('PLANE', 'MODE', 'CRITERIA'),
-                   help='Slice number in X, in voxel space.')
-    p.add_argument('--y_plane', nargs=3, action='append',
-                   metavar=('PLANE', 'MODE', 'CRITERIA'),
-                   help='Slice number in Y, in voxel space.')
-    p.add_argument('--z_plane', nargs=3, action='append',
-                   metavar=('PLANE', 'MODE', 'CRITERIA'),
-                   help='Slice number in Z, in voxel space.')
+    p.add_argument('--drawn_roi', nargs='+', action='append',
+                   help="ROI_NAME MODE CRITERIA DISTANCE "
+                        "(distance in voxel is optional)\n"
+                        "Filename of a hand drawn ROI (.nii or .nii.gz).")
+    p.add_argument('--atlas_roi', nargs='+', action='append',
+                   help="ROI_NAME ID MODE CRITERIA DISTANCE "
+                        "(distance in voxel is optional)\n"
+                        "Filename of an atlas (.nii or .nii.gz).")
+    p.add_argument('--bdo', nargs='+', action='append',
+                   help="BDO_NAME MODE CRITERIA DISTANCE "
+                        "(distance in mm is optional)\n"
+                        "Filename of a bounding box (bdo) file from MI-Brain.")
+
+    p.add_argument('--x_plane', nargs='+', action='append',
+                   help="PLANE MODE CRITERIA DISTANCE "
+                        "(distance in voxel is optional)\n"
+                        "Slice number in X, in voxel space.")
+    p.add_argument('--y_plane', nargs='+', action='append',
+                   help="PLANE MODE CRITERIA DISTANCE "
+                        "(distance in voxel is optional)\n"
+                        "Slice number in Y, in voxel space.")
+    p.add_argument('--z_plane', nargs='+', action='append',
+                   help="PLANE MODE CRITERIA DISTANCE "
+                        "(distance in voxel is optional)\n"
+                        "Slice number in Z, in voxel space.")
     p.add_argument('--filtering_list',
                    help='Text file containing one rule per line\n'
-                   '(i.e. drawn_roi mask.nii.gz both_ends include).')
+                   '(i.e. drawn_roi mask.nii.gz both_ends include 1).')
+
+    p.add_argument('--overwrite_distance', nargs='+', action='append',
+                   help='MODE CRITERIA DISTANCE (distance in voxel for ROIs '
+                        'and in mm for bounding box).\n'
+                        'If set, it will overwrite the distance associated to '
+                        'a specific mode/criteria.')
 
-    p.add_argument('--soft_distance', type=int,
-                   help='All ROIs are enlarged by the specified value.\n'
-                        'The value is in voxel (NOT mm).\n'
-                        'Anisotropic data will affect each direction '
-                        'differently')
     p.add_argument('--extract_masks_atlas_roi', action='store_true',
                    help='Extract atlas roi masks.')
     p.add_argument('--no_empty', action='store_true',
                    help='Do not write file if there is no streamline.')
     p.add_argument('--display_counts', action='store_true',
                    help='Print streamline count before and after filtering')
     p.add_argument('--save_rejected', metavar='FILENAME',
                    help='Save rejected streamlines to output tractogram.')
 
+    add_json_args(p)
     add_reference_arg(p)
     add_verbose_arg(p)
     add_overwrite_arg(p)
-    add_json_args(p)
 
     return p
 
 
 def prepare_filtering_list(parser, args):
     roi_opt_list = []
     only_filtering_list = True
 
-    if args.soft_distance is not None:
-        if args.soft_distance < 1:
-            parser.error('The minimum soft distance is 1 voxel.')
-        elif args.soft_distance > 5:
-            logging.warning('Soft distance above 5 voxels leads to weird'
-                            ' results.')
-
     if args.drawn_roi:
         only_filtering_list = False
         for roi_opt in args.drawn_roi:
             roi_opt_list.append(['drawn_roi'] + roi_opt)
     if args.atlas_roi:
         only_filtering_list = False
         for roi_opt in args.atlas_roi:
@@ -156,72 +162,139 @@
             if "\"" in roi_opt:
                 tmp_opt = [i.strip() for i in roi_opt.strip().split("\"")]
                 roi_opt_list.append(
                     tmp_opt[0].split() + [tmp_opt[1]] + tmp_opt[2].split())
             else:
                 roi_opt_list.append(roi_opt.strip().split())
 
-    for roi_opt in roi_opt_list:
+    if (len(roi_opt_list[-1]) < 4 or len(roi_opt_list[-1]) > 5) and \
+            roi_opt_list[-1][0] != 'atlas_roi':
+        logging.error("Please specify 3 or 4 values "
+                      "for {} filtering.".format(roi_opt_list[-1][0]))
+    elif (len(roi_opt_list[-1]) < 5 or len(roi_opt_list[-1]) > 6) and \
+            roi_opt_list[-1][0] == 'atlas_roi':
+        logging.error("Please specify 4 or 5 values"
+                      " for {} filtering.".format(roi_opt_list[-1][0]))
+
+    filter_distance = 0
+    for index, roi_opt in enumerate(roi_opt_list):
         if roi_opt[0] == 'atlas_roi':
-            filter_type, filter_arg, _, filter_mode, filter_criteria = roi_opt
-        else:
+            if len(roi_opt) == 5:
+                filter_type, filter_arg, _, filter_mode, filter_criteria = \
+                                                                    roi_opt
+                roi_opt_list[index].append(0)
+            else:
+                filter_type, filter_arg, _, filter_mode, filter_criteria, \
+                    filter_distance = roi_opt
+        elif len(roi_opt) == 4:
             filter_type, filter_arg, filter_mode, filter_criteria = roi_opt
+            roi_opt_list[index].append(0)
+        else:
+            filter_type, filter_arg, filter_mode, filter_criteria, \
+                filter_distance = roi_opt
+
         if filter_type not in ['x_plane', 'y_plane', 'z_plane']:
             if not os.path.isfile(filter_arg):
                 parser.error('{} does not exist'.format(filter_arg))
         if filter_mode not in ['any', 'all', 'either_end', 'both_ends']:
             parser.error('{} is not a valid option for filter_mode'.format(
                 filter_mode))
         if filter_criteria not in ['include', 'exclude']:
             parser.error('{} is not a valid option for filter_criteria'.format(
                 filter_criteria))
 
+        if int(filter_distance) < 0:
+            parser.error("Distance should be positive. "
+                         "{} is not a valid option.".format(filter_distance))
+
     return roi_opt_list, only_filtering_list
 
 
+def check_overwrite_distance(parser, args):
+    dict_distance = {}
+    if args.overwrite_distance:
+        for distance in args.overwrite_distance:
+            if len(distance) != 3:
+                parser.error('overwrite_distance is not well formated.\n'
+                             'It should be MODE CRITERIA DISTANCE.')
+            elif '-'.join([distance[0], distance[1]]) in dict_distance:
+                parser.error('Overwrite distance dictionnary MODE '
+                             '"{}" has been set multiple times.'.format(
+                                                                distance[0]))
+            elif distance[0] in MODES and distance[1] in CRITERIA:
+                curr_key = '-'.join([distance[0], distance[1]])
+                dict_distance[curr_key] = distance[2]
+            else:
+                curr_key = '-'.join([distance[0], distance[1]])
+                parser.error('Overwrite distance dictionnary MODE-CRITERIA '
+                             '"{}" does not exist.'.format(curr_key))
+    else:
+        return dict_distance
+
+    return dict_distance
+
+
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, args.in_tractogram)
+    overwrite_distance = check_overwrite_distance(parser, args)
+
+    # Todo. Prepare now the names of other files (ex, ROI) and verify if
+    #  exist and compatible.
+    assert_inputs_exist(parser, args.in_tractogram, args.reference)
     assert_outputs_exist(parser, args, args.out_tractogram, args.save_rejected)
-    if args.verbose:
-        logging.getLogger().setLevel(logging.DEBUG)
-        set_sft_logger_level('WARNING')
+
+    if overwrite_distance:
+        logging.info('Overwrite distance dictionnary {}'.format(
+                                                        overwrite_distance))
 
     roi_opt_list, only_filtering_list = prepare_filtering_list(parser, args)
     o_dict = {}
 
-    logging.debug("Loading the tractogram...")
+    logging.info("Loading the tractogram...")
     sft = load_tractogram_with_reference(parser, args, args.in_tractogram)
     if args.save_rejected:
         initial_sft = deepcopy(sft)
-    bin_struct = ndimage.generate_binary_structure(3, 2)
 
     # Streamline count before filtering
     o_dict['streamline_count_before_filtering'] = len(sft.streamlines)
 
     atlas_roi_item = 0
 
     total_kept_ids = np.arange(len(sft.streamlines))
     for i, roi_opt in enumerate(roi_opt_list):
-        logging.debug("Preparing filtering from option: {}".format(roi_opt))
+        logging.info("Preparing filtering from option: {}".format(roi_opt))
         curr_dict = {}
         # Atlas needs an extra argument (value in the LUT)
         if roi_opt[0] == 'atlas_roi':
             filter_type, filter_arg, filter_arg_2, \
-                filter_mode, filter_criteria = roi_opt
+                filter_mode, filter_criteria, filter_distance = roi_opt
         else:
-            filter_type, filter_arg, filter_mode, filter_criteria = roi_opt
+            filter_type, filter_arg, filter_mode, filter_criteria, \
+                filter_distance = roi_opt
 
         curr_dict['filename'] = os.path.abspath(filter_arg)
         curr_dict['type'] = filter_type
         curr_dict['mode'] = filter_mode
         curr_dict['criteria'] = filter_criteria
 
+        key_distance = '-'.join([curr_dict['mode'], curr_dict['criteria']])
+        if key_distance in overwrite_distance:
+            curr_dict['distance'] = overwrite_distance[key_distance]
+        else:
+            curr_dict['distance'] = filter_distance
+
+        try:
+            filter_distance = int(curr_dict['distance'])
+        except ValueError:
+            parser.error('Distance filter {} should is not an integer.'.format(
+                                                        curr_dict['distance']))
+
         is_exclude = False if filter_criteria == 'include' else True
 
         if filter_type == 'drawn_roi' or filter_type == 'atlas_roi':
             img = nib.load(filter_arg)
             if not is_header_compatible(img, sft):
                 parser.error('Headers from the tractogram and the mask are '
                              'not compatible.')
@@ -230,21 +303,21 @@
             else:
                 atlas = get_data_as_labels(img)
                 mask = merge_labels_into_mask(atlas, filter_arg_2)
 
                 if args.extract_masks_atlas_roi:
                     atlas_roi_item = atlas_roi_item + 1
                     nib.Nifti1Image(mask.astype(np.uint16),
-                                    img.affine).to_filename('mask_atlas_roi_{}.nii.gz'.format(str(atlas_roi_item)))
+                                    img.affine).to_filename(
+                                        'mask_atlas_roi_{}.nii.gz'.format(
+                                            str(atlas_roi_item)))
 
-            if args.soft_distance is not None:
-                mask = ndimage.binary_dilation(mask, bin_struct,
-                                               iterations=args.soft_distance)
             filtered_sft, kept_ids = filter_grid_roi(sft, mask,
-                                                     filter_mode, is_exclude)
+                                                     filter_mode, is_exclude,
+                                                     filter_distance)
 
         # For every case, the input number must be greater or equal to 0 and
         # below the dimension, since this is a voxel space operation
         elif filter_type in ['x_plane', 'y_plane', 'z_plane']:
             filter_arg = int(filter_arg)
             _, dim, _, _ = sft.space_attributes
             mask = np.zeros(dim, dtype=np.int16)
@@ -267,35 +340,35 @@
                 else:
                     error_msg = 'Z plane ' + str(filter_arg)
 
             if error_msg:
                 parser.error('{} is not valid according to the '
                              'tractogram header.'.format(error_msg))
 
-            if args.soft_distance is not None:
-                mask = ndimage.binary_dilation(mask, bin_struct,
-                                               iterations=args.soft_distance)
             filtered_sft, kept_ids = filter_grid_roi(sft, mask,
-                                                     filter_mode, is_exclude)
+                                                     filter_mode, is_exclude,
+                                                     filter_distance)
 
         elif filter_type == 'bdo':
             geometry, radius, center = read_info_from_mb_bdo(filter_arg)
-            if args.soft_distance is not None:
-                radius += args.soft_distance * sft.space_attributes[2]
+
+            if filter_distance != 0:
+                radius += filter_distance * sft.space_attributes[2]
+
             if geometry == 'Ellipsoid':
                 filtered_sft, kept_ids = filter_ellipsoid(
                     sft, radius, center, filter_mode, is_exclude)
             elif geometry == 'Cuboid':
                 filtered_sft, kept_ids = filter_cuboid(
                     sft, radius, center, filter_mode, is_exclude)
         else:
             raise ValueError("Unexpected filter type.")
 
-        logging.debug('The filtering options {0} resulted in '
-                      '{1} streamlines'.format(roi_opt, len(filtered_sft)))
+        logging.info('The filtering options {0} resulted in '
+                     '{1} streamlines'.format(roi_opt, len(filtered_sft)))
 
         sft = filtered_sft
 
         if only_filtering_list:
             filtering_Name = 'Filter_' + str(i)
             curr_dict['streamline_count_after_filtering'] = len(
                 sft.streamlines)
@@ -304,34 +377,21 @@
         total_kept_ids = total_kept_ids[kept_ids]
 
     # Streamline count after filtering
     o_dict['streamline_count_final_filtering'] = len(sft.streamlines)
     if args.display_counts:
         print(json.dumps(o_dict, indent=args.indent))
 
-    if not filtered_sft:
-        if args.no_empty:
-            logging.debug("The file {} won't be written (0 streamline)".format(
-                args.out_tractogram))
-
-            return
-
-        logging.debug('The file {} contains 0 streamline'.format(
-            args.out_tractogram))
-
-    save_tractogram(sft, args.out_tractogram)
+    save_tractogram(sft, args.out_tractogram,
+                    args.no_empty)
 
     if args.save_rejected:
         rejected_ids = np.setdiff1d(np.arange(len(initial_sft.streamlines)),
                                     total_kept_ids)
 
-        if len(rejected_ids) == 0 and args.no_empty:
-            logging.debug("Rejected streamlines file won't be written (0 "
-                          "streamline).")
-            return
-
         sft = initial_sft[rejected_ids]
-        save_tractogram(sft, args.save_rejected)
+        save_tractogram(sft, args.save_rejected,
+                        args.no_empty)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_filter_tractogram_anatomically.py` & `scilpy-2.0.0/scripts/scil_tractogram_filter_by_anatomy.py`

 * *Files 11% similar despite different names*

```diff
@@ -31,50 +31,51 @@
 file including the cerebrospinal fluid and gray matter (cortical) regions
 according to the Desikan-Killiany atlas. Intermediate tractograms (results of
 each step and outliers) and volumes can be saved throughout the process.
 
 Example usages:
 
 # Filter length, looping angle and anatomical ending region
->>> scil_filter_tractogram_anatomically.py tractogram.trk wmparc.nii.gz
+>>> scil_tractogram_filter_by_anatomy.py tractogram.trk wmparc.nii.gz
     path/to/output/directory --minL 20 --maxL 200 -a 300
 # Filter only anatomical ending region, with WM dilation and provided csf mask
->>> scil_filter_tractogram_anatomically.py tractogram.trk wmparc.nii.gz
+>>> scil_tractogram_filter_by_anatomy.py tractogram.trk wmparc.nii.gz
     path/to/output/directory --csf_bin csf_bin.nii.gz --ctx_dilation_radius 2
+
+Formerly: scil_filter_streamlines_anatomically.py
 """
 
 import argparse
+from copy import deepcopy
 import json
 import logging
 import os
-import pkg_resources
+import importlib.resources as resources
 
 from dipy.io.streamline import save_tractogram
-from dipy.io.utils import is_header_compatible
 import nibabel as nib
 import numpy as np
 from scipy.spatial import cKDTree
 
 from scilpy.io.streamlines import load_tractogram_with_reference
 from scilpy.io.image import get_data_as_mask
 from scilpy.io.utils import (add_json_args,
                              add_overwrite_arg,
                              add_processes_arg,
                              add_reference_arg,
                              add_verbose_arg,
                              assert_inputs_exist,
                              assert_output_dirs_exist_and_empty,
-                             validate_nbr_processes)
+                             validate_nbr_processes, assert_headers_compatible)
 from scilpy.image.labels import get_data_as_labels
 from scilpy.segment.streamlines import filter_grid_roi
-from scilpy.tractanalysis.features import remove_loops_and_sharp_turns
-from scilpy.tractograms.tractogram_operations import (
-    difference, perform_tractogram_operation)
-from scilpy.tracking.tools import filter_streamlines_by_length
-from scilpy.utils.streamlines import filter_tractogram_data
+from scilpy.tractograms.streamline_operations import \
+    filter_streamlines_by_length, remove_loops_and_sharp_turns
+from scilpy.tractograms.tractogram_operations import \
+    perform_tractogram_operation_on_sft
 
 
 EPILOG = """
     References:
         [1] Jörgens, D., Descoteaux, M., Moreno, R., 2021. Challenges for
         tractogram ﬁltering. In: Özarslan, E., Schultz, T., Zhang, E., Fuster,
         A. (Eds.), Anisotropy Across Fields and Scales. Springer. Mathematics
@@ -115,33 +116,35 @@
                    help='Save accepted and discarded streamlines\n' +
                         ' after each step.')
     p.add_argument('--save_volumes', action='store_true',
                    help='Save volumetric images (e.g. binarised label\n' +
                         ' images, etc) in the filtering process.')
     p.add_argument('--save_counts', action='store_true',
                    help='Save the streamline counts to a file (.json)')
+    p.add_argument('--save_rejected', action='store_true',
+                   help='Save rejected streamlines to output tractogram.')
     p.add_argument('--no_empty', action='store_true',
                    help='Do not write file if there is no streamlines.')
 
+    add_json_args(p)
     add_processes_arg(p)
     add_reference_arg(p)
     add_verbose_arg(p)
     add_overwrite_arg(p)
-    add_json_args(p)
+
     return p
 
 
 def load_wmparc_labels():
     """
     Load labels dictionary of different parcellations from the
     Desikan-Killiany atlas
     """
-    resource_package = pkg_resources.get_distribution('scilpy').location
-    labels_path = os.path.join(
-        resource_package, 'data/LUT/dk_aggregate_structures.json')
+    lut_package = resources.files('data').joinpath('LUT')
+    labels_path = lut_package.joinpath('dk_aggregate_structures.json')
     with open(labels_path) as labels_file:
         labels = json.load(labels_file)
     return labels
 
 
 def binarize_labels(atlas, label_list):
     """
@@ -198,163 +201,180 @@
 def save_intermediate_sft(sft, outliers_sft, new_path, in_sft_name,
                           step_name, steps_combined, ext, no_empty):
     """
     Save the provided stateful tractograms.
     """
     sft_name = os.path.join(new_path, in_sft_name + "_" + steps_combined + ext)
     outliers_sft_name = os.path.join(
-        new_path, in_sft_name + "_" + step_name + "_outliers" + ext)
+        new_path, in_sft_name + "_" + steps_combined + "_outliers" + ext)
 
     if len(sft.streamlines) == 0:
         if no_empty:
-            logging.debug("The file" + sft_name +
-                          " won't be written (0 streamlines)")
+            logging.info("The file" + sft_name +
+                         " won't be written (0 streamlines)")
         save_tractogram(sft, sft_name)
     else:
         save_tractogram(sft, sft_name)
 
     if len(outliers_sft.streamlines):
         if no_empty:
-            logging.debug("The file" + outliers_sft_name +
-                          " won't be written (0 streamlines)")
+            logging.info("The file" + outliers_sft_name +
+                         " won't be written (0 streamlines)")
         save_tractogram(outliers_sft, outliers_sft_name)
     else:
         save_tractogram(outliers_sft, outliers_sft_name)
 
 
 def compute_outliers(sft, new_sft):
     """
     Return a stateful tractogram whose streamlines are the difference of the
     two input stateful tractograms
     """
-    streamlines_list = [sft.streamlines, new_sft.streamlines]
-    _, indices = perform_tractogram_operation(
-        difference, streamlines_list, precision=0)
-    outliers_sft = sft[indices]
+    outliers_sft, _ = perform_tractogram_operation_on_sft('difference_robust',
+                                                          [sft, new_sft],
+                                                          precision=3,
+                                                          no_metadata=True,
+                                                          fake_metadata=False)
     return outliers_sft
 
 
+def save_rejected(sft, new_sft, rejected_sft_name, no_empty):
+    """
+    Save rejected streamlines
+    """
+    rejected_sft = compute_outliers(sft, new_sft)
+
+    if len(rejected_sft.streamlines) == 0:
+        if no_empty:
+            logging.info("The file" + rejected_sft_name +
+                         " won't be written (0 streamlines)")
+            return
+
+    save_tractogram(rejected_sft, rejected_sft_name)
+
+
 def display_count(o_dict, indent, sort_keys):
     """
     Display the streamline count.
     """
     o_dict_str = json.dumps(o_dict, indent=indent, sort_keys=sort_keys)
-    logging.debug("Streamline count:\n{}".format(o_dict_str))
+    logging.info("Streamline count:\n{}".format(o_dict_str))
 
 
 def save_count(o_dict, out_path, indent, sort_keys):
     """
     Save the streamline count to a JSON file.
     """
     fname = os.path.join(out_path, "streamline_count.json")
     with open(fname, 'w') as outfile:
         json.dump(o_dict, outfile, indent=indent, sort_keys=sort_keys)
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, args.in_tractogram)
-    assert_inputs_exist(parser, args.in_wmparc)
-    assert_output_dirs_exist_and_empty(parser, args,
-                                       args.out_path,
+    assert_inputs_exist(parser, [args.in_tractogram, args.in_wmparc],
+                        [args.csf_bin, args.reference])
+    assert_output_dirs_exist_and_empty(parser, args, args.out_path,
                                        create_dir=True)
+    assert_headers_compatible(parser, [args.in_tractogram, args.in_wmparc],
+                              args.csf_bin, reference=args.reference)
 
     nbr_cpu = validate_nbr_processes(parser, args)
 
-    if args.verbose:
-        logging.getLogger().setLevel(logging.DEBUG)
-
     if args.angle <= 0:
         parser.error('Angle "{}" '.format(args.angle) +
                      'must be greater than or equal to 0')
     if args.ctx_dilation_radius < 0:
         parser.error('Cortex dilation radius "{}" '.format(
                      args.ctx_dilation_radius) + 'must be greater than 0')
     sft = load_tractogram_with_reference(parser, args, args.in_tractogram)
+    sft.to_vox()
+    sft.to_corner()
 
     img_wmparc = nib.load(args.in_wmparc)
-    if not is_header_compatible(img_wmparc, sft):
-        parser.error('Headers from the tractogram and the wmparc are '
-                     'not compatible.')
     if args.csf_bin:
         img_csf = nib.load(args.csf_bin)
-        if not is_header_compatible(img_csf, sft):
-            parser.error('Headers from the tractogram and the CSF mask are '
-                         'not compatible.')
 
     if args.minL == 0 and np.isinf(args.maxL):
-        logging.debug("You have not specified minL nor maxL. Output will "
-                      "not be filtered according to length!")
+        logging.info("You have not specified minL nor maxL. Output will "
+                     "not be filtered according to length!")
     if np.isinf(args.angle):
-        logging.debug("You have not specified the angle. Loops will "
-                      "not be filtered!")
+        logging.info("You have not specified the angle. Loops will "
+                     "not be filtered!")
     if args.ctx_dilation_radius == 0:
-        logging.debug("You have not specified the cortex dilation radius. "
-                      "The wmparc atlas will not be dilated!")
+        logging.info("You have not specified the cortex dilation radius. "
+                     "The wmparc atlas will not be dilated!")
 
     o_dict = {}
-    step_dict = ['length', 'no_end_csf', 'end_in_atlas', 'no_loops']
+    step_dict = ['length', 'no_csf', 'end_in_atlas', 'no_loops']
     wm_labels = load_wmparc_labels()
 
     in_sft_name = os.path.splitext(os.path.basename(args.in_tractogram))[0]
     out_sft_rootname = in_sft_name + "_filtered"
     _, ext = os.path.splitext(args.in_tractogram)
     out_sft_name = os.path.join(args.out_path,
                                 out_sft_rootname + ext)
 
+    if args.save_rejected:
+        initial_sft = deepcopy(sft)
+        rejected_sft_name = os.path.join(args.out_path,
+                                         in_sft_name +
+                                         "_rejected" + ext)
+
     # STEP 1 - Filter length
     step = step_dict[0]
     steps_combined = step
     new_sft = filter_streamlines_by_length(sft, args.minL, args.maxL)
-
     # Streamline count before and after filtering lengths
     o_dict[in_sft_name + ext] =\
         dict({'streamline_count': len(sft.streamlines)})
     o_dict[in_sft_name + '_' + steps_combined + ext] =\
         dict({'streamline_count': len(new_sft.streamlines)})
 
     if args.save_intermediate_tractograms:
         outliers_sft = compute_outliers(sft, new_sft)
-        new_path = create_dir(args.out_path, step)
+        new_path = create_dir(args.out_path, '01-' + step)
         save_intermediate_sft(new_sft, outliers_sft, new_path, in_sft_name,
                               step, steps_combined, ext, args.no_empty)
-        o_dict[in_sft_name + '_' + step + '_outliers' + ext] =\
+        o_dict[in_sft_name + '_' + steps_combined + '_outliers' + ext] =\
             dict({'streamline_count': len(outliers_sft.streamlines)})
 
     if len(new_sft.streamlines) == 0:
         if args.no_empty:
-            logging.debug("The file {} won't be written".format(
-                          out_sft_name) + "(0 streamlines after "
-                          + step + " filtering).")
+            logging.info("The file {} won't be written".format(
+                         out_sft_name) + "(0 streamlines after "
+                         + step + " filtering).")
 
             if args.verbose:
                 display_count(o_dict, args.indent, args.sort_keys)
-
             if args.save_counts:
                 save_count(o_dict, args.out_path, args.indent, args.sort_keys)
-
+            if args.save_rejected:
+                save_tractogram(initial_sft, rejected_sft_name)
             return
 
-        logging.debug('The file {} contains 0 streamlines after '.format(
-                      out_sft_name) + step + ' filtering')
+        logging.info('The file {} contains 0 streamlines after '.format(
+                     out_sft_name) + step + ' filtering')
         save_tractogram(new_sft, out_sft_name)
 
+        if args.save_rejected:
+            save_rejected(initial_sft, new_sft,
+                          rejected_sft_name, args.no_empty)
         if args.verbose:
             display_count(o_dict, args.indent, args.sort_keys)
-
         if args.save_counts:
             save_count(o_dict, args.out_path, args.indent, args.sort_keys)
-
         return
 
     sft = new_sft
 
-    # STEP 2 - Filter CSF endings
+    # STEP 2 - Filter CSF
     step = step_dict[1]
     steps_combined += "_" + step
 
     # Mask creation
     if args.csf_bin:
         mask = get_data_as_mask(img_csf)
     else:
@@ -364,53 +384,53 @@
     # Filter tractogram
     new_sft, _ = filter_grid_roi(sft, mask, 'any', True)
     # Streamline count after filtering CSF endings
     o_dict[in_sft_name + '_' + steps_combined + ext] =\
         dict({'streamline_count': len(new_sft.streamlines)})
 
     if args.save_volumes:
-        new_path = create_dir(args.out_path, step)
+        new_path = create_dir(args.out_path, '02-' + step)
         if not args.csf_bin:
             nib.save(nib.Nifti1Image(mask, img_wmparc.affine,
                                      img_wmparc.header),
                      os.path.join(new_path, 'csf_bin' + '.nii.gz'))
 
     if args.save_intermediate_tractograms:
         outliers_sft = compute_outliers(sft, new_sft)
-        new_path = create_dir(args.out_path, step)
+        new_path = create_dir(args.out_path, '02-' + step)
         save_intermediate_sft(new_sft, outliers_sft, new_path, in_sft_name,
                               step, steps_combined, ext, args.no_empty)
-        o_dict[in_sft_name + '_' + step + '_outliers' + ext] =\
+        o_dict[in_sft_name + '_' + steps_combined + '_outliers' + ext] =\
             dict({'streamline_count': len(outliers_sft.streamlines)})
 
     if len(new_sft.streamlines) == 0:
         if args.no_empty:
-            logging.debug("The file {} won't be written".format(
-                          out_sft_name) + "(0 streamlines after "
-                          + step + " filtering).")
+            logging.info("The file {} won't be written".format(
+                         out_sft_name) + "(0 streamlines after "
+                         + step + " filtering).")
 
             if args.verbose:
                 display_count(o_dict, args.indent, args.sort_keys)
-
             if args.save_counts:
                 save_count(o_dict, args.out_path, args.indent, args.sort_keys)
-
+            if args.save_rejected:
+                save_tractogram(sft, rejected_sft_name)
             return
 
-        logging.debug('The file {} contains 0 streamlines after '.format(
-                      out_sft_name) + step + ' filtering')
-
+        logging.info('The file {} contains 0 streamlines after '.format(
+                     out_sft_name) + step + ' filtering')
         save_tractogram(new_sft, out_sft_name)
 
+        if args.save_rejected:
+            save_rejected(initial_sft, new_sft,
+                          rejected_sft_name, args.no_empty)
         if args.verbose:
             display_count(o_dict, args.indent, args.sort_keys)
-
         if args.save_counts:
             save_count(o_dict, args.out_path, args.indent, args.sort_keys)
-
         return
 
     sft = new_sft
 
     # STEP 3 - Filter WM endings
     step = step_dict[2]
     steps_combined += "_" + step
@@ -434,96 +454,107 @@
     freesurfer_mask = np.zeros(atlas_shape, dtype=np.uint16)
     freesurfer_mask[np.logical_or(wmparc_nuclei, ctx_mask)] = 1
 
     # Filter tractogram
     new_sft, _ = filter_grid_roi(sft, freesurfer_mask, 'both_ends', False)
 
     # Streamline count after final filtering
-    o_dict[out_sft_rootname + ext] =\
+    o_dict[in_sft_name + '_' + steps_combined + ext] =\
         dict({'streamline_count': len(new_sft.streamlines)})
 
     if args.save_volumes:
-        new_path = create_dir(args.out_path, step)
+        new_path = create_dir(args.out_path, '03-' + step)
         nib.save(nib.Nifti1Image(freesurfer_mask, img_wmparc.affine,
                                  img_wmparc.header),
                  os.path.join(new_path, 'atlas_bin' + '.nii.gz'))
 
     if args.save_intermediate_tractograms:
         outliers_sft = compute_outliers(sft, new_sft)
-        new_path = create_dir(args.out_path, step)
+        new_path = create_dir(args.out_path, '03-' + step)
         save_intermediate_sft(new_sft, outliers_sft, new_path, in_sft_name,
                               step, steps_combined, ext, args.no_empty)
-        o_dict[in_sft_name + '_' + step + '_outliers' + ext] =\
+        o_dict[in_sft_name + '_' + steps_combined + '_outliers' + ext] =\
             dict({'streamline_count': len(outliers_sft.streamlines)})
 
-    # Finish filtering
-    if args.verbose:
-        display_count(o_dict, args.indent, args.sort_keys)
-
-    if args.save_counts:
-        save_count(o_dict, args.out_path, args.indent, args.sort_keys)
-
     if len(new_sft.streamlines) == 0:
         if args.no_empty:
-            logging.debug("The file {} won't be written".format(
-                          out_sft_name) + "(0 streamlines after "
-                          + step + " filtering).")
+            logging.info("The file {} won't be written".format(
+                         out_sft_name) + "(0 streamlines after "
+                         + step + " filtering).")
+
+            if args.verbose:
+                display_count(o_dict, args.indent, args.sort_keys)
+            if args.save_counts:
+                save_count(o_dict, args.out_path, args.indent, args.sort_keys)
+            if args.save_rejected:
+                save_tractogram(sft, rejected_sft_name)
             return
-        logging.debug('The file {} contains 0 streamlines after '.format(
-                      out_sft_name) + step + ' filtering')
+
+        logging.info('The file {} contains 0 streamlines after '.format(
+                     out_sft_name) + step + ' filtering')
+        save_tractogram(new_sft, out_sft_name)
+
+        if args.save_rejected:
+            save_rejected(initial_sft, new_sft,
+                          rejected_sft_name, args.no_empty)
+        if args.verbose:
+            display_count(o_dict, args.indent, args.sort_keys)
+        if args.save_counts:
+            save_count(o_dict, args.out_path, args.indent, args.sort_keys)
+        return
+
+    sft = new_sft
 
     # STEP 4 - Filter loops
     step = step_dict[3]
     steps_combined += "_" + step
 
     if args.angle != np.inf:
         ids_c = remove_loops_and_sharp_turns(sft.streamlines, args.angle,
                                              num_processes=nbr_cpu)
-        new_sft = filter_tractogram_data(sft, ids_c)
+        new_sft = sft[ids_c]
     else:
-        new_sft = sft
+        new_sft = deepcopy(sft)
 
     # Streamline count after filtering loops
     o_dict[in_sft_name + '_' + steps_combined + ext] =\
         dict({'streamline_count': len(new_sft.streamlines)})
 
     if args.save_intermediate_tractograms:
         outliers_sft = compute_outliers(sft, new_sft)
-        new_path = create_dir(args.out_path, step)
+        new_path = create_dir(args.out_path, '04-' + step)
         save_intermediate_sft(new_sft, outliers_sft, new_path, in_sft_name,
                               step, steps_combined, ext, args.no_empty)
-        o_dict[in_sft_name + '_' + step + '_outliers' + ext] =\
+        o_dict[in_sft_name + '_' + steps_combined + '_outliers' + ext] =\
             dict({'streamline_count': len(outliers_sft.streamlines)})
 
     if len(new_sft.streamlines) == 0:
         if args.no_empty:
-            logging.debug("The file {} won't be written".format(
-                          out_sft_name) + "(0 streamlines after "
-                          + step + " filtering).")
+            logging.info("The file {} won't be written".format(
+                         out_sft_name) + "(0 streamlines after "
+                         + step + " filtering).")
 
             if args.verbose:
                 display_count(o_dict, args.indent, args.sort_keys)
-
             if args.save_counts:
                 save_count(o_dict, args.out_path, args.indent, args.sort_keys)
-
+            if args.save_rejected:
+                save_tractogram(sft, rejected_sft_name)
             return
 
-        logging.debug('The file {} contains 0 streamlines after '.format(
-                          out_sft_name) + step + ' filtering')
-
+        logging.info('The file {} contains 0 streamlines after '.format(
+                     out_sft_name) + step + ' filtering')
         save_tractogram(new_sft, out_sft_name)
 
-        if args.verbose:
-            display_count(o_dict, args.indent, args.sort_keys)
-
-        if args.save_counts:
-            save_count(o_dict, args.out_path, args.indent, args.sort_keys)
-
-        return
+    if args.verbose == "INFO" or args.verbose == "DEBUG":
+        display_count(o_dict, args.indent, args.sort_keys)
+    if args.save_counts:
+        save_count(o_dict, args.out_path, args.indent, args.sort_keys)
 
     sft = new_sft
     save_tractogram(sft, out_sft_name)
+    if args.save_rejected:
+        save_rejected(initial_sft, sft, rejected_sft_name, args.no_empty)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_fit_bingham_to_fodf.py` & `scilpy-2.0.0/scripts/scil_fodf_to_bingham.py`

 * *Files 7% similar despite different names*

```diff
@@ -7,27 +7,27 @@
 The Bingham fit is saved, with each Bingham distribution described by 7
 coefficients (for example, for a maximum number of lobes of 5, the number
 of coefficients is 7 x 5 = 35 -- less than the number of coefficients for
 SH of maximum order 8).
 
 Using 12 threads, the execution takes approximately 30 minutes for a brain with
 1mm isotropic resolution.
+
+Formerly: scil_fit_bingham_to_fodf.py
 """
 
 import nibabel as nib
 import time
 import argparse
 import logging
 
-from scilpy.io.utils import (add_overwrite_arg,
-                             add_processes_arg,
-                             add_verbose_arg,
-                             assert_inputs_exist,
-                             assert_outputs_exist,
-                             validate_nbr_processes)
+from scilpy.io.utils import (add_overwrite_arg, add_processes_arg,
+                             add_verbose_arg, assert_inputs_exist,
+                             assert_outputs_exist, validate_nbr_processes,
+                             assert_headers_compatible)
 from scilpy.io.image import get_data_as_mask
 from scilpy.reconst.bingham import (bingham_fit_sh)
 
 
 EPILOG = """
 [1] T. W. Riffert, J. Schreiber, A. Anwander, and T. R. Knösche, “Beyond
     fractional anisotropy: Extraction of bundle-specific structural metrics
@@ -66,33 +66,34 @@
     p.add_argument('--max_fit_angle', type=float, default=15.,
                    help='Maximum distance in degrees around a peak direction'
                         ' for fitting the Bingham function. [%(default)s]')
     p.add_argument('--mask',
                    help='Optional mask file. Only SH inside'
                         ' the mask are fitted.')
 
-    add_overwrite_arg(p)
     add_verbose_arg(p)
     add_processes_arg(p)
+    add_overwrite_arg(p)
+
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
-    if args.verbose:
-        logging.getLogger().setLevel(logging.INFO)
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, args.in_sh, args.mask)
     assert_outputs_exist(parser, args, args.out_bingham)
+    assert_headers_compatible(parser, args.in_sh, args.mask)
 
     sh_im = nib.load(args.in_sh)
     data = sh_im.get_fdata()
-    mask = get_data_as_mask(nib.load(args.mask), dtype=bool)\
-        if args.mask else None
+    mask = get_data_as_mask(nib.load(args.mask),
+                            dtype=bool) if args.mask else None
 
     # validate number of processes
     nbr_processes = validate_nbr_processes(parser, args)
     logging.info('Number of processes: {}'.format(nbr_processes))
 
     t0 = time.perf_counter()
     logging.info('Fitting Bingham functions.')
```

### Comparing `scilpy-1.5.post2/scripts/scil_fix_dsi_studio_trk.py` & `scilpy-2.0.0/scripts/scil_tractogram_fix_trk.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,13 +1,17 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-This script is made to fix DSI-Studio TRK file (unknown space/convention) to
-make it compatible with TrackVis, MI-Brain, Dipy Horizon (Stateful Tractogram).
+This script is made to fix DSI-Studio or Startrack TRK file
+(unknown space/convention) to make it compatible with TrackVis,
+MI-Brain, Dipy Horizon (Stateful Tractogram).
+
+DSI-Studio
+==========
 
 The script either make it match with an anatomy from DSI-Studio (AC-PC aligned,
 sometimes flipped) or if --in_native_fa is provided it moves it back to native
 DWI space (this involved registration).
 
 Since DSI-Studio sometimes leaves some skull around the brain, the --auto_crop
 aims to stabilize registration. If this option fails, manually BET both FA.
@@ -20,194 +24,235 @@
 We recommand the --cut_invalid to remove invalid points of streamlines rather
 removing entire streamlines.
 
 This script was tested on various datasets and worked on all of them. However,
 always verify the results and if a specific case does not work. Open an issue
 on the Scilpy GitHub repository.
 
-WARNING: This script is still experimental, DSI-Studio evolves quickly and
-results may vary depending on the data itself as well as DSI-studio version.
+Startrack
+==========
+
+The script will create a new stateful tractogram using the reference in
+order to fix the missing information in the header of the trk.
+
+
+WARNING: This script is still experimental, DSI-Studio and Startrack
+evolve quickly and results may vary depending on the data itself
+as well as DSI-studio/Startrack version.
+
+Formerly: scil_fix_dsi_studio_trk.py
 """
 
 import argparse
+import logging
 
 from dipy.align.imaffine import (transform_centers_of_mass,
                                  MutualInformationMetric,
                                  AffineRegistration)
 from dipy.align.transforms import RigidTransform3D
 from dipy.io.stateful_tractogram import StatefulTractogram, Space
 from dipy.io.utils import get_reference_info
 from dipy.io.streamline import save_tractogram, load_tractogram
-from dipy.reconst.utils import _roi_in_volume, _mask_from_roi
 import nibabel as nib
 import numpy as np
 
+from scilpy.image.volume_operations import crop_data_with_default_cube
 from scilpy.io.utils import (add_bbox_arg,
+                             add_verbose_arg,
                              add_overwrite_arg,
                              assert_inputs_exist,
                              assert_outputs_exist)
 from scilpy.tractograms.tractogram_operations import (flip_sft,
-                                                      transform_warp_sft)
-from scilpy.utils.streamlines import cut_invalid_streamlines
+                                                      transform_warp_sft,
+                                                      get_axis_flip_vector)
+from scilpy.tractograms.streamline_operations import cut_invalid_streamlines
+
+softwares = ['dsi_studio', 'startrack']
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__, formatter_class=argparse.RawTextHelpFormatter)
 
-    p.add_argument('in_dsi_tractogram',
+    p.add_argument('in_tractogram',
                    help='Path of the input tractogram file from DSI studio '
                         '(.trk).')
-    p.add_argument('in_dsi_fa',
-                   help='Path of the input FA from DSI Studio (.nii.gz).')
     p.add_argument('out_tractogram',
                    help='Path of the output tractogram file.')
-    p.add_argument('--in_native_fa',
-                   help='Path of the input FA from Dipy/MRtrix (.nii.gz).\n'
-                        'Move the tractogram back to a "proper" space, include'
-                        'registration.')
-    p.add_argument('--auto_crop', action='store_true',
-                   help='If both FA are not already BET, perform registration \n'
-                        'using a centered-cube crop to ignore the skull.\n'
-                        'A good BET for both is more robust.')
-    transfo = p.add_mutually_exclusive_group()
-    transfo.add_argument('--save_transfo', metavar='FILE',
-                         help='Save estimated transformation to avoid '
-                              'recomputing (.txt).')
-    transfo.add_argument('--load_transfo', metavar='FILE',
-                         help='Load estimated transformation to apply to other '
-                              'files (.txt).')
+    p.add_argument('--software', metavar='string', default='None',
+                   choices=softwares,
+                   help='Software used to create in_tractogram.\n'
+                        'Choices: {}'.format(softwares))
+
     invalid = p.add_mutually_exclusive_group()
     invalid.add_argument('--cut_invalid', action='store_true',
                          help='Cut invalid streamlines rather than removing '
                               'them.\nKeep the longest segment only.')
     invalid.add_argument('--remove_invalid', action='store_true',
                          help='Remove the streamlines landing out of the '
                               'bounding box.')
 
-    add_overwrite_arg(p)
-    add_bbox_arg(p)
-
-    return p
+    g1 = p.add_argument_group(title='DSI options')
+    g1.add_argument('--in_dsi_fa',
+                    help='Path of the input FA from DSI Studio (.nii.gz).')
+
+    g1.add_argument('--in_native_fa',
+                    help='Path of the input FA from Dipy/MRtrix (.nii.gz).\n'
+                         'Move the tractogram back to a "proper" space, '
+                         'include registration.')
+    g1.add_argument('--auto_crop', action='store_true',
+                    help='If both FA are not already BET, '
+                         'perform registration \n'
+                         'using a centered-cube crop to ignore the skull.\n'
+                         'A good BET for both is more robust.')
+    transfo = g1.add_mutually_exclusive_group()
+    transfo.add_argument('--save_transfo', metavar='FILE',
+                         help='Save estimated transformation to avoid '
+                              'recomputing (.txt).')
+    transfo.add_argument('--load_transfo', metavar='FILE',
+                         help='Load estimated transformation to apply '
+                              'to other files (.txt).')
 
+    g2 = p.add_argument_group(title='StarTrack options')
+    g2.add_argument('--reference',
+                    help='Reference anatomy (.nii or .nii.gz).')
 
-def get_axis_shift_vector(flip_axes):
-    shift_vector = np.zeros(3)
-    if 'x' in flip_axes:
-        shift_vector[0] = -1.0
-    if 'y' in flip_axes:
-        shift_vector[1] = -1.0
-    if 'z' in flip_axes:
-        shift_vector[2] = -1.0
-
-    return shift_vector
-
-
-def cube_crop_data(data):
-    shape = np.array(data.shape[:3])
-    roi_center = shape // 2
-    roi_radii = _roi_in_volume(shape, roi_center, shape // 3)
-    roi_mask = _mask_from_roi(shape, roi_center, roi_radii)
+    add_bbox_arg(p)
+    add_verbose_arg(p)
+    add_overwrite_arg(p)
 
-    return data * roi_mask
+    return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
-    if args.load_transfo and args.in_native_fa is None:
-        parser.error('When loading a transformation, the final reference is '
-                     'needed, use --in_native_fa.')
-    assert_inputs_exist(parser, [args.in_dsi_tractogram, args.in_dsi_fa],
-                        optional=args.in_native_fa)
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
+
+    warning_msg = """
+        # This script is still experimental, DSI-Studio and Startrack
+        # evolve quickly and results may vary depending on the data itself
+        # as well as DSI-studio/Startrack version.
+    """
+
+    logging.warning(warning_msg)
+
     assert_outputs_exist(parser, args, args.out_tractogram)
 
-    sft = load_tractogram(args.in_dsi_tractogram, 'same',
-                          bbox_valid_check=args.bbox_check)
+    if args.software == 'startrack':
+        assert_inputs_exist(parser, [args.in_tractogram, args.reference])
+        sft = load_tractogram(args.in_tractogram, 'same',
+                              bbox_valid_check=args.bbox_check,
+                              trk_header_check=False)
+        new_sft = StatefulTractogram(sft.streamlines, args.reference,
+                                     Space.VOX)
+
+        # Startrack flips the TRK
+        flip_axis = ['x']
+        new_sft.to_vox()
+        new_sft.streamlines._data -= get_axis_flip_vector(flip_axis)
+        new_sft = flip_sft(new_sft, flip_axis)
+        new_sft.to_rasmm()
 
-    # LPS -> RAS convention in voxel space
-    sft.to_vox()
-    flip_axis = ['x', 'y']
-    sft_fix = StatefulTractogram(sft.streamlines, args.in_dsi_fa,
-                                 Space.VOXMM)
-    sft_fix.to_vox()
-    sft_fix.streamlines._data -= get_axis_shift_vector(flip_axis)
-
-    sft_flip = flip_sft(sft_fix, flip_axis)
-
-    sft_flip.to_rasmm()
-    sft_flip.streamlines._data -= [0.5, 0.5, -0.5]
-
-    if not args.in_native_fa:
-        if args.cut_invalid:
-            sft_flip, _ = cut_invalid_streamlines(sft_flip)
-        elif args.remove_invalid:
-            sft_flip.remove_invalid_streamlines()
-        save_tractogram(sft_flip, args.out_tractogram,
-                        bbox_valid_check=args.bbox_check)
     else:
-        static_img = nib.load(args.in_native_fa)
-        static_data = static_img.get_fdata()
-        moving_img = nib.load(args.in_dsi_fa)
-        moving_data = moving_img.get_fdata()
-
-        # DSI-Studio flips the volume without changing the affine (I think)
-        # So this has to be reversed (not the same problem as above)
-        vox_order = get_reference_info(moving_img)[3]
-        flip_axis = []
-        if vox_order[0] == 'L':
-            moving_data = moving_data[::-1, :, :]
-            flip_axis.append('x')
-        if vox_order[1] == 'P':
-            moving_data = moving_data[:, ::-1, :]
-            flip_axis.append('y')
-        if vox_order[2] == 'I':
-            moving_data = moving_data[:, :, ::-1]
-            flip_axis.append('z')
-        sft_flip_back = flip_sft(sft_flip, flip_axis)
-
-        if args.load_transfo:
-            transfo = np.loadtxt(args.load_transfo)
+        if args.load_transfo and args.in_native_fa is None:
+            parser.error('When loading a transformation, the final '
+                         'reference is needed, use --in_native_fa.')
+
+        assert_inputs_exist(parser, [args.in_tractogram, args.in_dsi_fa],
+                            optional=args.in_native_fa)
+
+        sft = load_tractogram(args.in_tractogram, 'same',
+                              bbox_valid_check=args.bbox_check)
+        # LPS -> RAS convention in voxel space
+        sft.to_vox()
+        flip_axis = ['x', 'y']
+        sft_fix = StatefulTractogram(sft.streamlines, args.in_dsi_fa,
+                                     Space.VOXMM)
+        sft_fix.to_vox()
+        sft_fix.streamlines._data -= get_axis_flip_vector(flip_axis)
+
+        sft_flip = flip_sft(sft_fix, flip_axis)
+
+        sft_flip.to_rasmm()
+        sft_flip.streamlines._data -= [0.5, 0.5, -0.5]
+
+        if not args.in_native_fa:
+            if args.cut_invalid:
+                sft_flip, _ = cut_invalid_streamlines(sft_flip)
+            elif args.remove_invalid:
+                sft_flip.remove_invalid_streamlines()
+            save_tractogram(sft_flip, args.out_tractogram,
+                            bbox_valid_check=args.bbox_check)
         else:
-            # Sometimes DSI studio has quite a lot of skull left
-            # Dipy Median Otsu does not work with FA/GFA
-            if args.auto_crop:
-                moving_data = cube_crop_data(moving_data)
-                static_data = cube_crop_data(static_data)
-
-            # Since DSI Studio register to AC/PC and does not save the
-            # transformation We must estimate the transformation, since it's
-            # rigid it is 'easy'
-            c_of_mass = transform_centers_of_mass(static_data, static_img.affine,
-                                                  moving_data, moving_img.affine)
-
-            nbins = 32
-            sampling_prop = None
-            level_iters = [1000, 100, 10]
-            sigmas = [3.0, 2.0, 1.0]
-            factors = [3, 2, 1]
-            metric = MutualInformationMetric(nbins, sampling_prop)
-            affreg = AffineRegistration(metric=metric, level_iters=level_iters,
-                                        sigmas=sigmas, factors=factors)
-            transform = RigidTransform3D()
-            rigid = affreg.optimize(static_data, moving_data, transform, None,
-                                    static_img.affine, moving_img.affine,
-                                    starting_affine=c_of_mass.affine)
-            transfo = rigid.affine
-            if args.save_transfo:
-                np.savetxt(args.save_transfo, transfo)
-
-        new_sft = transform_warp_sft(sft_flip_back, transfo,
-                                     static_img, inverse=True,
-                                     remove_invalid=args.remove_invalid,
-                                     cut_invalid=args.cut_invalid)
-
-        if args.cut_invalid:
-            new_sft, _ = cut_invalid_streamlines(new_sft)
-        elif args.remove_invalid:
-            new_sft.remove_invalid_streamlines()
-        save_tractogram(new_sft, args.out_tractogram,
-                        bbox_valid_check=args.bbox_check)
+            static_img = nib.load(args.in_native_fa)
+            static_data = static_img.get_fdata()
+            moving_img = nib.load(args.in_dsi_fa)
+            moving_data = moving_img.get_fdata()
+
+            # DSI-Studio flips the volume without changing the affine (I think)
+            # So this has to be reversed (not the same problem as above)
+            vox_order = get_reference_info(moving_img)[3]
+            flip_axis = []
+            if vox_order[0] == 'L':
+                moving_data = moving_data[::-1, :, :]
+                flip_axis.append('x')
+            if vox_order[1] == 'P':
+                moving_data = moving_data[:, ::-1, :]
+                flip_axis.append('y')
+            if vox_order[2] == 'I':
+                moving_data = moving_data[:, :, ::-1]
+                flip_axis.append('z')
+            sft_flip_back = flip_sft(sft_flip, flip_axis)
+
+            if args.load_transfo:
+                transfo = np.loadtxt(args.load_transfo)
+            else:
+                # Sometimes DSI studio has quite a lot of skull left
+                # Dipy Median Otsu does not work with FA/GFA
+                if args.auto_crop:
+                    moving_data = crop_data_with_default_cube(moving_data)
+                    static_data = crop_data_with_default_cube(static_data)
+
+                # Since DSI Studio register to AC/PC and does not save the
+                # transformation We must estimate the transformation,
+                # since it's rigid it is 'easy'
+                c_of_mass = transform_centers_of_mass(static_data,
+                                                      static_img.affine,
+                                                      moving_data,
+                                                      moving_img.affine)
+
+                nbins = 32
+                sampling_prop = None
+                level_iters = [1000, 100, 10]
+                sigmas = [3.0, 2.0, 1.0]
+                factors = [3, 2, 1]
+                metric = MutualInformationMetric(nbins, sampling_prop)
+                affreg = AffineRegistration(metric=metric,
+                                            level_iters=level_iters,
+                                            sigmas=sigmas,
+                                            factors=factors)
+                transform = RigidTransform3D()
+                rigid = affreg.optimize(static_data, moving_data, transform,
+                                        None, static_img.affine,
+                                        moving_img.affine,
+                                        starting_affine=c_of_mass.affine)
+                transfo = rigid.affine
+                if args.save_transfo:
+                    np.savetxt(args.save_transfo, transfo)
+
+            new_sft = transform_warp_sft(sft_flip_back, transfo,
+                                         static_img, inverse=True,
+                                         remove_invalid=args.remove_invalid,
+                                         cut_invalid=args.cut_invalid)
+
+    if args.cut_invalid:
+        new_sft, _ = cut_invalid_streamlines(new_sft)
+    elif args.remove_invalid:
+        new_sft.remove_invalid_streamlines()
+
+    save_tractogram(new_sft, args.out_tractogram,
+                    bbox_valid_check=args.bbox_check)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_flip_streamlines.py` & `scilpy-2.0.0/scripts/scil_tractogram_filter_by_length.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,59 +1,82 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-Flip streamlines locally around specific axes.
+Script to filter streamlines based on their lengths.
 
-IMPORTANT: this script should only be used in case of absolute necessity.
-It's better to fix the real tools than to force flipping streamlines to
-have them fit in the tools.
+Formerly: scil_filter_streamlines_by_length.py
 """
 
 import argparse
+import json
+import logging
 
-from dipy.io.streamline import save_tractogram
+import numpy as np
 
-from scilpy.io.streamlines import load_tractogram_with_reference
-from scilpy.io.utils import (add_reference_arg,
+from scilpy.io.streamlines import load_tractogram_with_reference, \
+    save_tractogram
+from scilpy.io.utils import (add_json_args,
                              add_overwrite_arg,
+                             add_reference_arg,
+                             add_verbose_arg,
                              assert_inputs_exist,
                              assert_outputs_exist)
-from scilpy.tractograms.tractogram_operations import flip_sft
+from scilpy.tractograms.streamline_operations import \
+    filter_streamlines_by_length
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
-        description=__doc__, formatter_class=argparse.RawTextHelpFormatter)
+        formatter_class=argparse.RawTextHelpFormatter, description=__doc__)
 
     p.add_argument('in_tractogram',
-                   help='Path of the input tractogram file.')
+                   help='Streamlines input file name.')
     p.add_argument('out_tractogram',
-                   help='Path of the output tractogram file.')
-
-    p.add_argument('axes',
-                   choices=['x', 'y', 'z'], nargs='+',
-                   help='The axes you want to flip. eg: to flip the x '
-                        'and y axes use: x y.')
+                   help='Streamlines output file name.')
+    p.add_argument('--minL', default=0., type=float,
+                   help='Minimum length of streamlines, in mm. [%(default)s]')
+    p.add_argument('--maxL', default=np.inf, type=float,
+                   help='Maximum length of streamlines, in mm. [%(default)s]')
+    p.add_argument('--no_empty', action='store_true',
+                   help='Do not write file if there is no streamline.')
+    p.add_argument('--display_counts', action='store_true',
+                   help='Print streamline count before and after filtering')
 
+    add_json_args(p)
     add_reference_arg(p)
+    add_verbose_arg(p)
     add_overwrite_arg(p)
+
     return p
 
 
 def main():
+
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, args.in_tractogram)
+    assert_inputs_exist(parser, args.in_tractogram, args.reference)
     assert_outputs_exist(parser, args, args.out_tractogram)
 
+    if args.minL == 0 and np.isinf(args.maxL):
+        logging.info("You have not specified minL nor maxL. Output will "
+                     "simply be a copy of your input!")
+
     sft = load_tractogram_with_reference(parser, args, args.in_tractogram)
-    sft.to_vox()
-    sft.to_corner()
 
-    new_sft = flip_sft(sft, args.axes)
-    save_tractogram(new_sft, args.out_tractogram)
+    new_sft = filter_streamlines_by_length(sft, args.minL, args.maxL)
+
+    if args.display_counts:
+        sc_bf = len(sft.streamlines)
+        sc_af = len(new_sft.streamlines)
+        print(json.dumps({'streamline_count_before_filtering': int(sc_bf),
+                         'streamline_count_after_filtering': int(sc_af)},
+                         indent=args.indent))
+
+    save_tractogram(new_sft, args.out_tractogram,
+                    args.no_empty)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_generate_gradient_sampling.py` & `scilpy-2.0.0/scripts/scil_dwi_compute_snr.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,205 +1,194 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-Generate multi-shell gradient sampling with various processing to accelerate
-acquisition and help artefact correction.
+Script to compute signal to noise ratio (SNR) in a region of interest (ROI)
+of a DWI volume.
 
-Multi-shell gradient sampling is generated as in [1], the bvecs are then
-flipped to maximize spread for eddy current correction, b0s are interleaved
-at equal spacing and the non-b0 samples are finally shuffled
-to minimize the total diffusion gradient amplitude over a few TR.
+It will compute the SNR for all DWI volumes of the input image seperately.
+The output will contain the SNR which is the ratio of
+mean(signal) / std(noise).
+The mean of the signal is computed inside the mask.
+The standard deviation of the noise is estimated inside the noise_mask
+or inside the same mask if a noise_map is provided.
+If it's not supplied, it will be estimated using the data outside the brain,
+computed with Dipy medotsu
+
+If verbose is True, the SNR for every DWI volume will be output.
+
+This works best in a well-defined ROI such as the corpus callosum.
+It is heavily dependent on the ROI and its quality.
+
+We highly recommend using a noise_map if you can acquire one.
+See refs [1, 2] that describe the noise map acquisition.
+[1] St-Jean, et al (2016). Non Local Spatial and Angular Matching...
+    https://doi.org/10.1016/j.media.2016.02.010
+[2] Reymbaut, et al (2021). Magic DIAMOND...
+    https://doi.org/10.1016/j.media.2021.101988
+
+Formerly: scil_snr_in_roi.py
 """
 
 import argparse
 import logging
-import numpy as np
 import os
 
-from scilpy.io.utils import (assert_outputs_exist,
-                             add_overwrite_arg, add_verbose_arg)
-from scilpy.gradientsampling.gen_gradient_sampling import generate_gradient_sampling
-from scilpy.gradientsampling.optimize_gradient_sampling import (add_b0s,
-                                                                add_bvalue_b0,
-                                                                correct_b0s_philips,
-                                                                compute_bvalue_lin_b,
-                                                                compute_bvalue_lin_q,
-                                                                compute_min_duty_cycle_bruteforce,
-                                                                swap_sampling_eddy)
-from scilpy.gradientsampling.save_gradient_sampling import (save_gradient_sampling_fsl,
-                                                            save_gradient_sampling_mrtrix)
-
-
-EPILOG = """
-References: [1] Emmanuel Caruyer, Christophe Lenglet, Guillermo Sapiro,
-Rachid Deriche. Design of multishell gradient sampling with uniform coverage
-in diffusion MRI. Magnetic Resonance in Medicine, Wiley, 2013, 69 (6),
-pp. 1534-1540. <http://dx.doi.org/10.1002/mrm.24736>
-    """
+from dipy.io.gradients import read_bvals_bvecs
+import matplotlib.pyplot as plt
+import nibabel as nib
+import numpy as np
+import pandas as pd
+
+from scilpy.io.utils import (add_json_args, add_overwrite_arg,
+                             add_verbose_arg,
+                             assert_inputs_exist)
+from scilpy.utils.filenames import split_name_with_nii
+from scilpy.image.volume_operations import compute_snr
 
 
 def _build_arg_parser():
-    p = argparse.ArgumentParser(
-        formatter_class=argparse.RawDescriptionHelpFormatter,
-        description=__doc__,
-        epilog=EPILOG)
-    p._optionals.title = "Options and Parameters"
-
-    p.add_argument('nb_samples',
-                   type=int, nargs='+',
-                   help='Number of samples on each non b0 shell. '
-                        'If multishell, provide a number per shell.')
-    p.add_argument('out_basename',
-                   help='Gradient sampling output basename (don\'t '
-                        'include extension).\n'
-                        'Please add options --fsl and/or --mrtrix below.')
 
-    p.add_argument('--eddy',
-                   action='store_true',
-                   help='Apply eddy optimization.\nB-vectors are flipped '
-                        'to be well spread without symmetry. [%(default)s]')
-    p.add_argument('--duty',
-                   action='store_true',
-                   help='Apply duty cycle optimization. '
-                        '\nB-vectors are shuffled to reduce consecutive '
-                        'colinearity in the samples. [%(default)s]')
-    p.add_argument('--b0_every',
-                   type=int, default=-1,
-                   help='Interleave a b0 every b0_every. \nNo b0 if 0. '
-                        '\nOnly 1 b0 at beginning if > number of samples '
-                        'or negative. [%(default)s]')
-    p.add_argument('--b0_end',
-                   action='store_true',
-                   help='Add a b0 as last sample. [%(default)s]')
-    p.add_argument('--b0_value',
+    p = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter,
+                                description=__doc__)
+
+    p.add_argument('in_dwi',
+                   help='Path of the input diffusion volume.')
+
+    p.add_argument('in_bval',
+                   help='Path of the bvals file, in FSL format.')
+
+    p.add_argument('in_bvec',
+                   help='Path of the bvecs file, in FSL format.')
+
+    p.add_argument('in_mask',
+                   help='Binary mask of the region used to estimate SNR.')
+
+    g1 = p.add_argument_group(title='Masks options')
+    mask = g1.add_mutually_exclusive_group()
+    mask.add_argument('--noise_mask',
+                      help='Binary mask used to estimate the noise '
+                           'from the DWI.')
+    mask.add_argument('--noise_map',
+                      help='Noise map.')
+
+    p.add_argument('--b0_thr',
                    type=float, default=0.0,
-                   help='b-value of the b0s. [%(default)s]')
-    p.add_argument('--b0_philips',
+                   help='All b-values with values less than or equal '
+                        'to b0_thr are considered as b0s i.e. without '
+                        'diffusion weighting. [%(default)s]')
+    p.add_argument('--out_basename',
+                   help='Path and prefix for the various saved file.')
+    p.add_argument('--split_shells',
                    action='store_true',
-                   help='Replace values of b0s bvecs by existing bvecs for '
-                        'Philips handling. [%(default)s]')
-
-    bvals_group = p.add_mutually_exclusive_group(required=True)
-    bvals_group.add_argument('--bvals',
-                             type=float, nargs='+', metavar='bvals',
-                             help='bval of each non-b0 shell.')
-    bvals_group.add_argument('--b_lin_max',
-                             type=float,
-                             help='b-max for linear bval distribution '
-                                  'in *b*. [replaces -bvals]')
-    bvals_group.add_argument('--q_lin_max',
-                             type=float,
-                             help='b-max for linear bval distribution '
-                                  'in *q*. [replaces -bvals]')
-
-    g1 = p.add_argument_group(title='Save as')
-    g1.add_argument('--fsl',
-                    action='store_true',
-                    help='Save in FSL format (.bvec/.bval). [%(default)s]')
-    g1.add_argument('--mrtrix',
-                    action='store_true',
-                    help='Save in MRtrix format (.b). [%(default)s]')
+                   help='SNR will be split into shells.')
 
+    add_json_args(p)
     add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def main():
-
     parser = _build_arg_parser()
     args = parser.parse_args()
+    if args.verbose == "WARNING":
+        logging.getLogger().setLevel(logging.INFO)
+    else:
+        logging.getLogger().setLevel(logging.getLevelName(args.verbose))
+
+    assert_inputs_exist(parser, [args.in_dwi, args.in_bval,
+                                 args.in_bvec, args.in_mask],
+                        [args.noise_mask, args.noise_map])
 
-    fsl = args.fsl
-    mrtrix = args.mrtrix
+    basename, ext = split_name_with_nii(os.path.basename(args.in_dwi))
 
-    if not (fsl or mrtrix):
-        parser.error('Select at least one save format.')
-        return
+    if args.out_basename:
+        basename = args.out_basename
+
+    logging.info('Basename: {}'.format(basename))
+
+    # Loadings inputs.
+    dwi = nib.load(args.in_dwi)
+    bvals, bvecs = read_bvals_bvecs(args.in_bval, args.in_bvec)
+    mask = nib.load(args.in_mask)
+
+    if args.noise_mask:
+        noise_mask = nib.load(args.noise_mask)
+        noise_map = None
+    else:
+        noise_map = nib.load(args.noise_map)
+        noise_mask = None
 
-    out_basename, _ = os.path.splitext(args.out_basename)
+    values = compute_snr(dwi, bvals, bvecs, args.b0_thr,
+                         mask,
+                         noise_mask=noise_mask,
+                         noise_map=noise_map,
+                         split_shells=args.split_shells,
+                         basename=basename)
+
+    df = pd.DataFrame.from_dict(values).T
+
+    if args.split_shells:
+        for curr_shell in np.unique(df['bval']):
+            curr_values = df.loc[df['bval'] == curr_shell]['snr']
+            plt.plot(curr_values,
+                     marker='+', linestyle='--')
+            plt.legend(["SNR for bval: " + str(curr_shell)])
+            plt.xlabel("Directions")
+            plt.xlim([-1, len(df)])
+            plt.ylim([0, np.max(df['snr'])])
+            plt.ylabel("Estimated SNR")
+            plt.text(1, -9, 'Min SNR = ' + str(np.min(curr_values)))
+            plt.text(1, -13, 'Max SNR = ' + str(np.max(curr_values)))
+            plt.text(1, -17, 'Mean SNR = ' + str(np.mean(curr_values)))
+            out_png = basename + "_graph_SNR_bval_" + str(curr_shell) + ".png"
+            plt.savefig(out_png, bbox_inches='tight', dpi=300)
+            plt.clf()
+
+            logging.info('Min SNR for B={} is {}'
+                         .format(str(curr_shell),
+                                 str(np.min(curr_values))))
+            logging.info('Max SNR for B={} is {}'
+                         .format(str(curr_shell),
+                                 str(np.max(curr_values))))
+            logging.info('Mean SNR for B={} is {}'
+                         .format(str(curr_shell),
+                                 str(np.mean(curr_values))))
 
-    if fsl:
-        out_filename = [out_basename + '.bval', out_basename + '.bvec']
     else:
-        out_filename = out_basename + '.b'
+        b0_values = df.loc[df['bval'] == 0.0]['snr']
 
-    assert_outputs_exist(parser, args, out_filename)
+        logging.info('Mean SNR for b0 is {}'.format(str(np.mean(b0_values))))
 
-    log_level = logging.DEBUG if args.verbose else logging.INFO
-    logging.getLogger().setLevel(log_level)
+        curr_values = df.loc[df['bval'] != 0.0]['snr']
+        plt.plot(range(len(curr_values)), curr_values,
+                 marker='+', linestyle='--')
+        plt.legend(["SNR"])
+        plt.xlabel("Volume (excluding B0)")
+        plt.ylabel("Estimated SNR")
+        plt.xlim([-1, len(df)])
+        plt.text(1, 9, 'Min SNR B0 = ' +
+                 str(np.min(df.loc[df['bval'] == 0.0]['snr'])))
+        plt.text(1, 5, 'Max SNR B0 = ' +
+                 str(np.max(df.loc[df['bval'] == 0.0]['snr'])))
+        plt.text(1, 1, 'Mean SNR B0 = ' +
+                 str(np.mean(df.loc[df['bval'] == 0.0]['snr'])))
+        plt.savefig(basename + "_graph.png", bbox_inches='tight', dpi=300)
+        plt.close()
+
+    min_value = df[df['snr'] == np.min(df['snr'])].index[0]
+    max_value = df[df['snr'] == np.max(df['snr'])].index[0]
+    logging.info('Min SNR is {} and from B={}'
+                 .format(str(df['snr'][min_value]),
+                         str(df['bval'][min_value])))
+    logging.info('Max SNR is {} and from B={}'
+                 .format(str(df['snr'][max_value]),
+                         str(df['bval'][max_value])))
 
-    Ks = args.nb_samples
-    eddy = args.eddy
-    duty = args.duty
-
-    # Total number of samples
-    K = np.sum(Ks)
-    # Number of non-b0 shells
-    S = len(Ks)
-
-    b0_every = args.b0_every
-    b0_end = args.b0_end
-    b0_value = args.b0_value
-    b0_philips = args.b0_philips
-
-    # Only a b0 at the beginning
-    if (b0_every > K) or (b0_every < 0):
-        b0_every = K + 1
-
-    # Compute bval list
-    if args.bvals is not None:
-        bvals = args.bvals
-        unique_bvals = np.unique(bvals)
-        if len(unique_bvals) != S:
-            parser.error('You have provided {} shells '.format(S) +
-                         'but {} unique bvals.'.format(len(unique_bvals)))
-
-    elif args.b_lin_max is not None:
-        bvals = compute_bvalue_lin_b(bmin=0.0, bmax=args.b_lin_max,
-                                     nb_of_b_inside=S - 1, exclude_bmin=True)
-    elif args.q_lin_max is not None:
-        bvals = compute_bvalue_lin_q(bmin=0.0, bmax=args.q_lin_max,
-                                     nb_of_b_inside=S - 1, exclude_bmin=True)
-    # Add b0 b-value
-    if b0_every != 0:
-        bvals = add_bvalue_b0(bvals, b0_value=b0_value)
-
-    # Gradient sampling generation
-    points, shell_idx = generate_gradient_sampling(Ks, verbose=int(
-        3 - logging.getLogger().getEffectiveLevel()//10))
-
-    # eddy current optimization
-    if eddy:
-        points, shell_idx = swap_sampling_eddy(points, shell_idx)
-
-    # Adding interleaved b0s
-    if b0_every != 0:
-        points, shell_idx = add_b0s(points,
-                                    shell_idx,
-                                    b0_every=b0_every,
-                                    finish_b0=b0_end)
-
-    # duty cycle optimization
-    if duty:
-        points, shell_idx = compute_min_duty_cycle_bruteforce(
-            points, shell_idx, bvals)
-
-    # Correcting b0s bvecs for Philips
-    if b0_philips and np.sum(shell_idx == -1) > 1:
-        points, shell_idx = correct_b0s_philips(points, shell_idx)
-
-    if fsl:
-        save_gradient_sampling_fsl(points, shell_idx, bvals,
-                                   out_filename[0], out_filename[1])
-
-    if mrtrix:
-        if not points.shape[0] == 3:
-            points = points.transpose()
-            save_gradient_sampling_mrtrix(points, shell_idx, bvals,
-                                          filename=out_filename)
+    with open(basename + "_SNR.json", "w") as f:
+        df.T.to_json(f, indent=args.indent)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_generate_priors_from_bundle.py` & `scilpy-2.0.0/scripts/scil_bundle_generate_priors.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,14 +1,16 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
 Generation of priors and enhanced-FOD from an example/template bundle.
 The bundle must have been cleaned thorougly before use. The E-FOD can then
 be used for bundle-specific tractography, but not for FOD metrics.
+
+Formerly: scil_generate_priors_from_bundle.py
 """
 
 import argparse
 import logging
 import os
 
 from dipy.data import get_sphere
@@ -17,16 +19,19 @@
 import numpy as np
 
 from scilpy.io.image import get_data_as_mask
 from scilpy.io.streamlines import load_tractogram_with_reference
 from scilpy.io.utils import (add_overwrite_arg,
                              add_reference_arg,
                              add_sh_basis_args,
+                             add_verbose_arg,
                              assert_inputs_exist,
-                             assert_outputs_exist)
+                             assert_outputs_exist,
+                             parse_sh_basis_arg,
+                             assert_headers_compatible)
 from scilpy.reconst.utils import find_order_from_nb_coeff
 from scilpy.tractanalysis.todi import TrackOrientationDensityImaging
 
 
 EPILOG = """
     References:
         [1] Rheault, Francois, et al. "Bundle-specific tractography with
@@ -56,27 +61,29 @@
     p.add_argument('--out_prefix', default='',
                    help='Add a prefix to all output filename, \n'
                         'default is no prefix.')
     p.add_argument('--out_dir', default='./',
                    help='Output directory for all generated files,\n'
                         'default is current directory.')
 
-    add_overwrite_arg(p)
     add_reference_arg(p)
+    add_verbose_arg(p)
+    add_overwrite_arg(p)
 
     return p
 
 
 def main():
-    logging.getLogger().setLevel(logging.INFO)
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     required = [args.in_bundle, args.in_fodf, args.in_mask]
-    assert_inputs_exist(parser, required)
+    assert_inputs_exist(parser, required, args.reference)
+    assert_headers_compatible(parser, required, reference=args.reference)
 
     out_efod = os.path.join(args.out_dir,
                             '{0}efod.nii.gz'.format(args.out_prefix))
     out_priors = os.path.join(args.out_dir,
                               '{0}priors.nii.gz'.format(args.out_prefix))
     out_todi_mask = os.path.join(args.out_dir,
                                  '{0}todi_mask.nii.gz'.format(args.out_prefix))
@@ -89,81 +96,84 @@
 
     required = [out_efod, out_priors, out_todi_mask, out_endpoints_mask]
     assert_outputs_exist(parser, args, required)
 
     img_sh = nib.load(args.in_fodf)
     sh_shape = img_sh.shape
     sh_order = find_order_from_nb_coeff(sh_shape)
+    sh_basis, is_legacy = parse_sh_basis_arg(args)
     img_mask = nib.load(args.in_mask)
+    mask_data = get_data_as_mask(img_mask)
 
     sft = load_tractogram_with_reference(parser, args, args.in_bundle)
     sft.to_vox()
     if len(sft.streamlines) < 1:
         raise ValueError('The input bundle contains no streamline.')
 
     # Compute TODI from streamlines
     with TrackOrientationDensityImaging(img_mask.shape,
                                         'repulsion724') as todi_obj:
         todi_obj.compute_todi(sft.streamlines, length_weights=True)
         todi_obj.smooth_todi_dir()
         todi_obj.smooth_todi_spatial(sigma=args.todi_sigma)
 
         # Fancy masking of 1d indices to limit spatial dilation to WM
-        sub_mask_3d = np.logical_and(get_data_as_mask(img_mask),
-                                     todi_obj.reshape_to_3d(
-                                         todi_obj.get_mask()))
+        sub_mask_3d = np.logical_and(
+            mask_data, todi_obj.reshape_to_3d(todi_obj.get_mask()))
         sub_mask_1d = sub_mask_3d.flatten()[todi_obj.get_mask()]
         todi_sf = todi_obj.get_todi()[sub_mask_1d] ** 2
 
     # The priors should always be between 0 and 1
     # A minimum threshold is set to prevent misaligned FOD from disappearing
     todi_sf /= np.max(todi_sf, axis=-1, keepdims=True)
     todi_sf[todi_sf < args.sf_threshold] = args.sf_threshold
 
     # Memory friendly saving, as soon as possible saving then delete
     priors_3d = np.zeros(sh_shape)
     sphere = get_sphere('repulsion724')
     priors_3d[sub_mask_3d] = sf_to_sh(todi_sf, sphere,
-                                      sh_order=sh_order,
-                                      basis_type=args.sh_basis)
+                                      sh_order_max=sh_order,
+                                      basis_type=sh_basis,
+                                      legacy=is_legacy)
     nib.save(nib.Nifti1Image(priors_3d, img_mask.affine), out_priors)
     del priors_3d
 
     input_sh_3d = img_sh.get_fdata(dtype=np.float32)
     input_sf_1d = sh_to_sf(input_sh_3d[sub_mask_3d],
-                           sphere, sh_order=sh_order, basis_type=args.sh_basis)
+                           sphere, sh_order_max=sh_order,
+                           basis_type=sh_basis, legacy=is_legacy)
 
     # Creation of the enhanced-FOD (direction-wise multiplication)
     mult_sf_1d = input_sf_1d * todi_sf
     del todi_sf
 
     input_max_value = np.max(input_sf_1d, axis=-1, keepdims=True)
     mult_max_value = np.max(mult_sf_1d, axis=-1, keepdims=True)
     mult_positive_mask = np.squeeze(mult_max_value) > 0.0
     mult_sf_1d[mult_positive_mask] = mult_sf_1d[mult_positive_mask] * \
         input_max_value[mult_positive_mask] / \
         mult_max_value[mult_positive_mask]
 
     # Memory friendly saving
     input_sh_3d[sub_mask_3d] = sf_to_sh(mult_sf_1d, sphere,
-                                        sh_order=sh_order,
-                                        basis_type=args.sh_basis)
+                                        sh_order_max=sh_order,
+                                        basis_type=sh_basis,
+                                        legacy=is_legacy)
     nib.save(nib.Nifti1Image(input_sh_3d, img_mask.affine), out_efod)
     del input_sh_3d
 
     nib.save(nib.Nifti1Image(sub_mask_3d.astype(
         np.uint8), img_mask.affine), out_todi_mask)
 
     endpoints_mask = np.zeros(img_mask.shape, dtype=np.uint8)
     sft.to_corner()
     sft.streamlines._data = sft.streamlines._data.astype(np.uint16)
     for streamline in sft.streamlines:
         endpoints_mask[tuple(streamline[0])] = 1
         endpoints_mask[tuple(streamline[-1])] = 1
 
-    in_mask_data = get_data_as_mask(img_mask)
-    nib.save(nib.Nifti1Image(endpoints_mask*in_mask_data,
+    nib.save(nib.Nifti1Image(endpoints_mask * mask_data,
                              img_mask.affine), out_endpoints_mask)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_group_comparison.py` & `scilpy-2.0.0/scripts/scil_stats_group_comparison.py`

 * *Files 9% similar despite different names*

```diff
@@ -28,14 +28,16 @@
 https://en.wikipedia.org/wiki/Kruskal%E2%80%93Wallis_one-way_analysis_of_variance
 
 
 5) If the group difference test is positive and number of group is greater than
    2, test the group difference two by two.
 
 6) Generate the result for all metrics and bundles
+
+Formerly: scil_group_comparison.py
 """
 
 import argparse
 import json
 import logging
 import os
 
@@ -111,17 +113,15 @@
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
-
-    if args.verbose:
-        logging.getLogger().setLevel(logging.DEBUG)
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     required_args = [args.in_json, args.in_participants]
     assert_inputs_exist(parser, required_args)
 
     req_folder = os.path.join(args.out_dir, 'Graph')
     assert_output_dirs_exist_and_empty(parser, args, req_folder)
 
@@ -148,37 +148,37 @@
 
     # We do the comparison for every single combianaison of metric-bundle-value
     for b, m, v in product(bundles, metrics, values):
         # First we extract the basic information on that comparison
 
         curr_comparison_measure = ('_').join([b, m, v])
 
-        logging.debug('______________________')
-        logging.debug('Measure to compare: {}'.format(curr_comparison_measure))
+        logging.info('______________________')
+        logging.info('Measure to compare: {}'.format(curr_comparison_measure))
 
         # Check normality of that metric across all groups
 
         current_normality = {}
         overall_normality = True
         groups_array = []
         for group in my_group_dict:
             curr_sample = get_group_data_sample(my_group_dict, group, b, m, v)
-            logging.debug('Group {}'.format(group))
+            logging.info('Group {}'.format(group))
             current_normality[group] = verify_normality(
                                             curr_sample,
                                             alpha_error)
             if not current_normality[group][0]:
                 overall_normality = False
             groups_array.append(curr_sample)
-        logging.debug('Normality result:')
-        logging.debug(current_normality)
-        logging.debug('Overall Normality:')
-        logging.debug(overall_normality)
-        logging.debug('Groups array:')
-        logging.debug(groups_array)
+        logging.info('Normality result:')
+        logging.info(current_normality)
+        logging.info('Overall Normality:')
+        logging.info(overall_normality)
+        logging.info('Groups array:')
+        logging.info(groups_array)
 
         # Generate graph of the metric
         if args.generate_graph:
             visualise_distribution(groups_array,
                                    my_data.get_participants_list(),
                                    b, m, v,
                                    args.out_dir,
@@ -192,26 +192,26 @@
                                 '{}'.format(args.group_by))
 
         # Check homoscedasticity
         variance_equality = verify_homoscedasticity(
                                             groups_array,
                                             normality=overall_normality,
                                             alpha=alpha_error)
-        logging.debug('Equality of variance result:')
-        logging.debug(variance_equality)
+        logging.info('Equality of variance result:')
+        logging.info(variance_equality)
 
         # Now we compare the groups population
 
         difference = verify_group_difference(
                                     groups_array,
                                     normality=overall_normality,
                                     homoscedasticity=variance_equality[1],
                                     alpha=alpha_error)
-        logging.debug('Main test result:')
-        logging.debug(difference)
+        logging.info('Main test result:')
+        logging.info(difference)
 
         # Finally if we have more than 2 groups and found a difference
         # We do a post hoc analysis to explore where is this difference
         if difference[1] and difference[0] == 'ANOVA':
             diff_2_by_2 = verify_post_hoc(
                                     groups_array,
                                     my_data.get_groups_list(args.group_by),
@@ -228,16 +228,16 @@
                                     groups_array,
                                     my_data.get_groups_list(args.group_by),
                                     test='Wilcoxon',
                                     alpha=alpha_error)
         else:
             diff_2_by_2 = []
 
-        logging.debug('Summary of difference 2 by 2:')
-        logging.debug(diff_2_by_2)
+        logging.info('Summary of difference 2 by 2:')
+        logging.info(diff_2_by_2)
 
         # Write the current metric in the report
         curr_dict = write_current_dictionnary(curr_comparison_measure,
                                               current_normality,
                                               variance_equality,
                                               difference,
                                               diff_2_by_2)
```

### Comparing `scilpy-1.5.post2/scripts/scil_image_math.py` & `scilpy-2.0.0/scripts/scil_volume_math.py`

 * *Files 9% similar despite different names*

```diff
@@ -6,30 +6,32 @@
 listed below.
 
 This script is loading all images in memory, will often crash after a few
 hundred images.
 
 Some operations such as multiplication or addition accept float value as
 parameters instead of images.
-> scil_image_math.py multiplication img.nii.gz 10 mult_10.nii.gz
+> scil_volume_math.py multiplication img.nii.gz 10 mult_10.nii.gz
+
+Formerly: scil_image_math.py
 """
 
 import argparse
 import logging
-import os
 
 from dipy.io.utils import is_header_compatible
 import nibabel as nib
 import numpy as np
 
-from scilpy.image.operations import (get_image_ops, get_operations_doc)
+from scilpy.image.volume_math import (get_image_ops, get_operations_doc)
+from scilpy.io.image import load_img
 from scilpy.io.utils import (add_overwrite_arg,
                              add_verbose_arg,
                              assert_outputs_exist)
-from scilpy.utils.util import is_float
+from scilpy.utils import is_float
 
 OPERATIONS = get_image_ops()
 
 __doc__ += get_operations_doc(OPERATIONS)
 
 
 def _build_arg_parser():
@@ -49,49 +51,24 @@
     p.add_argument('--data_type',
                    help='Data type of the output image. Use the format: '
                         'uint8, int16, int/float32, int/float64.')
     p.add_argument('--exclude_background', action='store_true',
                    help='Does not affect the background of the original '
                         'images.')
 
-    add_overwrite_arg(p)
     add_verbose_arg(p)
+    add_overwrite_arg(p)
 
     return p
 
 
-def load_img(arg):
-    if is_float(arg):
-        img = float(arg)
-        dtype = np.float64
-    else:
-        if not os.path.isfile(arg):
-            raise ValueError('Input file {} does not exist.'.format(arg))
-        img = nib.load(arg)
-        shape = img.header.get_data_shape()
-        dtype = img.header.get_data_dtype()
-        logging.info('Loaded {} of shape {} and data_type {}.'.format(
-                     arg, shape, dtype))
-
-        if len(shape) > 3:
-            logging.warning('{} has {} dimensions, be careful.'.format(
-                arg, len(shape)))
-        elif len(shape) < 3:
-            raise ValueError('{} has {} dimensions, not valid.'.format(
-                arg, len(shape)))
-
-    return img, dtype
-
-
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
-
-    if args.verbose:
-        logging.getLogger().setLevel(logging.INFO)
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_outputs_exist(parser, args, args.out_image)
 
     # Binary operations require specific verifications
     binary_op = ['union', 'intersection', 'difference', 'invert',
                  'dilation', 'erosion', 'closing', 'opening']
```

### Comparing `scilpy-1.5.post2/scripts/scil_merge_sh.py` & `scilpy-2.0.0/scripts/scil_sh_fusion.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,68 +1,77 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 """
 Merge a list of Spherical Harmonics files.
 
-This merges the coefficients of multiple Spherical Harmonics files
-by taking, for each coefficient, the one with the largest magnitude.
+This merges the coefficients of multiple Spherical Harmonics files by taking,
+for each coefficient, the one with the largest magnitude.
 
 Can be used to merge fODFs computed from different shells into 1, while
 conserving the most relevant information.
 
-Based on [1].
+Based on [1] and [2].
+
+Formerly: scil_merge_sh.py
 """
 
 import argparse
+import logging
 
 import nibabel as nib
 import numpy as np
 
 from scilpy.io.image import assert_same_resolution
 from scilpy.io.utils import (add_overwrite_arg, assert_inputs_exist,
-                             assert_outputs_exist)
+                             assert_outputs_exist, add_verbose_arg)
 
 
 EPILOG = """
 Reference:
 [1] Garyfallidis, E., Zucchelli, M., Houde, J-C., Descoteaux, M.
     How to perform best ODF reconstruction from the Human Connectome
     Project sampling scheme?
     ISMRM 2014.
+
+[2] Khachaturian, M. H., Wisco, J. J., & Tuch, D. S. (2007). Boosting the
+    sampling efficiency of q‐ball imaging using multiple wavevector fusion.
+    Magnetic Resonance in Medicine: An Official Journal of the International
+    Society for Magnetic Resonance in Medicine, 57(2), 289-296.
 """
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter,
                                 description=__doc__, epilog=EPILOG)
 
     p.add_argument('in_shs', nargs="+",
                    help='List of SH files.')
     p.add_argument('out_sh',
                    help='output SH file.')
 
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, args.in_shs)
     assert_outputs_exist(parser, args, args.out_sh)
     assert_same_resolution(args.in_shs)
 
     first_im = nib.load(args.in_shs[0])
     out_coeffs = first_im.get_fdata(dtype=np.float32)
 
     for sh_file in args.in_shs[1:]:
-        im = nib.load(sh_file)
-        im_dat = im.get_fdata(dtype=np.float32)
+        im_dat = nib.load(sh_file).get_fdata(dtype=np.float32)
 
         out_coeffs = np.where(np.abs(im_dat) > np.abs(out_coeffs),
                               im_dat, out_coeffs)
 
     nib.save(nib.Nifti1Image(out_coeffs, first_im.affine,
                              header=first_im.header),
              args.out_sh)
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `scilpy-1.5.post2/scripts/scil_normalize_connectivity.py` & `scilpy-2.0.0/scripts/scil_connectivity_normalize.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-Normalize a connectivity matrix coming from scil_decompose_connectivity.py.
+Normalize a connectivity matrix coming from
+scil_tractogram_segment_bundles_for_connectivity.py.
 3 categories of normalization are available:
 -- Edge attributes
  - length: Multiply each edge by the average bundle length.
    Compensate for far away connections when using interface seeding.
    Cannot be used with inverse_length.
 
  - inverse_length: Divide each edge by the average bundle length.
@@ -27,40 +28,41 @@
 
 -- Matrix scaling (Mutually exclusive)
  - max_at_one: Maximum value of the matrix will be set to one.
  - sum_to_one: Ensure the sum of all edges weight is one
  - log_10: Apply a base 10 logarithm to all edges weight
 
 The volume and length matrix should come from the
-scil_decompose_connectivity.py script.
+scil_tractogram_segment_bundles_for_connectivity.py script.
 
 A review of the type of normalization is available in:
 Colon-Perez, Luis M., et al. "Dimensionless, scale-invariant, edge weight
 metric for the study of complex structural networks." PLOS one 10.7 (2015).
 
 However, the proposed weighting of edge presented in this publication is not
 implemented.
+
+Formerly: scil_normalize_connectivity.py
 """
 
 import argparse
-from copy import copy
-import itertools
+import logging
 
 import nibabel as nib
 import numpy as np
 
-from scilpy.image.labels import get_data_as_labels
-from scilpy.image.operations import normalize_max, normalize_sum, base_10_log
+from scilpy.connectivity.connectivity_tools import \
+    normalize_matrix_from_values, normalize_matrix_from_parcel
+from scilpy.image.volume_math import normalize_max, normalize_sum, base_10_log
 from scilpy.io.utils import (add_overwrite_arg,
+                             add_verbose_arg,
                              assert_inputs_exist,
                              assert_outputs_exist,
                              load_matrix_in_any_format,
                              save_matrix_in_any_format)
-from scilpy.tractanalysis.reproducibility_measures import \
-    approximate_surface_node
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         formatter_class=argparse.RawTextHelpFormatter,
         description=__doc__,)
 
@@ -69,22 +71,23 @@
                         'streamline_count matrix (.npy).')
     p.add_argument('out_matrix',
                    help='Output normalized matrix (.npy).')
 
     edge_p = p.add_argument_group('Edge-wise options')
     length = edge_p.add_mutually_exclusive_group()
     length.add_argument('--length', metavar='LENGTH_MATRIX',
-                        help='Length matrix used for '
-                        'edge-wise multiplication.')
+                        help='Length matrix used for edge-wise '
+                             'multiplication.')
     length.add_argument('--inverse_length', metavar='LENGTH_MATRIX',
                         help='Length matrix used for edge-wise division.')
     edge_p.add_argument('--bundle_volume', metavar='VOLUME_MATRIX',
                         help='Volume matrix used for edge-wise division.')
 
     vol = edge_p.add_mutually_exclusive_group()
+    # toDo. Same description for the two options
     vol.add_argument('--parcel_volume', nargs=2,
                      metavar=('ATLAS', 'LABELS_LIST'),
                      help='Atlas and labels list for edge-wise division.')
     vol.add_argument('--parcel_surface', nargs=2,
                      metavar=('ATLAS', 'LABELS_LIST'),
                      help='Atlas and labels list for edge-wise division.')
 
@@ -93,98 +96,66 @@
     scale.add_argument('--max_at_one', action='store_true',
                        help='Scale matrix with maximum value at one.')
     scale.add_argument('--sum_to_one', action='store_true',
                        help='Scale matrix with sum of all elements at one.')
     scale.add_argument('--log_10', action='store_true',
                        help='Apply a base 10 logarithm to the matrix.')
 
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, args.in_matrix, [args.length,
                                                  args.inverse_length,
                                                  args.bundle_volume])
     assert_outputs_exist(parser, args, args.out_matrix)
 
-    in_matrix = load_matrix_in_any_format(args.in_matrix)
-
-    # Parcel volume and surface normalization require the atlas
-    # This script should be used directly after scil_decompose_connectivity.py
+    atlas_filepath = None
+    labels_filepath = None
     if args.parcel_volume or args.parcel_surface:
         atlas_tuple = args.parcel_volume if args.parcel_volume \
             else args.parcel_surface
         atlas_filepath, labels_filepath = atlas_tuple
         assert_inputs_exist(parser, [atlas_filepath, labels_filepath])
 
-        atlas_img = nib.load(atlas_filepath)
-        atlas_data = get_data_as_labels(atlas_img)
-
-        voxels_size = atlas_img.header.get_zooms()[:3]
-        if voxels_size[0] != voxels_size[1] \
-           or voxels_size[0] != voxels_size[2]:
-            parser.error('Atlas must have an isotropic resolution.')
-
-        voxels_vol = np.prod(atlas_img.header.get_zooms()[:3])
-        voxels_sur = np.prod(atlas_img.header.get_zooms()[:2])
-
-        # Excluding background (0)
-        labels_list = np.loadtxt(labels_filepath)
-        if len(labels_list) != in_matrix.shape[0] \
-                and len(labels_list) != in_matrix.shape[1]:
-            parser.error('Atlas should have the same number of label as the '
-                         'input matrix.')
+    in_matrix = load_matrix_in_any_format(args.in_matrix)
 
-    # Normalization can be combined together
+    # Normalization can be combined.
     out_matrix = in_matrix
-    if args.length:
-        length_mat = load_matrix_in_any_format(args.length)
-        out_matrix[length_mat > 0] *= length_mat[length_mat > 0]
-    elif args.inverse_length:
-        length_mat = load_matrix_in_any_format(args.inverse_length)
-        out_matrix[length_mat > 0] /= length_mat[length_mat > 0]
+    if args.length or args.inverse_length:
+        inverse = args.inverse_length is not None
+        matrix_file = args.inverse_length if inverse else args.length
+        length_matrix = load_matrix_in_any_format(matrix_file)
+        out_matrix = normalize_matrix_from_values(
+            out_matrix, length_matrix, inverse)
 
     if args.bundle_volume:
         volume_mat = load_matrix_in_any_format(args.bundle_volume)
-        out_matrix[volume_mat > 0] /= volume_mat[volume_mat > 0]
+        out_matrix = normalize_matrix_from_values(
+            out_matrix, volume_mat, inverse=True)
 
     # Node-wise computation are necessary for this type of normalize
+    # Parcel volume and surface normalization require the atlas
+    # This script should be used directly after
+    # scil_tractogram_segment_bundles_for_connectivity.py
     if args.parcel_volume or args.parcel_surface:
-        out_matrix = copy(in_matrix)
-        pos_list = range(len(labels_list))
-        all_comb = list(itertools.combinations(pos_list, r=2))
-        all_comb.extend(zip(pos_list, pos_list))
-
-        # Prevent useless computions for approximate_surface_node()
-        factor_list = []
-        for label in labels_list:
-            if args.parcel_volume:
-                factor_list.append(np.count_nonzero(
-                    atlas_data == label) * voxels_vol)
-            else:
-                if np.count_nonzero(atlas_data == label):
-                    roi = np.zeros(atlas_data.shape)
-                    roi[atlas_data == label] = 1
-                    factor_list.append(
-                        approximate_surface_node(roi) * voxels_sur)
-                else:
-                    factor_list.append(0)
-
-        for pos_1, pos_2 in all_comb:
-            factor = factor_list[pos_1] + factor_list[pos_2]
-            if abs(factor) > 0.001:
-                out_matrix[pos_1, pos_2] /= factor
-                out_matrix[pos_2, pos_1] /= factor
+        atlas_img = nib.load(atlas_filepath)
+        labels_list = np.loadtxt(labels_filepath)
+        out_matrix = normalize_matrix_from_parcel(
+            out_matrix, atlas_img, labels_list,
+            parcel_from_volume=args.parcel_volume)
 
-    # Load as image
+    # Save as image
     ref_matrix = nib.Nifti1Image(in_matrix, np.eye(4))
     # Simple scaling of the whole matrix, facilitate comparison across subject
     if args.max_at_one:
         out_matrix = nib.Nifti1Image(out_matrix, np.eye(4))
         out_matrix = normalize_max([out_matrix], ref_matrix)
     elif args.sum_to_one:
         out_matrix = nib.Nifti1Image(out_matrix, np.eye(4))
```

### Comparing `scilpy-1.5.post2/scripts/scil_outlier_rejection.py` & `scilpy-2.0.0/scripts/scil_bundle_reject_outliers.py`

 * *Files 3% similar despite different names*

```diff
@@ -12,20 +12,21 @@
 import json
 import logging
 
 from dipy.io.streamline import save_tractogram
 
 from scilpy.io.streamlines import load_tractogram_with_reference
 from scilpy.io.utils import (add_json_args,
+                             add_verbose_arg,
                              add_overwrite_arg,
                              add_reference_arg,
                              assert_inputs_exist,
                              assert_outputs_exist,
                              check_tracts_same_format)
-from scilpy.tractanalysis.features import remove_outliers
+from scilpy.tractanalysis.bundle_operations import remove_outliers
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(description=__doc__,
                                 formatter_class=argparse.RawTextHelpFormatter)
     p.add_argument('in_bundle',
                    help='Fiber bundle file to remove outliers from.')
@@ -34,26 +35,29 @@
     p.add_argument('--remaining_bundle',
                    help='Removed outliers.')
     p.add_argument('--alpha', type=float, default=0.6,
                    help='Percent of the length of the tree that clusters '
                    'of individual streamlines will be pruned. [%(default)s]')
     p.add_argument('--display_counts', action='store_true',
                    help='Print streamline count before and after filtering')
+
+    add_json_args(p)
     add_reference_arg(p)
+    add_verbose_arg(p)
     add_overwrite_arg(p)
-    add_json_args(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, args.in_bundle)
+    assert_inputs_exist(parser, args.in_bundle, args.reference)
     assert_outputs_exist(parser, args, args.out_bundle, args.remaining_bundle)
     if args.alpha <= 0 or args.alpha > 1:
         parser.error('--alpha should be ]0, 1]')
 
     sft = load_tractogram_with_reference(parser, args, args.in_bundle)
     if len(sft) == 0:
         logging.warning("Bundle file contains no streamline")
```

### Comparing `scilpy-1.5.post2/scripts/scil_plot_mean_std_per_point.py` & `scilpy-2.0.0/scripts/scil_plot_stats_per_point.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,66 +1,73 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-Plot all mean/std per point for a subject or population json file from 
+Plot all mean/std per point for a subject or population json file from
 tractometry-flow.
 WARNING: For population, the displayed STDs is only showing the variation
 of the means. It does not account intra-subject STDs.
+
+Formerly: scil_plot_mean_std_per_point.py
 """
 
 import argparse
 import itertools
 import json
+import logging
 import os
 
 import numpy as np
 
 from scilpy.io.utils import (add_overwrite_arg, assert_inputs_exist,
-                             assert_output_dirs_exist_and_empty)
+                             assert_output_dirs_exist_and_empty,
+                             add_verbose_arg)
 from scilpy.utils.metrics_tools import plot_metrics_stats
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(description=__doc__,
                                 formatter_class=argparse.RawTextHelpFormatter)
     p.add_argument('in_json',
                    help='JSON file containing the mean/std per point. For '
                         'example, can be created using '
-                        'scil_compute_metrics_along_streamline.')
+                        'scil_bundle_mean_std.py.')
     p.add_argument('out_dir',
                    help='Output directory.')
 
     p.add_argument('--stats_over_population', action='store_true',
                    help='If set, consider the input stats to be over an '
                         'entire population and not subject-based.')
     p.add_argument('--nb_pts', type=int,
                    help='Force the number of divisions for the bundles.\n'
                         'Avoid unequal plots across datasets, replace missing '
                         'data with zeros.')
     p.add_argument('--display_means', action='store_true',
-                   help='Display the subjects means as semi-transparent line.\n'
-                        'Poor results when the number of subject is high.')
+                   help='Display the subjects means as semi-transparent line.'
+                        '\nPoor results when the number of subject is high.')
 
     p1 = p.add_mutually_exclusive_group()
     p1.add_argument('--fill_color',
                     help='Hexadecimal RGB color filling the region between '
                     'mean +/- std. The hexadecimal RGB color should be '
                     'formatted as 0xRRGGBB.')
     p1.add_argument('--dict_colors',
                     help='Dictionnary mapping basename to color.'
                          'Same convention as --color.')
 
+    add_verbose_arg(p)
     add_overwrite_arg(p)
+    
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, args.in_json)
     assert_output_dirs_exist_and_empty(parser, args, args.out_dir,
                                        create_dir=True)
 
     if args.fill_color and len(args.fill_color) != 8:
         parser.error('Hexadecimal RGB color should be formatted as 0xRRGGBB')
```

### Comparing `scilpy-1.5.post2/scripts/scil_prepare_topup_command.py` & `scilpy-2.0.0/scripts/scil_dwi_prepare_topup_command.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
 Prepare a typical command for topup and create the necessary files.
 The reversed b0 must be in a different file.
+
+Formerly: scil_prepare_topup_command.py
 """
 
 import argparse
 import logging
 import os
 import subprocess
 
@@ -28,14 +30,17 @@
 
     p.add_argument('in_reverse_b0',
                    help='Input b0 Nifti image with reversed phase encoding.')
 
     p.add_argument('--config', default='b02b0.cnf',
                    help='Topup config file [%(default)s].')
 
+    p.add_argument('--synb0', action='store_true',
+                   help='If set, will use SyNb0 custom acqparams file.')
+
     p.add_argument('--encoding_direction', default='y',
                    choices=['x', 'y', 'z'],
                    help='Acquisition direction of the forward b0 '
                         'image, default is AP [%(default)s].')
 
     p.add_argument('--readout', type=float, default=0.062,
                    help='Total readout time from the DICOM metadata '
@@ -60,36 +65,34 @@
                         'terminal [%(default)s].')
 
     p.add_argument('--topup_options',  default='',
                    help='Additional options you want to use to run topup.\n'
                         'Add these options using quotes (i.e. "--fwhm=6'
                         ' --miter=4").')
 
-    add_overwrite_arg(p)
     add_verbose_arg(p)
+    add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     try:
         devnull = open(os.devnull)
         subprocess.call("topup", stderr=devnull)
     except:
         logging.warning(
             "topup not found. If executing locally, please install "
             "the command from the FSL library and make sure it is "
             "available in your path.")
 
-    if args.verbose:
-        logging.getLogger().setLevel(logging.INFO)
-
     required_args = [args.in_forward_b0, args.in_reverse_b0]
 
     assert_inputs_exist(parser, required_args)
     assert_outputs_exist(parser, args, [], args.out_b0s)
     assert_fsl_options_exist(parser, args.topup_options, 'topup')
 
     if os.path.splitext(args.out_prefix)[1] != '':
@@ -115,16 +118,16 @@
     elif len(rev_b0.shape) == 3:
         rev_b0 = rev_b0[..., None]
 
     fused_b0s = np.concatenate((b0, rev_b0), axis=-1)
     fused_b0s_path = os.path.join(args.out_directory, args.out_b0s)
     nib.save(nib.Nifti1Image(fused_b0s, b0_img.affine), fused_b0s_path)
 
-    acqparams = create_acqparams(
-        args.readout, args.encoding_direction, b0.shape[-1], rev_b0.shape[-1])
+    acqparams = create_acqparams(args.readout, args.encoding_direction,
+                                 args.synb0, b0.shape[-1], rev_b0.shape[-1])
 
     if not os.path.exists(args.out_directory):
         os.makedirs(args.out_directory)
 
     acqparams_path = os.path.join(args.out_directory, args.out_params)
     np.savetxt(acqparams_path, acqparams, fmt='%1.4f', delimiter=' ')
```

### Comparing `scilpy-1.5.post2/scripts/scil_print_connectivity_filenames.py` & `scilpy-2.0.0/scripts/scil_connectivity_print_filenames.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,69 +1,76 @@
 #! /usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
 Output the list of filenames using the coordinates from a binary connectivity
 matrix. Typically used to move around files that are considered valid after
-the scil_filter_connectivity.py script.
+the scil_connectivity_filter.py script.
 
 Example:
 # Keep connections with more than 1000 streamlines for 100% of a population
-scil_filter_connectivity.py filtering_mask.npy
+scil_connectivity_filter.py filtering_mask.npy
     --greater_than */streamlines_count.npy 1000 1.0
-scil_print_connectivity_filenames.py filtering_mask.npy
+scil_connectivity_print_filenames.py filtering_mask.npy
     labels_list.txt pass.txt
 for file in $(cat pass.txt);
     do mv ${SOMEWHERE}/${FILE} ${SOMEWHERE_ELSE}/;
 done
+
+Formerly: scil_print_connectivity_filenames.py
 """
 
 import argparse
+import logging
 
 import numpy as np
 
 from scilpy.io.utils import (add_overwrite_arg,
+                             add_verbose_arg,
                              assert_inputs_exist,
                              assert_outputs_exist,
                              load_matrix_in_any_format)
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter,
                                 description=__doc__)
 
     p.add_argument('in_matrix',
                    help='Binary matrix in numpy (.npy) format.\n'
-                        'Typically from scil_filter_connectivity.py')
+                        'Typically from scil_connectivity_filter.py')
     p.add_argument('labels_list',
                    help='List saved by the decomposition script.')
     p.add_argument('out_txt',
                    help='Output text file containing all filenames.')
 
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, args.in_matrix)
     assert_outputs_exist(parser, args, args.out_txt)
 
     matrix = load_matrix_in_any_format(args.in_matrix)
     labels_list = np.loadtxt(args.labels_list).astype(np.uint16)
 
     text_file = open(args.out_txt, 'w')
     for pos_1, pos_2 in np.argwhere(matrix > 0):
         in_label = labels_list[pos_1]
         out_label = labels_list[pos_2]
 
-        # scil_decompose_connectivity.py only save the lower triangular files
+        # scil_tractogram_segment_bundles_for_connectivity.py only save the
+        # lower triangular files
         if out_label < in_label:
             continue
         text_file.write('{}_{}.trk\n'.format(in_label, out_label))
     text_file.close()
 
 
 if __name__ == "__main__":
```

### Comparing `scilpy-1.5.post2/scripts/scil_print_header.py` & `scilpy-2.0.0/scripts/scil_header_print_info.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,39 +1,45 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
 Print the raw header from the provided file or only the specified keys.
 Support trk, nii and mgz files.
+
+Formerly: scil_print_header.py
 """
 
 import argparse
+import logging
 import pprint
 
 import nibabel as nib
 
-from scilpy.io.utils import assert_inputs_exist
+from scilpy.io.utils import assert_inputs_exist, add_verbose_arg
 from scilpy.utils.filenames import split_name_with_nii
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(description=__doc__,
                                 formatter_class=argparse.RawTextHelpFormatter)
 
     p.add_argument('in_file',
                    help='Input file (trk, nii and mgz).')
     p.add_argument('--keys', nargs='+',
                    help='Print only the specified keys.')
 
+    add_verbose_arg(p)
+
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, args.in_file)
 
     _, in_extension = split_name_with_nii(args.in_file)
 
     if in_extension in ['.tck', '.trk']:
         header = nib.streamlines.load(args.in_file, lazy_load=True).header
```

### Comparing `scilpy-1.5.post2/scripts/scil_project_streamlines_to_map.py` & `scilpy-2.0.0/scripts/scil_tractogram_project_streamlines_to_map.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,232 +1,236 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-Projects metrics onto the endpoints of streamlines. The idea is to visualize
-the cortical areas affected by metrics (assuming streamlines start/end in
-the cortex).
-
-This script can project data from maps (--in_metrics), from data_per_point
-(dpp) or data_per_streamline (dps): --load_dpp and --load_dps require an array
-from a file (must be the right shape), --use_dpp and --use_dps work only for
-.trk file and the key must exist in the metadata.
-
-The default options will take data from endpoints and project it to endpoints.
---from_wm will use data from whole streamlines.
---to_wm will project the data to whole streamline coverage.
-This creates 4 combinations of data source and projection.
+Projects metrics onto the underlying voxels of a streamlines. This script can
+project data from data_per_point (dpp) or data_per_streamline (dps) to maps.
+
+You choose to project data from all points of the streamlines, or from the
+endpoints only. The idea then is to visualize the cortical areas affected by
+metrics (assuming streamlines start/end in the cortex).
+
+See also scil_tractogram_project_map_to_streamlines.py for the reverse action.
+
+How to the data is loaded:
+    - From dps: uses the same value for each point of the streamline.
+    - From dpp: one value per point.
+
+How the data is used:
+    1. Average all points of the streamline to get a mean value, set this value
+       to all points.
+    2. Average the two endpoints and get their mean value, set this value to
+       all points.
+    3. Keep each point individually.
+
+How the data is projected to a map:
+    A. Using each point.
+    B. Using the endpoints only.
+
+For more complex operations than the average per streamline, see
+scil_tractogram_dpp_math.py.
 """
 
 import argparse
 import logging
 import os
 
 import nibabel as nib
-from nibabel.streamlines import ArraySequence
 import numpy as np
 
-from scilpy.io.image import assert_same_resolution
-from scilpy.io.streamlines import load_tractogram_with_reference
-from scilpy.io.utils import (add_overwrite_arg,
-                             assert_inputs_exist,
-                             assert_output_dirs_exist_and_empty,
-                             add_reference_arg,
-                             load_matrix_in_any_format)
+from scilpy.io.streamlines import (load_dpp_files_as_dpp,
+                                   load_dps_files_as_dps,
+                                   load_tractogram_with_reference)
+from scilpy.io.utils import (add_overwrite_arg, add_reference_arg,
+                             add_verbose_arg, assert_inputs_exist,
+                             assert_outputs_exist)
+from scilpy.tractograms.dps_and_dpp_management import (
+    convert_dps_to_dpp, perform_operation_dpp_to_dps, project_dpp_to_map)
 from scilpy.utils.filenames import split_name_with_nii
-from scilpy.tractanalysis.streamlines_metrics import \
-    compute_tract_counts_map
-from scilpy.tractanalysis.uncompress import uncompress
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__,
         formatter_class=argparse.RawTextHelpFormatter)
 
     p.add_argument('in_bundle',
                    help='Fiber bundle file.')
-    p.add_argument('out_folder',
-                   help='Folder where to save endpoints metric.')
+    p.add_argument('out_prefix',
+                   help='Folder + prefix to save endpoints metric(s). We will '
+                        'save \none nifti file per per dpp/dps key given.\n'
+                        'Ex: my_path/subjX_bundleY_ with --use_dpp key1 '
+                        'will output \nmy_path/subjX_bundleY_key1.nii.gz')
+
+    p1 = p.add_argument_group(
+        description='Where to get the statistics from. (Choose one)')
+    p1 = p1.add_mutually_exclusive_group(required=True)
+    p1.add_argument('--use_dps', metavar='key', nargs='+',
+                    help='Use the data_per_streamline from the tractogram.\n'
+                         'It must be a .trk')
+    p1.add_argument('--use_dpp', metavar='key', nargs='+', default=[],
+                    help='Use the data_per_point from the tractogram. \n'
+                         'It must be a trk.')
+    p1.add_argument('--load_dps', metavar='file', nargs='+', default=[],
+                    help='Load data per streamline (scalar) .txt or .npy.\n'
+                         'Must load an array with the right shape.')
+    p1.add_argument('--load_dpp', metavar='file', nargs='+', default=[],
+                    help='Load data per point (scalar) from .txt or .npy.\n'
+                         'Must load an array with the right shape.')
+
+    p2 = p.add_argument_group(description='Processing choices. (Choose one)')
+    p2 = p2.add_mutually_exclusive_group(required=True)
+    p2.add_argument('--mean_endpoints', action='store_true',
+                    help="Uses one single value per streamline: the mean "
+                         "of the two \nendpoints.")
+    p2.add_argument('--mean_streamline', action='store_true',
+                    help='Use one single value per streamline: '
+                         'the mean of all \npoints of the streamline.')
+    p2.add_argument('--point_by_point', action='store_true',
+                    help="Directly project the streamlines values onto the "
+                         "map.\n")
+
+    p3 = p.add_argument_group(
+        description='Where to send the statistics. (Choose one)')
+    p3 = p3.add_mutually_exclusive_group(required=True)
+    p3.add_argument('--to_endpoints', action='store_true',
+                    help="Project metrics onto a mask of the endpoints.")
+    p3.add_argument('--to_wm', action='store_true',
+                    help='Project metrics into streamlines coverage.')
 
-    p1 = p.add_mutually_exclusive_group(required=True)
-    p1.add_argument('--in_metrics', nargs='+', default=[],
-                    help='Nifti metric(s) to compute statistics on.')
-    p1.add_argument('--use_dps', metavar='DPS_KEY', nargs='+',
-                    help='Use the data_per_streamline (scalar) from file, '
-                         'e.g. commit_weights.')
-    p1.add_argument('--use_dpp', metavar='DPP_KEY', nargs='+', default=[],
-                    help='Use the data_per_point (scalar) from file.')
-    p1.add_argument('--load_dps', metavar='DPS_KEY', nargs='+', default=[],
-                    help='Load data per streamline (scalar) .txt or .npy.')
-    p1.add_argument('--load_dpp', metavar='DPP_KEY', nargs='+', default=[],
-                    help='Load data per point (scalar) from .txt or .npy.')
-
-    p.add_argument('--from_wm', action='store_true',
-                   help='Project metrics from whole streamlines coverage.')
-    p.add_argument('--to_wm', action='store_true',
-                   help='Project metrics into streamlines coverage.')
     add_reference_arg(p)
+    add_verbose_arg(p)
     add_overwrite_arg(p)
-    return p
 
-
-def _compute_streamline_mean(cur_ind, cur_min, cur_max, data):
-    # From the precomputed indices, compute the binary map
-    # and use it to weight the metric data for this specific streamline.
-    cur_range = tuple(cur_max - cur_min)
-    streamline_density = compute_tract_counts_map(ArraySequence([cur_ind]),
-                                                  cur_range)
-    streamline_data = data[cur_min[0]:cur_max[0],
-                           cur_min[1]:cur_max[1],
-                           cur_min[2]:cur_max[2]]
-    streamline_average = np.average(streamline_data,
-                                    weights=streamline_density)
-    return streamline_average
-
-
-def _process_streamlines(streamlines, just_endpoints):
-    # Compute the bounding boxes and indices for all streamlines.
-    # just_endpoints will get the indices of the endpoints only for the
-    # usecase of projecting GM metrics into the WM.
-    mins = []
-    maxs = []
-    offset_streamlines = []
-
-    # Offset the streamlines to compute the indices only in the bounding box.
-    # Reduces memory use later on.
-    for idx, s in enumerate(streamlines):
-        mins.append(np.min(s.astype(int), 0))
-        maxs.append(np.max(s.astype(int), 0) + 1)
-        if just_endpoints:
-            s = np.stack((s[0, :], s[-1, :]), axis=0)
-        offset_streamlines.append((s - mins[-1]).astype(np.float32))
-
-    offset_streamlines = ArraySequence(offset_streamlines)
-    indices = uncompress(offset_streamlines)
-
-    return mins, maxs, indices
-
-
-def _project_metrics(curr_metric_map, count, orig_s, streamline_mean,
-                     just_endpoints):
-    if just_endpoints:
-        xyz = orig_s[0, :].astype(int)
-        curr_metric_map[xyz[0], xyz[1], xyz[2]] += streamline_mean
-        count[xyz[0], xyz[1], xyz[2]] += 1
-
-        xyz = orig_s[-1, :].astype(int)
-        curr_metric_map[xyz[0], xyz[1], xyz[2]] += streamline_mean
-        count[xyz[0], xyz[1], xyz[2]] += 1
-    else:
-        for x, y, z in orig_s[:].astype(int):
-            curr_metric_map[x, y, z] += streamline_mean
-            count[x, y, z] += 1
+    return p
 
 
-def _pick_data(args, sft):
-    if args.use_dps or args.load_dps:
-        if args.use_dps:
-            for dps in args.use_dps:
-                if dps not in sft.data_per_streamline:
-                    raise IOError('DPS key not in the sft: {}'.format(dps))
-            name = args.use_dps
-            data = [sft.data_per_streamline[dps] for dps in args.use_dps]
+def _load_dpp_dps(args, parser, sft):
+    # In call cases: only one of the values below can be set at the time.
+    dps_to_use = None
+    dpp_to_use = None
+
+    # 1. With options --use_dps, --use_dpp: check that dps / dpp key is found.
+    # 2. With options --load_dps, --load_dpp: Load them now to SFT, check that
+    #    they fit with the data.
+    if args.use_dps:
+        dps_to_use = args.use_dps
+        possible_dps = list(sft.data_per_streamline.keys())
+        for key in args.use_dps:
+            if key not in possible_dps:
+                parser.error('DPS key not ({}) not found in your tractogram!'
+                             .format(key))
+    elif args.use_dpp:
+        dpp_to_use = args.use_dpp
+        possible_dpp = list(sft.data_per_point.keys())
+        for key in args.use_dpp:
+            if key not in possible_dpp:
+                parser.error('DPP key ({}) not found in your tractogram!'
+                             .format(key))
+    elif args.load_dps:
+        logging.info("Loading dps from file.")
+
+        # It does not matter if we overwrite: Not saving the result sft.
+        sft, dps_to_use = load_dps_files_as_dps(parser, args.load_dps, sft,
+                                                overwrite=True)
+    else:  # args.load_dpp:
+        # Loading dpp for all points even if we won't use them all to make
+        # sure that the loaded files have the correct shape.
+        logging.info("Loading dpp from file")
+        sft, dpp_to_use = load_dpp_files_as_dpp(parser, args.load_dpp, sft,
+                                                overwrite=True)
+
+    # Verify that we have singular values. (Ex, not colors)
+    # Remove unused keys to save memory.
+    all_keys = list(sft.data_per_point.keys())
+    for key in all_keys:
+        if dpp_to_use is not None and key in dpp_to_use:
+            d0 = sft.data_per_point[key][0][0]
+            if len(d0) > 1:
+                raise ValueError(
+                    "Expecting scalar values as data_per_point. Got data of "
+                    "shape {} for key {}".format(d0.shape, key))
         else:
-            name = args.load_dps
-            data = [load_matrix_in_any_format(dps) for dps in args.load_dps]
-        for i in range(len(data)):
-            if len(data[i]) != len(sft):
-                raise IOError('DPS length does not match the SFT: {}'
-                              .format(name[i]))
-    elif args.use_dpp or args.load_dpp:
-        if args.use_dpp:
-            name = args.use_dpp
-            data = [sft.data_per_point[dpp]._data for dpp in args.use_dpp]
+            del sft.data_per_point[key]
+
+    all_keys = list(sft.data_per_streamline.keys())
+    for key in all_keys:
+        if dps_to_use is not None and key in dps_to_use:
+            d0 = sft.data_per_streamline[key][0]
+            if len(d0) > 1:
+                raise ValueError(
+                    "Expecting scalar values as data_per_streamline. Got data "
+                    "of shape {} for key {}.".format(d0.shape, key))
         else:
-            name = args.load_dpp
-            data = [load_matrix_in_any_format(dpp) for dpp in args.load_dpp]
-        for i in range(len(data)):
-            if len(data[i]) != len(sft.streamlines._data):
-                raise IOError('DPP length does not match the SFT: {}'
-                              .format(name[i]))
-    return zip(name, data)
+            del sft.data_per_streamline[key]
+
+    return sft, dps_to_use, dpp_to_use
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, [args.in_bundle], args.in_metrics +
-                        args.load_dps + args.load_dpp)
-    assert_output_dirs_exist_and_empty(parser, args,
-                                       args.out_folder,
-                                       create_dir=True)
+    # -------- General checks ----------
+    assert_inputs_exist(parser, [args.in_bundle],
+                        args.load_dps + args.load_dpp + [args.reference])
+
+    # Find all final output files (one per metric).
+    if args.load_dps or args.load_dpp:
+        files = args.load_dps or args.load_dpp
+        metrics_names = []
+        for file in files:
+            # Prepare dpp key from filename.
+            name = os.path.basename(file)
+            name, ext = split_name_with_nii(name)
+            metrics_names.append(name)
+    else:
+        metrics_names = args.use_dpp or args.use_dps
+    out_files = [args.out_prefix + m + '.nii.gz' for m in metrics_names]
+    assert_outputs_exist(parser, args, out_files)
 
+    # -------- Load streamlines and checking compatibility ----------
+    logging.info("Loading tractogram {}".format(args.in_bundle))
     sft = load_tractogram_with_reference(parser, args, args.in_bundle)
     sft.to_vox()
     sft.to_corner()
 
     if len(sft.streamlines) == 0:
         logging.warning('Empty bundle file {}. Skipping'.format(args.bundle))
         return
 
-    mins, maxs, indices = _process_streamlines(sft.streamlines,
-                                               not args.from_wm)
+    # -------- Load dps / dpp. ----------
+    sft, dps_to_use, dpp_to_use = _load_dpp_dps(args, parser, sft)
 
-    if args.in_metrics:
-        assert_same_resolution(args.in_metrics)
-        metrics = [nib.load(metric) for metric in args.in_metrics]
-        for metric in metrics:
-            data = metric.get_fdata(dtype=np.float32)
-            curr_metric_map = np.zeros(metric.shape)
-            count = np.zeros(metric.shape)
-            for cur_min, cur_max, cur_ind, orig_s in zip(mins, maxs, indices,
-                                                         sft.streamlines):
-
-                streamline_mean = _compute_streamline_mean(cur_ind,
-                                                           cur_min,
-                                                           cur_max,
-                                                           data)
-                _project_metrics(curr_metric_map, count, orig_s,
-                                 streamline_mean, not args.to_wm)
-            curr_metric_map[count != 0] /= count[count != 0]
-            metric_fname, ext = split_name_with_nii(
-                os.path.basename(metric.get_filename()))
-            nib.save(nib.Nifti1Image(curr_metric_map, metric.affine,
-                                     metric.header),
-                     os.path.join(args.out_folder,
-                                  '{}_endpoints_metric{}'.format(metric_fname,
-                                                                 ext)))
+    # Convert dps to dpp. Easier to manage all the remaining options without
+    # multiplying if - else calls.
+    if dps_to_use is not None:
+        # Then dpp_to_use is None, and the sft contains no dpp key.
+        # Can overwrite.
+        sft = convert_dps_to_dpp(sft, dps_to_use, overwrite=True)
+        all_keys = dps_to_use
     else:
-        for fname, data in _pick_data(args, sft):
-            curr_metric_map = np.zeros(sft.dimensions)
-            count = np.zeros(sft.dimensions)
-
-            for j in range(len(sft.streamlines)):
-                if args.use_dps or args.load_dps:
-                    streamline_mean = np.mean(data[j])
-                else:
-                    tmp_data = ArraySequence()
-                    tmp_data._data = data
-                    tmp_data._offsets = sft.streamlines._offsets
-                    tmp_data._lengths = sft.streamlines._lengths
-
-                    if not args.to_wm:
-                        streamline_mean = (np.mean(tmp_data[j][-1])
-                                           + np.mean(tmp_data[j][0])) / 2
-                    else:
-                        streamline_mean = np.mean(tmp_data[j])
-
-                _project_metrics(curr_metric_map, count, sft.streamlines[j],
-                                 streamline_mean, not args.to_wm)
-
-            curr_metric_map[count != 0] /= count[count != 0]
-            metric_fname, _ = os.path.splitext(os.path.basename(fname))
-            nib.save(nib.Nifti1Image(curr_metric_map, sft.affine),
-                     os.path.join(args.out_folder,
-                                  '{}_endpoints_metric{}'.format(metric_fname,
-                                                                 '.nii.gz')))
+        all_keys = dpp_to_use
+
+    # -------- Format values  ----------
+    # In case where we average the dpp, average it now and pretend it's a dps,
+    # then re-copy to all dpp.
+    if args.mean_streamline or args.mean_endpoints:
+        logging.info("Averaging values for all streamlines.")
+        for key in all_keys:
+            sft.data_per_streamline[key] = perform_operation_dpp_to_dps(
+                'mean', sft, key, endpoints_only=args.mean_endpoints)
+        sft = convert_dps_to_dpp(sft, all_keys, overwrite=True)
+
+    # -------- Projection and saving ----------
+    for key in all_keys:
+        logging.info("Projecting streamlines metric {} to a map".format(key))
+        the_map = project_dpp_to_map(sft, key, endpoints_only=args.to_endpoints)
+
+        out_file = args.out_prefix + key + '.nii.gz'
+        logging.info("Saving file {}".format(out_file))
+        nib.save(nib.Nifti1Image(the_map, sft.affine), out_file)
 
 
 if __name__ == '__main__':
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_recognize_multi_bundles.py` & `scilpy-2.0.0/scripts/scil_tractogram_segment_one_bundle.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,57 +1,44 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-Compute RecobundlesX (multi-atlas & multi-parameters).
-The model needs to be cleaned and lightweight.
+Compute a simple Recobundles (single-atlas & single-parameters).
+The model need to be cleaned and lightweight.
 Transform should come from ANTs: (using the --inverse flag)
 AntsRegistrationSyNQuick.sh -d 3 -m MODEL_REF -f SUBJ_REF
 
-If you are not sure about the transformation 'direction' you can try
-scil_recognize_single_bundle.py (with the -v option), a warning will popup if
-the provided transformation is not use correctly.
-
-The next two arguments are multi-parameters related:
---multi_parameters must be lower than len(model_clustering_thr) *
-len(bundle_pruning_thr) * len(tractogram_clustering_thr)
-
---seeds can be more than one value. Multiple values will result in
-a overall multiplicative factor of len(seeds) * '--multi_parameters'
-
-The number of folders provided by 'models_directories' will further multiply
-the total number of runs. Meaning that the total number of Recobundles
-execution will be len(seeds) * '--multi_parameters' * len(models_directories)
-
---minimal_vote_ratio is a value between 0 and 1. The actual number of votes
-required will be '--minimal_vote_ratio' * len(seeds) * '--multi_parameters'
-* len(models_directories).
-
-Example: 5 atlas, 9 multi-parameters, 2 seeds with a minimal vote_ratio
-of 0.50 will results in 90 executions (for each bundle in the config file)
-and a minimal vote of 45 / 90.
+If you are unsure about the transformation 'direction' use the verbose
+option (-v) and try with and without the --inverse flag. If you are not using
+the right transformation 'direction' a warning will popup. If there is no
+warning in both case it means the transformation is very close to identity and
+both 'direction' will work.
 
-Example data and usage available at: https://zenodo.org/record/3928503
+Formerly: scil_recognize_single_bundles.py
 """
 
 import argparse
 import logging
-import json
-import os
-import random
+import pickle
 
-import coloredlogs
+from dipy.io.stateful_tractogram import Space, StatefulTractogram
+from dipy.io.streamline import save_tractogram
+from dipy.segment.bundles import RecoBundles
+from dipy.tracking.streamline import transform_streamlines
+from nibabel.streamlines.array_sequence import ArraySequence
 import numpy as np
 
+from scilpy.io.streamlines import load_tractogram_with_reference
 from scilpy.io.utils import (add_overwrite_arg,
-                             add_processes_arg,
+                             add_reference_arg,
+                             add_verbose_arg,
                              assert_inputs_exist,
-                             assert_output_dirs_exist_and_empty,
+                             assert_outputs_exist,
                              load_matrix_in_any_format)
-from scilpy.segment.voting_scheme import VotingScheme
+from scilpy.utils.spatial import compute_distance_barycenters
 
 EPILOG = """
 Garyfallidis, E., Cote, M. A., Rheault, F., ... &
 Descoteaux, M. (2018). Recognition of white matter
 bundles using local and global streamline-based registration and
 clustering. NeuroImage, 170, 283-295.
 """
@@ -60,92 +47,110 @@
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         formatter_class=argparse.RawTextHelpFormatter,
         description=__doc__,
         epilog=EPILOG)
 
     p.add_argument('in_tractogram',
-                   help='Input tractogram filename (.trk or .tck).')
-    p.add_argument('in_config_file',
-                   help='Path of the config file (.json)')
-    p.add_argument('in_models_directories', nargs='+',
-                   help='Path for the directories containing model.')
+                   help='Input tractogram filename.')
+    p.add_argument('in_model',
+                   help='Model to use for recognition.')
     p.add_argument('in_transfo',
                    help='Path for the transformation to model space '
                         '(.txt, .npy or .mat).')
+    p.add_argument('out_tractogram',
+                   help='Output tractogram filename.')
 
-    p.add_argument('--out_dir', default='voting_results',
-                   help='Path for the output directory [%(default)s].')
-    p.add_argument('--log_level', default='INFO',
-                   choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
-                   help='Log level of the logging class.')
-
-    p.add_argument('--multi_parameters', type=int, default=1,
-                   help='Pick parameters from the potential combinations\n'
-                        'Will multiply the number of times Recobundles is ran.\n'
-                        'See the documentation [%(default)s].')
-    p.add_argument('--minimal_vote_ratio', type=float, default=0.5,
-                   help='Streamlines will only be considered for saving if\n'
-                        'recognized often enough [%(default)s].')
-
-    p.add_argument('--tractogram_clustering_thr',
-                   type=int, default=[12], nargs='+',
-                   help='Input tractogram clustering thresholds %(default)smm.')
-
-    p.add_argument('--seeds', type=int, default=[0], nargs='+',
-                   help='Random number generator seed %(default)s\n'
-                        'Will multiply the number of times Recobundles is ran.')
+    p.add_argument('--tractogram_clustering_thr', type=float, default=8,
+                   help='Clustering threshold used for the whole brain '
+                        '[%(default)smm].')
+    p.add_argument('--model_clustering_thr', type=float, default=4,
+                   help='Clustering threshold used for the model '
+                        '[%(default)smm].')
+    p.add_argument('--pruning_thr', type=float, default=6,
+                   help='MDF threshold used for final streamlines selection '
+                        '[%(default)smm].')
+
+    p.add_argument('--slr_threads', type=int, default=1,
+                   help='Number of threads for SLR [%(default)s].')
+    p.add_argument('--seed', type=int, default=None,
+                   help='Random number generator seed [%(default)s].')
     p.add_argument('--inverse', action='store_true',
                    help='Use the inverse transformation.')
+    p.add_argument('--no_empty', action='store_true',
+                   help='Do not write file if there is no streamline.')
 
-    add_processes_arg(p)
+    group = p.add_mutually_exclusive_group()
+    group.add_argument('--in_pickle',
+                       help='Input pickle clusters map file.\nWill override '
+                            'the tractogram_clustering_thr parameter.')
+    group.add_argument('--out_pickle',
+                       help='Output pickle clusters map file.')
+
+    add_reference_arg(p)
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, [args.in_tractogram,
-                                 args.in_config_file,
-                                 args.in_transfo])
-
-    for directory in args.in_models_directories:
-        if not os.path.isdir(directory):
-            parser.error('Input folder {0} does not exist'.format(directory))
-
-    assert_output_dirs_exist_and_empty(parser, args, args.out_dir)
-
-    file_handler = logging.FileHandler(filename=os.path.join(args.out_dir,
-                                                             'logfile.txt'))
-    formatter = logging.Formatter(fmt='%(asctime)s, %(name)s %(levelname)s %(message)s',
-                                  datefmt='%H:%M:%S')
-    file_handler.setFormatter(formatter)
-    logging.getLogger().setLevel(args.log_level)
-    logging.getLogger().addHandler(file_handler)
-    coloredlogs.install(level=args.log_level)
+    assert_inputs_exist(parser, [args.in_tractogram, args.in_transfo],
+                        args.reference)
+    assert_outputs_exist(parser, args, args.out_tractogram)
+
+    wb_file = load_tractogram_with_reference(parser, args, args.in_tractogram)
+    wb_streamlines = wb_file.streamlines
+    model_file = load_tractogram_with_reference(parser, args, args.in_model)
 
     transfo = load_matrix_in_any_format(args.in_transfo)
     if args.inverse:
         transfo = np.linalg.inv(load_matrix_in_any_format(args.in_transfo))
 
-    with open(args.in_config_file) as json_data:
-        config = json.load(json_data)
-
-    voting = VotingScheme(config, args.in_models_directories,
-                          transfo, args.out_dir,
-                          tractogram_clustering_thr=args.tractogram_clustering_thr,
-                          minimal_vote_ratio=args.minimal_vote_ratio,
-                          multi_parameters=args.multi_parameters)
-
-    if args.seeds is None:
-        seeds = [random.randint(1, 1000)]
+    before, after = compute_distance_barycenters(wb_file, model_file, transfo)
+    if after > before:
+        logging.warning('The distance between volumes barycenter should be '
+                        'lower after registration. Maybe try using/removing '
+                        '--inverse.')
+        logging.info('Distance before: {}, Distance after: {}'.format(
+            np.round(before, 3), np.round(after, 3)))
+    model_streamlines = transform_streamlines(model_file.streamlines, transfo)
+
+    rng = np.random.RandomState(args.seed)
+    if args.in_pickle:
+        with open(args.in_pickle, 'rb') as infile:
+            cluster_map = pickle.load(infile)
+        reco_obj = RecoBundles(wb_streamlines,
+                               cluster_map=cluster_map,
+                               rng=rng, less_than=1,
+                               verbose=args.verbose)
     else:
-        seeds = args.seeds
-
-    voting(args.in_tractogram, nbr_processes=args.nbr_processes, seeds=seeds)
+        reco_obj = RecoBundles(wb_streamlines,
+                               clust_thr=args.tractogram_clustering_thr,
+                               rng=rng, greater_than=1,
+                               verbose=args.verbose)
+
+    if args.out_pickle:
+        with open(args.out_pickle, 'wb') as outfile:
+            pickle.dump(reco_obj.cluster_map, outfile)
+    _, indices = reco_obj.recognize(ArraySequence(model_streamlines),
+                                    args.model_clustering_thr,
+                                    pruning_thr=args.pruning_thr,
+                                    num_threads=args.slr_threads)
+    new_streamlines = wb_streamlines[indices]
+    new_data_per_streamlines = wb_file.data_per_streamline[indices]
+    new_data_per_points = wb_file.data_per_point[indices]
+
+    if not args.no_empty or new_streamlines:
+        sft = StatefulTractogram(new_streamlines, wb_file.space_attributes,
+                                 Space.RASMM,
+                                 data_per_streamline=new_data_per_streamlines,
+                                 data_per_point=new_data_per_points)
+        save_tractogram(sft, args.out_tractogram)
 
 
 if __name__ == '__main__':
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_recognize_single_bundle.py` & `scilpy-2.0.0/scripts/scil_tractogram_resample.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,155 +1,156 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-Compute a simple Recobundles (single-atlas & single-parameters).
-The model need to be cleaned and lightweight.
-Transform should come from ANTs: (using the --inverse flag)
-AntsRegistrationSyNQuick.sh -d 3 -m MODEL_REF -f SUBJ_REF
-
-If you are unsure about the transformation 'direction' use the verbose
-option (-v) and try with and without the --inverse flag. If you are not using
-the right transformation 'direction' a warning will popup. If there is no
-warning in both case it means the transformation is very close to identity and
-both 'direction' will work.
+Script to resample a tractogram to a set number of streamlines.
+Default behavior:
+- IF number of requested streamlines is lower than streamline count: DOWNSAMPLE
+- IF number of requested streamlines is higher than streamline count: UPSAMPLE
+To prevent upsample if not desired use --never_upsample.
+
+Can be useful to build training sets for machine learning algorithms, to
+upsample under-represented bundles or downsample over-represented bundles.
+
+Works by either selecting a subset of streamlines or by generating new
+streamlines by adding gaussian noise to existing ones.
+
+Upsampling:
+    Includes smoothing to compensate for the noisiness of new streamlines
+    generated by the process.
+Downsampling:
+    Includes the possibility of choosing randomly *per Quickbundle cluster* to
+    ensure that all clusters are represented in the final tractogram.
+
+Example usage:
+$ scil_tractogram_resample.py input.trk 1000 output.trk \
+--point_wise_std 0.5 --gaussian 5 --keep_invalid_streamlines
+$ scil_visualize_bundles.py output.trk --local_coloring --width=0.1
 """
 
 import argparse
 import logging
-import pickle
 
-from dipy.io.stateful_tractogram import Space, StatefulTractogram
 from dipy.io.streamline import save_tractogram
-from dipy.segment.bundles import RecoBundles
-from dipy.tracking.streamline import transform_streamlines
-from nibabel.streamlines.array_sequence import ArraySequence
-import numpy as np
 
 from scilpy.io.streamlines import load_tractogram_with_reference
-from scilpy.io.utils import (add_overwrite_arg,
-                             add_reference_arg,
+from scilpy.io.utils import (add_overwrite_arg, add_reference_arg,
                              add_verbose_arg,
                              assert_inputs_exist,
-                             assert_outputs_exist,
-                             load_matrix_in_any_format)
-from scilpy.utils.util import compute_distance_barycenters
-
-EPILOG = """
-Garyfallidis, E., Cote, M. A., Rheault, F., ... &
-Descoteaux, M. (2018). Recognition of white matter
-bundles using local and global streamline-based registration and
-clustering. NeuroImage, 170, 283-295.
-"""
+                             assert_outputs_exist)
+from scilpy.tractograms.tractogram_operations import (
+    split_sft_randomly,
+    split_sft_randomly_per_cluster,
+    upsample_tractogram)
 
 
 def _build_arg_parser():
-    p = argparse.ArgumentParser(
-        formatter_class=argparse.RawTextHelpFormatter,
-        description=__doc__,
-        epilog=EPILOG)
+    p = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter,
+                                description=__doc__)
 
     p.add_argument('in_tractogram',
-                   help='Input tractogram filename.')
-    p.add_argument('in_model',
-                   help='Model to use for recognition.')
-    p.add_argument('in_transfo',
-                   help='Path for the transformation to model space '
-                        '(.txt, .npy or .mat).')
+                   help='Input tractography file.')
+    p.add_argument('nb_streamlines', type=int,
+                   help='Number of streamlines to resample the tractogram to.')
     p.add_argument('out_tractogram',
-                   help='Output tractogram filename.')
+                   help='Output tractography file.')
 
-    p.add_argument('--tractogram_clustering_thr', type=float, default=8,
-                   help='Clustering threshold used for the whole brain '
-                        '[%(default)smm].')
-    p.add_argument('--model_clustering_thr', type=float, default=4,
-                   help='Clustering threshold used for the model '
-                        '[%(default)smm].')
-    p.add_argument('--pruning_thr', type=float, default=6,
-                   help='MDF threshold used for final streamlines selection '
-                        '[%(default)smm].')
-
-    p.add_argument('--slr_threads', type=int, default=1,
-                   help='Number of threads for SLR [%(default)s].')
-    p.add_argument('--seed', type=int, default=None,
-                   help='Random number generator seed [%(default)s].')
-    p.add_argument('--inverse', action='store_true',
-                   help='Use the inverse transformation.')
-    p.add_argument('--no_empty', action='store_true',
-                   help='Do not write file if there is no streamline.')
-
-    group = p.add_mutually_exclusive_group()
-    group.add_argument('--in_pickle',
-                       help='Input pickle clusters map file.\nWill override '
-                            'the tractogram_clustering_thr parameter.')
-    group.add_argument('--out_pickle',
-                       help='Output pickle clusters map file.')
+    p.add_argument('--never_upsample', action='store_true',
+                   help='Make sure to never upsample a tractogram.\n'
+                        'Useful when downsample batch of files using bash.')
+
+    # For upsampling:
+    upsampling_group = p.add_argument_group('Upsampling params')
+    upsampling_group.add_argument('--point_wise_std', type=float, default=1,
+                                  help='Noise to add to existing streamlines '
+                                       'points to generate new ones [%(default)s].')
+    upsampling_group.add_argument('--tube_radius', type=float, default=1,
+                                  help='Maximum distance to generate streamlines '
+                                       'around the original ones [%(default)s].')
+    upsampling_group.add_argument('--gaussian', metavar='SIGMA', type=int,
+                                  help='Sigma for smoothing. Use the value of '
+                                       'surrounding X,Y,Z points on the '
+                                       'streamline to blur the streamlines.\n'
+                                       'A good sigma choice would be around 5.')
+    upsampling_group.add_argument('-e', dest='error_rate', type=float, default=0.1,
+                                  help='Maximum compression distance in mm '
+                                       '[%(default)s].')
+
+    upsampling_group.add_argument('--keep_invalid_streamlines',
+                                  action='store_true',
+                                  help='Keep invalid newly generated streamlines'
+                                       ' that may go out of the \nbounding box.')
+
+    # For downsampling:
+    downsampling_group = p.add_argument_group('Downsampling params')
+    downsampling_group.add_argument(
+        '--downsample_per_cluster', action='store_true',
+        help='If set, downsampling will be done per cluster (computed with \n'
+             'Quickbundles) to ensure that at least some streamlines are \n'
+             'kept per bundle. Else, random downsampling is performed '
+             '(default).')
+    downsampling_group.add_argument(
+        '--qbx_thresholds', nargs='+', type=float, default=[40, 30, 20],
+        help="If you chose option '--downsample_per_cluster', you may set \n"
+             "the QBx threshold value(s) here. Default: %(default)s")
+
+    # General
+    p.add_argument('--seed', default=None, type=int,
+                   help='Use a specific random seed for the resampling.')
 
     add_reference_arg(p)
     add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, [args.in_tractogram, args.in_transfo])
-    assert_outputs_exist(parser, args, args.out_tractogram)
+    if args.point_wise_std is not None and args.point_wise_std <= 0:
+        parser.error('argument --point_wise_std: must be > 0')
+    if args.tube_radius is not None and args.tube_radius <= 0:
+        parser.error('argument --tube_radius: must be > 0')
 
-    log_level = logging.INFO if args.verbose else logging.WARNING
-    logging.getLogger().setLevel(log_level)
+    assert_inputs_exist(parser, args.in_tractogram, args.reference)
+    assert_outputs_exist(parser, args, args.out_tractogram)
 
-    wb_file = load_tractogram_with_reference(parser, args, args.in_tractogram)
-    wb_streamlines = wb_file.streamlines
-    model_file = load_tractogram_with_reference(parser, args, args.in_model)
-
-    transfo = load_matrix_in_any_format(args.in_transfo)
-    if args.inverse:
-        transfo = np.linalg.inv(load_matrix_in_any_format(args.in_transfo))
-
-    before, after = compute_distance_barycenters(wb_file, model_file, transfo)
-    if after > before:
-        logging.warning('The distance between volumes barycenter should be '
-                        'lower after registration. Maybe try using/removing '
-                        '--inverse.')
-        logging.info('Distance before: {}, Distance after: {}'.format(
-            np.round(before, 3), np.round(after, 3)))
-    model_streamlines = transform_streamlines(model_file.streamlines, transfo)
-
-    rng = np.random.RandomState(args.seed)
-    if args.in_pickle:
-        with open(args.in_pickle, 'rb') as infile:
-            cluster_map = pickle.load(infile)
-        reco_obj = RecoBundles(wb_streamlines,
-                               cluster_map=cluster_map,
-                               rng=rng, less_than=1,
-                               verbose=args.verbose)
-    else:
-        reco_obj = RecoBundles(wb_streamlines,
-                               clust_thr=args.tractogram_clustering_thr,
-                               rng=rng, greater_than=1,
-                               verbose=args.verbose)
-
-    if args.out_pickle:
-        with open(args.out_pickle, 'wb') as outfile:
-            pickle.dump(reco_obj.cluster_map, outfile)
-    _, indices = reco_obj.recognize(ArraySequence(model_streamlines),
-                                    args.model_clustering_thr,
-                                    pruning_thr=args.pruning_thr,
-                                    num_threads=args.slr_threads)
-    new_streamlines = wb_streamlines[indices]
-    new_data_per_streamlines = wb_file.data_per_streamline[indices]
-    new_data_per_points = wb_file.data_per_point[indices]
-
-    if not args.no_empty or new_streamlines:
-        sft = StatefulTractogram(new_streamlines, wb_file.space_attributes,
-                                 Space.RASMM,
-                                 data_per_streamline=new_data_per_streamlines,
-                                 data_per_point=new_data_per_points)
-        save_tractogram(sft, args.out_tractogram)
+    logging.info("Loading sft.")
+    sft = load_tractogram_with_reference(parser, args, args.in_tractogram)
+    original_number = len(sft.streamlines)
+
+    if args.never_upsample and args.nb_streamlines > original_number:
+        args.nb_streamlines = original_number
+
+    logging.info("Done. Now getting {} streamlines."
+                 .format(args.nb_streamlines))
+
+    if args.nb_streamlines > original_number or args.tube_radius:
+        # Check is done here because it is not required if downsampling
+        if not args.point_wise_std and not args.tube_radius:
+            parser.error("one of the arguments --point_wise_std " +
+                         "--tube_radius is required")
+        sft = upsample_tractogram(sft, args.nb_streamlines,
+                                  args.point_wise_std, args.tube_radius,
+                                  args.gaussian, args.error_rate, args.seed)
+    elif args.nb_streamlines < original_number:
+        if args.downsample_per_cluster:
+            # output contains rejected streamlines, we don't use them.
+            sft, _ = split_sft_randomly_per_cluster(
+                sft, [args.nb_streamlines], args.seed, args.qbx_thresholds)
+            logging.info("Kept {} out of {} expected streamlines."
+                         .format(len(sft), args.nb_streamlines))
+        else:
+            # output is a list of two: kept and rejected.
+            sft = split_sft_randomly(sft, args.nb_streamlines, args.seed)[0]
+
+    if not args.keep_invalid_streamlines:
+        sft.remove_invalid_streamlines()
+    save_tractogram(sft, args.out_tractogram,
+                    bbox_valid_check=not args.keep_invalid_streamlines)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_register_tractogram.py` & `scilpy-2.0.0/scripts/scil_tractogram_register.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,20 +1,23 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
 Generate a linear transformation matrix from the registration of
 2 tractograms. Typically, this script is run before
-scil_apply_transform_to_tractogram.py.
+scil_tractogram_apply_transform.py.
 
 For more informations on how to use the various registration scripts
 see the doc/tractogram_registration.md readme file
+
+Formerly: scil_register_tractogram.py
 """
 
 import argparse
+import logging
 import os
 
 from dipy.align.streamlinear import whole_brain_slr
 import numpy as np
 
 from scilpy.io.streamlines import load_tractogram_with_reference
 from scilpy.io.utils import (add_overwrite_arg, add_reference_arg,
@@ -44,26 +47,29 @@
                         '[<out_name>_<affine/rigid>.txt]')
     p.add_argument('--only_rigid', action='store_true',
                    help='Will only use a rigid transformation, '
                         'uses affine by default.')
 
     add_reference_arg(p, 'moving_tractogram')
     add_reference_arg(p, 'static_tractogram')
-    add_overwrite_arg(p)
     add_verbose_arg(p)
+    add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, [args.moving_tractogram,
-                                 args.static_tractogram])
+                                 args.static_tractogram],
+                        [args.moving_tractogram_ref,
+                         args.static_tractogram_ref])
 
     if args.only_rigid:
         matrix_filename = os.path.splitext(args.out_name)[0] + '_rigid.txt'
     else:
         matrix_filename = os.path.splitext(args.out_name)[0] + '_affine.txt'
 
     assert_outputs_exist(parser, args, matrix_filename, args.out_name)
```

### Comparing `scilpy-1.5.post2/scripts/scil_remove_invalid_streamlines.py` & `scilpy-2.0.0/scripts/scil_tractogram_remove_invalid.py`

 * *Files 26% similar despite different names*

```diff
@@ -4,28 +4,30 @@
 """
 Removal of streamlines that are out of the volume bounding box. In voxel space
 no negative coordinate and no above volume dimension coordinate are possible.
 Any streamline that do not respect these two conditions are removed.
 
 The --cut_invalid option will cut streamlines so that their longest segment are
 within the bounding box
+
+Formerly: scil_remove_invalid_streamlines.py
 """
 
 import argparse
 import logging
 
-from dipy.io.stateful_tractogram import StatefulTractogram
-from dipy.io.streamline import save_tractogram
-import numpy as np
-
-from scilpy.io.streamlines import load_tractogram_with_reference
-from scilpy.io.utils import (add_bbox_arg, add_overwrite_arg,
+from scilpy.io.streamlines import load_tractogram_with_reference, \
+    save_tractogram
+from scilpy.io.utils import (add_overwrite_arg, add_verbose_arg,
                              add_reference_arg, assert_inputs_exist,
                              assert_outputs_exist)
-from scilpy.utils.streamlines import cut_invalid_streamlines
+from scilpy.tractograms.streamline_operations import (
+    cut_invalid_streamlines,
+    remove_overlapping_points_streamlines,
+    remove_single_point_streamlines)
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(description=__doc__,
                                 formatter_class=argparse.RawTextHelpFormatter)
 
     p.add_argument('in_tractogram',
@@ -45,63 +47,55 @@
     p.add_argument('--threshold', type=float, default=0.001,
                    help='Maximum distance between two points to be considered'
                         ' overlapping [%(default)s mm].')
     p.add_argument('--no_empty', action='store_true',
                    help='Do not save empty tractogram.')
 
     add_reference_arg(p)
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     # Equivalent of add_bbox_arg(p): always ignoring invalid streamlines for
     # this script.
     args.bbox_check = False
 
     assert_inputs_exist(parser, args.in_tractogram, args.reference)
     assert_outputs_exist(parser, args, args.out_tractogram)
 
     if args.threshold < 0:
         parser.error("Threshold must be positive.")
 
     sft = load_tractogram_with_reference(parser, args, args.in_tractogram)
     ori_len = len(sft)
+    ori_len_pts = len(sft.streamlines._data)
     if args.cut_invalid:
-        sft, cutting_counter = cut_invalid_streamlines(sft)
+        sft, cutting_counter = cut_invalid_streamlines(sft,
+                                                       epsilon=args.threshold)
         logging.warning('Cut {} invalid streamlines.'.format(cutting_counter))
     else:
         sft.remove_invalid_streamlines()
 
-    indices = []
     if args.remove_single_point:
-        # Will try to do a PR in Dipy
-        indices = [i for i in range(len(sft)) if len(sft.streamlines[i]) <= 1]
+        sft = remove_single_point_streamlines(sft)
 
     if args.remove_overlapping_points:
-        for i in np.setdiff1d(range(len(sft)), indices):
-            norm = np.linalg.norm(np.diff(sft.streamlines[i], axis=0),
-                                  axis=1)
-            if (norm < args.threshold).any():
-                indices.append(i)
-
-    indices = np.setdiff1d(range(len(sft)), indices).astype(np.uint32)
-    if len(indices):
-        new_sft = sft[indices]
-    else:
-        new_sft = StatefulTractogram.from_sft([], sft)
+        sft = remove_overlapping_points_streamlines(sft, args.threshold)
+        logging.warning("data_per_point will be discarded.")
+        logging.warning('Removed {} overlapping points from tractogram.'.format(
+            ori_len_pts - len(sft.streamlines._data)))
 
     logging.warning('Removed {} invalid streamlines.'.format(
-        ori_len - len(new_sft)))
+        ori_len - len(sft)))
 
-    if len(new_sft) > 0 or (not args.no_empty and len(new_sft) == 0):
-        save_tractogram(new_sft, args.out_tractogram)
-    else:
-        logging.warning('No valid streamline, not saving due to --no_empty.')
+    save_tractogram(sft, args.out_tractogram, args.no_empty)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_remove_labels.py` & `scilpy-2.0.0/scripts/scil_labels_remove.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,56 +1,59 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-    Script to remove specific labels from an atlas volume.
+Script to remove specific labels from an atlas volume.
 
-    >>> scil_remove_labels.py DKT_labels.nii out_labels.nii.gz -i 5001 5002
+    >>> scil_labels_remove.py DKT_labels.nii out_labels.nii.gz -i 5001 5002
+
+Formerly: scil_remove_labels.py
 """
 
 
 import argparse
 import logging
 
 import nibabel as nib
-import numpy as np
 
 from scilpy.image.labels import get_data_as_labels, remove_labels
 from scilpy.io.utils import (add_overwrite_arg, assert_inputs_exist,
-                             assert_outputs_exist)
+                             add_verbose_arg, assert_outputs_exist)
 EPILOG = """
     References:
         [1] Al-Sharif N.B., St-Onge E., Vogel J.W., Theaud G.,
             Evans A.C. and Descoteaux M. OHBM 2019.
             Surface integration for connectome analysis in age prediction.
     """
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(description=__doc__, epilog=EPILOG,
                                 formatter_class=argparse.RawTextHelpFormatter)
 
     p.add_argument('in_labels',
                    help='Input labels volume.')
-
     p.add_argument('out_labels',
                    help='Output labels volume.')
 
     p.add_argument('-i', '--indices', type=int, nargs='+', required=True,
                    help='List of labels indices to remove.')
-
     p.add_argument('--background', type=int, default=0,
                    help='Integer used for removed labels [%(default)s].')
+    
+    add_verbose_arg(p)
     add_overwrite_arg(p)
+    
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, args.in_labels)
     assert_outputs_exist(parser, args, args.out_labels)
 
     # Load volume
     label_img = nib.load(args.in_labels)
     labels_volume = get_data_as_labels(label_img)
```

### Comparing `scilpy-1.5.post2/scripts/scil_reorder_connectivity.py` & `scilpy-2.0.0/scripts/scil_connectivity_reorder_rois.py`

 * *Files 18% similar despite different names*

```diff
@@ -3,41 +3,44 @@
 
 """
 Re-order one or many connectivity matrices using a text file format.
 The first row are the (x) and the second row the (y), must be space separated.
 The resulting matrix does not have to be square (support unequal number of
 x and y).
 
-The values refers to the coordinates (starting at 0) in the matrix, but if the
---labels_list parameter is used, the values will refers to the label which will
+The values refer to the coordinates (starting at 0) in the matrix, but if the
+--labels_list parameter is used, the values will refer to the label which will
 be converted to the appropriate coordinates. This file must be the same as the
-one provided to the scil_decompose_connectivity.py
+one provided to the scil_tractogram_segment_bundles_for_connectivity.py.
 
 To subsequently use scil_visualize_connectivity.py with a lookup table, you
 must use a label-based reording json and use --labels_list.
 
 You can also use the Optimal Leaf Ordering(OLO) algorithm to transform a
 sparse matrix into an ordering that reduces the matrix bandwidth. The output
 file can then be re-used with --in_ordering. Only one input can be used with
 this option, we recommand an average streamline count or volume matrix.
+
+Formerly: scil_reorder_connectivity.py
 """
 
 import argparse
+import logging
 import os
 
 import numpy as np
 
-from scilpy.connectivity.utils import (compute_OLO,
-                                       parse_ordering,
-                                       apply_reordering)
+from scilpy.connectivity.connectivity_tools import (compute_olo,
+                                                    apply_reordering)
 from scilpy.io.utils import (add_overwrite_arg,
                              assert_inputs_exist,
                              load_matrix_in_any_format,
                              save_matrix_in_any_format,
                              assert_outputs_exist,
+                             add_verbose_arg,
                              assert_output_dirs_exist_and_empty)
 
 
 EPILOG = """
 [1] Rubinov, Mikail, and Olaf Sporns. "Complex network measures of brain
     connectivity: uses and interpretations." Neuroimage 52.3 (2010):
     1059-1069.
@@ -62,34 +65,56 @@
     p.add_argument('--out_dir',
                    help='Output directory for the re-ordered matrices.')
     p.add_argument('--labels_list',
                    help='List saved by the decomposition script,\n'
                         '--in_ordering must contain labels rather than '
                         'coordinates (.txt).')
 
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
+def parse_ordering(in_ordering_file, labels_list=None):
+    """
+    toDo. Docstring please.
+    """
+    with open(in_ordering_file, 'r') as my_file:
+        lines = my_file.readlines()
+        ordering = [[int(val) for val in lines[0].split()],
+                    [int(val) for val in lines[1].split()]]
+    if labels_list:
+        labels_list = np.loadtxt(labels_list,
+                                 dtype=np.int16).tolist()
+        # If the reordering file refers to labels and not indices
+        real_ordering = [[], []]
+        real_ordering[0] = [labels_list.index(i) for i in ordering[0]]
+        real_ordering[1] = [labels_list.index(i) for i in ordering[1]]
+        return real_ordering
+
+    return ordering
+
+
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, args.in_matrices,
                         [args.labels_list, args.in_ordering])
     assert_output_dirs_exist_and_empty(parser, args, [], args.out_dir)
 
     if args.optimal_leaf_ordering is not None:
         if len(args.in_matrices) > 1:
             parser.error('Only one input is supported with RCM.')
         assert_outputs_exist(parser, args, args.optimal_leaf_ordering)
 
         matrix = load_matrix_in_any_format(args.in_matrices[0])
-        perm = compute_OLO(matrix).astype(np.uint16)
+        perm = compute_olo(matrix).astype(np.uint16)
         np.savetxt(args.optimal_leaf_ordering, [perm.tolist(), perm.tolist()],
                    fmt='%i')
     else:
         # Verify all the possible outputs to avoid overwriting files
         if args.out_suffix is None:
             args.out_suffix = ""
```

### Comparing `scilpy-1.5.post2/scripts/scil_resample_streamlines.py` & `scilpy-2.0.0/scripts/scil_viz_tractogram_seeds.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,62 +1,82 @@
-#!/usr/bin/env python3
+#! /usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-Script to resample a set of streamlines to either a new number of points per
-streamline or to a fixed step size. WARNING: data_per_point is not carried.
+Visualize seeds used to generate the tractogram or bundle.
+When tractography was run, each streamline produced by the tracking algorithm
+saved its seeding point (its origin).
+
+The tractogram must have been generated from scil_tracking_local.py or
+scil_tracking_pft.py with the --save_seeds option.
 """
+
 import argparse
+import logging
 
-from dipy.io.streamline import save_tractogram
+from dipy.io.streamline import load_tractogram
+from fury import window, actor
+from nibabel.streamlines import detect_format, TrkFile
 
-from scilpy.io.streamlines import load_tractogram_with_reference
 from scilpy.io.utils import (add_overwrite_arg,
-                             add_reference_arg,
+                             add_verbose_arg,
                              assert_inputs_exist,
                              assert_outputs_exist)
-from scilpy.tracking.tools import (resample_streamlines_num_points,
-                                   resample_streamlines_step_size)
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
-        formatter_class=argparse.RawTextHelpFormatter, description=__doc__)
-
-    p.add_argument('in_tractogram',
-                   help='Streamlines input file name.')
-    p.add_argument('out_tractogram',
-                   help='Streamlines output file name.')
-
-    g = p.add_mutually_exclusive_group(required=True)
-    g.add_argument('--nb_pts_per_streamline', type=int,
-                   help='Number of points per streamline in the output.')
-    g.add_argument('--step_size', type=float,
-                   help='Step size in the output (in mm).')
+        description=__doc__,
+        formatter_class=argparse.RawTextHelpFormatter)
+    p.add_argument('tractogram',
+                   help='Tractogram file (must be trk)')
+    p.add_argument('--save',
+                   help='If set, save a screenshot of the result in the '
+                        'specified filename')
 
-    add_reference_arg(p)
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def main():
-
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, args.in_tractogram)
-    assert_outputs_exist(parser, args, args.out_tractogram)
-
-    sft = load_tractogram_with_reference(parser, args, args.in_tractogram)
-
-    if args.nb_pts_per_streamline:
-        new_sft = resample_streamlines_num_points(sft,
-                                                  args.nb_pts_per_streamline)
-    else:
-        new_sft = resample_streamlines_step_size(sft, args.step_size)
+    assert_inputs_exist(parser, [args.tractogram])
+    assert_outputs_exist(parser, args, [], [args.save])
 
-    save_tractogram(new_sft, args.out_tractogram)
+    tracts_format = detect_format(args.tractogram)
+    if tracts_format is not TrkFile:
+        raise ValueError("Invalid input streamline file format " +
+                         "(must be trk): {0}".format(args.tractogram_filename))
+
+    # Load files and data. TRKs can have 'same' as reference
+    tractogram = load_tractogram(args.tractogram, 'same')
+    # Streamlines are saved in RASMM but seeds are saved in VOX
+    # This might produce weird behavior with non-iso
+    tractogram.to_vox()
+
+    streamlines = tractogram.streamlines
+    if 'seeds' not in tractogram.data_per_streamline:
+        parser.error('Tractogram does not contain seeds')
+    seeds = tractogram.data_per_streamline['seeds']
+
+    # Make display objects
+    streamlines_actor = actor.line(streamlines)
+    points = actor.dot(seeds, color=(1., 1., 1.))
+
+    # Add display objects to canvas
+    s = window.Scene()
+    s.add(streamlines_actor)
+    s.add(points)
+
+    # Show and record if needed
+    if args.save is not None:
+        window.record(s, out_path=args.save, size=(1000, 1000))
+    window.show(s)
 
 
-if __name__ == "__main__":
+if __name__ == '__main__':
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_resample_tractogram.py` & `scilpy-2.0.0/scripts/scil_tractogram_project_map_to_streamlines.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,168 +1,163 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-Script to resample a tractogram to a set number of streamlines.
-Default behavior:
-- IF number of requested streamlines is lower than streamline count: DOWNSAMPLE
-- IF number of requested streamlines is higher than streamline count: UPSAMPLE
-To prevent upsample if not desired use --never_upsample.
-
-Can be useful to build training sets for machine learning algorithms, to
-upsample under-represented bundles or downsample over-represented bundles.
-
-Works by either selecting a subset of streamlines or by generating new
-streamlines by adding gaussian noise to existing ones.
-
-Upsampling:
-    Includes smoothing to compensate for the noisiness of new streamlines
-    generated by the process.
-Downsampling:
-    Includes the possibility of choosing randomly *per Quickbundle cluster* to
-    ensure that all clusters are represented in the final tractogram.
-
-Example usage:
-$ scil_resample_tractogram.py input.trk 1000 output.trk \
---point_wise_std 0.5 --spline 5 10 --keep_invalid_streamlines
-$ scil_visualize_bundles.py output.trk --local_coloring --width=0.1
+Projects maps extracted from a map onto the points of streamlines.
+
+The default options will take data from a nifti image (3D or 4D) and
+project it onto the points of streamlines. If the image is 4D, the data
+is stored as a list of 1D arrays per streamline. If the image is 3D,
+the data is stored as a list of values per streamline.
+
+See also scil_tractogram_project_streamlines_to_map.py for the reverse action.
+
+* Note that the data from your maps will be projected only on the coordinates
+of the points of your streamlines. Data underlying the whole segments between
+two consecutive points is not used. If your streamlines are strongly
+compressed, or if they have a very big step size, the result will possibly
+reflect poorly your map. You may use scil_tractogram_resample.py to upsample
+your streamlines first.
+* Hint: The streamlines themselves are not modified here, only their dpp. To
+avoid multiplying data on disk, you could use the following arguments to save
+the new dpp in your current tractogram:
+>> scil_tractogram_project_map_to_streamlines.py $in_bundle $in_bundle
+       --keep_all_dpp -f
 """
 
 import argparse
 import logging
 
-from dipy.io.stateful_tractogram import set_sft_logger_level
+import nibabel as nib
 from dipy.io.streamline import save_tractogram
 
 from scilpy.io.streamlines import load_tractogram_with_reference
-from scilpy.io.utils import (add_overwrite_arg, add_reference_arg,
+from scilpy.io.utils import (add_overwrite_arg,
+                             add_reference_arg,
                              add_verbose_arg,
                              assert_inputs_exist,
                              assert_outputs_exist)
-from scilpy.tractograms.tractogram_operations import (
-    split_sft_randomly,
-    split_sft_randomly_per_cluster,
-    upsample_tractogram)
+from scilpy.image.volume_space_management import DataVolume
+from scilpy.tractograms.dps_and_dpp_management import (
+    project_map_to_streamlines)
 
 
 def _build_arg_parser():
-    p = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter,
-                                description=__doc__)
+    p = argparse.ArgumentParser(
+        description=__doc__,
+        formatter_class=argparse.RawTextHelpFormatter)
 
+    # Mandatory arguments input and output tractogram must be in trk format
     p.add_argument('in_tractogram',
-                   help='Input tractography file.')
-    p.add_argument('nb_streamlines', type=int,
-                   help='Number of streamlines to resample the tractogram to.')
+                   help='Fiber bundle file.')
     p.add_argument('out_tractogram',
-                   help='Output tractography file.')
+                   help='Output file.')
+    p.add_argument('--in_maps', nargs='+', required=True,
+                   help='Nifti map to project onto streamlines.')
+    p.add_argument('--out_dpp_name', nargs='+', required=True,
+                   help='Name of the data_per_point to be saved in the \n'
+                   'output tractogram.')
+
+    # Optional arguments
+    p.add_argument('--trilinear', action='store_true',
+                   help='If set, will use trilinear interpolation \n'
+                        'else will use nearest neighbor interpolation \n'
+                        'by default.')
+    p.add_argument('--endpoints_only', action='store_true',
+                   help='If set, will only project the map onto the \n'
+                   'endpoints of the streamlines (all other values along \n'
+                   'streamlines will be NaN). If not set, will project \n'
+                   'the map onto all points of the streamlines.')
+
+    p.add_argument('--keep_all_dpp', action='store_true',
+                   help='If set, previous data_per_point will be preserved \n'
+                   'in the output tractogram. Else, only --out_dpp_name \n'
+                   'keys will be saved.')
+    p.add_argument('--overwrite_dpp', action='store_true',
+                   help='If set, if --keep_all_dpp is set and some \n'
+                   '--out_dpp_name keys already existed in your \n'
+                   'data_per_point, allow overwriting old data_per_point.')
 
-    p.add_argument('--never_upsample', action='store_true',
-                   help='Make sure to never upsample a tractogram.\n'
-                        'Useful when downsample batch of files using bash.')
-
-    # For upsampling:
-    upsampling_group = p.add_argument_group('Upsampling params')
-    std_group = upsampling_group.add_mutually_exclusive_group()
-    std_group.add_argument('--point_wise_std', type=float,
-                           help='Noise to add to existing streamlines\'' +
-                                ' points to generate new ones.')
-    std_group.add_argument('--streamline_wise_std', type=float,
-                           help='Noise to add to existing whole' +
-                                ' streamlines to generate new ones.')
-    sub_p = upsampling_group.add_mutually_exclusive_group()
-    sub_p.add_argument('--gaussian', metavar='SIGMA', type=int,
-                       help='Sigma for smoothing. Use the value of surronding'
-                            ' X,Y,Z points on \nthe streamline to blur the'
-                            ' streamlines. A good sigma choice would \nbe '
-                            'around 5.')
-    sub_p.add_argument('--spline', nargs=2, metavar=('SIGMA', 'NB_CTRL_POINT'),
-                       type=int,
-                       help='Sigma and number of points for smoothing. Models '
-                            'each streamline \nas a spline. A good sigma '
-                            'choice would be around 5 and control \npoints '
-                            'around 10.')
-
-    upsampling_group.add_argument(
-        '--keep_invalid_streamlines', action='store_true',
-        help='Keep invalid newly generated streamlines that may '
-             'go out of the \nbounding box.')
-
-    # For downsampling:
-    downsampling_group = p.add_argument_group('Downsampling params')
-    downsampling_group.add_argument(
-        '--downsample_per_cluster', action='store_true',
-        help='If set, downsampling will be done per cluster (computed with \n'
-             'Quickbundles) to ensure that at least some streamlines are \n'
-             'kept per bundle. Else, random downsampling is performed '
-             '(default).')
-    downsampling_group.add_argument(
-        '--qbx_thresholds', nargs='+', type=float, default=[40, 30, 20],
-        metavar='t',
-        help="If you chose option '--downsample_per_cluster', you may set \n"
-             "the QBx threshold value(s) here. Default: %(default)s")
-
-    # General
-    p.add_argument('--seed', default=None, type=int,
-                   help='Use a specific random seed for the resampling.')
     add_reference_arg(p)
     add_overwrite_arg(p)
     add_verbose_arg(p)
-
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
 
-    if (args.point_wise_std is not None and args.point_wise_std <= 0) or \
-            (args.streamline_wise_std is not None and
-             args.streamline_wise_std <= 0):
-        parser.error('STD needs to be above 0.')
-
-    assert_inputs_exist(parser, args.in_tractogram)
-    assert_outputs_exist(parser, args, args.out_tractogram)
+    assert_inputs_exist(parser, [args.in_tractogram] + args.in_maps,
+                        args.reference)
+    assert_outputs_exist(parser, args, [args.out_tractogram])
 
-    log_level = logging.WARNING
     if args.verbose:
-        log_level = logging.DEBUG
-        set_sft_logger_level('INFO')
-    logging.getLogger().setLevel(log_level)
+        logging.getLogger().setLevel(logging.INFO)
 
-    logging.debug("Loading sft.")
+    logging.info("Loading the tractogram...")
     sft = load_tractogram_with_reference(parser, args, args.in_tractogram)
-    original_number = len(sft.streamlines)
+    sft.to_voxmm()
+    sft.to_corner()
 
-    if args.never_upsample and args.nb_streamlines > original_number:
-        args.nb_streamlines = original_number
+    if len(sft.streamlines) == 0:
+        logging.warning('Empty bundle file {}. Skipping'.format(
+            args.in_tractogram))
+        return
+
+    # Check to see if the number of maps and dpp_names are the same
+    if len(args.in_maps) != len(args.out_dpp_name):
+        parser.error('The number of maps and dpp_names must be the same.')
+
+    # Check to see if there are duplicates in the out_dpp_names
+    if len(args.out_dpp_name) != len(set(args.out_dpp_name)):
+        parser.error('The output names (out_dpp_names) must be unique.')
+
+    # Check to see if the output names already exist in the input tractogram
+    if not args.overwrite_dpp:
+        for out_dpp_name in args.out_dpp_name:
+            if out_dpp_name in sft.data_per_point:
+                logging.info('out_name {} already exists in input tractogram. '
+                             'Set overwrite_data or choose a different '
+                             'out_name. Exiting.'.format(out_dpp_name))
+                return
+
+    data_per_point = {}
+    for fmap, dpp_name in zip(args.in_maps, args.out_dpp_name):
+        logging.info("Loading the map...")
+        map_img = nib.load(fmap)
+        map_data = map_img.get_fdata(caching='unchanged', dtype=float)
+        map_res = map_img.header.get_zooms()[:3]
 
-    logging.debug("Done. Now getting {} streamlines."
-                  .format(args.nb_streamlines))
-
-    if args.nb_streamlines > original_number:
-        # Check is done here because it is not required if downsampling
-        if not args.point_wise_std and not args.streamline_wise_std:
-            parser.error("one of the arguments --point_wise_std " +
-                         "--streamline_wise_std is required")
-        sft = upsample_tractogram(
-            sft, args.nb_streamlines,
-            args.point_wise_std, args.streamline_wise_std,
-            args.gaussian, args.spline, args.seed)
-    elif args.nb_streamlines < original_number:
-        if args.downsample_per_cluster:
-            # output contains rejected streamlines, we don't use them.
-            sft, _ = split_sft_randomly_per_cluster(
-                sft, [args.nb_streamlines], args.seed, args.qbx_thresholds)
-            logging.debug("Kept {} out of {} expected streamlines."
-                          .format(len(sft), args.nb_streamlines))
+        if args.trilinear:
+            interp = "trilinear"
         else:
-            # output is a list of two: kept and rejected.
-            sft = split_sft_randomly(sft, args.nb_streamlines, args.seed)[0]
+            interp = "nearest"
+
+        map_volume = DataVolume(map_data, map_res, interp)
+
+        logging.info("Projecting map onto streamlines")
+        streamline_data = project_map_to_streamlines(
+            sft, map_volume,
+            endpoints_only=args.endpoints_only)
+
+        logging.info("Saving the tractogram...")
+
+        data_per_point[dpp_name] = streamline_data
+
+    if args.keep_all_dpp:
+        sft.data_per_point.update(data_per_point)
+        out_sft = sft
+    else:
+        out_sft = sft.from_sft(sft.streamlines, sft,
+                               data_per_point=data_per_point)
+
+    print("New data_per_point keys are: ")
+    for key in args.out_dpp_name:
+        print("  - {} with shape per point {}"
+              .format(key, out_sft.data_per_point[key][0].shape[1:]))
 
-    if not args.keep_invalid_streamlines:
-        sft.remove_invalid_streamlines()
-    save_tractogram(sft, args.out_tractogram,
-                    bbox_valid_check=not args.keep_invalid_streamlines)
+    save_tractogram(out_sft, args.out_tractogram)
 
 
-if __name__ == "__main__":
+if __name__ == '__main__':
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_resample_volume.py` & `scilpy-2.0.0/scripts/scil_volume_resample.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,23 +1,26 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
 Script to resample a dataset to match the resolution of another
 reference dataset or to the resolution specified as in argument.
+
+Formerly: scil_resample_volume.py
 """
 
 import argparse
 import logging
 
 import nibabel as nib
 
 from scilpy.io.utils import (add_verbose_arg, add_overwrite_arg,
-                             assert_inputs_exist, assert_outputs_exist)
-from scilpy.image.resample_volume import resample_volume
+                             assert_inputs_exist, assert_outputs_exist,
+                             assert_headers_compatible)
+from scilpy.image.volume_operations import resample_volume
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__, formatter_class=argparse.RawTextHelpFormatter)
 
     p.add_argument('in_image',
@@ -55,42 +58,42 @@
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     # Checking args
     assert_inputs_exist(parser, args.in_image, args.ref)
     assert_outputs_exist(parser, args, args.out_image)
+    assert_headers_compatible(parser, args.in_image, args.ref)
+
     if args.enforce_dimensions and not args.ref:
         parser.error("Cannot enforce dimensions without a reference image")
 
-    if args.verbose:
-        logging.getLogger().setLevel(logging.DEBUG)
-
     if args.volume_size and (not len(args.volume_size) == 1 and
                              not len(args.volume_size) == 3):
         parser.error('Invalid dimensions for --volume_size.')
 
     if args.voxel_size and (not len(args.voxel_size) == 1 and
                             not len(args.voxel_size) == 3):
         parser.error('Invalid dimensions for --voxel_size.')
 
-    logging.debug('Loading raw data from %s', args.in_image)
+    logging.info('Loading raw data from %s', args.in_image)
 
     img = nib.load(args.in_image)
 
     # Resampling volume
     resampled_img = resample_volume(img, ref=args.ref, res=args.volume_size,
                                     iso_min=args.iso_min, zoom=args.voxel_size,
                                     interp=args.interp,
                                     enforce_dimensions=args.enforce_dimensions)
 
     # Saving results
-    logging.debug('Saving resampled data to %s', args.out_image)
+    logging.info('Saving resampled data to %s', args.out_image)
     nib.save(resampled_img, args.out_image)
 
 
 if __name__ == '__main__':
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_reshape_to_reference.py` & `scilpy-2.0.0/scripts/scil_volume_reshape_to_reference.py`

 * *Files 21% similar despite different names*

```diff
@@ -2,26 +2,29 @@
 # -*- coding: utf-8 -*-
 
 """
 Reshape / reslice / resample *.nii or *.nii.gz using a reference.
 This script can be used to align freesurfer/civet output, as .mgz,
 to the original input image.
 
-
->>> scil_reshape_to_reference.py wmparc.mgz t1.nii.gz wmparc_t1.nii.gz \\
+>>> scil_volume_reshape_to_reference.py wmparc.mgz t1.nii.gz wmparc_t1.nii.gz\\
     --interpolation nearest
+
+Formerly: scil_reshape_to_reference.py
 """
 
 import argparse
+import logging
 
+import nibabel as nib
 import numpy as np
 
+from scilpy.image.volume_operations import apply_transform
 from scilpy.io.utils import (add_overwrite_arg, assert_inputs_exist,
-                             assert_outputs_exist)
-from scilpy.utils.image import transform_anatomy
+                             add_verbose_arg, assert_outputs_exist)
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(description=__doc__,
                                 formatter_class=argparse.RawTextHelpFormatter)
 
     p.add_argument('in_file',
@@ -35,25 +38,34 @@
                    choices=['linear', 'nearest'],
                    help='Interpolation: "linear" or "nearest". [%(default)s]')
 
     p.add_argument('--keep_dtype', action='store_true',
                    help='If True, keeps the data_type of the input image '
                         '(in_file) when saving the output image (out_file).')
 
+    add_verbose_arg(p)
     add_overwrite_arg(p)
+
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, [args.in_file, args.in_ref_file])
     assert_outputs_exist(parser, args, args.out_file)
 
-    transform_anatomy(np.eye(4), args.in_ref_file, args.in_file,
-                      args.out_file, interp=args.interpolation,
-                      keep_dtype=args.keep_dtype)
+    # Load images.
+    in_file = nib.load(args.in_file)
+    ref_file = nib.load(args.in_ref_file)
+
+    reshaped_img = apply_transform(np.eye(4), ref_file, in_file,
+                                   interp=args.interpolation,
+                                   keep_dtype=args.keep_dtype)
+
+    nib.save(reshaped_img, args.out_file)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_run_commit.py` & `scilpy-2.0.0/scripts/scil_tractogram_commit.py`

 * *Files 7% similar despite different names*

```diff
@@ -18,80 +18,98 @@
 The output from COMMIT is:
 - fit_NRMSE.nii.gz
     fiting error (Normalized Root Mean Square Error)
 - fit_RMSE.nii.gz
     fiting error (Root Mean Square Error)
 - results.pickle
     Dictionary containing the experiment parameters and final weights
-- compartment_EC.nii.gz (est. Extra-Cellular signal fraction)
-- compartment_IC.nii.gz (est. Intra-Cellular signal fraction)
-- compartment_ISO.nii.gz (est. isotropic signal fraction (freewater comportment))
+- compartment_EC.nii.gz
+    (est. Extra-Cellular signal fraction)
+- compartment_IC.nii.gz
+    (est. Intra-Cellular signal fraction)
+- compartment_ISO.nii.gz
+    (est. isotropic signal fraction (freewater comportment)):
     Each of COMMIT compartments
 - streamline_weights.txt
     Text file containing the commit weights for each streamline of the
     input tractogram.
 - streamlines_length.txt
-    Text file containing the length (mm) of each streamline
+    Text file containing the length (mm) of each streamline.
+- streamline_weights_by_length.txt
+    Text file containing the commit weights for each streamline of the
+    input tractogram, ordered by their length.
 - tot_streamline_weights
     Text file containing the total commit weights of each streamline.
     Equal to commit_weights * streamlines_length (W_i * L_i)
 - essential.trk / non_essential.trk
     Tractograms containing the streamlines below or equal (essential) and
     above (non_essential) a threshold_weights of 0.
+- decompose_commit.h5
+    In the case where the input is a hdf5 file only, we will save an output
+    hdf5 with the following information separated into each bundle's dps:
+    - streamlines_weights
+    - streamline_weights_by_length
+    For each bundle, only the essential streamlines are kept.
 
 This script can divide the input tractogram in two using a threshold to apply
-on the streamlines' weight. The threshold used is 0.0, keeping only
-streamlines that have non-zero weight and that contribute to explain the DWI
-signal. Streamlines with 0 weight are essentially not necessary according to
-COMMIT.
+on the streamlines' weight. The threshold used is 0.0, keeping only streamlines
+that have non-zero weight and that contribute to explain the DWI signal.
+Streamlines with 0 weight are essentially not necessary according to COMMIT.
 
-COMMIT2 is available only for HDF5 data from scil_decompose_connectivity.py and
+COMMIT2 is available only for HDF5 data from
+scil_tractogram_segment_bundles_for_connectivity.py and
 with the --ball_stick option. Use the --commit2 option to activite it, slightly
-longer computation time. This wrapper offers a simplify way to call COMMIT, but
-does not allow to use (or fine-tune) every parameters. If you want to use COMMIT
-with full access to all parameters, visit: https://github.com/daducci/COMMIT
+longer computation time. This wrapper offers a simplify way to call COMMIT,
+but does not allow to use (or fine-tune) every parameter. If you want to use
+COMMIT with full access to all parameters,
+visit: https://github.com/daducci/COMMIT
 
 When tunning parameters, such as --iso_diff, --para_diff, --perp_diff or
 --lambda_commit_2 you should evaluate the quality of results by:
     - Looking at the 'density' (GTM) of the connnectome (essential tractogram)
     - Confirm the quality of WM bundles reconstruction (essential tractogram)
     - Inspect the (N)RMSE map and look for peaks or anomalies
     - Compare the density map before and after (essential tractogram)
+
+Formerly: scil_run_commit.py
 """
 
 import argparse
 from contextlib import redirect_stdout
 import io
 import logging
 import os
 import shutil
 import sys
 import tempfile
 
 import commit
 from commit import trk2dictionary
-from dipy.io.stateful_tractogram import (Origin, Space,
-                                         StatefulTractogram)
+
 from dipy.io.streamline import save_tractogram, load_tractogram
-from dipy.io.utils import is_header_compatible
 from dipy.io.gradients import read_bvals_bvecs
 from dipy.tracking.streamlinespeed import length
 import h5py
 import numpy as np
 import nibabel as nib
 
-from scilpy.io.streamlines import (reconstruct_streamlines,
-                                   reconstruct_streamlines_from_hdf5)
+from scilpy.io.gradients import fsl2mrtrix
+from scilpy.io.hdf5 import (reconstruct_sft_from_hdf5,
+                            construct_hdf5_group_from_streamlines,
+                            construct_hdf5_header)
+from scilpy.io.streamlines import reconstruct_streamlines
 from scilpy.io.utils import (add_overwrite_arg,
                              add_processes_arg,
                              add_verbose_arg,
                              assert_inputs_exist,
-                             assert_output_dirs_exist_and_empty)
-from scilpy.utils.bvec_bval_tools import fsl2mrtrix, identify_shells
-
+                             assert_output_dirs_exist_and_empty,
+                             redirect_stdout_c, add_tolerance_arg,
+                             add_skip_b0_check_arg, assert_headers_compatible)
+from scilpy.gradients.bvec_bval_tools import identify_shells, \
+    check_b0_threshold
 
 EPILOG = """
 References:
 [1] Daducci, Alessandro, et al. "COMMIT: convex optimization modeling for
     microstructure informed tractography." IEEE transactions on medical
     imaging 34.1 (2014): 246-257.
 [2] Schiavi, Simona, et al. "A new method for accurate in vivo mapping of
@@ -103,58 +121,55 @@
 def _build_arg_parser():
     p = argparse.ArgumentParser(description=__doc__, epilog=EPILOG,
                                 formatter_class=argparse.RawTextHelpFormatter)
 
     p.add_argument('in_tractogram',
                    help='Input tractogram (.trk or .tck or .h5).')
     p.add_argument('in_dwi',
-                   help='Diffusion-weighted images used by COMMIT (.nii.gz).')
+                   help='Diffusion-weighted image used by COMMIT (.nii.gz).')
     p.add_argument('in_bval',
                    help='b-values in the FSL format (.bval).')
     p.add_argument('in_bvec',
                    help='b-vectors in the FSL format (.bvec).')
     p.add_argument('out_dir',
                    help='Output directory for the COMMIT maps.')
 
-    p.add_argument('--b_thr', type=int, default=40,
-                   help='Limit value to consider that a b-value is on an '
-                        'existing shell.\nAbove this limit, the b-value is '
-                        'placed on a new shell. This includes b0s values.')
     p.add_argument('--nbr_dir', type=int, default=500,
                    help='Number of directions, on the half of the sphere,\n'
                         'representing the possible orientations of the '
                         'response functions [%(default)s].')
     p.add_argument('--nbr_iter', type=int, default=1000,
                    help='Maximum number of iterations [%(default)s].')
     p.add_argument('--in_peaks',
                    help='Peaks file representing principal direction(s) '
-                        'locally,\n typically coming from fODFs. This file is '
-                        'mandatory for the default\n stick-zeppelin-ball '
+                        'locally,\ntypically coming from fODFs. This file is '
+                        'mandatory for the default \nstick-zeppelin-ball '
                         'model.')
     p.add_argument('--in_tracking_mask',
                    help='Binary mask where tratography was allowed.\n'
                         'If not set, uses a binary mask computed from '
                         'the streamlines.')
 
     g0 = p.add_argument_group(title='COMMIT2 options')
     g0.add_argument('--commit2', action='store_true',
                     help='Run commit2, requires .h5 as input and will force\n'
                          'ball&stick model.')
     g0.add_argument('--lambda_commit_2', type=float, default=1e-3,
-                    help='Specify the clustering prior strength [%(default)s].')
+                    help='Specify the clustering prior strength '
+                         '[%(default)s].')
 
     g1 = p.add_argument_group(title='Model options')
     g1.add_argument('--ball_stick', action='store_true',
                     help='Use the ball&Stick model, disable the zeppelin '
                          'compartment.\nOnly model suitable for single-shell '
                          'data.')
-    g1.add_argument('--para_diff', type=float,
+    g1.add_argument('--para_diff', type=float, default=1.7E-3,
                     help='Parallel diffusivity in mm^2/s.\n'
-                         'Default for ball_stick: 1.7E-3\n'
-                         'Default for stick_zeppelin_ball: 1.7E-3')
+                         'Default for both ball_stick and '
+                         'stick_zeppelin_ball: 1.7E-3.')
     g1.add_argument('--perp_diff', nargs='+', type=float,
                     help='Perpendicular diffusivity in mm^2/s.\n'
                          'Default for ball_stick: None\n'
                          'Default for stick_zeppelin_ball: [0.51E-3]')
     g1.add_argument('--iso_diff', nargs='+', type=float,
                     help='Istropic diffusivity in mm^2/s.\n'
                          'Default for ball_stick: [2.0E-3]\n'
@@ -170,272 +185,262 @@
     kern.add_argument('--save_kernels', metavar='DIRECTORY',
                       help='Output directory for the COMMIT kernels.')
     kern.add_argument('--load_kernels', metavar='DIRECTORY',
                       help='Input directory where the COMMIT kernels are '
                            'located.')
     g2.add_argument('--compute_only', action='store_true',
                     help='Compute kernels only, --save_kernels must be used.')
+
+    add_tolerance_arg(p)
+    add_skip_b0_check_arg(p, will_overwrite_with_min=True)
     add_processes_arg(p)
-    add_overwrite_arg(p)
     add_verbose_arg(p)
+    add_overwrite_arg(p)
 
     return p
 
 
-def redirect_stdout_c():
-    sys.stdout.flush()
-    newstdout = os.dup(1)
-    devnull = os.open(os.devnull, os.O_WRONLY)
-    os.dup2(devnull, 1)
-    os.close(devnull)
-    sys.stdout = os.fdopen(newstdout, 'w')
+def _save_tmp_tractogram_from_hdf5(tmp_dir, args, dwi_img):
+    logging.info('Reconstructing {} into a tractogram for COMMIT.'
+                 .format(args.in_tractogram))
+
+    # Keep track of the order of connections/streamlines in relation to the
+    # tractogram as well as the number of streamlines for each connection.
+    with h5py.File(args.in_tractogram, 'r') as hdf5_file:
+        sft, bundle_groups_len = reconstruct_sft_from_hdf5(
+            hdf5_file, group_keys=None, ref_img=dwi_img, merge_groups=True)
+
+    offsets_list = np.cumsum([0] + bundle_groups_len)
+    tmp_tractogram_filename = os.path.join(tmp_dir.name, 'tractogram.trk')
 
+    # Keeping the input variable, saving trk file for COMMIT internal use
+    save_tractogram(sft, tmp_tractogram_filename)
+    args.in_tractogram = tmp_tractogram_filename
 
-def _save_results_wrapper(args, tmp_dir, ext, hdf5_file, offsets_list,
-                          sub_dir, is_commit_2):
+    return hdf5_file, offsets_list, bundle_groups_len
+
+
+def _save_results(args, tmp_dir, ext, in_hdf5_file, offsets_list, sub_dir,
+                  is_commit_2):
+    # Will convert results saved by commit in:
+    commit_results_dir = os.path.join(tmp_dir.name,
+                                      'Results_StickZeppelinBall')
+
+    # Create the output dir.
     out_dir = os.path.join(args.out_dir, sub_dir)
     os.mkdir(out_dir)
+
     # Simplifying output for streamlines and cleaning output directory
-    commit_results_dir = os.path.join(tmp_dir.name,
-                                      'Results_StickZeppelinBall')
     streamline_weights = np.loadtxt(os.path.join(commit_results_dir,
                                                  'streamline_weights.txt'))
 
+    # Loading the tractogram (we never did yet! Only sent the filename to
+    # commit). Reminder. If input was a hdf5, we have changed
+    # args.in_tractogram to our tmp_tractogram saved in tmp_dir.
     sft = load_tractogram(args.in_tractogram, 'same')
     length_list = length(sft.streamlines)
     np.savetxt(os.path.join(commit_results_dir, 'streamlines_length.txt'),
                length_list)
     np.savetxt(os.path.join(commit_results_dir,
                             'streamline_weights_by_length.txt'),
-               streamline_weights*length_list)
-
-    if ext == '.h5':
-        new_filename = os.path.join(commit_results_dir,
-                                    'decompose_commit.h5')
-        with h5py.File(new_filename, 'w') as new_hdf5_file:
-            new_hdf5_file.attrs['affine'] = sft.affine
-            new_hdf5_file.attrs['dimensions'] = sft.dimensions
-            new_hdf5_file.attrs['voxel_sizes'] = sft.voxel_sizes
-            new_hdf5_file.attrs['voxel_order'] = sft.voxel_order
-            # Assign the weights into the hdf5, while respecting the ordering of
-            # connections/streamlines
-            logging.debug('Adding commit weights to {}.'.format(new_filename))
-            for i, key in enumerate(list(hdf5_file.keys())):
-                new_group = new_hdf5_file.create_group(key)
-                old_group = hdf5_file[key]
-                tmp_streamline_weights = \
-                    streamline_weights[offsets_list[i]:offsets_list[i+1]]
-
-                essential_ind = np.where(tmp_streamline_weights > 0)[0]
-                tmp_streamline_weights = tmp_streamline_weights[essential_ind]
-
-                tmp_streamlines = reconstruct_streamlines(old_group['data'],
-                                                          old_group['offsets'],
-                                                          old_group['lengths'],
-                                                          indices=essential_ind)
-                tmp_length_list = length(tmp_streamlines)
-                # Replacing the data with the one above the threshold
-                # Safe since this hdf5 was a copy in the first place
-                new_group.create_dataset('data',
-                                         data=tmp_streamlines.get_data(),
-                                         dtype=np.float32)
-                new_group.create_dataset('offsets',
-                                         data=tmp_streamlines._offsets,
-                                         dtype=np.int64)
-                new_group.create_dataset('lengths',
-                                         data=tmp_streamlines._lengths,
-                                         dtype=np.int32)
-
-                for dps_key in hdf5_file[key].keys():
-                    if dps_key not in ['data', 'offsets', 'lengths']:
-                        new_group.create_dataset(
-                            key, data=hdf5_file[key][dps_key][essential_ind])
-
-                dps_key = 'commit2_weights' if is_commit_2 else \
-                    'commit1_weights'
-                dps_key_tot = 'tot_commit2_weights' if is_commit_2 else \
-                    'tot_commit1_weights'
-                new_group.create_dataset(dps_key,
-                                         data=tmp_streamline_weights)
-                new_group.create_dataset(dps_key_tot,
-                                         data=tmp_streamline_weights*tmp_length_list)
+               streamline_weights * length_list)
 
     files = os.listdir(commit_results_dir)
     for f in files:
         shutil.copy(os.path.join(commit_results_dir, f), out_dir)
 
-    dps_key = 'commit2_weights' if is_commit_2 else \
-        'commit1_weights'
+    dps_key = 'commit2_weights' if is_commit_2 else 'commit1_weights'
     dps_key_tot = 'tot_commit2_weights' if is_commit_2 else \
         'tot_commit1_weights'
     # Reload is needed because of COMMIT handling its file by itself
     sft.data_per_streamline[dps_key] = streamline_weights
-    sft.data_per_streamline[dps_key_tot] = streamline_weights*length_list
+    sft.data_per_streamline[dps_key_tot] = streamline_weights * length_list
 
-    essential_ind = np.where(streamline_weights > 0)[0]
-    nonessential_ind = np.where(streamline_weights <= 0)[0]
-    logging.debug('{} essential streamlines were kept at'.format(
-        len(essential_ind)))
-    logging.debug('{} nonessential streamlines were kept'.format(
-        len(nonessential_ind)))
-
-    save_tractogram(sft[essential_ind],
-                    os.path.join(out_dir,
-                    'essential_tractogram.trk'))
-    save_tractogram(sft[nonessential_ind],
-                    os.path.join(out_dir,
-                    'nonessential_tractogram.trk'))
     if args.keep_whole_tractogram:
         output_filename = os.path.join(out_dir, 'tractogram.trk')
-        logging.debug('Saving tractogram with weights as {}'.format(
+        logging.info('Saving tractogram with weights as {}'.format(
             output_filename))
         save_tractogram(sft, output_filename)
 
+    essential_ind = np.where(streamline_weights > 0)[0]
+    nonessential_ind = np.where(streamline_weights <= 0)[0]
+    logging.info('Tractogram separated into {} essential streamlines and '
+                 '{} nonessential streamlines'
+                 .format(len(essential_ind), len(nonessential_ind)))
+    save_tractogram(sft[essential_ind],
+                    os.path.join(out_dir, 'essential_tractogram.trk'))
+    save_tractogram(sft[nonessential_ind],
+                    os.path.join(out_dir, 'nonessential_tractogram.trk'))
+
+    if ext == '.h5':
+        _save_out_hdf5(commit_results_dir, sft, in_hdf5_file,
+                       streamline_weights, offsets_list, is_commit_2)
+
+
+def _save_out_hdf5(commit_results_dir, sft, in_hdf5_file,
+                   streamline_weights, offsets_list, is_commit_2):
+    new_filename = os.path.join(commit_results_dir, 'decompose_commit.h5')
+    with h5py.File(new_filename, 'w') as out_hdf5_file:
+        construct_hdf5_header(out_hdf5_file, sft)
+
+        # Assign the weights into the hdf5, while respecting
+        # the ordering of connections/streamlines
+        logging.info('Adding commit weights to {}.'.format(new_filename))
+        for i, key in enumerate(list(in_hdf5_file.keys())):
+            new_group = out_hdf5_file.create_group(key)
+            old_group = in_hdf5_file[key]
+
+            # Recomputing again essential streamlines, but only for this
+            # bundle.
+            tmp_streamline_weights = \
+                streamline_weights[offsets_list[i]:offsets_list[i + 1]]
+            essential_ind = np.where(tmp_streamline_weights > 0)[0]
+            tmp_streamline_weights = tmp_streamline_weights[essential_ind]
+
+            # toDo. Could we use bundle_groups_len to recreate from sft?
+            #  Would not need to keep in_hdf5_file in memory.
+            #  Would not need to reset again the DPS
+            tmp_streamlines = reconstruct_streamlines(
+                old_group['data'], old_group['offsets'],
+                old_group['lengths'], indices=essential_ind)
+            tmp_length_list = length(tmp_streamlines)
+            dps = {key: value[essential_ind]
+                   for key, value in in_hdf5_file[key].items()
+                   if key not in ['data', 'offsets', 'lengths']}
+
+            # Adding commit values as dps
+            dps_commit_key = 'commit2_weights' if is_commit_2 else \
+                'commit1_weights'
+            dps[dps_commit_key] = tmp_streamline_weights
+            dps_key_tot = 'tot_commit2_weights' if is_commit_2 else \
+                'tot_commit1_weights'
+            dps[dps_key_tot] = tmp_streamline_weights * tmp_length_list
+
+            # Replacing the data with the one above the threshold
+            # Safe since this hdf5 was a copy in the first place
+            construct_hdf5_group_from_streamlines(
+                new_group, tmp_streamlines, dps=dps, dpp=None)
+
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    # COMMIT has some c-level stdout and non-logging print that cannot
+    # be easily stopped. Manual redirection of all printed output
+    if args.verbose == "WARNING":
+        f = io.StringIO()
+        redirected_stdout = redirect_stdout(f)
+        redirect_stdout_c()
+    else:
+        logging.getLogger().setLevel(logging.getLevelName(args.verbose))
+        redirected_stdout = redirect_stdout(sys.stdout)
 
+    # === Verifications ===
     assert_inputs_exist(parser, [args.in_tractogram, args.in_dwi,
                                  args.in_bval, args.in_bvec],
                         [args.in_peaks, args.in_tracking_mask])
     assert_output_dirs_exist_and_empty(parser, args, args.out_dir,
                                        optional=args.save_kernels)
+    _, ext = os.path.splitext(args.in_tractogram)
+    if ext == '.trk':
+        assert_headers_compatible(parser, [args.in_tractogram, args.in_dwi])
 
     if args.commit2:
         if os.path.splitext(args.in_tractogram)[1] != '.h5':
             parser.error('COMMIT2 requires .h5 file for connectomics.')
         args.ball_stick = True
 
     if args.load_kernels and not os.path.isdir(args.load_kernels):
         parser.error('Kernels directory does not exist.')
 
     if args.compute_only and not args.save_kernels:
         parser.error('--compute_only must be used with --save_kernels.')
 
-    if args.load_kernels and args.save_kernels:
-        parser.error('Cannot load and save kernels at the same time.')
-
     if args.ball_stick and args.perp_diff:
         parser.error('Cannot use --perp_diff with ball&stick.')
 
     if not args.ball_stick and not args.in_peaks:
         parser.error('Stick Zeppelin Ball model requires --in_peaks')
 
     if args.ball_stick and args.iso_diff and len(args.iso_diff) > 1:
         parser.error('Cannot use more than one --iso_diff with '
                      'ball&stick.')
 
-    # If it is a trk, check compatibility of header since COMMIT does not do it
-    dwi_img = nib.load(args.in_dwi)
-    _, ext = os.path.splitext(args.in_tractogram)
-    if ext == '.trk' and not is_header_compatible(args.in_tractogram,
-                                                  dwi_img):
-        parser.error('{} does not have a compatible header with {}'.format(
-            args.in_tractogram, args.in_dwi))
-
-    # COMMIT has some c-level stdout and non-logging print that cannot
-    # be easily stopped. Manual redirection of all printed output
-    if args.verbose:
-        logging.getLogger().setLevel(logging.DEBUG)
-        redirected_stdout = redirect_stdout(sys.stdout)
+    # Case-dependant defaults:
+    tol_fun = 1e-2 if args.commit2 else 1e-3
+    if args.ball_stick:
+        logging.info('Disabled zeppelin, using the Ball & Stick model.')
+        perp_diff = []
+        isotropc_diff = args.iso_diff or [2.0E-3]
     else:
-        f = io.StringIO()
-        redirected_stdout = redirect_stdout(f)
-        redirect_stdout_c()
+        logging.info('Using the Stick Zeppelin Ball model.')
+        perp_diff = args.perp_diff or [0.85E-3, 0.51E-3]
+        isotropc_diff = args.iso_diff or [1.7E-3, 3.0E-3]
 
+    # Prepare tmp dir for all our intermediate files
     tmp_dir = tempfile.TemporaryDirectory()
+
+    # === Loading ===
+    dwi_img = nib.load(args.in_dwi)
+
+    # Load bvals
+    bvals, _ = read_bvals_bvecs(args.in_bval, args.in_bvec)
+    _ = check_b0_threshold(bvals.min(), b0_thr=args.tolerance,
+                           skip_b0_check=args.skip_b0_check)
+    shells_centroids, indices_shells = identify_shells(bvals, args.tolerance,
+                                                       round_centroids=True)
+    if len(shells_centroids) == 2 and not args.ball_stick:
+        parser.error('The DWI data appears to be single-shell.\n'
+                     'Use --ball_stick for single-shell.')
+
+    # If hdf5: reconstruct and save to a temporary trk file on disk
+    bundle_groups_len = []
     hdf5_file = None
     offsets_list = None
     if ext == '.h5':
-        logging.debug('Reconstructing {} into a tractogram for COMMIT.'.format(
-            args.in_tractogram))
-
-        hdf5_file = h5py.File(args.in_tractogram, 'r')
-        if not (np.allclose(hdf5_file.attrs['affine'], dwi_img.affine,
-                            atol=1e-03)
-                and np.array_equal(hdf5_file.attrs['dimensions'],
-                                   dwi_img.shape[0:3])):
-            parser.error('{} does not have a compatible header with {}'.format(
-                args.in_tractogram, args.in_dwi))
-
-        # Keep track of the order of connections/streamlines in relation to the
-        # tractogram as well as the number of streamlines for each connection.
-        bundle_groups_len = []
-        hdf5_keys = list(hdf5_file.keys())
-        streamlines = []
-        for key in hdf5_keys:
-            tmp_streamlines = reconstruct_streamlines_from_hdf5(hdf5_file,
-                                                                key)
-            streamlines.extend(tmp_streamlines)
-            bundle_groups_len.append(len(tmp_streamlines))
-
-        offsets_list = np.cumsum([0]+bundle_groups_len)
-        sft = StatefulTractogram(streamlines, args.in_dwi,
-                                 Space.VOX, origin=Origin.TRACKVIS)
-        tmp_tractogram_filename = os.path.join(tmp_dir.name, 'tractogram.trk')
-
-        # Keeping the input variable, saving trk file for COMMIT internal use
-        save_tractogram(sft, tmp_tractogram_filename)
-        args.in_tractogram = tmp_tractogram_filename
+        hdf5_file, offsets_list, bundle_groups_len = \
+            _save_tmp_tractogram_from_hdf5(tmp_dir, args, dwi_img)
 
     # Writing the scheme file with proper shells
-    tmp_scheme_filename = os.path.join(tmp_dir.name, 'gradients.scheme')
+    tmp_scheme_filename = os.path.join(tmp_dir.name, 'gradients.b')
     tmp_bval_filename = os.path.join(tmp_dir.name, 'bval')
-    bvals, _ = read_bvals_bvecs(args.in_bval, args.in_bvec)
-    shells_centroids, indices_shells = identify_shells(bvals, args.b_thr,
-                                                       roundCentroids=True)
     np.savetxt(tmp_bval_filename, shells_centroids[indices_shells],
                newline=' ', fmt='%i')
     fsl2mrtrix(tmp_bval_filename, args.in_bvec, tmp_scheme_filename)
-    logging.debug('Lauching COMMIT on {} shells at found at {}.'.format(
-        len(shells_centroids),
-        shells_centroids))
 
-    if len(shells_centroids) == 2 and not args.ball_stick:
-        parser.error('The DWI data appears to be single-shell.\n'
-                     'Use --ball_stick for single-shell.')
+    # === Main processing ===
 
+    logging.info('Lauching COMMIT on {} shells at found at {}.'.format(
+        len(shells_centroids),
+        shells_centroids))
     with redirected_stdout:
         # Setting up the tractogram and nifti files
         trk2dictionary.run(filename_tractogram=args.in_tractogram,
                            filename_peaks=args.in_peaks,
                            peaks_use_affine=False,
                            filename_mask=args.in_tracking_mask,
                            ndirs=args.nbr_dir,
                            path_out=tmp_dir.name)
 
         # Preparation for fitting
-        commit.core.setup(ndirs=args.nbr_dir)
+        commit.core.setup()
         mit = commit.Evaluation('.', '.')
 
         # FIX for very small values during HCP processing
         # (based on order of magnitude of signal)
-        img = nib.load(args.in_dwi)
-        data = img.get_fdata(dtype=np.float32)
-        data[data < (0.001*10**np.floor(np.log10(np.mean(data[data > 0]))))] = 0
-        nib.save(nib.Nifti1Image(data, img.affine),
+        data = dwi_img.get_fdata(dtype=np.float32)
+        data[data <
+             (0.001 * 10 ** np.floor(np.log10(np.mean(data[data > 0]))))] = 0
+        nib.save(nib.Nifti1Image(data, dwi_img.affine),
                  os.path.join(tmp_dir.name, 'dwi_zero_fix.nii.gz'))
 
         mit.load_data(os.path.join(tmp_dir.name, 'dwi_zero_fix.nii.gz'),
                       tmp_scheme_filename)
         mit.set_model('StickZeppelinBall')
-
-        if args.ball_stick:
-            logging.debug('Disabled zeppelin, using the Ball & Stick model.')
-            para_diff = args.para_diff or 1.7E-3
-            perp_diff = []
-            isotropc_diff = args.iso_diff or [2.0E-3]
-            mit.model.set(para_diff, perp_diff, isotropc_diff)
-        else:
-            logging.debug('Using the Stick Zeppelin Ball model.')
-            para_diff = args.para_diff or 1.7E-3
-            perp_diff = args.perp_diff or [0.85E-3, 0.51E-3]
-            isotropc_diff = args.iso_diff or [1.7E-3, 3.0E-3]
-            mit.model.set(para_diff, perp_diff, isotropc_diff)
+        mit.model.set(args.para_diff, perp_diff, isotropc_diff)
 
         # The kernels are, by default, set to be in the current directory
         # Depending on the choice, manually change the saving location
         if args.save_kernels:
             kernels_dir = os.path.join(args.save_kernels)
             regenerate_kernels = True
         elif args.load_kernels:
@@ -447,43 +452,41 @@
         mit.set_config('ATOMS_path', kernels_dir)
 
         mit.generate_kernels(ndirs=args.nbr_dir, regenerate=regenerate_kernels)
         if args.compute_only:
             return
         mit.load_kernels()
         use_mask = args.in_tracking_mask is not None
-        mit.load_dictionary(tmp_dir.name,
-                            use_all_voxels_in_mask=use_mask)
+        mit.load_dictionary(tmp_dir.name, use_all_voxels_in_mask=use_mask)
         mit.set_threads(args.nbr_processes)
 
         mit.build_operator(build_dir=os.path.join(tmp_dir.name, 'build/'))
-        tol_fun = 1e-2 if args.commit2 else 1e-3
         mit.fit(tol_fun=tol_fun, max_iter=args.nbr_iter, verbose=False)
         mit.save_results()
-        _save_results_wrapper(args, tmp_dir, ext, hdf5_file, offsets_list,
-                              'commit_1/', False)
+        _save_results(args, tmp_dir, ext, hdf5_file, offsets_list,
+                      'commit_1/', False)
 
         if args.commit2:
             tmp = np.insert(np.cumsum(bundle_groups_len), 0, 0)
-            group_idx = np.array([np.arange(tmp[i], tmp[i+1])
-                                  for i in range(len(tmp)-1)])
+            group_idx = np.array([np.arange(tmp[i], tmp[i + 1])
+                                  for i in range(len(tmp) - 1)])
             group_w = np.empty_like(bundle_groups_len, dtype=np.float64)
             for k in range(len(bundle_groups_len)):
                 group_w[k] = np.sqrt(bundle_groups_len[k]) / \
-                    (np.linalg.norm(mit.x[group_idx[k]]) + 1e-12)
+                             (np.linalg.norm(mit.x[group_idx[k]]) + 1e-12)
             prior_on_bundles = commit.solvers.init_regularisation(
                 mit, structureIC=group_idx, weightsIC=group_w,
                 regnorms=[commit.solvers.group_sparsity,
                           commit.solvers.non_negative,
                           commit.solvers.non_negative],
                 lambdas=[args.lambda_commit_2, 0.0, 0.0])
             mit.fit(tol_fun=1e-3, max_iter=args.nbr_iter,
                     regularisation=prior_on_bundles, verbose=False)
             mit.save_results()
-            _save_results_wrapper(args, tmp_dir, ext, hdf5_file, offsets_list,
-                                  'commit_2/', True)
+            _save_results(args, tmp_dir, ext, hdf5_file, offsets_list,
+                          'commit_2/', True)
 
     tmp_dir.cleanup()
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_run_nlmeans.py` & `scilpy-2.0.0/scripts/scil_denoising_nlmeans.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
 Script to denoise a dataset with the Non Local Means algorithm.
+
+Formerly: scil_run_nlmeans.py
 """
 
 import argparse
 import logging
 import warnings
 
 from dipy.denoise.nlmeans import nlmeans
@@ -15,15 +17,16 @@
 import numpy as np
 
 from scilpy.io.image import get_data_as_mask
 from scilpy.io.utils import (add_processes_arg,
                              add_overwrite_arg,
                              add_verbose_arg,
                              assert_inputs_exist,
-                             assert_outputs_exist)
+                             assert_outputs_exist,
+                             assert_headers_compatible)
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__, formatter_class=argparse.RawTextHelpFormatter)
 
     p.add_argument('in_image',
@@ -69,20 +72,19 @@
     # Broadcast the single value to a whole 3D volume for nlmeans
     return np.ones(data.shape[:3]) * sigma
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, args.in_image)
+    assert_inputs_exist(parser, args.in_image, args.mask)
     assert_outputs_exist(parser, args, args.out_image, args.logfile)
-
-    log_level = logging.INFO if args.verbose else logging.WARNING
-    logging.getLogger().setLevel(log_level)
+    assert_headers_compatible(parser, args.in_image, args.mask)
 
     if args.logfile is not None:
         logging.getLogger().addHandler(logging.FileHandler(args.logfile,
                                                            mode='w'))
 
     vol = nib.load(args.in_image)
     data = vol.get_fdata(dtype=np.float32)
```

### Comparing `scilpy-1.5.post2/scripts/scil_save_connections_from_hdf5.py` & `scilpy-2.0.0/scripts/scil_tractogram_convert_hdf5_to_trk.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,40 +1,43 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-Save individual connection of an hd5f from scil_decompose_connectivity.py.
+Save connections of a hdf5 created with
+>> scil_tractogram_segment_bundles_for_connectivity.py.
+
 Useful for quality control and visual inspections.
 
-It can either save all connections, individual connections specified with
-edge_keys or connections from specific nodes with node_keys.
+It can either save all connections (default), individual connections specified
+with --edge_keys or connections from specific nodes specified with --node_keys.
 
-With the option save_empty, a label_lists, as a txt file, must be provided.
+With the option --save_empty, a label_lists, as a txt file, must be provided.
 This option saves existing connections and empty connections.
 
 The output is a directory containing the thousands of connections:
 out_dir/
     |-- LABEL1_LABEL1.trk
     |-- LABEL1_LABEL2.trk
     |-- [...]
     |-- LABEL90_LABEL90.trk
+
+Formerly: scil_save_connections_from_hdf5.py
 """
 
 import argparse
+import logging
 import os
 
-from dipy.io.stateful_tractogram import Space, Origin, StatefulTractogram
 from dipy.io.streamline import save_tractogram
-from dipy.io.utils import create_nifti_header
 import h5py
 import itertools
 import numpy as np
 
-from scilpy.io.streamlines import reconstruct_streamlines_from_hdf5
-from scilpy.io.utils import (add_overwrite_arg,
+from scilpy.io.hdf5 import reconstruct_sft_from_hdf5
+from scilpy.io.utils import (add_overwrite_arg, add_verbose_arg,
                              assert_inputs_exist,
                              assert_output_dirs_exist_and_empty)
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__, formatter_class=argparse.RawTextHelpFormatter)
@@ -44,78 +47,87 @@
     p.add_argument('out_dir',
                    help='Path of the output directory.')
 
     p.add_argument('--include_dps', action='store_true',
                    help='Include the data_per_streamline the metadata.')
 
     group = p.add_mutually_exclusive_group()
-    group.add_argument('--edge_keys', nargs='+',
-                       help='Keys to identify the edges of '
-                            'interest (LABEL1_LABEL2).')
-
-    group.add_argument('--node_keys', nargs='+',
-                       help='Node keys to identify the '
-                            'sub-network of interest.')
-
-    p.add_argument('--save_empty', action='store_true',
-                   help='Save empty connections.')
-    p.add_argument('--labels_list',
-                   help='A txt file containing a list '
-                        'saved by the decomposition script.')
+    group.add_argument('--edge_keys', nargs='+', metavar='LABEL1_LABEL2',
+                       help='Keys to identify the edges (connections) of '
+                            'interest.')
+    group.add_argument('--node_keys', nargs='+', metavar='NODE',
+                       help='Node keys to identify the sub-networks of '
+                            'interest.\nEquivalent to adding any --edge_keys '
+                            'node_LABEL2 or LABEL2_node.')
+
+    p.add_argument('--save_empty', metavar='labels_list', dest='labels_list',
+                   help='Save empty connections. Then, the list of possible '
+                        'connections is \nnot found from the hdf5 but '
+                        'inferred from labels_list, a txt file \ncontaining '
+                        'a list of nodes saved by the decomposition script.\n'
+                        '*If used together with edge_keys or node_keys, the '
+                        'provided nodes must \nexist in labels_list.')
+
+    add_verbose_arg(p)
+    add_overwrite_arg(p, will_delete_dirs=True)
 
-    add_overwrite_arg(p)
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, args.in_hdf5)
+    # Verifications
+    assert_inputs_exist(parser, args.in_hdf5, args.labels_list)
     assert_output_dirs_exist_and_empty(parser, args, args.out_dir,
                                        create_dir=True)
-    if args.save_empty and args.labels_list is None:
-        parser.error("The option --save_empty requires --labels_list.")
 
+    # Processing
     with h5py.File(args.in_hdf5, 'r') as hdf5_file:
-        if args.save_empty:
+        all_hdf5_keys = list(hdf5_file.keys())
+
+        if args.labels_list:
             all_labels = np.loadtxt(args.labels_list, dtype='str')
             comb_list = list(itertools.combinations(all_labels, r=2))
             comb_list.extend(zip(all_labels, all_labels))
-            keys = [i[0]+'_'+i[1] for i in comb_list]
+            all_keys = [i[0]+'_'+i[1] for i in comb_list]
+            keys_origin = "the labels_list file's labels combination"
+            allow_empty = True
         else:
-            keys = hdf5_file.keys()
+            all_keys = all_hdf5_keys
+            keys_origin = "the hdf5 stored keys"
+            allow_empty = False
 
         if args.edge_keys is not None:
-            selected_keys = [key for key in keys if key in args.edge_keys]
+            selected_keys = args.edge_keys
+
+            # Check that all selected_keys exist.
+            impossible_keys = np.setdiff1d(selected_keys, all_keys)
+            if len(impossible_keys) > 0:
+                parser.error("The following key(s) to not exist in {}: {}\n"
+                             "Please verify your --edge_keys."
+                             .format(keys_origin, impossible_keys))
+
         elif args.node_keys is not None:
             selected_keys = []
             for node in args.node_keys:
-                selected_keys.extend([key for key in keys
+                selected_keys.extend([key for key in all_keys
                                       if key.startswith(node + '_')
                                       or key.endswith('_' + node)])
+            logging.debug("All keys found for provided nodes are: {}"
+                          .format(selected_keys))
         else:
-            selected_keys = keys
+            selected_keys = all_keys
+            logging.debug("All keys are: {}".format(selected_keys))
 
-        affine = hdf5_file.attrs['affine']
-        dimensions = hdf5_file.attrs['dimensions']
-        voxel_sizes = hdf5_file.attrs['voxel_sizes']
-        header = create_nifti_header(affine, dimensions, voxel_sizes)
         for key in selected_keys:
-            streamlines = reconstruct_streamlines_from_hdf5(hdf5_file, key)
-
-            if len(streamlines) == 0 and not args.save_empty:
-                continue
-
-            sft = StatefulTractogram(streamlines, header, Space.VOX,
-                                     origin=Origin.TRACKVIS)
-            if args.include_dps:
-                for dps_key in hdf5_file[key].keys():
-                    if dps_key not in ['data', 'offsets', 'lengths']:
-                        sft.data_per_streamline[dps_key] = hdf5_file[key][dps_key]
-
+            sft, _ = reconstruct_sft_from_hdf5(hdf5_file, key,
+                                               load_dps=args.include_dps,
+                                               allow_empty=allow_empty)
             save_tractogram(sft, '{}.trk'
                             .format(os.path.join(args.out_dir, key)))
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_score_bundles.py` & `scilpy-2.0.0/scripts/scil_bundle_score_many_bundles_one_tractogram.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,37 +1,47 @@
 #!/usr/bin/env python
 # -*- coding: utf-8 -*-
 
 
 """
-This script is similar to scil_score_tractogram, but it supposes that the
-bundles are already segmented, and saved as follow:
+This script is intended to score all bundles from a single tractogram. Each
+valid bundle is compared to its ground truth.
+Ex: It was used for the ISMRM 2015 Challenge scoring.
+
+See also scil_bundle_score_same_bundle_many_segmentations.py to score many
+versions of a same bundle, compared to ONE ground truth / gold standard.
+
+This script is the second part of script scil_score_tractogram, which also
+segments the wholebrain tractogram into bundles first.
+
+Here we suppose that the bundles are already segmented and saved as follows:
     main_dir/
         segmented_VB/*_VS.trk.
         segmented_IB/*_*_IC.trk   (optional)
         segmented_WPC/*_wpc.trk  (optional)
         IS.trk  OR  NC.trk  (if segmented_IB is present)
 
 Config file
 -----------
-
 The config file needs to be a json containing a dict of the ground-truth
 bundles as keys. The value for each bundle is itself a dictionnary with:
 
     - gt_mask: expected result. OL and OR metrics will be computed from this.*
 
 * Files must be .tck, .trk, .nii or .nii.gz. If it is a tractogram, a mask will
 be created. If it is a nifti file, it will be considered to be a mask.
 
 Exemple config file:
 {
   "Ground_truth_bundle_0": {
     "gt_mask": "PATH/bundle0.nii.gz",
   }
 }
+
+Formerly: scil_score_bundles.py
 """
 import argparse
 import glob
 import json
 import logging
 import os
 
@@ -70,25 +80,25 @@
     g = p.add_argument_group("Additions to gt_config")
     g.add_argument("--gt_dir", metavar='DIR',
                    help="Root path of the ground truth files listed in the "
                         "gt_config.\nIf not set, filenames in the config "
                         "file are considered \nas absolute paths.")
 
     add_json_args(p)
-    add_overwrite_arg(p)
     add_reference_arg(p)
-    add_verbose_arg(p)
     add_bbox_arg(p)
+    add_verbose_arg(p)
+    add_overwrite_arg(p)
 
     return p
 
 
 def load_and_verify_everything(parser, args):
 
-    assert_inputs_exist(parser, [args.gt_config])
+    assert_inputs_exist(parser, args.gt_config, args.reference)
     if not os.path.isdir(args.bundles_dir):
         parser.error("Bundles dir ({}) does not exist."
                      .format(args.bundles_dir))
 
     args.json_prefix = os.path.join(args.bundles_dir, args.json_prefix)
     json_output = args.json_prefix + 'results.json'
     assert_outputs_exist(parser, args, json_output)
@@ -163,15 +173,15 @@
     ib_names = []
     if ib_path is not None:
         logging.info("Loading invalid bundles")
         for bundle in glob.glob(ib_path + '/*'):
             ib_names.append(os.path.basename(bundle))
             sft = load_tractogram(bundle, 'same',
                                   bbox_valid_check=args.bbox_check)
-            ib_sft_list.append(ref_sft)
+            ib_sft_list.append(sft)
             if ref_sft is None:
                 ref_sft = sft
     else:
         logging.info("Did not find any invalid bundles.")
 
     # Load either NC or IS
     if nc_filename is not None:
@@ -220,17 +230,15 @@
 
     return bundles, gt_masks
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
-
-    if args.verbose:
-        logging.getLogger().setLevel(logging.INFO)
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     (bundle_names, gt_masks, dimensions,
      vb_sft_list, wpc_sft_list, ib_sft_list, nc_sft,
      ib_names, out_filename) = load_and_verify_everything(parser, args)
 
     args.compute_ic = True if len(ib_sft_list) > 0 else False
     args.save_wpc_separately = True if len(wpc_sft_list) > 0 else False
```

### Comparing `scilpy-1.5.post2/scripts/scil_screenshot_bundle.py` & `scilpy-2.0.0/scripts/scil_viz_dti_screenshot.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,255 +1,215 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-Register bundle to a template for screenshots using a reference.
-The template can be any MNI152 (any resolution, cropped or not)
-If your in_anat has a skull, select a MNI152 template with a skull and
-vice-versa.
-
-If the bundle is already in MNI152 space, do not use --target_template.
+Register DWI to a template for screenshots.
+The templates are on http://www.bic.mni.mcgill.ca/ServicesAtlases/ICBM152NLin2009
 
+For quick quality control, the MNI template can be downsampled to 2mm iso.
 Axial, coronal and sagittal slices are captured.
-Sagittal can be capture from the left (default) or the right.
-"""
+""" # noqa
 
 import argparse
 import logging
 import os
 
-from dipy.io.stateful_tractogram import Space, StatefulTractogram
-from dipy.io.streamline import load_tractogram
-from dipy.tracking.streamline import transform_streamlines
+from dipy.core.gradients import gradient_table, get_bval_indices
+from dipy.io.gradients import read_bvals_bvecs
+from dipy.reconst.dti import fractional_anisotropy, TensorModel
 from fury import actor
 import nibabel as nib
-from nilearn import plotting
 import numpy as np
-from scipy.ndimage import map_coordinates
 
-from scilpy.tractanalysis.streamlines_metrics import compute_tract_counts_map
 from scilpy.io.utils import (add_overwrite_arg,
                              add_verbose_arg,
                              assert_inputs_exist,
                              assert_outputs_exist)
-from scilpy.utils.image import register_image
-from scilpy.viz.screenshot import display_slices
-from scilpy.viz.utils import get_colormap
+from scilpy.gradients.bvec_bval_tools import normalize_bvecs
+from scilpy.image.volume_operations import register_image
+from scilpy.utils.spatial import RAS_AXES_NAMES
+from scilpy.utils.spatial import get_axis_name
+from scilpy.viz.legacy import display_slices
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__,
         formatter_class=argparse.RawTextHelpFormatter)
-    p.add_argument('in_bundle',
-                   help='Path of the input bundle.')
-    p.add_argument('in_anat',
-                   help='Path of the reference file (.nii or nii.gz).')
-    p.add_argument('--target_template',
-                   help='Path to the target MNI152 template for registration. \n'
-                        'If in_anat has a skull, select a MNI152 template \n'
-                        'with a skull and vice-versa.')
-
-    sub_color = p.add_mutually_exclusive_group()
-    sub_color.add_argument('--local_coloring', action='store_true',
-                           help='Color streamlines local segments orientation.')
-    sub_color.add_argument('--uniform_coloring', nargs=3,
-                           metavar=('R', 'G', 'B'), type=float,
-                           help='Color streamlines with uniform coloring.')
-    sub_color.add_argument('--reference_coloring',
-                           metavar='COLORBAR',
-                           help='Color streamlines with reference coloring '
-                                '(0-255).')
-
-    p.add_argument('--right', action='store_true',
-                   help='Take screenshot from the right instead of the left \n'
-                        'for the sagittal plane.')
-    p.add_argument('--anat_opacity', type=float, default=0.3,
-                   help='Set the opacity for the anatomy, use 0 for complete \n'
-                        'transparency, 1 for opaque. [%(default)s]')
-    p.add_argument('--output_suffix',
-                   help='Add a suffix to the output, else the axis name is used.')
+    p.add_argument('in_dwi',
+                   help='Path of the input diffusion volume.')
+    p.add_argument('in_bval',
+                   help='Path of the bval file, in FSL format.')
+    p.add_argument('in_bvec',
+                   help='Path of the bvec file, in FSL format.')
+    p.add_argument('in_template',
+                   help='Path to the target MNI152 template for \n'
+                        'registration, use the one provided online.')
+    p.add_argument('--shells', type=int, nargs='+',
+                   help='Shells to use for DTI fit (usually below 1200), '
+                        'b0 must be listed.')
+    p.add_argument('--out_suffix',
+                   help='Add a suffix to the output, else the '
+                        'axis name is used.')
     p.add_argument('--out_dir', default='',
                    help='Put all images in a specific directory.')
 
     add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
-def prepare_data_for_actors(bundle_filename, reference_filename,
-                            target_template_filename):
-    sft = load_tractogram(bundle_filename, reference_filename)
-    streamlines = sft.streamlines
-
+def prepare_data_for_actors(dwi_filename, bvals_filename, bvecs_filename,
+                            target_template_filename, slices_choice,
+                            shells=None):
     # Load and prepare the data
-    reference_img = nib.load(reference_filename)
-    reference_data = reference_img.get_fdata(dtype=np.float32)
-    reference_affine = reference_img.affine
-
-    if target_template_filename:
-        target_template_img = nib.load(target_template_filename)
-        target_template_data = target_template_img.get_fdata(dtype=np.float32)
-        target_template_affine = target_template_img.affine
-
-        # Register the DWI data to the template
-        logging.debug('Starting registration...')
-        transformed_reference, transformation = register_image(target_template_data,
-                                                               target_template_affine,
-                                                               reference_data,
-                                                               reference_affine)
-        logging.debug('Transforming streamlines...')
-        streamlines = transform_streamlines(streamlines,
-                                            np.linalg.inv(transformation),
-                                            in_place=True)
-
-        new_sft = StatefulTractogram(streamlines, target_template_filename,
-                                     Space.RASMM)
-
-        return new_sft, transformed_reference
-
-    return sft, reference_data
-
-
-def plot_glass_brain(args, sft, img, output_filenames):
-    sft.to_vox()
-    sft.to_corner()
-    _, dimensions, _, _ = sft.space_attributes
-    data = compute_tract_counts_map(sft.streamlines, dimensions)
-    data[data > 100] = 100
-    img = nib.Nifti1Image(data, img.affine)
-
-    axes = 'yz'
-    if args.right:
-        axes = 'r' + axes
+    dwi_img = nib.load(dwi_filename)
+    dwi_data = dwi_img.get_fdata(dtype=np.float32)
+    dwi_affine = dwi_img.affine
+
+    bvals, bvecs = read_bvals_bvecs(bvals_filename, bvecs_filename)
+
+    target_template_img = nib.load(target_template_filename)
+    target_template_data = target_template_img.get_fdata(dtype=np.float32)
+    target_template_affine = target_template_img.affine
+    mask_data = np.zeros(target_template_data.shape)
+    mask_data[target_template_data > 0] = 1
+
+    # Prepare mask for tensors fit
+    x_slice, y_slice, z_slice = slices_choice
+    mask_data = prepare_slices_mask(mask_data,
+                                    x_slice, y_slice, z_slice)
+
+    # Extract B0
+    gtab = gradient_table(bvals, normalize_bvecs(bvecs), b0_threshold=10)
+    b0_idx = np.where(gtab.b0s_mask)[0]
+    mean_b0 = np.mean(dwi_data[..., b0_idx], axis=3, dtype=dwi_data.dtype)
+
+    if shells:
+        indices = [get_bval_indices(bvals, shell) for shell in shells]
+        indices = np.sort(np.hstack(indices))
+
+        if len(indices) < 1:
+            raise ValueError(
+                'There are no volumes that have the supplied b-values.')
+        shell_data = np.zeros((dwi_data.shape[:-1] + (len(indices),)),
+                              dtype=dwi_data.dtype)
+        shell_bvecs = np.zeros((len(indices), 3))
+        shell_bvals = np.zeros((len(indices),))
+        for i, indice in enumerate(indices):
+            shell_data[..., i] = dwi_data[..., indice]
+            shell_bvals[i] = bvals[indice]
+            shell_bvecs[i, :] = bvecs[indice, :]
     else:
-        axes = 'l' + axes
+        shell_data = dwi_data
+        shell_bvals = bvals
+        shell_bvecs = bvecs
+
+    # Register the DWI data to the template
+    transformed_dwi, transformation = register_image(
+        target_template_data, target_template_affine, mean_b0, dwi_affine,
+        transformation_type='rigid',
+        dwi=shell_data)
+
+    # Rotate gradients
+    rotated_bvecs = np.dot(shell_bvecs, transformation[0:3, 0:3])
+
+    rotated_bvecs = normalize_bvecs(rotated_bvecs)
+    rotated_gtab = gradient_table(shell_bvals, rotated_bvecs, b0_threshold=10)
+
+    # Get tensors
+    tensor_model = TensorModel(rotated_gtab, fit_method='LS')
+    tensor_fit = tensor_model.fit(transformed_dwi, mask_data)
+    # Get FA
+    fa_map = np.clip(fractional_anisotropy(tensor_fit.evals), 0, 1)
+
+    # Get eigen vals/vecs
+    evals = np.zeros(target_template_data.shape + (1,))
+    evals[..., 0] = tensor_fit.evals[..., 0] / np.max(tensor_fit.evals[..., 0])
+    evecs = np.zeros(target_template_data.shape + (1, 3))
+    evecs[:, :, :, 0, :] = tensor_fit.evecs[..., 0]
+
+    return fa_map, evals, evecs
+
+
+def prepare_slices_mask(mask_data, x_slice, y_slice, z_slice):
+    mask_slices = np.zeros(mask_data.shape)
+    mask_slices[x_slice, :, :] += mask_data[x_slice, :, :]
+    mask_slices[:, y_slice, :] += mask_data[:, y_slice, :]
+    mask_slices[:, :, z_slice] += mask_data[:, :, z_slice]
+    mask_slices[mask_slices > 1] = 1
 
-    for i, axe in enumerate(axes):
-        display = plotting.plot_glass_brain(img,
-                                            black_bg=True,
-                                            display_mode=axe,
-                                            alpha=0.5)
-        display.savefig(output_filenames[i], dpi=300)
+    return mask_slices.astype(np.uint8)
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
-    required = [args.in_bundle, args.in_anat]
-    assert_inputs_exist(parser, required, args.target_template)
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    if args.verbose:
-        logging.getLogger().setLevel(logging.DEBUG)
+    required = [args.in_dwi, args.in_bval, args.in_bvec, args.in_template]
+    assert_inputs_exist(parser, required)
 
-    output_filenames_3d = []
-    output_filenames_glass = []
-    for axis_name in ['sagittal', 'coronal', 'axial']:
-        if args.output_suffix:
-            output_filenames_3d.append(os.path.join(args.out_dir,
-                                                    '{0}_{1}_3d.png'.format(
-                                                        axis_name,
-                                                        args.output_suffix)))
-
-            output_filenames_glass.append(os.path.join(args.out_dir,
-                                                       '{0}_{1}_glass.png'.format(
-                                                           axis_name,
-                                                           args.output_suffix)))
+    output_filenames = []
+    for axis_name in RAS_AXES_NAMES:
+        if args.out_suffix:
+            output_filenames.append(os.path.join(args.out_dir,
+                                                 '{0}_{1}.png'.format(
+                                                     axis_name,
+                                                     args.out_suffix)))
         else:
-            output_filenames_3d.append(os.path.join(args.out_dir,
-                                                    '{0}_3d.png'.format(
-                                                        axis_name)))
-            output_filenames_glass.append(os.path.join(args.out_dir,
-                                                       '{0}_glass.png'.format(
-                                                           axis_name)))
-    assert_outputs_exist(parser, args,
-                         output_filenames_3d+output_filenames_glass)
+            output_filenames.append(os.path.join(args.out_dir,
+                                                 '{0}.png'.format(axis_name)))
 
     if args.out_dir and not os.path.isdir(args.out_dir):
         os.mkdir(args.out_dir)
 
-    if args.anat_opacity < 0.0 or args.anat_opacity > 1.0:
-        parser.error('Opacity must be between 0 and 1')
-
-    if args.uniform_coloring:
-        for val in args.uniform_coloring:
-            if val < 0 or val > 255:
-                parser.error('{0} is not a valid RGB value'.format(val))
+    assert_outputs_exist(parser, args, output_filenames)
 
     # Get the relevant slices from the template
-    if args.target_template:
-        mni_space_img = nib.load(args.target_template)
-        affine = nib.load(args.target_template).affine
-    else:
-        mni_space_img = nib.load(args.in_anat)
-        affine = nib.load(args.in_anat).affine
+    target_template_img = nib.load(args.in_template)
+    zooms = 1 / float(target_template_img.header.get_zooms()[0])
 
-    x_slice = int(mni_space_img.shape[0] / 2)
-    y_slice = int(mni_space_img.shape[1] / 2)
-    z_slice = int(mni_space_img.shape[2] / 2)
+    x_slice = int(target_template_img.shape[0] / 2 + zooms*30)
+    y_slice = int(target_template_img.shape[1] / 2)
+    z_slice = int(target_template_img.shape[2] / 2)
     slices_choice = (x_slice, y_slice, z_slice)
 
-    subject_data = prepare_data_for_actors(args.in_bundle, args.in_anat,
-                                           args.target_template)
+    FA, evals, evecs = prepare_data_for_actors(args.in_dwi, args.in_bval,
+                                               args.in_bvec,
+                                               args.in_template,
+                                               slices_choice,
+                                               shells=args.shells)
 
     # Create actors from each dataset for Dipy
-    sft, reference_data = subject_data
-    streamlines = sft.streamlines
-
-    volume_actor = actor.slicer(reference_data,
-                                affine=affine,
-                                opacity=args.anat_opacity,
+    volume_actor = actor.slicer(FA,
+                                affine=nib.load(args.in_template).affine,
+                                opacity=0.3,
                                 interpolation='nearest')
-    if args.local_coloring:
-        colors = []
-        for i in streamlines:
-            local_color = np.gradient(i, axis=0)
-            local_color = np.abs(local_color)
-            local_color = (local_color.T / np.max(local_color, axis=1)).T
-            colors.append(local_color)
-    elif args.uniform_coloring:
-        colors = (args.uniform_coloring[0] / 255.0,
-                  args.uniform_coloring[1] / 255.0,
-                  args.uniform_coloring[2] / 255.0)
-    elif args.reference_coloring:
-        sft.to_vox()
-        streamlines_vox = sft.get_streamlines_copy()
-        sft.to_rasmm()
-        colors = []
-        normalized_data = reference_data / np.max(reference_data)
-        cmap = get_colormap(args.reference_coloring)
-        for points in streamlines_vox:
-            values = map_coordinates(normalized_data, points.T,
-                                     order=1, mode='nearest')
-            colors.append(cmap(values)[:, 0:3])
-    else:
-        colors = None
+    peaks_actor = actor.peak_slicer(evecs,
+                                    affine=nib.load(
+                                        args.in_template).affine,
+                                    peaks_values=evals,
+                                    colors=None, linewidth=1)
 
-    streamlines_actor = actor.line(streamlines, colors=colors, linewidth=0.2)
-    # Take a snapshot of each dataset, camera settings are fixed for the
+    # Take a snapshot of each dataset, camera setting are fixed for the
     # known template, won't work with another.
-    if args.right:
-        side_pos = (300, -10, 10)
-    else:
-        side_pos = (-300, 10, 10)
     display_slices(volume_actor, slices_choice,
-                   output_filenames_3d[0], 'sagittal',
-                   view_position=tuple([x for x in side_pos]),
+                   output_filenames[0], get_axis_name(0),
+                   view_position=tuple([x for x in (-125, 10, 10)]),
                    focal_point=tuple([x for x in (0, -10, 10)]),
-                   streamlines_actor=streamlines_actor)
+                   peaks_actor=peaks_actor)
     display_slices(volume_actor, slices_choice,
-                   output_filenames_3d[1], 'coronal',
-                   view_position=tuple([x for x in (0, -300, 15)]),
-                   focal_point=tuple([x for x in (0, 0, 15)]),
-                   streamlines_actor=streamlines_actor)
+                   output_filenames[1], get_axis_name(1),
+                   view_position=tuple([x for x in (0, 150, 30)]),
+                   focal_point=tuple([x for x in (0, 0, 30)]),
+                   peaks_actor=peaks_actor)
     display_slices(volume_actor, slices_choice,
-                   output_filenames_3d[2], 'axial',
-                   view_position=tuple([x for x in (0, -15, 350)]),
-                   focal_point=tuple([x for x in (0, -15, 0)]),
-                   streamlines_actor=streamlines_actor)
-
-    plot_glass_brain(args, sft, mni_space_img, output_filenames_glass)
+                   output_filenames[2], get_axis_name(2),
+                   view_position=tuple([x for x in (0, 25, 150)]),
+                   focal_point=tuple([x for x in (0, 25, 0)]),
+                   peaks_actor=peaks_actor)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_search_keywords.py` & `scilpy-2.0.0/scripts/scil_search_keywords.py`

 * *Files 4% similar despite different names*

```diff
@@ -37,25 +37,27 @@
     p.add_argument('keywords', nargs='+',
                    help='Search the provided list of keywords.')
 
     p.add_argument('--search_parser', action='store_true',
                    help='Search through and display the full script argparser '
                         'instead of looking only at the docstring. (warning: '
                         'much slower).')
+
     add_verbose_arg(p)
+
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
-
-    # Use INFO as default log level, switch to DEBUG if verbose
-    log_level = logging.DEBUG if args.verbose else logging.INFO
-    logging.getLogger().setLevel(log_level)
+    if args.verbose == "WARNING":
+        logging.getLogger().setLevel(logging.INFO)
+    else:
+        logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     # Use directory of this script, should work with most installation setups
     script_dir = pathlib.Path(__file__).parent
     matches = []
 
     keywords_regexes = [re.compile('(' + re.escape(kw) + ')', re.IGNORECASE)
                         for kw in args.keywords]
```

### Comparing `scilpy-1.5.post2/scripts/scil_smooth_streamlines.py` & `scilpy-2.0.0/scripts/scil_tractogram_smooth.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,103 +1,103 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-This script will smooth the streamlines, usually to remove the
-'wiggles' in probabilistic tracking.
+This script will smooth the streamlines, usually to remove the 'wiggles' in
+probabilistic tracking.
+
 Two choices of methods are available:
-- Gaussian will use the surrounding coordinates for smoothing.
-Streamlines are resampled to 1mm step-size and the smoothing is
-performed on the coordinate array. The sigma will be indicative of the
-number of points surrounding the center points to be used for blurring.
-
-- Spline will fit a spline curve to every streamline using a sigma and
-the number of control points. The sigma represents the allowed distance
-from the control points. The control points for the spline fit will be
-the resampled streamline.
+- Gaussian will use the surrounding coordinates for smoothing. Streamlines are
+resampled to 1mm step-size and the smoothing is performed on the coordinate
+array. The sigma will be indicative of the  number of points surrounding the
+center points to be used for blurring.
+- Spline will fit a spline curve to every streamline using a sigma and the
+number of control points. The sigma represents the allowed distance from the
+control points. The final streamlines are obtained by evaluating the spline at
+constant intervals so that it will have the same number of points as initially.
 
 This script enforces endpoints to remain the same.
 
 WARNING:
 - too low of a sigma (e.g: 1) with a lot of control points (e.g: 15)
 will create crazy streamlines that could end up out of the bounding box.
 - data_per_point will be lost.
+
+Formerly: scil_smooth_streamlines.py
 """
 
 import argparse
 import logging
 
 from dipy.io.stateful_tractogram import StatefulTractogram
 from dipy.io.streamline import save_tractogram
 from dipy.tracking.streamlinespeed import compress_streamlines
 
 from scilpy.io.streamlines import load_tractogram_with_reference
 from scilpy.io.utils import (add_overwrite_arg, add_reference_arg,
                              add_verbose_arg,
                              assert_inputs_exist,
-                             assert_outputs_exist)
-from scilpy.tracking.tools import smooth_line_gaussian, smooth_line_spline
+                             assert_outputs_exist, add_compression_arg)
+from scilpy.tractograms.streamline_operations import smooth_line_gaussian, \
+    smooth_line_spline
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter,
                                 description=__doc__)
 
     p.add_argument('in_tractogram',
                    help='Input tractography file.')
 
     p.add_argument('out_tractogram',
                    help='Output tractography file.')
 
     sub_p = p.add_mutually_exclusive_group(required=True)
     sub_p.add_argument('--gaussian', metavar='SIGMA', type=int,
-                       help='Sigma for smoothing. Use the value of surronding\n'
-                            'X,Y,Z points on the streamline to blur the'
-                            ' streamlines.\nA good sigma choice would be '
+                       help='Sigma for smoothing. Use the value of surronding'
+                            '\nX,Y,Z points on the streamline to blur the '
+                            'streamlines.\nA good sigma choice would be '
                             'around 5.')
     sub_p.add_argument('--spline', nargs=2, metavar=('SIGMA', 'NB_CTRL_POINT'),
                        type=int,
                        help='Sigma for smoothing. Model each streamline as a '
                             'spline.\nA good sigma choice would be around 5 '
                             'and control point around 10.')
 
-    p.add_argument('-e', dest='error_rate', type=float, default=0.1,
-                   help='Maximum compression distance in mm after smoothing. '
-                        '[%(default)s]')
-
+    add_compression_arg(p)
     add_reference_arg(p)
-    add_overwrite_arg(p)
     add_verbose_arg(p)
+    add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, args.in_tractogram)
+    assert_inputs_exist(parser, args.in_tractogram, args.reference)
     assert_outputs_exist(parser, args, args.out_tractogram)
 
-    log_level = logging.INFO if args.verbose else logging.WARNING
-    logging.getLogger().setLevel(log_level)
-
     sft = load_tractogram_with_reference(parser, args, args.in_tractogram)
     smoothed_streamlines = []
     for streamline in sft.streamlines:
         if args.gaussian:
             tmp_streamlines = smooth_line_gaussian(streamline, args.gaussian)
         else:
             tmp_streamlines = smooth_line_spline(streamline, args.spline[0],
                                                  args.spline[1])
 
-        if args.error_rate:
-            smoothed_streamlines.append(compress_streamlines(tmp_streamlines,
-                                                             args.error_rate))
-
-    smoothed_sft = StatefulTractogram.from_sft(smoothed_streamlines, sft,
-                                               data_per_streamline=sft.data_per_streamline)
+        if args.compress_th:
+            tmp_streamlines = compress_streamlines(tmp_streamlines,
+                                                   args.compress_th)
+        smoothed_streamlines.append(tmp_streamlines)
+
+    smoothed_sft = StatefulTractogram.from_sft(
+                        smoothed_streamlines, sft,
+                        data_per_streamline=sft.data_per_streamline)
     save_tractogram(smoothed_sft, args.out_tractogram)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_smooth_surface.py` & `scilpy-2.0.0/scripts/scil_surface_smooth.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,18 +1,22 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
 Script to smooth a surface with a Laplacian blur.
 
-step_size from 0.1 to 10 is recommended
-Smoothing_time = step_size * nb_steps
-    [1, 10] for a small smoothing
-    [10, 100] for a moderate smoothing
-    [100, 1000] for a big smoothing
+For a standard FreeSurfer white matter mesh a step_size from 0.1 to 10
+is recommended
+
+Smoothing time = step_size * nb_steps
+    small amount of smoothing [step_size 1, nb_steps 10]
+    moderate amount of smoothing [step_size 10, nb_steps 100]
+    large amount of smoothing [step_size 100, nb_steps 1000]
+
+Formerly: scil_smooth_surface.py
 """
 
 import argparse
 import logging
 
 import numpy as np
 from trimeshpy.io import load_mesh_from_file
@@ -37,43 +41,42 @@
     p.add_argument('in_surface',
                    help='Input surface (.vtk).')
 
     p.add_argument('out_surface',
                    help='Output smoothed surface (.vtk).')
 
     p.add_argument('-m', '--vts_mask',
-                   help='Vertices mask, where to apply the flow (.npy).')
+                   help='Vertex mask no smoothing where mask equals 0 (.npy).')
 
     p.add_argument('-n', '--nb_steps', type=int, default=2,
                    help='Number of steps for laplacian smooth [%(default)s].')
 
     p.add_argument('-s', '--step_size', type=float, default=5.0,
                    help='Laplacian smooth step size [%(default)s].')
 
-    add_overwrite_arg(p)
     add_verbose_arg(p)
+    add_overwrite_arg(p)
+
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, args.in_surface, args.vts_mask)
     assert_outputs_exist(parser, args, args.out_surface)
 
     # Check smoothing parameters
     if args.nb_steps < 1:
-        parser.error("Number of steps should be strictly positive")
+        parser.error("Number of steps should be positive")
 
     if args.step_size <= 0.0:
-        parser.error("Step size should be strictly positive")
-
-    if args.verbose:
-        logging.getLogger().setLevel(logging.DEBUG)
+        parser.error("Step size should be positive")
 
     # Step size (zero for masked vertices)
     if args.vts_mask:
         mask = np.load(args.vts_mask)
         step_size_per_vts = args.step_size * mask.astype(float)
     else:
         step_size_per_vts = args.step_size
```

### Comparing `scilpy-1.5.post2/scripts/scil_split_image.py` & `scilpy-2.0.0/scripts/scil_dwi_split_by_indices.py`

 * *Files 6% similar despite different names*

```diff
@@ -7,14 +7,15 @@
 volumes are in the same order as in the original file. Also outputs the
 corresponding .bval and .bvec files.
 
 This script can be useful for splitting images at places where a b-value
 extraction does not work. For instance, if one wants to split the x first
 b-1500s from the rest of the b-1500s in an image, simply put x as an index.
 
+Formerly: scil_split_image.py
 """
 
 import argparse
 import logging
 
 from dipy.io import read_bvals_bvecs
 import nibabel as nib
@@ -27,18 +28,16 @@
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__,
         formatter_class=argparse.RawTextHelpFormatter)
 
     p.add_argument('in_dwi',
                    help='The DW image file to split.')
-
     p.add_argument('in_bval',
                    help='The b-values file in FSL format (.bval).')
-
     p.add_argument('in_bvec',
                    help='The b-vectors file in FSL format (.bvec).')
 
     p.add_argument('out_basename',
                    help='The basename of the output files. Indices number '
                         'will be appended to out_basename. For example, if '
                         'split_indices were 3 10, the files would be saved as '
@@ -57,17 +56,15 @@
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
-
-    if args.verbose:
-        logging.getLogger().setLevel(logging.INFO)
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, [args.in_dwi, args.in_bval, args.in_bvec])
 
     bvals, bvecs = read_bvals_bvecs(args.in_bval, args.in_bvec)
 
     img = nib.load(args.in_dwi)
```

### Comparing `scilpy-1.5.post2/scripts/scil_split_tractogram.py` & `scilpy-2.0.0/scripts/scil_tractogram_split.py`

 * *Files 7% similar despite different names*

```diff
@@ -6,20 +6,21 @@
 Split into X files, or split into files of Y streamlines.
 
 By default, streamlines to add to each chunk will be chosen randomly.
 Optionally, you can split streamlines...
     - sequentially (the first n/nb_chunks streamlines in the first chunk and so
      on).
     - randomly, but per Quickbundles clusters.
+
+Formerly: scil_split_tractogram.py
 """
 import argparse
 import logging
 import os
 
-from dipy.io.stateful_tractogram import set_sft_logger_level
 from dipy.io.streamline import save_tractogram
 import numpy as np
 
 from scilpy.io.streamlines import load_tractogram_with_reference
 from scilpy.io.utils import (add_overwrite_arg, add_reference_arg,
                              assert_inputs_exist, assert_outputs_exist,
                              assert_output_dirs_exist_and_empty,
@@ -66,41 +67,37 @@
                    default=[40, 30, 20], metavar='t',
                    help="If you chose option '--split_per_cluster', you may "
                         "set the \nQBx threshold value(s) here. Default: "
                         "%(default)s")
 
     p.add_argument('--seed', default=None, type=int,
                    help='Use a specific random seed for the subsampling.')
+
     add_reference_arg(p)
-    add_overwrite_arg(p)
     add_verbose_arg(p)
+    add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, args.in_tractogram)
+    assert_inputs_exist(parser, args.in_tractogram, args.reference)
     _, out_extension = os.path.splitext(args.in_tractogram)
 
     assert_output_dirs_exist_and_empty(parser, args, [], optional=args.out_dir)
     # Check only the first potential output filename, we don't know how many
     # there are yet.
     assert_outputs_exist(parser, args, os.path.join(
         args.out_dir, '{}_0{}'.format(args.out_prefix, out_extension)))
 
-    log_level = logging.WARNING
-    if args.verbose:
-        log_level = logging.DEBUG
-        set_sft_logger_level('INFO')
-    logging.getLogger().setLevel(log_level)
-
-    logging.debug("Loading sft.")
+    logging.info("Loading sft.")
     sft = load_tractogram_with_reference(parser, args, args.in_tractogram)
     streamlines_count = len(sft.streamlines)
 
     if args.nb_chunks:
         chunk_size = int(streamlines_count/args.nb_chunks)
         nb_chunks = args.nb_chunks
     else:
```

### Comparing `scilpy-1.5.post2/scripts/scil_split_volume_by_ids.py` & `scilpy-2.0.0/scripts/scil_labels_split_volume_by_ids.py`

 * *Files 8% similar despite different names*

```diff
@@ -3,24 +3,27 @@
 """
 Split a label image into multiple images where the name of the output images
 is the id of the label (ex. 35.nii.gz, 36.nii.gz, ...). If the --range option
 is not provided, all labels of the image are extracted. The label 0 is
 considered as the background and is ignored.
 
 IMPORTANT: your label image must be of an integer type.
+
+Formerly: scil_split_volume_by_ids.py
 """
 
 import argparse
+import logging
 import os
 
 import nibabel as nib
 import numpy as np
 
 from scilpy.image.labels import get_data_as_labels, split_labels
-from scilpy.io.utils import (add_overwrite_arg,
+from scilpy.io.utils import (add_overwrite_arg, add_verbose_arg,
                              assert_inputs_exist, assert_outputs_exist,
                              assert_output_dirs_exist_and_empty)
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__,
@@ -37,25 +40,26 @@
                    action='append',
                    help='Specifies a subset of labels to split, formatted as '
                         'min max. Ex: -r 3 5 will give files _3, _4, _5.')
     p.add_argument('--background', type=int, default=0,
                    help="Background value. Will not be saved as a separate "
                         "label. Default: 0.")
 
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    required = args.in_labels
-    assert_inputs_exist(parser, required)
+    assert_inputs_exist(parser, args.in_labels)
 
     label_img = nib.load(args.in_labels)
     label_img_data = get_data_as_labels(label_img)
 
     if args.range is not None:
         # Ex: From 173 to 175 = range(173, 176).
         label_indices = np.concatenate(
```

### Comparing `scilpy-1.5.post2/scripts/scil_split_volume_by_labels.py` & `scilpy-2.0.0/scripts/scil_labels_split_volume_from_lut.py`

 * *Files 10% similar despite different names*

```diff
@@ -4,24 +4,27 @@
 """
 Split a label image into multiple images where the name of the output images
 is taken from a lookup table (ex: left-lateral-occipital.nii.gz,
 right-thalamus.nii.gz, ...). Only the labels included in the lookup table
 are extracted.
 
 IMPORTANT: your label image must be of an integer type.
+
+Formerly: scil_split_volume_by_labels.py
 """
 
 import argparse
 import json
+import logging
 import os
 
 import nibabel as nib
 
 from scilpy.image.labels import get_data_as_labels, get_lut_dir, split_labels
-from scilpy.io.utils import (add_overwrite_arg,
+from scilpy.io.utils import (add_overwrite_arg, add_verbose_arg,
                              assert_inputs_exist, assert_outputs_exist,
                              assert_output_dirs_exist_and_empty)
 
 
 def _build_arg_parser():
     luts = [os.path.splitext(f)[0] for f in os.listdir(get_lut_dir())]
 
@@ -41,22 +44,24 @@
         '--scilpy_lut', choices=luts,
         help='Lookup table, in the file scilpy/data/LUT, used to name the '
              'output files.')
     mutual_group.add_argument(
         '--custom_lut',
         help='Path of the lookup table file, used to name the output files.')
 
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, args.in_label)
 
     label_img = nib.load(args.in_label)
     label_img_data = get_data_as_labels(label_img)
 
     if args.scilpy_lut:
```

### Comparing `scilpy-1.5.post2/scripts/scil_tractogram_math.py` & `scilpy-2.0.0/scripts/scil_tractogram_math.py`

 * *Files 8% similar despite different names*

```diff
@@ -23,22 +23,25 @@
 duplicated streamlines.
 
 To allow a soft match, use the --precision option to increase the allowed
 threshold for similarity. A precision of 1 represents 10**(-1), so a
 maximum distance of 0.1mm is allowed. If the streamlines are identical, the
 default value of 3 (or 0.001mm distance) should work.
 
-If there is a 0.5mm shift, use a precision of 0 (or 1mm distance) the --robust
-option should make it work, but slightly slower.
+If there is a 0.5mm shift, use a precision of 0 (or 1mm distance) and the
+--robust option. Should make it work, but slightly slower. Will merge all
+streamlines similar when rounded to that precision level.
 
 The metadata (data per point, data per streamline) of the streamlines that
-are kept in the output will preserved. This requires that all input files
+are kept in the output will be preserved. This requires that all input files
 share the same type of metadata. If this is not the case, use the option
 --no_metadata to strip the metadata from the output. Or --fake_metadata to
 initialize dummy metadata in the file missing them.
+
+Formerly: scil_streamlines_math.py
 """
 
 import argparse
 import json
 import logging
 import os
 
@@ -49,40 +52,29 @@
 from scilpy.io.streamlines import load_tractogram_with_reference
 from scilpy.io.utils import (add_bbox_arg,
                              add_json_args,
                              add_overwrite_arg,
                              add_reference_arg,
                              add_verbose_arg,
                              assert_inputs_exist,
-                             assert_outputs_exist)
+                             assert_outputs_exist,
+                             assert_headers_compatible)
 from scilpy.tractograms.lazy_tractogram_operations import lazy_concatenate
 from scilpy.tractograms.tractogram_operations import (
-    difference_robust, difference, union_robust, union,
-    intersection_robust, intersection, perform_tractogram_operation,
-    concatenate_sft)
-
-
-OPERATIONS = {
-    'difference_robust': difference_robust,
-    'intersection_robust': intersection_robust,
-    'union_robust': union_robust,
-    'difference': difference,
-    'intersection': intersection,
-    'union': union,
-    'concatenate': 'concatenate',
-    'lazy_concatenate': 'lazy_concatenate'
-}
+    perform_tractogram_operation_on_sft, concatenate_sft)
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         formatter_class=argparse.RawTextHelpFormatter,
         description=__doc__)
 
-    p.add_argument('operation', choices=OPERATIONS.keys(), metavar='OPERATION',
+    p.add_argument('operation', metavar='OPERATION',
+                   choices=['difference', 'intersection', 'union',
+                            'concatenate', 'lazy_concatenate'],
                    help='The type of operation to be performed on the '
                         'streamlines. Must\nbe one of the following: '
                         '%(choices)s.')
     p.add_argument('in_tractograms', metavar='INPUT_FILES', nargs='+',
                    help='The list of files that contain the ' +
                         'streamlines to operate on.')
     p.add_argument('out_tractogram', metavar='OUTPUT_FILE',
@@ -99,34 +91,37 @@
                    help='Strip the streamline metadata from the output.')
     p.add_argument('--fake_metadata', action='store_true',
                    help='Skip the metadata verification, create fake metadata '
                         'if missing, can lead to unexpected behavior.')
     p.add_argument('--save_indices', '-s', metavar='OUT_INDEX_FILE',
                    help='Save the streamline indices to the supplied '
                         'json file.')
+    p.add_argument('--save_empty', action='store_true',
+                   help="If set, we will save all results, even if tractogram "
+                        "if empty.")
 
+    add_bbox_arg(p)
     add_json_args(p)
     add_reference_arg(p)
     add_verbose_arg(p)
     add_overwrite_arg(p)
-    add_bbox_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    if args.verbose:
-        logging.getLogger().setLevel(logging.INFO)
-
-    assert_inputs_exist(parser, args.in_tractograms)
+    assert_inputs_exist(parser, args.in_tractograms, args.reference)
     assert_outputs_exist(parser, args, args.out_tractogram,
                          optional=args.save_indices)
+    assert_headers_compatible(parser, args.in_tractograms,
+                              reference=args.reference)
 
     if args.operation == 'lazy_concatenate':
         logging.info('Using lazy_concatenate, no spatial or metadata related '
                      'checks are performed.\nMetadata will be lost, only '
                      'trk/tck file are supported.\n To use trk, at least one '
                      'input must be a trk.')
         _, out_ext = os.path.splitext(args.out_tractogram)
@@ -142,48 +137,58 @@
                              header=header)
         return
 
     # Load all input streamlines.
     sft_list = []
     for f in args.in_tractograms:
         logging.info("Loading file {}".format(f))
-        sft_list.append(load_tractogram_with_reference(parser, args, f))
+        # Using in a millimeter space so that the precision level is in mm.
+        # Note. Sending to_voxmm() returns None with no streamlines.
+        tmp_sft = load_tractogram_with_reference(parser, args, f)
+        tmp_sft.to_voxmm()
+
+        sft_list.append(tmp_sft)
+
+    if np.all([len(sft) == 0 for sft in sft_list]):
+        return
 
     # Apply the requested operation to each input file.
-    logging.info('Performing operation \'{}\'.'.format(args.operation))
-    new_sft = concatenate_sft(sft_list, args.no_metadata, args.fake_metadata)
     if args.operation == 'concatenate':
-        indices = np.arange(len(new_sft), dtype=np.uint32)
+        logging.info('Performing operation "concatenate"')
+        sft_list = [s for s in sft_list if s is not None]
+        new_sft = concatenate_sft(sft_list, args.no_metadata,
+                                  args.fake_metadata)
+        indices_per_sft = [np.arange(len(new_sft), dtype=np.uint32)]
     else:
-        streamlines_list = [sft.streamlines for sft in sft_list]
         op_name = args.operation
         if args.robust:
             op_name += '_robust'
-            _, indices = OPERATIONS[op_name](streamlines_list,
-                                             precision=args.precision)
-        else:
-            _, indices = perform_tractogram_operation(
-                OPERATIONS[op_name], streamlines_list,
-                precision=args.precision)
+
+        logging.info('Performing operation \'{}\'.'.format(op_name))
+        new_sft, indices_per_sft = perform_tractogram_operation_on_sft(
+            op_name, sft_list, precision=args.precision,
+            no_metadata=args.no_metadata, fake_metadata=args.fake_metadata)
+
+        if len(new_sft) == 0 and not args.save_empty:
+            logging.info("Empty resulting tractogram. Not saving results.")
+            return
 
     # Save the indices to a file if requested.
     if args.save_indices:
-        start = 0
         out_dict = {}
-        streamlines_len_cumsum = [len(sft) for sft in sft_list]
-        for name, nb in zip(args.in_tractograms, streamlines_len_cumsum):
-            end = start + nb
+        for name, ind in zip(args.in_tractograms, indices_per_sft):
             # Switch to int32 for json
-            out_dict[name] = [int(i - start)
-                              for i in indices if start <= i < end]
-            start = end
+            out_dict[name] = ind
 
         with open(args.save_indices, 'wt') as f:
-            json.dump(out_dict, f,
-                      indent=args.indent,
+            json.dump(out_dict, f, indent=args.indent,
                       sort_keys=args.sort_keys)
 
     # Save the new streamlines (and metadata)
-    logging.info('Saving {} streamlines to {}.'.format(len(indices),
+    logging.info('Saving {} streamlines to {}.'.format(len(new_sft),
                                                        args.out_tractogram))
-    save_tractogram(new_sft[indices], args.out_tractogram,
+    save_tractogram(new_sft, args.out_tractogram,
                     bbox_valid_check=args.bbox_check)
+
+
+if __name__ == "__main__":
+    main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_uniformize_streamlines_endpoints.py` & `scilpy-2.0.0/scripts/scil_tractogram_uniformize_endpoints.py`

 * *Files 6% similar despite different names*

```diff
@@ -7,17 +7,19 @@
 
 The --auto option will automatically calculate the main orientation.
 If the input bundle is poorly defined, it is possible heuristic will be wrong.
 
 The default is to flip each streamline so their first point's coordinate in the
 defined axis is smaller than their last point (--swap does the opposite).
 
-The --target option will use the barycenter of the target mask to define the
-axis. The target mask can be a binary mask or an atlas. If an atlas is
-used, labels are expected in the form of --target atlas.nii.gz 2 3 5:7.
+The --target_roi option will use the barycenter of the target mask to define
+the axis. The target mask can be a binary mask or an atlas. If an atlas is
+used, labels are expected in the form of --target_roi atlas.nii.gz 2 3 5:7.
+
+Formerly: scil_uniformize_streamlines_endpoints.py
 """
 
 import argparse
 import logging
 
 from dipy.io.streamline import save_tractogram
 import nibabel as nib
@@ -25,63 +27,67 @@
 from scilpy.image.labels import get_data_as_labels
 from scilpy.io.image import merge_labels_into_mask
 from scilpy.io.streamlines import load_tractogram_with_reference
 from scilpy.io.utils import (add_overwrite_arg,
                              add_reference_arg,
                              add_verbose_arg,
                              assert_outputs_exist,
-                             assert_inputs_exist)
-from scilpy.segment.streamlines import filter_grid_roi
-from scilpy.utils.streamlines import (uniformize_bundle_sft,
-                                      uniformize_bundle_sft_using_mask)
+                             assert_inputs_exist, assert_headers_compatible)
+from scilpy.tractanalysis.bundle_operations import \
+    uniformize_bundle_sft, uniformize_bundle_sft_using_mask
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__,
         formatter_class=argparse.RawTextHelpFormatter)
     p.add_argument('in_bundle',
                    help='Input path of the tractography file.')
     p.add_argument('out_bundle',
                    help='Output path of the uniformized file.')
 
     method = p.add_mutually_exclusive_group(required=True)
     method.add_argument('--axis', choices=['x', 'y', 'z'],
-                        help='Match endpoints of the streamlines along this axis.'
-                        '\nSUGGESTION: Commissural = x, Association = y, '
+                        help='Match endpoints of the streamlines along this '
+                        'axis.\nSUGGESTION: Commissural = x, Association = y, '
                         'Projection = z')
     method.add_argument('--auto', action='store_true',
                         help='Match endpoints of the streamlines along an '
                              'automatically determined axis.')
-    method.add_argument('--centroid', metavar='FILE',
-                        help='Match endpoints of the streamlines along an '
-                             'automatically determined axis.')
+    method.add_argument('--centroid', metavar='tractogram',
+                        help='Match endpoints of the streamlines to align it '
+                             'to a reference unique streamline (centroid).')
     method.add_argument('--target_roi', nargs='+',
-                        help='Provide a target ROI and the labels to use.\n'
-                             'Align heads to be closest to the mask barycenter.\n'
-                             'If no labels are provided, all labels will be used.')
+                        help='Provide a target ROI: either a binary mask or a '
+                             'label map and the labels to use.\n'
+                             'Will align heads to be closest to the mask '
+                             'barycenter.\n'
+                             '(atlas: if no labels are provided, all labels '
+                             'will be used.')
     p.add_argument('--swap', action='store_true',
                    help='Swap head <-> tail convention. '
                         'Can be useful when the reference is not in RAS.')
+
     add_reference_arg(p)
     add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    if args.verbose:
-        logging.getLogger().setLevel(logging.INFO)
-
-    assert_inputs_exist(parser, args.in_bundle)
+    roi_file = args.target_roi[0] if args.target_roi else None
+    assert_inputs_exist(parser, args.in_bundle, [roi_file, args.reference])
     assert_outputs_exist(parser, args, args.out_bundle)
+    assert_headers_compatible(parser, args.in_bundle, roi_file,
+                              reference=args.reference)
 
     sft = load_tractogram_with_reference(parser, args, args.in_bundle)
     if args.auto:
         uniformize_bundle_sft(sft, None, swap=args.swap)
 
     if args.centroid:
         centroid_sft = load_tractogram_with_reference(parser, args,
```

### Comparing `scilpy-1.5.post2/scripts/scil_validate_and_correct_eddy_gradients.py` & `scilpy-2.0.0/scripts/scil_gradients_validate_correct_eddy.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,22 +1,25 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
 Validate and correct gradients from eddy outputs
 With full AP-PA eddy outputs a full bvec bval (2x nb of dirs and bval)
 that doesnt fit with the output dwi (1x nb of dir)
+
+Formerly: scil_validate_and_correct_eddy_gradients.py
 """
 
 import argparse
+import logging
 
 import numpy as np
 
 from scilpy.io.utils import (add_overwrite_arg, assert_inputs_exist,
-                             assert_outputs_exist)
+                             assert_outputs_exist, add_verbose_arg)
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__,
         formatter_class=argparse.RawTextHelpFormatter)
     p.add_argument('in_bvec',
@@ -25,21 +28,25 @@
                    help='In bval file.')
     p.add_argument('nb_dirs', type=int,
                    help='Number of directions per DWI.')
     p.add_argument('out_bvec',
                    help='Out bvec file.')
     p.add_argument('out_bval',
                    help='Out bval file.')
+    
+    add_verbose_arg(p)
     add_overwrite_arg(p)
+    
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, [args.in_bvec, args.in_bval])
     assert_outputs_exist(parser, args, [args.out_bval, args.out_bvec])
 
     """
     IN BVEC
     """
```

### Comparing `scilpy-1.5.post2/scripts/scil_validate_bids.py` & `scilpy-2.0.0/scripts/scil_bids_validate.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,23 +1,26 @@
 #! /usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
-Create a json file from a BIDS dataset detailling all info 
+Create a json file from a BIDS dataset detailling all info
 needed for tractoflow
 - DWI/rev_DWI
 - T1
 - fmap/sbref (based on IntendedFor entity)
-- Freesurfer (optional - one per participant)
+- Freesurfer (optional - could be one per participant
+              or one per participant/session)
 
 The BIDS dataset MUST be homogeneous.
 The metadata need to be uniform across all participants/sessions/runs
 
 Mandatory entity: IntendedFor
 Sensitive entities: PhaseEncodingDirection, TotalReadoutTime, direction
+
+Formerly: scil_validate_bids.py
 """
 
 import os
 
 import argparse
 from bids import BIDSLayout, BIDSLayoutIndexer
 from bids.layout import Query
@@ -55,30 +58,31 @@
 
     p.add_argument("out_json",
                    help="Output json file.")
 
     p.add_argument('--bids_ignore',
                    help="If you want to ignore some subjects or some files, "
                         "you can provide an extra bidsignore file."
-                        "Check: https://github.com/bids-standard/bids-validator#bidsignore")
+                        "Check: https://github.com/bids-standard"
+                        "/bids-validator#bidsignore")
 
     p.add_argument("--fs",
                    help='Output freesurfer path. It will add keys wmparc and '
                         'aparc+aseg.')
 
     p.add_argument('--clean',
                    action='store_true',
                    help='If set, it will remove all the participants that '
                         'are missing any information.')
 
     p.add_argument("--readout", type=float, default=0.062,
                    help="Default total readout time value [%(default)s].")
 
-    add_overwrite_arg(p)
     add_verbose_arg(p)
+    add_overwrite_arg(p)
 
     return p
 
 
 def _load_bidsignore_(bids_root, additional_bidsignore=None):
     """Load .bidsignore file from a BIDS dataset, returns list of regexps"""
     bids_root = pathlib.Path(bids_root)
@@ -101,16 +105,17 @@
                 for bi in bids_ignores
                 if len(bi) and bi.strip()[0] != "#"
             ]
         )
     return tuple()
 
 
-def get_opposite_phase_encoding_direction(phase_encoding_direction):
-    """ Return opposite direction (works with direction or PhaseEncodingDirection)
+def get_opposite_pe_direction(phase_encoding_direction):
+    """ Return opposite direction (works with direction
+        or PhaseEncodingDirection)
 
     Parameters
     ----------
     phase_encoding_direction: String
         Phase encoding direction either AP/LR or j/j- i/i- format
 
     Returns
@@ -186,26 +191,24 @@
     if 'session' in curr_dwi.entities:
         nSess = curr_dwi.entities['session']
 
     if 'run' in curr_dwi.entities:
         nRun = curr_dwi.entities['run']
 
     IntendedForPath = os.path.sep.join(curr_dwi.relpath.split(os.path.sep)[1:])
-
     related_files = layout.get(part="mag",
                                IntendedFor=IntendedForPath,
                                regex_search=True,
                                TotalReadoutTime=totalreadout,
                                invalid_filters='drop') +\
         layout.get(part=Query.NONE,
                    IntendedFor=IntendedForPath,
                    regex_search=True,
                    TotalReadoutTime=totalreadout,
                    invalid_filters='drop')
-
     related_files_filtered = []
     for curr_related in related_files:
         if curr_related.entities['suffix'] != 'dwi' and\
            curr_related.entities['extension'] == '.nii.gz':
             related_files_filtered.append(curr_related)
 
     related_files = related_files_filtered
@@ -218,49 +221,69 @@
     dwi_direction = curr_dwi.entities[direction_key]
     PE[0] = conversion[dwi_direction]
 
     if related_files and direction_key:
         related_files_suffixes = []
         for curr_related in related_files:
             related_files_suffixes.append(curr_related.entities['suffix'])
-            if dwi_direction == get_opposite_phase_encoding_direction(curr_related.entities[direction_key]):
+            if dwi_direction == get_opposite_pe_direction(curr_related.entities[direction_key]):
                 PE[1] = conversion[curr_related.entities[direction_key]]
                 topup_suffix[curr_related.entities['suffix']][1] = curr_related.path
             else:
                 topup_suffix[curr_related.entities['suffix']][0] = curr_related.path
 
         if related_files_suffixes.count('epi') > 2 or related_files_suffixes.count('sbref') > 2:
             topup_suffix = {'epi': ['', ''], 'sbref': ['', '']}
-            logging.info('Too many files pointing to {}.'.format(dwis[0].path))
+            logging.warning("Too many files "
+                            "pointing to {}.".format(dwis[0].path))
     else:
         topup = ['', '']
-        logging.info('IntendedFor: No file pointing to {}'.format(dwis[0].path))
+        logging.warning("IntendedFor: No file"
+                        " pointing to {}".format(dwis[0].path))
 
     if len(dwis) == 2:
         if not any(s == '' for s in topup_suffix['sbref']):
             topup = topup_suffix['sbref']
         elif not any(s == '' for s in topup_suffix['epi']):
             topup = topup_suffix['epi']
         else:
             topup = ['', '']
     elif len(dwis) == 1:
         # If one DWI you cannot have a reverse sbref
         # since sbref is a derivate of multi-band dwi
         if topup_suffix['epi'][1] != '':
             topup = topup_suffix['epi']
+        elif not any(s == '' for s in topup_suffix['sbref']):
+            logging.warning("You have two sbref but "
+                            "only one dwi this scheme is not accepted.")
+            topup = ['', '']
         else:
             topup = ['', '']
     else:
         print(dwis)
-        logging.info("""
-                     BIDS structure unkown.Please send an issue:
-                     https://github.com/scilus/scilpy/issues
-                     """)
+        logging.warning("""
+                        BIDS structure unkown.Please send an issue:
+                        https://github.com/scilus/scilpy/issues
+                        """)
         return {}
 
+    if not any(s == '' for s in topup):
+        logging.info("Found rev b0 and b0 images "
+                     "to correct for geometrical distorsion")
+    elif not topup[1]:
+        logging.warning("No rev image found "
+                        "to correct for geometrical distorsion")
+    elif topup[1]:
+        logging.info("Found rev b0 to correct "
+                     "for geometrical distorsion")
+    else:
+        logging.warning("Only found one b0 with same "
+                        "PhaseEncodedDirection won't be enough to "
+                        "correct for geometrical distorsion")
+
     # T1 setup
     t1_path = 'todo'
     wmparc_path = ''
     aparc_aseg_path = ''
     if fs:
         t1_path = fs[0]
         wmparc_path = fs[1]
@@ -276,19 +299,19 @@
                     t1_nSess.append(t1)
             else:
                 t1_nSess.append(t1)
 
         if len(t1_nSess) == 1:
             t1_path = t1_nSess[0].path
         elif len(t1_nSess) == 0:
-            logging.info('No T1 file found.')
+            logging.warning('No T1 file found.')
         else:
             t1_paths = [curr_t1.path for curr_t1 in t1_nSess]
-            logging.info('More than one T1 file found.'
-                         ' [{}]'.format(','.join(t1_paths)))
+            logging.warning('More than one T1 file found.'
+                            ' [{}]'.format(','.join(t1_paths)))
 
     return {'subject': nSub,
             'session': nSess,
             'run': nRun,
             't1': t1_path,
             'wmparc': wmparc_path,
             'aparc_aseg': aparc_aseg_path,
@@ -328,32 +351,37 @@
     # Get possible directions
     phaseEncodingDirection = [Query.ANY, Query.ANY]
     directions = layout.get_direction(**base_dict)
 
     directions.sort()
 
     if not directions and 'PhaseEncodingDirection' in layout.get_entities():
-        logging.info("Found no directions.")
+        logging.info("Found no directions")
         directions = [Query.ANY, Query.ANY]
         phaseEncodingDirection = layout.get_PhaseEncodingDirection(**base_dict)
         if len(phaseEncodingDirection) == 1:
-            logging.info("Found one phaseEncodingDirection.")
+            logging.info("Found one phaseEncodingDirection")
             return [[el] for el in layout.get(part=Query.NONE, **base_dict) +
                     layout.get(part='mag', **base_dict)]
+        elif len(phaseEncodingDirection) == 0:
+            logging.warning("PhaseEncodingDirection exists in this "
+                            "dataset, but no DWI was found")
+            return []
     elif len(directions) == 1:
         logging.info("Found one direction.")
         return [[el] for el in layout.get(part=Query.NONE, **base_dict) +
                 layout.get(part='mag', **base_dict)]
     elif not directions:
-        logging.info("Found no directions or PhaseEncodingDirections.")
+        logging.info("Found no directions or PhaseEncodingDirections")
         return [[el] for el in layout.get(part=Query.NONE, **base_dict) +
                 layout.get(part='mag', **base_dict)]
 
     if len(phaseEncodingDirection) > 2 or len(directions) > 2:
-        logging.info("These acquisitions have too many encoding directions.")
+        logging.warning("These acquisitions have "
+                        "too many encoding directions")
         return []
 
     all_dwis = layout.get(part=Query.NONE,
                           PhaseEncodingDirection=phaseEncodingDirection[0],
                           direction=directions[0],
                           **base_dict) +\
         layout.get(part='mag',
@@ -387,93 +415,109 @@
             direction = False
             if 'direction' in curr_dwi.entities:
                 direction = 'direction'
             elif 'PhaseEncodingDirection' in curr_dwi.entities:
                 direction = 'PhaseEncodingDirection'
 
             if direction:
-                rev_curr_entity[direction] = get_opposite_phase_encoding_direction(rev_curr_entity[direction])
+                rev_curr_entity[direction] = get_opposite_pe_direction(rev_curr_entity[direction])
                 if rev_curr_entity == rev_dwi.get_entities():
                     curr_association.append(rev_dwi)
                     rev_iter_to_rm.append(iter_rev)
                 else:
-                    if rev_curr_entity[direction] == rev_dwi[direction]:
+                    if rev_curr_entity[direction] == rev_dwi.entities[direction]:
                         # Print difference between entities
-                        logging.info('DWIs {} and {} have opposite phase encoding directions but different entities.'
-                                     'Please check their respective json files.'.format(curr_dwi, rev_dwi))
+                        logging.warning('DWIs {} and {} have opposite phase encoding directions but different entities.'
+                                        'Please check their respective json files.'.format(curr_dwi, rev_dwi))
 
         # drop all rev_dwi used
         logging.info('Checking dwi {}'.format(all_dwis[0]))
         for item_to_remove in rev_iter_to_rm[::-1]:
             logging.info('Found rev_dwi {}'.format(all_rev_dwis[item_to_remove]))
             del all_rev_dwis[item_to_remove]
 
         # Add to associated list
         if len(curr_association) < 3:
             all_associated_dwis.append(curr_association)
         else:
-            logging.info("These acquisitions have too many associated dwis.")
+            logging.warning("These acquisitions have "
+                            "too many associated dwis.")
         del all_dwis[0]
 
     if len(all_rev_dwis):
         for curr_rev_dwi in all_rev_dwis:
             all_associated_dwis.append([curr_rev_dwi])
 
     return all_associated_dwis
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
+    coloredlogs.install(level=logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, [], args.bids_ignore)
     assert_outputs_exist(parser, args, args.out_json)
 
-    log_level = logging.INFO if args.verbose else logging.WARNING
-    logging.getLogger().setLevel(log_level)
-    coloredlogs.install(level=log_level)
-
     data = []
     bids_indexer = BIDSLayoutIndexer(validate=False,
                                      ignore=_load_bidsignore_(os.path.abspath(args.in_bids),
                                                               args.bids_ignore))
     layout = BIDSLayout(os.path.abspath(args.in_bids), indexer=bids_indexer)
 
     subjects = layout.get_subjects()
     subjects.sort()
 
-    logging.warning("Found {} subject(s)".format(len(subjects)))
+    logging.info("Found {} subject(s)".format(len(subjects)))
 
     for nSub in subjects:
-        mess = '# Validating subject: {}'.format(nSub)
-        logging.warning("-" * len(mess))
-        logging.warning(mess)
+        mess = 'Validating subject: {}'.format(nSub)
+        logging.info("-" * len(mess))
+        logging.info(mess)
         dwis = associate_dwis(layout, nSub)
-        fs_inputs = []
-        t1s = []
 
-        if args.fs:
-            abs_fs = os.path.abspath(args.fs)
-            logging.warning("# Looking for FS files")
-            t1_fs = glob(os.path.join(abs_fs, 'sub-' + nSub, 'mri/T1.mgz'))
-            wmparc = glob(os.path.join(abs_fs, 'sub-' + nSub, 'mri/wmparc.mgz'))
-            aparc_aseg = glob(os.path.join(abs_fs, 'sub-' + nSub,
-                                           'mri/aparc+aseg.mgz'))
-            if len(t1_fs) == 1 and len(wmparc) == 1 and len(aparc_aseg) == 1:
-                fs_inputs = [t1_fs[0], wmparc[0], aparc_aseg[0]]
-                logging.warning("# Found FS files")
-        else:
-            logging.warning("# Looking for T1 files")
-            t1s = layout.get(subject=nSub,
-                             datatype='anat', extension='nii.gz',
-                             suffix='T1w')
-            if t1s:
-                logging.warning("# Found {} T1 files".format(len(t1s)))
         # Get the data for each run of DWIs
         for dwi in dwis:
+            fs_inputs = []
+            t1s = []
+            if args.fs:
+                abs_fs = os.path.abspath(args.fs)
+
+                logging.info("Looking for FS files")
+                test_fs_sub_path = os.path.join(abs_fs, 'sub-' + nSub)
+                fs_sub_path = ""
+                if os.path.exists(test_fs_sub_path):
+                    fs_sub_path = test_fs_sub_path
+                elif 'session' in dwi[0].entities:
+                    nSess = dwi[0].entities['session']
+                    test_fs_sub_path = os.path.join(abs_fs,
+                                                    'sub-' + nSub + '_ses-' + nSess)
+                    if os.path.exists(test_fs_sub_path):
+                        fs_sub_path = test_fs_sub_path
+
+                if fs_sub_path:
+                    t1_fs = glob(os.path.join(fs_sub_path, 'mri/T1.mgz'))
+                    wmparc = glob(os.path.join(fs_sub_path, 'mri/wmparc.mgz'))
+                    aparc_aseg = glob(os.path.join(fs_sub_path,
+                                                   'mri/aparc+aseg.mgz'))
+
+                    if len(t1_fs) == 1 and len(wmparc) == 1 and len(aparc_aseg) == 1:
+                        fs_inputs = [t1_fs[0], wmparc[0], aparc_aseg[0]]
+                        logging.info("Found FS files")
+                else:
+                    logging.info("NOT Found FS files")
+            else:
+                logging.info("Looking for T1 files")
+                t1s = layout.get(subject=nSub,
+                                 datatype='anat', extension='nii.gz',
+                                 suffix='T1w')
+                if t1s:
+                    logging.info("Found {} T1 files".format(len(t1s)))
+
             data.append(get_data(layout,
                                  nSub,
                                  dwi,
                                  t1s,
                                  fs_inputs,
                                  args.readout,
                                  args.clean))
```

### Comparing `scilpy-1.5.post2/scripts/scil_visualize_bingham_fit.py` & `scilpy-2.0.0/scripts/scil_viz_bingham_fit.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,29 +1,36 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 """
 Visualize 2-dimensional Bingham volume slice loaded from disk. The volume is
-assumed to be saved from scil_fit_bingham_to_fodf.py.
+assumed to be saved from scil_fodf_to_bingham.py.
 
 Given an image of Bingham coefficients, this script displays a slice in a
 given orientation.
 """
 
 import argparse
+import logging
 
 import nibabel as nib
-import numpy as np
 
 from dipy.data import get_sphere, SPHERE_FILES
 
 from scilpy.io.utils import (add_overwrite_arg,
+                             add_verbose_arg,
                              assert_inputs_exist,
                              assert_outputs_exist)
-from scilpy.viz.scene_utils import (create_bingham_slicer,
-                                    create_scene, render_scene)
+from scilpy.utils.spatial import RAS_AXES_NAMES
+from scilpy.utils.spatial import get_axis_index
+
+from scilpy.viz.backends.fury import (create_interactive_window,
+                                      create_scene,
+                                      snapshot_scenes)
+from scilpy.viz.screenshot import compose_image
+from scilpy.viz.slice import create_bingham_slicer
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(description=__doc__,
                                 formatter_class=argparse.RawTextHelpFormatter)
 
     # Positional arguments
@@ -40,22 +47,23 @@
 
     p.add_argument('--interactor', default='trackball',
                    choices={'image', 'trackball'},
                    help='Specify interactor mode for vtk window. '
                         '[%(default)s]')
 
     p.add_argument('--axis_name', default='axial', type=str,
-                   choices={'axial', 'coronal', 'sagittal'},
+                   choices=RAS_AXES_NAMES,
                    help='Name of the axis to visualize. [%(default)s]')
 
     p.add_argument('--silent', action='store_true',
                    help='Disable interactive visualization.')
 
     p.add_argument('--output', help='Path to output file.')
 
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     p.add_argument('--sphere', default='symmetric362',
                    choices=sorted(SPHERE_FILES.keys()),
                    help='Name of the sphere used to reconstruct SF. '
                         '[%(default)s]')
 
@@ -80,64 +88,63 @@
 
     assert_inputs_exist(parser, inputs)
     assert_outputs_exist(parser, args, output)
 
     return args
 
 
-def _axis_name_to_dim(axis_name):
-    """
-    Convert the axis name to its axis index in the data volume.
-    """
-    if axis_name == 'sagittal':
-        return 0
-    if axis_name == 'coronal':
-        return 1
-    if axis_name == 'axial':
-        return 2
-
-
 def _get_slicing_for_axis(axis_name, index, shape):
     """
     Get a tuple of slice representing the slice of interest at `index`
     along the axis `axis_name` in an input volume of dimensions `shape`.
     """
     slicing = [slice(shape[0]), slice(shape[1]), slice(shape[2])]
-    slicing[_axis_name_to_dim(axis_name)] = slice(index, index+1)
+    slicing[get_axis_index(axis_name)] = slice(index, index + 1)
     return tuple(slicing)
 
 
 def _get_data_from_inputs(args):
     """
     Load data given by args.
     """
-    bingham = nib.nifti1.load(args.in_bingham).get_fdata(dtype=np.float32)
+    bingham = nib.load(args.in_bingham).get_fdata()
     if not args.slice_index:
-        slice_index = bingham.shape[_axis_name_to_dim(args.axis_name)] // 2
+        slice_index = bingham.shape[get_axis_index(args.axis_name)] // 2
     else:
         slice_index = args.slice_index
     bingham = bingham[_get_slicing_for_axis(args.axis_name,
                                             slice_index,
                                             bingham.shape)]
     return bingham
 
 
 def main():
     parser = _build_arg_parser()
     args = _parse_args(parser)
     data = _get_data_from_inputs(args)
     sph = get_sphere(args.sphere)
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     actors = create_bingham_slicer(data, args.axis_name,
                                    args.slice_index, sph,
                                    args.color_per_lobe)
 
     # Prepare and display the scene
     scene = create_scene(actors, args.axis_name,
                          args.slice_index,
-                         data.shape[:3])
-    render_scene(scene, args.win_dims, args.interactor,
-                 args.output, args.silent)
+                         data.shape[:3],
+                         args.win_dims[0] / args.win_dims[1])
+
+    if not args.silent:
+        create_interactive_window(
+            scene, args.win_dims, args.interactor)
+
+    if args.output:
+        # Legacy. When this snapshotting gets updated to align with the
+        # viz module, snapshot_scenes should be called directly
+        snapshot = next(snapshot_scenes([scene], args.win_dims))
+        image = compose_image(snapshot, args.win_dims, args.slice_index)
+        image.save(args.output)
 
 
 if __name__ == '__main__':
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_visualize_bundles_mosaic.py` & `scilpy-2.0.0/scripts/scil_viz_bundle_screenshot_mosaic.py`

 * *Files 13% similar despite different names*

```diff
@@ -7,31 +7,31 @@
 """
 
 import argparse
 import logging
 import os
 import random
 
-from dipy.io.utils import is_header_compatible
 from fury import actor, window
 import nibabel as nib
 import numpy as np
 
 from PIL import Image
-from PIL import ImageFont
 from PIL import ImageDraw
 
 from scilpy.io.image import get_data_as_mask
-from scilpy.io.streamlines import load_tractogram_with_reference
 from scilpy.io.utils import (add_overwrite_arg,
                              add_reference_arg,
+                             add_verbose_arg,
                              assert_inputs_exist,
                              assert_outputs_exist,
-                             assert_output_dirs_exist_and_empty)
+                             assert_output_dirs_exist_and_empty,
+                             assert_headers_compatible)
 from scilpy.utils.filenames import split_name_with_nii
+from scilpy.viz.backends.pil import fetch_truetype_font
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         description=__doc__, formatter_class=argparse.RawTextHelpFormatter)
     p.add_argument('in_volume',
                    help='Volume used as background (e.g. T1, FA, b0).')
@@ -72,82 +72,63 @@
                         '[%(default)s].')
     p.add_argument('--no_bundle_name', action='store_true',
                    help='Don\'t display bundle name '
                         '[%(default)s].')
     p.add_argument('--no_streamline_number', action='store_true',
                    help='Don\'t display bundle streamlines number '
                         '[%(default)s].')
+
     add_reference_arg(p)
+    add_verbose_arg(p)
     add_overwrite_arg(p)
-    return p
-
 
-def get_font(args):
-    """ Returns a ttf font object. """
-    if args.ttf is not None:
-        try:
-            font = ImageFont.truetype(args.ttf, args.ttf_size)
-        except Exception:
-            logging.error('Font {} was not found. '
-                          'Default font will be used.'.format(args.ttf))
-            font = ImageFont.load_default()
-    elif args.ttf_size is not None:
-        # default font is not a truetype font, so size can't be changed.
-        # to allow users to change the size without having to know where fonts
-        # are in their computer, we could try to find a truetype font
-        # ourselves. They are often present in /usr/share/fonts/
-        font_path = '/usr/share/fonts/truetype/freefont/FreeSans.ttf'
-        try:
-            font = ImageFont.truetype(font_path, args.ttf_size)
-        except Exception:
-            logging.error('You did not specify a font. It is difficult'
-                          'for us to adjust size. We tried on font {} '
-                          'but it was not found.'
-                          'Default font will be used, for which font '
-                          'cannot be changed.'.format(font_path))
-            font = ImageFont.load_default()
-    else:
-        font = ImageFont.load_default()
-    return font
+    return p
 
 
-def draw_column_with_names(draw, output_names, text_pos_x,
-                           text_pos_y, height, font, view_number):
+def draw_column_with_names(draw, output_names, cell_width, cell_height,
+                           font, row_count, no_bundle_name, no_elements_count):
     """
     Draw the first column with row's description
     (views and bundle information to display).
     """
+    cell_half_width = cell_width / 2.
+    cell_half_height = cell_height / 2.
+
     # Orientation's names
     for num, name in enumerate(output_names):
-        j = height * num + 50
-        i = 0
-        # Name splited in two lines
-        draw.text((i + text_pos_x, j + text_pos_y),
-                  name[:name.find('_')], font=font)
-        draw.text((i + text_pos_x, j + text_pos_y + font.getsize(' ')[1]*1.5),
-                  name[1+name.find('_'):], font=font)
+        j = cell_height * num
+        draw.multiline_text((cell_half_width, j + cell_half_height),
+                            name.replace('_', '\n'), font=font,
+                            anchor='mm', align='center')
 
     # First column, last row: description of the information to show
-    j = height * view_number
-    i = 0
-    draw.text((i + text_pos_x, j + text_pos_y),
-              ('Bundle'), font=font)
-    draw.text((i + text_pos_x, j + text_pos_y + font.getsize(' ')[1]*1.5),
-              ('Elements'), font=font)
+    if not (no_bundle_name and no_elements_count):
+        text = []
+        if not no_bundle_name:
+            text.append('Bundle')
+        if not no_elements_count:
+            text.append('Elements')
+
+        j = cell_height * row_count
+        padding = np.clip(cell_width // 10, 1, font.size)
+        spacing = np.clip(cell_height // 10, 1, font.size)
+        draw.multiline_text((cell_width - padding, j + cell_half_height),
+                            '\n'.join(text), font=font, anchor='rm',
+                            align='right', spacing=spacing)
 
 
-def draw_bundle_information(draw, bundle_file_name, nbr_of_elem,
-                            pos_x, pos_y, font):
+def draw_bundle_information(draw, bundle_name, nbr_of_elem, col_center,
+                            cell_height, font, view_number):
     """ Draw text with bundle information. """
-    if bundle_file_name is not None:
-        draw.text((pos_x, pos_y),
-                  (bundle_file_name), font=font)
-    if nbr_of_elem is not None:
-        draw.text((pos_x, pos_y + font.getsize(' ')[1]*1.5),
-                  ('{}'.format(nbr_of_elem)), font=font)
+    row_center = cell_height / 2.
+    spacing = np.clip(cell_height // 10, 1, font.size)
+    draw.multiline_text((col_center, cell_height * view_number + row_center),
+                        "\n".join(filter(None, [bundle_name, nbr_of_elem])),
+                        font=font, anchor='mm', align='center',
+                        spacing=spacing)
 
 
 def set_img_in_cell(mosaic, ren, view_number, width, height, i):
     """ Set a snapshot of the bundle in a cell of mosaic """
 
     out = window.snapshot(ren, size=(width, height))
     j = height * view_number
@@ -163,79 +144,70 @@
     b = random.randint(0, 255)
     return np.array([r, g, b]) / 255.0
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, args.in_volume)
+    assert_inputs_exist(parser, args.in_bundles + [args.in_volume],
+                        args.reference)
     assert_outputs_exist(parser, args, args.out_image)
+    assert_headers_compatible(parser, args.in_bundles + [args.in_volume],
+                              reference=args.reference)
 
     output_names = ['axial_superior', 'axial_inferior',
                     'coronal_posterior', 'coronal_anterior',
                     'sagittal_left', 'sagittal_right']
     if args.light_screenshot:
         output_names = ['axial_superior',
                         'coronal_posterior',
                         'sagittal_left']
 
-    for filename in args.in_bundles:
-        _, ext = os.path.splitext(filename)
-        if ext == '.tck':
-            tractogram = load_tractogram_with_reference(parser, args, filename)
-        else:
-            tractogram = filename
-        if not is_header_compatible(args.in_volume, tractogram):
-            parser.error('{} does not have a compatible header with {}'.format(
-                filename, args.in_volume))
-        # Delete temporary tractogram
-        else:
-            del tractogram
-
     output_dir = os.path.dirname(args.out_image)
     if output_dir:
         assert_output_dirs_exist_and_empty(parser, args, output_dir,
                                            create_dir=True)
 
-    _, extension = os.path.splitext(args.out_image)
-
     # ----------------------------------------------------------------------- #
     # Mosaic, column 0: orientation names and data description
     # ----------------------------------------------------------------------- #
     width = args.resolution_of_thumbnails
     height = args.resolution_of_thumbnails
+    cell_half_width = width / 2.
+
     rows = 6
     if args.light_screenshot:
         rows = 3
     cols = len(args.in_bundles)
-    text_pos_x = 50
-    text_pos_y = 50
 
     # Creates a new empty image, RGB mode
     if args.no_information:
         mosaic = Image.new('RGB', (cols * width, rows * height))
+    elif args.no_bundle_name and args.no_streamline_number:
+        mosaic = Image.new('RGB', ((cols + 1) * width, rows * height))
     else:
         mosaic = Image.new('RGB', ((cols + 1) * width, (rows + 1) * height))
 
     # Prepare draw and font objects to render text
     draw = ImageDraw.Draw(mosaic)
-    font = get_font(args)
+    font = fetch_truetype_font(args.ttf or "freesans", args.ttf_size)
 
     # Data of the volume used as background
     ref_img = nib.load(args.in_volume)
     data = ref_img.get_fdata(dtype=np.float32)
     affine = ref_img.affine
     mean, std = data[data > 0].mean(), data[data > 0].std()
     value_range = (mean - 0.5 * std, mean + 1.5 * std)
 
     # First column with rows description
     if not args.no_information:
-        draw_column_with_names(draw, output_names, text_pos_x,
-                               text_pos_y, height, font, rows)
+        draw_column_with_names(draw, output_names, width, height, font, rows,
+                               args.no_bundle_name, args.no_streamline_number)
 
     # ----------------------------------------------------------------------- #
     # Columns with bundles
     # ----------------------------------------------------------------------- #
     random.seed(args.random_coloring)
     for idx_bundle, bundle_file in enumerate(args.in_bundles):
 
@@ -244,49 +216,43 @@
 
         if args.no_information:
             i = idx_bundle * width
         else:
             i = (idx_bundle + 1) * width
 
         if not os.path.isfile(bundle_file) and not args.no_information:
-            print('\nInput file {} doesn\'t exist.'.format(bundle_file))
+            logging.warning(
+                '\nInput file {} doesn\'t exist.'.format(bundle_file))
 
             number_streamlines = 0
-
-            view_number = rows
-            j = height * view_number
-
-            draw_bundle_information(draw, bundle_file_name, number_streamlines,
-                                    i + text_pos_x, j + text_pos_y, font)
-
         else:
             if args.uniform_coloring:
                 colors = args.uniform_coloring
             elif args.random_coloring is not None:
                 colors = random_rgb()
             # Select the streamlines to plot
             if bundle_ext in ['.tck', '.trk']:
                 if (args.random_coloring is None
                         and args.uniform_coloring is None):
                     colors = None
                 bundle_tractogram_file = nib.streamlines.load(bundle_file)
                 streamlines = bundle_tractogram_file.streamlines
                 if len(streamlines):
                     bundle_actor = actor.line(streamlines, colors)
-                nbr_of_elem = len(streamlines)
+                number_streamlines = str(len(streamlines))
             # Select the volume to plot
             elif bundle_ext in ['.nii.gz', '.nii']:
                 if not args.random_coloring and not args.uniform_coloring:
                     colors = [1.0, 1.0, 1.0]
                 bundle_img_file = nib.load(bundle_file)
                 roi = get_data_as_mask(bundle_img_file)
                 bundle_actor = actor.contour_from_roi(roi,
                                                       bundle_img_file.affine,
                                                       colors)
-                nbr_of_elem = np.count_nonzero(roi)
+                number_streamlines = str(np.count_nonzero(roi))
 
             # Render
             ren = window.Scene()
             zoom = args.zoom
             opacity = args.opacity_background
 
             # Structural data
@@ -345,25 +311,29 @@
             if not args.light_screenshot:
                 ren.yaw(180)
                 ren.reset_camera()
                 ren.zoom(zoom)
                 view_number += 1
                 set_img_in_cell(mosaic, ren, view_number, width, height, i)
 
-            if not args.no_information:
-                view_number = rows
-                j = height * view_number
-                if args.no_bundle_name:
-                    bundle_file_name = None
-                if args.no_streamline_number:
-                    nbr_of_elem = None
-
-                draw_bundle_information(draw, bundle_file_name,
-                                        nbr_of_elem, i + text_pos_x,
-                                        j + text_pos_y, font)
+        if not args.no_information:
+            if args.no_bundle_name:
+                bundle_name = None
+            if args.no_streamline_number:
+                number_streamlines = None
+            if not (args.no_bundle_name and args.no_streamline_number):
+                draw_bundle_information(draw, bundle_name, number_streamlines,
+                                        i + cell_half_width, height,
+                                        font, rows)
+
+    if not args.no_information:
+        j = rows * height
+        draw.line([(width, 0), (width, j)], fill='white')
+        if not (args.no_bundle_name and args.no_streamline_number):
+            draw.line([(0, j), ((cols + 1) * width, j)], fill='white')
 
     # Save image to file
     mosaic.save(args.out_image)
 
 
 if __name__ == '__main__':
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_visualize_connectivity.py` & `scilpy-2.0.0/scripts/scil_viz_connectivity.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,50 +1,55 @@
 #! /usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 """
 Script to display a connectivity matrix and adjust the desired visualization.
-Made to work with scil_decompose_connectivity.py and
-scil_reorder_connectivity.py.
+Made to work with scil_tractogram_segment_bundles_for_connectivity.py and
+scil_connectivity_reorder_rois.py.
 
 This script can either display the axis labels as:
 - Coordinates (0..N)
 - Labels (using --labels_list)
 - Names (using --labels_list and --lookup_table)
 Examples of labels_list.txt and lookup_table.json can be found in the
 freesurfer_flow output (https://github.com/scilus/freesurfer_flow)
 
-If the matrix was made from a bigger matrix using scil_reorder_connectivity.py,
-provide the text file(s), using --labels_list and/or --reorder_txt.
+If the matrix was made from a bigger matrix using
+scil_connectivity_reorder_rois.py, provide the text file(s), using
+--labels_list and/or --reorder_txt.
 
 The chord chart is always displaying parting in the order they are defined
 (clockwise), the color is attributed in that order following a colormap. The
 thickness of the line represent the 'size/intensity', the greater the value is
 the thicker the line will be. In order to hide the low values, two options are
 available:
-- Angle threshold + alpha, any connections with a small angle on the chord chart
-    will be slightly transparent to increase the focus on bigger connections.
+- Angle threshold + alpha, any connections with a small angle on the chord
+    chart will be slightly transparent to increase the focus on bigger
+    connections.
 - Percentile, hide any connections with a value below that percentile
 """
 
 import argparse
 import copy
 import json
 import math
 import logging
 
 import matplotlib.pyplot as plt
 from matplotlib.font_manager import FontProperties
 import numpy as np
 
-from scilpy.image.operations import EPSILON
-from scilpy.io.utils import (add_overwrite_arg, assert_inputs_exist,
-                             assert_outputs_exist, load_matrix_in_any_format)
-from scilpy.viz.chord_chart import chordDiagram, polar2xy
-from scilpy.viz.utils import get_colormap
+from scilpy.image.volume_math import EPSILON
+from scilpy.io.utils import (add_overwrite_arg,
+                             assert_inputs_exist,
+                             assert_outputs_exist,
+                             add_verbose_arg,
+                             load_matrix_in_any_format)
+from scilpy.viz.legacy.chord_chart import chordDiagram, polar2xy
+from scilpy.viz.color import get_lookup_table
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter,
                                 description=__doc__)
 
     p.add_argument('in_matrix',
@@ -77,54 +82,58 @@
     g2.add_argument('--colormap', default='viridis',
                     help='Colormap to use for the matrix. [%(default)s]')
     g2.add_argument('--display_legend', action='store_true',
                     help='Display the colorbar next to the matrix.')
     g2.add_argument('--legend_min_max', nargs=2, metavar=('MIN', 'MAX'),
                     type=float, default=None,
                     help='Manually define the min/max of the legend.')
-    g2.add_argument('--write_values', nargs=2, metavar=('FONT_SIZE', 'DECIMAL'),
+    g2.add_argument('--write_values', nargs=2, metavar=('FONT_SIZE',
+                                                        'DECIMAL'),
                     default=None, type=int,
                     help='Write the values at the center of each node.\n'
                          'The font size and the rouding parameters can be '
                          'adjusted.')
 
     histo = p.add_argument_group(title='Histogram options')
     histo.add_argument('--histogram', metavar='FILENAME',
-                       help='Compute and display/save an histogram of weights.')
+                       help='Compute and display/save an histogram of weights.'
+                       )
     histo.add_argument('--nb_bins', type=int,
                        help='Number of bins to use for the histogram.')
     histo.add_argument('--exclude_zeros', action='store_true',
                        help='Exclude the zeros from the histogram.')
 
     chord = p.add_argument_group(title='Chord chart options')
     chord.add_argument('--chord_chart', metavar='FILENAME',
-                       help='Compute and display/save a chord chart of weigth.')
+                       help='Compute and display/save a chord chart of weigth.'
+                       )
     chord.add_argument('--percentile_threshold', type=int, default=0,
                        help='Discard connections below that percentile.'
                             '[%(default)s]')
     chord.add_argument('--angle_threshold', type=float, default=1,
                        help='Angle below that theshold will be transparent.\n'
                             'Use --alpha to set opacity. Value typically'
                             'between 0.1 and 5 degrees. [%(default)s]')
     chord.add_argument('--alpha', type=float, default=0.9,
-                       help='Opacity for the smaller angle on the chord (0-1). '
-                            '[%(default)s]')
+                       help='Opacity for the smaller angle on the chord (0-1).'
+                            ' [%(default)s]')
     chord.add_argument('--text_size', default=10, type=float,
                        help='Size of the font for the parcels name/number '
                             '[%(default)s].')
     chord.add_argument('--text_distance', type=float, default=1.1,
                        help='Distance from the center so the parcels '
                             'name/number do not overlap \nwith the diagram '
                             '[%(default)s].')
 
     p.add_argument('--log', action='store_true',
                    help='Apply a base 10 logarithm to the matrix.')
     p.add_argument('--show_only', action='store_true',
                    help='Do not save the figure, simply display it.')
 
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def write_values(ax, matrix, properties):
     width, height = matrix.shape
@@ -137,14 +146,15 @@
                         horizontalalignment='center')
     return ax
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, args.in_matrix)
     if not args.show_only:
         assert_outputs_exist(parser, args, args.out_png, args.histogram)
 
     if args.lookup_table and not args.labels_list:
         parser.error('Lookup table axis naming requires --labels_list.')
@@ -159,15 +169,15 @@
         min_value = np.min(matrix)
 
     max_value = None
     if args.legend_min_max is not None:
         min_value = args.legend_min_max[0]
         max_value = args.legend_min_max[1]
 
-    cmap = get_colormap(args.colormap)
+    cmap = get_lookup_table(args.colormap)
     fig, ax = plt.subplots()
     im = ax.imshow(matrix.T,
                    interpolation='nearest',
                    cmap=cmap, vmin=min_value, vmax=max_value)
 
     if args.write_values:
         if np.prod(matrix.shape) > 1000:
@@ -227,19 +237,21 @@
                         for x in x_list]
 
         if len(y_ticks) != len(y_legend) \
                 or len(x_ticks) != len(x_legend):
             logging.warning('Legend is not the same size as the data.'
                             'Make sure you are using the same reordering '
                             'json.')
+
         plt.xticks(x_ticks, x_legend,
-                   rotation=args.axis_text_angle[0],
+                   rotation=int(args.axis_text_angle[0]),
+                   ha='right',
                    fontsize=args.axis_text_size[0])
         plt.yticks(y_ticks, y_legend,
-                   rotation=args.axis_text_angle[1],
+                   rotation=int(args.axis_text_angle[1]),
                    fontsize=args.axis_text_size[1])
 
     if args.show_only:
         plt.show()
     else:
         plt.savefig(args.out_png, dpi=300, bbox_inches='tight')
 
@@ -248,15 +260,15 @@
         if args.exclude_zeros:
             matrix_hist = matrix[matrix != 0]
         else:
             matrix_hist = matrix.ravel()
 
         _, _, patches = ax.hist(matrix_hist, bins=args.nb_bins)
         nbr_bins = len(patches)
-        color = get_colormap(args.colormap)(np.linspace(0, 1, nbr_bins))
+        color = get_lookup_table(args.colormap)(np.linspace(0, 1, nbr_bins))
         for i in range(0, nbr_bins):
             patches[i].set_facecolor(color[i])
 
         if args.show_only:
             plt.show()
         else:
             plt.savefig(args.histogram, dpi=300, bbox_inches='tight')
```

### Comparing `scilpy-1.5.post2/scripts/scil_visualize_fodf.py` & `scilpy-2.0.0/scripts/scil_viz_fodf.py`

 * *Files 3% similar despite different names*

```diff
@@ -7,30 +7,45 @@
 given orientation. The user can also add a background on top of which the
 fODF are to be displayed. Using a full SH basis, the script can be used to
 visualize asymmetric fODF. The user can supply a peaks image to visualize
 peaks on top of fODF.
 
 If a transparency_mask is given (e.g. a brain mask), all values outside the
 mask non-zero values are set to full transparency in the saved scene.
+
+!!! CAUTION !!! The script is memory intensive about (9kB of allocated RAM per
+voxel, or 9GB for a 1M voxel volume) with a sphere interpolated to 362 points.
 """
 
 import argparse
+import logging
 
 import nibabel as nib
 import numpy as np
 
 from dipy.data import get_sphere
 
 from scilpy.reconst.utils import get_sh_order_and_fullness
-from scilpy.io.utils import (add_sh_basis_args, add_overwrite_arg,
-                             assert_inputs_exist, assert_outputs_exist)
+from scilpy.io.utils import (add_overwrite_arg,
+                             add_sh_basis_args,
+                             assert_inputs_exist,
+                             add_verbose_arg,
+                             assert_outputs_exist,
+                             parse_sh_basis_arg,
+                             assert_headers_compatible)
 from scilpy.io.image import assert_same_resolution, get_data_as_mask
-from scilpy.viz.scene_utils import (create_odf_slicer, create_texture_slicer,
-                                    create_peaks_slicer, create_scene,
-                                    render_scene)
+from scilpy.utils.spatial import RAS_AXES_NAMES
+from scilpy.viz.backends.fury import (create_interactive_window,
+                                      create_scene,
+                                      snapshot_scenes)
+from scilpy.viz.backends.pil import any2grayscale
+from scilpy.viz.screenshot import compose_image
+from scilpy.viz.slice import (create_odf_slicer,
+                              create_peaks_slicer,
+                              create_texture_slicer)
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(description=__doc__,
                                 formatter_class=argparse.RawTextHelpFormatter)
 
     # Positional arguments
@@ -47,15 +62,15 @@
 
     p.add_argument('--interactor', default='trackball',
                    choices={'image', 'trackball'},
                    help='Specify interactor mode for vtk window. '
                         '[%(default)s]')
 
     p.add_argument('--axis_name', default='axial', type=str,
-                   choices={'axial', 'coronal', 'sagittal'},
+                   choices=RAS_AXES_NAMES,
                    help='Name of the axis to visualize. [%(default)s]')
 
     p.add_argument('--silent', action='store_true',
                    help='Disable interactive visualization.')
 
     p.add_argument('--in_transparency_mask', help='Input mask image file.')
 
@@ -64,15 +79,15 @@
     add_overwrite_arg(p)
 
     # Optional FODF personalization arguments
     add_sh_basis_args(p)
 
     sphere_choices = {'symmetric362', 'symmetric642', 'symmetric724',
                       'repulsion724', 'repulsion100', 'repulsion200'}
-    p.add_argument('--sphere', default='symmetric724', choices=sphere_choices,
+    p.add_argument('--sphere', default='symmetric362', choices=sphere_choices,
                    help='Name of the sphere used to reconstruct SF. '
                         '[%(default)s]')
 
     p.add_argument('--sph_subdivide', type=int,
                    help='Number of subdivisions for given sphere. If not '
                         'supplied, use the given sphere as is.')
 
@@ -95,54 +110,55 @@
 
     p.add_argument('--radial_scale_off', action='store_true',
                    help='Disable radial scale for ODF slicer.')
 
     p.add_argument('--norm_off', action='store_true',
                    help='Disable normalization of ODF slicer.')
 
+    add_verbose_arg(p)
+
     # Background image options
     bg = p.add_argument_group('Background arguments')
     bg.add_argument('--background',
-                   help='Background image file. If RGB, values must '
-                        'be between 0 and 255.')
+                    help='Background image file. If RGB, values must '
+                         'be between 0 and 255.')
 
     bg.add_argument('--bg_range', nargs=2, metavar=('MIN', 'MAX'), type=float,
-                   help='The range of values mapped to range [0, 1] '
-                        'for background image. [(bg.min(), bg.max())]')
+                    help='The range of values mapped to range [0, 1] '
+                         'for background image. [(bg.min(), bg.max())]')
 
     bg.add_argument('--bg_opacity', type=float, default=1.0,
-                   help='The opacity of the background image. Opacity of 0.0 '
-                        'means transparent and 1.0 is completely visible. '
-                        '[%(default)s]')
+                    help='The opacity of the background image. Opacity of 0.0 '
+                         'means transparent and 1.0 is completely visible. '
+                         '[%(default)s]')
 
     bg.add_argument('--bg_offset', type=float, default=0.5,
-                   help='The offset of the background image. [%(default)s]')
+                    help='The offset of the background image. [%(default)s]')
 
     bg.add_argument('--bg_interpolation',
-                   default='nearest', choices={'linear', 'nearest'},
-                   help='Interpolation mode for the background image. '
-                        '[%(default)s]')
+                    default='nearest', choices={'linear', 'nearest'},
+                    help='Interpolation mode for the background image. '
+                         '[%(default)s]')
 
     bg.add_argument('--bg_color', nargs=3, type=float, default=(0, 0, 0),
-                   help='The color of the overall background, behind '
-                        'everything. Must be RGB values scaled between 0 and '
-                        '1. [%(default)s]')
+                    help='The color of the overall background, behind '
+                         'everything. Must be RGB values scaled between 0 and '
+                         '1. [%(default)s]')
 
     # Peaks input file options
     peaks = p.add_argument_group('Peaks arguments')
-    peaks.add_argument('--peaks',
-                   help='Peaks image file.')
+    peaks.add_argument('--peaks', help='Peaks image file.')
 
     peaks.add_argument('--peaks_color', nargs=3, type=float,
-                   help='Color used for peaks, as RGB values scaled between 0 '
-                        'and 1. If None, then a RGB colormap is used. '
-                        '[%(default)s]')
+                       help='Color used for peaks, as RGB values scaled '
+                            'between 0 and 1. If None, then a RGB colormap is '
+                            'used. [%(default)s]')
 
     peaks.add_argument('--peaks_width', default=1.0, type=float,
-                   help='Width of peaks segments. [%(default)s]')
+                       help='Width of peaks segments. [%(default)s]')
 
     peaks_scale = p.add_argument_group('Peaks scaling arguments', 'Choose '
                                        'between peaks values and arbitrary '
                                        'length.')
     peaks_scale_group = peaks_scale.add_mutually_exclusive_group()
     peaks_scale_group.add_argument('--peaks_values',
                                    help='Peaks values file.')
@@ -153,102 +169,90 @@
 
     # fODF variance options
     var = p.add_argument_group('Variance arguments', 'For the visualization '
                                'of fodf uncertainty, the variance is used '
                                'as follow: mean + k * sqrt(variance), where '
                                'mean is the input fodf (in_fodf) and k is the '
                                'scaling factor (variance_k).')
-    var.add_argument('--variance',
-                   help='FODF variance file.')
+    var.add_argument('--variance', help='FODF variance file.')
     var.add_argument('--variance_k', default=1, type=float,
-                   help='Scaling factor (k) for the computation of the fodf '
-                        'uncertainty. [%(default)s]')
+                     help='Scaling factor (k) for the computation of the fodf '
+                          'uncertainty. [%(default)s]')
     var.add_argument('--var_color', nargs=3, type=float, default=(1, 1, 1),
-                   help='Color of variance outline. Must be RGB values scaled '
-                        'between 0 and 1. [%(default)s]')
+                     help='Color of variance outline. Must be RGB values '
+                          'scaled between 0 and 1. [%(default)s]')
 
     return p
 
 
 def _parse_args(parser):
     args = parser.parse_args()
-    inputs = []
     output = []
-    inputs.append(args.in_fodf)
     if args.output:
         output.append(args.output)
     else:
         if args.silent:
             parser.error('Silent mode is enabled but no output is specified.'
                          'Specify an output with --output to use silent mode.')
-    if args.in_transparency_mask:
-        inputs.append(args.in_transparency_mask)
-    if args.mask:
-        inputs.append(args.mask)
-    if args.background:
-        inputs.append(args.background)
 
-    if args.peaks:
-        inputs.append(args.peaks)
-        if args.peaks_values:
-            inputs.append(args.peaks_values)
-    else:
-        if args.peaks_values:
-            parser.error('Peaks values image supplied without peaks. Specify '
-                         'a peaks image with --peaks to use this feature.')
-
-    assert_inputs_exist(parser, inputs)
+    if args.peaks_values and not args.peaks:
+        parser.error('Peaks values image supplied without peaks. Specify '
+                     'a peaks image with --peaks to use this feature.')
+
+    optional = [args.in_transparency_mask, args.mask, args.background,
+                args.peaks, args.peaks_values]
+    assert_inputs_exist(parser, args.in_fodf, optional)
     assert_outputs_exist(parser, args, output)
+    assert_headers_compatible(parser, args.in_fodf, optional)
 
     return args
 
 
 def _get_data_from_inputs(args):
     """
     Load data given by args. Perform checks to ensure dimensions agree
     between the data for mask, background, peaks and fODF.
     """
 
-    fodf = nib.nifti1.load(args.in_fodf).get_fdata(dtype=np.float32)
+    fodf = nib.load(args.in_fodf).get_fdata(dtype=np.float32)
     data = {'fodf': fodf}
     if args.background:
         assert_same_resolution([args.background, args.in_fodf])
-        bg = nib.nifti1.load(args.background).get_fdata(dtype=np.float32)
+        bg = nib.load(args.background).get_fdata()
         data['bg'] = bg
     if args.in_transparency_mask:
-        assert_same_resolution([args.in_transparency_mask, args.in_fodf])
         transparency_mask = get_data_as_mask(
-            nib.nifti1.load(args.in_transparency_mask), dtype=bool
+            nib.load(args.in_transparency_mask), dtype=bool
         )
         data['transparency_mask'] = transparency_mask
     if args.mask:
         assert_same_resolution([args.mask, args.in_fodf])
-        mask = get_data_as_mask(nib.nifti1.load(args.mask), dtype=bool)
+        mask = get_data_as_mask(nib.load(args.mask), dtype=bool)
         data['mask'] = mask
     if args.peaks:
         assert_same_resolution([args.peaks, args.in_fodf])
-        peaks = nib.nifti1.load(args.peaks).get_fdata(dtype=np.float32)
+        peaks = nib.load(args.peaks).get_fdata()
         if len(peaks.shape) == 4:
             last_dim = peaks.shape[-1]
             if last_dim % 3 == 0:
                 npeaks = int(last_dim / 3)
                 peaks = peaks.reshape((peaks.shape[:3] + (npeaks, 3)))
             else:
                 raise ValueError('Peaks volume last dimension ({0}) cannot '
                                  'be reshaped as (npeaks, 3).'
                                  .format(peaks.shape[-1]))
         data['peaks'] = peaks
         if args.peaks_values:
             assert_same_resolution([args.peaks_values, args.in_fodf])
             peak_vals =\
-                nib.nifti1.load(args.peaks_values).get_fdata(dtype=np.float32)
+                nib.load(args.peaks_values).get_fdata()
             data['peaks_values'] = peak_vals
     if args.variance:
         assert_same_resolution([args.variance, args.in_fodf])
-        variance = nib.nifti1.load(args.variance).get_fdata(dtype=np.float32)
+        variance = nib.load(args.variance).get_fdata(dtype=np.float32)
         if len(variance.shape) == 3:
             variance = np.reshape(variance, variance.shape + (1,))
         if variance.shape != fodf.shape:
             raise ValueError('Dimensions mismatch between fODF {0} and '
                              'variance {1}.'
                              .format(fodf.shape, variance.shape))
         data['variance'] = variance
@@ -258,14 +262,16 @@
 
 def main():
     parser = _build_arg_parser()
     args = _parse_args(parser)
     data = _get_data_from_inputs(args)
     sph = get_sphere(args.sphere)
     sh_order, full_basis = get_sh_order_and_fullness(data['fodf'].shape[-1])
+    sh_basis, is_legacy = parse_sh_basis_arg(args)
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     actors = []
 
     # Retrieve the mask if supplied
     if 'mask' in data:
         mask = data['mask']
     else:
@@ -276,24 +282,24 @@
     else:
         color_rgb = None
 
     variance = data['variance'] if args.variance else None
     var_color = np.asarray(args.var_color) * 255
     # Instantiate the ODF slicer actor
     odf_actor, var_actor = create_odf_slicer(data['fodf'], args.axis_name,
-                                             args.slice_index, mask, sph,
-                                             args.sph_subdivide, sh_order,
-                                             args.sh_basis, full_basis,
-                                             args.scale,
+                                             args.slice_index, sph, sh_order,
+                                             sh_basis, full_basis,
+                                             args.scale, variance, mask,
+                                             args.sph_subdivide,
                                              not args.radial_scale_off,
                                              not args.norm_off,
                                              args.colormap or color_rgb,
-                                             sh_variance=variance,
                                              variance_k=args.variance_k,
-                                             variance_color=var_color)
+                                             variance_color=var_color,
+                                             is_legacy=is_legacy)
     actors.append(odf_actor)
 
     # Instantiate a variance slicer actor if a variance image is supplied
     if 'variance' in data:
         actors.append(var_actor)
 
     # Instantiate a texture slicer actor if a background image is supplied
@@ -327,15 +333,16 @@
 
         actors.append(peaks_actor)
 
     # Prepare and display the scene
     scene = create_scene(actors, args.axis_name,
                          args.slice_index,
                          data['fodf'].shape[:3],
-                         args.bg_color)
+                         args.win_dims[0] / args.win_dims[1],
+                         bg_color=args.bg_color)
 
     mask_scene = None
     if 'transparency_mask' in data:
         mask_actor = create_texture_slicer(
             data['transparency_mask'].astype("uint8"),
             args.axis_name,
             args.slice_index,
@@ -343,16 +350,31 @@
             )
 
         mask_scene = create_scene(
             [mask_actor],
             args.axis_name,
             args.slice_index,
             data['transparency_mask'].shape,
-            args.bg_color,
-        )
+            args.win_dims[0] / args.win_dims[1],
+            bg_color=args.bg_color)
+
+    if not args.silent:
+        create_interactive_window(
+            scene, args.win_dims, args.interactor)
+
+    if args.output:
+        snapshots = snapshot_scenes(filter(None, [mask_scene, scene]),
+                                    args.win_dims)
+        _mask_arr = None
+        if mask_scene:
+            _mask_arr = any2grayscale(next(snapshots))
+
+        image = compose_image(next(snapshots),
+                              args.win_dims,
+                              args.slice_index,
+                              overlays_scene=_mask_arr)
 
-    render_scene(scene, args.win_dims, args.interactor,
-                 args.output, args.silent, mask_scene=mask_scene)
+        image.save(args.output)
 
 
 if __name__ == '__main__':
     main()
```

### Comparing `scilpy-1.5.post2/scripts/scil_visualize_gradients.py` & `scilpy-2.0.0/scripts/scil_viz_gradients_screenshot.py`

 * *Files 4% similar despite different names*

```diff
@@ -9,26 +9,26 @@
 import argparse
 import logging
 import numpy as np
 import os
 
 from dipy.data import get_sphere
 
-from scilpy.utils.bvec_bval_tools import identify_shells
+from scilpy.gradients.bvec_bval_tools import identify_shells
 from scilpy.io.utils import (add_overwrite_arg,
                              add_verbose_arg,
                              assert_gradients_filenames_valid,
                              assert_inputs_exist,
                              assert_outputs_exist)
-from scilpy.viz.gradient_sampling import (build_ms_from_shell_idx,
-                                          plot_each_shell,
-                                          plot_proj_shell)
+from scilpy.viz.gradients import (build_ms_from_shell_idx,
+                                  plot_each_shell,
+                                  plot_proj_shell)
 
 sphere_choices = ['symmetric362', 'symmetric642', 'symmetric724',
-                  'repulsion724','repulsion100', 'repulsion200']
+                  'repulsion724', 'repulsion100', 'repulsion200']
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(
         formatter_class=argparse.RawDescriptionHelpFormatter,
         description=__doc__)
 
@@ -67,24 +67,24 @@
     g2.add_argument(
         '--same-color', action='store_true', dest='same_color',
         help='Use same color for all shell.')
     g2.add_argument(
         '--opacity', type=float, default=1.0,
         help='Opacity for the shells.')
 
-    add_overwrite_arg(p)
     add_verbose_arg(p)
+    add_overwrite_arg(p)
+
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
-    if args.verbose:
-        logging.getLogger().setLevel(logging.INFO)
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     # -- Perform checks
     if args.in_gradient_scheme is not None:
         assert_inputs_exist(parser, args.in_gradient_scheme)
 
         if len(args.in_gradient_scheme) == 2:
             assert_gradients_filenames_valid(parser, args.in_gradient_scheme,
```

### Comparing `scilpy-1.5.post2/scripts/scil_visualize_histogram.py` & `scilpy-2.0.0/scripts/scil_viz_volume_histogram.py`

 * *Files 10% similar despite different names*

```diff
@@ -2,28 +2,29 @@
 # -*- coding: utf-8 -*-
 
 """
 Script to display a histogram of a metric (FA, MD, etc.) from a binary mask
 (wm mask, vascular mask, ect.).
 These two images must be coregister with each other.
 
->>> scil_visualize_histogram.py metric.nii.gz mask_bin.nii.gz 8
+>>> scil_viz_volume_histogram.py metric.nii.gz mask_bin.nii.gz 8
     out_filename_image.png
-
 """
 
 import argparse
+import logging
 
 import matplotlib.pyplot as plt
 import nibabel as nib
 import numpy as np
 
-from scilpy.io.image import (get_data_as_mask, assert_same_resolution)
+from scilpy.io.image import get_data_as_mask
 from scilpy.io.utils import (add_overwrite_arg, assert_inputs_exist,
-                             assert_outputs_exist)
+                             assert_outputs_exist, add_verbose_arg,
+                             assert_headers_compatible)
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter,
                                 description=__doc__)
 
     p.add_argument('in_metric',
@@ -44,38 +45,38 @@
     hist.add_argument('--colors', default='#0504aa',
                       help='Use the provided info for the bars color.'
                            ' [%(default)s]')
 
     p.add_argument('--show_only', action='store_true',
                    help='Do not save the figure, only display.')
 
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, [args.in_metric, args.in_mask])
     assert_outputs_exist(parser, args, args.out_png)
+    assert_headers_compatible(parser, [args.in_metric, args.in_mask])
 
     # Load metric image
     metric_img = nib.load(args.in_metric)
     metric_img_data = metric_img.get_fdata(dtype=np.float32)
 
     # Load mask image
-    mask_img = nib.load(args.in_mask)
-    mask_img_data = get_data_as_mask(mask_img)
-
-    assert_same_resolution((metric_img, mask_img))
+    mask = get_data_as_mask(nib.load(args.in_mask))
 
     # Select value from mask
-    curr_data = metric_img_data[np.where(mask_img_data > 0)]
+    curr_data = metric_img_data[np.where(mask > 0)]
 
     # Display figure
     fig, ax = plt.subplots()
     n, bins, patches = ax.hist(curr_data, bins=args.n_bins,
                                color=args.colors, alpha=0.5, rwidth=0.85)
     plt.xlabel(args.x_label)
     plt.title(args.title)
```

### Comparing `scilpy-1.5.post2/scripts/scil_visualize_scatterplot.py` & `scilpy-2.0.0/scripts/scil_viz_volume_scatterplot.py`

 * *Files 4% similar despite different names*

```diff
@@ -14,46 +14,47 @@
     "1": "lh_A8m",
     "2": "rh_A8m",
     The numbers must be corresponding to the label indices in the json file.
 
 Be careful, you can not use all of them at the same time.
 
 For general scatter plot without mask:
->>> scil_visualize_scatterplot.py FA.nii.gz MD.nii.gz out_filename_image.png
+>>> scil_viz_volume_scatterplot.py FA.nii.gz MD.nii.gz out_filename_image.png
 
 For scatter plot with mask:
->>> scil_visualize_scatterplot.py FA.nii.gz MD.nii.gz out_filename_image.png
+>>> scil_viz_volume_scatterplot.py FA.nii.gz MD.nii.gz out_filename_image.png
     --in_bin_mask mask_wm.nii.gz
 
 For tissue probability scatter plot:
->>> scil_visualize_scatterplot.py FA.nii.gz MD.nii.gz out_filename_image.png
+>>> scil_viz_volume_scatterplot.py FA.nii.gz MD.nii.gz out_filename_image.png
     --prob_maps wm_map.nii.gz gm_map.nii.gz
 
 For scatter plot using atlas:
->>> scil_visualize_scatterplot.py FA.nii.gz MD.nii.gz out_filename_image.png
+>>> scil_viz_volume_scatterplot.py FA.nii.gz MD.nii.gz out_filename_image.png
     --in_atlas atlas.nii.gz --atlas_lut atlas.json
 
->>> scil_visualize_scatterplot.py FA.nii.gz MD.nii.gz out_filename_image.png
+>>> scil_viz_volume_scatterplot.py FA.nii.gz MD.nii.gz out_filename_image.png
     --in_atlas atlas.nii.gz --atlas_lut atlas.json
     --specific_label 34 67 87
-
 """
 
 import argparse
 import copy
 import json
+import logging
 import os
 
 import matplotlib.pyplot as plt
 import nibabel as nib
 import numpy as np
 
 from scilpy.image.labels import get_data_as_labels
 from scilpy.io.image import get_data_as_mask
-from scilpy.io.utils import add_overwrite_arg, assert_inputs_exist
+from scilpy.io.utils import (add_overwrite_arg, add_verbose_arg,
+                             assert_inputs_exist, assert_headers_compatible)
 
 
 def _build_arg_parser():
     p = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter,
                                 description=__doc__)
 
     p.add_argument('in_x_map',
@@ -125,50 +126,53 @@
     scat.add_argument('--colors', nargs=2, metavar=('color1', 'color2'),
                       default=('r', 'b'))
 
     p.add_argument('--show_only', action='store_true',
                    help='Do not save the figure, only display. '
                         ' Not avalaible with --in_atlas option.')
 
+    add_verbose_arg(p)
     add_overwrite_arg(p)
 
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
-    assert_inputs_exist(parser, [args.in_x_map, args.in_y_map])
+    maps = [args.in_x_map, args.in_y_map]
+    prob_maps = args.in_prob_maps or []
+    assert_inputs_exist(parser, maps,
+                        optional=prob_maps +
+                        [args.in_bin_mask, args.in_atlas, args.atlas_lut])
+    assert_headers_compatible(parser, maps,
+                              optional=prob_maps +
+                              [args.in_bin_mask, args.in_atlas])
 
     if args.out_dir is None:
         args.out_dir = './'
 
-    if args.in_atlas:
-        assert_inputs_exist(parser, args.atlas_lut)
-
     # Load x and y images
-    maps = [args.in_x_map, args.in_y_map]
-
     maps_data = []
     for curr_map in maps:
         maps_image = nib.load(curr_map)
         if args.not_exclude_zero:
             maps_data.append(maps_image.get_fdata(dtype=np.float32))
         else:
             data = maps_image.get_fdata(dtype=np.float32)
             data[np.where(data == 0)] = np.nan
             maps_data.append(data)
 
     if args.in_bin_mask:
         if args.label is None:
             args.label = 'Masking data'
         # Load and apply binary mask
-        mask_image = nib.load(args.in_bin_mask)
-        mask_data = get_data_as_mask(mask_image)
+        mask_data = get_data_as_mask(nib.load(args.in_bin_mask))
         for curr_map in maps_data:
             curr_map[np.where(mask_data == 0)] = np.nan
 
     if args.in_prob_maps:
         if args.label is None:
             args.label = 'Threshold prob_map 1'
         # Load tissue probability maps
```

### Comparing `scilpy-1.5.post2/scripts/scil_visualize_seeds_3d.py` & `scilpy-2.0.0/scripts/scil_viz_tractogram_seeds_3d.py`

 * *Files 20% similar despite different names*

```diff
@@ -2,25 +2,28 @@
 # -*- coding: utf-8 -*-
 
 """
 Visualize seeds as 3D points, with heatmaps corresponding to seed density
 
 Example usages:
 
-scil_visualize_seeds_3d.py seeds.nii.gz --tractogram tractogram.trk
-
+>>> scil_viz_tractogram_seeds_3d.py seeds.nii.gz --tractogram tractogram.trk
 """
 
 import argparse
+import logging
 import nibabel as nib
 import numpy as np
 
 from fury import window, actor
 
-from scilpy.io.utils import assert_inputs_exist, parser_color_type
+from scilpy.io.utils import (assert_inputs_exist,
+                             add_verbose_arg,
+                             parser_color_type)
+from scilpy.viz.color import lut_from_matplotlib_name
 
 
 streamline_actor = {'tube': actor.streamtube,
                     'line': actor.line}
 
 
 def _build_arg_parser():
@@ -51,42 +54,51 @@
                    nargs='+', default=None, type=parser_color_type,
                    help='Color for the tractogram.')
     p.add_argument('--background', metavar='R G B', nargs='+',
                    default=[0, 0, 0], type=parser_color_type,
                    help='RBG values [0, 255] of the color of the background.'
                    '\n[Default: %(default)s]')
 
+    add_verbose_arg(p)
+
     return p
 
 
 def main():
     parser = _build_arg_parser()
     args = parser.parse_args()
+    logging.getLogger().setLevel(logging.getLevelName(args.verbose))
 
     assert_inputs_exist(parser, args.in_seed_map, [args.tractogram])
 
     # Seed map informations
     seed_map_img = nib.load(args.in_seed_map)
     seed_map_data = seed_map_img.get_fdata().astype(np.uint8)
     seed_map_affine = seed_map_img.affine
 
     # Load seed density as labels
-    values = np.delete(np.unique(seed_map_data), 0)
+    values = np.unique(seed_map_data)
+
     # Create colormap based on labels
-    cmap = actor.create_colormap(values, name=args.colormap, auto=False)
-    # Append opacity to colormap
-    cmap = np.concatenate(
-        (cmap, np.full((cmap.shape[0], 1), args.seed_opacity)),
-        axis=-1)
+    lut = lut_from_matplotlib_name(args.colormap, [values.min(), values.max()],
+                                   len(values))
+
+    # Delete 0 from values
+    values = np.delete(values, 0)
+
+    colors = np.zeros((len(values), 4))
+    for i, v in enumerate(values):
+        lut.GetColor(v, colors[i, :3])
+        colors[i, 3] = lut.GetOpacity(v)
 
     scene = window.Scene()
     scene.background(tuple(map(int, args.background)))
 
     seedroi_actor = actor.contour_from_label(
-        seed_map_data, seed_map_affine, color=cmap)
+        seed_map_data, seed_map_affine, color=colors)
     scene.add(seedroi_actor)
 
     # Load tractogram as tubes or lines, with color if specified
     if args.tractogram:
         tractogram = nib.streamlines.load(args.tractogram).tractogram
         color = None
         if args.tractogram_color:
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_apply_bias_field_on_dwi.py` & `scilpy-2.0.0/scripts/tests/test_tractogram_print_info.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,28 +1,24 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['processing.zip'])
+fetch_data(get_testing_files_dict(), keys=['filtering.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_apply_bias_field_on_dwi.py', '--help')
+    ret = script_runner.run('scil_tractogram_print_info.py', '--help')
     assert ret.success
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_dwi = os.path.join(get_home(), 'processing',
-                          'dwi_crop.nii.gz')
-    in_bias = os.path.join(get_home(), 'processing',
-                           'bias_field_b0.nii.gz')
-    ret = script_runner.run('scil_apply_bias_field_on_dwi.py', in_dwi,
-                            in_bias, 'dwi_crop_n4.nii.gz')
+def test_execution_filtering(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_bundle = os.path.join(SCILPY_HOME, 'filtering', 'bundle_4.trk')
+    ret = script_runner.run('scil_tractogram_print_info.py', in_bundle)
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_apply_transform_to_bvecs.py` & `scilpy-2.0.0/scripts/tests/test_surface_apply_transform.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,30 +1,28 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-
-import os
-import tempfile
-
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
-
-# If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['bst.zip'])
-fetch_data(get_testing_files_dict(), keys=['processing.zip'])
-tmp_dir = tempfile.TemporaryDirectory()
-
-
-def test_help_option(script_runner):
-    ret = script_runner.run('scil_apply_transform_to_bvecs.py', '--help')
-    assert ret.success
-
-
-def test_execution_bst(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_bvecs = os.path.join(get_home(), 'processing',
-                           'dwi.bvec')
-    in_aff = os.path.join(get_home(), 'bst',
-                          'output0GenericAffine.mat')
-    ret = script_runner.run('scil_apply_transform_to_bvecs.py',
-                            in_bvecs, in_aff,
-                            'bvecs_transformed.bvec', '--inverse')
-    assert ret.success
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+
+import os
+import tempfile
+
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
+
+# If they already exist, this only takes 5 seconds (check md5sum)
+fetch_data(get_testing_files_dict(), keys='surface_vtk_fib.zip')
+tmp_dir = tempfile.TemporaryDirectory()
+
+
+def test_help_option(script_runner):
+    ret = script_runner.run('scil_surface_apply_transform.py', '--help')
+    assert ret.success
+
+
+def test_execution_surface_vtk_fib(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_surf = os.path.join(SCILPY_HOME, 'surface_vtk_fib',
+                           'lhpialt.vtk')
+    in_aff = os.path.join(SCILPY_HOME, 'surface_vtk_fib',
+                          'affine.txt')
+    ret = script_runner.run('scil_surface_apply_transform.py', in_surf,
+                            in_aff, 'lhpialt_lin.vtk')
+    assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_apply_transform_to_hdf5.py` & `scilpy-2.0.0/scripts/tests/test_tractogram_segment_bundles_for_connectivity.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,30 +1,31 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['connectivity.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_apply_transform_to_hdf5.py', '--help')
+    ret = script_runner.run(
+        'scil_tractogram_segment_bundles_for_connectivity.py', '--help')
     assert ret.success
 
 
-def test_execution_connectivity(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_h5 = os.path.join(get_home(), 'connectivity',
-                         'decompose.h5')
-    in_target = os.path.join(get_home(), 'connectivity',
-                             'endpoints_atlas.nii.gz')
-    in_transfo = os.path.join(get_home(), 'connectivity',
-                              'affine.txt')
-    ret = script_runner.run('scil_apply_transform_to_hdf5.py', in_h5,
-                            in_target, in_transfo, 'decompose_lin.h5')
+def test_execution_connectivity(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_bundle = os.path.join(SCILPY_HOME, 'connectivity', 'bundle_all_1mm.trk')
+    in_atlas = os.path.join(SCILPY_HOME, 'connectivity',
+                            'endpoints_atlas.nii.gz')
+    ret = script_runner.run(
+        'scil_tractogram_segment_bundles_for_connectivity.py', in_bundle,
+        in_atlas, 'decompose.h5', '--min_length', '20', '--max_length', '200',
+        '--outlier_threshold', '0.5', '--loop_max_angle', '330',
+        '--curv_qb_distance', '10', '--processes', '1')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_apply_transform_to_image.py` & `scilpy-2.0.0/scripts/tests/test_rgb_convert.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,31 +1,26 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['bst.zip'])
+fetch_data(get_testing_files_dict(), keys=['others.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_apply_transform_to_image.py', '--help')
+    ret = script_runner.run('scil_rgb_convert.py', '--help')
     assert ret.success
 
 
-def test_execution_bst(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_model = os.path.join(get_home(), 'bst', 'template',
-                            'template0.nii.gz')
-    in_fa = os.path.join(get_home(), 'bst',
-                         'fa.nii.gz')
-    in_aff = os.path.join(get_home(), 'bst',
-                          'output0GenericAffine.mat')
-    ret = script_runner.run('scil_apply_transform_to_image.py',
-                            in_model, in_fa, in_aff,
-                            'template_lin.nii.gz', '--inverse')
+def test_execution_others(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_img = os.path.join(SCILPY_HOME, 'others',
+                          'rgb.nii.gz')
+    ret = script_runner.run('scil_rgb_convert.py',
+                            in_img, 'rgb_4D.nii.gz')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_apply_transform_to_surface.py` & `scilpy-2.0.0/scripts/tests/test_tractogram_compress.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,28 +1,26 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys='surface_vtk_fib.zip')
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_apply_transform_to_surface.py', '--help')
+    ret = script_runner.run('scil_tractogram_compress.py', '--help')
     assert ret.success
 
 
-def test_execution_surface_vtk_fib(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_surf = os.path.join(get_home(), 'surface_vtk_fib',
-                           'lhpialt.vtk')
-    in_aff = os.path.join(get_home(), 'surface_vtk_fib',
-                          'affine.txt')
-    ret = script_runner.run('scil_apply_transform_to_surface.py', in_surf,
-                            in_aff, 'lhpialt_lin.vtk')
+def test_execution_surface_vtk_fib(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_fib = os.path.join(SCILPY_HOME, 'surface_vtk_fib',
+                          'gyri_fanning.trk')
+    ret = script_runner.run('scil_tractogram_compress.py', in_fib,
+                            'gyri_fanning_c.trk', '-e', '0.1')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_apply_transform_to_tractogram.py` & `scilpy-2.0.0/scripts/tests/test_tracking_pft_maps_edit.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,34 +1,32 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['bst.zip'])
+fetch_data(get_testing_files_dict(), keys=['tracking.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_apply_transform_to_tractogram.py', '--help')
+    ret = script_runner.run('scil_tracking_pft_maps_edit.py', '--help')
     assert ret.success
 
 
-def test_execution_bst(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_model = os.path.join(get_home(), 'bst', 'template',
-                            'rpt_m.trk')
-    in_fa = os.path.join(get_home(), 'bst',
-                         'fa.nii.gz')
-    in_aff = os.path.join(get_home(), 'bst',
-                          'output0GenericAffine.mat')
-    in_warp = os.path.join(get_home(), 'bst',
-                           'output1InverseWarp.nii.gz')
-    ret = script_runner.run('scil_apply_transform_to_tractogram.py',
-                            in_model, in_fa, in_aff, 'rpt_m_warp.trk',
-                            '--inverse', '--in_deformation', in_warp,
-                            '--cut')
+def test_execution_tracking(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_include = os.path.join(SCILPY_HOME, 'tracking',
+                              'map_include.nii.gz')
+    in_exclude = os.path.join(SCILPY_HOME, 'tracking',
+                              'map_exclude.nii.gz')
+    in_mask = os.path.join(SCILPY_HOME, 'tracking',
+                           'seeding_mask.nii.gz')
+    ret = script_runner.run('scil_tracking_pft_maps_edit.py',
+                            in_include, in_exclude, in_mask,
+                            'map_include_corr.nii.gz',
+                            'map_exclude_corr.nii.gz')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_assign_custom_color_to_tractogram.py` & `scilpy-2.0.0/scripts/tests/test_tractogram_flip.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,30 +1,28 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['tractometry.zip'])
+fetch_data(get_testing_files_dict(), keys='surface_vtk_fib.zip')
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_assign_custom_color_to_tractogram.py',
-                            '--help')
+    ret = script_runner.run('scil_tractogram_flip.py', '--help')
     assert ret.success
 
 
-def test_execution_tractometry(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_bundle = os.path.join(get_home(), 'tractometry',
-                             'IFGWM.trk')
-    in_anat = os.path.join(get_home(), 'tractometry',
-                           'IFGWM_labels_map.nii.gz')
-    ret = script_runner.run('scil_assign_custom_color_to_tractogram.py',
-                            in_bundle, 'colored.trk', '--from_anatomy',
-                            in_anat)
+def test_execution_surface_vtk_fib(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_fib = os.path.join(SCILPY_HOME, 'surface_vtk_fib',
+                          'gyri_fanning.fib')
+    in_fa = os.path.join(SCILPY_HOME, 'surface_vtk_fib',
+                         'fa.nii.gz')
+    ret = script_runner.run('scil_tractogram_flip.py', in_fib,
+                            'gyri_fanning.tck', 'x', '--reference', in_fa)
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_assign_uniform_color_to_tractograms.py` & `scilpy-2.0.0/scripts/tests/test_tractogram_compute_density_map.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,28 +1,34 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['tractometry.zip'])
+fetch_data(get_testing_files_dict(), keys=['others.zip', 'tractometry.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_assign_uniform_color_to_tractograms.py',
+    ret = script_runner.run('scil_tractogram_compute_density_map.py',
                             '--help')
     assert ret.success
 
 
-def test_execution_tractometry(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_bundle = os.path.join(get_home(), 'tractometry',
-                             'IFGWM.trk')
-    ret = script_runner.run('scil_assign_uniform_color_to_tractograms.py',
-                            in_bundle, '--fill_color', '0x000000',
-                            '--out_tractogram', 'colored.trk')
+def test_execution_others(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_bundle = os.path.join(SCILPY_HOME, 'others', 'IFGWM.trk')
+    ret = script_runner.run('scil_tractogram_compute_density_map.py',
+                            in_bundle, 'binary.nii.gz', '--binary')
+    assert ret.success
+
+
+def test_execution_tractometry(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_bundle = os.path.join(SCILPY_HOME, 'tractometry', 'IFGWM.trk')
+    ret = script_runner.run('scil_tractogram_compute_density_map.py',
+                            in_bundle, 'IFGWM.nii.gz', '--binary')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compare_connectivity.py` & `scilpy-2.0.0/scripts/tests/test_connectivity_compare_populations.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,31 +1,29 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['connectivity.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compare_connectivity.py', '--help')
+    ret = script_runner.run('scil_connectivity_compare_populations.py',
+                            '--help')
     assert ret.success
 
 
-def test_execution_connectivity(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_1 = os.path.join(get_home(), 'connectivity',
-                        'sc.npy')
-    in_2 = os.path.join(get_home(), 'connectivity',
-                        'sc_norm.npy')
-    in_mask = os.path.join(get_home(), 'connectivity',
-                           'mask.npy')
-    ret = script_runner.run('scil_compare_connectivity.py', 'pval.npy',
-                            '--in_g1', in_1, '--in_g2', in_2,
+def test_execution_connectivity(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_1 = os.path.join(SCILPY_HOME, 'connectivity', 'sc.npy')
+    in_2 = os.path.join(SCILPY_HOME, 'connectivity', 'sc_norm.npy')
+    in_mask = os.path.join(SCILPY_HOME, 'connectivity', 'mask.npy')
+    ret = script_runner.run('scil_connectivity_compare_populations.py',
+                            'pval.npy', '--in_g1', in_1, '--in_g2', in_2,
                             '--filtering_mask', in_mask)
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compress_streamlines.py` & `scilpy-2.0.0/scripts/tests/test_tractogram_count_streamlines.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,26 +1,25 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys='surface_vtk_fib.zip')
+fetch_data(get_testing_files_dict(), keys=['others.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compress_streamlines.py', '--help')
+    ret = script_runner.run('scil_tractogram_count_streamlines.py', '--help')
     assert ret.success
 
 
-def test_execution_surface_vtk_fib(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_fib = os.path.join(get_home(), 'surface_vtk_fib',
-                          'gyri_fanning.trk')
-    ret = script_runner.run('scil_compress_streamlines.py', in_fib,
-                            'gyri_fanning_c.trk', '-e', '0.1')
+def test_execution_others(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_bundle = os.path.join(SCILPY_HOME, 'others',
+                             'IFGWM_sub.trk')
+    ret = script_runner.run('scil_tractogram_count_streamlines.py', in_bundle)
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_NODDI.py` & `scilpy-2.0.0/scripts/tests/test_NODDI_maps.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,35 +1,36 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['commit_amico.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_NODDI.py', '--help')
+    ret = script_runner.run('scil_NODDI_maps.py', '--help')
     assert ret.success
 
 
-def test_execution_commit_amico(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_dwi = os.path.join(get_home(), 'commit_amico',
+def test_execution_commit_amico(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_dwi = os.path.join(SCILPY_HOME, 'commit_amico',
                           'dwi.nii.gz')
-    in_bval = os.path.join(get_home(), 'commit_amico',
+    in_bval = os.path.join(SCILPY_HOME, 'commit_amico',
                            'dwi.bval')
-    in_bvec = os.path.join(get_home(), 'commit_amico',
+    in_bvec = os.path.join(SCILPY_HOME, 'commit_amico',
                            'dwi.bvec')
-    mask = os.path.join(get_home(), 'commit_amico',
+    mask = os.path.join(SCILPY_HOME, 'commit_amico',
                         'mask.nii.gz')
-    ret = script_runner.run('scil_compute_NODDI.py', in_dwi,
+    ret = script_runner.run('scil_NODDI_maps.py', in_dwi,
                             in_bval, in_bvec, '--mask', mask,
-                            '--out_dir', 'noddi', '--b_thr', '30',
+                            '--out_dir', 'noddi', '--tol', '30',
                             '--para_diff', '0.0017', '--iso_diff', '0.003',
                             '--lambda1', '0.5', '--lambda2', '0.001',
                             '--processes', '1')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_NODDI_priors.py` & `scilpy-2.0.0/scripts/tests/test_NODDI_priors.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,32 +1,36 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['commit_amico.zip'])
+fetch_data(get_testing_files_dict(), keys=['processing.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_NODDI_priors.py', '--help')
+    ret = script_runner.run('scil_NODDI_priors.py', '--help')
     assert ret.success
 
 
-def test_execution_commit_amico(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_fa = os.path.join(get_home(), 'commit_amico',
+def test_execution_commit_amico(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_fa = os.path.join(SCILPY_HOME, 'processing',
                          'fa.nii.gz')
-    in_ad = os.path.join(get_home(), 'commit_amico',
+    in_ad = os.path.join(SCILPY_HOME, 'processing',
                          'ad.nii.gz')
-    in_md = os.path.join(get_home(), 'commit_amico',
+    in_md = os.path.join(SCILPY_HOME, 'processing',
                          'md.nii.gz')
-    ret = script_runner.run('scil_compute_NODDI_priors.py', in_fa, in_ad, in_md,
-                            '--out_txt_1fiber', '1fiber.txt',
+    in_rd = os.path.join(SCILPY_HOME, 'processing',
+                         'rd.nii.gz')
+    ret = script_runner.run('scil_NODDI_priors.py', in_fa, in_ad, in_rd, in_md,
+                            '--out_txt_1fiber_para', '1fiber_para.txt',
+                            '--out_txt_1fiber_perp', '1fiber_perp.txt',
                             '--out_mask_1fiber', '1fiber.nii.gz',
                             '--out_txt_ventricles', 'ventricules.txt',
                             '--out_mask_ventricles', 'ventricules.nii.gz')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_asym_odf_metrics.py` & `scilpy-2.0.0/scripts/tests/test_surface_convert.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,49 +1,38 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['processing.zip'])
+fetch_data(get_testing_files_dict(), keys='surface_vtk_fib.zip')
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_asym_odf_metrics.py', '--help')
+    ret = script_runner.run('scil_surface_convert.py', '--help')
     assert ret.success
 
 
-def test_execution(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_fodf = os.path.join(get_home(), 'processing',
-                           'fodf_descoteaux07_sub_full.nii.gz')
-
-    # Using a low resolution sphere for peak extraction reduces process time
-    ret = script_runner.run('scil_compute_asym_odf_metrics.py', in_fodf,
-                            '--sphere', 'repulsion100')
+def test_execution_surface_vtk_fib(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_surf = os.path.join(SCILPY_HOME, 'surface_vtk_fib',
+                           'lhpialt.vtk')
+    ret = script_runner.run('scil_surface_convert.py', in_surf,
+                            'rhpialt.ply')
     assert ret.success
 
 
-def test_assert_not_all(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_fodf = os.path.join(get_home(), 'processing',
-                           'fodf_descoteaux07_sub_full.nii.gz')
-
-    ret = script_runner.run('scil_compute_asym_odf_metrics.py', in_fodf,
-                            '--not_all')
-    assert not ret.success
-
-
-def test_execution_not_all(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_fodf = os.path.join(get_home(), 'processing',
-                           'fodf_descoteaux07_sub_full.nii.gz')
-
-    ret = script_runner.run('scil_compute_asym_odf_metrics.py', in_fodf,
-                            '--not_all', '--cos_asym_map',
-                            'cos_asym_map.nii.gz', '-f')
+def test_execution_surface_vtk_xfrom(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_surf = os.path.join(SCILPY_HOME, 'surface_vtk_fib',
+                           'lh.pialt_xform')
+    x_form = os.path.join(SCILPY_HOME, 'surface_vtk_fib',
+                          'log.txt')
+    ret = script_runner.run('scil_surface_convert.py', in_surf,
+                            'lh.pialt_xform.vtk', '--xform', x_form,
+                            '--to_lps')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_bundle_mean_std.py` & `scilpy-2.0.0/scripts/tests/test_tracking_local_dev.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,28 +1,32 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['tractometry.zip'])
+fetch_data(get_testing_files_dict(), keys=['tracking.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_bundle_mean_std.py', '--help')
+    ret = script_runner.run('scil_tracking_local_dev.py',
+                            '--help')
     assert ret.success
 
 
-def test_execution_tractometry(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_bundle = os.path.join(get_home(), 'tractometry',
-                             'IFGWM.trk')
-    in_ref = os.path.join(get_home(), 'tractometry',
-                          'mni_masked.nii.gz')
-    ret = script_runner.run('scil_compute_bundle_mean_std.py', in_bundle,
-                            in_ref, '--density_weighting', '--include_dps')
+def test_execution_tracking_fodf(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_fodf = os.path.join(SCILPY_HOME, 'tracking',
+                           'fodf.nii.gz')
+    in_mask = os.path.join(SCILPY_HOME, 'tracking',
+                           'seeding_mask.nii.gz')
+    ret = script_runner.run('scil_tracking_local_dev.py', in_fodf,
+                            in_mask, in_mask, 'local_prob.trk', '--nt', '10',
+                            '--compress', '0.1', '--sh_basis', 'descoteaux07',
+                            '--min_length', '20', '--max_length', '200',
+                            '--save_seeds', '--rng_seed', '0')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_bundle_mean_std_per_point.py` & `scilpy-2.0.0/scripts/tests/test_plot_stats_per_point.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,34 +1,27 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['tractometry.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_bundle_mean_std_per_point.py',
-                            '--help')
+    ret = script_runner.run('scil_plot_stats_per_point.py', '--help')
     assert ret.success
 
 
-def test_execution_tractometry(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_bundle = os.path.join(get_home(), 'tractometry',
-                             'IFGWM.trk')
-    in_label = os.path.join(get_home(), 'tractometry',
-                            'IFGWM_labels_map.nii.gz')
-    in_ref = os.path.join(get_home(), 'tractometry',
-                          'mni_masked.nii.gz')
-    ret = script_runner.run('scil_compute_bundle_mean_std_per_point.py',
-                            in_bundle, in_label, in_ref,
-                            '--density_weighting',
-                            '--out_json', 'metric_label.json')
+def test_execution_tractometry(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_json = os.path.join(SCILPY_HOME, 'tractometry',
+                           'metric_label.json')
+    ret = script_runner.run('scil_plot_stats_per_point.py', in_json,
+                            'out/', '--stats_over_population')
 
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_bundle_volume.py` & `scilpy-2.0.0/scripts/tests/test_viz_volume_screenshot.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,25 +1,24 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['tractometry.zip'])
+fetch_data(get_testing_files_dict(), keys=['bst.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
-def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_bundle_volume.py', '--help')
+def test_screenshot(script_runner):
+    in_fa = os.path.join(SCILPY_HOME, 'bst', 'fa.nii.gz')
+
+    ret = script_runner.run("scil_viz_volume_screenshot.py", in_fa, 'fa.png')
     assert ret.success
 
 
-def test_execution_tractometry(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_bundle = os.path.join(get_home(), 'tractometry',
-                             'IFGWM.trk')
-    ret = script_runner.run('scil_compute_bundle_volume.py', in_bundle)
+def test_help_option(script_runner):
+    ret = script_runner.run("scil_viz_volume_screenshot.py", "--help")
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_bundle_volume_per_label.py` & `scilpy-2.0.0/scripts/tests/test_tractogram_resample_nb_points.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,27 +1,27 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['tractometry.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_bundle_volume_per_label.py',
-                            '--help')
+    ret = script_runner.run('scil_tractogram_resample_nb_points.py', '--help')
     assert ret.success
 
 
-def test_execution_tractometry(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_label_map = os.path.join(get_home(), 'tractometry',
-                                'IFGWM_labels_map.nii.gz')
-    ret = script_runner.run('scil_compute_bundle_volume_per_label.py',
-                            in_label_map, 'IFGWM')
+def test_execution_tractometry(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_bundle = os.path.join(SCILPY_HOME, 'tractometry',
+                             'IFGWM_uni_c.trk')
+    ret = script_runner.run('scil_tractogram_resample_nb_points.py',
+                            in_bundle, 'IFGWM_uni_c_10.trk',
+                            '--nb_pts_per_streamline', '10')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_bundle_voxel_label_map.py` & `scilpy-2.0.0/scripts/tests/test_bundle_label_map.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,30 +1,27 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['tractometry.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_bundle_voxel_label_map.py', '--help')
+    ret = script_runner.run('scil_bundle_label_map.py', '--help')
     assert ret.success
 
 
-def test_execution_tractometry(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_bundle = os.path.join(get_home(), 'tractometry',
-                             'IFGWM.trk')
-    in_centroid = os.path.join(get_home(), 'tractometry',
+def test_execution_tractometry(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_bundle = os.path.join(SCILPY_HOME, 'tractometry', 'IFGWM.trk')
+    in_centroid = os.path.join(SCILPY_HOME, 'tractometry',
                                'IFGWM_uni_c_10.trk')
-    ret = script_runner.run('scil_compute_bundle_voxel_label_map.py',
-                            in_bundle, in_centroid,
-                            'results_dir/',
-                            '--colormap', 'viridis')
+    ret = script_runner.run('scil_bundle_label_map.py', in_bundle, in_centroid,
+                            'results_dir/', '--colormap', 'viridis')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_centroid.py` & `scilpy-2.0.0/scripts/tests/test_bundle_compute_centroid.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,26 +1,25 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['tractometry.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_centroid.py', '--help')
+    ret = script_runner.run('scil_bundle_compute_centroid.py', '--help')
     assert ret.success
 
 
-def test_execution_tractometry(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_bundle = os.path.join(get_home(), 'tractometry',
-                             'IFGWM_uni.trk')
-    ret = script_runner.run('scil_compute_centroid.py',
+def test_execution_tractometry(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_bundle = os.path.join(SCILPY_HOME, 'tractometry', 'IFGWM_uni.trk')
+    ret = script_runner.run('scil_bundle_compute_centroid.py',
                             in_bundle, 'IFGWM_uni_c.trk')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_connectivity.py` & `scilpy-2.0.0/scripts/tests/test_connectivity_math.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,38 +1,46 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['connectivity.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_connectivity.py', '--help')
+    ret = script_runner.run('scil_connectivity_math.py', '--help')
+    assert ret.success
+
+
+def test_execution_connectivity_div(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_sc = os.path.join(SCILPY_HOME, 'connectivity',
+                         'sc.npy')
+    in_vol = os.path.join(SCILPY_HOME, 'connectivity',
+                          'vol.npy')
+    ret = script_runner.run('scil_connectivity_math.py', 'division',
+                            in_sc, in_vol, 'sc_norm_vol.npy')
+    assert ret.success
+
+
+def test_execution_connectivity_add(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_sc = os.path.join(SCILPY_HOME, 'connectivity',
+                         'sc.npy')
+    ret = script_runner.run('scil_connectivity_math.py', 'addition',
+                            in_sc, '10', 'sc_add_10.npy')
     assert ret.success
 
 
-def test_execution_connectivity(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_h5 = os.path.join(get_home(), 'connectivity',
-                         'decompose.h5')
-    in_atlas = os.path.join(get_home(), 'connectivity',
-                            'endpoints_atlas.nii.gz')
-    in_avg = os.path.join(get_home(), 'connectivity',
-                          'avg_density_maps/')
-    in_afd = os.path.join(get_home(), 'connectivity',
-                          'afd_max.nii.gz')
-    ret = script_runner.run('scil_compute_connectivity.py', in_h5,
-                            in_atlas, '--volume', 'vol.npy',
-                            '--streamline_count', 'sc.npy',
-                            '--length', 'len.npy',
-                            '--similarity', in_avg, 'sim.npy',
-                            '--metrics', in_afd, 'afd_max.npy',
-                            '--density_weighting', '--no_self_connection',
-                            '--processes', '1')
+def test_execution_connectivity_lower_threshold(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_sc = os.path.join(SCILPY_HOME, 'connectivity',
+                         'sc.npy')
+    ret = script_runner.run('scil_connectivity_math.py', 'lower_threshold',
+                            in_sc, '5', 'sc_lower_threshold.npy')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_divide.py` & `scilpy-2.0.0/scripts/tests/test_btensor_metrics.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,118 +1,128 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 fetch_data(get_testing_files_dict(), keys=['btensor_testdata.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_divide.py', '--help')
+    ret = script_runner.run('scil_btensor_metrics.py', '--help')
     assert ret.success
 
 
-def test_nb_btensors_check(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_dwi_lin = os.path.join(get_home(), 'btensor_testdata',
+def test_nb_btensors_check(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_dwi_lin = os.path.join(SCILPY_HOME, 'btensor_testdata',
                               'dwi_linear.nii.gz')
-    in_bval_lin = os.path.join(get_home(), 'btensor_testdata',
+    in_bval_lin = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'linear.bvals')
-    in_bvec_lin = os.path.join(get_home(), 'btensor_testdata',
+    in_bvec_lin = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'linear.bvecs')
-    in_dwi_plan = os.path.join(get_home(), 'btensor_testdata',
+    in_dwi_plan = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'dwi_planar.nii.gz')
-    in_bval_plan = os.path.join(get_home(), 'btensor_testdata',
+    in_bval_plan = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                 'planar.bvals')
-    in_bvec_plan = os.path.join(get_home(), 'btensor_testdata',
+    in_bvec_plan = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                 'planar.bvecs')
-    fa = os.path.join(get_home(), 'btensor',
+    fa = os.path.join(SCILPY_HOME, 'btensor',
                       'fa.nii.gz')
 
-    ret = script_runner.run('scil_compute_divide.py', '--in_dwis',
+    ret = script_runner.run('scil_btensor_metrics.py', '--in_dwis',
                             in_dwi_lin, '--in_bvals', in_bval_lin,
                             '--in_bvecs', in_bvec_lin, '--in_bdeltas', '1',
                             '--fa', fa, '--do_weight_bvals',
                             '--do_weight_pa', '--do_multiple_s0',
                             '--processes', '1', '-f')
     assert (not ret.success)
 
-    ret = script_runner.run('scil_compute_divide.py', '--in_dwis',
+    ret = script_runner.run('scil_btensor_metrics.py', '--in_dwis',
                             in_dwi_lin, in_dwi_plan, '--in_bvals', in_bval_lin,
                             in_bval_plan, '--in_bvecs', in_bvec_lin,
                             in_bvec_plan, '--in_bdeltas', '1', '1',
                             '--fa', fa, '--do_weight_bvals',
                             '--do_weight_pa', '--do_multiple_s0',
                             '--processes', '1', '-f')
     assert (not ret.success)
 
 
-def test_inputs_check(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_dwi_lin = os.path.join(get_home(), 'btensor_testdata',
+def test_inputs_check(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_dwi_lin = os.path.join(SCILPY_HOME, 'btensor_testdata',
                               'dwi_linear.nii.gz')
-    in_bval_lin = os.path.join(get_home(), 'btensor_testdata',
+    in_bval_lin = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'linear.bvals')
-    in_bvec_lin = os.path.join(get_home(), 'btensor_testdata',
+    in_bvec_lin = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'linear.bvecs')
-    in_dwi_plan = os.path.join(get_home(), 'btensor_testdata',
+    in_dwi_plan = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'dwi_planar.nii.gz')
-    in_bval_plan = os.path.join(get_home(), 'btensor_testdata',
+    in_bval_plan = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                 'planar.bvals')
-    in_bvec_plan = os.path.join(get_home(), 'btensor_testdata',
+    in_bvec_plan = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                 'planar.bvecs')
-    fa = os.path.join(get_home(), 'btensor',
+    fa = os.path.join(SCILPY_HOME, 'btensor',
                       'fa.nii.gz')
 
-    ret = script_runner.run('scil_compute_divide.py', '--in_dwis',
+    ret = script_runner.run('scil_btensor_metrics.py', '--in_dwis',
                             in_dwi_lin, in_dwi_plan, '--in_bvals', in_bval_lin,
                             '--in_bvecs', in_bvec_lin, '--in_bdeltas', '1',
                             '--fa', fa, '--do_weight_bvals',
                             '--do_weight_pa', '--do_multiple_s0',
                             '--processes', '1', '-f')
     assert (not ret.success)
 
-    ret = script_runner.run('scil_compute_divide.py', '--in_dwis',
+    ret = script_runner.run('scil_btensor_metrics.py', '--in_dwis',
                             in_dwi_lin, in_dwi_plan, '--in_bvals',
                             in_bval_lin, in_bval_plan, '--in_bvecs',
                             in_bvec_lin, in_bvec_plan, '--in_bdeltas', '1',
                             '-0.5', '0', '--fa', fa, '--do_weight_bvals',
                             '--do_weight_pa', '--do_multiple_s0',
                             '--processes', '1', '-f')
     assert (not ret.success)
 
+    ret = script_runner.run('scil_btensor_metrics.py', '--in_dwis',
+                            in_dwi_lin, in_dwi_plan, '--in_bvals',
+                            in_bval_lin, in_bval_plan, '--in_bvecs',
+                            in_bvec_lin, in_bvec_plan, '--in_bdeltas', '1',
+                            '-0.5', '--op', fa, '--do_weight_bvals',
+                            '--do_weight_pa', '--do_multiple_s0',
+                            '--processes', '1', '-f')
+    assert (not ret.success)
+
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_dwi_lin = os.path.join(get_home(), 'btensor_testdata',
+def test_execution_processing(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_dwi_lin = os.path.join(SCILPY_HOME, 'btensor_testdata',
                               'dwi_linear.nii.gz')
-    in_bval_lin = os.path.join(get_home(), 'btensor_testdata',
+    in_bval_lin = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'linear.bvals')
-    in_bvec_lin = os.path.join(get_home(), 'btensor_testdata',
+    in_bvec_lin = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'linear.bvecs')
-    in_dwi_plan = os.path.join(get_home(), 'btensor_testdata',
+    in_dwi_plan = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'dwi_planar.nii.gz')
-    in_bval_plan = os.path.join(get_home(), 'btensor_testdata',
+    in_bval_plan = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                 'planar.bvals')
-    in_bvec_plan = os.path.join(get_home(), 'btensor_testdata',
+    in_bvec_plan = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                 'planar.bvecs')
-    in_dwi_sph = os.path.join(get_home(), 'btensor_testdata',
+    in_dwi_sph = os.path.join(SCILPY_HOME, 'btensor_testdata',
                               'dwi_spherical.nii.gz')
-    in_bval_sph = os.path.join(get_home(), 'btensor_testdata',
+    in_bval_sph = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'spherical.bvals')
-    in_bvec_sph = os.path.join(get_home(), 'btensor_testdata',
+    in_bvec_sph = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'spherical.bvecs')
-    fa = os.path.join(get_home(), 'btensor_testdata',
+    fa = os.path.join(SCILPY_HOME, 'btensor_testdata',
                       'fa.nii.gz')
 
-    ret = script_runner.run('scil_compute_divide.py', '--in_dwis',
+    ret = script_runner.run('scil_btensor_metrics.py', '--in_dwis',
                             in_dwi_lin, in_dwi_plan, in_dwi_sph,
                             '--in_bvals', in_bval_lin, in_bval_plan,
                             in_bval_sph, '--in_bvecs', in_bvec_lin,
                             in_bvec_plan, in_bvec_sph, '--in_bdeltas',
                             '1', '-0.5', '0', '--fa', fa, '--do_weight_bvals',
                             '--do_weight_pa', '--do_multiple_s0',
                             '--processes', '1', '-f')
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_dti_metrics.py` & `scilpy-2.0.0/scripts/tests/test_dwi_to_sh.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,31 +1,36 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['processing.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_dti_metrics.py', '--help')
+    ret = script_runner.run('scil_dwi_to_sh.py', '--help')
     assert ret.success
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_dwi = os.path.join(get_home(), 'processing',
-                          'dwi_crop_1000.nii.gz')
-    in_bval = os.path.join(get_home(), 'processing',
-                           '1000.bval')
-    in_bvec = os.path.join(get_home(), 'processing',
-                           '1000.bvec')
-    ret = script_runner.run('scil_compute_dti_metrics.py', in_dwi,
-                            in_bval, in_bvec, '--not_all', '--fa', 'fa.nii.gz',
-                            '--md', 'md.nii.gz', '--ad', 'ad.nii.gz',
-                            '--rd', 'rd.nii.gz')
+def test_execution_processing(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_dwi = os.path.join(SCILPY_HOME, 'processing',
+                          'dwi_crop_3000.nii.gz')
+    in_bval = os.path.join(SCILPY_HOME, 'processing',
+                           '3000.bval')
+    in_bvec = os.path.join(SCILPY_HOME, 'processing',
+                           '3000.bvec')
+    ret = script_runner.run('scil_dwi_to_sh.py', in_dwi, in_bval,
+                            in_bvec, 'sh_1000.nii.gz')
     assert ret.success
+
+    # Test wrong b0. Current minimal b-value is 5.
+    ret = script_runner.run('scil_dwi_to_sh.py', in_dwi, in_bval,
+                            in_bvec, 'sh_1000.nii.gz', '--b0_threshold', '1',
+                            '-f')
+    assert not ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_endpoints_map.py` & `scilpy-2.0.0/scripts/tests/test_bundle_compute_endpoints_map.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,27 +1,26 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['tractometry.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_endpoints_map.py', '--help')
+    ret = script_runner.run('scil_bundle_compute_endpoints_map.py', '--help')
     assert ret.success
 
 
-def test_execution_tractometry(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_bundle = os.path.join(get_home(), 'tractometry',
-                             'IFGWM_uni.trk')
-    ret = script_runner.run('scil_compute_endpoints_map.py', in_bundle,
+def test_execution_tractometry(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_bundle = os.path.join(SCILPY_HOME, 'tractometry', 'IFGWM_uni.trk')
+    ret = script_runner.run('scil_bundle_compute_endpoints_map.py', in_bundle,
                             'head.nii.gz', 'tail.nii.gz', '--binary')
 
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_fodf_max_in_ventricles.py` & `scilpy-2.0.0/scripts/tests/test_sh_to_rish.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,29 +1,25 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['processing.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_fodf_max_in_ventricles.py', '--help')
+    ret = script_runner.run('scil_sh_to_rish.py', '--help')
     assert ret.success
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_fodf = os.path.join(get_home(), 'processing',
-                           'fodf.nii.gz')
-    in_fa = os.path.join(get_home(), 'processing',
-                         'fa.nii.gz')
-    in_md = os.path.join(get_home(), 'processing',
-                         'md.nii.gz')
-    ret = script_runner.run('scil_compute_fodf_max_in_ventricles.py', in_fodf,
-                            in_fa, in_md, '--sh_basis', 'tournier07')
+def test_execution_processing(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_sh = os.path.join(SCILPY_HOME, 'processing',
+                          'sh.nii.gz')
+    ret = script_runner.run('scil_sh_to_rish.py', in_sh, 'rish.nii.gz')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_fodf_metrics.py` & `scilpy-2.0.0/scripts/tests/test_fodf_metrics.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,29 +1,30 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['processing.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_fodf_metrics.py', '--help')
+    ret = script_runner.run('scil_fodf_metrics.py', '--help')
     assert ret.success
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_fodf = os.path.join(get_home(), 'processing',
+def test_execution_processing(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_fodf = os.path.join(SCILPY_HOME, 'processing',
                            'fodf_descoteaux07.nii.gz')
-    ret = script_runner.run('scil_compute_fodf_metrics.py', in_fodf, '--not_al',
+    ret = script_runner.run('scil_fodf_metrics.py', in_fodf, '--not_al',
                             '--peaks', 'peaks.nii.gz',
                             '--afd_max', 'afd_max.nii.gz',
                             '--afd_total', 'afd_tot.nii.gz',
                             '--afd_sum', 'afd_sum.nii.gz',
                             '--nufo', 'nufo.nii.gz', '--processes', '1')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_freewater.py` & `scilpy-2.0.0/scripts/tests/test_freewater_maps.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,36 +1,37 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['commit_amico.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_freewater.py', '--help')
+    ret = script_runner.run('scil_freewater_maps.py', '--help')
     assert ret.success
 
 
-def test_execution_commit_amico(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_dwi = os.path.join(get_home(), 'commit_amico',
+def test_execution_commit_amico(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_dwi = os.path.join(SCILPY_HOME, 'commit_amico',
                           'dwi.nii.gz')
-    in_bval = os.path.join(get_home(), 'commit_amico',
+    in_bval = os.path.join(SCILPY_HOME, 'commit_amico',
                            'dwi.bval')
-    in_bvec = os.path.join(get_home(), 'commit_amico',
+    in_bvec = os.path.join(SCILPY_HOME, 'commit_amico',
                            'dwi.bvec')
-    mask = os.path.join(get_home(), 'commit_amico',
+    mask = os.path.join(SCILPY_HOME, 'commit_amico',
                         'mask.nii.gz')
-    ret = script_runner.run('scil_compute_freewater.py', in_dwi,
+    ret = script_runner.run('scil_freewater_maps.py', in_dwi,
                             in_bval, in_bvec, '--mask', mask,
                             '--out_dir', 'freewater', '--b_thr', '30',
                             '--para_diff', '0.0015',
                             '--perp_diff_min', '0.0001',
                             '--perp_diff_max', '0.0007',
                             '--lambda1', '0.0', '--lambda2', '0.001',
                             '--processes', '1')
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_hdf5_average_density_map.py` & `scilpy-2.0.0/scripts/tests/test_connectivity_hdf5_average_density_map.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,28 +1,27 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['connectivity.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_hdf5_average_density_map.py',
+    ret = script_runner.run('scil_connectivity_hdf5_average_density_map.py',
                             '--help')
     assert ret.success
 
 
-def test_execution_connectivity(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_h5 = os.path.join(get_home(), 'connectivity',
-                            'decompose.h5')
-    ret = script_runner.run('scil_compute_hdf5_average_density_map.py',
+def test_execution_connectivity(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_h5 = os.path.join(SCILPY_HOME, 'connectivity', 'decompose.h5')
+    ret = script_runner.run('scil_connectivity_hdf5_average_density_map.py',
                             in_h5, 'avg_density_maps/', '--binary',
                             '--processes', '1')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_kurtosis_metrics.py` & `scilpy-2.0.0/scripts/tests/test_bundle_mean_fixel_bingham_metric.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,33 +1,33 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['processing.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_kurtosis_metrics.py', '--help')
+    ret = script_runner.run(
+        'scil_bundle_mean_fixel_bingham_metric.py', '--help')
+
     assert ret.success
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_dwi = os.path.join(get_home(), 'processing',
-                          'dwi_crop.nii.gz')
-    in_bval = os.path.join(get_home(), 'processing',
-                           'dwi.bval')
-    in_bvec = os.path.join(get_home(), 'processing',
-                           'dwi.bvec')
-    ret = script_runner.run('scil_compute_kurtosis_metrics.py', in_dwi,
-                            in_bval, in_bvec, '--not_all',
-                            '--dki_fa', 'dki_fa.nii.gz',
-                            '--dki_md', 'dki_md.nii.gz',
-                            '--dki_rd', 'dki_rd.nii.gz',
-                            '--dki_ad', 'dki_ad.nii.gz')
+def test_execution_processing(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_bingham = os.path.join(SCILPY_HOME, 'processing', 'fodf_bingham.nii.gz')
+    in_metric = os.path.join(SCILPY_HOME, 'processing', 'fd.nii.gz')
+    in_bundles = os.path.join(SCILPY_HOME, 'processing', 'tracking.trk')
+
+    ret = script_runner.run(
+        'scil_bundle_mean_fixel_bingham_metric.py',
+        in_bundles, in_bingham, in_metric,
+        'fixel_mean_fd.nii.gz', '--length_weighting')
+
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_lobe_specific_fodf_metrics.py` & `scilpy-2.0.0/scripts/tests/test_fodf_to_bingham.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,56 +1,51 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['processing.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_lobe_specific_fodf_metrics.py',
+    ret = script_runner.run('scil_fodf_to_bingham.py',
                             '--help')
     assert ret.success
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_bingham = os.path.join(get_home(), 'processing',
-                              'fodf_bingham.nii.gz')
-
-    ret = script_runner.run('scil_compute_lobe_specific_fodf_metrics.py',
-                            in_bingham, '--nbr_integration_steps', '10',
+def test_execution_processing(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_fodf = os.path.join(SCILPY_HOME, 'processing',
+                           'fodf_descoteaux07.nii.gz')
+    ret = script_runner.run('scil_fodf_to_bingham.py',
+                            in_fodf, 'bingham.nii.gz',
+                            '--max_lobes', '1',
+                            '--at', '0.0',
+                            '--rt', '0.1',
+                            '--min_sep_angle', '25.',
+                            '--max_fit_angle', '15.',
                             '--processes', '1')
-
     assert ret.success
 
 
-def test_execution_processing_mask(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_bingham = os.path.join(get_home(), 'processing',
-                              'fodf_bingham.nii.gz')
-    in_mask = os.path.join(get_home(), 'processing',
+def test_execution_processing_mask(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_fodf = os.path.join(SCILPY_HOME, 'processing',
+                           'fodf_descoteaux07.nii.gz')
+    in_mask = os.path.join(SCILPY_HOME, 'processing',
                            'seed.nii.gz')
-
-    ret = script_runner.run('scil_compute_lobe_specific_fodf_metrics.py',
-                            in_bingham, '--nbr_integration_steps', '10',
-                            '--processes', '1', '--mask', in_mask, '-f')
-
-    assert ret.success
-
-
-def test_execution_processing_not_all(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_bingham = os.path.join(get_home(), 'processing',
-                              'fodf_bingham.nii.gz')
-
-    ret = script_runner.run('scil_compute_lobe_specific_fodf_metrics.py',
-                            in_bingham, '--nbr_integration_steps', '10',
-                            '--processes', '1', '--not_all', '--out_fs',
-                            'fs.nii.gz', '-f')
-
+    ret = script_runner.run('scil_fodf_to_bingham.py',
+                            in_fodf, 'bingham.nii.gz',
+                            '--max_lobes', '1',
+                            '--at', '0.0',
+                            '--rt', '0.1',
+                            '--min_sep_angle', '25.',
+                            '--max_fit_angle', '15.',
+                            '--processes', '1',
+                            '--mask', in_mask, '-f')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_local_tracking.py` & `scilpy-2.0.0/scripts/tests/test_tractogram_compute_TODI.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,61 +1,44 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['tracking.zip'])
+fetch_data(get_testing_files_dict(), keys=['bst.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_local_tracking.py',
-                            '--help')
+    ret = script_runner.run('scil_tractogram_compute_TODI.py', '--help')
     assert ret.success
 
 
-def test_execution_tracking_fodf(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_fodf = os.path.join(get_home(), 'tracking',
-                           'fodf.nii.gz')
-    in_mask = os.path.join(get_home(), 'tracking',
-                           'seeding_mask.nii.gz')
-
-    ret = script_runner.run('scil_compute_local_tracking.py', in_fodf,
-                            in_mask, in_mask, 'local_prob.trk', '--nt', '1000',
-                            '--compress', '0.1', '--sh_basis', 'descoteaux07',
-                            '--min_length', '20', '--max_length', '200')
+def test_execution_bst(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_bundle = os.path.join(SCILPY_HOME, 'bst', 'rpt_m_warp.trk')
+    in_mask = os.path.join(SCILPY_HOME, 'bst', 'mask.nii.gz')
+
+    ret = script_runner.run('scil_tractogram_compute_TODI.py', in_bundle,
+                            '--mask', in_mask,
+                            '--out_mask', 'todi_mask.nii.gz',
+                            '--out_tdi', 'tdi.nii.gz',
+                            '--out_todi_sh', 'todi_sh.nii.gz',
+                            '--out_todi_sf', 'todi_sf.nii.gz',
+                            '--sh_order', '6',
+                            '--normalize_per_voxel', '--smooth_todi',
+                            '--sh_basis', 'descoteaux07')
     assert ret.success
 
 
-def test_execution_tracking_fodf_no_compression(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_fodf = os.path.join(get_home(), 'tracking',
-                           'fodf.nii.gz')
-    in_mask = os.path.join(get_home(), 'tracking',
-                           'seeding_mask.nii.gz')
-
-    ret = script_runner.run('scil_compute_local_tracking.py', in_fodf,
-                            in_mask, in_mask, 'local_prob2.trk',
-                            '--nt', '100', '--sh_basis', 'descoteaux07',
-                            '--max_length', '200')
-
-    assert ret.success
-
+def test_execution_asym(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_bundle = os.path.join(SCILPY_HOME, 'bst', 'rpt_m_warp.trk')
+    ret = script_runner.run('scil_tractogram_compute_TODI.py', in_bundle,
+                            '--out_todi_sh', 'atodi_sh_8.nii.gz',
+                            '--asymmetric', '--n_steps', '2')
 
-def test_execution_tracking_peaks(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_peaks = os.path.join(get_home(), 'tracking',
-                            'peaks.nii.gz')
-    in_mask = os.path.join(get_home(), 'tracking',
-                           'seeding_mask.nii.gz')
-    ret = script_runner.run('scil_compute_local_tracking.py', in_peaks,
-                            in_mask, in_mask, 'local_eudx.trk', '--nt', '1000',
-                            '--compress', '0.1', '--sh_basis', 'descoteaux07',
-                            '--min_length', '20', '--max_length', '200',
-                            '--algo', 'eudx')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_local_tracking_dev.py` & `scilpy-2.0.0/scripts/tests/test_tractogram_convert.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,32 +1,28 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['tracking.zip'])
+fetch_data(get_testing_files_dict(), keys='surface_vtk_fib.zip')
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_local_tracking_dev.py',
-                            '--help')
+    ret = script_runner.run('scil_tractogram_convert.py', '--help')
     assert ret.success
 
 
-def test_execution_tracking_fodf(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_fodf = os.path.join(get_home(), 'tracking',
-                           'fodf.nii.gz')
-    in_mask = os.path.join(get_home(), 'tracking',
-                           'seeding_mask.nii.gz')
-    ret = script_runner.run('scil_compute_local_tracking_dev.py', in_fodf,
-                            in_mask, in_mask, 'local_prob.trk', '--nt', '10',
-                            '--compress', '0.1', '--sh_basis', 'descoteaux07',
-                            '--min_length', '20', '--max_length', '200',
-                            '--save_seeds', '--rng_seed', '0')
+def test_execution_surface_vtk_fib(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_fib = os.path.join(SCILPY_HOME, 'surface_vtk_fib',
+                          'gyri_fanning.fib')
+    in_fa = os.path.join(SCILPY_HOME, 'surface_vtk_fib',
+                         'fa.nii.gz')
+    ret = script_runner.run('scil_tractogram_convert.py', in_fib,
+                            'gyri_fanning.trk', '--reference', in_fa)
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_maps_for_particle_filter_tracking.py` & `scilpy-2.0.0/scripts/tests/test_tractogram_smooth.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,31 +1,27 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['tracking.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_maps_for_particle_filter_tracking.py',
-                            '--help')
+    ret = script_runner.run('scil_tractogram_smooth.py', '--help')
     assert ret.success
 
 
-def test_execution_tracking(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_wm = os.path.join(get_home(), 'tracking',
-                         'map_wm.nii.gz')
-    in_gm = os.path.join(get_home(), 'tracking',
-                         'map_gm.nii.gz')
-    in_csf = os.path.join(get_home(), 'tracking',
-                          'map_csf.nii.gz')
-    ret = script_runner.run('scil_compute_maps_for_particle_filter_tracking.py',
-                            in_wm, in_gm, in_csf)
+def test_execution_tracking(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tracto = os.path.join(SCILPY_HOME, 'tracking',
+                             'union_shuffle_sub.trk')
+    ret = script_runner.run('scil_tractogram_smooth.py', in_tracto,
+                            'union_shuffle_sub_smooth.trk', '--gaussian', '10',
+                            '--compress', '0.05')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_mean_fixel_afd_from_bundles.py` & `scilpy-2.0.0/scripts/tests/test_gradients_round_bvals.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,30 +1,27 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['processing.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_mean_fixel_afd_from_bundles.py',
+    ret = script_runner.run('scil_gradients_round_bvals.py',
                             '--help')
     assert ret.success
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_tracking = os.path.join(get_home(), 'processing',
-                               'tracking.trk')
-    in_fodf = os.path.join(get_home(), 'processing',
-                           'fodf_descoteaux07.nii.gz')
-    ret = script_runner.run('scil_compute_mean_fixel_afd_from_bundles.py',
-                            in_tracking, in_fodf,
-                            'afd_test.nii.gz',
-                            '--sh_basis', 'descoteaux07', '--length_weighting')
+def test_execution_processing(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_bval = os.path.join(SCILPY_HOME, 'processing', '1000.bval')
+    ret = script_runner.run('scil_gradients_round_bvals.py',
+                            in_bval, '0', '1000', '1000_resample.b', "20",
+                            "-v")
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_mean_fixel_afd_from_hdf5.py` & `scilpy-2.0.0/scripts/tests/test_bundle_mean_fixel_afd_from_hdf5.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,31 +1,29 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['connectivity.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_mean_fixel_afd_from_hdf5.py',
+    ret = script_runner.run('scil_bundle_mean_fixel_afd_from_hdf5.py',
                             '--help')
     assert ret.success
 
 
-def test_execution_connectivity(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_h5 = os.path.join(get_home(), 'connectivity',
-                         'decompose.h5')
-    in_fodf = os.path.join(get_home(), 'connectivity',
-                           'fodf.nii.gz')
-    ret = script_runner.run('scil_compute_mean_fixel_afd_from_hdf5.py',
+def test_execution_connectivity(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_h5 = os.path.join(SCILPY_HOME, 'connectivity', 'decompose.h5')
+    in_fodf = os.path.join(SCILPY_HOME, 'connectivity', 'fodf.nii.gz')
+    ret = script_runner.run('scil_bundle_mean_fixel_afd_from_hdf5.py',
                             in_h5, in_fodf, 'decompose_afd.nii.gz',
                             '--length_weighting', '--sh_basis', 'descoteaux07',
                             '--processes', '1')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_mean_fixel_lobe_metric_from_bundles.py` & `scilpy-2.0.0/scripts/tests/test_dwi_detect_volume_outliers.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,35 +1,35 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['processing.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run(
-        'scil_compute_mean_fixel_lobe_metric_from_bundles.py',
-        '--help')
-
+    ret = script_runner.run('scil_dwi_detect_volume_outliers.py', '--help')
     assert ret.success
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_bingham = os.path.join(get_home(), 'processing',
-                              'fodf_bingham.nii.gz')
-    in_metric = os.path.join(get_home(), 'processing',
-                             'fd.nii.gz')
-    in_bundles = os.path.join(get_home(), 'processing', 'tracking.trk')
-
-    ret = script_runner.run(
-        'scil_compute_mean_fixel_lobe_metric_from_bundles.py',
-        in_bundles, in_bingham, in_metric,
-        'fixel_mean_fd.nii.gz', '--length_weighting')
-
+def test_execution(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_dwi = os.path.join(SCILPY_HOME, 'processing',
+                          'dwi_crop.nii.gz')
+    in_bval = os.path.join(SCILPY_HOME, 'processing',
+                           'dwi.bval')
+    in_bvec = os.path.join(SCILPY_HOME, 'processing',
+                           'dwi.bvec')
+    ret = script_runner.run('scil_dwi_detect_volume_outliers.py', in_dwi,
+                            in_bval, in_bvec, '-v')
     assert ret.success
+
+    # Test wrong b0. Current minimal b-value is 5.
+    ret = script_runner.run('scil_dwi_detect_volume_outliers.py', in_dwi,
+                            in_bval, in_bvec, '--b0_threshold', '1')
+    assert not ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_mean_frf.py` & `scilpy-2.0.0/scripts/tests/test_connectivity_pairwise_agreement.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,24 +1,27 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['processing.zip'])
+fetch_data(get_testing_files_dict(), keys=['connectivity.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_mean_frf.py', '--help')
+    ret = script_runner.run('scil_connectivity_pairwise_agreement.py',
+                            '--help')
     assert ret.success
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_frf = os.path.join(get_home(), 'processing',
-                          'frf.txt')
-    ret = script_runner.run('scil_compute_mean_frf.py', in_frf, 'mfrf.txt')
+def test_execution_connectivity(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_sc = os.path.join(SCILPY_HOME, 'connectivity', 'sc_norm.npy')
+    in_len = os.path.join(SCILPY_HOME, 'connectivity', 'len.npy')
+    ret = script_runner.run('scil_connectivity_pairwise_agreement.py', in_sc,
+                            in_len, 'diff.json', '--single_compare', in_sc)
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_memsmt_fodf.py` & `scilpy-2.0.0/scripts/tests/test_fodf_memsmt.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,93 +1,94 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 fetch_data(get_testing_files_dict(), keys=['btensor_testdata.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_memsmt_fodf.py', '--help')
+    ret = script_runner.run('scil_fodf_memsmt.py', '--help')
     assert ret.success
 
 
-def test_inputs_check(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_dwi_lin = os.path.join(get_home(), 'btensor_testdata',
+def test_inputs_check(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_dwi_lin = os.path.join(SCILPY_HOME, 'btensor_testdata',
                               'dwi_linear.nii.gz')
-    in_bval_lin = os.path.join(get_home(), 'btensor_testdata',
+    in_bval_lin = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'linear.bvals')
-    in_bvec_lin = os.path.join(get_home(), 'btensor_testdata',
+    in_bvec_lin = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'linear.bvecs')
-    in_dwi_plan = os.path.join(get_home(), 'btensor_testdata',
+    in_dwi_plan = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'dwi_planar.nii.gz')
-    in_bval_plan = os.path.join(get_home(), 'btensor_testdata',
+    in_bval_plan = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                 'planar.bvals')
-    in_bvec_plan = os.path.join(get_home(), 'btensor_testdata',
+    in_bvec_plan = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                 'planar.bvecs')
-    in_wm_frf = os.path.join(get_home(), 'btensor_testdata',
+    in_wm_frf = os.path.join(SCILPY_HOME, 'btensor_testdata',
                              'wm_frf.txt')
-    in_gm_frf = os.path.join(get_home(), 'btensor_testdata',
+    in_gm_frf = os.path.join(SCILPY_HOME, 'btensor_testdata',
                              'gm_frf.txt')
-    in_csf_frf = os.path.join(get_home(), 'btensor_testdata',
+    in_csf_frf = os.path.join(SCILPY_HOME, 'btensor_testdata',
                               'csf_frf.txt')
 
-    ret = script_runner.run('scil_compute_memsmt_fodf.py', in_wm_frf,
+    ret = script_runner.run('scil_fodf_memsmt.py', in_wm_frf,
                             in_gm_frf, in_csf_frf, '--in_dwis',
                             in_dwi_lin, in_dwi_plan, '--in_bvals',
                             in_bval_lin, '--in_bvecs', in_bvec_lin,
                             '--in_bdeltas', '1',
                             '--wm_out_fODF', 'wm_fodf.nii.gz',
                             '--gm_out_fODF', 'gm_fodf.nii.gz',
                             '--csf_out_fODF', 'csf_fodf.nii.gz', '--vf',
                             'vf.nii.gz', '--sh_order', '4', '--sh_basis',
                             'tournier07', '--processes', '1', '-f')
     assert (not ret.success)
 
-    ret = script_runner.run('scil_compute_memsmt_fodf.py', in_wm_frf,
+    ret = script_runner.run('scil_fodf_memsmt.py', in_wm_frf,
                             in_gm_frf, in_csf_frf, '--in_dwis',
                             in_dwi_lin, in_dwi_plan, '--in_bvals',
                             in_bval_lin, in_bval_plan, '--in_bvecs',
                             in_bvec_lin, in_bvec_plan, '--in_bdeltas',
                             '1', '-0.5', '0',
                             '--wm_out_fODF', 'wm_fodf.nii.gz',
                             '--gm_out_fODF', 'gm_fodf.nii.gz',
                             '--csf_out_fODF', 'csf_fodf.nii.gz', '--vf',
                             'vf.nii.gz', '--sh_order', '4', '--sh_basis',
                             'tournier07', '--processes', '1', '-f')
     assert (not ret.success)
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_dwi_lin = os.path.join(get_home(), 'btensor_testdata',
+def test_execution_processing(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_dwi_lin = os.path.join(SCILPY_HOME, 'btensor_testdata',
                               'dwi_linear.nii.gz')
-    in_bval_lin = os.path.join(get_home(), 'btensor_testdata',
+    in_bval_lin = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'linear.bvals')
-    in_bvec_lin = os.path.join(get_home(), 'btensor_testdata',
+    in_bvec_lin = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'linear.bvecs')
-    in_dwi_sph = os.path.join(get_home(), 'btensor_testdata',
+    in_dwi_sph = os.path.join(SCILPY_HOME, 'btensor_testdata',
                               'dwi_spherical.nii.gz')
-    in_bval_sph = os.path.join(get_home(), 'btensor_testdata',
+    in_bval_sph = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'spherical.bvals')
-    in_bvec_sph = os.path.join(get_home(), 'btensor_testdata',
+    in_bvec_sph = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'spherical.bvecs')
-    in_wm_frf = os.path.join(get_home(), 'btensor_testdata',
+    in_wm_frf = os.path.join(SCILPY_HOME, 'btensor_testdata',
                              'wm_frf.txt')
-    in_gm_frf = os.path.join(get_home(), 'btensor_testdata',
+    in_gm_frf = os.path.join(SCILPY_HOME, 'btensor_testdata',
                              'gm_frf.txt')
-    in_csf_frf = os.path.join(get_home(), 'btensor_testdata',
+    in_csf_frf = os.path.join(SCILPY_HOME, 'btensor_testdata',
                               'csf_frf.txt')
 
-    ret = script_runner.run('scil_compute_memsmt_fodf.py', in_wm_frf,
+    ret = script_runner.run('scil_fodf_memsmt.py', in_wm_frf,
                             in_gm_frf, in_csf_frf, '--in_dwis',
                             in_dwi_lin, in_dwi_sph, '--in_bvals',
                             in_bval_lin, in_bval_sph,
                             '--in_bvecs', in_bvec_lin,
                             in_bvec_sph, '--in_bdeltas', '1', '0',
                             '--sh_order', '8', '--processes', '8', '-f')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_memsmt_frf.py` & `scilpy-2.0.0/scripts/tests/test_frf_memsmt.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,159 +1,160 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 fetch_data(get_testing_files_dict(), keys=['btensor_testdata.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_memsmt_frf.py', '--help')
+    ret = script_runner.run('scil_frf_memsmt.py', '--help')
     assert ret.success
 
 
-def test_roi_center_shape_parameter(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_dwi_lin = os.path.join(get_home(), 'btensor_testdata',
+def test_roi_center_shape_parameter(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_dwi_lin = os.path.join(SCILPY_HOME, 'btensor_testdata',
                               'dwi_linear.nii.gz')
-    in_bval_lin = os.path.join(get_home(), 'btensor_testdata',
+    in_bval_lin = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'linear.bvals')
-    in_bvec_lin = os.path.join(get_home(), 'btensor_testdata',
+    in_bvec_lin = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'linear.bvecs')
-    in_dwi_plan = os.path.join(get_home(), 'btensor_testdata',
+    in_dwi_plan = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'dwi_planar.nii.gz')
-    in_bval_plan = os.path.join(get_home(), 'btensor_testdata',
+    in_bval_plan = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                 'planar.bvals')
-    in_bvec_plan = os.path.join(get_home(), 'btensor_testdata',
+    in_bvec_plan = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                 'planar.bvecs')
-    in_dwi_sph = os.path.join(get_home(), 'btensor_testdata',
+    in_dwi_sph = os.path.join(SCILPY_HOME, 'btensor_testdata',
                               'dwi_spherical.nii.gz')
-    in_bval_sph = os.path.join(get_home(), 'btensor_testdata',
+    in_bval_sph = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'spherical.bvals')
-    in_bvec_sph = os.path.join(get_home(), 'btensor_testdata',
+    in_bvec_sph = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'spherical.bvecs')
 
-    ret = script_runner.run('scil_compute_memsmt_frf.py', 'wm_frf.txt',
+    ret = script_runner.run('scil_frf_memsmt.py', 'wm_frf.txt',
                             'gm_frf.txt', 'csf_frf.txt', '--in_dwis',
                             in_dwi_lin, in_dwi_plan, in_dwi_sph, '--in_bvals',
                             in_bval_lin, in_bval_plan, in_bval_sph,
                             '--in_bvecs', in_bvec_lin, in_bvec_plan,
                             in_bvec_sph, '--in_bdeltas', '1', '-0.5', '0',
                             '--roi_center', '1', '--min_nvox', '1', '-f')
 
     assert (not ret.success)
 
 
-def test_roi_radii_shape_parameter(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_dwi_lin = os.path.join(get_home(), 'btensor_testdata',
+def test_roi_radii_shape_parameter(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_dwi_lin = os.path.join(SCILPY_HOME, 'btensor_testdata',
                               'dwi_linear.nii.gz')
-    in_bval_lin = os.path.join(get_home(), 'btensor_testdata',
+    in_bval_lin = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'linear.bvals')
-    in_bvec_lin = os.path.join(get_home(), 'btensor_testdata',
+    in_bvec_lin = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'linear.bvecs')
-    in_dwi_plan = os.path.join(get_home(), 'btensor_testdata',
+    in_dwi_plan = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'dwi_planar.nii.gz')
-    in_bval_plan = os.path.join(get_home(), 'btensor_testdata',
+    in_bval_plan = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                 'planar.bvals')
-    in_bvec_plan = os.path.join(get_home(), 'btensor_testdata',
+    in_bvec_plan = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                 'planar.bvecs')
-    in_dwi_sph = os.path.join(get_home(), 'btensor_testdata',
+    in_dwi_sph = os.path.join(SCILPY_HOME, 'btensor_testdata',
                               'dwi_spherical.nii.gz')
-    in_bval_sph = os.path.join(get_home(), 'btensor_testdata',
+    in_bval_sph = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'spherical.bvals')
-    in_bvec_sph = os.path.join(get_home(), 'btensor_testdata',
+    in_bvec_sph = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'spherical.bvecs')
-    ret = script_runner.run('scil_compute_memsmt_frf.py', 'wm_frf.txt',
+    ret = script_runner.run('scil_frf_memsmt.py', 'wm_frf.txt',
                             'gm_frf.txt', 'csf_frf.txt', '--in_dwis',
                             in_dwi_lin, in_dwi_plan, in_dwi_sph, '--in_bvals',
                             in_bval_lin, in_bval_plan, in_bval_sph,
                             '--in_bvecs', in_bvec_lin, in_bvec_plan,
                             in_bvec_sph, '--in_bdeltas', '1', '-0.5', '0',
                             '--roi_radii', '37', '--min_nvox', '1', '-f')
     assert ret.success
 
-    ret = script_runner.run('scil_compute_memsmt_frf.py', 'wm_frf.txt',
+    ret = script_runner.run('scil_frf_memsmt.py', 'wm_frf.txt',
                             'gm_frf.txt', 'csf_frf.txt', '--in_dwis',
                             in_dwi_lin, in_dwi_plan, in_dwi_sph, '--in_bvals',
                             in_bval_lin, in_bval_plan, in_bval_sph,
                             '--in_bvecs', in_bvec_lin, in_bvec_plan,
                             in_bvec_sph, '--in_bdeltas', '1', '-0.5', '0',
                             '--roi_radii', '37', '37', '37',
                             '--min_nvox', '1', '-f')
     assert ret.success
 
-    ret = script_runner.run('scil_compute_memsmt_frf.py', 'wm_frf.txt',
+    ret = script_runner.run('scil_frf_memsmt.py', 'wm_frf.txt',
                             'gm_frf.txt', 'csf_frf.txt', '--in_dwis',
                             in_dwi_lin, in_dwi_plan, in_dwi_sph, '--in_bvals',
                             in_bval_lin, in_bval_plan, in_bval_sph,
                             '--in_bvecs', in_bvec_lin, in_bvec_plan,
                             in_bvec_sph, '--in_bdeltas', '1', '-0.5', '0',
                             '--roi_radii', '37', '37', '37', '37', '37',
                             '--min_nvox', '1', '-f')
 
     assert (not ret.success)
 
 
-def test_inputs_check(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_dwi_lin = os.path.join(get_home(), 'btensor_testdata',
+def test_inputs_check(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_dwi_lin = os.path.join(SCILPY_HOME, 'btensor_testdata',
                               'dwi_linear.nii.gz')
-    in_bval_lin = os.path.join(get_home(), 'btensor_testdata',
+    in_bval_lin = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'linear.bvals')
-    in_bvec_lin = os.path.join(get_home(), 'btensor_testdata',
+    in_bvec_lin = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'linear.bvecs')
-    in_dwi_plan = os.path.join(get_home(), 'btensor_testdata',
+    in_dwi_plan = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'dwi_planar.nii.gz')
-    in_bval_plan = os.path.join(get_home(), 'btensor_testdata',
+    in_bval_plan = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                 'planar.bvals')
-    in_bvec_plan = os.path.join(get_home(), 'btensor_testdata',
+    in_bvec_plan = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                 'planar.bvecs')
 
-    ret = script_runner.run('scil_compute_memsmt_frf.py', 'wm_frf.txt',
+    ret = script_runner.run('scil_frf_memsmt.py', 'wm_frf.txt',
                             'gm_frf.txt', 'csf_frf.txt', '--in_dwis',
                             in_dwi_lin, in_dwi_plan, '--in_bvals',
                             in_bval_lin, '--in_bvecs', in_bvec_lin,
                             '--in_bdeltas', '1', '--min_nvox', '1', '-f')
     assert (not ret.success)
 
-    ret = script_runner.run('scil_compute_memsmt_frf.py', 'wm_frf.txt',
+    ret = script_runner.run('scil_frf_memsmt.py', 'wm_frf.txt',
                             'gm_frf.txt', 'csf_frf.txt', '--in_dwis',
                             in_dwi_lin, in_dwi_plan, '--in_bvals',
                             in_bval_lin, in_bval_plan, '--in_bvecs',
                             in_bvec_lin, in_bvec_plan, '--in_bdeltas',
                             '1', '-0.5', '0', '--min_nvox', '1', '-f')
     assert (not ret.success)
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_dwi_lin = os.path.join(get_home(), 'btensor_testdata',
+def test_execution_processing(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_dwi_lin = os.path.join(SCILPY_HOME, 'btensor_testdata',
                               'dwi_linear.nii.gz')
-    in_bval_lin = os.path.join(get_home(), 'btensor_testdata',
+    in_bval_lin = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'linear.bvals')
-    in_bvec_lin = os.path.join(get_home(), 'btensor_testdata',
+    in_bvec_lin = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'linear.bvecs')
-    in_dwi_plan = os.path.join(get_home(), 'btensor_testdata',
+    in_dwi_plan = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'dwi_planar.nii.gz')
-    in_bval_plan = os.path.join(get_home(), 'btensor_testdata',
+    in_bval_plan = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                 'planar.bvals')
-    in_bvec_plan = os.path.join(get_home(), 'btensor_testdata',
+    in_bvec_plan = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                 'planar.bvecs')
-    in_dwi_sph = os.path.join(get_home(), 'btensor_testdata',
+    in_dwi_sph = os.path.join(SCILPY_HOME, 'btensor_testdata',
                               'dwi_spherical.nii.gz')
-    in_bval_sph = os.path.join(get_home(), 'btensor_testdata',
+    in_bval_sph = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'spherical.bvals')
-    in_bvec_sph = os.path.join(get_home(), 'btensor_testdata',
+    in_bvec_sph = os.path.join(SCILPY_HOME, 'btensor_testdata',
                                'spherical.bvecs')
-    ret = script_runner.run('scil_compute_memsmt_frf.py', 'wm_frf.txt',
+    ret = script_runner.run('scil_frf_memsmt.py', 'wm_frf.txt',
                             'gm_frf.txt', 'csf_frf.txt', '--in_dwis',
                             in_dwi_lin, in_dwi_plan, in_dwi_sph, '--in_bvals',
                             in_bval_lin, in_bval_plan, in_bval_sph,
                             '--in_bvecs', in_bvec_lin, in_bvec_plan,
                             in_bvec_sph, '--in_bdeltas', '1', '-0.5', '0',
                             '--min_nvox', '1', '-f')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_metrics_stats_in_ROI.py` & `scilpy-2.0.0/scripts/tests/test_volume_stats_in_ROI.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,27 +1,28 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['tractometry.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_metrics_stats_in_ROI.py', '--help')
+    ret = script_runner.run('scil_volume_stats_in_ROI.py', '--help')
     assert ret.success
 
 
-def test_execution_tractometry(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_mask = os.path.join(get_home(), 'tractometry',
+def test_execution_tractometry(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_mask = os.path.join(SCILPY_HOME, 'tractometry',
                            'IFGWM.nii.gz')
-    in_ref = os.path.join(get_home(), 'tractometry',
+    in_ref = os.path.join(SCILPY_HOME, 'tractometry',
                           'mni_masked.nii.gz')
-    ret = script_runner.run('scil_compute_metrics_stats_in_ROI.py',
+    ret = script_runner.run('scil_volume_stats_in_ROI.py',
                             in_mask, '--metrics', in_ref)
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_msmt_fodf.py` & `scilpy-2.0.0/scripts/tests/test_fodf_msmt.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,35 +1,36 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 fetch_data(get_testing_files_dict(), keys=['commit_amico.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_msmt_fodf.py', '--help')
+    ret = script_runner.run('scil_fodf_msmt.py', '--help')
     assert ret.success
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_dwi = os.path.join(get_home(), 'commit_amico', 'dwi.nii.gz')
-    in_bval = os.path.join(get_home(), 'commit_amico', 'dwi.bval')
-    in_bvec = os.path.join(get_home(), 'commit_amico', 'dwi.bvec')
-    in_wm_frf = os.path.join(get_home(), 'commit_amico', 'wm_frf.txt')
-    in_gm_frf = os.path.join(get_home(), 'commit_amico', 'gm_frf.txt')
-    in_csf_frf = os.path.join(get_home(), 'commit_amico', 'csf_frf.txt')
-    mask = os.path.join(get_home(), 'commit_amico', 'mask.nii.gz')
+def test_execution_processing(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_dwi = os.path.join(SCILPY_HOME, 'commit_amico', 'dwi.nii.gz')
+    in_bval = os.path.join(SCILPY_HOME, 'commit_amico', 'dwi.bval')
+    in_bvec = os.path.join(SCILPY_HOME, 'commit_amico', 'dwi.bvec')
+    in_wm_frf = os.path.join(SCILPY_HOME, 'commit_amico', 'wm_frf.txt')
+    in_gm_frf = os.path.join(SCILPY_HOME, 'commit_amico', 'gm_frf.txt')
+    in_csf_frf = os.path.join(SCILPY_HOME, 'commit_amico', 'csf_frf.txt')
+    mask = os.path.join(SCILPY_HOME, 'commit_amico', 'mask.nii.gz')
 
-    ret = script_runner.run('scil_compute_msmt_fodf.py', in_dwi, in_bval,
+    ret = script_runner.run('scil_fodf_msmt.py', in_dwi, in_bval,
                             in_bvec, in_wm_frf, in_gm_frf, in_csf_frf,
                             '--mask', mask,
                             '--wm_out_fODF', 'wm_fodf.nii.gz',
                             '--gm_out_fODF', 'gm_fodf.nii.gz',
                             '--csf_out_fODF', 'csf_fodf.nii.gz',
                             '--vf', 'vf.nii.gz', '--sh_order', '4',
                             '--sh_basis', 'tournier07',
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_msmt_frf.py` & `scilpy-2.0.0/scripts/tests/test_frf_msmt.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,84 +1,91 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 fetch_data(get_testing_files_dict(), keys=['commit_amico.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_msmt_frf.py', '--help')
+    ret = script_runner.run('scil_frf_msmt.py', '--help')
     assert ret.success
 
 
-def test_roi_radii_shape_parameter(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_dwi = os.path.join(get_home(), 'commit_amico',
+def test_roi_radii_shape_parameter(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_dwi = os.path.join(SCILPY_HOME, 'commit_amico',
                           'dwi.nii.gz')
-    in_bval = os.path.join(get_home(), 'commit_amico',
+    in_bval = os.path.join(SCILPY_HOME, 'commit_amico',
                            'dwi.bval')
-    in_bvec = os.path.join(get_home(), 'commit_amico',
+    in_bvec = os.path.join(SCILPY_HOME, 'commit_amico',
                            'dwi.bvec')
-    mask = os.path.join(get_home(), 'commit_amico',
-                           'mask.nii.gz')
-    ret = script_runner.run('scil_compute_msmt_frf.py', in_dwi,
+    mask = os.path.join(SCILPY_HOME, 'commit_amico', 'mask.nii.gz')
+    ret = script_runner.run('scil_frf_msmt.py', in_dwi,
                             in_bval, in_bvec, 'wm_frf.txt', 'gm_frf.txt',
                             'csf_frf.txt', '--mask', mask, '--roi_center',
                             '15', '15', '15', '-f')
     assert ret.success
 
-    ret = script_runner.run('scil_compute_msmt_frf.py', in_dwi,
+    # Test wrong tolerance, leading to no b0. Current minimal b-val is 5.
+    ret = script_runner.run('scil_frf_msmt.py', in_dwi,
+                            in_bval, in_bvec, 'wm_frf.txt', 'gm_frf.txt',
+                            'csf_frf.txt', '--mask', mask, '--roi_center',
+                            '15', '15', '15', '-f', '--tol', '1')
+    assert not ret.success
+
+    ret = script_runner.run('scil_frf_msmt.py', in_dwi,
                             in_bval, in_bvec, 'wm_frf.txt', 'gm_frf.txt',
                             'csf_frf.txt', '--mask', mask, '--roi_center',
                             '15', '-f')
 
     assert (not ret.success)
 
-def test_roi_radii_shape_parameter(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_dwi = os.path.join(get_home(), 'commit_amico',
+
+def test_roi_radii_shape_parameter2(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_dwi = os.path.join(SCILPY_HOME, 'commit_amico',
                           'dwi.nii.gz')
-    in_bval = os.path.join(get_home(), 'commit_amico',
+    in_bval = os.path.join(SCILPY_HOME, 'commit_amico',
                            'dwi.bval')
-    in_bvec = os.path.join(get_home(), 'commit_amico',
+    in_bvec = os.path.join(SCILPY_HOME, 'commit_amico',
                            'dwi.bvec')
-    mask = os.path.join(get_home(), 'commit_amico',
-                           'mask.nii.gz')
-    ret = script_runner.run('scil_compute_msmt_frf.py', in_dwi,
+    mask = os.path.join(SCILPY_HOME, 'commit_amico', 'mask.nii.gz')
+    ret = script_runner.run('scil_frf_msmt.py', in_dwi,
                             in_bval, in_bvec, 'wm_frf.txt', 'gm_frf.txt',
                             'csf_frf.txt', '--mask', mask, '--roi_radii',
                             '37', '-f')
     assert ret.success
 
-    ret = script_runner.run('scil_compute_msmt_frf.py', in_dwi,
+    ret = script_runner.run('scil_frf_msmt.py', in_dwi,
                             in_bval, in_bvec, 'wm_frf.txt', 'gm_frf.txt',
                             'csf_frf.txt', '--mask', mask, '--roi_radii',
                             '37', '37', '37', '-f')
     assert ret.success
 
-    ret = script_runner.run('scil_compute_msmt_frf.py', in_dwi,
+    ret = script_runner.run('scil_frf_msmt.py', in_dwi,
                             in_bval, in_bvec, 'wm_frf.txt', 'gm_frf.txt',
                             'csf_frf.txt', '--mask', mask, '--roi_radii',
                             '37', '37', '37', '37', '37', '-f')
 
     assert (not ret.success)
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_dwi = os.path.join(get_home(), 'commit_amico',
+
+def test_execution_processing(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_dwi = os.path.join(SCILPY_HOME, 'commit_amico',
                           'dwi.nii.gz')
-    in_bval = os.path.join(get_home(), 'commit_amico',
+    in_bval = os.path.join(SCILPY_HOME, 'commit_amico',
                            'dwi.bval')
-    in_bvec = os.path.join(get_home(), 'commit_amico',
+    in_bvec = os.path.join(SCILPY_HOME, 'commit_amico',
                            'dwi.bvec')
-    mask = os.path.join(get_home(), 'commit_amico',
-                           'mask.nii.gz')
-    ret = script_runner.run('scil_compute_msmt_frf.py', in_dwi,
+    mask = os.path.join(SCILPY_HOME, 'commit_amico', 'mask.nii.gz')
+    ret = script_runner.run('scil_frf_msmt.py', in_dwi,
                             in_bval, in_bvec, 'wm_frf.txt', 'gm_frf.txt',
                             'csf_frf.txt', '--mask', mask, '--min_nvox', '20',
                             '-f')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_pca.py` & `scilpy-2.0.0/scripts/tests/test_tractogram_filter_by_orientation.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,30 +1,29 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['stats.zip'])
+fetch_data(get_testing_files_dict(), keys=['filtering.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_pca.py',
+    ret = script_runner.run('scil_tractogram_filter_by_orientation.py',
                             '--help')
     assert ret.success
 
 
-def test_execution_pca(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    input_folder = os.path.join(get_home(), 'stats/pca')
-    output_folder = os.path.join(get_home(), 'stats/pca_out')
-    ids = os.path.join(get_home(), 'stats/pca',
-                       'list_id.txt')
-    ret = script_runner.run('scil_compute_pca.py', input_folder, output_folder, '--metrics', 'ad',
-                            'fa', 'md', 'rd', 'nufo', 'afd_total', 'afd_fixel', '--list_ids',
-                            ids, '-f')
+def test_execution_filtering(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_bundle = os.path.join(SCILPY_HOME, 'filtering',
+                             'bundle_4.trk')
+    ret = script_runner.run('scil_tractogram_filter_by_orientation.py',
+                            in_bundle,  'bundle_4_filtered.trk',
+                            '--min_x', '20', '--max_y', '230', '--min_z', '30',
+                            '--use_abs')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_pft.py` & `scilpy-2.0.0/scripts/tests/test_tracking_pft.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,36 +1,36 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['tracking.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_pft.py',
+    ret = script_runner.run('scil_tracking_pft.py',
                             '--help')
     assert ret.success
 
 
-def test_execution_tracking(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_fodf = os.path.join(get_home(), 'tracking',
+def test_execution_tracking(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_fodf = os.path.join(SCILPY_HOME, 'tracking',
                            'fodf.nii.gz')
-    in_interface = os.path.join(get_home(), 'tracking',
+    in_interface = os.path.join(SCILPY_HOME, 'tracking',
                                 'interface.nii.gz')
-    in_include = os.path.join(get_home(), 'tracking',
+    in_include = os.path.join(SCILPY_HOME, 'tracking',
                               'map_include.nii.gz')
-    in_exclude = os.path.join(get_home(), 'tracking',
+    in_exclude = os.path.join(SCILPY_HOME, 'tracking',
                               'map_exclude.nii.gz')
-    ret = script_runner.run('scil_compute_pft.py', in_fodf,
+    ret = script_runner.run('scil_tracking_pft.py', in_fodf,
                             in_interface, in_include, in_exclude,
                             'pft.trk', '--nt', '1000', '--compress', '0.1',
                             '--sh_basis', 'descoteaux07', '--min_length', '20',
                             '--max_length', '200')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_powder_average.py` & `scilpy-2.0.0/scripts/tests/test_dwi_apply_bias_field.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,29 +1,28 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['processing.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_powder_average.py', '--help')
+    ret = script_runner.run('scil_dwi_apply_bias_field.py', '--help')
     assert ret.success
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-
-    in_dwi = os.path.join(get_home(), 'processing',
-                          'dwi_crop_1000.nii.gz')
-    in_bval = os.path.join(get_home(), 'processing',
-                           '1000.bval')
-
-    ret = script_runner.run('scil_compute_powder_average.py', in_dwi,
-                            in_bval, 'out_pwd_avg.nii.gz', '--shells', '1000')
+def test_execution_processing(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_dwi = os.path.join(SCILPY_HOME, 'processing',
+                          'dwi_crop.nii.gz')
+    in_bias = os.path.join(SCILPY_HOME, 'processing',
+                           'bias_field_b0.nii.gz')
+    ret = script_runner.run('scil_dwi_apply_bias_field.py', in_dwi,
+                            in_bias, 'dwi_crop_n4.nii.gz')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_qball_metrics.py` & `scilpy-2.0.0/scripts/tests/test_qball_metrics.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,41 +1,48 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 fetch_data(get_testing_files_dict(), keys=['processing.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_qball_metrics.py', '--help')
+    ret = script_runner.run('scil_qball_metrics.py', '--help')
     assert ret.success
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_dwi = os.path.join(get_home(), 'processing',
+def test_execution_processing(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_dwi = os.path.join(SCILPY_HOME, 'processing',
                           'dwi_crop_1000.nii.gz')
-    in_bval = os.path.join(get_home(), 'processing',
+    in_bval = os.path.join(SCILPY_HOME, 'processing',
                            '1000.bval')
-    in_bvec = os.path.join(get_home(), 'processing',
+    in_bvec = os.path.join(SCILPY_HOME, 'processing',
                            '1000.bvec')
-    ret = script_runner.run('scil_compute_qball_metrics.py', in_dwi,
+    ret = script_runner.run('scil_qball_metrics.py', in_dwi,
                             in_bval, in_bvec)
     assert ret.success
 
 
-def test_execution_not_all(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_dwi = os.path.join(get_home(), 'processing',
+def test_execution_not_all(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_dwi = os.path.join(SCILPY_HOME, 'processing',
                           'dwi_crop_1000.nii.gz')
-    in_bval = os.path.join(get_home(), 'processing',
+    in_bval = os.path.join(SCILPY_HOME, 'processing',
                            '1000.bval')
-    in_bvec = os.path.join(get_home(), 'processing',
+    in_bvec = os.path.join(SCILPY_HOME, 'processing',
                            '1000.bvec')
-    ret = script_runner.run('scil_compute_qball_metrics.py', in_dwi,
+    ret = script_runner.run('scil_qball_metrics.py', in_dwi,
                             in_bval, in_bvec, "--not_all", "--sh", "2.nii.gz")
     assert ret.success
+
+    # Test wrong b0. Current minimal b-val is 5.
+    ret = script_runner.run('scil_qball_metrics.py', in_dwi,
+                            in_bval, in_bvec, "--not_all", "--sh", "2.nii.gz",
+                            '--b0_threshold', '1', '-f')
+    assert not ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_qbx.py` & `scilpy-2.0.0/scripts/tests/test_labels_split_volume_by_ids.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,26 +1,26 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['filtering.zip'])
+fetch_data(get_testing_files_dict(), keys=['atlas.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_qbx.py', '--help')
+    ret = script_runner.run('scil_labels_split_volume_by_ids.py', '--help')
     assert ret.success
 
 
-def test_execution_filtering(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_bundle = os.path.join(get_home(), 'filtering',
-                             'bundle_all_1mm.trk')
-    ret = script_runner.run('scil_compute_qbx.py', in_bundle, '12',
-                            'clusters/', '--out_centroids', 'centroids.trk')
+def test_execution_atlas(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_atlas = os.path.join(SCILPY_HOME, 'atlas',
+                            'atlas_freesurfer_v2.nii.gz')
+    ret = script_runner.run('scil_labels_split_volume_by_ids.py', in_atlas,
+                            '--out_prefix', 'brainstem', '-r', '173', '175')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_rish_from_sh.py` & `scilpy-2.0.0/scripts/tests/test_surface_smooth.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,24 +1,26 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['processing.zip'])
+fetch_data(get_testing_files_dict(), keys='surface_vtk_fib.zip')
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_rish_from_sh.py', '--help')
+    ret = script_runner.run('scil_surface_smooth.py', '--help')
     assert ret.success
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_sh = os.path.join(get_home(), 'processing',
-                          'sh.nii.gz')
-    ret = script_runner.run('scil_compute_rish_from_sh.py', in_sh, 'rish.nii.gz')
+def test_execution_surface_vtk_fib(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_surf = os.path.join(SCILPY_HOME, 'surface_vtk_fib',
+                           'lhpialt.vtk')
+    ret = script_runner.run('scil_surface_smooth.py', in_surf,
+                            'lhpialt_smooth.vtk', '-n', '5', '-s', '1')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_seed_density_map.py` & `scilpy-2.0.0/scripts/tests/test_tractogram_seed_density_map.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,25 +1,25 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['processing.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_seed_density_map.py', '--help')
+    ret = script_runner.run('scil_tractogram_seed_density_map.py', '--help')
     assert ret.success
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_tracking = os.path.join(get_home(), 'processing',
-                               'tracking.trk')
-    ret = script_runner.run('scil_compute_seed_density_map.py', in_tracking,
+def test_execution_processing(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tracking = os.path.join(SCILPY_HOME, 'processing', 'tracking.trk')
+    ret = script_runner.run('scil_tractogram_seed_density_map.py', in_tracking,
                             'seeds_density.nii.gz')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_sf_from_sh.py` & `scilpy-2.0.0/scripts/tests/test_sh_convert.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,30 +1,27 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['processing.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_sf_from_sh.py', '--help')
+    ret = script_runner.run('scil_sh_convert.py', '--help')
     assert ret.success
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_sh = os.path.join(get_home(), 'processing', 'sh_1000.nii.gz')
-    in_b0 = os.path.join(get_home(), 'processing', 'fa.nii.gz')
-    in_bval = os.path.join(get_home(), 'processing', '1000.bval')
-
-    ret = script_runner.run('scil_compute_sf_from_sh.py', in_sh,
-                            'sf_724.nii.gz', '--in_bval',
-                            in_bval, '--in_b0', in_b0, '--out_bval',
-                            'sf_724.bval', '--out_bvec', 'sf_724.bvec',
-                            '--sphere', 'symmetric724', '--dtype', 'float32')
+def test_execution_processing(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_fodf = os.path.join(SCILPY_HOME, 'processing',
+                           'fodf.nii.gz')
+    ret = script_runner.run('scil_sh_convert.py', in_fodf,
+                            'fodf_descoteaux07.nii.gz', 'tournier07',
+                            'descoteaux07_legacy', '--processes', '1')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_sh_from_signal.py` & `scilpy-2.0.0/scripts/tests/test_dki_metrics.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,29 +1,35 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['processing.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_sh_from_signal.py', '--help')
+    ret = script_runner.run('scil_dki_metrics.py', '--help')
     assert ret.success
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_dwi = os.path.join(get_home(), 'processing',
-                          'dwi_crop_3000.nii.gz')
-    in_bval = os.path.join(get_home(), 'processing',
-                           '3000.bval')
-    in_bvec = os.path.join(get_home(), 'processing',
-                           '3000.bvec')
-    ret = script_runner.run('scil_compute_sh_from_signal.py', in_dwi, in_bval,
-                            in_bvec, 'sh_1000.nii.gz')
+def test_execution_processing(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_dwi = os.path.join(SCILPY_HOME, 'processing',
+                          'dwi_crop.nii.gz')
+    in_bval = os.path.join(SCILPY_HOME, 'processing',
+                           'dwi.bval')
+    in_bvec = os.path.join(SCILPY_HOME, 'processing',
+                           'dwi.bvec')
+    ret = script_runner.run('scil_dki_metrics.py', in_dwi,
+                            in_bval, in_bvec, '--not_all',
+                            '--dki_fa', 'dki_fa.nii.gz',
+                            '--dki_md', 'dki_md.nii.gz',
+                            '--dki_rd', 'dki_rd.nii.gz',
+                            '--dki_ad', 'dki_ad.nii.gz',
+                            '--dki_residual', 'dki_res.nii.gz')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_ssst_fodf.py` & `scilpy-2.0.0/scripts/tests/test_fodf_ssst.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,32 +1,40 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['processing.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_ssst_fodf.py', '--help')
+    ret = script_runner.run('scil_fodf_ssst.py', '--help')
     assert ret.success
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_dwi = os.path.join(get_home(), 'processing',
+def test_execution_processing(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_dwi = os.path.join(SCILPY_HOME, 'processing',
                           'dwi_crop_3000.nii.gz')
-    in_bval = os.path.join(get_home(), 'processing',
+    in_bval = os.path.join(SCILPY_HOME, 'processing',
                            '3000.bval')
-    in_bvec = os.path.join(get_home(), 'processing',
+    in_bvec = os.path.join(SCILPY_HOME, 'processing',
                            '3000.bvec')
-    in_frf = os.path.join(get_home(), 'processing',
+    in_frf = os.path.join(SCILPY_HOME, 'processing',
                           'frf.txt')
-    ret = script_runner.run('scil_compute_ssst_fodf.py', in_dwi, in_bval,
+    ret = script_runner.run('scil_fodf_ssst.py', in_dwi, in_bval,
                             in_bvec, in_frf, 'fodf.nii.gz', '--sh_order', '4',
                             '--sh_basis', 'tournier07', '--processes', '1')
     assert ret.success
+
+    # Test wrong b0. Current minimal b-value is 5.
+    ret = script_runner.run('scil_fodf_ssst.py', in_dwi, in_bval,
+                            in_bvec, in_frf, 'fodf.nii.gz', '--sh_order', '4',
+                            '--sh_basis', 'tournier07', '--processes', '1',
+                            '--b0_threshold', '1', '-f')
+    assert not ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_streamlines_density_map.py` & `scilpy-2.0.0/scripts/tests/test_bundle_reject_outliers.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,36 +1,28 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['others.zip', 'tractometry.zip'])
+fetch_data(get_testing_files_dict(), keys=['filtering.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_streamlines_density_map.py',
-                            '--help')
-    assert ret.success
-
-
-def test_execution_others(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_bundle = os.path.join(get_home(), 'others',
-                             'IFGWM.trk')
-    ret = script_runner.run('scil_compute_streamlines_density_map.py',
-                            in_bundle, 'binary.nii.gz', '--binary')
+    ret = script_runner.run('scil_bundle_reject_outliers.py', '--help')
     assert ret.success
 
 
-def test_execution_tractometry(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_bundle = os.path.join(get_home(), 'tractometry',
-                             'IFGWM.trk')
-    ret = script_runner.run('scil_compute_streamlines_density_map.py',
-                            in_bundle, 'IFGWM.nii.gz', '--binary')
+def test_execution_filtering(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_bundle = os.path.join(SCILPY_HOME, 'filtering', 'bundle_all_1mm.trk')
+    ret = script_runner.run('scil_bundle_reject_outliers.py', in_bundle,
+                            'inliers.trk', '--alpha', '0.6',
+                            '--remaining_bundle', 'outliers.trk',
+                            '--display_counts', '--indent', '4',
+                            '--sort_keys')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_compute_streamlines_length_stats.py` & `scilpy-2.0.0/scripts/tests/test_viz_fodf.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,27 +1,25 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
-
-# If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['filtering.zip'])
+fetch_data(get_testing_files_dict(), keys=['tracking.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_compute_streamlines_length_stats.py',
-                            '--help')
+    ret = script_runner.run('scil_viz_fodf.py', '--help')
     assert ret.success
 
 
-def test_execution_filtering(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_bundle = os.path.join(get_home(), 'filtering',
-                             'bundle_4.trk')
-    ret = script_runner.run('scil_compute_streamlines_length_stats.py',
-                            in_bundle)
-    assert ret.success
+def test_silent_without_output(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_fodf = os.path.join(SCILPY_HOME, 'tracking', 'fodf.nii.gz')
+
+    ret = script_runner.run('scil_viz_fodf.py', in_fodf, '--silent')
+
+    assert (not ret.success)
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_concatenate_dwi.py` & `scilpy-2.0.0/scripts/tests/test_dwi_concatenate.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,32 +1,33 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['processing.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_concatenate_dwi.py', '--help')
+    ret = script_runner.run('scil_dwi_concatenate.py', '--help')
     assert ret.success
 
 
-def test_execution_processing_concatenate(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_dwi = os.path.join(get_home(), 'processing',
+def test_execution_processing_concatenate(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_dwi = os.path.join(SCILPY_HOME, 'processing',
                           'dwi_crop.nii.gz')
-    in_bval = os.path.join(get_home(), 'processing',
+    in_bval = os.path.join(SCILPY_HOME, 'processing',
                            'dwi.bval')
-    in_bvec = os.path.join(get_home(), 'processing',
+    in_bvec = os.path.join(SCILPY_HOME, 'processing',
                            'dwi.bvec')
-    ret = script_runner.run('scil_concatenate_dwi.py', 'dwi_concat.nii.gz',
+    ret = script_runner.run('scil_dwi_concatenate.py', 'dwi_concat.nii.gz',
                             'concat.bval', 'concat.bvec',
                             '--in_dwi', in_dwi, in_dwi,
                             '--in_bvals', in_bval, in_bval,
                             '--in_bvecs', in_bvec, in_bvec)
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_convert_gradients_fsl_to_mrtrix.py` & `scilpy-2.0.0/scripts/tests/test_sh_fusion.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,28 +1,27 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['processing.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_convert_gradients_fsl_to_mrtrix.py',
-                            '--help')
+    ret = script_runner.run('scil_sh_fusion.py', '--help')
     assert ret.success
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_bval = os.path.join(get_home(), 'processing',
-                           '1000.bval')
-    in_bvec = os.path.join(get_home(), 'processing',
-                           '1000.bvec')
-    ret = script_runner.run('scil_convert_gradients_fsl_to_mrtrix.py',
-                            in_bval, in_bvec, '1000.b')
+def test_execution_processing(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_sh_1 = os.path.join(SCILPY_HOME, 'processing',
+                           'sh_1000.nii.gz')
+    in_sh_2 = os.path.join(SCILPY_HOME, 'processing',
+                           'sh_3000.nii.gz')
+    ret = script_runner.run('scil_sh_fusion.py', in_sh_1, in_sh_2, 'sh.nii.gz')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_convert_gradients_mrtrix_to_fsl.py` & `scilpy-2.0.0/scripts/tests/test_header_print_info.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,26 +1,31 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['processing.zip'])
+fetch_data(get_testing_files_dict(), keys=['others.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_convert_gradients_mrtrix_to_fsl.py',
-                            '--help')
+    ret = script_runner.run('scil_header_print_info.py', '--help')
     assert ret.success
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_encoding = os.path.join(get_home(), 'processing',
-                               '1000.b')
-    ret = script_runner.run('scil_convert_gradients_mrtrix_to_fsl.py',
-                            in_encoding, '1000.bval', '1000.bvec')
+def test_execution_img(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_img = os.path.join(SCILPY_HOME, 'others', 'fa.nii.gz')
+    ret = script_runner.run('scil_header_print_info.py', in_img)
+    assert ret.success
+
+
+def test_execution_tractogram(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tracto = os.path.join(SCILPY_HOME, 'others', 'IFGWM.trk')
+    ret = script_runner.run('scil_header_print_info.py', in_tracto)
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_convert_json_to_xlsx.py` & `scilpy-2.0.0/scripts/tests/test_json_convert_entries_to_xlsx.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,27 +1,27 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['tractometry.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_convert_json_to_xlsx.py', '--help')
+    ret = script_runner.run('scil_json_convert_entries_to_xlsx.py', '--help')
     assert ret.success
 
 
-def test_execution_tractometry(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_json = os.path.join(get_home(), 'tractometry',
+def test_execution_tractometry(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_json = os.path.join(SCILPY_HOME, 'tractometry',
                            'length_stats_1.json')
-    ret = script_runner.run('scil_convert_json_to_xlsx.py', in_json,
+    ret = script_runner.run('scil_json_convert_entries_to_xlsx.py', in_json,
                             'length_stats.xlsx')
 
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_convert_rgb.py` & `scilpy-2.0.0/scripts/tests/test_volume_resample.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,25 +1,26 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['others.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_convert_rgb.py', '--help')
+    ret = script_runner.run('scil_volume_resample.py', '--help')
     assert ret.success
 
 
-def test_execution_others(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_img = os.path.join(get_home(), 'others',
-                          'rgb.nii.gz')
-    ret = script_runner.run('scil_convert_rgb.py', in_img, 'rgb_4D.nii.gz')
+def test_execution_others(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_img = os.path.join(SCILPY_HOME, 'others',
+                          'fa.nii.gz')
+    ret = script_runner.run('scil_volume_resample.py', in_img,
+                            'fa_resample.nii.gz', '--voxel_size', '2')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_convert_sh_basis.py` & `scilpy-2.0.0/scripts/tests/test_bundle_shape_measures.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,26 +1,29 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['processing.zip'])
+fetch_data(get_testing_files_dict(), keys=['bundles.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_convert_sh_basis.py', '--help')
+    ret = script_runner.run('scil_bundle_shape_measures.py', '--help')
     assert ret.success
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_fodf = os.path.join(get_home(), 'processing',
-                           'fodf.nii.gz')
-    ret = script_runner.run('scil_convert_sh_basis.py', in_fodf,
-                            'fodf_descoteaux07.nii.gz', 'tournier07',
-                            '--processes', '1')
+def test_execution_bundles(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_1 = os.path.join(SCILPY_HOME, 'bundles', 'bundle_0_reco.tck')
+    in_2 = os.path.join(SCILPY_HOME, 'bundles', 'voting_results',
+                        'bundle_0.trk')
+    in_ref = os.path.join(SCILPY_HOME, 'bundles', 'bundle_all_1mm.nii.gz')
+    ret = script_runner.run('scil_bundle_shape_measures.py',
+                            in_1, in_2, '--out_json', 'AF_L_measures.json',
+                            '--reference', in_ref, '--processes', '1')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_convert_surface.py` & `scilpy-2.0.0/scripts/tests/test_volume_flip.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,26 +1,26 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys='surface_vtk_fib.zip')
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_convert_surface.py', '--help')
+    ret = script_runner.run('scil_volume_flip.py', '--help')
     assert ret.success
 
 
-def test_execution_surface_vtk_fib(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_surf = os.path.join(get_home(), 'surface_vtk_fib',
-                           'lhpialt.vtk')
-    ret = script_runner.run('scil_convert_surface.py', in_surf,
-                            'rhpialt.ply')
+def test_execution_surface_vtk_fib(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_fa = os.path.join(SCILPY_HOME, 'surface_vtk_fib',
+                         'fa.nii.gz')
+    ret = script_runner.run('scil_volume_flip.py', in_fa, 'fa_flip.nii.gz',
+                            'x')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_convert_tensors.py` & `scilpy-2.0.0/scripts/tests/test_tractogram_filter_by_length.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,37 +1,28 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['processing.zip'])
+fetch_data(get_testing_files_dict(), keys=['filtering.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_convert_tensors.py', '--help')
+    ret = script_runner.run('scil_tractogram_filter_by_length.py',
+                            '--help')
     assert ret.success
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-
-    # No tensor in the current test data! I'm running the compute_dti_metrics
-    # to create one.
-    in_dwi = os.path.join(get_home(), 'processing',
-                          'dwi_crop_1000.nii.gz')
-    in_bval = os.path.join(get_home(), 'processing',
-                           '1000.bval')
-    in_bvec = os.path.join(get_home(), 'processing',
-                           '1000.bvec')
-    script_runner.run('scil_compute_dti_metrics.py', in_dwi,
-                      in_bval, in_bvec, '--not_all',
-                      '--tensor', 'tensors.nii.gz', '--tensor_format', 'fsl')
-
-    ret = script_runner.run('scil_convert_tensors.py', 'tensors.nii.gz',
-                            'converted_tensors.nii.gz', 'fsl', 'mrtrix')
-
+def test_execution_filtering(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_bundle = os.path.join(SCILPY_HOME, 'filtering',
+                             'bundle_4.trk')
+    ret = script_runner.run('scil_tractogram_filter_by_length.py',
+                            in_bundle,  'bundle_4_filtered.trk',
+                            '--minL', '125', '--maxL', '130')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_convert_tractogram.py` & `scilpy-2.0.0/scripts/tests/test_tractogram_shuffle.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,28 +1,25 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys='surface_vtk_fib.zip')
+fetch_data(get_testing_files_dict(), keys=['tracking.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_convert_tractogram.py', '--help')
+    ret = script_runner.run('scil_tractogram_shuffle.py', '--help')
     assert ret.success
 
 
-def test_execution_surface_vtk_fib(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_fib = os.path.join(get_home(), 'surface_vtk_fib',
-                          'gyri_fanning.fib')
-    in_fa = os.path.join(get_home(), 'surface_vtk_fib',
-                         'fa.nii.gz')
-    ret = script_runner.run('scil_convert_tractogram.py', in_fib,
-                            'gyri_fanning.trk', '--reference', in_fa)
+def test_execution_tracking(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tracto = os.path.join(SCILPY_HOME, 'tracking', 'union.trk')
+    ret = script_runner.run('scil_tractogram_shuffle.py', in_tracto,
+                            'union_shuffle.trk')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_count_non_zero_voxels.py` & `scilpy-2.0.0/scripts/tests/test_bundle_volume_per_label.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,25 +1,27 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['others.zip'])
+fetch_data(get_testing_files_dict(), keys=['tractometry.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_count_non_zero_voxels.py', '--help')
+    ret = script_runner.run('scil_bundle_volume_per_label.py',
+                            '--help')
     assert ret.success
 
 
-def test_execution_others(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_img = os.path.join(get_home(), 'others',
-                          'rgb.nii.gz')
-    ret = script_runner.run('scil_count_non_zero_voxels.py', in_img)
+def test_execution_tractometry(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_label_map = os.path.join(SCILPY_HOME, 'tractometry',
+                                'IFGWM_labels_map.nii.gz')
+    ret = script_runner.run('scil_bundle_volume_per_label.py',
+                            in_label_map, 'IFGWM')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_count_streamlines.py` & `scilpy-2.0.0/scripts/tests/test_connectivity_graph_measures.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,25 +1,27 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['others.zip'])
+fetch_data(get_testing_files_dict(), keys=['connectivity.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_count_streamlines.py', '--help')
+    ret = script_runner.run('scil_connectivity_graph_measures.py', '--help')
     assert ret.success
 
 
-def test_execution_others(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_bundle = os.path.join(get_home(), 'others',
-                             'IFGWM_sub.trk')
-    ret = script_runner.run('scil_count_streamlines.py', in_bundle)
+def test_execution_connectivity(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_sc = os.path.join(SCILPY_HOME, 'connectivity', 'sc_norm.npy')
+    in_len = os.path.join(SCILPY_HOME, 'connectivity', 'len.npy')
+    ret = script_runner.run('scil_connectivity_graph_measures.py', in_sc,
+                            in_len, 'gtm.json', '--avg_node_wise',
+                            '--small_world')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_crop_volume.py` & `scilpy-2.0.0/scripts/tests/test_viz_volume_histogram.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,24 +1,28 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['processing.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_crop_volume.py', '--help')
+    ret = script_runner.run('scil_viz_volume_histogram.py', '--help')
     assert ret.success
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_dwi = os.path.join(get_home(), 'processing',
-                          'dwi.nii.gz')
-    ret = script_runner.run('scil_crop_volume.py', in_dwi, 'dwi_crop.nii.gz')
+def test_execution_processing(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_fa = os.path.join(SCILPY_HOME, 'processing',
+                         'fa.nii.gz')
+    in_mask = os.path.join(SCILPY_HOME, 'processing',
+                           'seed.nii.gz')
+    ret = script_runner.run('scil_viz_volume_histogram.py', in_fa, in_mask,
+                            '20', 'histogram.png')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_cut_streamlines.py` & `scilpy-2.0.0/scripts/tests/test_tractogram_cut_streamlines.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,30 +1,43 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['filtering.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_cut_streamlines.py',
+    ret = script_runner.run('scil_tractogram_cut_streamlines.py',
                             '--help')
     assert ret.success
 
 
-def test_execution_filtering(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_tractogram = os.path.join(get_home(), 'filtering',
+def test_execution_two_roi(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tractogram = os.path.join(SCILPY_HOME, 'filtering',
                                  'bundle_all_1mm.trk')
-    in_mask = os.path.join(get_home(), 'filtering',
-                             'mask.nii.gz')
-    ret = script_runner.run('scil_cut_streamlines.py',
-                            in_tractogram, in_mask, 'out_tractogram_cut.trk',
+    in_mask = os.path.join(SCILPY_HOME, 'filtering', 'mask.nii.gz')
+    ret = script_runner.run('scil_tractogram_cut_streamlines.py',
+                            in_tractogram, 'out_tractogram_cut.trk',
+                            '--mask', in_mask,
                             '--resample', '0.2', '--compress', '0.1')
     assert ret.success
+
+
+def test_execution_biggest(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tractogram = os.path.join(SCILPY_HOME, 'filtering',
+                                 'bundle_all_1mm.trk')
+    in_mask = os.path.join(SCILPY_HOME, 'filtering', 'mask.nii.gz')
+    ret = script_runner.run('scil_tractogram_cut_streamlines.py',
+                            in_tractogram, '--mask', in_mask,
+                            'out_tractogram_cut2.trk',
+                            '--resample', '0.2', '--compress', '0.1',
+                            '--biggest')
+    assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_decompose_connectivity.py` & `scilpy-2.0.0/scripts/tests/test_connectivity_print_filenames.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,33 +1,28 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['connectivity.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_decompose_connectivity.py', '--help')
+    ret = script_runner.run('scil_connectivity_print_filenames.py', '--help')
     assert ret.success
 
 
-def test_execution_connectivity(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_bundle = os.path.join(get_home(), 'connectivity',
-                             'bundle_all_1mm.trk')
-    in_atlas = os.path.join(get_home(), 'connectivity',
-                            'endpoints_atlas.nii.gz')
-    ret = script_runner.run('scil_decompose_connectivity.py', in_bundle,
-                            in_atlas, 'decompose.h5',
-                            '--min_length', '20', '--max_length', '200',
-                            '--outlier_threshold', '0.5',
-                            '--loop_max_angle', '330',
-                            '--curv_qb_distance', '10',
-                            '--processes', '1')
+def test_execution_connectivity(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_sc = os.path.join(SCILPY_HOME, 'connectivity',
+                         'sc_norm.npy')
+    in_labels_list = os.path.join(SCILPY_HOME, 'connectivity',
+                                  'labels_list.txt')
+    ret = script_runner.run('scil_connectivity_print_filenames.py', in_sc,
+                            in_labels_list, 'success.txt')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_detect_streamlines_loops.py` & `scilpy-2.0.0/scripts/tests/test_tractogram_detect_loops.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,30 +1,30 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['filtering.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_detect_streamlines_loops.py', '--help')
+    ret = script_runner.run('scil_tractogram_detect_loops.py', '--help')
     assert ret.success
 
 
-def test_execution_filtering(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_bundle = os.path.join(get_home(), 'filtering',
+def test_execution_filtering(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_bundle = os.path.join(SCILPY_HOME, 'filtering',
                              'bundle_4_filtered.trk')
-    ret = script_runner.run('scil_detect_streamlines_loops.py',
+    ret = script_runner.run('scil_tractogram_detect_loops.py',
                             in_bundle, 'bundle_4_filtered_no_loops.trk',
                             '--looping_tractogram',
                             'bundle_4_filtered_loops.trk',
-                            '-a', '270', '--qb', '--threshold', '4',
-                            '--processes', '1')
+                            '--angle', '270', '--qb', '4',
+                            '--processes', '1', '--display_counts')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_dilate_labels.py` & `scilpy-2.0.0/scripts/tests/test_tractogram_segment_bundles.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,26 +1,41 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
+import json
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict 
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['atlas.zip'])
+fetch_data(get_testing_files_dict(), keys=['bundles.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_dilate_labels.py', '--help')
+    ret = script_runner.run('scil_tractogram_segment_bundles.py', '--help')
     assert ret.success
 
 
-def test_execution_atlas(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_atlas = os.path.join(get_home(), 'atlas',
-                            'atlas_freesurfer_v2_single_brainstem.nii.gz')
-    ret = script_runner.run('scil_dilate_labels.py', in_atlas,
-                            'atlas_freesurfer_v2_single_brainstem_dil.nii.gz',
-                            '--processes', '1', '--distance', '2')
+def test_execution_bundles(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tractogram = os.path.join(SCILPY_HOME, 'bundles',
+                                 'bundle_all_1mm.trk')
+    in_models = os.path.join(SCILPY_HOME, 'bundles', 'fibercup_atlas')
+    in_aff = os.path.join(SCILPY_HOME, 'bundles',
+                          'affine.txt')
+
+    tmp_config = {}
+    for i in range(1, 6):
+        tmp_config['bundle_{}.trk'.format(i)] = 4
+
+    with open('config.json', 'w') as outfile:
+        json.dump(tmp_config, outfile)
+
+    ret = script_runner.run('scil_tractogram_segment_bundles.py',
+                            in_tractogram, 'config.json',
+                            in_models,
+                            in_aff, '--inverse',
+                            '--processes', '1', '-v', 'WARNING')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_estimate_bundles_diameter.py` & `scilpy-2.0.0/scripts/tests/test_tractogram_remove_invalid.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,29 +1,27 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['tractometry.zip'])
+fetch_data(get_testing_files_dict(), keys=['bundles.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_estimate_bundles_diameter.py', '--help')
+    ret = script_runner.run('scil_tractogram_remove_invalid.py', '--help')
     assert ret.success
 
 
-def test_execution_tractometry(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_bundle = os.path.join(get_home(), 'tractometry',
-                             'IFGWM.trk')
-    in_labels = os.path.join(get_home(), 'tractometry',
-                             'IFGWM_labels_map.nii.gz')
-    ret = script_runner.run('scil_estimate_bundles_diameter.py',
-                            in_bundle, in_labels,
-                            '--wireframe', '--fitting_func', 'lin_up')
+def test_execution_bundles(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tractogram = os.path.join(SCILPY_HOME, 'bundles',
+                                 'bundle_all_1mm.trk')
+    ret = script_runner.run('scil_tractogram_remove_invalid.py',
+                            in_tractogram, 'bundle_all_1mm.trk', '--cut',
+                            '--remove_overlapping', '--remove_single', '-f')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_evaluate_bundles_binary_classification_measures.py` & `scilpy-2.0.0/scripts/tests/test_tractogram_pairwise_comparison.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,38 +1,42 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import get_testing_files_dict, fetch_data
 
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['bundles.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_evaluate_bundles_binary_classification_measures.py',
-                            '--help')
+    ret = script_runner.run(
+        'scil_tractogram_pairwise_comparison.py', '--help')
     assert ret.success
 
 
-def test_execution_bundles(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_1 = os.path.join(get_home(), 'bundles',
-                        'bundle_0_reco.tck')
-    in_2 = os.path.join(get_home(), 'bundles', 'voting_results',
+def test_execution_bundles_skip(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_1 = os.path.join(SCILPY_HOME, 'bundles', 'bundle_0_reco.tck')
+    in_2 = os.path.join(SCILPY_HOME, 'bundles', 'voting_results',
                         'bundle_0.trk')
-    in_ref = os.path.join(get_home(), 'bundles',
-                          'bundle_all_1mm.nii.gz')
-    in_tractogram = os.path.join(get_home(), 'bundles',
-                                 'bundle_all_1mm.trk')
-    in_model = os.path.join(get_home(), 'bundles', 'fibercup_atlas',
-                            'subj_1', 'bundle_0.trk')
-    ret = script_runner.run('scil_evaluate_bundles_binary_classification_measures.py',
-                            in_1, in_2, 'AF_L_binary.json',
-                            '--streamlines_measures', in_model,
-                            in_tractogram, '--processes', '1',
-                            '--reference', in_ref)
+    in_ref = os.path.join(SCILPY_HOME, 'bundles', 'bundle_all_1mm.nii.gz')
+    ret = script_runner.run('scil_tractogram_pairwise_comparison.py',
+        in_1, in_2, '--out_dir', tmp_dir.name, '--reference', in_ref,
+        '--skip_streamlines_distance')
     assert ret.success
+
+def test_execution_bundles(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_1 = os.path.join(SCILPY_HOME, 'bundles', 'bundle_0_reco.tck')
+    in_2 = os.path.join(SCILPY_HOME, 'bundles', 'voting_results',
+                        'bundle_0.trk')
+    in_ref = os.path.join(SCILPY_HOME, 'bundles', 'bundle_all_1mm.nii.gz')
+    ret = script_runner.run('scil_tractogram_pairwise_comparison.py',
+        in_1, in_2, '--out_dir', tmp_dir.name, '--reference', in_ref, '-f',
+        '--skip_streamlines_distance')
+    assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_evaluate_bundles_individual_measures.py` & `scilpy-2.0.0/scripts/tests/test_stats_group_comparison.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,32 +1,35 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['bundles.zip'])
+fetch_data(get_testing_files_dict(), keys=['stats.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_evaluate_bundles_individual_measures.py',
-                            '--help')
+    ret = script_runner.run(
+        'scil_stats_group_comparison.py',
+        '--help')
     assert ret.success
 
 
-def test_execution_bundles(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_1 = os.path.join(get_home(), 'bundles',
-                        'bundle_0_reco.tck')
-    in_2 = os.path.join(get_home(), 'bundles', 'voting_results',
-                        'bundle_0.trk')
-    in_ref = os.path.join(get_home(), 'bundles',
-                          'bundle_all_1mm.nii.gz')
-    ret = script_runner.run('scil_evaluate_bundles_individual_measures.py',
-                            in_1, in_2, 'AF_L_measures.json',
-                            '--reference', in_ref, '--processes', '1')
+def test_execution_bundles(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_json = os.path.join(SCILPY_HOME, 'stats/group', 'participants.tsv')
+    in_participants = os.path.join(SCILPY_HOME, 'stats/group',
+                                   'meanstd_all.json')
+
+    ret = script_runner.run('scil_stats_group_comparison.py',
+                            in_participants, in_json, 'Group',
+                            '-b', 'AF_L',
+                            '-m', 'FIT_FW',
+                            '--va', 'mean',
+                            '--gg')
+
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_evaluate_bundles_pairwise_agreement_measures.py` & `scilpy-2.0.0/scripts/tests/test_bundle_pairwise_comparison.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,107 +1,96 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['bundles.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
     ret = script_runner.run(
-        'scil_evaluate_bundles_pairwise_agreement_measures.py',
-        '--help')
+        'scil_bundle_pairwise_comparison.py', '--help')
     assert ret.success
 
 
-def test_execution_bundles(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_1 = os.path.join(get_home(), 'bundles',
-                        'bundle_0_reco.tck')
-    in_2 = os.path.join(get_home(), 'bundles', 'voting_results',
+def test_execution_bundles(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_1 = os.path.join(SCILPY_HOME, 'bundles', 'bundle_0_reco.tck')
+    in_2 = os.path.join(SCILPY_HOME, 'bundles', 'voting_results',
                         'bundle_0.trk')
-    in_ref = os.path.join(get_home(), 'bundles',
-                          'bundle_all_1mm.nii.gz')
+    in_ref = os.path.join(SCILPY_HOME, 'bundles', 'bundle_all_1mm.nii.gz')
     ret = script_runner.run(
-        'scil_evaluate_bundles_pairwise_agreement_measures.py',
+        'scil_bundle_pairwise_comparison.py',
         in_1, in_2, 'AF_L_similarity.json',
         '--streamline_dice', '--reference', in_ref,
         '--processes', '1')
     assert ret.success
 
 
-def test_single(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_1 = os.path.join(get_home(), 'bundles',
-                        'bundle_0_reco.tck')
-    in_2 = os.path.join(get_home(), 'bundles', 'voting_results',
+def test_single(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_1 = os.path.join(SCILPY_HOME, 'bundles', 'bundle_0_reco.tck')
+    in_2 = os.path.join(SCILPY_HOME, 'bundles', 'voting_results',
                         'bundle_0.trk')
-    in_ref = os.path.join(get_home(), 'bundles',
-                          'bundle_all_1mm.nii.gz')
+    in_ref = os.path.join(SCILPY_HOME, 'bundles', 'bundle_all_1mm.nii.gz')
     ret = script_runner.run(
-        'scil_evaluate_bundles_pairwise_agreement_measures.py',
+        'scil_bundle_pairwise_comparison.py',
         in_2, 'AF_L_similarity_single.json',
         '--streamline_dice', '--reference', in_ref,
         '--single_compare', in_1,
         '--processes', '1')
     assert ret.success
 
 
-def test_no_overlap(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_1 = os.path.join(get_home(), 'bundles',
-                        'bundle_0_reco.tck')
-    in_2 = os.path.join(get_home(), 'bundles', 'voting_results',
+def test_no_overlap(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_1 = os.path.join(SCILPY_HOME, 'bundles', 'bundle_0_reco.tck')
+    in_2 = os.path.join(SCILPY_HOME, 'bundles', 'voting_results',
                         'bundle_0.trk')
-    in_ref = os.path.join(get_home(), 'bundles',
-                          'bundle_all_1mm.nii.gz')
+    in_ref = os.path.join(SCILPY_HOME, 'bundles', 'bundle_all_1mm.nii.gz')
     ret = script_runner.run(
-        'scil_evaluate_bundles_pairwise_agreement_measures.py',
+        'scil_bundle_pairwise_comparison.py', in_1,
         in_2, 'AF_L_similarity_no_overlap.json',
         '--streamline_dice', '--reference', in_ref,
         '--bundle_adjency_no_overlap',
         '--processes', '1')
     assert ret.success
 
 
-def test_ratio(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_1 = os.path.join(get_home(), 'bundles',
-                        'bundle_0_reco.tck')
-    in_2 = os.path.join(get_home(), 'bundles', 'voting_results',
+def test_ratio(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_1 = os.path.join(SCILPY_HOME, 'bundles', 'bundle_0_reco.tck')
+    in_2 = os.path.join(SCILPY_HOME, 'bundles', 'voting_results',
                         'bundle_0.trk')
-    in_ref = os.path.join(get_home(), 'bundles',
-                          'bundle_all_1mm.nii.gz')
+    in_ref = os.path.join(SCILPY_HOME, 'bundles', 'bundle_all_1mm.nii.gz')
     ret = script_runner.run(
-        'scil_evaluate_bundles_pairwise_agreement_measures.py',
+        'scil_bundle_pairwise_comparison.py',
         in_2, 'AF_L_similarity_ratio.json',
         '--streamline_dice', '--reference', in_ref,
         '--single_compare', in_1,
         '--processes', '1',
         '--ratio')
     assert ret.success
 
 
-def test_ratio_fail(script_runner):
+def test_ratio_fail(script_runner, monkeypatch):
     """ Test ratio without single_compare argument.
     The test should fail.
     """
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_1 = os.path.join(get_home(), 'bundles',
-                        'bundle_0_reco.tck')
-    in_2 = os.path.join(get_home(), 'bundles', 'voting_results',
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_1 = os.path.join(SCILPY_HOME, 'bundles',  'bundle_0_reco.tck')
+    in_2 = os.path.join(SCILPY_HOME, 'bundles', 'voting_results',
                         'bundle_0.trk')
-    in_ref = os.path.join(get_home(), 'bundles',
-                          'bundle_all_1mm.nii.gz')
+    in_ref = os.path.join(SCILPY_HOME, 'bundles', 'bundle_all_1mm.nii.gz')
     ret = script_runner.run(
-        'scil_evaluate_bundles_pairwise_agreement_measures.py',
+        'scil_bundle_pairwise_comparison.py',
         in_1, in_2, 'AF_L_similarity_fail.json',
         '--streamline_dice', '--reference', in_ref,
         '--processes', '1',
         '--ratio')
     assert not ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_evaluate_connectivity_graph_measures.py` & `scilpy-2.0.0/scripts/tests/test_connectivity_normalize.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,29 +1,31 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['connectivity.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_evaluate_connectivity_graph_measures.py', '--help')
+    ret = script_runner.run('scil_connectivity_normalize.py', '--help')
     assert ret.success
 
 
-def test_execution_connectivity(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_sc = os.path.join(get_home(), 'connectivity',
-                         'sc_norm.npy')
-    in_len = os.path.join(get_home(), 'connectivity',
-                          'len.npy')
-    ret = script_runner.run('scil_evaluate_connectivity_graph_measures.py', in_sc,
-                            in_len, 'gtm.json', '--avg_node_wise',
-                            '--small_world')
+def test_execution_connectivity(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_sc = os.path.join(SCILPY_HOME, 'connectivity', 'sc.npy')
+    in_len = os.path.join(SCILPY_HOME, 'connectivity', 'len.npy')
+    in_atlas = os.path.join(SCILPY_HOME, 'connectivity',
+                            'endpoints_atlas.nii.gz')
+    in_labels_list = os.path.join(SCILPY_HOME, 'connectivity',
+                                  'labels_list.txt')
+    ret = script_runner.run('scil_connectivity_normalize.py', in_sc,
+                            'sc_norm.npy', '--length', in_len,
+                            '--parcel_volume', in_atlas, in_labels_list)
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_execute_asymmetric_filtering.py` & `scilpy-2.0.0/scripts/tests/test_tractogram_assign_uniform_color.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,52 +1,44 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
-
+import json
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['processing.zip'])
+fetch_data(get_testing_files_dict(), keys=['tractometry.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
+in_bundle = os.path.join(SCILPY_HOME, 'tractometry', 'IFGWM.trk')
+
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_execute_asymmetric_filtering.py',
+    ret = script_runner.run('scil_tractogram_assign_uniform_color.py',
                             '--help')
     assert ret.success
 
 
-def test_asym_basis_output(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_fodf = os.path.join(get_home(), 'processing',
-                           'fodf_descoteaux07_sub.nii.gz')
-
-    # We use a low resolution sphere to reduce execution time
-    ret = script_runner.run('scil_execute_asymmetric_filtering.py', in_fodf,
-                            'out_0.nii.gz', '--sphere', 'repulsion100')
-    assert ret.success
-
+def test_execution_fill(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
 
-def test_sym_basis_output(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_fodf = os.path.join(get_home(), 'processing',
-                           'fodf_descoteaux07_sub.nii.gz')
-
-    # We use a low resolution sphere to reduce execution time
-    ret = script_runner.run('scil_execute_asymmetric_filtering.py', in_fodf,
-                            'out_1.nii.gz', '--out_sym', 'out_sym.nii.gz',
-                            '--sphere', 'repulsion100')
+    ret = script_runner.run('scil_tractogram_assign_uniform_color.py',
+                            in_bundle, '--fill_color', '0x000000',
+                            '--out_tractogram', 'colored.trk')
     assert ret.success
 
 
-def test_asym_input(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_fodf = os.path.join(get_home(), 'processing',
-                           'fodf_descoteaux07_sub_full.nii.gz')
-
-    # We use a low resolution sphere to reduce execution time
-    ret = script_runner.run('scil_execute_asymmetric_filtering.py', in_fodf,
-                            'out_2.nii.gz', '--sphere', 'repulsion100', '-f')
+def test_execution_dict(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+
+    # Create a fake dictionary. Using the other hexadecimal format.
+    my_dict = {'IFGWM': '#000000'}
+    json_file = 'my_json_dict.json'
+    with open(json_file, "w+") as f:
+        json.dump(my_dict, f)
+
+    ret = script_runner.run('scil_tractogram_assign_uniform_color.py',
+                            in_bundle, '--dict_colors', json_file,
+                            '--out_suffix', 'colored')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_extract_b0.py` & `scilpy-2.0.0/scripts/tests/test_volume_reshape_to_reference.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,29 +1,37 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['processing.zip'])
+fetch_data(get_testing_files_dict(), keys=['others.zip', 'commit_amico.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_extract_b0.py', '--help')
+    ret = script_runner.run('scil_volume_reshape_to_reference.py', '--help')
     assert ret.success
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_dwi = os.path.join(get_home(), 'processing',
-                          'dwi_crop.nii.gz')
-    in_bval = os.path.join(get_home(), 'processing',
-                           'dwi.bval')
-    in_bvec = os.path.join(get_home(), 'processing',
-                           'dwi.bvec')
-    ret = script_runner.run('scil_extract_b0.py', in_dwi, in_bval, in_bvec,
-                            'b0_mean.nii.gz', '--mean', '--b0', '20')
+def test_execution_others(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_img = os.path.join(SCILPY_HOME, 'others', 't1_crop.nii.gz')
+    in_ref = os.path.join(SCILPY_HOME, 'others', 't1.nii.gz')
+    ret = script_runner.run('scil_volume_reshape_to_reference.py', in_img,
+                            in_ref, 't1_reshape.nii.gz',
+                            '--interpolation', 'nearest')
+    assert ret.success
+
+
+def test_execution_4D(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_img = os.path.join(SCILPY_HOME, 'commit_amico', 'dwi.nii.gz')
+    in_ref = os.path.join(SCILPY_HOME, 'others', 't1.nii.gz')
+    ret = script_runner.run('scil_volume_reshape_to_reference.py', in_img,
+                            in_ref, 'dwi_reshape.nii.gz',
+                            '--interpolation', 'nearest')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_extract_dwi_shell.py` & `scilpy-2.0.0/scripts/tests/test_dwi_extract_shell.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,46 +1,63 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['processing.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_extract_dwi_shell.py', '--help')
+    ret = script_runner.run('scil_dwi_extract_shell.py', '--help')
     assert ret.success
 
 
-def test_execution_processing_1000(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_dwi = os.path.join(get_home(), 'processing',
+def test_execution_processing_1000(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_dwi = os.path.join(SCILPY_HOME, 'processing',
                           'dwi_crop.nii.gz')
-    in_bval = os.path.join(get_home(), 'processing',
+    in_bval = os.path.join(SCILPY_HOME, 'processing',
                            'dwi.bval')
-    in_bvec = os.path.join(get_home(), 'processing',
+    in_bvec = os.path.join(SCILPY_HOME, 'processing',
                            'dwi.bvec')
-    ret = script_runner.run('scil_extract_dwi_shell.py', in_dwi,
+    ret = script_runner.run('scil_dwi_extract_shell.py', in_dwi,
                             in_bval, in_bvec, '0', '1000',
                             'dwi_crop_1000.nii.gz', '1000.bval', '1000.bvec',
                             '-t', '30')
     assert ret.success
 
 
-def test_execution_processing_3000(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_dwi = os.path.join(get_home(), 'processing',
+def test_execution_out_indices(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_dwi = os.path.join(SCILPY_HOME, 'processing',
                           'dwi_crop.nii.gz')
-    in_bval = os.path.join(get_home(), 'processing',
+    in_bval = os.path.join(SCILPY_HOME, 'processing',
                            'dwi.bval')
-    in_bvec = os.path.join(get_home(), 'processing',
+    in_bvec = os.path.join(SCILPY_HOME, 'processing',
                            'dwi.bvec')
-    ret = script_runner.run('scil_extract_dwi_shell.py', in_dwi,
+    ret = script_runner.run('scil_dwi_extract_shell.py', in_dwi,
+                            in_bval, in_bvec, '0', '1000',
+                            'dwi_crop_1000__1.nii.gz', '1000__1.bval',
+                            '1000__1.bvec', '-t', '30', '--out_indices',
+                            'out_indices.txt')
+    assert ret.success
+
+
+def test_execution_processing_3000(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_dwi = os.path.join(SCILPY_HOME, 'processing',
+                          'dwi_crop.nii.gz')
+    in_bval = os.path.join(SCILPY_HOME, 'processing',
+                           'dwi.bval')
+    in_bvec = os.path.join(SCILPY_HOME, 'processing',
+                           'dwi.bvec')
+    ret = script_runner.run('scil_dwi_extract_shell.py', in_dwi,
                             in_bval, in_bvec, '0', '3000',
                             'dwi_crop_3000.nii.gz', '3000.bval', '3000.bvec',
                             '-t', '30')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_extract_ushape.py` & `scilpy-2.0.0/scripts/tests/test_tractogram_extract_ushape.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,31 +1,32 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['tracking.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_extract_ushape.py',
+    ret = script_runner.run('scil_tractogram_extract_ushape.py',
                             '--help')
     assert ret.success
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_trk = os.path.join(get_home(), 'tracking', 'union.trk')
+def test_execution_processing(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_trk = os.path.join(SCILPY_HOME, 'tracking', 'union.trk')
     out_trk = 'ushape.trk'
     remaining_trk = 'remaining.trk'
-    ret = script_runner.run('scil_extract_ushape.py', in_trk, out_trk,
+    ret = script_runner.run('scil_tractogram_extract_ushape.py',
+                            in_trk, out_trk,
                             '--minU', '0.5',
                             '--maxU', '1',
                             '--remaining_tractogram', remaining_trk,
                             '--display_counts')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_filter_connectivity.py` & `scilpy-2.0.0/scripts/tests/test_connectivity_filter.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,30 +1,30 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['connectivity.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_filter_connectivity.py', '--help')
+    ret = script_runner.run('scil_connectivity_filter.py', '--help')
     assert ret.success
 
 
-def test_execution_connectivity(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_sc = os.path.join(get_home(), 'connectivity',
+def test_execution_connectivity(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_sc = os.path.join(SCILPY_HOME, 'connectivity',
                          'sc.npy')
-    in_sim = os.path.join(get_home(), 'connectivity',
+    in_sim = os.path.join(SCILPY_HOME, 'connectivity',
                           'len.npy')
-    ret = script_runner.run('scil_filter_connectivity.py', 'mask.npy',
+    ret = script_runner.run('scil_connectivity_filter.py', 'mask.npy',
                             '--greater_than', in_sc, '5', '1',
                             '--greater_than', in_sim, '0', '1',
                             '--keep_condition_count')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_filter_streamlines_by_length.py` & `scilpy-2.0.0/scripts/tests/test_tractogram_qbx.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,28 +1,26 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['filtering.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_filter_streamlines_by_length.py',
-                            '--help')
+    ret = script_runner.run('scil_tractogram_qbx.py', '--help')
     assert ret.success
 
 
-def test_execution_filtering(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_bundle = os.path.join(get_home(), 'filtering',
-                             'bundle_4.trk')
-    ret = script_runner.run('scil_filter_streamlines_by_length.py',
-                            in_bundle,  'bundle_4_filtered.trk',
-                            '--minL', '125', '--maxL', '130')
+def test_execution_filtering(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_bundle = os.path.join(SCILPY_HOME, 'filtering',
+                             'bundle_all_1mm.trk')
+    ret = script_runner.run('scil_tractogram_qbx.py', in_bundle, '12',
+                            'clusters/', '--out_centroids', 'centroids.trk')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_filter_streamlines_by_orientation.py` & `scilpy-2.0.0/scripts/tests/test_bundle_diameter.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,29 +1,27 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['filtering.zip'])
+fetch_data(get_testing_files_dict(), keys=['tractometry.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_filter_streamlines_by_orientation.py',
-                            '--help')
+    ret = script_runner.run('scil_bundle_diameter.py', '--help')
     assert ret.success
 
 
-def test_execution_filtering(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_bundle = os.path.join(get_home(), 'filtering',
-                             'bundle_4.trk')
-    ret = script_runner.run('scil_filter_streamlines_by_orientation.py',
-                            in_bundle,  'bundle_4_filtered.trk',
-                            '--min_x', '20', '--max_y', '230', '--min_z', '30',
-                            '--use_abs')
+def test_execution_tractometry(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_bundle = os.path.join(SCILPY_HOME, 'tractometry', 'IFGWM.trk')
+    in_labels = os.path.join(SCILPY_HOME, 'tractometry',
+                             'IFGWM_labels_map.nii.gz')
+    ret = script_runner.run('scil_bundle_diameter.py', in_bundle, in_labels,
+                            '--wireframe', '--fitting_func', 'lin_up')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_filter_tractogram.py` & `scilpy-2.0.0/scripts/tests/test_tractogram_split.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,33 +1,36 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['filtering.zip'])
+fetch_data(get_testing_files_dict(), keys=['tracking.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_filter_tractogram.py', '--help')
+    ret = script_runner.run('scil_tractogram_split.py', '--help')
     assert ret.success
 
 
-def test_execution_filtering(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_tractogram = os.path.join(get_home(), 'filtering',
-                                 'bundle_all_1mm_inliers.trk')
-    in_roi = os.path.join(get_home(), 'filtering',
-                          'mask.nii.gz')
-    in_bdo = os.path.join(get_home(), 'filtering',
-                          'sc.bdo')
-    ret = script_runner.run('scil_filter_tractogram.py', in_tractogram,
-                            'bundle_4.trk', '--display_counts',
-                            '--drawn_roi', in_roi, 'any', 'include',
-                            '--bdo', in_bdo, 'any', 'include',
-                            '--save_rejected', 'bundle_4_rejected.trk')
+def test_execution_tracking(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tracto = os.path.join(SCILPY_HOME, 'tracking',
+                             'local.trk')
+    ret = script_runner.run('scil_tractogram_split.py', in_tracto,
+                            'local_split', '--nb_chunks', '3', '-f')
+    assert ret.success
+
+    ret = script_runner.run('scil_tractogram_split.py', in_tracto,
+                            'local_split', '--nb_chunks', '3', '-f',
+                            '--split_per_cluster')
+    assert ret.success
+
+    ret = script_runner.run('scil_tractogram_split.py', in_tracto,
+                            'local_split', '--nb_chunks', '3', '-f',
+                            '--do_not_randomize')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_flip_gradients.py` & `scilpy-2.0.0/scripts/tests/test_bundle_mean_fixel_afd.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,25 +1,28 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['processing.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_flip_gradients.py', '--help')
+    ret = script_runner.run('scil_bundle_mean_fixel_afd.py', '--help')
     assert ret.success
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_encoding = os.path.join(get_home(), 'processing',
-                               '1000.b')
-    ret = script_runner.run('scil_flip_gradients.py', in_encoding,
-                            '1000_flip.b', 'x', '--mrtrix')
+def test_execution_processing(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tracking = os.path.join(SCILPY_HOME, 'processing', 'tracking.trk')
+    in_fodf = os.path.join(SCILPY_HOME, 'processing',
+                           'fodf_descoteaux07.nii.gz')
+    ret = script_runner.run('scil_bundle_mean_fixel_afd.py', in_tracking,
+                            in_fodf, 'afd_test.nii.gz',
+                            '--sh_basis', 'descoteaux07', '--length_weighting')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_flip_surface.py` & `scilpy-2.0.0/scripts/tests/test_surface_flip.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,28 +1,28 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys='surface_vtk_fib.zip')
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_flip_surface.py', '--help')
+    ret = script_runner.run('scil_surface_flip.py', '--help')
     assert ret.success
 
 
-def test_execution_surface_vtk_fib(script_runner):
+def test_execution_surface_vtk_fib(script_runner, monkeypatch):
     # Weird behavior, flip around the origin in RASMM rather than the center of
     # the volume in VOX
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_surf = os.path.join(get_home(), 'surface_vtk_fib',
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_surf = os.path.join(SCILPY_HOME, 'surface_vtk_fib',
                            'lhpialt.vtk')
-    ret = script_runner.run('scil_flip_surface.py', in_surf, 'rhpialt.vtk',
+    ret = script_runner.run('scil_surface_flip.py', in_surf, 'rhpialt.vtk',
                             'x')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_flip_volume.py` & `scilpy-2.0.0/scripts/tests/test_labels_dilate.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,26 +1,27 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys='surface_vtk_fib.zip')
+fetch_data(get_testing_files_dict(), keys=['atlas.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_flip_volume.py', '--help')
+    ret = script_runner.run('scil_labels_dilate.py', '--help')
     assert ret.success
 
 
-def test_execution_surface_vtk_fib(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_fa = os.path.join(get_home(), 'surface_vtk_fib',
-                         'fa.nii.gz')
-    ret = script_runner.run('scil_flip_volume.py', in_fa, 'fa_flip.nii.gz',
-                            'x')
+def test_execution_atlas(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_atlas = os.path.join(SCILPY_HOME, 'atlas',
+                            'atlas_freesurfer_v2_single_brainstem.nii.gz')
+    ret = script_runner.run('scil_labels_dilate.py', in_atlas,
+                            'atlas_freesurfer_v2_single_brainstem_dil.nii.gz',
+                            '--processes', '1', '--distance', '2')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_generate_priors_from_bundle.py` & `scilpy-2.0.0/scripts/tests/test_tractogram_segment_one_bundles.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,32 +1,36 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['bst.zip'])
+fetch_data(get_testing_files_dict(), keys=['bundles.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_generate_priors_from_bundle.py', '--help')
+    ret = script_runner.run('scil_tractogram_segment_one_bundle.py', '--help')
     assert ret.success
 
 
-def test_execution_bst(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_bundle = os.path.join(get_home(), 'bst',
-                             'rpt_m_lin.trk')
-    in_fodf = os.path.join(get_home(), 'bst',
-                           'fodf.nii.gz')
-    in_mask = os.path.join(get_home(), 'bst',
-                           'mask.nii.gz')
-    ret = script_runner.run('scil_generate_priors_from_bundle.py',
-                            in_bundle, in_fodf, in_mask,
-                            '--todi_sigma', '1', '--out_prefix', 'rpt_m',
-                            '--sh_basis', 'descoteaux07')
+def test_execution_bundles(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tractogram = os.path.join(SCILPY_HOME, 'bundles',
+                                 'bundle_all_1mm.trk')
+    in_model = os.path.join(SCILPY_HOME, 'bundles', 'fibercup_atlas',
+                            'subj_1', 'bundle_0.trk')
+    in_aff = os.path.join(SCILPY_HOME, 'bundles',
+                          'affine.txt')
+    in_ref = os.path.join(SCILPY_HOME, 'bundles',
+                          'bundle_all_1mm.nii.gz')
+    ret = script_runner.run('scil_tractogram_segment_one_bundle.py',
+                            in_tractogram, in_model, in_aff,
+                            'bundle_0_reco.tck', '--inverse',
+                            '--tractogram_clustering_thr', '12',
+                            '--slr_threads', '1', '--out_pickle',
+                            'clusters.pkl', '--reference', in_ref)
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_image_math.py` & `scilpy-2.0.0/scripts/tests/test_volume_math.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,71 +1,72 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['atlas.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_image_math.py', '--help')
+    ret = script_runner.run('scil_volume_math.py', '--help')
     assert ret.success
 
 
-def test_execution_add(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_img_1 = os.path.join(get_home(), 'atlas',
+def test_execution_add(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_img_1 = os.path.join(SCILPY_HOME, 'atlas',
                             'brainstem_173.nii.gz')
-    in_img_2 = os.path.join(get_home(), 'atlas',
+    in_img_2 = os.path.join(SCILPY_HOME, 'atlas',
                             'brainstem_174.nii.gz')
-    in_img_3 = os.path.join(get_home(), 'atlas',
+    in_img_3 = os.path.join(SCILPY_HOME, 'atlas',
                             'brainstem_175.nii.gz')
-    ret = script_runner.run('scil_image_math.py', 'addition',
+    ret = script_runner.run('scil_volume_math.py', 'addition',
                             in_img_1, in_img_2, in_img_3, 'brainstem.nii.gz')
     assert ret.success
 
 
-def test_execution_low_thresh(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_img = os.path.join(get_home(), 'atlas', 'brainstem.nii.gz')
-    ret = script_runner.run('scil_image_math.py', 'lower_threshold',
+def test_execution_low_thresh(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_img = os.path.join(SCILPY_HOME, 'atlas', 'brainstem.nii.gz')
+    ret = script_runner.run('scil_volume_math.py', 'lower_threshold',
                             in_img, '1', 'brainstem_bin.nii.gz')
     assert ret.success
 
 
-def test_execution_low_mult(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_img = os.path.join(get_home(), 'atlas', 'brainstem_bin.nii.gz')
-    ret = script_runner.run('scil_image_math.py', 'multiplication',
+def test_execution_low_mult(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_img = os.path.join(SCILPY_HOME, 'atlas', 'brainstem_bin.nii.gz')
+    ret = script_runner.run('scil_volume_math.py', 'multiplication',
                             in_img, '16', 'brainstem_unified.nii.gz')
     assert ret.success
 
 
-def test_execution_concatenate(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_img_1 = os.path.join(get_home(), 'atlas', 'ids', '10.nii.gz')
-    in_img_2 = os.path.join(get_home(), 'atlas', 'ids', '11.nii.gz')
-    in_img_3 = os.path.join(get_home(), 'atlas', 'ids', '12.nii.gz')
-    in_img_4 = os.path.join(get_home(), 'atlas', 'ids', '13.nii.gz')
-    in_img_5 = os.path.join(get_home(), 'atlas', 'ids', '17.nii.gz')
-    in_img_6 = os.path.join(get_home(), 'atlas', 'ids', '18.nii.gz')
-    ret = script_runner.run('scil_image_math.py', 'concatenate',
+def test_execution_concatenate(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_img_1 = os.path.join(SCILPY_HOME, 'atlas', 'ids', '10.nii.gz')
+    in_img_2 = os.path.join(SCILPY_HOME, 'atlas', 'ids', '11.nii.gz')
+    in_img_3 = os.path.join(SCILPY_HOME, 'atlas', 'ids', '12.nii.gz')
+    in_img_4 = os.path.join(SCILPY_HOME, 'atlas', 'ids', '13.nii.gz')
+    in_img_5 = os.path.join(SCILPY_HOME, 'atlas', 'ids', '17.nii.gz')
+    in_img_6 = os.path.join(SCILPY_HOME, 'atlas', 'ids', '18.nii.gz')
+    ret = script_runner.run('scil_volume_math.py', 'concatenate',
                             in_img_1, in_img_2, in_img_3, in_img_4, in_img_5,
                             in_img_6, 'concat_ids.nii.gz')
     assert ret.success
 
 
-def test_execution_concatenate_4D(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_img_1 = os.path.join(get_home(), 'atlas', 'ids', '10.nii.gz')
-    in_img_2 = os.path.join(get_home(), 'atlas', 'ids', '8_10.nii.gz')
-    in_img_3 = os.path.join(get_home(), 'atlas', 'ids', '12.nii.gz')
-    in_img_4 = os.path.join(get_home(), 'atlas', 'ids', '8_10.nii.gz')
-    ret = script_runner.run('scil_image_math.py', 'concatenate',
+def test_execution_concatenate_4D(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_img_1 = os.path.join(SCILPY_HOME, 'atlas', 'ids', '10.nii.gz')
+    in_img_2 = os.path.join(SCILPY_HOME, 'atlas', 'ids', '8_10.nii.gz')
+    in_img_3 = os.path.join(SCILPY_HOME, 'atlas', 'ids', '12.nii.gz')
+    in_img_4 = os.path.join(SCILPY_HOME, 'atlas', 'ids', '8_10.nii.gz')
+    ret = script_runner.run('scil_volume_math.py', 'concatenate',
                             in_img_1, in_img_2, in_img_3, in_img_4,
                             'concat_ids_4d.nii.gz')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_merge_json.py` & `scilpy-2.0.0/scripts/tests/test_json_merge_entries.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,29 +1,29 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['tractometry.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_merge_json.py', '--help')
+    ret = script_runner.run('scil_json_merge_entries.py', '--help')
     assert ret.success
 
 
-def test_execution_tractometry(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_json_1 = os.path.join(get_home(), 'tractometry',
+def test_execution_tractometry(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_json_1 = os.path.join(SCILPY_HOME, 'tractometry',
                            'length_stats_1.json')
-    in_json_2 = os.path.join(get_home(), 'tractometry',
+    in_json_2 = os.path.join(SCILPY_HOME, 'tractometry',
                            'length_stats_2.json')
-    ret = script_runner.run('scil_merge_json.py', in_json_1,
+    ret = script_runner.run('scil_json_merge_entries.py', in_json_1,
                             in_json_2, 'merge.json', '--keep_separate')
 
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_merge_sh.py` & `scilpy-2.0.0/scripts/tests/test_bundle_filter_by_occurence.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,26 +1,31 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['processing.zip'])
+fetch_data(get_testing_files_dict(), keys=['filtering.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_merge_sh.py', '--help')
+    ret = script_runner.run('scil_bundle_filter_by_occurence.py', '--help')
     assert ret.success
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_sh_1 = os.path.join(get_home(), 'processing',
-                           'sh_1000.nii.gz')
-    in_sh_2 = os.path.join(get_home(), 'processing',
-                           'sh_3000.nii.gz')
-    ret = script_runner.run('scil_merge_sh.py', in_sh_1, in_sh_2, 'sh.nii.gz')
+def test_execution(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_1 = os.path.join(SCILPY_HOME, 'filtering', 'bundle_4.trk')
+    in_2 = os.path.join(SCILPY_HOME, 'filtering', 'bundle_4_filtered.trk')
+    in_3 = os.path.join(SCILPY_HOME, 'filtering',
+                        'bundle_4_filtered_no_loops.trk')
+
+    prefix = 'test_voting_'
+    ret = script_runner.run('scil_bundle_filter_by_occurence.py', in_1, in_2,
+                            in_3, prefix, '--ratio_streamlines', '0.5',
+                            '--ratio_voxels', '0.5')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_normalize_connectivity.py` & `scilpy-2.0.0/scripts/tests/test_tractogram_apply_transform_to_hdf5.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,33 +1,33 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['connectivity.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_normalize_connectivity.py', '--help')
+    ret = script_runner.run('scil_tractogram_apply_transform_to_hdf5.py',
+                            '--help')
     assert ret.success
 
 
-def test_execution_connectivity(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_sc = os.path.join(get_home(), 'connectivity',
-                         'sc.npy')
-    in_len = os.path.join(get_home(), 'connectivity',
-                          'len.npy')
-    in_atlas = os.path.join(get_home(), 'connectivity',
-                            'endpoints_atlas.nii.gz')
-    in_labels_list = os.path.join(get_home(), 'connectivity',
-                                  'labels_list.txt')
-    ret = script_runner.run('scil_normalize_connectivity.py', in_sc,
-                            'sc_norm.npy', '--length', in_len,
-                            '--parcel_volume', in_atlas, in_labels_list)
+def test_execution_connectivity(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_h5 = os.path.join(SCILPY_HOME, 'connectivity', 'decompose.h5')
+    in_target = os.path.join(SCILPY_HOME, 'connectivity',
+                             'endpoints_atlas.nii.gz')
+    in_transfo = os.path.join(SCILPY_HOME, 'connectivity', 'affine.txt')
+
+    # toDo. Add a --in_deformation file in our test data, fitting with hdf5.
+    #  (See test_tractogram_apply_transform)
+    # toDo. Add some dps in the hdf5's data for more line coverage.
+    ret = script_runner.run('scil_tractogram_apply_transform_to_hdf5.py',
+                            in_h5, in_target, in_transfo, 'decompose_lin.h5')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_outlier_rejection.py` & `scilpy-2.0.0/scripts/tests/test_volume_remove_outliers_ransac.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,29 +1,26 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['filtering.zip'])
+fetch_data(get_testing_files_dict(), keys=['processing.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_outlier_rejection.py', '--help')
+    ret = script_runner.run('scil_volume_remove_outliers_ransac.py', '--help')
     assert ret.success
 
 
-def test_execution_filtering(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_bundle = os.path.join(get_home(), 'filtering',
-                             'bundle_all_1mm.trk')
-    ret = script_runner.run('scil_outlier_rejection.py', in_bundle,
-                            'inliers.trk', '--alpha', '0.6',
-                            '--remaining_bundle', 'outliers.trk',
-                            '--display_counts', '--indent', '4',
-                            '--sort_keys')
+def test_execution_processing(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_ad = os.path.join(SCILPY_HOME, 'processing',
+                         'ad.nii.gz')
+    ret = script_runner.run('scil_volume_remove_outliers_ransac.py', in_ad,
+                            'ad_ransanc.nii.gz')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_plot_mean_std_per_point.py` & `scilpy-2.0.0/scripts/tests/test_labels_remove.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,27 +1,27 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['tractometry.zip'])
+fetch_data(get_testing_files_dict(), keys=['atlas.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_plot_mean_std_per_point.py', '--help')
+    ret = script_runner.run('scil_labels_remove.py', '--help')
     assert ret.success
 
 
-def test_execution_tractometry(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_json = os.path.join(get_home(), 'tractometry',
-                           'metric_label.json')
-    ret = script_runner.run('scil_plot_mean_std_per_point.py', in_json,
-                            'out/', '--stats_over_population')
-
+def test_execution_atlas(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_atlas = os.path.join(SCILPY_HOME, 'atlas',
+                            'atlas_freesurfer_v2.nii.gz')
+    ret = script_runner.run('scil_labels_remove.py', in_atlas,
+                            'atlas_freesurfer_v2_no_brainstem.nii.gz',
+                            '-i', '173', '174', '175')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_project_streamlines_to_map.py` & `scilpy-2.0.0/scripts/tests/test_bundle_mean_std.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,42 +1,38 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['tractometry.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_project_streamlines_to_map.py', '--help')
+    ret = script_runner.run('scil_bundle_mean_std.py', '--help')
     assert ret.success
 
 
-def test_execution_tractometry_default(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_bundle = os.path.join(get_home(), 'tractometry',
-                             'IFGWM_uni.trk')
-    in_ref = os.path.join(get_home(), 'tractometry',
-                          'mni_masked.nii.gz')
-    ret = script_runner.run('scil_project_streamlines_to_map.py', in_bundle,
-                            'out_def/', '--in_metrics', in_ref)
-
+def test_execution_tractometry_whole(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_bundle = os.path.join(SCILPY_HOME, 'tractometry', 'IFGWM.trk')
+    in_ref = os.path.join(SCILPY_HOME, 'tractometry', 'mni_masked.nii.gz')
+    ret = script_runner.run('scil_bundle_mean_std.py', in_bundle, in_ref,
+                            '--density_weighting', '--include_dps')
     assert ret.success
 
 
-def test_execution_tractometry_wm(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_bundle = os.path.join(get_home(), 'tractometry',
-                             'IFGWM_uni.trk')
-    in_ref = os.path.join(get_home(), 'tractometry',
-                          'mni_masked.nii.gz')
-    ret = script_runner.run('scil_project_streamlines_to_map.py', in_bundle,
-                            'out_wm/', '--in_metrics', in_ref,
-                            '--to_wm', '--from_wm')
+def test_execution_tractometry_per_point(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_bundle = os.path.join(SCILPY_HOME, 'tractometry', 'IFGWM.trk')
+    in_label = os.path.join(SCILPY_HOME, 'tractometry',
+                            'IFGWM_labels_map.nii.gz')
+    in_ref = os.path.join(SCILPY_HOME, 'tractometry', 'mni_masked.nii.gz')
+    ret = script_runner.run('scil_bundle_mean_std.py', in_bundle, in_ref,
+                            '--per_point', in_label, '--density_weighting')
 
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_recognize_single_bundle.py` & `scilpy-2.0.0/scripts/tests/test_bundle_score_same_bundle_many_segmentations.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,35 +1,34 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['bundles.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_recognize_single_bundle.py', '--help')
+    ret = script_runner.run(
+        'scil_bundle_score_same_bundle_many_segmentations.py', '--help')
     assert ret.success
 
 
-def test_execution_bundles(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_tractogram = os.path.join(get_home(), 'bundles',
-                                 'bundle_all_1mm.trk')
-    in_model = os.path.join(get_home(), 'bundles', 'fibercup_atlas',
+def test_execution_bundles(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_1 = os.path.join(SCILPY_HOME, 'bundles', 'bundle_0_reco.tck')
+    in_2 = os.path.join(SCILPY_HOME, 'bundles', 'voting_results',
+                        'bundle_0.trk')
+    in_ref = os.path.join(SCILPY_HOME, 'bundles', 'bundle_all_1mm.nii.gz')
+    in_tractogram = os.path.join(SCILPY_HOME, 'bundles', 'bundle_all_1mm.trk')
+    in_model = os.path.join(SCILPY_HOME, 'bundles', 'fibercup_atlas',
                             'subj_1', 'bundle_0.trk')
-    in_aff = os.path.join(get_home(), 'bundles',
-                          'affine.txt')
-    in_ref = os.path.join(get_home(), 'bundles',
-                          'bundle_all_1mm.nii.gz')
-    ret = script_runner.run('scil_recognize_single_bundle.py', in_tractogram,
-                            in_model, in_aff, 'bundle_0_reco.tck',
-                            '--inverse', '--tractogram_clustering_thr', '12',
-                            '--slr_threads', '1', '--out_pickle',
-                            'clusters.pkl', '--reference', in_ref)
+    ret = script_runner.run(
+        'scil_bundle_score_same_bundle_many_segmentations.py',
+        in_1, in_2, 'AF_L_binary.json', '--streamlines_measures', in_model,
+        in_tractogram, '--processes', '1', '--reference', in_ref)
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_register_tractogram.py` & `scilpy-2.0.0/scripts/tests/test_header_validate_compatibility.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,31 +1,26 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['bundles.zip'])
+fetch_data(get_testing_files_dict(), keys=['filtering.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_register_tractogram.py', '--help')
+    ret = script_runner.run('scil_header_validate_compatibility.py', '--help')
     assert ret.success
 
 
-def test_execution_bundles(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_moving = os.path.join(get_home(), 'bundles',
-                             'bundle_0_reco.tck')
-    in_static = os.path.join(get_home(), 'bundles', 'voting_results',
-                             'bundle_0.trk')
-    in_ref = os.path.join(get_home(), 'bundles',
-                          'bundle_all_1mm.nii.gz')
-    ret = script_runner.run('scil_register_tractogram.py', in_moving,
-                            in_static, '--only_rigid',
-                            '--moving_tractogram_ref', in_ref)
+def test_execution_filtering(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_bundle = os.path.join(SCILPY_HOME, 'filtering', 'bundle_all_1mm.trk')
+    in_roi = os.path.join(SCILPY_HOME, 'filtering', 'mask.nii.gz')
+    ret = script_runner.run('scil_header_validate_compatibility.py',
+                            in_bundle, in_roi)
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_remove_outliers_ransac.py` & `scilpy-2.0.0/scripts/tests/test_mti_adjust_B1_header.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,25 +1,31 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['processing.zip'])
+fetch_data(get_testing_files_dict(), keys=['ihMT.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_remove_outliers_ransac.py', '--help')
+    ret = script_runner.run('scil_mti_adjust_B1_header.py', '--help')
     assert ret.success
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_ad = os.path.join(get_home(), 'processing',
-                         'ad.nii.gz')
-    ret = script_runner.run('scil_remove_outliers_ransac.py', in_ad,
-                            'ad_ransanc.nii.gz')
+def test_execution_ihMT_no_option(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+
+    in_b1_map = os.path.join(SCILPY_HOME,
+                             'MT', 'sub-001_run-01_B1map.nii.gz')
+    in_b1_json = os.path.join(SCILPY_HOME,
+                              'MT', 'sub-001_run-01_B1map.json')
+
+    # no option
+    ret = script_runner.run('scil_mti_adjust_B1_header.py', in_b1_map,
+                            tmp_dir.name, in_b1_json, '-f')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_reorder_connectivity.py` & `scilpy-2.0.0/scripts/tests/test_connectivity_reorder_rois.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,46 +1,44 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['connectivity.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_reorder_connectivity.py', '--help')
+    ret = script_runner.run('scil_connectivity_reorder_rois.py', '--help')
     assert ret.success
 
 
-def test_execution_compute_OLO(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_sc = os.path.join(get_home(), 'connectivity',
+def test_execution_compute_OLO(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_sc = os.path.join(SCILPY_HOME, 'connectivity',
                          'sc_norm.npy')
-    in_labels_list = os.path.join(get_home(), 'connectivity',
+    in_labels_list = os.path.join(SCILPY_HOME, 'connectivity',
                                   'labels_list.txt')
-    ret = script_runner.run('scil_reorder_connectivity.py', in_sc,
+    ret = script_runner.run('scil_connectivity_reorder_rois.py', in_sc,
                             '--optimal_leaf_ordering', 'OLO.txt',
                             '--out_dir', os.path.expanduser(tmp_dir.name),
                             '--labels_list', in_labels_list, '-f')
     assert ret.success
 
 
-def test_execution_apply_ordering(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_sc = os.path.join(get_home(), 'connectivity',
-                         'sc_norm.npy')
-    in_txt = os.path.join(get_home(), 'connectivity',
-                          'reorder.txt')
-    in_labels_list = os.path.join(get_home(), 'connectivity',
+def test_execution_apply_ordering(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_sc = os.path.join(SCILPY_HOME, 'connectivity', 'sc_norm.npy')
+    in_txt = os.path.join(SCILPY_HOME, 'connectivity', 'reorder.txt')
+    in_labels_list = os.path.join(SCILPY_HOME, 'connectivity',
                                   'labels_list.txt')
-    ret = script_runner.run('scil_reorder_connectivity.py', in_sc,
+    ret = script_runner.run('scil_connectivity_reorder_rois.py', in_sc,
                             '--in_ordering', in_txt,
                             '--out_suffix', '_sc_reo',
                             '--out_dir', os.path.expanduser(tmp_dir.name),
                             '--labels_list', in_labels_list, '-f')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_resample_tractogram.py` & `scilpy-2.0.0/scripts/tests/test_tractogram_resample.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,49 +1,61 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['tracking.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_resample_tractogram.py', '--help')
+    ret = script_runner.run('scil_tractogram_resample.py', '--help')
     assert ret.success
 
 
-def test_execution_downsample(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_tracto = os.path.join(get_home(), 'tracking',
+def test_execution_downsample(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tracto = os.path.join(SCILPY_HOME, 'tracking',
                              'union_shuffle_sub.trk')
-    ret = script_runner.run('scil_resample_tractogram.py', in_tracto,
+    ret = script_runner.run('scil_tractogram_resample.py', in_tracto,
                             '500', 'union_shuffle_sub_downsampled.trk')
     assert ret.success
 
 
-def test_execution_upsample(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_tracto = os.path.join(get_home(), 'tracking',
+def test_execution_upsample(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tracto = os.path.join(SCILPY_HOME, 'tracking',
                              'union_shuffle_sub.trk')
 
     # in_tracto contains 761 streamlines. 2000=upsample. 200=downsample.
 
-    ret = script_runner.run('scil_resample_tractogram.py', in_tracto,
+    ret = script_runner.run('scil_tractogram_resample.py', in_tracto,
                             '2000', 'union_shuffle_sub_upsampled.trk', '-f',
                             '--point_wise_std', '0.5')
     assert ret.success
 
-    ret = script_runner.run('scil_resample_tractogram.py', in_tracto,
+    ret = script_runner.run('scil_tractogram_resample.py', in_tracto,
                             '200', 'union_shuffle_sub_downsampled.trk',
                             '--point_wise_std', '0.5', '-f')
     assert ret.success
 
-    ret = script_runner.run('scil_resample_tractogram.py', in_tracto,
+    ret = script_runner.run('scil_tractogram_resample.py', in_tracto,
                             '200', 'union_shuffle_sub_downsampled.trk',
                             '--point_wise_std', '0.5', '-f',
                             '--downsample_per_cluster')
     assert ret.success
+
+
+def test_execution_upsample_ptt(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tracto = os.path.join(SCILPY_HOME, 'tracking',
+                             'union_shuffle_sub.trk')
+
+    ret = script_runner.run('scil_tractogram_resample.py', in_tracto,
+                            '500', 'union_shuffle_sub_upsampled.trk', '-f',
+                            '--point_wise_std', '10', '--tube_radius', '5')
+    assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_reshape_to_reference.py` & `scilpy-2.0.0/scripts/tests/test_volume_count_non_zero_voxels.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,37 +1,37 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['others.zip', 'commit_amico.zip'])
+fetch_data(get_testing_files_dict(), keys=['others.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_reshape_to_reference.py', '--help')
+    ret = script_runner.run('scil_volume_count_non_zero_voxels.py', '--help')
     assert ret.success
 
 
-def test_execution_others(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_img = os.path.join(get_home(), 'others', 't1_crop.nii.gz')
-    in_ref = os.path.join(get_home(), 'others', 't1.nii.gz')
-    ret = script_runner.run('scil_reshape_to_reference.py', in_img,
-                            in_ref, 't1_reshape.nii.gz',
-                            '--interpolation', 'nearest')
+def test_execution_simple_print(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_img = os.path.join(SCILPY_HOME, 'others', 'rgb.nii.gz')
+    ret = script_runner.run('scil_volume_count_non_zero_voxels.py', in_img)
     assert ret.success
 
 
-def test_execution_4D(script_runner):
+def test_execution_save_in_file(script_runner):
     os.chdir(os.path.expanduser(tmp_dir.name))
-    in_img = os.path.join(get_home(), 'commit_amico', 'dwi.nii.gz')
-    in_ref = os.path.join(get_home(), 'others', 't1.nii.gz')
-    ret = script_runner.run('scil_reshape_to_reference.py', in_img,
-                            in_ref, 'dwi_reshape.nii.gz',
-                            '--interpolation', 'nearest')
+    in_img = os.path.join(SCILPY_HOME, 'others', 'rgb.nii.gz')
+    ret = script_runner.run('scil_volume_count_non_zero_voxels.py', in_img,
+                            '--out', 'printed.txt')
+    assert ret.success
+
+    # Then re-use the same out file with --stats
+    ret = script_runner.run('scil_volume_count_non_zero_voxels.py', in_img,
+                            '--out', 'printed.txt', '--stats')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_run_commit.py` & `scilpy-2.0.0/scripts/tests/test_tractogram_commit.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,42 +1,55 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['commit_amico.zip'])
-tmp_dir = tempfile.TemporaryDirectory()
+
+# ToDo: For more coverage, add test using a hdf5 as input.
+in_tracking = os.path.join(SCILPY_HOME, 'commit_amico', 'tracking.trk')
+in_dwi = os.path.join(SCILPY_HOME, 'commit_amico', 'dwi.nii.gz')
+in_bval = os.path.join(SCILPY_HOME, 'commit_amico', 'dwi.bval')
+in_bvec = os.path.join(SCILPY_HOME, 'commit_amico', 'dwi.bvec')
+in_mask = os.path.join(SCILPY_HOME, 'commit_amico', 'mask.nii.gz')
+in_peaks = os.path.join(SCILPY_HOME, 'commit_amico', 'peaks.nii.gz')
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_run_commit.py', '--help')
+    ret = script_runner.run('scil_tractogram_commit.py', '--help')
     assert ret.success
 
 
-def test_execution_commit_amico(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_tracking = os.path.join(get_home(), 'commit_amico',
-                               'tracking.trk')
-    in_dwi = os.path.join(get_home(), 'commit_amico',
-                          'dwi.nii.gz')
-    in_bval = os.path.join(get_home(), 'commit_amico',
-                           'dwi.bval')
-    in_bvec = os.path.join(get_home(), 'commit_amico',
-                           'dwi.bvec')
-    in_mask = os.path.join(get_home(), 'commit_amico',
-                           'mask.nii.gz')
-    in_peaks = os.path.join(get_home(), 'commit_amico',
-                            'peaks.nii.gz')
-    ret = script_runner.run('scil_run_commit.py', in_tracking, in_dwi,
-                            in_bval, in_bvec, 'results_bzs/',
-                            '--b_thr', '30', '--nbr_dir', '500',
-                            '--nbr_iter', '500', '--in_peaks', in_peaks,
-                            '--in_tracking_mask', in_mask,
-                            '--para_diff', '1.7E-3',
-                            '--perp_diff', '1.19E-3', '0.85E-3', '0.51E-3', '0.17E-3',
-                            '--iso_diff', '1.7E-3', '3.0E-3',
-                            '--processes', '1')
+def test_execution_commit_amico(script_runner, monkeypatch):
+    tmp_dir = tempfile.TemporaryDirectory()
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+
+    ret = script_runner.run(
+        'scil_tractogram_commit.py', in_tracking, in_dwi, in_bval, in_bvec,
+        'results_bzs/', '--tol', '30', '--nbr_dir', '500',
+        '--nbr_iter', '500', '--in_peaks', in_peaks,
+        '--in_tracking_mask', in_mask,
+        '--perp_diff', '1.19E-3', '0.85E-3', '0.51E-3', '0.17E-3',
+        '--iso_diff', '1.7E-3', '3.0E-3',
+        '--processes', '1', '--keep_whole_tractogram')
     assert ret.success
+
+
+def test_execution_commit2(script_runner, monkeypatch):
+    tmp_dir = tempfile.TemporaryDirectory()
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+
+    # TODO Add a HDF5 in our test data that could be used here.
+    ret = script_runner.run(
+        'scil_tractogram_commit.py', in_tracking, in_dwi, in_bval, in_bvec,
+        'results_bzs/', '--tol', '30', '--nbr_dir', '500',
+        '--nbr_iter', '500', '--in_peaks', in_peaks,
+        '--in_tracking_mask', in_mask,
+        '--perp_diff', '1.19E-3', '0.85E-3', '0.51E-3', '0.17E-3',
+        '--iso_diff', '1.7E-3', '3.0E-3',
+        '--processes', '1', '--commit2')
+    assert not ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_run_nlmeans.py` & `scilpy-2.0.0/scripts/tests/test_denoising_nlmeans.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,26 +1,25 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['others.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_run_nlmeans.py', '--help')
+    ret = script_runner.run('scil_denoising_nlmeans.py', '--help')
     assert ret.success
 
 
-def test_execution_others(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_img = os.path.join(get_home(), 'others',
-                          't1_resample.nii.gz')
-    ret = script_runner.run('scil_run_nlmeans.py', in_img,
+def test_execution_others(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_img = os.path.join(SCILPY_HOME, 'others', 't1_resample.nii.gz')
+    ret = script_runner.run('scil_denoising_nlmeans.py', in_img,
                             't1_denoised.nii.gz',  '4', '--processes', '1')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_score_bundles.py` & `scilpy-2.0.0/scripts/tests/test_bundle_score_many_bundles_one_tractogram.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,48 +1,48 @@
 import json
 import os
 import shutil
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['tracking.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_score_bundles.py',
+    ret = script_runner.run('scil_bundle_score_many_bundles_one_tractogram.py',
                             '--help')
     assert ret.success
 
 
-def test_score_bundles(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_tractogram = os.path.join(get_home(), 'tracking', 'pft.trk')
+def test_score_bundles(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tractogram = os.path.join(SCILPY_HOME, 'tracking', 'pft.trk')
 
     # Pretend we have our segmented bundles on disk
     shutil.copyfile(in_tractogram, './NC.trk')
     os.mkdir('./segmented_VB')
     shutil.copyfile(in_tractogram, './segmented_VB/bundle1_VS.trk')
     shutil.copyfile(in_tractogram, './segmented_VB/bundle2_VS.trk')
     os.mkdir('./segmented_IB')
     shutil.copyfile(in_tractogram, './segmented_IB/roi1_roi2_VS.trk')
 
     json_contents = {
         "bundle1": {
-            "gt_mask": os.path.join(get_home(), 'tracking',
+            "gt_mask": os.path.join(SCILPY_HOME, 'tracking',
                                     'seeding_mask.nii.gz'),
         },
         "bundle2": {
-            "gt_mask": os.path.join(get_home(), 'tracking',
+            "gt_mask": os.path.join(SCILPY_HOME, 'tracking',
                                     'seeding_mask.nii.gz'),
         }
     }
     with open(os.path.join("config_file.json"), "w") as f:
         json.dump(json_contents, f)
 
-    ret = script_runner.run('scil_score_bundles.py',
+    ret = script_runner.run('scil_bundle_score_many_bundles_one_tractogram.py',
                             "config_file.json", "./", '--no_bbox_check')
 
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_score_tractogram.py` & `scilpy-2.0.0/scripts/tests/test_tractogram_segment_and_score.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,42 +1,41 @@
 import json
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['tracking.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_score_tractogram.py',
-                            '--help')
+    ret = script_runner.run('scil_tractogram_segment_and_score.py', '--help')
     assert ret.success
 
 
-def test_score_bundles(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_tractogram = os.path.join(get_home(), 'tracking', 'pft.trk')
+def test_score_bundles(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tractogram = os.path.join(SCILPY_HOME, 'tracking', 'pft.trk')
 
     json_contents = {
         "example_bundle": {
             "angle": 300,
             "length": [30, 190],
-            "any_mask": os.path.join(get_home(), 'tracking',
+            "any_mask": os.path.join(SCILPY_HOME, 'tracking',
                                      'seeding_mask.nii.gz'),
-            "gt_mask": os.path.join(get_home(), 'tracking',
+            "gt_mask": os.path.join(SCILPY_HOME, 'tracking',
                                     'seeding_mask.nii.gz'),
-            "endpoints": os.path.join(get_home(), 'tracking',
+            "endpoints": os.path.join(SCILPY_HOME, 'tracking',
                                       'interface.nii.gz')
         }
     }
     with open(os.path.join("config_file.json"), "w") as f:
         json.dump(json_contents, f)
 
-    ret = script_runner.run('scil_score_tractogram.py',
+    ret = script_runner.run('scil_tractogram_segment_and_score.py',
                             in_tractogram, "config_file.json",
                             'scoring_tractogram/', '--no_empty',
                             '--use_gt_masks_as_all_masks')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_snr_in_roi.py` & `scilpy-2.0.0/scripts/tests/test_dwi_extract_b0.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,36 +1,35 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
+# If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['processing.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_snr_in_roi.py', '--help')
+    ret = script_runner.run('scil_dwi_extract_b0.py', '--help')
     assert ret.success
 
 
-def test_snr(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_dwi = os.path.join(get_home(), 'processing',
-                          'dwi.nii.gz')
-    in_bval = os.path.join(get_home(), 'processing',
+def test_execution_processing(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_dwi = os.path.join(SCILPY_HOME, 'processing',
+                          'dwi_crop.nii.gz')
+    in_bval = os.path.join(SCILPY_HOME, 'processing',
                            'dwi.bval')
-    in_bvec = os.path.join(get_home(), 'processing',
+    in_bvec = os.path.join(SCILPY_HOME, 'processing',
                            'dwi.bvec')
-    in_mask = os.path.join(get_home(), 'processing',
-                           'cc.nii.gz')
-    noise_mask = os.path.join(get_home(), 'processing',
-                              'small_roi_gm_mask.nii.gz')
-
-    ret = script_runner.run('scil_snr_in_roi.py', in_dwi,
-                            in_bval, in_bvec, in_mask,
-                            '--noise_mask', noise_mask,
-                            '--b0_thr', '10')
+    ret = script_runner.run('scil_dwi_extract_b0.py', in_dwi, in_bval, in_bvec,
+                            'b0_mean.nii.gz', '--mean', '--b0', '20')
     assert ret.success
+
+    # Test wrong b0. Current minimal b-value is 5.
+    ret = script_runner.run('scil_dwi_extract_b0.py', in_dwi, in_bval, in_bvec,
+                            'b0_mean.nii.gz', '--mean', '--b0', '1', '-f')
+    assert not ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_split_image.py` & `scilpy-2.0.0/scripts/tests/test_dwi_split_by_indices.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,45 +1,40 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['processing.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_split_image.py', '--help')
+    ret = script_runner.run('scil_dwi_split_by_indices.py', '--help')
     assert ret.success
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_dwi = os.path.join(get_home(), 'processing',
-                          'dwi_crop.nii.gz')
-    in_bval = os.path.join(get_home(), 'processing',
-                           'dwi.bval')
-    in_bvec = os.path.join(get_home(), 'processing',
-                           'dwi.bvec')
-    ret = script_runner.run('scil_split_image.py', in_dwi,
+def test_execution_processing(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_dwi = os.path.join(SCILPY_HOME, 'processing', 'dwi_crop.nii.gz')
+    in_bval = os.path.join(SCILPY_HOME, 'processing', 'dwi.bval')
+    in_bvec = os.path.join(SCILPY_HOME, 'processing', 'dwi.bvec')
+    ret = script_runner.run('scil_dwi_split_by_indices.py', in_dwi,
                             in_bval, in_bvec, 'dwi', '5', '15', '25')
     assert ret.success
 
 
-def test_execution_processing_wrong_indices_given(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_dwi = os.path.join(get_home(), 'processing',
-                          'dwi_crop.nii.gz')
-    in_bval = os.path.join(get_home(), 'processing',
-                           'dwi.bval')
-    in_bvec = os.path.join(get_home(), 'processing',
-                           'dwi.bvec')
-    ret = script_runner.run('scil_split_image.py', in_dwi,
+def test_execution_processing_wrong_indices_given(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_dwi = os.path.join(SCILPY_HOME, 'processing', 'dwi_crop.nii.gz')
+    in_bval = os.path.join(SCILPY_HOME, 'processing', 'dwi.bval')
+    in_bvec = os.path.join(SCILPY_HOME, 'processing', 'dwi.bvec')
+    ret = script_runner.run('scil_dwi_split_by_indices.py', in_dwi,
                             in_bval, in_bvec, 'dwi', '0', '15', '25')
     assert (not ret.success)
-    ret = script_runner.run('scil_split_image.py', in_dwi,
+    ret = script_runner.run('scil_dwi_split_by_indices.py', in_dwi,
                             in_bval, in_bvec, 'dwi', '5', '15', '200')
     assert (not ret.success)
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_split_volume_by_labels.py` & `scilpy-2.0.0/scripts/tests/test_labels_split_volume_from_lut.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,28 +1,29 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['atlas.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_split_volume_by_labels.py', '--help')
+    ret = script_runner.run('scil_labels_split_volume_from_lut.py', '--help')
     assert ret.success
 
 
-def test_execution_atlas(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_atlas = os.path.join(get_home(), 'atlas',
+def test_execution_atlas(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_atlas = os.path.join(SCILPY_HOME, 'atlas',
                             'atlas_freesurfer_v2.nii.gz')
-    in_json = os.path.join(get_home(), 'atlas',
+    in_json = os.path.join(SCILPY_HOME, 'atlas',
                            'atlas_freesurfer_v2_LUT.json')
-    ret = script_runner.run('scil_split_volume_by_labels.py', in_atlas,
+    ret = script_runner.run('scil_labels_split_volume_from_lut.py', in_atlas,
                             '--out_prefix', 'brainstem',
                             '--custom_lut', in_json)
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_streamlines_math.py` & `scilpy-2.0.0/scripts/tests/test_tractogram_register.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,30 +1,31 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['others.zip'])
+fetch_data(get_testing_files_dict(), keys=['bundles.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
-##### Deprecated file but it should still be running.
-##### For more exhaustive tests, see test_tractogram_math.py
-
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_streamlines_math.py', '--help')
+    ret = script_runner.run('scil_tractogram_register.py', '--help')
     assert ret.success
 
 
-def test_execution_union_color(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_tracto_1 = os.path.join(get_home(), 'others',
-                               'fibercup_bundles_color.trk')
-    in_tracto_2 = os.path.join(get_home(), 'others',
-                               'fibercup_bundle_0_color.trk')
-    ret = script_runner.run('scil_streamlines_math.py', 'union',
-                            in_tracto_1, in_tracto_2, 'union_color.trk')
+def test_execution_bundles(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_moving = os.path.join(SCILPY_HOME, 'bundles',
+                             'bundle_0_reco.tck')
+    in_static = os.path.join(SCILPY_HOME, 'bundles', 'voting_results',
+                             'bundle_0.trk')
+    in_ref = os.path.join(SCILPY_HOME, 'bundles',
+                          'bundle_all_1mm.nii.gz')
+    ret = script_runner.run('scil_tractogram_register.py', in_moving,
+                            in_static, '--only_rigid',
+                            '--moving_tractogram_ref', in_ref)
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_swap_gradient_axis.py` & `scilpy-2.0.0/scripts/tests/test_viz_bingham_fit.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,25 +1,28 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
-# If they already exist, this only takes 5 seconds (check md5sum)
-fetch_data(get_testing_files_dict(), keys=['processing.zip'])
+fetch_data(get_testing_files_dict(), keys=['tracking.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_swap_gradient_axis.py', '--help')
+    ret = script_runner.run('scil_viz_bingham_fit.py', '--help')
     assert ret.success
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_encoding = os.path.join(get_home(), 'processing',
-                               '1000.b')
-    ret = script_runner.run('scil_swap_gradient_axis.py', in_encoding,
-                            '1000_swaped.b', 'x', 'y', '--mrtrix', '-f')
-    assert ret.success
+def test_silent_without_output(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+
+    # dummy dataset (the script should raise an error before using it)
+    in_dummy = os.path.join(SCILPY_HOME, 'tracking', 'fodf.nii.gz')
+
+    ret = script_runner.run('scil_viz_bingham_fit.py', in_dummy,
+                            '--silent')
+
+    assert (not ret.success)
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_tractogram_math.py` & `scilpy-2.0.0/scripts/tests/test_tractogram_math.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,224 +1,190 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['others.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
+trk_path = os.path.join(SCILPY_HOME, 'others')
 
 
 def test_help_option(script_runner):
     ret = script_runner.run('scil_tractogram_math.py', '--help')
     assert ret.success
 
 
-def test_execution_lazy_concatenate_no_color(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_tracto_1 = os.path.join(get_home(), 'others',
-                               'fibercup_bundles.trk')
-    in_tracto_2 = os.path.join(get_home(), 'others',
-                               'fibercup_bundle_0.trk')
+def test_execution_lazy_concatenate_no_color(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tracto_1 = os.path.join(trk_path, 'fibercup_bundles.trk')
+    in_tracto_2 = os.path.join(trk_path, 'fibercup_bundle_0.trk')
     ret = script_runner.run('scil_tractogram_math.py', 'lazy_concatenate',
                             in_tracto_1, in_tracto_2,
                             'lazy_concatenate.trk')
     assert ret.success
 
 
-def test_execution_lazy_concatenate_mix(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_tracto_1 = os.path.join(get_home(), 'others',
-                               'fibercup_bundles_color.trk')
-    in_tracto_2 = os.path.join(get_home(), 'others',
-                               'fibercup_bundle_0.trk')
+def test_execution_lazy_concatenate_mix(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tracto_1 = os.path.join(trk_path, 'fibercup_bundles_color.trk')
+    in_tracto_2 = os.path.join(trk_path, 'fibercup_bundle_0.trk')
     ret = script_runner.run('scil_tractogram_math.py', 'lazy_concatenate',
                             in_tracto_1, in_tracto_2,
                             'lazy_concatenate_mix.trk')
     assert ret.success
 
 
-def test_execution_union_no_color(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_tracto_1 = os.path.join(get_home(), 'others',
-                               'fibercup_bundles.trk')
-    in_tracto_2 = os.path.join(get_home(), 'others',
-                               'fibercup_bundle_0.trk')
+def test_execution_union_no_color(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tracto_1 = os.path.join(trk_path, 'fibercup_bundles.trk')
+    in_tracto_2 = os.path.join(trk_path, 'fibercup_bundle_0.trk')
     ret = script_runner.run('scil_tractogram_math.py', 'union',
                             in_tracto_1, in_tracto_2, 'union.trk')
     assert ret.success
 
 
-def test_execution_intersection_no_color(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_tracto_1 = os.path.join(get_home(), 'others',
-                               'fibercup_bundles.trk')
-    in_tracto_2 = os.path.join(get_home(), 'others',
-                               'fibercup_bundle_0.trk')
+def test_execution_intersection_no_color(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tracto_1 = os.path.join(trk_path, 'fibercup_bundles.trk')
+    in_tracto_2 = os.path.join(trk_path, 'fibercup_bundle_0.trk')
     ret = script_runner.run('scil_tractogram_math.py', 'intersection',
                             in_tracto_1, in_tracto_2, 'intersection.trk')
     assert ret.success
 
 
-def test_execution_difference_no_color(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_tracto_1 = os.path.join(get_home(), 'others',
-                               'fibercup_bundles.trk')
-    in_tracto_2 = os.path.join(get_home(), 'others',
-                               'fibercup_bundle_0.trk')
+def test_execution_difference_no_color(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tracto_1 = os.path.join(trk_path, 'fibercup_bundles.trk')
+    in_tracto_2 = os.path.join(trk_path, 'fibercup_bundle_0.trk')
     ret = script_runner.run('scil_tractogram_math.py', 'difference',
                             in_tracto_1, in_tracto_2, 'difference.trk')
     assert ret.success
 
 
-def test_execution_concatenate_no_color(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_tracto_1 = os.path.join(get_home(), 'others',
-                               'fibercup_bundles.trk')
-    in_tracto_2 = os.path.join(get_home(), 'others',
-                               'fibercup_bundle_0.trk')
+def test_execution_concatenate_no_color(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tracto_1 = os.path.join(trk_path, 'fibercup_bundles.trk')
+    in_tracto_2 = os.path.join(trk_path, 'fibercup_bundle_0.trk')
     ret = script_runner.run('scil_tractogram_math.py', 'concatenate',
                             in_tracto_1, in_tracto_2, 'concatenate.trk')
     assert ret.success
 
 
-def test_execution_union_no_color_robust(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_tracto_1 = os.path.join(get_home(), 'others',
-                               'fibercup_bundles.trk')
-    in_tracto_2 = os.path.join(get_home(), 'others',
-                               'fibercup_bundle_0.trk')
+def test_execution_union_no_color_robust(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tracto_1 = os.path.join(trk_path, 'fibercup_bundles.trk')
+    in_tracto_2 = os.path.join(trk_path, 'fibercup_bundle_0.trk')
     ret = script_runner.run('scil_tractogram_math.py', 'union',
                             in_tracto_1, in_tracto_2, 'union_r.trk',
                             '--robust')
     assert ret.success
 
 
-def test_execution_intersection_no_color_robust(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_tracto_1 = os.path.join(get_home(), 'others',
-                               'fibercup_bundles.trk')
-    in_tracto_2 = os.path.join(get_home(), 'others',
-                               'fibercup_bundle_0.trk')
+def test_execution_intersection_no_color_robust(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tracto_1 = os.path.join(trk_path, 'fibercup_bundles.trk')
+    in_tracto_2 = os.path.join(trk_path, 'fibercup_bundle_0.trk')
     ret = script_runner.run('scil_tractogram_math.py', 'intersection',
                             in_tracto_1, in_tracto_2, 'intersection_r.trk',
                             '--robust')
     assert ret.success
 
 
-def test_execution_difference_no_color_robust(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_tracto_1 = os.path.join(get_home(), 'others',
-                               'fibercup_bundles.trk')
-    in_tracto_2 = os.path.join(get_home(), 'others',
-                               'fibercup_bundle_0.trk')
+def test_execution_difference_no_color_robust(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tracto_1 = os.path.join(trk_path, 'fibercup_bundles.trk')
+    in_tracto_2 = os.path.join(trk_path, 'fibercup_bundle_0.trk')
     ret = script_runner.run('scil_tractogram_math.py', 'difference',
                             in_tracto_1, in_tracto_2, 'difference_r.trk',
                             '--robust')
     assert ret.success
 
 
-def test_execution_union_color(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_tracto_1 = os.path.join(get_home(), 'others',
-                               'fibercup_bundles_color.trk')
-    in_tracto_2 = os.path.join(get_home(), 'others',
-                               'fibercup_bundle_0_color.trk')
+def test_execution_union_color(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tracto_1 = os.path.join(trk_path, 'fibercup_bundles_color.trk')
+    in_tracto_2 = os.path.join(trk_path, 'fibercup_bundle_0_color.trk')
     ret = script_runner.run('scil_tractogram_math.py', 'union',
                             in_tracto_1, in_tracto_2, 'union_color.trk')
     assert ret.success
 
 
-def test_execution_intersection_color(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_tracto_1 = os.path.join(get_home(), 'others',
-                               'fibercup_bundles_color.trk')
-    in_tracto_2 = os.path.join(get_home(), 'others',
-                               'fibercup_bundle_0_color.trk')
+def test_execution_intersection_color(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tracto_1 = os.path.join(trk_path, 'fibercup_bundles_color.trk')
+    in_tracto_2 = os.path.join(trk_path, 'fibercup_bundle_0_color.trk')
     ret = script_runner.run('scil_tractogram_math.py', 'intersection',
                             in_tracto_1, in_tracto_2, 'intersection_color.trk')
     assert ret.success
 
 
-def test_execution_difference_color(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_tracto_1 = os.path.join(get_home(), 'others',
-                               'fibercup_bundles_color.trk')
-    in_tracto_2 = os.path.join(get_home(), 'others',
-                               'fibercup_bundle_0_color.trk')
+def test_execution_difference_color(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tracto_1 = os.path.join(trk_path, 'fibercup_bundles_color.trk')
+    in_tracto_2 = os.path.join(trk_path, 'fibercup_bundle_0_color.trk')
     ret = script_runner.run('scil_tractogram_math.py', 'difference',
                             in_tracto_1, in_tracto_2, 'difference_color.trk')
     assert ret.success
 
 
-def test_execution_concatenate_color(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_tracto_1 = os.path.join(get_home(), 'others',
-                               'fibercup_bundles_color.trk')
-    in_tracto_2 = os.path.join(get_home(), 'others',
-                               'fibercup_bundle_0_color.trk')
+def test_execution_concatenate_color(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tracto_1 = os.path.join(trk_path, 'fibercup_bundles_color.trk')
+    in_tracto_2 = os.path.join(trk_path, 'fibercup_bundle_0_color.trk')
     ret = script_runner.run('scil_tractogram_math.py', 'concatenate',
                             in_tracto_1, in_tracto_2, 'concatenate_color.trk')
     assert ret.success
 
 
-def test_execution_union_mix(script_runner):
+def test_execution_union_mix(script_runner, monkeypatch):
     # This is intentionally failing
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_tracto_1 = os.path.join(get_home(), 'others',
-                               'fibercup_bundles_color.trk')
-    in_tracto_2 = os.path.join(get_home(), 'others',
-                               'fibercup_bundle_0.trk')
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tracto_1 = os.path.join(trk_path, 'fibercup_bundles_color.trk')
+    in_tracto_2 = os.path.join(trk_path, 'fibercup_bundle_0.trk')
     ret = script_runner.run('scil_tractogram_math.py', 'union',
                             in_tracto_1, in_tracto_2, 'union_mix.trk')
     assert not ret.success
 
 
-def test_execution_intersection_mix_fake(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_tracto_1 = os.path.join(get_home(), 'others',
-                               'fibercup_bundles_color.trk')
-    in_tracto_2 = os.path.join(get_home(), 'others',
-                               'fibercup_bundle_0.trk')
+def test_execution_intersection_mix_fake(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tracto_1 = os.path.join(trk_path, 'fibercup_bundles_color.trk')
+    in_tracto_2 = os.path.join(trk_path, 'fibercup_bundle_0.trk')
     ret = script_runner.run('scil_tractogram_math.py', 'intersection',
                             in_tracto_1, in_tracto_2, 'intersection_mix.trk',
                             '--fake_metadata')
     assert ret.success
 
 
-def test_execution_difference_empty_result(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_tracto_1 = os.path.join(get_home(), 'others',
-                               'fibercup_bundle_0.trk')
-    in_tracto_2 = os.path.join(get_home(), 'others',
-                               'fibercup_bundle_0_color.trk')
+def test_execution_difference_empty_result(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tracto_1 = os.path.join(trk_path, 'fibercup_bundle_0.trk')
+    in_tracto_2 = os.path.join(trk_path, 'fibercup_bundle_0_color.trk')
     ret = script_runner.run('scil_tractogram_math.py', 'difference',
                             in_tracto_1, in_tracto_2,
                             'difference_empty_results.trk', '--no_metadata')
     assert ret.success
 
 
-def test_execution_difference_empty_input_1(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_tracto_1 = os.path.join(get_home(), 'others',
-                               'empty.trk')
-    in_tracto_2 = os.path.join(get_home(), 'others',
-                               'fibercup_bundle_0_color.trk')
+def test_execution_difference_empty_input_1(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tracto_1 = os.path.join(trk_path, 'empty.trk')
+    in_tracto_2 = os.path.join(trk_path, 'fibercup_bundle_0_color.trk')
     ret = script_runner.run('scil_tractogram_math.py', 'difference',
                             in_tracto_1, in_tracto_2, 'difference_empty_1.trk',
                             '--no_metadata')
     assert ret.success
 
 
-def test_execution_difference_empty_input_2(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_tracto_1 = os.path.join(get_home(), 'others',
-                               'fibercup_bundle_0_color.trk')
-    in_tracto_2 = os.path.join(get_home(), 'others',
-                               'empty.trk')
+def test_execution_difference_empty_input_2(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_tracto_1 = os.path.join(trk_path, 'fibercup_bundle_0_color.trk')
+    in_tracto_2 = os.path.join(trk_path, 'empty.trk')
     ret = script_runner.run('scil_tractogram_math.py', 'difference',
                             in_tracto_1, in_tracto_2, 'difference_empty_2.trk',
                             '--no_metadata')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_validate_and_correct_bvecs.py` & `scilpy-2.0.0/scripts/tests/test_dti_convert_tensors.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,35 +1,35 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['processing.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_validate_and_correct_bvecs.py', '--help')
+    ret = script_runner.run('scil_dti_convert_tensors.py', '--help')
     assert ret.success
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_dwi = os.path.join(get_home(), 'processing',
-                          'dwi_crop_1000.nii.gz')
-    in_bval = os.path.join(get_home(), 'processing',
-                           '1000.bval')
-    in_bvec = os.path.join(get_home(), 'processing',
-                           '1000.bvec')
-
-    # generate the peaks file and fa map we'll use to test our script
-    script_runner.run('scil_compute_dti_metrics.py', in_dwi, in_bval, in_bvec,
-                      '--not_all', '--fa', 'fa.nii.gz',
-                      '--evecs', 'evecs.nii.gz')
-    # test the actual script
-    ret = script_runner.run('scil_validate_and_correct_bvecs.py', in_bvec,
-                            'evecs_v1.nii.gz', 'fa.nii.gz', 'bvec_corr', '-v')
+def test_execution_processing(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+
+    # No tensor in the current test data! I'm running the dti_metrics
+    # to create one.
+    in_dwi = os.path.join(SCILPY_HOME, 'processing', 'dwi_crop_1000.nii.gz')
+    in_bval = os.path.join(SCILPY_HOME, 'processing', '1000.bval')
+    in_bvec = os.path.join(SCILPY_HOME, 'processing', '1000.bvec')
+    script_runner.run('scil_dti_metrics.py', in_dwi,
+                      in_bval, in_bvec, '--not_all',
+                      '--tensor', 'tensors.nii.gz', '--tensor_format', 'fsl')
+
+    ret = script_runner.run('scil_dti_convert_tensors.py', 'tensors.nii.gz',
+                            'converted_tensors.nii.gz', 'fsl', 'mrtrix')
+
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_validate_bids.py` & `scilpy-2.0.0/scripts/tests/test_bids_validate.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,18 +1,20 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import json
+import os
+import tempfile
+
 import nibabel as nib
 import numpy as np
-import os
 import pytest
-import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['bids_json.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 conversion = {"LR": "i",
               "RL": "i-",
@@ -307,29 +309,29 @@
 
     # Clean test_json
     for key, value in test_json.items():
         if isinstance(value, str):
             test_json[key] = value.replace(test_dir + os.path.sep,'')
 
     # Open correct json file
-    result_json = os.path.join(get_home(), 'bids_json', json_output.replace('test', 'result'))
+    result_json = os.path.join(SCILPY_HOME, 'bids_json', json_output.replace('test', 'result'))
 
     with open(result_json, 'r') as f:
         result = json.load(f)[0]
 
     if not sorted(test_json.items()) == sorted(result.items()):
         print(test_json)
         print(result)
 
     # Compare json files
     return sorted(test_json.items()) == sorted(result.items())
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_validate_bids.py', '--help')
+    ret = script_runner.run('scil_bids_validate.py', '--help')
     assert ret.success
 
 
 
 @pytest.mark.parametrize(
     "dwi_is_complex,json_output",
     [(False, 'test_real_dwi_epi.json'),
@@ -340,15 +342,15 @@
     test_dir = generate_fake_bids_structure(
         tmpdir, 1, 1,
         gen_anat_t1=True,
         gen_epi=True,
         complex_dwi=dwi_is_complex)
 
     ret = script_runner.run(
-        'scil_validate_bids.py',
+        'scil_bids_validate.py',
         test_dir,
         os.path.join(test_dir, json_output),
         '-f', '-v')
 
     if ret.success:
         assert compare_jsons(json_output, test_dir)
     else:
@@ -367,15 +369,15 @@
         gen_anat_t1=True,
         gen_sbref=True,
         gen_epi=True,
         complex_dwi=dwi_is_complex,
         complex_sbref=sbref_is_complex)
 
     ret = script_runner.run(
-        'scil_validate_bids.py',
+        'scil_bids_validate.py',
         test_dir,
         os.path.join(test_dir, json_output),
         '-f', '-v')
 
     if ret.success:
         assert compare_jsons(json_output, test_dir)
     else:
@@ -393,15 +395,15 @@
         tmpdir, 1, 1,
         gen_anat_t1=True,
         gen_rev_dwi=True,
         complex_dwi=dwi_is_complex,
         complex_rev_dwi=rev_is_complex)
 
     ret = script_runner.run(
-        'scil_validate_bids.py',
+        'scil_bids_validate.py',
         test_dir,
         os.path.join(test_dir, json_output),
         '-f', '-v')
 
     if ret.success:
         assert compare_jsons(json_output, test_dir)
     else:
@@ -421,15 +423,15 @@
         gen_rev_dwi=True,
         gen_sbref=True,
         gen_rev_sbref=True,
         complex_dwi=dwi_is_complex,
         complex_rev_dwi=rev_is_complex)
 
     ret = script_runner.run(
-        'scil_validate_bids.py',
+        'scil_bids_validate.py',
         test_dir,
         os.path.join(test_dir, json_output),
         '-f', '-v')
 
     if ret.success:
         assert compare_jsons(json_output, test_dir)
     else:
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_visualize_bingham_fit.py` & `scilpy-2.0.0/scripts/tests/test_viz_bundle.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,27 +1,25 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
-import os
-import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
-
-fetch_data(get_testing_files_dict(), keys=['tracking.zip'])
-tmp_dir = tempfile.TemporaryDirectory()
+# If they already exist, this only takes 5 seconds (check md5sum)
+# fetch_data(get_testing_files_dict(), keys=['bundles.zip'])
+# tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_visualize_bingham_fit.py', '--help')
+    ret = script_runner.run('scil_viz_bundle.py', '--help')
     assert ret.success
 
-
-def test_silent_without_output(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-
-    # dummy dataset (the script should raise an error before using it)
-    in_dummy = os.path.join(get_home(), 'tracking', 'fodf.nii.gz')
-
-    ret = script_runner.run('scil_visualize_bingham_fit.py', in_dummy,
-                            '--silent')
-
-    assert (not ret.success)
+# Tests including VTK do not work on a server without a display
+# def test_image_create(script_runner):
+#     monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+#     in_vol = os.path.join(
+#         SCILPY_HOME, 'bundles', 'fibercup_atlas', 'bundle_all_1mm.nii.gz')
+
+#     in_bundle = os.path.join(
+#         SCILPY_HOME, 'bundles', 'fibercup_atlas', 'subj_1', 'bundle_0.trk')
+
+#     ret = script_runner.run('scil_viz_bundle.py',
+#                             in_vol, in_bundle, 'out.png')
+#     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_visualize_connectivity.py` & `scilpy-2.0.0/scripts/tests/test_viz_connectivity.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,31 +1,31 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import get_testing_files_dict, fetch_data, get_home
-
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['connectivity.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_visualize_connectivity.py', '--help')
+    ret = script_runner.run('scil_viz_connectivity.py', '--help')
     assert ret.success
 
 
-def test_execution_connectivity(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_sc = os.path.join(get_home(), 'connectivity',
+def test_execution_connectivity(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_sc = os.path.join(SCILPY_HOME, 'connectivity',
                          'sc_norm.npy')
-    in_labels_list = os.path.join(get_home(), 'connectivity',
+    in_labels_list = os.path.join(SCILPY_HOME, 'connectivity',
                                   'labels_list.txt')
-    ret = script_runner.run('scil_visualize_connectivity.py', in_sc,
+    ret = script_runner.run('scil_viz_connectivity.py', in_sc,
                             'sc_norm.png', '--log', '--display_legend',
                             '--labels_list', in_labels_list,
                             '--histogram', 'hist.png', '--nb_bins', '50',
                             '--exclude_zeros', '--chord_chart', 'sc_chord.png')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/scripts/tests/test_visualize_scatterplot.py` & `scilpy-2.0.0/scripts/tests/test_viz_volume_scatterplot.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,107 +1,109 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 import os
 import tempfile
 
-from scilpy.io.fetcher import fetch_data, get_home, get_testing_files_dict
+from scilpy import SCILPY_HOME
+from scilpy.io.fetcher import fetch_data, get_testing_files_dict
 
 # If they already exist, this only takes 5 seconds (check md5sum)
 fetch_data(get_testing_files_dict(), keys=['plot.zip'])
 tmp_dir = tempfile.TemporaryDirectory()
 
 
 def test_help_option(script_runner):
-    ret = script_runner.run('scil_visualize_scatterplot.py', '--help')
+    ret = script_runner.run('scil_viz_volume_scatterplot.py', '--help')
     assert ret.success
 
 
-def test_execution_processing(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_x = os.path.join(get_home(), 'plot',
+def test_execution_processing(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_x = os.path.join(SCILPY_HOME, 'plot',
                         'fa.nii.gz')
-    in_y = os.path.join(get_home(), 'plot',
+    in_y = os.path.join(SCILPY_HOME, 'plot',
                         'ad.nii.gz')
-    ret = script_runner.run('scil_visualize_scatterplot.py', in_x, in_y,
+    ret = script_runner.run('scil_viz_volume_scatterplot.py', in_x, in_y,
                             'scatter_plot.png')
     assert ret.success
 
 
-def test_execution_processing_bin_mask(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_x = os.path.join(get_home(), 'plot',
+def test_execution_processing_bin_mask(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_x = os.path.join(SCILPY_HOME, 'plot',
                         'fa.nii.gz')
-    in_y = os.path.join(get_home(), 'plot',
+    in_y = os.path.join(SCILPY_HOME, 'plot',
                         'ad.nii.gz')
-    in_mask = os.path.join(get_home(), 'plot',
+    in_mask = os.path.join(SCILPY_HOME, 'plot',
                            'mask_wm.nii.gz')
-    ret = script_runner.run('scil_visualize_scatterplot.py', in_x, in_y,
+    ret = script_runner.run('scil_viz_volume_scatterplot.py', in_x, in_y,
                             'scatter_plot_m.png', '--in_bin_mask', in_mask)
     assert ret.success
 
 
-def test_execution_processing_prob_map(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_x = os.path.join(get_home(), 'plot',
+def test_execution_processing_prob_map(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_x = os.path.join(SCILPY_HOME, 'plot',
                         'fa.nii.gz')
-    in_y = os.path.join(get_home(), 'plot',
+    in_y = os.path.join(SCILPY_HOME, 'plot',
                         'ad.nii.gz')
-    in_prob_1 = os.path.join(get_home(), 'plot',
+    in_prob_1 = os.path.join(SCILPY_HOME, 'plot',
                              'map_wm.nii.gz')
-    in_prob_2 = os.path.join(get_home(), 'plot',
+    in_prob_2 = os.path.join(SCILPY_HOME, 'plot',
                              'map_gm.nii.gz')
-    ret = script_runner.run('scil_visualize_scatterplot.py', in_x, in_y,
+    ret = script_runner.run('scil_viz_volume_scatterplot.py', in_x, in_y,
                             'scatter_plot_prob.png',
                             '--in_prob_maps', in_prob_1, in_prob_2)
     assert ret.success
 
 
-def test_execution_processing_atlas(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_x = os.path.join(get_home(), 'plot',
+def test_execution_processing_atlas(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_x = os.path.join(SCILPY_HOME, 'plot',
                         'fa.nii.gz')
-    in_y = os.path.join(get_home(), 'plot',
+    in_y = os.path.join(SCILPY_HOME, 'plot',
                         'ad.nii.gz')
-    in_atlas = os.path.join(get_home(), 'plot',
+    in_atlas = os.path.join(SCILPY_HOME, 'plot',
                             'atlas_brainnetome.nii.gz')
-    atlas_lut = os.path.join(get_home(), 'plot',
+    atlas_lut = os.path.join(SCILPY_HOME, 'plot',
                              'atlas_brainnetome.json')
-    ret = script_runner.run('scil_visualize_scatterplot.py', in_x, in_y,
+    ret = script_runner.run('scil_viz_volume_scatterplot.py', in_x, in_y,
                             'scatter_plot', '--in_atlas', in_atlas,
                             '--atlas_lut', atlas_lut)
     assert ret.success
 
 
-def test_execution_processing_atlas_folder(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_x = os.path.join(get_home(), 'plot',
+def test_execution_processing_atlas_folder(script_runner, monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_x = os.path.join(SCILPY_HOME, 'plot',
                         'fa.nii.gz')
-    in_y = os.path.join(get_home(), 'plot',
+    in_y = os.path.join(SCILPY_HOME, 'plot',
                         'ad.nii.gz')
-    in_atlas = os.path.join(get_home(), 'plot',
+    in_atlas = os.path.join(SCILPY_HOME, 'plot',
                             'atlas_brainnetome.nii.gz')
-    atlas_lut = os.path.join(get_home(), 'plot',
+    atlas_lut = os.path.join(SCILPY_HOME, 'plot',
                              'atlas_brainnetome.json')
-    ret = script_runner.run('scil_visualize_scatterplot.py', in_x, in_y,
+    ret = script_runner.run('scil_viz_volume_scatterplot.py', in_x, in_y,
                             'scatter_plot', '--in_atlas', in_atlas,
                             '--atlas_lut', atlas_lut,
                             '--in_folder')
     assert ret.success
 
 
-def test_execution_processing_atlas_folder_specific_label(script_runner):
-    os.chdir(os.path.expanduser(tmp_dir.name))
-    in_x = os.path.join(get_home(), 'plot',
+def test_execution_processing_atlas_folder_specific_label(script_runner,
+                                                          monkeypatch):
+    monkeypatch.chdir(os.path.expanduser(tmp_dir.name))
+    in_x = os.path.join(SCILPY_HOME, 'plot',
                         'fa.nii.gz')
-    in_y = os.path.join(get_home(), 'plot',
+    in_y = os.path.join(SCILPY_HOME, 'plot',
                         'ad.nii.gz')
-    in_atlas = os.path.join(get_home(), 'plot',
+    in_atlas = os.path.join(SCILPY_HOME, 'plot',
                             'atlas_brainnetome.nii.gz')
-    atlas_lut = os.path.join(get_home(), 'plot',
+    atlas_lut = os.path.join(SCILPY_HOME, 'plot',
                              'atlas_brainnetome.json')
-    ret = script_runner.run('scil_visualize_scatterplot.py', in_x, in_y,
+    ret = script_runner.run('scil_viz_volume_scatterplot.py', in_x, in_y,
                             'scatter_plot', '--in_atlas', in_atlas,
                             '--atlas_lut', atlas_lut,
                             '--specific_label', '2', '5', '7',
                             '--in_folder')
     assert ret.success
```

### Comparing `scilpy-1.5.post2/setup.py` & `scilpy-2.0.0/setup.py`

 * *Files 8% similar despite different names*

```diff
@@ -12,16 +12,16 @@
             repo_url = dependency[3:]
             external_dependencies.append('{} @ {}'.format(repo_name, repo_url))
         else:
             external_dependencies.append(dependency)
 
 
 def get_extensions():
-    uncompress = Extension('scilpy.tractanalysis.uncompress',
-                           ['scilpy/tractanalysis/uncompress.pyx'])
+    uncompress = Extension('scilpy.tractograms.uncompress',
+                           ['scilpy/tractograms/uncompress.pyx'])
     quick_tools = Extension('scilpy.tractanalysis.quick_tools',
                             ['scilpy/tractanalysis/quick_tools.pyx'])
     grid_intersections = Extension('scilpy.tractanalysis.grid_intersections',
                                    ['scilpy/tractanalysis/grid_intersections.pyx'])
     streamlines_metrics = Extension('scilpy.tractanalysis.streamlines_metrics',
                                     ['scilpy/tractanalysis/streamlines_metrics.pyx'])
     return [uncompress, quick_tools, grid_intersections, streamlines_metrics]
@@ -42,41 +42,57 @@
             self.distribution.ext_modules)
 
         # Call original build_ext command
         build_ext.finalize_options(self)
         build_ext.run(self)
 
 
+# Get the requiered python version
+PYTHON_VERSION = ""
+with open('.python-version') as f:
+    f.readline()
+    PYTHON_VERSION = f.readline().strip("\n")
+
 # Get version and release info, which is all stored in scilpy/version.py
 ver_file = os.path.join('scilpy', 'version.py')
 with open(ver_file) as f:
     exec(f.read())
+
+entry_point_legacy = []
+if os.getenv('SCILPY_LEGACY') != 'False':
+    entry_point_legacy = ["{}=scripts.legacy.{}:main".format(
+                          os.path.basename(s),
+                          os.path.basename(s).split(".")[0]) for s in LEGACY_SCRIPTS]
+
 opts = dict(name=NAME,
             maintainer=MAINTAINER,
             maintainer_email=MAINTAINER_EMAIL,
             description=DESCRIPTION,
             long_description=LONG_DESCRIPTION,
             url=URL,
             download_url=DOWNLOAD_URL,
             license=LICENSE,
             classifiers=CLASSIFIERS,
             author=AUTHOR,
             author_email=AUTHOR_EMAIL,
             platforms=PLATFORMS,
             version=VERSION,
             packages=find_packages(),
-            cmdclass={'build_ext': CustomBuildExtCommand},
+            cmdclass={
+                'build_ext': CustomBuildExtCommand
+            },
             ext_modules=get_extensions(),
             python_requires=PYTHON_VERSION,
             setup_requires=['cython', 'numpy'],
             install_requires=external_dependencies,
             entry_points={
                 'console_scripts': ["{}=scripts.{}:main".format(
                     os.path.basename(s),
-                    os.path.basename(s).split(".")[0]) for s in SCRIPTS]
+                    os.path.basename(s).split(".")[0]) for s in SCRIPTS] +
+                entry_point_legacy
             },
             data_files=[('data/LUT',
                          ["data/LUT/freesurfer_desikan_killiany.json",
                           "data/LUT/freesurfer_subcortical.json",
                           "data/LUT/dk_aggregate_structures.json"])],
             include_package_data=True)
```

