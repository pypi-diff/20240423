# Comparing `tmp/spine_engine-0.23.3.tar.gz` & `tmp/spine_engine-0.23.4.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "spine_engine-0.23.3.tar", last modified: Tue Jan  9 10:23:14 2024, max compression
+gzip compressed data, was "spine_engine-0.23.4.tar", last modified: Tue Apr 23 14:08:09 2024, max compression
```

## Comparing `spine_engine-0.23.3.tar` & `spine_engine-0.23.4.tar`

### file list

```diff
@@ -1,254 +1,226 @@
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:14.229558 spine_engine-0.23.3/
--rw-rw-rw-   0        0        0      156 2023-08-30 10:51:24.000000 spine_engine-0.23.3/.coveragerc
--rw-rw-rw-   0        0        0      174 2023-08-30 10:51:24.000000 spine_engine-0.23.3/.gitattributes
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:12.689378 spine_engine-0.23.3/.github/
--rw-rw-rw-   0        0        0      306 2023-08-30 10:51:24.000000 spine_engine-0.23.3/.github/pull_request_template.md
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:12.697417 spine_engine-0.23.3/.github/workflows/
--rw-rw-rw-   0        0        0     1778 2024-01-09 10:12:18.000000 spine_engine-0.23.3/.github/workflows/run_unit_tests.yml
--rw-rw-rw-   0        0        0      221 2023-08-30 10:51:24.000000 spine_engine-0.23.3/.gitignore
--rw-rw-rw-   0        0        0    35823 2023-08-30 10:51:24.000000 spine_engine-0.23.3/COPYING
--rw-rw-rw-   0        0        0     7815 2023-08-30 10:51:24.000000 spine_engine-0.23.3/COPYING.LESSER
--rw-rw-rw-   0        0        0     3196 2024-01-09 10:23:14.226883 spine_engine-0.23.3/PKG-INFO
--rw-rw-rw-   0        0        0     2182 2023-08-30 10:51:24.000000 spine_engine-0.23.3/README.md
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:12.722968 spine_engine-0.23.3/bin/
--rwxrwxrwx   0        0        0       74 2023-08-30 10:51:24.000000 spine_engine-0.23.3/bin/build_doc.bat
--rw-rw-rw-   0        0        0      353 2023-08-30 10:51:24.000000 spine_engine-0.23.3/bin/build_doc.py
--rw-rw-rw-   0        0        0     1302 2023-08-30 10:51:24.000000 spine_engine-0.23.3/bin/update_copyrights.py
--rw-rw-rw-   0        0        0       83 2023-08-30 10:51:24.000000 spine_engine-0.23.3/dev-requirements.txt
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:12.756103 spine_engine-0.23.3/docs/
--rw-rw-rw-   0        0        0      632 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/Makefile
--rwxrwxrwx   0        0        0      820 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/make.bat
--rw-rw-rw-   0        0        0      245 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/requirements.txt
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:12.775422 spine_engine-0.23.3/docs/source/
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:12.786484 spine_engine-0.23.3/docs/source/autoapi/
--rw-rw-rw-   0        0        0      259 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/index.rst
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:12.794403 spine_engine-0.23.3/docs/source/autoapi/spine_engine/
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:12.805053 spine_engine-0.23.3/docs/source/autoapi/spine_engine/config/
--rw-rw-rw-   0        0        0     1475 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/config/index.rst
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:12.812595 spine_engine-0.23.3/docs/source/autoapi/spine_engine/execution_managers/
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:12.821447 spine_engine-0.23.3/docs/source/autoapi/spine_engine/execution_managers/conda_kernel_spec_manager/
--rw-rw-rw-   0        0        0     4202 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/execution_managers/conda_kernel_spec_manager/index.rst
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:12.829173 spine_engine-0.23.3/docs/source/autoapi/spine_engine/execution_managers/conda_kernel_spec_runner/
--rw-rw-rw-   0        0        0      447 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/execution_managers/conda_kernel_spec_runner/index.rst
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:12.839390 spine_engine-0.23.3/docs/source/autoapi/spine_engine/execution_managers/execution_manager_base/
--rw-rw-rw-   0        0        0      973 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/execution_managers/execution_manager_base/index.rst
--rw-rw-rw-   0        0        0      481 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/execution_managers/index.rst
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:12.849198 spine_engine-0.23.3/docs/source/autoapi/spine_engine/execution_managers/kernel_execution_manager/
--rw-rw-rw-   0        0        0     4906 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/execution_managers/kernel_execution_manager/index.rst
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:12.858416 spine_engine-0.23.3/docs/source/autoapi/spine_engine/execution_managers/persistent_execution_manager/
--rw-rw-rw-   0        0        0    13917 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/execution_managers/persistent_execution_manager/index.rst
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:12.868582 spine_engine-0.23.3/docs/source/autoapi/spine_engine/execution_managers/process_execution_manager/
--rw-rw-rw-   0        0        0     1372 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/execution_managers/process_execution_manager/index.rst
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:12.878562 spine_engine-0.23.3/docs/source/autoapi/spine_engine/execution_managers/spine_repl/
--rw-rw-rw-   0        0        0     2477 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/execution_managers/spine_repl/index.rst
--rw-rw-rw-   0        0        0    10487 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/index.rst
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:12.885751 spine_engine-0.23.3/docs/source/autoapi/spine_engine/load_project_items/
--rw-rw-rw-   0        0        0     1218 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/load_project_items/index.rst
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:12.892613 spine_engine-0.23.3/docs/source/autoapi/spine_engine/multithread_executor/
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:12.903882 spine_engine-0.23.3/docs/source/autoapi/spine_engine/multithread_executor/executor/
--rw-rw-rw-   0        0        0     1413 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/multithread_executor/executor/index.rst
--rw-rw-rw-   0        0        0      303 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/multithread_executor/index.rst
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:12.917275 spine_engine-0.23.3/docs/source/autoapi/spine_engine/multithread_executor/multithread/
--rw-rw-rw-   0        0        0     2040 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/multithread_executor/multithread/index.rst
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:12.926480 spine_engine-0.23.3/docs/source/autoapi/spine_engine/multithread_executor/thread_executor/
--rw-rw-rw-   0        0        0     3800 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/multithread_executor/thread_executor/index.rst
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:12.932824 spine_engine-0.23.3/docs/source/autoapi/spine_engine/project_item/
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:12.941246 spine_engine-0.23.3/docs/source/autoapi/spine_engine/project_item/connection/
--rw-rw-rw-   0        0        0     4137 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/project_item/connection/index.rst
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:12.956101 spine_engine-0.23.3/docs/source/autoapi/spine_engine/project_item/executable_item_base/
--rw-rw-rw-   0        0        0     5456 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/project_item/executable_item_base/index.rst
--rw-rw-rw-   0        0        0      567 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/project_item/index.rst
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:12.967488 spine_engine-0.23.3/docs/source/autoapi/spine_engine/project_item/project_item_info/
--rw-rw-rw-   0        0        0      875 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/project_item/project_item_info/index.rst
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:12.976397 spine_engine-0.23.3/docs/source/autoapi/spine_engine/project_item/project_item_resource/
--rw-rw-rw-   0        0        0     4821 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/project_item/project_item_resource/index.rst
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:12.988886 spine_engine-0.23.3/docs/source/autoapi/spine_engine/project_item/project_item_specification/
--rw-rw-rw-   0        0        0     2309 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/project_item/project_item_specification/index.rst
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:13.001249 spine_engine-0.23.3/docs/source/autoapi/spine_engine/project_item/project_item_specification_factory/
--rw-rw-rw-   0        0        0     1296 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/project_item/project_item_specification_factory/index.rst
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:13.010307 spine_engine-0.23.3/docs/source/autoapi/spine_engine/project_item_loader/
--rw-rw-rw-   0        0        0     1593 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/project_item_loader/index.rst
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:13.025648 spine_engine-0.23.3/docs/source/autoapi/spine_engine/spine_engine/
--rw-rw-rw-   0        0        0    10608 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/spine_engine/index.rst
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:13.035968 spine_engine-0.23.3/docs/source/autoapi/spine_engine/spine_engine_server/
--rw-rw-rw-   0        0        0     2968 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/spine_engine_server/index.rst
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:13.046849 spine_engine-0.23.3/docs/source/autoapi/spine_engine/utils/
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:13.055659 spine_engine-0.23.3/docs/source/autoapi/spine_engine/utils/command_line_arguments/
--rw-rw-rw-   0        0        0      939 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/utils/command_line_arguments/index.rst
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:13.071613 spine_engine-0.23.3/docs/source/autoapi/spine_engine/utils/helpers/
--rw-rw-rw-   0        0        0     4147 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/utils/helpers/index.rst
--rw-rw-rw-   0        0        0      325 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/utils/index.rst
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:13.082188 spine_engine-0.23.3/docs/source/autoapi/spine_engine/utils/queue_logger/
--rw-rw-rw-   0        0        0     1224 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/utils/queue_logger/index.rst
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:13.089511 spine_engine-0.23.3/docs/source/autoapi/spine_engine/utils/returning_process/
--rw-rw-rw-   0        0        0     1088 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/utils/returning_process/index.rst
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:13.102349 spine_engine-0.23.3/docs/source/autoapi/spine_engine/utils/serialization/
--rw-rw-rw-   0        0        0     1919 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/utils/serialization/index.rst
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:13.108609 spine_engine-0.23.3/docs/source/autoapi/spine_engine/version/
--rw-rw-rw-   0        0        0      204 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/autoapi/spine_engine/version/index.rst
--rw-rw-rw-   0        0        0     5889 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/conf.py
--rw-rw-rw-   0        0        0      260 2023-08-30 10:51:24.000000 spine_engine-0.23.3/docs/source/index.rst
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:13.130565 spine_engine-0.23.3/fig/
--rw-rw-rw-   0        0        0    55634 2023-08-30 10:51:24.000000 spine_engine-0.23.3/fig/eu-emblem-low-res.jpg
--rw-rw-rw-   0        0        0     5077 2023-08-30 10:51:24.000000 spine_engine-0.23.3/fig/spineengine_logo.svg
--rw-rw-rw-   0        0        0     4868 2023-08-30 10:51:24.000000 spine_engine-0.23.3/fig/spineengine_on_wht.svg
--rw-rw-rw-   0        0        0    16433 2023-08-30 10:51:24.000000 spine_engine-0.23.3/pylintrc
--rw-rw-rw-   0        0        0     2032 2024-01-09 10:12:23.000000 spine_engine-0.23.3/pyproject.toml
--rw-rw-rw-   0        0        0       42 2024-01-09 10:23:14.229558 spine_engine-0.23.3/setup.cfg
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:13.213300 spine_engine-0.23.3/spine_engine/
--rw-rw-rw-   0        0        0     1143 2023-08-30 10:51:24.000000 spine_engine-0.23.3/spine_engine/__init__.py
--rw-rw-rw-   0        0        0     1826 2023-08-30 10:51:24.000000 spine_engine-0.23.3/spine_engine/config.py
--rw-rw-rw-   0        0        0     1244 2023-10-17 10:26:41.000000 spine_engine-0.23.3/spine_engine/exception.py
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:13.345902 spine_engine-0.23.3/spine_engine/execution_managers/
--rw-rw-rw-   0        0        0      977 2023-08-30 10:51:24.000000 spine_engine-0.23.3/spine_engine/execution_managers/__init__.py
--rw-rw-rw-   0        0        0    17417 2023-08-30 10:51:24.000000 spine_engine-0.23.3/spine_engine/execution_managers/conda_kernel_spec_manager.py
--rw-rw-rw-   0        0        0     1882 2023-08-30 10:51:24.000000 spine_engine-0.23.3/spine_engine/execution_managers/conda_kernel_spec_runner.py
--rw-rw-rw-   0        0        0     1628 2023-10-17 10:26:41.000000 spine_engine-0.23.3/spine_engine/execution_managers/execution_manager_base.py
--rw-rw-rw-   0        0        0    15466 2024-01-09 10:12:19.000000 spine_engine-0.23.3/spine_engine/execution_managers/kernel_execution_manager.py
--rw-rw-rw-   0        0        0    30912 2024-01-09 10:12:19.000000 spine_engine-0.23.3/spine_engine/execution_managers/persistent_execution_manager.py
--rw-rw-rw-   0        0        0     3939 2024-01-09 10:12:19.000000 spine_engine-0.23.3/spine_engine/execution_managers/process_execution_manager.py
--rw-rw-rw-   0        0        0     3371 2023-08-30 10:51:24.000000 spine_engine-0.23.3/spine_engine/execution_managers/spine_repl.jl
--rw-rw-rw-   0        0        0     3850 2023-08-30 10:51:24.000000 spine_engine-0.23.3/spine_engine/execution_managers/spine_repl.py
--rw-rw-rw-   0        0        0     3345 2023-08-30 10:51:24.000000 spine_engine-0.23.3/spine_engine/load_project_items.py
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:13.381581 spine_engine-0.23.3/spine_engine/multithread_executor/
--rw-rw-rw-   0        0        0      977 2023-08-30 10:51:24.000000 spine_engine-0.23.3/spine_engine/multithread_executor/__init__.py
--rw-rw-rw-   0        0        0     2682 2023-08-30 10:51:24.000000 spine_engine-0.23.3/spine_engine/multithread_executor/executor.py
--rw-rw-rw-   0        0        0    16640 2023-08-30 10:51:24.000000 spine_engine-0.23.3/spine_engine/multithread_executor/multithread.py
--rw-rw-rw-   0        0        0     4196 2023-08-30 10:51:24.000000 spine_engine-0.23.3/spine_engine/multithread_executor/thread_executor.py
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:13.454123 spine_engine-0.23.3/spine_engine/project_item/
--rw-rw-rw-   0        0        0     1049 2023-08-30 10:51:24.000000 spine_engine-0.23.3/spine_engine/project_item/__init__.py
--rw-rw-rw-   0        0        0    26985 2024-01-09 10:12:19.000000 spine_engine-0.23.3/spine_engine/project_item/connection.py
--rw-rw-rw-   0        0        0     9258 2023-08-30 10:51:24.000000 spine_engine-0.23.3/spine_engine/project_item/executable_item_base.py
--rw-rw-rw-   0        0        0     1487 2023-10-17 10:26:41.000000 spine_engine-0.23.3/spine_engine/project_item/project_item_info.py
--rw-rw-rw-   0        0        0    15702 2024-01-09 10:12:19.000000 spine_engine-0.23.3/spine_engine/project_item/project_item_resource.py
--rw-rw-rw-   0        0        0     4401 2023-08-30 10:51:24.000000 spine_engine-0.23.3/spine_engine/project_item/project_item_specification.py
--rw-rw-rw-   0        0        0     1757 2023-08-30 10:51:24.000000 spine_engine-0.23.3/spine_engine/project_item/project_item_specification_factory.py
--rw-rw-rw-   0        0        0     2886 2023-08-30 10:51:24.000000 spine_engine-0.23.3/spine_engine/project_item_loader.py
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:13.587751 spine_engine-0.23.3/spine_engine/server/
--rw-rw-rw-   0        0        0      221 2023-08-30 10:51:24.000000 spine_engine-0.23.3/spine_engine/server/README.txt
--rw-rw-rw-   0        0        0     1057 2023-08-30 10:51:25.000000 spine_engine-0.23.3/spine_engine/server/__init__.py
--rw-rw-rw-   0        0        0     3497 2023-08-30 10:51:25.000000 spine_engine-0.23.3/spine_engine/server/certificate_creator.py
--rw-rw-rw-   0        0        0    20054 2023-08-30 10:51:25.000000 spine_engine-0.23.3/spine_engine/server/engine_server.py
--rw-rw-rw-   0        0        0     4494 2023-08-30 10:51:25.000000 spine_engine-0.23.3/spine_engine/server/persistent_execution_service.py
--rw-rw-rw-   0        0        0     2133 2023-08-30 10:51:25.000000 spine_engine-0.23.3/spine_engine/server/ping_service.py
--rw-rw-rw-   0        0        0     5744 2023-08-30 10:51:25.000000 spine_engine-0.23.3/spine_engine/server/project_extractor_service.py
--rw-rw-rw-   0        0        0     2630 2023-08-30 10:51:25.000000 spine_engine-0.23.3/spine_engine/server/project_remover_service.py
--rw-rw-rw-   0        0        0     3813 2023-08-30 10:51:25.000000 spine_engine-0.23.3/spine_engine/server/project_retriever_service.py
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:12.605878 spine_engine-0.23.3/spine_engine/server/received_projects/
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:13.609200 spine_engine-0.23.3/spine_engine/server/received_projects/hello_world_on_server__49e90ea3f66842bf9eb455448c363cdc/
--rw-rw-rw-   0        0        0        0 2023-12-22 08:32:56.000000 spine_engine-0.23.3/spine_engine/server/received_projects/hello_world_on_server__49e90ea3f66842bf9eb455448c363cdc/__init__.py
--rw-rw-rw-   0        0        0     3543 2023-12-22 08:32:56.000000 spine_engine-0.23.3/spine_engine/server/received_projects/hello_world_on_server__49e90ea3f66842bf9eb455448c363cdc/execution_test.py
--rw-rw-rw-   0        0        0      189 2023-12-22 08:32:56.000000 spine_engine-0.23.3/spine_engine/server/received_projects/hello_world_on_server__49e90ea3f66842bf9eb455448c363cdc/simple_script.py
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:13.616948 spine_engine-0.23.3/spine_engine/server/received_projects/hello_world_on_server__75bf5d45bfea433d9f7e37ee93724313/
--rw-rw-rw-   0        0        0        0 2023-12-22 08:28:01.000000 spine_engine-0.23.3/spine_engine/server/received_projects/hello_world_on_server__75bf5d45bfea433d9f7e37ee93724313/__init__.py
--rw-rw-rw-   0        0        0     3543 2023-12-22 08:28:01.000000 spine_engine-0.23.3/spine_engine/server/received_projects/hello_world_on_server__75bf5d45bfea433d9f7e37ee93724313/execution_test.py
--rw-rw-rw-   0        0        0      189 2023-12-22 08:28:01.000000 spine_engine-0.23.3/spine_engine/server/received_projects/hello_world_on_server__75bf5d45bfea433d9f7e37ee93724313/simple_script.py
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:13.622438 spine_engine-0.23.3/spine_engine/server/received_projects/hello_world_on_server__772c2f8565a04e439f7e6ccfc383221c/
--rw-rw-rw-   0        0        0        0 2023-12-22 08:37:46.000000 spine_engine-0.23.3/spine_engine/server/received_projects/hello_world_on_server__772c2f8565a04e439f7e6ccfc383221c/__init__.py
--rw-rw-rw-   0        0        0     3543 2023-12-22 08:37:46.000000 spine_engine-0.23.3/spine_engine/server/received_projects/hello_world_on_server__772c2f8565a04e439f7e6ccfc383221c/execution_test.py
--rw-rw-rw-   0        0        0      189 2023-12-22 08:37:46.000000 spine_engine-0.23.3/spine_engine/server/received_projects/hello_world_on_server__772c2f8565a04e439f7e6ccfc383221c/simple_script.py
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:13.629723 spine_engine-0.23.3/spine_engine/server/received_projects/hello_world_on_server__8421db3482fc4ce5aa4fb5e1e1212874/
--rw-rw-rw-   0        0        0        0 2023-12-22 08:32:07.000000 spine_engine-0.23.3/spine_engine/server/received_projects/hello_world_on_server__8421db3482fc4ce5aa4fb5e1e1212874/__init__.py
--rw-rw-rw-   0        0        0     3543 2023-12-22 08:32:07.000000 spine_engine-0.23.3/spine_engine/server/received_projects/hello_world_on_server__8421db3482fc4ce5aa4fb5e1e1212874/execution_test.py
--rw-rw-rw-   0        0        0      189 2023-12-22 08:32:07.000000 spine_engine-0.23.3/spine_engine/server/received_projects/hello_world_on_server__8421db3482fc4ce5aa4fb5e1e1212874/simple_script.py
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:13.637270 spine_engine-0.23.3/spine_engine/server/received_projects/hello_world_on_server__aabd50f3fb9541c7806fafb9b56578cf/
--rw-rw-rw-   0        0        0        0 2023-12-22 08:34:51.000000 spine_engine-0.23.3/spine_engine/server/received_projects/hello_world_on_server__aabd50f3fb9541c7806fafb9b56578cf/__init__.py
--rw-rw-rw-   0        0        0     3543 2023-12-22 08:34:51.000000 spine_engine-0.23.3/spine_engine/server/received_projects/hello_world_on_server__aabd50f3fb9541c7806fafb9b56578cf/execution_test.py
--rw-rw-rw-   0        0        0      189 2023-12-22 08:34:51.000000 spine_engine-0.23.3/spine_engine/server/received_projects/hello_world_on_server__aabd50f3fb9541c7806fafb9b56578cf/simple_script.py
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:13.642481 spine_engine-0.23.3/spine_engine/server/received_projects/hello_world_on_server__d8b047f1708b40a2a28d580737de834c/
--rw-rw-rw-   0        0        0        0 2023-12-22 08:42:29.000000 spine_engine-0.23.3/spine_engine/server/received_projects/hello_world_on_server__d8b047f1708b40a2a28d580737de834c/__init__.py
--rw-rw-rw-   0        0        0     3543 2023-12-22 08:42:29.000000 spine_engine-0.23.3/spine_engine/server/received_projects/hello_world_on_server__d8b047f1708b40a2a28d580737de834c/execution_test.py
--rw-rw-rw-   0        0        0      189 2023-12-22 08:42:29.000000 spine_engine-0.23.3/spine_engine/server/received_projects/hello_world_on_server__d8b047f1708b40a2a28d580737de834c/simple_script.py
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:13.655016 spine_engine-0.23.3/spine_engine/server/received_projects/simple_importer_on_server__b1a6cf81eded41138838387d6a0dca62/
--rw-rw-rw-   0        0        0        0 2023-12-22 08:47:35.000000 spine_engine-0.23.3/spine_engine/server/received_projects/simple_importer_on_server__b1a6cf81eded41138838387d6a0dca62/__init__.py
--rw-rw-rw-   0        0        0     3054 2023-12-22 08:47:35.000000 spine_engine-0.23.3/spine_engine/server/received_projects/simple_importer_on_server__b1a6cf81eded41138838387d6a0dca62/execution_test.py
--rw-rw-rw-   0        0        0    12509 2023-08-30 10:51:25.000000 spine_engine-0.23.3/spine_engine/server/remote_execution_service.py
--rw-rw-rw-   0        0        0     5757 2023-08-30 10:51:25.000000 spine_engine-0.23.3/spine_engine/server/request.py
--rw-rw-rw-   0        0        0     1683 2023-08-30 10:51:25.000000 spine_engine-0.23.3/spine_engine/server/service_base.py
--rw-rw-rw-   0        0        0     2457 2023-08-30 10:51:25.000000 spine_engine-0.23.3/spine_engine/server/start_server.py
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:13.701843 spine_engine-0.23.3/spine_engine/server/util/
--rw-rw-rw-   0        0        0     1073 2023-08-30 10:51:25.000000 spine_engine-0.23.3/spine_engine/server/util/__init__.py
--rw-rw-rw-   0        0        0     6339 2023-08-30 10:51:25.000000 spine_engine-0.23.3/spine_engine/server/util/event_data_converter.py
--rw-rw-rw-   0        0        0     5363 2023-08-30 10:51:25.000000 spine_engine-0.23.3/spine_engine/server/util/server_message.py
--rw-rw-rw-   0        0        0     3601 2023-08-30 10:51:25.000000 spine_engine-0.23.3/spine_engine/server/util/zip_handler.py
--rw-rw-rw-   0        0        0     1673 2023-08-30 10:51:25.000000 spine_engine-0.23.3/spine_engine/shared_memory_io_manager.py
--rw-rw-rw-   0        0        0    42706 2023-11-06 06:36:23.000000 spine_engine-0.23.3/spine_engine/spine_engine.py
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:13.774334 spine_engine-0.23.3/spine_engine/utils/
--rw-rw-rw-   0        0        0      977 2023-08-30 10:51:25.000000 spine_engine-0.23.3/spine_engine/utils/__init__.py
--rw-rw-rw-   0        0        0     2064 2023-08-30 10:51:25.000000 spine_engine-0.23.3/spine_engine/utils/command_line_arguments.py
--rw-rw-rw-   0        0        0     3732 2023-08-30 10:51:25.000000 spine_engine-0.23.3/spine_engine/utils/execution_resources.py
--rw-rw-rw-   0        0        0    17431 2024-01-09 10:12:19.000000 spine_engine-0.23.3/spine_engine/utils/helpers.py
--rw-rw-rw-   0        0        0     5101 2023-08-30 10:51:25.000000 spine_engine-0.23.3/spine_engine/utils/queue_logger.py
--rw-rw-rw-   0        0        0     3876 2023-08-30 10:51:25.000000 spine_engine-0.23.3/spine_engine/utils/returning_process.py
--rw-rw-rw-   0        0        0     5242 2023-10-17 10:26:41.000000 spine_engine-0.23.3/spine_engine/utils/serialization.py
--rw-rw-rw-   0        0        0      429 2024-01-09 10:23:12.000000 spine_engine-0.23.3/spine_engine/version.py
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:14.225345 spine_engine-0.23.3/spine_engine.egg-info/
--rw-rw-rw-   0        0        0     3196 2024-01-09 10:23:12.000000 spine_engine-0.23.3/spine_engine.egg-info/PKG-INFO
--rw-rw-rw-   0        0        0     9527 2024-01-09 10:23:12.000000 spine_engine-0.23.3/spine_engine.egg-info/SOURCES.txt
--rw-rw-rw-   0        0        0        1 2024-01-09 10:23:12.000000 spine_engine-0.23.3/spine_engine.egg-info/dependency_links.txt
--rw-rw-rw-   0        0        0        2 2023-08-30 13:04:48.000000 spine_engine-0.23.3/spine_engine.egg-info/not-zip-safe
--rw-rw-rw-   0        0        0      185 2024-01-09 10:23:12.000000 spine_engine-0.23.3/spine_engine.egg-info/requires.txt
--rw-rw-rw-   0        0        0       18 2024-01-09 10:23:12.000000 spine_engine-0.23.3/spine_engine.egg-info/top_level.txt
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:13.807358 spine_engine-0.23.3/tests/
--rw-rw-rw-   0        0        0     1042 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/__init__.py
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:13.850685 spine_engine-0.23.3/tests/execution_managers/
--rw-rw-rw-   0        0        0     1031 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/execution_managers/__init__.py
--rw-rw-rw-   0        0        0    14503 2024-01-09 10:12:19.000000 spine_engine-0.23.3/tests/execution_managers/test_kernel_execution_manager.py
--rw-rw-rw-   0        0        0     3939 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/execution_managers/test_persistent_execution_manager.py
--rw-rw-rw-   0        0        0     3635 2024-01-09 10:12:19.000000 spine_engine-0.23.3/tests/execution_managers/test_process_execution_manager.py
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:12.607919 spine_engine-0.23.3/tests/mock_project_items/
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:13.852720 spine_engine-0.23.3/tests/mock_project_items/items_module/
--rw-rw-rw-   0        0        0        0 2023-10-17 10:26:41.000000 spine_engine-0.23.3/tests/mock_project_items/items_module/__init__.py
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:13.875315 spine_engine-0.23.3/tests/mock_project_items/items_module/test_item/
--rw-rw-rw-   0        0        0        0 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/mock_project_items/items_module/test_item/__init__.py
--rw-rw-rw-   0        0        0      194 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/mock_project_items/items_module/test_item/executable_item.py
--rw-rw-rw-   0        0        0      240 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/mock_project_items/items_module/test_item/specification_factory.py
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:13.919163 spine_engine-0.23.3/tests/project_item/
--rw-rw-rw-   0        0        0     1055 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/project_item/__init__.py
--rw-rw-rw-   0        0        0     2596 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/project_item/test_ExecutableItem.py
--rw-rw-rw-   0        0        0    15726 2024-01-09 10:12:19.000000 spine_engine-0.23.3/tests/project_item/test_connection.py
--rw-rw-rw-   0        0        0     8684 2023-11-30 07:00:14.000000 spine_engine-0.23.3/tests/project_item/test_project_item_resource.py
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:14.018334 spine_engine-0.23.3/tests/server/
--rw-rw-rw-   0        0        0     1039 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/server/__init__.py
--rw-rw-rw-   0        0        0     3504 2024-01-09 10:12:19.000000 spine_engine-0.23.3/tests/server/helloworld.zip
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:14.025811 spine_engine-0.23.3/tests/server/secfolder_for_tests/
--rw-rw-rw-   0        0        0       11 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/server/secfolder_for_tests/allowEndpoints.txt
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:14.036645 spine_engine-0.23.3/tests/server/secfolder_for_tests/certificates/
--rw-rw-rw-   0        0        0       75 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/server/secfolder_for_tests/certificates/.gitignore
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:14.055568 spine_engine-0.23.3/tests/server/secfolder_for_tests/private_keys/
--rw-rw-rw-   0        0        0      321 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/server/secfolder_for_tests/private_keys/client.key_secret
--rw-rw-rw-   0        0        0      321 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/server/secfolder_for_tests/private_keys/server.key_secret
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:14.073776 spine_engine-0.23.3/tests/server/secfolder_for_tests/public_keys/
--rw-rw-rw-   0        0        0      373 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/server/secfolder_for_tests/public_keys/client.key
--rw-rw-rw-   0        0        0      373 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/server/secfolder_for_tests/public_keys/server.key
--rw-rw-rw-   0        0        0    15637 2024-01-09 10:12:19.000000 spine_engine-0.23.3/tests/server/simple_importer.zip
--rw-rw-rw-   0        0        0     9097 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/server/test_EngineServer.py
--rw-rw-rw-   0        0        0     2676 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/server/test_PingService.py
--rw-rw-rw-   0        0        0     4636 2024-01-09 10:12:19.000000 spine_engine-0.23.3/tests/server/test_ProjectExtractorService.py
--rw-rw-rw-   0        0        0     3327 2024-01-09 10:12:19.000000 spine_engine-0.23.3/tests/server/test_ProjectRemoverService.py
--rw-rw-rw-   0        0        0    17286 2024-01-09 10:12:19.000000 spine_engine-0.23.3/tests/server/test_RemoteExecutionService.py
--rw-rw-rw-   0        0        0     2207 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/server/test_start_server.py
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:14.118466 spine_engine-0.23.3/tests/server/util/
--rw-rw-rw-   0        0        0      977 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/server/util/__init__.py
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:14.136763 spine_engine-0.23.3/tests/server/util/projectforpackagingtests/
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:14.147096 spine_engine-0.23.3/tests/server/util/projectforpackagingtests/.spinetoolbox/
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:14.163672 spine_engine-0.23.3/tests/server/util/projectforpackagingtests/.spinetoolbox/local/
--rw-rw-rw-   0        0        0      104 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/server/util/projectforpackagingtests/.spinetoolbox/local/project_local_data.json
--rw-rw-rw-   0        0        0      346 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/server/util/projectforpackagingtests/.spinetoolbox/local/specification_local_data.json
--rw-rw-rw-   0        0        0     1675 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/server/util/projectforpackagingtests/.spinetoolbox/project.json
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:12.616780 spine_engine-0.23.3/tests/server/util/projectforpackagingtests/.spinetoolbox/specifications/
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:14.184943 spine_engine-0.23.3/tests/server/util/projectforpackagingtests/.spinetoolbox/specifications/Tool/
--rw-rw-rw-   0        0        0      271 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/server/util/projectforpackagingtests/.spinetoolbox/specifications/Tool/helloworld.json
--rw-rw-rw-   0        0        0      300 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/server/util/projectforpackagingtests/.spinetoolbox/specifications/Tool/helloworld2.json
--rw-rw-rw-   0        0        0       70 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/server/util/projectforpackagingtests/helloworld.py
--rw-rw-rw-   0        0        0        8 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/server/util/projectforpackagingtests/input2.txt
--rw-rw-rw-   0        0        0     7162 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/server/util/test_EventDataConverter.py
--rw-rw-rw-   0        0        0    10556 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/server/util/test_ServerMessage.py
--rw-rw-rw-   0        0        0     2560 2024-01-09 10:12:19.000000 spine_engine-0.23.3/tests/server/util/test_ZipHandler.py
--rw-rw-rw-   0        0        0     2345 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/test_load_project_items.py
--rw-rw-rw-   0        0        0    53255 2024-01-09 10:12:19.000000 spine_engine-0.23.3/tests/test_spine_engine.py
-drwxrwxrwx   0        0        0        0 2024-01-09 10:23:14.223299 spine_engine-0.23.3/tests/utils/
--rw-rw-rw-   0        0        0     1042 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/utils/__init__.py
--rw-rw-rw-   0        0        0     2054 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/utils/test_command_line_args.py
--rw-rw-rw-   0        0        0     8261 2023-10-17 10:26:41.000000 spine_engine-0.23.3/tests/utils/test_helpers.py
--rw-rw-rw-   0        0        0     7540 2023-08-30 10:51:25.000000 spine_engine-0.23.3/tests/utils/test_serialization.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.487058 spine_engine-0.23.4/
+-rw-r--r--   0 runner    (1001) docker     (127)      148 2024-04-23 14:08:02.000000 spine_engine-0.23.4/.coveragerc
+-rw-r--r--   0 runner    (1001) docker     (127)      161 2024-04-23 14:08:02.000000 spine_engine-0.23.4/.gitattributes
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.459058 spine_engine-0.23.4/.github/
+-rw-r--r--   0 runner    (1001) docker     (127)      296 2024-04-23 14:08:02.000000 spine_engine-0.23.4/.github/pull_request_template.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.459058 spine_engine-0.23.4/.github/workflows/
+-rw-r--r--   0 runner    (1001) docker     (127)     1718 2024-04-23 14:08:02.000000 spine_engine-0.23.4/.github/workflows/run_unit_tests.yml
+-rw-r--r--   0 runner    (1001) docker     (127)      209 2024-04-23 14:08:02.000000 spine_engine-0.23.4/.gitignore
+-rw-r--r--   0 runner    (1001) docker     (127)    35149 2024-04-23 14:08:02.000000 spine_engine-0.23.4/COPYING
+-rw-r--r--   0 runner    (1001) docker     (127)     7651 2024-04-23 14:08:02.000000 spine_engine-0.23.4/COPYING.LESSER
+-rw-r--r--   0 runner    (1001) docker     (127)     3122 2024-04-23 14:08:09.487058 spine_engine-0.23.4/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)     2135 2024-04-23 14:08:02.000000 spine_engine-0.23.4/README.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.463058 spine_engine-0.23.4/bin/
+-rw-r--r--   0 runner    (1001) docker     (127)       70 2024-04-23 14:08:02.000000 spine_engine-0.23.4/bin/build_doc.bat
+-rw-r--r--   0 runner    (1001) docker     (127)      340 2024-04-23 14:08:02.000000 spine_engine-0.23.4/bin/build_doc.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1266 2024-04-23 14:08:02.000000 spine_engine-0.23.4/bin/update_copyrights.py
+-rw-r--r--   0 runner    (1001) docker     (127)       79 2024-04-23 14:08:02.000000 spine_engine-0.23.4/dev-requirements.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.463058 spine_engine-0.23.4/docs/
+-rw-r--r--   0 runner    (1001) docker     (127)      613 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/Makefile
+-rw-r--r--   0 runner    (1001) docker     (127)      784 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/make.bat
+-rw-r--r--   0 runner    (1001) docker     (127)      239 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/requirements.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.463058 spine_engine-0.23.4/docs/source/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.463058 spine_engine-0.23.4/docs/source/autoapi/
+-rw-r--r--   0 runner    (1001) docker     (127)      249 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.463058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.463058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/config/
+-rw-r--r--   0 runner    (1001) docker     (127)     1370 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/config/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.463058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/execution_managers/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.463058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/execution_managers/conda_kernel_spec_manager/
+-rw-r--r--   0 runner    (1001) docker     (127)     4038 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/execution_managers/conda_kernel_spec_manager/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.463058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/execution_managers/conda_kernel_spec_runner/
+-rw-r--r--   0 runner    (1001) docker     (127)      425 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/execution_managers/conda_kernel_spec_runner/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.463058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/execution_managers/execution_manager_base/
+-rw-r--r--   0 runner    (1001) docker     (127)      921 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/execution_managers/execution_manager_base/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      460 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/execution_managers/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.463058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/execution_managers/kernel_execution_manager/
+-rw-r--r--   0 runner    (1001) docker     (127)     4741 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/execution_managers/kernel_execution_manager/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.463058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/execution_managers/persistent_execution_manager/
+-rw-r--r--   0 runner    (1001) docker     (127)    13389 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/execution_managers/persistent_execution_manager/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.463058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/execution_managers/process_execution_manager/
+-rw-r--r--   0 runner    (1001) docker     (127)     1310 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/execution_managers/process_execution_manager/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.463058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/execution_managers/spine_repl/
+-rw-r--r--   0 runner    (1001) docker     (127)     2373 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/execution_managers/spine_repl/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)    10093 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.463058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/load_project_items/
+-rw-r--r--   0 runner    (1001) docker     (127)     1168 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/load_project_items/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.463058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/multithread_executor/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.463058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/multithread_executor/executor/
+-rw-r--r--   0 runner    (1001) docker     (127)     1361 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/multithread_executor/executor/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      286 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/multithread_executor/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.463058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/multithread_executor/multithread/
+-rw-r--r--   0 runner    (1001) docker     (127)     1958 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/multithread_executor/multithread/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.467058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/multithread_executor/thread_executor/
+-rw-r--r--   0 runner    (1001) docker     (127)     3667 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/multithread_executor/thread_executor/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.467058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/project_item/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.467058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/project_item/connection/
+-rw-r--r--   0 runner    (1001) docker     (127)     3972 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/project_item/connection/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.467058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/project_item/executable_item_base/
+-rw-r--r--   0 runner    (1001) docker     (127)     5270 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/project_item/executable_item_base/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      539 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/project_item/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.467058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/project_item/project_item_info/
+-rw-r--r--   0 runner    (1001) docker     (127)      825 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/project_item/project_item_info/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.467058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/project_item/project_item_resource/
+-rw-r--r--   0 runner    (1001) docker     (127)     4623 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/project_item/project_item_resource/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.467058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/project_item/project_item_specification/
+-rw-r--r--   0 runner    (1001) docker     (127)     2208 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/project_item/project_item_specification/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.467058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/project_item/project_item_specification_factory/
+-rw-r--r--   0 runner    (1001) docker     (127)     1240 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/project_item/project_item_specification_factory/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.467058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/project_item_loader/
+-rw-r--r--   0 runner    (1001) docker     (127)     1519 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/project_item_loader/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.467058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/spine_engine/
+-rw-r--r--   0 runner    (1001) docker     (127)    10213 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/spine_engine/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.467058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/spine_engine_server/
+-rw-r--r--   0 runner    (1001) docker     (127)     2825 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/spine_engine_server/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.467058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/utils/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.467058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/utils/command_line_arguments/
+-rw-r--r--   0 runner    (1001) docker     (127)      897 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/utils/command_line_arguments/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.467058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/utils/helpers/
+-rw-r--r--   0 runner    (1001) docker     (127)     3980 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/utils/helpers/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      306 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/utils/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.467058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/utils/queue_logger/
+-rw-r--r--   0 runner    (1001) docker     (127)     1155 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/utils/queue_logger/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.467058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/utils/returning_process/
+-rw-r--r--   0 runner    (1001) docker     (127)     1034 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/utils/returning_process/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.467058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/utils/serialization/
+-rw-r--r--   0 runner    (1001) docker     (127)     1837 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/utils/serialization/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.467058 spine_engine-0.23.4/docs/source/autoapi/spine_engine/version/
+-rw-r--r--   0 runner    (1001) docker     (127)      190 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/autoapi/spine_engine/version/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     5704 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/conf.py
+-rw-r--r--   0 runner    (1001) docker     (127)      245 2024-04-23 14:08:02.000000 spine_engine-0.23.4/docs/source/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.467058 spine_engine-0.23.4/fig/
+-rw-r--r--   0 runner    (1001) docker     (127)    55634 2024-04-23 14:08:02.000000 spine_engine-0.23.4/fig/eu-emblem-low-res.jpg
+-rw-r--r--   0 runner    (1001) docker     (127)     5077 2024-04-23 14:08:02.000000 spine_engine-0.23.4/fig/spineengine_logo.svg
+-rw-r--r--   0 runner    (1001) docker     (127)     4868 2024-04-23 14:08:02.000000 spine_engine-0.23.4/fig/spineengine_on_wht.svg
+-rw-r--r--   0 runner    (1001) docker     (127)    16433 2024-04-23 14:08:02.000000 spine_engine-0.23.4/pylintrc
+-rw-r--r--   0 runner    (1001) docker     (127)     1962 2024-04-23 14:08:02.000000 spine_engine-0.23.4/pyproject.toml
+-rw-r--r--   0 runner    (1001) docker     (127)       38 2024-04-23 14:08:09.487058 spine_engine-0.23.4/setup.cfg
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.471058 spine_engine-0.23.4/spine_engine/
+-rw-r--r--   0 runner    (1001) docker     (127)     1129 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1781 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1222 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/exception.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.471058 spine_engine-0.23.4/spine_engine/execution_managers/
+-rw-r--r--   0 runner    (1001) docker     (127)      967 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/execution_managers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17027 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/execution_managers/conda_kernel_spec_manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1822 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/execution_managers/conda_kernel_spec_runner.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1588 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/execution_managers/execution_manager_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15105 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/execution_managers/kernel_execution_manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)    30050 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/execution_managers/persistent_execution_manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3856 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/execution_managers/process_execution_manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3263 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/execution_managers/spine_repl.jl
+-rw-r--r--   0 runner    (1001) docker     (127)     3734 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/execution_managers/spine_repl.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3270 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/load_project_items.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.471058 spine_engine-0.23.4/spine_engine/multithread_executor/
+-rw-r--r--   0 runner    (1001) docker     (127)      967 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/multithread_executor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2627 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/multithread_executor/executor.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16322 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/multithread_executor/multithread.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4077 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/multithread_executor/thread_executor.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.475058 spine_engine-0.23.4/spine_engine/project_item/
+-rw-r--r--   0 runner    (1001) docker     (127)     1034 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/project_item/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26299 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/project_item/connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9027 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/project_item/executable_item_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1451 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/project_item/project_item_info.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15262 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/project_item/project_item_resource.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4276 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/project_item/project_item_specification.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1718 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/project_item/project_item_specification_factory.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2829 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/project_item_loader.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.475058 spine_engine-0.23.4/spine_engine/server/
+-rw-r--r--   0 runner    (1001) docker     (127)      216 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/server/README.txt
+-rw-r--r--   0 runner    (1001) docker     (127)     1042 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/server/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3420 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/server/certificate_creator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19693 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/server/engine_server.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4411 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/server/persistent_execution_service.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2089 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/server/ping_service.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5632 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/server/project_extractor_service.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2577 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/server/project_remover_service.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3738 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/server/project_retriever_service.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12290 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/server/remote_execution_service.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5634 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/server/request.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1646 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/server/service_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2395 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/server/start_server.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.475058 spine_engine-0.23.4/spine_engine/server/util/
+-rw-r--r--   0 runner    (1001) docker     (127)     1058 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/server/util/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6200 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/server/util/event_data_converter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5234 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/server/util/server_message.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3519 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/server/util/zip_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1637 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/shared_memory_io_manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)    41739 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/spine_engine.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.479058 spine_engine-0.23.4/spine_engine/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)      967 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2017 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/utils/command_line_arguments.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3643 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/utils/execution_resources.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16898 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/utils/helpers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4976 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/utils/queue_logger.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3772 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/utils/returning_process.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5098 2024-04-23 14:08:02.000000 spine_engine-0.23.4/spine_engine/utils/serialization.py
+-rw-r--r--   0 runner    (1001) docker     (127)      413 2024-04-23 14:08:09.000000 spine_engine-0.23.4/spine_engine/version.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.487058 spine_engine-0.23.4/spine_engine.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (127)     3122 2024-04-23 14:08:09.000000 spine_engine-0.23.4/spine_engine.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)     7327 2024-04-23 14:08:09.000000 spine_engine-0.23.4/spine_engine.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        1 2024-04-23 14:08:09.000000 spine_engine-0.23.4/spine_engine.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        1 2024-04-23 14:08:09.000000 spine_engine-0.23.4/spine_engine.egg-info/not-zip-safe
+-rw-r--r--   0 runner    (1001) docker     (127)      185 2024-04-23 14:08:09.000000 spine_engine-0.23.4/spine_engine.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (127)       18 2024-04-23 14:08:09.000000 spine_engine-0.23.4/spine_engine.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.479058 spine_engine-0.23.4/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)     1027 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.479058 spine_engine-0.23.4/tests/execution_managers/
+-rw-r--r--   0 runner    (1001) docker     (127)     1020 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/execution_managers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14251 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/execution_managers/test_kernel_execution_manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3863 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/execution_managers/test_persistent_execution_manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3565 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/execution_managers/test_process_execution_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.455058 spine_engine-0.23.4/tests/mock_project_items/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.479058 spine_engine-0.23.4/tests/mock_project_items/items_module/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/mock_project_items/items_module/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.479058 spine_engine-0.23.4/tests/mock_project_items/items_module/test_item/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/mock_project_items/items_module/test_item/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      187 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/mock_project_items/items_module/test_item/executable_item.py
+-rw-r--r--   0 runner    (1001) docker     (127)      233 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/mock_project_items/items_module/test_item/specification_factory.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.479058 spine_engine-0.23.4/tests/project_item/
+-rw-r--r--   0 runner    (1001) docker     (127)     1040 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/project_item/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2544 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/project_item/test_ExecutableItem.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15445 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/project_item/test_connection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8506 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/project_item/test_project_item_resource.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.483058 spine_engine-0.23.4/tests/server/
+-rw-r--r--   0 runner    (1001) docker     (127)     1025 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/server/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3504 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/server/helloworld.zip
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.483058 spine_engine-0.23.4/tests/server/secfolder_for_tests/
+-rw-r--r--   0 runner    (1001) docker     (127)       10 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/server/secfolder_for_tests/allowEndpoints.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.483058 spine_engine-0.23.4/tests/server/secfolder_for_tests/certificates/
+-rw-r--r--   0 runner    (1001) docker     (127)       71 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/server/secfolder_for_tests/certificates/.gitignore
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.483058 spine_engine-0.23.4/tests/server/secfolder_for_tests/private_keys/
+-rw-r--r--   0 runner    (1001) docker     (127)      313 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/server/secfolder_for_tests/private_keys/client.key_secret
+-rw-r--r--   0 runner    (1001) docker     (127)      313 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/server/secfolder_for_tests/private_keys/server.key_secret
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.483058 spine_engine-0.23.4/tests/server/secfolder_for_tests/public_keys/
+-rw-r--r--   0 runner    (1001) docker     (127)      364 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/server/secfolder_for_tests/public_keys/client.key
+-rw-r--r--   0 runner    (1001) docker     (127)      364 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/server/secfolder_for_tests/public_keys/server.key
+-rw-r--r--   0 runner    (1001) docker     (127)    15637 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/server/simple_importer.zip
+-rw-r--r--   0 runner    (1001) docker     (127)     8916 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/server/test_EngineServer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2617 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/server/test_PingService.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4542 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/server/test_ProjectExtractorService.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3259 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/server/test_ProjectRemoverService.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16934 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/server/test_RemoteExecutionService.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2160 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/server/test_start_server.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.483058 spine_engine-0.23.4/tests/server/util/
+-rw-r--r--   0 runner    (1001) docker     (127)      967 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/server/util/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.483058 spine_engine-0.23.4/tests/server/util/projectforpackagingtests/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.483058 spine_engine-0.23.4/tests/server/util/projectforpackagingtests/.spinetoolbox/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.483058 spine_engine-0.23.4/tests/server/util/projectforpackagingtests/.spinetoolbox/local/
+-rw-r--r--   0 runner    (1001) docker     (127)       98 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/server/util/projectforpackagingtests/.spinetoolbox/local/project_local_data.json
+-rw-r--r--   0 runner    (1001) docker     (127)      332 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/server/util/projectforpackagingtests/.spinetoolbox/local/specification_local_data.json
+-rw-r--r--   0 runner    (1001) docker     (127)     1617 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/server/util/projectforpackagingtests/.spinetoolbox/project.json
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.459058 spine_engine-0.23.4/tests/server/util/projectforpackagingtests/.spinetoolbox/specifications/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.483058 spine_engine-0.23.4/tests/server/util/projectforpackagingtests/.spinetoolbox/specifications/Tool/
+-rw-r--r--   0 runner    (1001) docker     (127)      259 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/server/util/projectforpackagingtests/.spinetoolbox/specifications/Tool/helloworld.json
+-rw-r--r--   0 runner    (1001) docker     (127)      286 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/server/util/projectforpackagingtests/.spinetoolbox/specifications/Tool/helloworld2.json
+-rw-r--r--   0 runner    (1001) docker     (127)       68 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/server/util/projectforpackagingtests/helloworld.py
+-rw-r--r--   0 runner    (1001) docker     (127)        7 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/server/util/projectforpackagingtests/input2.txt
+-rw-r--r--   0 runner    (1001) docker     (127)     7013 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/server/util/test_EventDataConverter.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10327 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/server/util/test_ServerMessage.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2506 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/server/util/test_ZipHandler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2297 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/test_load_project_items.py
+-rw-r--r--   0 runner    (1001) docker     (127)    52384 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/test_spine_engine.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:08:09.487058 spine_engine-0.23.4/tests/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)     1027 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2016 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/utils/test_command_line_args.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8070 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/utils/test_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7393 2024-04-23 14:08:02.000000 spine_engine-0.23.4/tests/utils/test_serialization.py
```

### Comparing `spine_engine-0.23.3/.github/workflows/run_unit_tests.yml` & `spine_engine-0.23.4/.github/workflows/run_unit_tests.yml`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,60 +1,60 @@
-# GitHub Action to run spine_engine unit tests
-
-name: Unit tests
-
-# Run workflow on every push
-on:
-  push
-
-jobs:
-  unit-tests:
-    name: Unit tests
-    runs-on: ${{ matrix.os }}
-    strategy:
-      matrix:
-        os: [windows-latest, ubuntu-22.04]
-        python-version: [3.8, 3.9, "3.10", 3.11]
-    steps:
-    - uses: actions/checkout@v3
-      with:
-        fetch-depth: 0
-    - name: Version from Git tags
-      run: git describe --tags
-    - name: Set up Python ${{ matrix.python-version }}
-      uses: actions/setup-python@v4
-      with:
-        python-version: ${{ matrix.python-version }}
-    - name: Display Python version
-      run:
-         python -c "import sys; print(sys.version)"
-    - name: Install additional packages for Linux
-      if: runner.os == 'Linux'
-      run: |
-        sudo apt-get install -y unixodbc-dev
-    - name: Install dependencies
-      env:
-        PYTHONUTF8: 1
-      run: |
-        python -m pip install --upgrade pip
-        pip install git+https://github.com/spine-tools/Spine-Database-API.git#egg=spinedb_api
-        pip install .[dev]
-        pip install --no-deps git+https://github.com/spine-tools/spine-items.git#egg=spine_items
-    - name: List packages
-      run:
-        pip list
-    - name: Install python3 kernelspecs
-      run: |
-        pip install ipykernel
-        python -m ipykernel install --user
-    - name: Run tests
-      run: |
-        if [ "$RUNNER_OS" != "Windows" ]; then
-          export QT_QPA_PLATFORM=offscreen
-        fi
-        coverage run -m unittest discover --verbose
-      shell: bash
-    - name: Combine Coverage reports
-      run:
-        coverage combine
-    - name: Upload coverage report to Codecov
-      uses: codecov/codecov-action@v3
+# GitHub Action to run spine_engine unit tests
+
+name: Unit tests
+
+# Run workflow on every push
+on:
+  push
+
+jobs:
+  unit-tests:
+    name: Unit tests
+    runs-on: ${{ matrix.os }}
+    strategy:
+      matrix:
+        os: [windows-latest, ubuntu-22.04]
+        python-version: [3.8, 3.9, "3.10", 3.11]
+    steps:
+    - uses: actions/checkout@v3
+      with:
+        fetch-depth: 0
+    - name: Version from Git tags
+      run: git describe --tags
+    - name: Set up Python ${{ matrix.python-version }}
+      uses: actions/setup-python@v4
+      with:
+        python-version: ${{ matrix.python-version }}
+    - name: Display Python version
+      run:
+         python -c "import sys; print(sys.version)"
+    - name: Install additional packages for Linux
+      if: runner.os == 'Linux'
+      run: |
+        sudo apt-get install -y unixodbc-dev
+    - name: Install dependencies
+      env:
+        PYTHONUTF8: 1
+      run: |
+        python -m pip install --upgrade pip
+        pip install git+https://github.com/spine-tools/Spine-Database-API.git#egg=spinedb_api
+        pip install .[dev]
+        pip install --no-deps git+https://github.com/spine-tools/spine-items.git#egg=spine_items
+    - name: List packages
+      run:
+        pip list
+    - name: Install python3 kernelspecs
+      run: |
+        pip install ipykernel
+        python -m ipykernel install --user
+    - name: Run tests
+      run: |
+        if [ "$RUNNER_OS" != "Windows" ]; then
+          export QT_QPA_PLATFORM=offscreen
+        fi
+        coverage run -m unittest discover --verbose
+      shell: bash
+    - name: Combine Coverage reports
+      run:
+        coverage combine
+    - name: Upload coverage report to Codecov
+      uses: codecov/codecov-action@v3
```

### Comparing `spine_engine-0.23.3/COPYING` & `spine_engine-0.23.4/COPYING`

 * *Ordering differences only*

 * *Files 7% similar despite different names*

```diff
@@ -1,674 +1,674 @@
-                    GNU GENERAL PUBLIC LICENSE
-                       Version 3, 29 June 2007
-
- Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
- Everyone is permitted to copy and distribute verbatim copies
- of this license document, but changing it is not allowed.
-
-                            Preamble
-
-  The GNU General Public License is a free, copyleft license for
-software and other kinds of works.
-
-  The licenses for most software and other practical works are designed
-to take away your freedom to share and change the works.  By contrast,
-the GNU General Public License is intended to guarantee your freedom to
-share and change all versions of a program--to make sure it remains free
-software for all its users.  We, the Free Software Foundation, use the
-GNU General Public License for most of our software; it applies also to
-any other work released this way by its authors.  You can apply it to
-your programs, too.
-
-  When we speak of free software, we are referring to freedom, not
-price.  Our General Public Licenses are designed to make sure that you
-have the freedom to distribute copies of free software (and charge for
-them if you wish), that you receive source code or can get it if you
-want it, that you can change the software or use pieces of it in new
-free programs, and that you know you can do these things.
-
-  To protect your rights, we need to prevent others from denying you
-these rights or asking you to surrender the rights.  Therefore, you have
-certain responsibilities if you distribute copies of the software, or if
-you modify it: responsibilities to respect the freedom of others.
-
-  For example, if you distribute copies of such a program, whether
-gratis or for a fee, you must pass on to the recipients the same
-freedoms that you received.  You must make sure that they, too, receive
-or can get the source code.  And you must show them these terms so they
-know their rights.
-
-  Developers that use the GNU GPL protect your rights with two steps:
-(1) assert copyright on the software, and (2) offer you this License
-giving you legal permission to copy, distribute and/or modify it.
-
-  For the developers' and authors' protection, the GPL clearly explains
-that there is no warranty for this free software.  For both users' and
-authors' sake, the GPL requires that modified versions be marked as
-changed, so that their problems will not be attributed erroneously to
-authors of previous versions.
-
-  Some devices are designed to deny users access to install or run
-modified versions of the software inside them, although the manufacturer
-can do so.  This is fundamentally incompatible with the aim of
-protecting users' freedom to change the software.  The systematic
-pattern of such abuse occurs in the area of products for individuals to
-use, which is precisely where it is most unacceptable.  Therefore, we
-have designed this version of the GPL to prohibit the practice for those
-products.  If such problems arise substantially in other domains, we
-stand ready to extend this provision to those domains in future versions
-of the GPL, as needed to protect the freedom of users.
-
-  Finally, every program is threatened constantly by software patents.
-States should not allow patents to restrict development and use of
-software on general-purpose computers, but in those that do, we wish to
-avoid the special danger that patents applied to a free program could
-make it effectively proprietary.  To prevent this, the GPL assures that
-patents cannot be used to render the program non-free.
-
-  The precise terms and conditions for copying, distribution and
-modification follow.
-
-                       TERMS AND CONDITIONS
-
-  0. Definitions.
-
-  "This License" refers to version 3 of the GNU General Public License.
-
-  "Copyright" also means copyright-like laws that apply to other kinds of
-works, such as semiconductor masks.
-
-  "The Program" refers to any copyrightable work licensed under this
-License.  Each licensee is addressed as "you".  "Licensees" and
-"recipients" may be individuals or organizations.
-
-  To "modify" a work means to copy from or adapt all or part of the work
-in a fashion requiring copyright permission, other than the making of an
-exact copy.  The resulting work is called a "modified version" of the
-earlier work or a work "based on" the earlier work.
-
-  A "covered work" means either the unmodified Program or a work based
-on the Program.
-
-  To "propagate" a work means to do anything with it that, without
-permission, would make you directly or secondarily liable for
-infringement under applicable copyright law, except executing it on a
-computer or modifying a private copy.  Propagation includes copying,
-distribution (with or without modification), making available to the
-public, and in some countries other activities as well.
-
-  To "convey" a work means any kind of propagation that enables other
-parties to make or receive copies.  Mere interaction with a user through
-a computer network, with no transfer of a copy, is not conveying.
-
-  An interactive user interface displays "Appropriate Legal Notices"
-to the extent that it includes a convenient and prominently visible
-feature that (1) displays an appropriate copyright notice, and (2)
-tells the user that there is no warranty for the work (except to the
-extent that warranties are provided), that licensees may convey the
-work under this License, and how to view a copy of this License.  If
-the interface presents a list of user commands or options, such as a
-menu, a prominent item in the list meets this criterion.
-
-  1. Source Code.
-
-  The "source code" for a work means the preferred form of the work
-for making modifications to it.  "Object code" means any non-source
-form of a work.
-
-  A "Standard Interface" means an interface that either is an official
-standard defined by a recognized standards body, or, in the case of
-interfaces specified for a particular programming language, one that
-is widely used among developers working in that language.
-
-  The "System Libraries" of an executable work include anything, other
-than the work as a whole, that (a) is included in the normal form of
-packaging a Major Component, but which is not part of that Major
-Component, and (b) serves only to enable use of the work with that
-Major Component, or to implement a Standard Interface for which an
-implementation is available to the public in source code form.  A
-"Major Component", in this context, means a major essential component
-(kernel, window system, and so on) of the specific operating system
-(if any) on which the executable work runs, or a compiler used to
-produce the work, or an object code interpreter used to run it.
-
-  The "Corresponding Source" for a work in object code form means all
-the source code needed to generate, install, and (for an executable
-work) run the object code and to modify the work, including scripts to
-control those activities.  However, it does not include the work's
-System Libraries, or general-purpose tools or generally available free
-programs which are used unmodified in performing those activities but
-which are not part of the work.  For example, Corresponding Source
-includes interface definition files associated with source files for
-the work, and the source code for shared libraries and dynamically
-linked subprograms that the work is specifically designed to require,
-such as by intimate data communication or control flow between those
-subprograms and other parts of the work.
-
-  The Corresponding Source need not include anything that users
-can regenerate automatically from other parts of the Corresponding
-Source.
-
-  The Corresponding Source for a work in source code form is that
-same work.
-
-  2. Basic Permissions.
-
-  All rights granted under this License are granted for the term of
-copyright on the Program, and are irrevocable provided the stated
-conditions are met.  This License explicitly affirms your unlimited
-permission to run the unmodified Program.  The output from running a
-covered work is covered by this License only if the output, given its
-content, constitutes a covered work.  This License acknowledges your
-rights of fair use or other equivalent, as provided by copyright law.
-
-  You may make, run and propagate covered works that you do not
-convey, without conditions so long as your license otherwise remains
-in force.  You may convey covered works to others for the sole purpose
-of having them make modifications exclusively for you, or provide you
-with facilities for running those works, provided that you comply with
-the terms of this License in conveying all material for which you do
-not control copyright.  Those thus making or running the covered works
-for you must do so exclusively on your behalf, under your direction
-and control, on terms that prohibit them from making any copies of
-your copyrighted material outside their relationship with you.
-
-  Conveying under any other circumstances is permitted solely under
-the conditions stated below.  Sublicensing is not allowed; section 10
-makes it unnecessary.
-
-  3. Protecting Users' Legal Rights From Anti-Circumvention Law.
-
-  No covered work shall be deemed part of an effective technological
-measure under any applicable law fulfilling obligations under article
-11 of the WIPO copyright treaty adopted on 20 December 1996, or
-similar laws prohibiting or restricting circumvention of such
-measures.
-
-  When you convey a covered work, you waive any legal power to forbid
-circumvention of technological measures to the extent such circumvention
-is effected by exercising rights under this License with respect to
-the covered work, and you disclaim any intention to limit operation or
-modification of the work as a means of enforcing, against the work's
-users, your or third parties' legal rights to forbid circumvention of
-technological measures.
-
-  4. Conveying Verbatim Copies.
-
-  You may convey verbatim copies of the Program's source code as you
-receive it, in any medium, provided that you conspicuously and
-appropriately publish on each copy an appropriate copyright notice;
-keep intact all notices stating that this License and any
-non-permissive terms added in accord with section 7 apply to the code;
-keep intact all notices of the absence of any warranty; and give all
-recipients a copy of this License along with the Program.
-
-  You may charge any price or no price for each copy that you convey,
-and you may offer support or warranty protection for a fee.
-
-  5. Conveying Modified Source Versions.
-
-  You may convey a work based on the Program, or the modifications to
-produce it from the Program, in the form of source code under the
-terms of section 4, provided that you also meet all of these conditions:
-
-    a) The work must carry prominent notices stating that you modified
-    it, and giving a relevant date.
-
-    b) The work must carry prominent notices stating that it is
-    released under this License and any conditions added under section
-    7.  This requirement modifies the requirement in section 4 to
-    "keep intact all notices".
-
-    c) You must license the entire work, as a whole, under this
-    License to anyone who comes into possession of a copy.  This
-    License will therefore apply, along with any applicable section 7
-    additional terms, to the whole of the work, and all its parts,
-    regardless of how they are packaged.  This License gives no
-    permission to license the work in any other way, but it does not
-    invalidate such permission if you have separately received it.
-
-    d) If the work has interactive user interfaces, each must display
-    Appropriate Legal Notices; however, if the Program has interactive
-    interfaces that do not display Appropriate Legal Notices, your
-    work need not make them do so.
-
-  A compilation of a covered work with other separate and independent
-works, which are not by their nature extensions of the covered work,
-and which are not combined with it such as to form a larger program,
-in or on a volume of a storage or distribution medium, is called an
-"aggregate" if the compilation and its resulting copyright are not
-used to limit the access or legal rights of the compilation's users
-beyond what the individual works permit.  Inclusion of a covered work
-in an aggregate does not cause this License to apply to the other
-parts of the aggregate.
-
-  6. Conveying Non-Source Forms.
-
-  You may convey a covered work in object code form under the terms
-of sections 4 and 5, provided that you also convey the
-machine-readable Corresponding Source under the terms of this License,
-in one of these ways:
-
-    a) Convey the object code in, or embodied in, a physical product
-    (including a physical distribution medium), accompanied by the
-    Corresponding Source fixed on a durable physical medium
-    customarily used for software interchange.
-
-    b) Convey the object code in, or embodied in, a physical product
-    (including a physical distribution medium), accompanied by a
-    written offer, valid for at least three years and valid for as
-    long as you offer spare parts or customer support for that product
-    model, to give anyone who possesses the object code either (1) a
-    copy of the Corresponding Source for all the software in the
-    product that is covered by this License, on a durable physical
-    medium customarily used for software interchange, for a price no
-    more than your reasonable cost of physically performing this
-    conveying of source, or (2) access to copy the
-    Corresponding Source from a network server at no charge.
-
-    c) Convey individual copies of the object code with a copy of the
-    written offer to provide the Corresponding Source.  This
-    alternative is allowed only occasionally and noncommercially, and
-    only if you received the object code with such an offer, in accord
-    with subsection 6b.
-
-    d) Convey the object code by offering access from a designated
-    place (gratis or for a charge), and offer equivalent access to the
-    Corresponding Source in the same way through the same place at no
-    further charge.  You need not require recipients to copy the
-    Corresponding Source along with the object code.  If the place to
-    copy the object code is a network server, the Corresponding Source
-    may be on a different server (operated by you or a third party)
-    that supports equivalent copying facilities, provided you maintain
-    clear directions next to the object code saying where to find the
-    Corresponding Source.  Regardless of what server hosts the
-    Corresponding Source, you remain obligated to ensure that it is
-    available for as long as needed to satisfy these requirements.
-
-    e) Convey the object code using peer-to-peer transmission, provided
-    you inform other peers where the object code and Corresponding
-    Source of the work are being offered to the general public at no
-    charge under subsection 6d.
-
-  A separable portion of the object code, whose source code is excluded
-from the Corresponding Source as a System Library, need not be
-included in conveying the object code work.
-
-  A "User Product" is either (1) a "consumer product", which means any
-tangible personal property which is normally used for personal, family,
-or household purposes, or (2) anything designed or sold for incorporation
-into a dwelling.  In determining whether a product is a consumer product,
-doubtful cases shall be resolved in favor of coverage.  For a particular
-product received by a particular user, "normally used" refers to a
-typical or common use of that class of product, regardless of the status
-of the particular user or of the way in which the particular user
-actually uses, or expects or is expected to use, the product.  A product
-is a consumer product regardless of whether the product has substantial
-commercial, industrial or non-consumer uses, unless such uses represent
-the only significant mode of use of the product.
-
-  "Installation Information" for a User Product means any methods,
-procedures, authorization keys, or other information required to install
-and execute modified versions of a covered work in that User Product from
-a modified version of its Corresponding Source.  The information must
-suffice to ensure that the continued functioning of the modified object
-code is in no case prevented or interfered with solely because
-modification has been made.
-
-  If you convey an object code work under this section in, or with, or
-specifically for use in, a User Product, and the conveying occurs as
-part of a transaction in which the right of possession and use of the
-User Product is transferred to the recipient in perpetuity or for a
-fixed term (regardless of how the transaction is characterized), the
-Corresponding Source conveyed under this section must be accompanied
-by the Installation Information.  But this requirement does not apply
-if neither you nor any third party retains the ability to install
-modified object code on the User Product (for example, the work has
-been installed in ROM).
-
-  The requirement to provide Installation Information does not include a
-requirement to continue to provide support service, warranty, or updates
-for a work that has been modified or installed by the recipient, or for
-the User Product in which it has been modified or installed.  Access to a
-network may be denied when the modification itself materially and
-adversely affects the operation of the network or violates the rules and
-protocols for communication across the network.
-
-  Corresponding Source conveyed, and Installation Information provided,
-in accord with this section must be in a format that is publicly
-documented (and with an implementation available to the public in
-source code form), and must require no special password or key for
-unpacking, reading or copying.
-
-  7. Additional Terms.
-
-  "Additional permissions" are terms that supplement the terms of this
-License by making exceptions from one or more of its conditions.
-Additional permissions that are applicable to the entire Program shall
-be treated as though they were included in this License, to the extent
-that they are valid under applicable law.  If additional permissions
-apply only to part of the Program, that part may be used separately
-under those permissions, but the entire Program remains governed by
-this License without regard to the additional permissions.
-
-  When you convey a copy of a covered work, you may at your option
-remove any additional permissions from that copy, or from any part of
-it.  (Additional permissions may be written to require their own
-removal in certain cases when you modify the work.)  You may place
-additional permissions on material, added by you to a covered work,
-for which you have or can give appropriate copyright permission.
-
-  Notwithstanding any other provision of this License, for material you
-add to a covered work, you may (if authorized by the copyright holders of
-that material) supplement the terms of this License with terms:
-
-    a) Disclaiming warranty or limiting liability differently from the
-    terms of sections 15 and 16 of this License; or
-
-    b) Requiring preservation of specified reasonable legal notices or
-    author attributions in that material or in the Appropriate Legal
-    Notices displayed by works containing it; or
-
-    c) Prohibiting misrepresentation of the origin of that material, or
-    requiring that modified versions of such material be marked in
-    reasonable ways as different from the original version; or
-
-    d) Limiting the use for publicity purposes of names of licensors or
-    authors of the material; or
-
-    e) Declining to grant rights under trademark law for use of some
-    trade names, trademarks, or service marks; or
-
-    f) Requiring indemnification of licensors and authors of that
-    material by anyone who conveys the material (or modified versions of
-    it) with contractual assumptions of liability to the recipient, for
-    any liability that these contractual assumptions directly impose on
-    those licensors and authors.
-
-  All other non-permissive additional terms are considered "further
-restrictions" within the meaning of section 10.  If the Program as you
-received it, or any part of it, contains a notice stating that it is
-governed by this License along with a term that is a further
-restriction, you may remove that term.  If a license document contains
-a further restriction but permits relicensing or conveying under this
-License, you may add to a covered work material governed by the terms
-of that license document, provided that the further restriction does
-not survive such relicensing or conveying.
-
-  If you add terms to a covered work in accord with this section, you
-must place, in the relevant source files, a statement of the
-additional terms that apply to those files, or a notice indicating
-where to find the applicable terms.
-
-  Additional terms, permissive or non-permissive, may be stated in the
-form of a separately written license, or stated as exceptions;
-the above requirements apply either way.
-
-  8. Termination.
-
-  You may not propagate or modify a covered work except as expressly
-provided under this License.  Any attempt otherwise to propagate or
-modify it is void, and will automatically terminate your rights under
-this License (including any patent licenses granted under the third
-paragraph of section 11).
-
-  However, if you cease all violation of this License, then your
-license from a particular copyright holder is reinstated (a)
-provisionally, unless and until the copyright holder explicitly and
-finally terminates your license, and (b) permanently, if the copyright
-holder fails to notify you of the violation by some reasonable means
-prior to 60 days after the cessation.
-
-  Moreover, your license from a particular copyright holder is
-reinstated permanently if the copyright holder notifies you of the
-violation by some reasonable means, this is the first time you have
-received notice of violation of this License (for any work) from that
-copyright holder, and you cure the violation prior to 30 days after
-your receipt of the notice.
-
-  Termination of your rights under this section does not terminate the
-licenses of parties who have received copies or rights from you under
-this License.  If your rights have been terminated and not permanently
-reinstated, you do not qualify to receive new licenses for the same
-material under section 10.
-
-  9. Acceptance Not Required for Having Copies.
-
-  You are not required to accept this License in order to receive or
-run a copy of the Program.  Ancillary propagation of a covered work
-occurring solely as a consequence of using peer-to-peer transmission
-to receive a copy likewise does not require acceptance.  However,
-nothing other than this License grants you permission to propagate or
-modify any covered work.  These actions infringe copyright if you do
-not accept this License.  Therefore, by modifying or propagating a
-covered work, you indicate your acceptance of this License to do so.
-
-  10. Automatic Licensing of Downstream Recipients.
-
-  Each time you convey a covered work, the recipient automatically
-receives a license from the original licensors, to run, modify and
-propagate that work, subject to this License.  You are not responsible
-for enforcing compliance by third parties with this License.
-
-  An "entity transaction" is a transaction transferring control of an
-organization, or substantially all assets of one, or subdividing an
-organization, or merging organizations.  If propagation of a covered
-work results from an entity transaction, each party to that
-transaction who receives a copy of the work also receives whatever
-licenses to the work the party's predecessor in interest had or could
-give under the previous paragraph, plus a right to possession of the
-Corresponding Source of the work from the predecessor in interest, if
-the predecessor has it or can get it with reasonable efforts.
-
-  You may not impose any further restrictions on the exercise of the
-rights granted or affirmed under this License.  For example, you may
-not impose a license fee, royalty, or other charge for exercise of
-rights granted under this License, and you may not initiate litigation
-(including a cross-claim or counterclaim in a lawsuit) alleging that
-any patent claim is infringed by making, using, selling, offering for
-sale, or importing the Program or any portion of it.
-
-  11. Patents.
-
-  A "contributor" is a copyright holder who authorizes use under this
-License of the Program or a work on which the Program is based.  The
-work thus licensed is called the contributor's "contributor version".
-
-  A contributor's "essential patent claims" are all patent claims
-owned or controlled by the contributor, whether already acquired or
-hereafter acquired, that would be infringed by some manner, permitted
-by this License, of making, using, or selling its contributor version,
-but do not include claims that would be infringed only as a
-consequence of further modification of the contributor version.  For
-purposes of this definition, "control" includes the right to grant
-patent sublicenses in a manner consistent with the requirements of
-this License.
-
-  Each contributor grants you a non-exclusive, worldwide, royalty-free
-patent license under the contributor's essential patent claims, to
-make, use, sell, offer for sale, import and otherwise run, modify and
-propagate the contents of its contributor version.
-
-  In the following three paragraphs, a "patent license" is any express
-agreement or commitment, however denominated, not to enforce a patent
-(such as an express permission to practice a patent or covenant not to
-sue for patent infringement).  To "grant" such a patent license to a
-party means to make such an agreement or commitment not to enforce a
-patent against the party.
-
-  If you convey a covered work, knowingly relying on a patent license,
-and the Corresponding Source of the work is not available for anyone
-to copy, free of charge and under the terms of this License, through a
-publicly available network server or other readily accessible means,
-then you must either (1) cause the Corresponding Source to be so
-available, or (2) arrange to deprive yourself of the benefit of the
-patent license for this particular work, or (3) arrange, in a manner
-consistent with the requirements of this License, to extend the patent
-license to downstream recipients.  "Knowingly relying" means you have
-actual knowledge that, but for the patent license, your conveying the
-covered work in a country, or your recipient's use of the covered work
-in a country, would infringe one or more identifiable patents in that
-country that you have reason to believe are valid.
-
-  If, pursuant to or in connection with a single transaction or
-arrangement, you convey, or propagate by procuring conveyance of, a
-covered work, and grant a patent license to some of the parties
-receiving the covered work authorizing them to use, propagate, modify
-or convey a specific copy of the covered work, then the patent license
-you grant is automatically extended to all recipients of the covered
-work and works based on it.
-
-  A patent license is "discriminatory" if it does not include within
-the scope of its coverage, prohibits the exercise of, or is
-conditioned on the non-exercise of one or more of the rights that are
-specifically granted under this License.  You may not convey a covered
-work if you are a party to an arrangement with a third party that is
-in the business of distributing software, under which you make payment
-to the third party based on the extent of your activity of conveying
-the work, and under which the third party grants, to any of the
-parties who would receive the covered work from you, a discriminatory
-patent license (a) in connection with copies of the covered work
-conveyed by you (or copies made from those copies), or (b) primarily
-for and in connection with specific products or compilations that
-contain the covered work, unless you entered into that arrangement,
-or that patent license was granted, prior to 28 March 2007.
-
-  Nothing in this License shall be construed as excluding or limiting
-any implied license or other defenses to infringement that may
-otherwise be available to you under applicable patent law.
-
-  12. No Surrender of Others' Freedom.
-
-  If conditions are imposed on you (whether by court order, agreement or
-otherwise) that contradict the conditions of this License, they do not
-excuse you from the conditions of this License.  If you cannot convey a
-covered work so as to satisfy simultaneously your obligations under this
-License and any other pertinent obligations, then as a consequence you may
-not convey it at all.  For example, if you agree to terms that obligate you
-to collect a royalty for further conveying from those to whom you convey
-the Program, the only way you could satisfy both those terms and this
-License would be to refrain entirely from conveying the Program.
-
-  13. Use with the GNU Affero General Public License.
-
-  Notwithstanding any other provision of this License, you have
-permission to link or combine any covered work with a work licensed
-under version 3 of the GNU Affero General Public License into a single
-combined work, and to convey the resulting work.  The terms of this
-License will continue to apply to the part which is the covered work,
-but the special requirements of the GNU Affero General Public License,
-section 13, concerning interaction through a network will apply to the
-combination as such.
-
-  14. Revised Versions of this License.
-
-  The Free Software Foundation may publish revised and/or new versions of
-the GNU General Public License from time to time.  Such new versions will
-be similar in spirit to the present version, but may differ in detail to
-address new problems or concerns.
-
-  Each version is given a distinguishing version number.  If the
-Program specifies that a certain numbered version of the GNU General
-Public License "or any later version" applies to it, you have the
-option of following the terms and conditions either of that numbered
-version or of any later version published by the Free Software
-Foundation.  If the Program does not specify a version number of the
-GNU General Public License, you may choose any version ever published
-by the Free Software Foundation.
-
-  If the Program specifies that a proxy can decide which future
-versions of the GNU General Public License can be used, that proxy's
-public statement of acceptance of a version permanently authorizes you
-to choose that version for the Program.
-
-  Later license versions may give you additional or different
-permissions.  However, no additional obligations are imposed on any
-author or copyright holder as a result of your choosing to follow a
-later version.
-
-  15. Disclaimer of Warranty.
-
-  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
-APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
-HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
-OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
-THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
-PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
-IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
-ALL NECESSARY SERVICING, REPAIR OR CORRECTION.
-
-  16. Limitation of Liability.
-
-  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
-WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
-THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
-GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
-USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
-DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
-PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
-EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
-SUCH DAMAGES.
-
-  17. Interpretation of Sections 15 and 16.
-
-  If the disclaimer of warranty and limitation of liability provided
-above cannot be given local legal effect according to their terms,
-reviewing courts shall apply local law that most closely approximates
-an absolute waiver of all civil liability in connection with the
-Program, unless a warranty or assumption of liability accompanies a
-copy of the Program in return for a fee.
-
-                     END OF TERMS AND CONDITIONS
-
-            How to Apply These Terms to Your New Programs
-
-  If you develop a new program, and you want it to be of the greatest
-possible use to the public, the best way to achieve this is to make it
-free software which everyone can redistribute and change under these terms.
-
-  To do so, attach the following notices to the program.  It is safest
-to attach them to the start of each source file to most effectively
-state the exclusion of warranty; and each file should have at least
-the "copyright" line and a pointer to where the full notice is found.
-
-    <one line to give the program's name and a brief idea of what it does.>
-    Copyright (C) <year>  <name of author>
-
-    This program is free software: you can redistribute it and/or modify
-    it under the terms of the GNU General Public License as published by
-    the Free Software Foundation, either version 3 of the License, or
-    (at your option) any later version.
-
-    This program is distributed in the hope that it will be useful,
-    but WITHOUT ANY WARRANTY; without even the implied warranty of
-    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-    GNU General Public License for more details.
-
-    You should have received a copy of the GNU General Public License
-    along with this program.  If not, see <https://www.gnu.org/licenses/>.
-
-Also add information on how to contact you by electronic and paper mail.
-
-  If the program does terminal interaction, make it output a short
-notice like this when it starts in an interactive mode:
-
-    <program>  Copyright (C) <year>  <name of author>
-    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.
-    This is free software, and you are welcome to redistribute it
-    under certain conditions; type `show c' for details.
-
-The hypothetical commands `show w' and `show c' should show the appropriate
-parts of the General Public License.  Of course, your program's commands
-might be different; for a GUI interface, you would use an "about box".
-
-  You should also get your employer (if you work as a programmer) or school,
-if any, to sign a "copyright disclaimer" for the program, if necessary.
-For more information on this, and how to apply and follow the GNU GPL, see
-<https://www.gnu.org/licenses/>.
-
-  The GNU General Public License does not permit incorporating your program
-into proprietary programs.  If your program is a subroutine library, you
-may consider it more useful to permit linking proprietary applications with
-the library.  If this is what you want to do, use the GNU Lesser General
-Public License instead of this License.  But first, please read
-<https://www.gnu.org/licenses/why-not-lgpl.html>.
+                    GNU GENERAL PUBLIC LICENSE
+                       Version 3, 29 June 2007
+
+ Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
+ Everyone is permitted to copy and distribute verbatim copies
+ of this license document, but changing it is not allowed.
+
+                            Preamble
+
+  The GNU General Public License is a free, copyleft license for
+software and other kinds of works.
+
+  The licenses for most software and other practical works are designed
+to take away your freedom to share and change the works.  By contrast,
+the GNU General Public License is intended to guarantee your freedom to
+share and change all versions of a program--to make sure it remains free
+software for all its users.  We, the Free Software Foundation, use the
+GNU General Public License for most of our software; it applies also to
+any other work released this way by its authors.  You can apply it to
+your programs, too.
+
+  When we speak of free software, we are referring to freedom, not
+price.  Our General Public Licenses are designed to make sure that you
+have the freedom to distribute copies of free software (and charge for
+them if you wish), that you receive source code or can get it if you
+want it, that you can change the software or use pieces of it in new
+free programs, and that you know you can do these things.
+
+  To protect your rights, we need to prevent others from denying you
+these rights or asking you to surrender the rights.  Therefore, you have
+certain responsibilities if you distribute copies of the software, or if
+you modify it: responsibilities to respect the freedom of others.
+
+  For example, if you distribute copies of such a program, whether
+gratis or for a fee, you must pass on to the recipients the same
+freedoms that you received.  You must make sure that they, too, receive
+or can get the source code.  And you must show them these terms so they
+know their rights.
+
+  Developers that use the GNU GPL protect your rights with two steps:
+(1) assert copyright on the software, and (2) offer you this License
+giving you legal permission to copy, distribute and/or modify it.
+
+  For the developers' and authors' protection, the GPL clearly explains
+that there is no warranty for this free software.  For both users' and
+authors' sake, the GPL requires that modified versions be marked as
+changed, so that their problems will not be attributed erroneously to
+authors of previous versions.
+
+  Some devices are designed to deny users access to install or run
+modified versions of the software inside them, although the manufacturer
+can do so.  This is fundamentally incompatible with the aim of
+protecting users' freedom to change the software.  The systematic
+pattern of such abuse occurs in the area of products for individuals to
+use, which is precisely where it is most unacceptable.  Therefore, we
+have designed this version of the GPL to prohibit the practice for those
+products.  If such problems arise substantially in other domains, we
+stand ready to extend this provision to those domains in future versions
+of the GPL, as needed to protect the freedom of users.
+
+  Finally, every program is threatened constantly by software patents.
+States should not allow patents to restrict development and use of
+software on general-purpose computers, but in those that do, we wish to
+avoid the special danger that patents applied to a free program could
+make it effectively proprietary.  To prevent this, the GPL assures that
+patents cannot be used to render the program non-free.
+
+  The precise terms and conditions for copying, distribution and
+modification follow.
+
+                       TERMS AND CONDITIONS
+
+  0. Definitions.
+
+  "This License" refers to version 3 of the GNU General Public License.
+
+  "Copyright" also means copyright-like laws that apply to other kinds of
+works, such as semiconductor masks.
+
+  "The Program" refers to any copyrightable work licensed under this
+License.  Each licensee is addressed as "you".  "Licensees" and
+"recipients" may be individuals or organizations.
+
+  To "modify" a work means to copy from or adapt all or part of the work
+in a fashion requiring copyright permission, other than the making of an
+exact copy.  The resulting work is called a "modified version" of the
+earlier work or a work "based on" the earlier work.
+
+  A "covered work" means either the unmodified Program or a work based
+on the Program.
+
+  To "propagate" a work means to do anything with it that, without
+permission, would make you directly or secondarily liable for
+infringement under applicable copyright law, except executing it on a
+computer or modifying a private copy.  Propagation includes copying,
+distribution (with or without modification), making available to the
+public, and in some countries other activities as well.
+
+  To "convey" a work means any kind of propagation that enables other
+parties to make or receive copies.  Mere interaction with a user through
+a computer network, with no transfer of a copy, is not conveying.
+
+  An interactive user interface displays "Appropriate Legal Notices"
+to the extent that it includes a convenient and prominently visible
+feature that (1) displays an appropriate copyright notice, and (2)
+tells the user that there is no warranty for the work (except to the
+extent that warranties are provided), that licensees may convey the
+work under this License, and how to view a copy of this License.  If
+the interface presents a list of user commands or options, such as a
+menu, a prominent item in the list meets this criterion.
+
+  1. Source Code.
+
+  The "source code" for a work means the preferred form of the work
+for making modifications to it.  "Object code" means any non-source
+form of a work.
+
+  A "Standard Interface" means an interface that either is an official
+standard defined by a recognized standards body, or, in the case of
+interfaces specified for a particular programming language, one that
+is widely used among developers working in that language.
+
+  The "System Libraries" of an executable work include anything, other
+than the work as a whole, that (a) is included in the normal form of
+packaging a Major Component, but which is not part of that Major
+Component, and (b) serves only to enable use of the work with that
+Major Component, or to implement a Standard Interface for which an
+implementation is available to the public in source code form.  A
+"Major Component", in this context, means a major essential component
+(kernel, window system, and so on) of the specific operating system
+(if any) on which the executable work runs, or a compiler used to
+produce the work, or an object code interpreter used to run it.
+
+  The "Corresponding Source" for a work in object code form means all
+the source code needed to generate, install, and (for an executable
+work) run the object code and to modify the work, including scripts to
+control those activities.  However, it does not include the work's
+System Libraries, or general-purpose tools or generally available free
+programs which are used unmodified in performing those activities but
+which are not part of the work.  For example, Corresponding Source
+includes interface definition files associated with source files for
+the work, and the source code for shared libraries and dynamically
+linked subprograms that the work is specifically designed to require,
+such as by intimate data communication or control flow between those
+subprograms and other parts of the work.
+
+  The Corresponding Source need not include anything that users
+can regenerate automatically from other parts of the Corresponding
+Source.
+
+  The Corresponding Source for a work in source code form is that
+same work.
+
+  2. Basic Permissions.
+
+  All rights granted under this License are granted for the term of
+copyright on the Program, and are irrevocable provided the stated
+conditions are met.  This License explicitly affirms your unlimited
+permission to run the unmodified Program.  The output from running a
+covered work is covered by this License only if the output, given its
+content, constitutes a covered work.  This License acknowledges your
+rights of fair use or other equivalent, as provided by copyright law.
+
+  You may make, run and propagate covered works that you do not
+convey, without conditions so long as your license otherwise remains
+in force.  You may convey covered works to others for the sole purpose
+of having them make modifications exclusively for you, or provide you
+with facilities for running those works, provided that you comply with
+the terms of this License in conveying all material for which you do
+not control copyright.  Those thus making or running the covered works
+for you must do so exclusively on your behalf, under your direction
+and control, on terms that prohibit them from making any copies of
+your copyrighted material outside their relationship with you.
+
+  Conveying under any other circumstances is permitted solely under
+the conditions stated below.  Sublicensing is not allowed; section 10
+makes it unnecessary.
+
+  3. Protecting Users' Legal Rights From Anti-Circumvention Law.
+
+  No covered work shall be deemed part of an effective technological
+measure under any applicable law fulfilling obligations under article
+11 of the WIPO copyright treaty adopted on 20 December 1996, or
+similar laws prohibiting or restricting circumvention of such
+measures.
+
+  When you convey a covered work, you waive any legal power to forbid
+circumvention of technological measures to the extent such circumvention
+is effected by exercising rights under this License with respect to
+the covered work, and you disclaim any intention to limit operation or
+modification of the work as a means of enforcing, against the work's
+users, your or third parties' legal rights to forbid circumvention of
+technological measures.
+
+  4. Conveying Verbatim Copies.
+
+  You may convey verbatim copies of the Program's source code as you
+receive it, in any medium, provided that you conspicuously and
+appropriately publish on each copy an appropriate copyright notice;
+keep intact all notices stating that this License and any
+non-permissive terms added in accord with section 7 apply to the code;
+keep intact all notices of the absence of any warranty; and give all
+recipients a copy of this License along with the Program.
+
+  You may charge any price or no price for each copy that you convey,
+and you may offer support or warranty protection for a fee.
+
+  5. Conveying Modified Source Versions.
+
+  You may convey a work based on the Program, or the modifications to
+produce it from the Program, in the form of source code under the
+terms of section 4, provided that you also meet all of these conditions:
+
+    a) The work must carry prominent notices stating that you modified
+    it, and giving a relevant date.
+
+    b) The work must carry prominent notices stating that it is
+    released under this License and any conditions added under section
+    7.  This requirement modifies the requirement in section 4 to
+    "keep intact all notices".
+
+    c) You must license the entire work, as a whole, under this
+    License to anyone who comes into possession of a copy.  This
+    License will therefore apply, along with any applicable section 7
+    additional terms, to the whole of the work, and all its parts,
+    regardless of how they are packaged.  This License gives no
+    permission to license the work in any other way, but it does not
+    invalidate such permission if you have separately received it.
+
+    d) If the work has interactive user interfaces, each must display
+    Appropriate Legal Notices; however, if the Program has interactive
+    interfaces that do not display Appropriate Legal Notices, your
+    work need not make them do so.
+
+  A compilation of a covered work with other separate and independent
+works, which are not by their nature extensions of the covered work,
+and which are not combined with it such as to form a larger program,
+in or on a volume of a storage or distribution medium, is called an
+"aggregate" if the compilation and its resulting copyright are not
+used to limit the access or legal rights of the compilation's users
+beyond what the individual works permit.  Inclusion of a covered work
+in an aggregate does not cause this License to apply to the other
+parts of the aggregate.
+
+  6. Conveying Non-Source Forms.
+
+  You may convey a covered work in object code form under the terms
+of sections 4 and 5, provided that you also convey the
+machine-readable Corresponding Source under the terms of this License,
+in one of these ways:
+
+    a) Convey the object code in, or embodied in, a physical product
+    (including a physical distribution medium), accompanied by the
+    Corresponding Source fixed on a durable physical medium
+    customarily used for software interchange.
+
+    b) Convey the object code in, or embodied in, a physical product
+    (including a physical distribution medium), accompanied by a
+    written offer, valid for at least three years and valid for as
+    long as you offer spare parts or customer support for that product
+    model, to give anyone who possesses the object code either (1) a
+    copy of the Corresponding Source for all the software in the
+    product that is covered by this License, on a durable physical
+    medium customarily used for software interchange, for a price no
+    more than your reasonable cost of physically performing this
+    conveying of source, or (2) access to copy the
+    Corresponding Source from a network server at no charge.
+
+    c) Convey individual copies of the object code with a copy of the
+    written offer to provide the Corresponding Source.  This
+    alternative is allowed only occasionally and noncommercially, and
+    only if you received the object code with such an offer, in accord
+    with subsection 6b.
+
+    d) Convey the object code by offering access from a designated
+    place (gratis or for a charge), and offer equivalent access to the
+    Corresponding Source in the same way through the same place at no
+    further charge.  You need not require recipients to copy the
+    Corresponding Source along with the object code.  If the place to
+    copy the object code is a network server, the Corresponding Source
+    may be on a different server (operated by you or a third party)
+    that supports equivalent copying facilities, provided you maintain
+    clear directions next to the object code saying where to find the
+    Corresponding Source.  Regardless of what server hosts the
+    Corresponding Source, you remain obligated to ensure that it is
+    available for as long as needed to satisfy these requirements.
+
+    e) Convey the object code using peer-to-peer transmission, provided
+    you inform other peers where the object code and Corresponding
+    Source of the work are being offered to the general public at no
+    charge under subsection 6d.
+
+  A separable portion of the object code, whose source code is excluded
+from the Corresponding Source as a System Library, need not be
+included in conveying the object code work.
+
+  A "User Product" is either (1) a "consumer product", which means any
+tangible personal property which is normally used for personal, family,
+or household purposes, or (2) anything designed or sold for incorporation
+into a dwelling.  In determining whether a product is a consumer product,
+doubtful cases shall be resolved in favor of coverage.  For a particular
+product received by a particular user, "normally used" refers to a
+typical or common use of that class of product, regardless of the status
+of the particular user or of the way in which the particular user
+actually uses, or expects or is expected to use, the product.  A product
+is a consumer product regardless of whether the product has substantial
+commercial, industrial or non-consumer uses, unless such uses represent
+the only significant mode of use of the product.
+
+  "Installation Information" for a User Product means any methods,
+procedures, authorization keys, or other information required to install
+and execute modified versions of a covered work in that User Product from
+a modified version of its Corresponding Source.  The information must
+suffice to ensure that the continued functioning of the modified object
+code is in no case prevented or interfered with solely because
+modification has been made.
+
+  If you convey an object code work under this section in, or with, or
+specifically for use in, a User Product, and the conveying occurs as
+part of a transaction in which the right of possession and use of the
+User Product is transferred to the recipient in perpetuity or for a
+fixed term (regardless of how the transaction is characterized), the
+Corresponding Source conveyed under this section must be accompanied
+by the Installation Information.  But this requirement does not apply
+if neither you nor any third party retains the ability to install
+modified object code on the User Product (for example, the work has
+been installed in ROM).
+
+  The requirement to provide Installation Information does not include a
+requirement to continue to provide support service, warranty, or updates
+for a work that has been modified or installed by the recipient, or for
+the User Product in which it has been modified or installed.  Access to a
+network may be denied when the modification itself materially and
+adversely affects the operation of the network or violates the rules and
+protocols for communication across the network.
+
+  Corresponding Source conveyed, and Installation Information provided,
+in accord with this section must be in a format that is publicly
+documented (and with an implementation available to the public in
+source code form), and must require no special password or key for
+unpacking, reading or copying.
+
+  7. Additional Terms.
+
+  "Additional permissions" are terms that supplement the terms of this
+License by making exceptions from one or more of its conditions.
+Additional permissions that are applicable to the entire Program shall
+be treated as though they were included in this License, to the extent
+that they are valid under applicable law.  If additional permissions
+apply only to part of the Program, that part may be used separately
+under those permissions, but the entire Program remains governed by
+this License without regard to the additional permissions.
+
+  When you convey a copy of a covered work, you may at your option
+remove any additional permissions from that copy, or from any part of
+it.  (Additional permissions may be written to require their own
+removal in certain cases when you modify the work.)  You may place
+additional permissions on material, added by you to a covered work,
+for which you have or can give appropriate copyright permission.
+
+  Notwithstanding any other provision of this License, for material you
+add to a covered work, you may (if authorized by the copyright holders of
+that material) supplement the terms of this License with terms:
+
+    a) Disclaiming warranty or limiting liability differently from the
+    terms of sections 15 and 16 of this License; or
+
+    b) Requiring preservation of specified reasonable legal notices or
+    author attributions in that material or in the Appropriate Legal
+    Notices displayed by works containing it; or
+
+    c) Prohibiting misrepresentation of the origin of that material, or
+    requiring that modified versions of such material be marked in
+    reasonable ways as different from the original version; or
+
+    d) Limiting the use for publicity purposes of names of licensors or
+    authors of the material; or
+
+    e) Declining to grant rights under trademark law for use of some
+    trade names, trademarks, or service marks; or
+
+    f) Requiring indemnification of licensors and authors of that
+    material by anyone who conveys the material (or modified versions of
+    it) with contractual assumptions of liability to the recipient, for
+    any liability that these contractual assumptions directly impose on
+    those licensors and authors.
+
+  All other non-permissive additional terms are considered "further
+restrictions" within the meaning of section 10.  If the Program as you
+received it, or any part of it, contains a notice stating that it is
+governed by this License along with a term that is a further
+restriction, you may remove that term.  If a license document contains
+a further restriction but permits relicensing or conveying under this
+License, you may add to a covered work material governed by the terms
+of that license document, provided that the further restriction does
+not survive such relicensing or conveying.
+
+  If you add terms to a covered work in accord with this section, you
+must place, in the relevant source files, a statement of the
+additional terms that apply to those files, or a notice indicating
+where to find the applicable terms.
+
+  Additional terms, permissive or non-permissive, may be stated in the
+form of a separately written license, or stated as exceptions;
+the above requirements apply either way.
+
+  8. Termination.
+
+  You may not propagate or modify a covered work except as expressly
+provided under this License.  Any attempt otherwise to propagate or
+modify it is void, and will automatically terminate your rights under
+this License (including any patent licenses granted under the third
+paragraph of section 11).
+
+  However, if you cease all violation of this License, then your
+license from a particular copyright holder is reinstated (a)
+provisionally, unless and until the copyright holder explicitly and
+finally terminates your license, and (b) permanently, if the copyright
+holder fails to notify you of the violation by some reasonable means
+prior to 60 days after the cessation.
+
+  Moreover, your license from a particular copyright holder is
+reinstated permanently if the copyright holder notifies you of the
+violation by some reasonable means, this is the first time you have
+received notice of violation of this License (for any work) from that
+copyright holder, and you cure the violation prior to 30 days after
+your receipt of the notice.
+
+  Termination of your rights under this section does not terminate the
+licenses of parties who have received copies or rights from you under
+this License.  If your rights have been terminated and not permanently
+reinstated, you do not qualify to receive new licenses for the same
+material under section 10.
+
+  9. Acceptance Not Required for Having Copies.
+
+  You are not required to accept this License in order to receive or
+run a copy of the Program.  Ancillary propagation of a covered work
+occurring solely as a consequence of using peer-to-peer transmission
+to receive a copy likewise does not require acceptance.  However,
+nothing other than this License grants you permission to propagate or
+modify any covered work.  These actions infringe copyright if you do
+not accept this License.  Therefore, by modifying or propagating a
+covered work, you indicate your acceptance of this License to do so.
+
+  10. Automatic Licensing of Downstream Recipients.
+
+  Each time you convey a covered work, the recipient automatically
+receives a license from the original licensors, to run, modify and
+propagate that work, subject to this License.  You are not responsible
+for enforcing compliance by third parties with this License.
+
+  An "entity transaction" is a transaction transferring control of an
+organization, or substantially all assets of one, or subdividing an
+organization, or merging organizations.  If propagation of a covered
+work results from an entity transaction, each party to that
+transaction who receives a copy of the work also receives whatever
+licenses to the work the party's predecessor in interest had or could
+give under the previous paragraph, plus a right to possession of the
+Corresponding Source of the work from the predecessor in interest, if
+the predecessor has it or can get it with reasonable efforts.
+
+  You may not impose any further restrictions on the exercise of the
+rights granted or affirmed under this License.  For example, you may
+not impose a license fee, royalty, or other charge for exercise of
+rights granted under this License, and you may not initiate litigation
+(including a cross-claim or counterclaim in a lawsuit) alleging that
+any patent claim is infringed by making, using, selling, offering for
+sale, or importing the Program or any portion of it.
+
+  11. Patents.
+
+  A "contributor" is a copyright holder who authorizes use under this
+License of the Program or a work on which the Program is based.  The
+work thus licensed is called the contributor's "contributor version".
+
+  A contributor's "essential patent claims" are all patent claims
+owned or controlled by the contributor, whether already acquired or
+hereafter acquired, that would be infringed by some manner, permitted
+by this License, of making, using, or selling its contributor version,
+but do not include claims that would be infringed only as a
+consequence of further modification of the contributor version.  For
+purposes of this definition, "control" includes the right to grant
+patent sublicenses in a manner consistent with the requirements of
+this License.
+
+  Each contributor grants you a non-exclusive, worldwide, royalty-free
+patent license under the contributor's essential patent claims, to
+make, use, sell, offer for sale, import and otherwise run, modify and
+propagate the contents of its contributor version.
+
+  In the following three paragraphs, a "patent license" is any express
+agreement or commitment, however denominated, not to enforce a patent
+(such as an express permission to practice a patent or covenant not to
+sue for patent infringement).  To "grant" such a patent license to a
+party means to make such an agreement or commitment not to enforce a
+patent against the party.
+
+  If you convey a covered work, knowingly relying on a patent license,
+and the Corresponding Source of the work is not available for anyone
+to copy, free of charge and under the terms of this License, through a
+publicly available network server or other readily accessible means,
+then you must either (1) cause the Corresponding Source to be so
+available, or (2) arrange to deprive yourself of the benefit of the
+patent license for this particular work, or (3) arrange, in a manner
+consistent with the requirements of this License, to extend the patent
+license to downstream recipients.  "Knowingly relying" means you have
+actual knowledge that, but for the patent license, your conveying the
+covered work in a country, or your recipient's use of the covered work
+in a country, would infringe one or more identifiable patents in that
+country that you have reason to believe are valid.
+
+  If, pursuant to or in connection with a single transaction or
+arrangement, you convey, or propagate by procuring conveyance of, a
+covered work, and grant a patent license to some of the parties
+receiving the covered work authorizing them to use, propagate, modify
+or convey a specific copy of the covered work, then the patent license
+you grant is automatically extended to all recipients of the covered
+work and works based on it.
+
+  A patent license is "discriminatory" if it does not include within
+the scope of its coverage, prohibits the exercise of, or is
+conditioned on the non-exercise of one or more of the rights that are
+specifically granted under this License.  You may not convey a covered
+work if you are a party to an arrangement with a third party that is
+in the business of distributing software, under which you make payment
+to the third party based on the extent of your activity of conveying
+the work, and under which the third party grants, to any of the
+parties who would receive the covered work from you, a discriminatory
+patent license (a) in connection with copies of the covered work
+conveyed by you (or copies made from those copies), or (b) primarily
+for and in connection with specific products or compilations that
+contain the covered work, unless you entered into that arrangement,
+or that patent license was granted, prior to 28 March 2007.
+
+  Nothing in this License shall be construed as excluding or limiting
+any implied license or other defenses to infringement that may
+otherwise be available to you under applicable patent law.
+
+  12. No Surrender of Others' Freedom.
+
+  If conditions are imposed on you (whether by court order, agreement or
+otherwise) that contradict the conditions of this License, they do not
+excuse you from the conditions of this License.  If you cannot convey a
+covered work so as to satisfy simultaneously your obligations under this
+License and any other pertinent obligations, then as a consequence you may
+not convey it at all.  For example, if you agree to terms that obligate you
+to collect a royalty for further conveying from those to whom you convey
+the Program, the only way you could satisfy both those terms and this
+License would be to refrain entirely from conveying the Program.
+
+  13. Use with the GNU Affero General Public License.
+
+  Notwithstanding any other provision of this License, you have
+permission to link or combine any covered work with a work licensed
+under version 3 of the GNU Affero General Public License into a single
+combined work, and to convey the resulting work.  The terms of this
+License will continue to apply to the part which is the covered work,
+but the special requirements of the GNU Affero General Public License,
+section 13, concerning interaction through a network will apply to the
+combination as such.
+
+  14. Revised Versions of this License.
+
+  The Free Software Foundation may publish revised and/or new versions of
+the GNU General Public License from time to time.  Such new versions will
+be similar in spirit to the present version, but may differ in detail to
+address new problems or concerns.
+
+  Each version is given a distinguishing version number.  If the
+Program specifies that a certain numbered version of the GNU General
+Public License "or any later version" applies to it, you have the
+option of following the terms and conditions either of that numbered
+version or of any later version published by the Free Software
+Foundation.  If the Program does not specify a version number of the
+GNU General Public License, you may choose any version ever published
+by the Free Software Foundation.
+
+  If the Program specifies that a proxy can decide which future
+versions of the GNU General Public License can be used, that proxy's
+public statement of acceptance of a version permanently authorizes you
+to choose that version for the Program.
+
+  Later license versions may give you additional or different
+permissions.  However, no additional obligations are imposed on any
+author or copyright holder as a result of your choosing to follow a
+later version.
+
+  15. Disclaimer of Warranty.
+
+  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
+APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
+HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
+OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
+THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
+IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
+ALL NECESSARY SERVICING, REPAIR OR CORRECTION.
+
+  16. Limitation of Liability.
+
+  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
+WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
+THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
+GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
+USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
+DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
+PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
+EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
+SUCH DAMAGES.
+
+  17. Interpretation of Sections 15 and 16.
+
+  If the disclaimer of warranty and limitation of liability provided
+above cannot be given local legal effect according to their terms,
+reviewing courts shall apply local law that most closely approximates
+an absolute waiver of all civil liability in connection with the
+Program, unless a warranty or assumption of liability accompanies a
+copy of the Program in return for a fee.
+
+                     END OF TERMS AND CONDITIONS
+
+            How to Apply These Terms to Your New Programs
+
+  If you develop a new program, and you want it to be of the greatest
+possible use to the public, the best way to achieve this is to make it
+free software which everyone can redistribute and change under these terms.
+
+  To do so, attach the following notices to the program.  It is safest
+to attach them to the start of each source file to most effectively
+state the exclusion of warranty; and each file should have at least
+the "copyright" line and a pointer to where the full notice is found.
+
+    <one line to give the program's name and a brief idea of what it does.>
+    Copyright (C) <year>  <name of author>
+
+    This program is free software: you can redistribute it and/or modify
+    it under the terms of the GNU General Public License as published by
+    the Free Software Foundation, either version 3 of the License, or
+    (at your option) any later version.
+
+    This program is distributed in the hope that it will be useful,
+    but WITHOUT ANY WARRANTY; without even the implied warranty of
+    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+    GNU General Public License for more details.
+
+    You should have received a copy of the GNU General Public License
+    along with this program.  If not, see <https://www.gnu.org/licenses/>.
+
+Also add information on how to contact you by electronic and paper mail.
+
+  If the program does terminal interaction, make it output a short
+notice like this when it starts in an interactive mode:
+
+    <program>  Copyright (C) <year>  <name of author>
+    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.
+    This is free software, and you are welcome to redistribute it
+    under certain conditions; type `show c' for details.
+
+The hypothetical commands `show w' and `show c' should show the appropriate
+parts of the General Public License.  Of course, your program's commands
+might be different; for a GUI interface, you would use an "about box".
+
+  You should also get your employer (if you work as a programmer) or school,
+if any, to sign a "copyright disclaimer" for the program, if necessary.
+For more information on this, and how to apply and follow the GNU GPL, see
+<https://www.gnu.org/licenses/>.
+
+  The GNU General Public License does not permit incorporating your program
+into proprietary programs.  If your program is a subroutine library, you
+may consider it more useful to permit linking proprietary applications with
+the library.  If this is what you want to do, use the GNU Lesser General
+Public License instead of this License.  But first, please read
+<https://www.gnu.org/licenses/why-not-lgpl.html>.
```

### Comparing `spine_engine-0.23.3/COPYING.LESSER` & `spine_engine-0.23.4/COPYING.LESSER`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,165 +1,165 @@
-                   GNU LESSER GENERAL PUBLIC LICENSE
-                       Version 3, 29 June 2007
-
- Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
- Everyone is permitted to copy and distribute verbatim copies
- of this license document, but changing it is not allowed.
-
-
-  This version of the GNU Lesser General Public License incorporates
-the terms and conditions of version 3 of the GNU General Public
-License, supplemented by the additional permissions listed below.
-
-  0. Additional Definitions.
-
-  As used herein, "this License" refers to version 3 of the GNU Lesser
-General Public License, and the "GNU GPL" refers to version 3 of the GNU
-General Public License.
-
-  "The Library" refers to a covered work governed by this License,
-other than an Application or a Combined Work as defined below.
-
-  An "Application" is any work that makes use of an interface provided
-by the Library, but which is not otherwise based on the Library.
-Defining a subclass of a class defined by the Library is deemed a mode
-of using an interface provided by the Library.
-
-  A "Combined Work" is a work produced by combining or linking an
-Application with the Library.  The particular version of the Library
-with which the Combined Work was made is also called the "Linked
-Version".
-
-  The "Minimal Corresponding Source" for a Combined Work means the
-Corresponding Source for the Combined Work, excluding any source code
-for portions of the Combined Work that, considered in isolation, are
-based on the Application, and not on the Linked Version.
-
-  The "Corresponding Application Code" for a Combined Work means the
-object code and/or source code for the Application, including any data
-and utility programs needed for reproducing the Combined Work from the
-Application, but excluding the System Libraries of the Combined Work.
-
-  1. Exception to Section 3 of the GNU GPL.
-
-  You may convey a covered work under sections 3 and 4 of this License
-without being bound by section 3 of the GNU GPL.
-
-  2. Conveying Modified Versions.
-
-  If you modify a copy of the Library, and, in your modifications, a
-facility refers to a function or data to be supplied by an Application
-that uses the facility (other than as an argument passed when the
-facility is invoked), then you may convey a copy of the modified
-version:
-
-   a) under this License, provided that you make a good faith effort to
-   ensure that, in the event an Application does not supply the
-   function or data, the facility still operates, and performs
-   whatever part of its purpose remains meaningful, or
-
-   b) under the GNU GPL, with none of the additional permissions of
-   this License applicable to that copy.
-
-  3. Object Code Incorporating Material from Library Header Files.
-
-  The object code form of an Application may incorporate material from
-a header file that is part of the Library.  You may convey such object
-code under terms of your choice, provided that, if the incorporated
-material is not limited to numerical parameters, data structure
-layouts and accessors, or small macros, inline functions and templates
-(ten or fewer lines in length), you do both of the following:
-
-   a) Give prominent notice with each copy of the object code that the
-   Library is used in it and that the Library and its use are
-   covered by this License.
-
-   b) Accompany the object code with a copy of the GNU GPL and this license
-   document.
-
-  4. Combined Works.
-
-  You may convey a Combined Work under terms of your choice that,
-taken together, effectively do not restrict modification of the
-portions of the Library contained in the Combined Work and reverse
-engineering for debugging such modifications, if you also do each of
-the following:
-
-   a) Give prominent notice with each copy of the Combined Work that
-   the Library is used in it and that the Library and its use are
-   covered by this License.
-
-   b) Accompany the Combined Work with a copy of the GNU GPL and this license
-   document.
-
-   c) For a Combined Work that displays copyright notices during
-   execution, include the copyright notice for the Library among
-   these notices, as well as a reference directing the user to the
-   copies of the GNU GPL and this license document.
-
-   d) Do one of the following:
-
-       0) Convey the Minimal Corresponding Source under the terms of this
-       License, and the Corresponding Application Code in a form
-       suitable for, and under terms that permit, the user to
-       recombine or relink the Application with a modified version of
-       the Linked Version to produce a modified Combined Work, in the
-       manner specified by section 6 of the GNU GPL for conveying
-       Corresponding Source.
-
-       1) Use a suitable shared library mechanism for linking with the
-       Library.  A suitable mechanism is one that (a) uses at run time
-       a copy of the Library already present on the user's computer
-       system, and (b) will operate properly with a modified version
-       of the Library that is interface-compatible with the Linked
-       Version.
-
-   e) Provide Installation Information, but only if you would otherwise
-   be required to provide such information under section 6 of the
-   GNU GPL, and only to the extent that such information is
-   necessary to install and execute a modified version of the
-   Combined Work produced by recombining or relinking the
-   Application with a modified version of the Linked Version. (If
-   you use option 4d0, the Installation Information must accompany
-   the Minimal Corresponding Source and Corresponding Application
-   Code. If you use option 4d1, you must provide the Installation
-   Information in the manner specified by section 6 of the GNU GPL
-   for conveying Corresponding Source.)
-
-  5. Combined Libraries.
-
-  You may place library facilities that are a work based on the
-Library side by side in a single library together with other library
-facilities that are not Applications and are not covered by this
-License, and convey such a combined library under terms of your
-choice, if you do both of the following:
-
-   a) Accompany the combined library with a copy of the same work based
-   on the Library, uncombined with any other library facilities,
-   conveyed under the terms of this License.
-
-   b) Give prominent notice with the combined library that part of it
-   is a work based on the Library, and explaining where to find the
-   accompanying uncombined form of the same work.
-
-  6. Revised Versions of the GNU Lesser General Public License.
-
-  The Free Software Foundation may publish revised and/or new versions
-of the GNU Lesser General Public License from time to time. Such new
-versions will be similar in spirit to the present version, but may
-differ in detail to address new problems or concerns.
-
-  Each version is given a distinguishing version number. If the
-Library as you received it specifies that a certain numbered version
-of the GNU Lesser General Public License "or any later version"
-applies to it, you have the option of following the terms and
-conditions either of that published version or of any later version
-published by the Free Software Foundation. If the Library as you
-received it does not specify a version number of the GNU Lesser
-General Public License, you may choose any version of the GNU Lesser
-General Public License ever published by the Free Software Foundation.
-
-  If the Library as you received it specifies that a proxy can decide
-whether future versions of the GNU Lesser General Public License shall
-apply, that proxy's public statement of acceptance of any version is
-permanent authorization for you to choose that version for the
+                   GNU LESSER GENERAL PUBLIC LICENSE
+                       Version 3, 29 June 2007
+
+ Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
+ Everyone is permitted to copy and distribute verbatim copies
+ of this license document, but changing it is not allowed.
+
+
+  This version of the GNU Lesser General Public License incorporates
+the terms and conditions of version 3 of the GNU General Public
+License, supplemented by the additional permissions listed below.
+
+  0. Additional Definitions.
+
+  As used herein, "this License" refers to version 3 of the GNU Lesser
+General Public License, and the "GNU GPL" refers to version 3 of the GNU
+General Public License.
+
+  "The Library" refers to a covered work governed by this License,
+other than an Application or a Combined Work as defined below.
+
+  An "Application" is any work that makes use of an interface provided
+by the Library, but which is not otherwise based on the Library.
+Defining a subclass of a class defined by the Library is deemed a mode
+of using an interface provided by the Library.
+
+  A "Combined Work" is a work produced by combining or linking an
+Application with the Library.  The particular version of the Library
+with which the Combined Work was made is also called the "Linked
+Version".
+
+  The "Minimal Corresponding Source" for a Combined Work means the
+Corresponding Source for the Combined Work, excluding any source code
+for portions of the Combined Work that, considered in isolation, are
+based on the Application, and not on the Linked Version.
+
+  The "Corresponding Application Code" for a Combined Work means the
+object code and/or source code for the Application, including any data
+and utility programs needed for reproducing the Combined Work from the
+Application, but excluding the System Libraries of the Combined Work.
+
+  1. Exception to Section 3 of the GNU GPL.
+
+  You may convey a covered work under sections 3 and 4 of this License
+without being bound by section 3 of the GNU GPL.
+
+  2. Conveying Modified Versions.
+
+  If you modify a copy of the Library, and, in your modifications, a
+facility refers to a function or data to be supplied by an Application
+that uses the facility (other than as an argument passed when the
+facility is invoked), then you may convey a copy of the modified
+version:
+
+   a) under this License, provided that you make a good faith effort to
+   ensure that, in the event an Application does not supply the
+   function or data, the facility still operates, and performs
+   whatever part of its purpose remains meaningful, or
+
+   b) under the GNU GPL, with none of the additional permissions of
+   this License applicable to that copy.
+
+  3. Object Code Incorporating Material from Library Header Files.
+
+  The object code form of an Application may incorporate material from
+a header file that is part of the Library.  You may convey such object
+code under terms of your choice, provided that, if the incorporated
+material is not limited to numerical parameters, data structure
+layouts and accessors, or small macros, inline functions and templates
+(ten or fewer lines in length), you do both of the following:
+
+   a) Give prominent notice with each copy of the object code that the
+   Library is used in it and that the Library and its use are
+   covered by this License.
+
+   b) Accompany the object code with a copy of the GNU GPL and this license
+   document.
+
+  4. Combined Works.
+
+  You may convey a Combined Work under terms of your choice that,
+taken together, effectively do not restrict modification of the
+portions of the Library contained in the Combined Work and reverse
+engineering for debugging such modifications, if you also do each of
+the following:
+
+   a) Give prominent notice with each copy of the Combined Work that
+   the Library is used in it and that the Library and its use are
+   covered by this License.
+
+   b) Accompany the Combined Work with a copy of the GNU GPL and this license
+   document.
+
+   c) For a Combined Work that displays copyright notices during
+   execution, include the copyright notice for the Library among
+   these notices, as well as a reference directing the user to the
+   copies of the GNU GPL and this license document.
+
+   d) Do one of the following:
+
+       0) Convey the Minimal Corresponding Source under the terms of this
+       License, and the Corresponding Application Code in a form
+       suitable for, and under terms that permit, the user to
+       recombine or relink the Application with a modified version of
+       the Linked Version to produce a modified Combined Work, in the
+       manner specified by section 6 of the GNU GPL for conveying
+       Corresponding Source.
+
+       1) Use a suitable shared library mechanism for linking with the
+       Library.  A suitable mechanism is one that (a) uses at run time
+       a copy of the Library already present on the user's computer
+       system, and (b) will operate properly with a modified version
+       of the Library that is interface-compatible with the Linked
+       Version.
+
+   e) Provide Installation Information, but only if you would otherwise
+   be required to provide such information under section 6 of the
+   GNU GPL, and only to the extent that such information is
+   necessary to install and execute a modified version of the
+   Combined Work produced by recombining or relinking the
+   Application with a modified version of the Linked Version. (If
+   you use option 4d0, the Installation Information must accompany
+   the Minimal Corresponding Source and Corresponding Application
+   Code. If you use option 4d1, you must provide the Installation
+   Information in the manner specified by section 6 of the GNU GPL
+   for conveying Corresponding Source.)
+
+  5. Combined Libraries.
+
+  You may place library facilities that are a work based on the
+Library side by side in a single library together with other library
+facilities that are not Applications and are not covered by this
+License, and convey such a combined library under terms of your
+choice, if you do both of the following:
+
+   a) Accompany the combined library with a copy of the same work based
+   on the Library, uncombined with any other library facilities,
+   conveyed under the terms of this License.
+
+   b) Give prominent notice with the combined library that part of it
+   is a work based on the Library, and explaining where to find the
+   accompanying uncombined form of the same work.
+
+  6. Revised Versions of the GNU Lesser General Public License.
+
+  The Free Software Foundation may publish revised and/or new versions
+of the GNU Lesser General Public License from time to time. Such new
+versions will be similar in spirit to the present version, but may
+differ in detail to address new problems or concerns.
+
+  Each version is given a distinguishing version number. If the
+Library as you received it specifies that a certain numbered version
+of the GNU Lesser General Public License "or any later version"
+applies to it, you have the option of following the terms and
+conditions either of that published version or of any later version
+published by the Free Software Foundation. If the Library as you
+received it does not specify a version number of the GNU Lesser
+General Public License, you may choose any version of the GNU Lesser
+General Public License ever published by the Free Software Foundation.
+
+  If the Library as you received it specifies that a proxy can decide
+whether future versions of the GNU Lesser General Public License shall
+apply, that proxy's public statement of acceptance of any version is
+permanent authorization for you to choose that version for the
 Library.
```

### Comparing `spine_engine-0.23.3/PKG-INFO` & `spine_engine-0.23.4/PKG-INFO`

 * *Files 12% similar despite different names*

```diff
@@ -1,74 +1,74 @@
-Metadata-Version: 2.1
-Name: spine_engine
-Version: 0.23.3
-Summary: A package to run Spine workflows.
-Author-email: Spine Project consortium <spine_info@vtt.fi>
-License: LGPL-3.0-or-later
-Project-URL: Repository, https://github.com/spine-tools/spine-engine
-Keywords: energy system modelling,workflow,optimisation,database
-Classifier: Programming Language :: Python :: 3
-Classifier: License :: OSI Approved :: GNU Lesser General Public License v3 (LGPLv3)
-Classifier: Operating System :: OS Independent
-Requires-Python: <3.12,>=3.8.1
-Description-Content-Type: text/markdown
-License-File: COPYING
-License-File: COPYING.LESSER
-Requires-Dist: dagster<0.12.9,>=0.12.6
-Requires-Dist: pendulum<3.0.0
-Requires-Dist: protobuf<3.21.0
-Requires-Dist: networkx>2.5.1
-Requires-Dist: datapackage<1.16,>=1.15.2
-Requires-Dist: jupyter_client>=6.0
-Requires-Dist: spinedb_api>=0.30.3
-Requires-Dist: pyzmq>=21.0
-Requires-Dist: markupsafe<2.1
-Provides-Extra: dev
-Requires-Dist: coverage[toml]; extra == "dev"
-
-# Spine Engine
-
-[![Python](https://img.shields.io/badge/python-3.8%20|%203.9%20|%203.10%20|%203.11-blue.svg)](https://www.python.org/downloads/release/python-379/)
-[![Unit tests](https://github.com/spine-tools/spine-engine/workflows/Unit%20tests/badge.svg)](https://github.com/spine-tools/spine-engine/actions?query=workflow%3A"Unit+tests")
-[![codecov](https://codecov.io/gh/spine-tools/spine-engine/branch/master/graph/badge.svg)](https://codecov.io/gh/spine-tools/spine-engine)
-[![PyPI version](https://badge.fury.io/py/spine-engine.svg)](https://badge.fury.io/py/spine-engine)
-
-A Python package to coordinate the execution of [Spine Toolbox](https://github.com/spine-tools/Spine-Toolbox) workflows.
-
-<p align="center" width="100%">
-  <picture>
-    <source media="(prefers-color-scheme: dark)" srcset="./fig/spineengine_logo.svg" width="50%">
-    <img alt="Spine Engine" src="./fig/spineengine_on_wht.svg" width="50%">
-  </picture>
-</p>
-
-## License
-
-Spine Engine is released under the GNU Lesser General Public License (LGPL) license. All accompanying
-documentation and manual are released under the [Creative Commons BY-SA 4.0 license](https://creativecommons.org/licenses/by-sa/4.0/).
-
-## Getting started
-
-### Installation
-
-To install Spine Engine into an existing Python environment, run
-
-    $ pip install spine_engine
-
-### Dependencies
-
-Spine Engine installation will install [dagster](https://dagster.readthedocs.io/en/master/index.html).
-
-&nbsp;
-<hr>
-<center>
-<table width=500px frame="none">
-<tr>
-<td valign="middle" width=100px>
-<img src=fig/eu-emblem-low-res.jpg alt="EU emblem" width=100%></td>
-<td valign="middle">This project has received funding from European Climate, Infrastructure and Environment Executive Agency under the European Union’s HORIZON Research and Innovation Actions under grant agreement N°101095998.</td>
-<tr>
-<td valign="middle" width=100px>
-<img src=fig/eu-emblem-low-res.jpg alt="EU emblem" width=100%></td>
-<td valign="middle">This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 774629.</td>
-</table>
-</center>
+Metadata-Version: 2.1
+Name: spine_engine
+Version: 0.23.4
+Summary: A package to run Spine workflows.
+Author-email: Spine Project consortium <spine_info@vtt.fi>
+License: LGPL-3.0-or-later
+Project-URL: Repository, https://github.com/spine-tools/spine-engine
+Keywords: energy system modelling,workflow,optimisation,database
+Classifier: Programming Language :: Python :: 3
+Classifier: License :: OSI Approved :: GNU Lesser General Public License v3 (LGPLv3)
+Classifier: Operating System :: OS Independent
+Requires-Python: <3.12,>=3.8.1
+Description-Content-Type: text/markdown
+License-File: COPYING
+License-File: COPYING.LESSER
+Requires-Dist: dagster<0.12.9,>=0.12.6
+Requires-Dist: pendulum<3.0.0
+Requires-Dist: protobuf<3.21.0
+Requires-Dist: networkx>2.5.1
+Requires-Dist: datapackage<1.16,>=1.15.2
+Requires-Dist: jupyter_client>=6.0
+Requires-Dist: spinedb_api>=0.30.5
+Requires-Dist: pyzmq>=21.0
+Requires-Dist: markupsafe<2.1
+Provides-Extra: dev
+Requires-Dist: coverage[toml]; extra == "dev"
+
+# Spine Engine
+
+[![Python](https://img.shields.io/badge/python-3.8%20|%203.9%20|%203.10%20|%203.11-blue.svg)](https://www.python.org/downloads/release/python-379/)
+[![Unit tests](https://github.com/spine-tools/spine-engine/workflows/Unit%20tests/badge.svg)](https://github.com/spine-tools/spine-engine/actions?query=workflow%3A"Unit+tests")
+[![codecov](https://codecov.io/gh/spine-tools/spine-engine/branch/master/graph/badge.svg)](https://codecov.io/gh/spine-tools/spine-engine)
+[![PyPI version](https://badge.fury.io/py/spine-engine.svg)](https://badge.fury.io/py/spine-engine)
+
+A Python package to coordinate the execution of [Spine Toolbox](https://github.com/spine-tools/Spine-Toolbox) workflows.
+
+<p align="center" width="100%">
+  <picture>
+    <source media="(prefers-color-scheme: dark)" srcset="./fig/spineengine_logo.svg" width="50%">
+    <img alt="Spine Engine" src="./fig/spineengine_on_wht.svg" width="50%">
+  </picture>
+</p>
+
+## License
+
+Spine Engine is released under the GNU Lesser General Public License (LGPL) license. All accompanying
+documentation and manual are released under the [Creative Commons BY-SA 4.0 license](https://creativecommons.org/licenses/by-sa/4.0/).
+
+## Getting started
+
+### Installation
+
+To install Spine Engine into an existing Python environment, run
+
+    $ pip install spine_engine
+
+### Dependencies
+
+Spine Engine installation will install [dagster](https://dagster.readthedocs.io/en/master/index.html).
+
+&nbsp;
+<hr>
+<center>
+<table width=500px frame="none">
+<tr>
+<td valign="middle" width=100px>
+<img src=fig/eu-emblem-low-res.jpg alt="EU emblem" width=100%></td>
+<td valign="middle">This project has received funding from European Climate, Infrastructure and Environment Executive Agency under the European Union’s HORIZON Research and Innovation Actions under grant agreement N°101095998.</td>
+<tr>
+<td valign="middle" width=100px>
+<img src=fig/eu-emblem-low-res.jpg alt="EU emblem" width=100%></td>
+<td valign="middle">This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 774629.</td>
+</table>
+</center>
```

#### html2text {}

```diff
@@ -1,19 +1,19 @@
-Metadata-Version: 2.1 Name: spine_engine Version: 0.23.3 Summary: A package to
+Metadata-Version: 2.1 Name: spine_engine Version: 0.23.4 Summary: A package to
 run Spine workflows. Author-email: Spine Project consortium
 vtt.fi> License: LGPL-3.0-or-later Project-URL: Repository, https://github.com/
 spine-tools/spine-engine Keywords: energy system
 modelling,workflow,optimisation,database Classifier: Programming Language ::
 Python :: 3 Classifier: License :: OSI Approved :: GNU Lesser General Public
 License v3 (LGPLv3) Classifier: Operating System :: OS Independent Requires-
 Python: <3.12,>=3.8.1 Description-Content-Type: text/markdown License-File:
 COPYING License-File: COPYING.LESSER Requires-Dist: dagster<0.12.9,>=0.12.6
 Requires-Dist: pendulum<3.0.0 Requires-Dist: protobuf<3.21.0 Requires-Dist:
 networkx>2.5.1 Requires-Dist: datapackage<1.16,>=1.15.2 Requires-Dist:
-jupyter_client>=6.0 Requires-Dist: spinedb_api>=0.30.3 Requires-Dist:
+jupyter_client>=6.0 Requires-Dist: spinedb_api>=0.30.5 Requires-Dist:
 pyzmq>=21.0 Requires-Dist: markupsafe<2.1 Provides-Extra: dev Requires-Dist:
 coverage[toml]; extra == "dev" # Spine Engine [![Python](https://
 img.shields.io/badge/python-3.8%20|%203.9%20|%203.10%20|%203.11-blue.svg)]
 (https://www.python.org/downloads/release/python-379/) [![Unit tests](https://
 github.com/spine-tools/spine-engine/workflows/Unit%20tests/badge.svg)](https://
 github.com/spine-tools/spine-engine/actions?query=workflow%3A"Unit+tests") [!
 [codecov](https://codecov.io/gh/spine-tools/spine-engine/branch/master/graph/
```

### Comparing `spine_engine-0.23.3/README.md` & `spine_engine-0.23.4/README.md`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,47 +1,47 @@
-# Spine Engine
-
-[![Python](https://img.shields.io/badge/python-3.8%20|%203.9%20|%203.10%20|%203.11-blue.svg)](https://www.python.org/downloads/release/python-379/)
-[![Unit tests](https://github.com/spine-tools/spine-engine/workflows/Unit%20tests/badge.svg)](https://github.com/spine-tools/spine-engine/actions?query=workflow%3A"Unit+tests")
-[![codecov](https://codecov.io/gh/spine-tools/spine-engine/branch/master/graph/badge.svg)](https://codecov.io/gh/spine-tools/spine-engine)
-[![PyPI version](https://badge.fury.io/py/spine-engine.svg)](https://badge.fury.io/py/spine-engine)
-
-A Python package to coordinate the execution of [Spine Toolbox](https://github.com/spine-tools/Spine-Toolbox) workflows.
-
-<p align="center" width="100%">
-  <picture>
-    <source media="(prefers-color-scheme: dark)" srcset="./fig/spineengine_logo.svg" width="50%">
-    <img alt="Spine Engine" src="./fig/spineengine_on_wht.svg" width="50%">
-  </picture>
-</p>
-
-## License
-
-Spine Engine is released under the GNU Lesser General Public License (LGPL) license. All accompanying
-documentation and manual are released under the [Creative Commons BY-SA 4.0 license](https://creativecommons.org/licenses/by-sa/4.0/).
-
-## Getting started
-
-### Installation
-
-To install Spine Engine into an existing Python environment, run
-
-    $ pip install spine_engine
-
-### Dependencies
-
-Spine Engine installation will install [dagster](https://dagster.readthedocs.io/en/master/index.html).
-
-&nbsp;
-<hr>
-<center>
-<table width=500px frame="none">
-<tr>
-<td valign="middle" width=100px>
-<img src=fig/eu-emblem-low-res.jpg alt="EU emblem" width=100%></td>
-<td valign="middle">This project has received funding from European Climate, Infrastructure and Environment Executive Agency under the European Union’s HORIZON Research and Innovation Actions under grant agreement N°101095998.</td>
-<tr>
-<td valign="middle" width=100px>
-<img src=fig/eu-emblem-low-res.jpg alt="EU emblem" width=100%></td>
-<td valign="middle">This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 774629.</td>
-</table>
-</center>
+# Spine Engine
+
+[![Python](https://img.shields.io/badge/python-3.8%20|%203.9%20|%203.10%20|%203.11-blue.svg)](https://www.python.org/downloads/release/python-379/)
+[![Unit tests](https://github.com/spine-tools/spine-engine/workflows/Unit%20tests/badge.svg)](https://github.com/spine-tools/spine-engine/actions?query=workflow%3A"Unit+tests")
+[![codecov](https://codecov.io/gh/spine-tools/spine-engine/branch/master/graph/badge.svg)](https://codecov.io/gh/spine-tools/spine-engine)
+[![PyPI version](https://badge.fury.io/py/spine-engine.svg)](https://badge.fury.io/py/spine-engine)
+
+A Python package to coordinate the execution of [Spine Toolbox](https://github.com/spine-tools/Spine-Toolbox) workflows.
+
+<p align="center" width="100%">
+  <picture>
+    <source media="(prefers-color-scheme: dark)" srcset="./fig/spineengine_logo.svg" width="50%">
+    <img alt="Spine Engine" src="./fig/spineengine_on_wht.svg" width="50%">
+  </picture>
+</p>
+
+## License
+
+Spine Engine is released under the GNU Lesser General Public License (LGPL) license. All accompanying
+documentation and manual are released under the [Creative Commons BY-SA 4.0 license](https://creativecommons.org/licenses/by-sa/4.0/).
+
+## Getting started
+
+### Installation
+
+To install Spine Engine into an existing Python environment, run
+
+    $ pip install spine_engine
+
+### Dependencies
+
+Spine Engine installation will install [dagster](https://dagster.readthedocs.io/en/master/index.html).
+
+&nbsp;
+<hr>
+<center>
+<table width=500px frame="none">
+<tr>
+<td valign="middle" width=100px>
+<img src=fig/eu-emblem-low-res.jpg alt="EU emblem" width=100%></td>
+<td valign="middle">This project has received funding from European Climate, Infrastructure and Environment Executive Agency under the European Union’s HORIZON Research and Innovation Actions under grant agreement N°101095998.</td>
+<tr>
+<td valign="middle" width=100px>
+<img src=fig/eu-emblem-low-res.jpg alt="EU emblem" width=100%></td>
+<td valign="middle">This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 774629.</td>
+</table>
+</center>
```

### Comparing `spine_engine-0.23.3/bin/update_copyrights.py` & `spine_engine-0.23.4/bin/update_copyrights.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,36 +1,36 @@
-#!/usr/bin/env python
-
-from pathlib import Path
-import time
-
-
-current_year = time.gmtime().tm_year
-root_dir = Path(__file__).parent.parent
-project_source_dir = Path(root_dir, "spine_engine")
-test_source_dir = Path(root_dir, "tests")
-
-expected = f"# Copyright (C) 2017-{current_year} Spine project consortium"
-
-
-def update_copyrights(path, suffix, recursive=True):
-    for path in path.iterdir():
-        if path.suffix == suffix:
-            i = 0
-            with open(path) as python_file:
-                lines = python_file.readlines()
-                for i, line in enumerate(lines[1:4]):
-                    if line.startswith("# Copyright (C) "):
-                        lines[i + 1] = lines[i + 1][:21] + str(current_year) + lines[i + 1][25:]
-                        break
-            if len(lines) <= i + 1 or not lines[i + 1].startswith(expected):
-                print(f"Confusing or no copyright: {path}")
-            else:
-                with open(path, "w") as python_file:
-                    python_file.writelines(lines)
-        elif recursive and path.is_dir():
-            update_copyrights(path, suffix)
-
-
-update_copyrights(root_dir, ".py", recursive=False)
-update_copyrights(project_source_dir, ".py")
-update_copyrights(test_source_dir, ".py")
+#!/usr/bin/env python
+
+from pathlib import Path
+import time
+
+
+current_year = time.gmtime().tm_year
+root_dir = Path(__file__).parent.parent
+project_source_dir = Path(root_dir, "spine_engine")
+test_source_dir = Path(root_dir, "tests")
+
+expected = f"# Copyright (C) 2017-{current_year} Spine project consortium"
+
+
+def update_copyrights(path, suffix, recursive=True):
+    for path in path.iterdir():
+        if path.suffix == suffix:
+            i = 0
+            with open(path) as python_file:
+                lines = python_file.readlines()
+                for i, line in enumerate(lines[1:4]):
+                    if line.startswith("# Copyright (C) "):
+                        lines[i + 1] = lines[i + 1][:21] + str(current_year) + lines[i + 1][25:]
+                        break
+            if len(lines) <= i + 1 or not lines[i + 1].startswith(expected):
+                print(f"Confusing or no copyright: {path}")
+            else:
+                with open(path, "w") as python_file:
+                    python_file.writelines(lines)
+        elif recursive and path.is_dir():
+            update_copyrights(path, suffix)
+
+
+update_copyrights(root_dir, ".py", recursive=False)
+update_copyrights(project_source_dir, ".py")
+update_copyrights(test_source_dir, ".py")
```

### Comparing `spine_engine-0.23.3/docs/Makefile` & `spine_engine-0.23.4/docs/Makefile`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,20 +1,20 @@
-# Minimal makefile for Sphinx documentation
-#
-
-# You can set these variables from the command line.
-SPHINXOPTS    =
-SPHINXBUILD   = sphinx-build
-SPHINXPROJ    = SpineToolbox
-SOURCEDIR     = source
-BUILDDIR      = build
-
-# Put it first so that "make" without argument is like "make help".
-help:
-	@$(SPHINXBUILD) -M help "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS) $(O)
-
-.PHONY: help Makefile
-
-# Catch-all target: route all unknown targets to Sphinx using the new
-# "make mode" option.  $(O) is meant as a shortcut for $(SPHINXOPTS).
-%: Makefile
+# Minimal makefile for Sphinx documentation
+#
+
+# You can set these variables from the command line.
+SPHINXOPTS    =
+SPHINXBUILD   = sphinx-build
+SPHINXPROJ    = SpineToolbox
+SOURCEDIR     = source
+BUILDDIR      = build
+
+# Put it first so that "make" without argument is like "make help".
+help:
+	@$(SPHINXBUILD) -M help "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS) $(O)
+
+.PHONY: help Makefile
+
+# Catch-all target: route all unknown targets to Sphinx using the new
+# "make mode" option.  $(O) is meant as a shortcut for $(SPHINXOPTS).
+%: Makefile
 	@$(SPHINXBUILD) -M $@ "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS) $(O)
```

### Comparing `spine_engine-0.23.3/docs/make.bat` & `spine_engine-0.23.4/docs/make.bat`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,36 +1,36 @@
-@ECHO OFF
-
-pushd %~dp0
-
-REM Command file for Sphinx documentation
-
-if "%SPHINXBUILD%" == "" (
-	set SPHINXBUILD=sphinx-build
-)
-set SOURCEDIR=source
-set BUILDDIR=build
-set SPHINXPROJ=SpineToolbox
-
-if "%1" == "" goto help
-
-%SPHINXBUILD% >NUL 2>NUL
-if errorlevel 9009 (
-	echo.
-	echo.The 'sphinx-build' command was not found. Make sure you have Sphinx
-	echo.installed, then set the SPHINXBUILD environment variable to point
-	echo.to the full path of the 'sphinx-build' executable. Alternatively you
-	echo.may add the Sphinx directory to PATH.
-	echo.
-	echo.If you don't have Sphinx installed, grab it from
-	echo.http://sphinx-doc.org/
-	exit /b 1
-)
-
-%SPHINXBUILD% -M %1 %SOURCEDIR% %BUILDDIR% %SPHINXOPTS%
-goto end
-
-:help
-%SPHINXBUILD% -M help %SOURCEDIR% %BUILDDIR% %SPHINXOPTS%
-
-:end
-popd
+@ECHO OFF
+
+pushd %~dp0
+
+REM Command file for Sphinx documentation
+
+if "%SPHINXBUILD%" == "" (
+	set SPHINXBUILD=sphinx-build
+)
+set SOURCEDIR=source
+set BUILDDIR=build
+set SPHINXPROJ=SpineToolbox
+
+if "%1" == "" goto help
+
+%SPHINXBUILD% >NUL 2>NUL
+if errorlevel 9009 (
+	echo.
+	echo.The 'sphinx-build' command was not found. Make sure you have Sphinx
+	echo.installed, then set the SPHINXBUILD environment variable to point
+	echo.to the full path of the 'sphinx-build' executable. Alternatively you
+	echo.may add the Sphinx directory to PATH.
+	echo.
+	echo.If you don't have Sphinx installed, grab it from
+	echo.http://sphinx-doc.org/
+	exit /b 1
+)
+
+%SPHINXBUILD% -M %1 %SOURCEDIR% %BUILDDIR% %SPHINXOPTS%
+goto end
+
+:help
+%SPHINXBUILD% -M help %SOURCEDIR% %BUILDDIR% %SPHINXOPTS%
+
+:end
+popd
```

### Comparing `spine_engine-0.23.3/docs/source/autoapi/spine_engine/execution_managers/conda_kernel_spec_manager/index.rst` & `spine_engine-0.23.4/docs/source/autoapi/spine_engine/execution_managers/conda_kernel_spec_manager/index.rst`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,164 +1,164 @@
-:py:mod:`spine_engine.execution_managers.conda_kernel_spec_manager`
-===================================================================
-
-.. py:module:: spine_engine.execution_managers.conda_kernel_spec_manager
-
-
-Module Contents
----------------
-
-Classes
-~~~~~~~
-
-.. autoapisummary::
-
-   spine_engine.execution_managers.conda_kernel_spec_manager.CondaKernelSpecManager
-
-
-
-
-Attributes
-~~~~~~~~~~
-
-.. autoapisummary::
-
-   spine_engine.execution_managers.conda_kernel_spec_manager.CACHE_TIMEOUT
-   spine_engine.execution_managers.conda_kernel_spec_manager.RUNNER_COMMAND
-
-
-.. py:data:: CACHE_TIMEOUT
-   :annotation: = 60
-
-   
-
-.. py:data:: RUNNER_COMMAND
-   :annotation: = ['python', '-m', 'spine_engine.execution_managers.conda_kernel_spec_runner']
-
-   
-
-.. py:class:: CondaKernelSpecManager(**kwargs)
-
-   Bases: :py:obj:`jupyter_client.kernelspec.KernelSpecManager`
-
-   A custom KernelSpecManager able to search for conda environments and
-   create kernelspecs for them.
-
-   Create a configurable given a config config.
-
-   :param config: If this is empty, default values are used. If config is a
-                  :class:`Config` instance, it will be used to configure the
-                  instance.
-   :type config: Config
-   :param parent: The parent Configurable instance of this object.
-   :type parent: Configurable instance, optional
-
-   .. rubric:: Notes
-
-   Subclasses of Configurable must call the :meth:`__init__` method of
-   :class:`Configurable` *before* doing anything else and using
-   :func:`super`::
-
-       class MyConfigurable(Configurable):
-           def __init__(self, config=None):
-               super(MyConfigurable, self).__init__(config=config)
-               # Then any other code you need to finish initialization.
-
-   This ensures that instances will be configured properly.
-
-   .. py:attribute:: conda_only
-      
-
-      
-
-   .. py:attribute:: env_filter
-      
-
-      
-
-   .. py:attribute:: kernelspec_path
-      
-
-      
-
-   .. py:attribute:: name_format
-      
-
-      
-
-   .. py:method:: _validate_kernelspec_path(self, proposal)
-
-
-   .. py:method:: clean_kernel_name(kname)
-      :staticmethod:
-
-      Replaces invalid characters in the Jupyter kernelname, with
-      a bit of effort to preserve readability.
-
-
-   .. py:method:: _conda_info(self)
-      :property:
-
-      Get and parse the whole conda information output
-
-      Caches the information for CACHE_TIMEOUT seconds, as this is
-      relatively expensive.
-
-
-   .. py:method:: _all_envs(self)
-
-      Find all of the environments we should be checking. We skip
-      environments in the conda-bld directory as well as environments
-      that match our env_filter regex. Returns a dict with canonical
-      environment names as keys, and full paths as values.
-
-
-   .. py:method:: _all_specs(self)
-
-      Find the all kernel specs in all environments.
-
-      Returns a dict with unique env names as keys, and the kernel.json
-      content as values, modified so that they can be run properly in
-      their native environments.
-
-      Caches the information for CACHE_TIMEOUT seconds, as this is
-      relatively expensive.
-
-
-   .. py:method:: _conda_kspecs(self)
-      :property:
-
-      Get (or refresh) the cache of conda kernels
-
-
-
-   .. py:method:: find_kernel_specs(self)
-
-      Returns a dict mapping kernel names to resource directories.
-
-      The update process also adds the resource dir for the conda
-      environments.
-
-
-   .. py:method:: get_kernel_spec(self, kernel_name)
-
-      Returns a :class:`KernelSpec` instance for the given kernel_name.
-
-      Additionally, conda kernelspecs are generated on the fly
-      accordingly with the detected environments.
-
-
-   .. py:method:: get_all_specs(self)
-
-      Returns a dict mapping kernel names to dictionaries with two
-      entries: "resource_dir" and "spec". This was added to fill out
-      the full public interface to KernelManagerSpec.
-
-
-   .. py:method:: remove_kernel_spec(self, name)
-
-      Remove a kernel spec directory by name.
-
-      Returns the path that was deleted.
-
-
-
+:py:mod:`spine_engine.execution_managers.conda_kernel_spec_manager`
+===================================================================
+
+.. py:module:: spine_engine.execution_managers.conda_kernel_spec_manager
+
+
+Module Contents
+---------------
+
+Classes
+~~~~~~~
+
+.. autoapisummary::
+
+   spine_engine.execution_managers.conda_kernel_spec_manager.CondaKernelSpecManager
+
+
+
+
+Attributes
+~~~~~~~~~~
+
+.. autoapisummary::
+
+   spine_engine.execution_managers.conda_kernel_spec_manager.CACHE_TIMEOUT
+   spine_engine.execution_managers.conda_kernel_spec_manager.RUNNER_COMMAND
+
+
+.. py:data:: CACHE_TIMEOUT
+   :annotation: = 60
+
+   
+
+.. py:data:: RUNNER_COMMAND
+   :annotation: = ['python', '-m', 'spine_engine.execution_managers.conda_kernel_spec_runner']
+
+   
+
+.. py:class:: CondaKernelSpecManager(**kwargs)
+
+   Bases: :py:obj:`jupyter_client.kernelspec.KernelSpecManager`
+
+   A custom KernelSpecManager able to search for conda environments and
+   create kernelspecs for them.
+
+   Create a configurable given a config config.
+
+   :param config: If this is empty, default values are used. If config is a
+                  :class:`Config` instance, it will be used to configure the
+                  instance.
+   :type config: Config
+   :param parent: The parent Configurable instance of this object.
+   :type parent: Configurable instance, optional
+
+   .. rubric:: Notes
+
+   Subclasses of Configurable must call the :meth:`__init__` method of
+   :class:`Configurable` *before* doing anything else and using
+   :func:`super`::
+
+       class MyConfigurable(Configurable):
+           def __init__(self, config=None):
+               super(MyConfigurable, self).__init__(config=config)
+               # Then any other code you need to finish initialization.
+
+   This ensures that instances will be configured properly.
+
+   .. py:attribute:: conda_only
+      
+
+      
+
+   .. py:attribute:: env_filter
+      
+
+      
+
+   .. py:attribute:: kernelspec_path
+      
+
+      
+
+   .. py:attribute:: name_format
+      
+
+      
+
+   .. py:method:: _validate_kernelspec_path(self, proposal)
+
+
+   .. py:method:: clean_kernel_name(kname)
+      :staticmethod:
+
+      Replaces invalid characters in the Jupyter kernelname, with
+      a bit of effort to preserve readability.
+
+
+   .. py:method:: _conda_info(self)
+      :property:
+
+      Get and parse the whole conda information output
+
+      Caches the information for CACHE_TIMEOUT seconds, as this is
+      relatively expensive.
+
+
+   .. py:method:: _all_envs(self)
+
+      Find all of the environments we should be checking. We skip
+      environments in the conda-bld directory as well as environments
+      that match our env_filter regex. Returns a dict with canonical
+      environment names as keys, and full paths as values.
+
+
+   .. py:method:: _all_specs(self)
+
+      Find the all kernel specs in all environments.
+
+      Returns a dict with unique env names as keys, and the kernel.json
+      content as values, modified so that they can be run properly in
+      their native environments.
+
+      Caches the information for CACHE_TIMEOUT seconds, as this is
+      relatively expensive.
+
+
+   .. py:method:: _conda_kspecs(self)
+      :property:
+
+      Get (or refresh) the cache of conda kernels
+
+
+
+   .. py:method:: find_kernel_specs(self)
+
+      Returns a dict mapping kernel names to resource directories.
+
+      The update process also adds the resource dir for the conda
+      environments.
+
+
+   .. py:method:: get_kernel_spec(self, kernel_name)
+
+      Returns a :class:`KernelSpec` instance for the given kernel_name.
+
+      Additionally, conda kernelspecs are generated on the fly
+      accordingly with the detected environments.
+
+
+   .. py:method:: get_all_specs(self)
+
+      Returns a dict mapping kernel names to dictionaries with two
+      entries: "resource_dir" and "spec". This was added to fill out
+      the full public interface to KernelManagerSpec.
+
+
+   .. py:method:: remove_kernel_spec(self, name)
+
+      Remove a kernel spec directory by name.
+
+      Returns the path that was deleted.
+
+
+
```

### Comparing `spine_engine-0.23.3/docs/source/autoapi/spine_engine/execution_managers/execution_manager_base/index.rst` & `spine_engine-0.23.4/docs/source/autoapi/spine_engine/execution_managers/execution_manager_base/index.rst`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,52 +1,52 @@
-:py:mod:`spine_engine.execution_managers.execution_manager_base`
-================================================================
-
-.. py:module:: spine_engine.execution_managers.execution_manager_base
-
-.. autoapi-nested-parse::
-
-   Contains the ExecutionManagerBase class.
-
-   :authors: M. Marin (KTH)
-   :date:   12.10.2020
-
-
-
-Module Contents
----------------
-
-Classes
-~~~~~~~
-
-.. autoapisummary::
-
-   spine_engine.execution_managers.execution_manager_base.ExecutionManagerBase
-
-
-
-
-.. py:class:: ExecutionManagerBase(logger)
-
-   Base class for all tool instance execution managers.
-
-   Class constructor.
-
-   :param logger: a logger instance
-   :type logger: LoggerInterface
-
-   .. py:method:: run_until_complete(self)
-      :abstractmethod:
-
-      Runs until completion.
-
-      :returns: return code
-      :rtype: int
-
-
-   .. py:method:: stop_execution(self)
-      :abstractmethod:
-
-      Stops execution gracefully.
-
-
-
+:py:mod:`spine_engine.execution_managers.execution_manager_base`
+================================================================
+
+.. py:module:: spine_engine.execution_managers.execution_manager_base
+
+.. autoapi-nested-parse::
+
+   Contains the ExecutionManagerBase class.
+
+   :authors: M. Marin (KTH)
+   :date:   12.10.2020
+
+
+
+Module Contents
+---------------
+
+Classes
+~~~~~~~
+
+.. autoapisummary::
+
+   spine_engine.execution_managers.execution_manager_base.ExecutionManagerBase
+
+
+
+
+.. py:class:: ExecutionManagerBase(logger)
+
+   Base class for all tool instance execution managers.
+
+   Class constructor.
+
+   :param logger: a logger instance
+   :type logger: LoggerInterface
+
+   .. py:method:: run_until_complete(self)
+      :abstractmethod:
+
+      Runs until completion.
+
+      :returns: return code
+      :rtype: int
+
+
+   .. py:method:: stop_execution(self)
+      :abstractmethod:
+
+      Stops execution gracefully.
+
+
+
```

### Comparing `spine_engine-0.23.3/docs/source/autoapi/spine_engine/execution_managers/process_execution_manager/index.rst` & `spine_engine-0.23.4/docs/source/autoapi/spine_engine/execution_managers/process_execution_manager/index.rst`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,62 +1,62 @@
-:py:mod:`spine_engine.execution_managers.process_execution_manager`
-===================================================================
-
-.. py:module:: spine_engine.execution_managers.process_execution_manager
-
-.. autoapi-nested-parse::
-
-   Contains the ProcessExecutionManager class.
-
-   :authors: M. Marin (KTH)
-   :date:   12.10.2020
-
-
-
-Module Contents
----------------
-
-Classes
-~~~~~~~
-
-.. autoapisummary::
-
-   spine_engine.execution_managers.process_execution_manager.ProcessExecutionManager
-
-
-
-
-.. py:class:: ProcessExecutionManager(logger, program, *args, workdir=None)
-
-   Bases: :py:obj:`spine_engine.execution_managers.execution_manager_base.ExecutionManagerBase`
-
-   Base class for all tool instance execution managers.
-
-   Class constructor.
-
-   :param logger: a logger instance
-   :type logger: LoggerInterface
-   :param program: Path to program to run in the subprocess (e.g. julia.exe)
-   :type program: str
-   :param args: List of argument for the program (e.g. path to script file)
-   :type args: list
-
-   .. py:method:: run_until_complete(self)
-
-      Runs until completion.
-
-      :returns: return code
-      :rtype: int
-
-
-   .. py:method:: stop_execution(self)
-
-      Stops execution gracefully.
-
-
-   .. py:method:: _log_stdout(self, stdout)
-
-
-   .. py:method:: _log_stderr(self, stderr)
-
-
-
+:py:mod:`spine_engine.execution_managers.process_execution_manager`
+===================================================================
+
+.. py:module:: spine_engine.execution_managers.process_execution_manager
+
+.. autoapi-nested-parse::
+
+   Contains the ProcessExecutionManager class.
+
+   :authors: M. Marin (KTH)
+   :date:   12.10.2020
+
+
+
+Module Contents
+---------------
+
+Classes
+~~~~~~~
+
+.. autoapisummary::
+
+   spine_engine.execution_managers.process_execution_manager.ProcessExecutionManager
+
+
+
+
+.. py:class:: ProcessExecutionManager(logger, program, *args, workdir=None)
+
+   Bases: :py:obj:`spine_engine.execution_managers.execution_manager_base.ExecutionManagerBase`
+
+   Base class for all tool instance execution managers.
+
+   Class constructor.
+
+   :param logger: a logger instance
+   :type logger: LoggerInterface
+   :param program: Path to program to run in the subprocess (e.g. julia.exe)
+   :type program: str
+   :param args: List of argument for the program (e.g. path to script file)
+   :type args: list
+
+   .. py:method:: run_until_complete(self)
+
+      Runs until completion.
+
+      :returns: return code
+      :rtype: int
+
+
+   .. py:method:: stop_execution(self)
+
+      Stops execution gracefully.
+
+
+   .. py:method:: _log_stdout(self, stdout)
+
+
+   .. py:method:: _log_stderr(self, stderr)
+
+
+
```

### Comparing `spine_engine-0.23.3/docs/source/autoapi/spine_engine/execution_managers/spine_repl/index.rst` & `spine_engine-0.23.4/docs/source/autoapi/spine_engine/execution_managers/spine_repl/index.rst`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,104 +1,104 @@
-:py:mod:`spine_engine.execution_managers.spine_repl`
-====================================================
-
-.. py:module:: spine_engine.execution_managers.spine_repl
-
-
-Module Contents
----------------
-
-Classes
-~~~~~~~
-
-.. autoapisummary::
-
-   spine_engine.execution_managers.spine_repl.SpineDBServer
-   spine_engine.execution_managers.spine_repl._RequestHandler
-
-
-
-Functions
-~~~~~~~~~
-
-.. autoapisummary::
-
-   spine_engine.execution_managers.spine_repl.completions
-   spine_engine.execution_managers.spine_repl.add_history
-   spine_engine.execution_managers.spine_repl.history_item
-   spine_engine.execution_managers.spine_repl.is_complete
-   spine_engine.execution_managers.spine_repl.start_server
-   spine_engine.execution_managers.spine_repl.send_sentinel
-
-
-
-Attributes
-~~~~~~~~~~
-
-.. autoapisummary::
-
-   spine_engine.execution_managers.spine_repl.readline
-
-
-.. py:data:: readline
-   
-
-   
-
-.. py:class:: SpineDBServer(server_address, RequestHandlerClass, bind_and_activate=True)
-
-   Bases: :py:obj:`socketserver.ThreadingMixIn`, :py:obj:`socketserver.TCPServer`
-
-   Mix-in class to handle each request in a new thread.
-
-   Constructor.  May be extended, do not override.
-
-   .. py:attribute:: allow_reuse_address
-      :annotation: = True
-
-      
-
-
-.. py:class:: _RequestHandler(request, client_address, server)
-
-   Bases: :py:obj:`socketserver.BaseRequestHandler`
-
-   Base class for request handler classes.
-
-   This class is instantiated for each request to be handled.  The
-   constructor sets the instance variables request, client_address
-   and server, and then calls the handle() method.  To implement a
-   specific service, all you need to do is to derive a class which
-   defines a handle() method.
-
-   The handle() method can find the request as self.request, the
-   client address as self.client_address, and the server (in case it
-   needs access to per-server information) as self.server.  Since a
-   separate instance is created for each request, the handle() method
-   can define other arbitrary instance variables.
-
-
-   .. py:method:: handle(self)
-
-
-
-.. py:function:: completions(text)
-
-
-.. py:function:: add_history(line)
-
-
-.. py:function:: history_item(index)
-
-
-.. py:function:: is_complete(cmd)
-
-
-.. py:function:: start_server(address)
-
-   :param address: Server address
-   :type address: tuple(str,int)
-
-
-.. py:function:: send_sentinel(host, port)
-
-
+:py:mod:`spine_engine.execution_managers.spine_repl`
+====================================================
+
+.. py:module:: spine_engine.execution_managers.spine_repl
+
+
+Module Contents
+---------------
+
+Classes
+~~~~~~~
+
+.. autoapisummary::
+
+   spine_engine.execution_managers.spine_repl.SpineDBServer
+   spine_engine.execution_managers.spine_repl._RequestHandler
+
+
+
+Functions
+~~~~~~~~~
+
+.. autoapisummary::
+
+   spine_engine.execution_managers.spine_repl.completions
+   spine_engine.execution_managers.spine_repl.add_history
+   spine_engine.execution_managers.spine_repl.history_item
+   spine_engine.execution_managers.spine_repl.is_complete
+   spine_engine.execution_managers.spine_repl.start_server
+   spine_engine.execution_managers.spine_repl.send_sentinel
+
+
+
+Attributes
+~~~~~~~~~~
+
+.. autoapisummary::
+
+   spine_engine.execution_managers.spine_repl.readline
+
+
+.. py:data:: readline
+   
+
+   
+
+.. py:class:: SpineDBServer(server_address, RequestHandlerClass, bind_and_activate=True)
+
+   Bases: :py:obj:`socketserver.ThreadingMixIn`, :py:obj:`socketserver.TCPServer`
+
+   Mix-in class to handle each request in a new thread.
+
+   Constructor.  May be extended, do not override.
+
+   .. py:attribute:: allow_reuse_address
+      :annotation: = True
+
+      
+
+
+.. py:class:: _RequestHandler(request, client_address, server)
+
+   Bases: :py:obj:`socketserver.BaseRequestHandler`
+
+   Base class for request handler classes.
+
+   This class is instantiated for each request to be handled.  The
+   constructor sets the instance variables request, client_address
+   and server, and then calls the handle() method.  To implement a
+   specific service, all you need to do is to derive a class which
+   defines a handle() method.
+
+   The handle() method can find the request as self.request, the
+   client address as self.client_address, and the server (in case it
+   needs access to per-server information) as self.server.  Since a
+   separate instance is created for each request, the handle() method
+   can define other arbitrary instance variables.
+
+
+   .. py:method:: handle(self)
+
+
+
+.. py:function:: completions(text)
+
+
+.. py:function:: add_history(line)
+
+
+.. py:function:: history_item(index)
+
+
+.. py:function:: is_complete(cmd)
+
+
+.. py:function:: start_server(address)
+
+   :param address: Server address
+   :type address: tuple(str,int)
+
+
+.. py:function:: send_sentinel(host, port)
+
+
```

### Comparing `spine_engine-0.23.3/docs/source/autoapi/spine_engine/index.rst` & `spine_engine-0.23.4/docs/source/autoapi/spine_engine/spine_engine/index.rst`

 * *Files 25% similar despite different names*

```diff
@@ -1,394 +1,395 @@
-:py:mod:`spine_engine`
-======================
-
-.. py:module:: spine_engine
-
-
-Subpackages
------------
-.. toctree::
-   :titlesonly:
-   :maxdepth: 3
-
-   execution_managers/index.rst
-   multithread_executor/index.rst
-   project_item/index.rst
-   utils/index.rst
-
-
-Submodules
-----------
-.. toctree::
-   :titlesonly:
-   :maxdepth: 1
-
-   config/index.rst
-   load_project_items/index.rst
-   project_item_loader/index.rst
-   spine_engine/index.rst
-   spine_engine_server/index.rst
-   version/index.rst
-
-
-Package Contents
-----------------
-
-Classes
-~~~~~~~
-
-.. autoapisummary::
-
-   spine_engine.ExecutionDirection
-   spine_engine.SpineEngine
-   spine_engine.SpineEngineState
-   spine_engine.ItemExecutionFinishState
-
-
-
-
-Attributes
-~~~~~~~~~~
-
-.. autoapisummary::
-
-   spine_engine.__version__
-
-
-.. py:class:: ExecutionDirection
-
-   Bases: :py:obj:`enum.Enum`
-
-   Generic enumeration.
-
-   Derive from this class to define new enumerations.
-
-   .. py:attribute:: FORWARD
-      
-
-      
-
-   .. py:attribute:: BACKWARD
-      
-
-      
-
-   .. py:method:: __str__(self)
-
-      Return str(self).
-
-
-
-.. py:class:: SpineEngine(items=None, specifications=None, connections=None, items_module_name='spine_items', settings=None, project_dir=None, execution_permits=None, node_successors=None, debug=False)
-
-   An engine for executing a Spine Toolbox DAG-workflow.
-
-   :param items: List of executable item dicts.
-   :type items: list(dict)
-   :param specifications: A mapping from item type to list of specification dicts.
-   :type specifications: dict(str,list(dict))
-   :param connections: List of connection dicts
-   :type connections: list of dict
-   :param items_module_name: name of the Python module that contains project items
-   :type items_module_name: str
-   :param settings: Toolbox execution settings.
-   :type settings: dict
-   :param project_dir: Path to project directory.
-   :type project_dir: str
-   :param execution_permits: A mapping from item name to a boolean value, False indicating that
-                             the item is not executed, only its resources are collected.
-   :type execution_permits: dict(str,bool)
-   :param node_successors: A mapping from item name to list of successor item names, dictating the dependencies.
-   :type node_successors: dict(str,list(str))
-   :param debug: Whether debug mode is active or not.
-   :type debug: bool
-
-   .. py:method:: _make_item_specifications(self, specifications, project_item_loader, items_module_name)
-
-      Instantiates item specifications.
-
-      :param specifications: A mapping from item type to list of specification dicts.
-      :type specifications: dict
-      :param project_item_loader: loader instance
-      :type project_item_loader: ProjectItemLoader
-      :param items_module_name: name of the Python module that contains the project items
-      :type items_module_name: str
-
-      :returns: mapping from item type to a dict that maps specification names to specification instances
-      :rtype: dict
-
-
-   .. py:method:: _make_item(self, item_name, direction)
-
-      Recreates item from project item dictionary. Note that all items are created twice.
-      One for the backward pipeline, the other one for the forward pipeline.
-
-
-   .. py:method:: get_event(self)
-
-      Returns the next event in the stream. Calling this after receiving the event of type "dag_exec_finished"
-      will raise StopIterationError.
-
-
-   .. py:method:: state(self)
-
-
-   .. py:method:: _get_event_stream(self)
-
-      Returns an iterator of tuples (event_type, event_data).
-
-      TODO: Describe the events in depth.
-
-
-   .. py:method:: answer_prompt(self, item_name, accepted)
-
-
-   .. py:method:: run(self)
-
-      Runs this engine.
-
-
-   .. py:method:: _process_event(self, event)
-
-      Processes events from a pipeline.
-
-      :param event: an event
-      :type event: DagsterEvent
-
-
-   .. py:method:: stop(self)
-
-      Stops the engine.
-
-
-   .. py:method:: _stop_item(self, item)
-
-
-   .. py:method:: _make_pipeline(self)
-
-      Returns a PipelineDefinition for executing this engine.
-
-      :returns: PipelineDefinition
-
-
-   .. py:method:: _make_backward_solid_def(self, item_name)
-
-      Returns a SolidDefinition for executing the given item in the backward sweep.
-
-      :param item_name: The project item that gets executed by the solid.
-      :type item_name: str
-
-
-   .. py:method:: _make_forward_solid_def(self, item_name)
-
-      Returns a SolidDefinition for executing the given item.
-
-      :param item_name:
-      :type item_name: str
-
-      :returns: SolidDefinition
-
-
-   .. py:method:: _execute_item(self, context, item_name, forward_resource_stacks, backward_resources)
-
-      Executes the given item using the given forward resource stacks and backward resources.
-      Returns list of output resource stacks.
-
-      Called by ``_make_forward_solid_def.compute_fn``.
-
-      For each element yielded by ``_filtered_resources_iterator``, spawns a thread that runs ``_execute_item_filtered``.
-
-      :param context:
-      :param item_name:
-      :type item_name: str
-      :param forward_resource_stacks:
-      :type forward_resource_stacks: list(tuple(ProjectItemResource))
-      :param backward_resources:
-      :type backward_resources: list(ProjectItemResource)
-
-      :returns: list(tuple(ProjectItemResource))
-
-
-   .. py:method:: _execute_item_filtered(self, item, filtered_forward_resources, filtered_backward_resources, output_resources_list, success)
-
-      Executes the given item using the given filtered resources. Target for threads in ``_execute_item``.
-
-      :param item:
-      :type item: ExecutableItemBase
-      :param filtered_forward_resources:
-      :type filtered_forward_resources: list(ProjectItemResource)
-      :param filtered_backward_resources:
-      :type filtered_backward_resources: list(ProjectItemResource)
-      :param output_resources_list: A list of lists, to append the
-                                    output resources generated by the item.
-      :type output_resources_list: list(list(ProjectItemResource))
-      :param success: A list of one element, to write the outcome of the execution.
-      :type success: list
-
-
-   .. py:method:: _filtered_resources_iterator(self, item_name, forward_resource_stacks, backward_resources, timestamp)
-
-      Yields tuples of (filtered forward resources, filtered backward resources, filter id).
-
-      Each tuple corresponds to a unique filter combination. Combinations are obtained by applying the cross-product
-      over forward resource stacks as yielded by ``_forward_resource_stacks_iterator``.
-
-      :param item_name:
-      :type item_name: str
-      :param forward_resource_stacks:
-      :type forward_resource_stacks: list(tuple(ProjectItemResource))
-      :param backward_resources:
-      :type backward_resources: list(ProjectItemResource)
-      :param timestamp: timestamp for the execution filter
-      :type timestamp: str
-
-      :returns: forward resources, backward resources, filter id
-      :rtype: Iterator(tuple(list,list,str))
-
-
-   .. py:method:: _expand_resource_stack(self, item_name, resource_stack)
-
-      Expands a resource stack if possible.
-
-      If the stack has more than one resource, returns the unaltered stack.
-
-      Otherwise, if the stack has only one resource but there are no filters defined for that resource,
-      again, returns the unaltered stack.
-
-      Otherwise, returns an expanded stack of as many resources as filter stacks defined for the only one resource.
-      Each resource in the expanded stack is a clone of the original, with one of the filter stacks
-      applied to the URL.
-
-      :param item_name: resource receiving item's name
-      :type item_name: str
-      :param resource_stack:
-      :type resource_stack: tuple(ProjectItemResource)
-
-      :returns: tuple(ProjectItemResource)
-
-
-   .. py:method:: _filter_stacks(self, item_name, resource_label)
-
-      Computes filter stacks.
-
-      Stacks are computed as the cross-product of all individual filters defined for a resource.
-
-      :param item_name: item's name
-      :type item_name: str
-      :param resource_label: resource's label
-      :type resource_label: str
-
-      :returns: filter stacks
-      :rtype: list of list
-
-
-   .. py:method:: _convert_forward_resources(self, item_name, resources)
-
-      Converts resources as they're being forwarded to given item.
-      The conversion is dictated by the connection the resources traverse in order to reach the item.
-
-      :param item_name: receiving item's name
-      :type item_name: str
-      :param resources: resources to convert
-      :type resources: list of ProjectItemResource
-
-      :returns: converted resources
-      :rtype: list of ProjectItemResource
-
-
-   .. py:method:: _make_dependencies(self)
-
-      Returns a dictionary of dependencies according to the given dictionaries of injectors.
-
-      :returns: a dictionary to pass to the PipelineDefinition constructor as dependencies
-      :rtype: dict
-
-
-
-.. py:class:: SpineEngineState
-
-   Bases: :py:obj:`enum.Enum`
-
-   Generic enumeration.
-
-   Derive from this class to define new enumerations.
-
-   .. py:attribute:: SLEEPING
-      :annotation: = 1
-
-      
-
-   .. py:attribute:: RUNNING
-      :annotation: = 2
-
-      
-
-   .. py:attribute:: USER_STOPPED
-      :annotation: = 3
-
-      
-
-   .. py:attribute:: FAILED
-      :annotation: = 4
-
-      
-
-   .. py:attribute:: COMPLETED
-      :annotation: = 5
-
-      
-
-   .. py:method:: __str__(self)
-
-      Return str(self).
-
-
-
-.. py:class:: ItemExecutionFinishState
-
-   Bases: :py:obj:`enum.Enum`
-
-   Generic enumeration.
-
-   Derive from this class to define new enumerations.
-
-   .. py:attribute:: SUCCESS
-      :annotation: = 1
-
-      
-
-   .. py:attribute:: FAILURE
-      :annotation: = 2
-
-      
-
-   .. py:attribute:: SKIPPED
-      :annotation: = 3
-
-      
-
-   .. py:attribute:: EXCLUDED
-      :annotation: = 4
-
-      
-
-   .. py:attribute:: STOPPED
-      :annotation: = 5
-
-      
-
-   .. py:attribute:: NEVER_FINISHED
-      :annotation: = 6
-
-      
-
-   .. py:method:: __str__(self)
-
-      Return str(self).
-
-
-
-.. py:data:: __version__
-   :annotation: = 0.10.1
-
-   
-
+:py:mod:`spine_engine.spine_engine`
+===================================
+
+.. py:module:: spine_engine.spine_engine
+
+.. autoapi-nested-parse::
+
+   Contains the SpineEngine class for running Spine Toolbox DAGs.
+
+   :authors: M. Marin (KTH)
+   :date:   20.11.2019
+
+
+
+Module Contents
+---------------
+
+Classes
+~~~~~~~
+
+.. autoapisummary::
+
+   spine_engine.spine_engine.ExecutionDirection
+   spine_engine.spine_engine.SpineEngineState
+   spine_engine.spine_engine.ItemExecutionFinishState
+   spine_engine.spine_engine.SpineEngine
+
+
+
+Functions
+~~~~~~~~~
+
+.. autoapisummary::
+
+   spine_engine.spine_engine._shorten
+   spine_engine.spine_engine._make_filter_id
+   spine_engine.spine_engine._filter_names_from_stack
+
+
+
+Attributes
+~~~~~~~~~~
+
+.. autoapisummary::
+
+   spine_engine.spine_engine.ED
+
+
+.. py:class:: ExecutionDirection
+
+   Bases: :py:obj:`enum.Enum`
+
+   Generic enumeration.
+
+   Derive from this class to define new enumerations.
+
+   .. py:attribute:: FORWARD
+      
+
+      
+
+   .. py:attribute:: BACKWARD
+      
+
+      
+
+   .. py:method:: __str__(self)
+
+      Return str(self).
+
+
+
+.. py:data:: ED
+   
+
+   
+
+.. py:class:: SpineEngineState
+
+   Bases: :py:obj:`enum.Enum`
+
+   Generic enumeration.
+
+   Derive from this class to define new enumerations.
+
+   .. py:attribute:: SLEEPING
+      :annotation: = 1
+
+      
+
+   .. py:attribute:: RUNNING
+      :annotation: = 2
+
+      
+
+   .. py:attribute:: USER_STOPPED
+      :annotation: = 3
+
+      
+
+   .. py:attribute:: FAILED
+      :annotation: = 4
+
+      
+
+   .. py:attribute:: COMPLETED
+      :annotation: = 5
+
+      
+
+   .. py:method:: __str__(self)
+
+      Return str(self).
+
+
+
+.. py:class:: ItemExecutionFinishState
+
+   Bases: :py:obj:`enum.Enum`
+
+   Generic enumeration.
+
+   Derive from this class to define new enumerations.
+
+   .. py:attribute:: SUCCESS
+      :annotation: = 1
+
+      
+
+   .. py:attribute:: FAILURE
+      :annotation: = 2
+
+      
+
+   .. py:attribute:: SKIPPED
+      :annotation: = 3
+
+      
+
+   .. py:attribute:: EXCLUDED
+      :annotation: = 4
+
+      
+
+   .. py:attribute:: STOPPED
+      :annotation: = 5
+
+      
+
+   .. py:attribute:: NEVER_FINISHED
+      :annotation: = 6
+
+      
+
+   .. py:method:: __str__(self)
+
+      Return str(self).
+
+
+
+.. py:class:: SpineEngine(items=None, specifications=None, connections=None, items_module_name='spine_items', settings=None, project_dir=None, execution_permits=None, node_successors=None, debug=False)
+
+   An engine for executing a Spine Toolbox DAG-workflow.
+
+   :param items: List of executable item dicts.
+   :type items: list(dict)
+   :param specifications: A mapping from item type to list of specification dicts.
+   :type specifications: dict(str,list(dict))
+   :param connections: List of connection dicts
+   :type connections: list of dict
+   :param items_module_name: name of the Python module that contains project items
+   :type items_module_name: str
+   :param settings: Toolbox execution settings.
+   :type settings: dict
+   :param project_dir: Path to project directory.
+   :type project_dir: str
+   :param execution_permits: A mapping from item name to a boolean value, False indicating that
+                             the item is not executed, only its resources are collected.
+   :type execution_permits: dict(str,bool)
+   :param node_successors: A mapping from item name to list of successor item names, dictating the dependencies.
+   :type node_successors: dict(str,list(str))
+   :param debug: Whether debug mode is active or not.
+   :type debug: bool
+
+   .. py:method:: _make_item_specifications(self, specifications, project_item_loader, items_module_name)
+
+      Instantiates item specifications.
+
+      :param specifications: A mapping from item type to list of specification dicts.
+      :type specifications: dict
+      :param project_item_loader: loader instance
+      :type project_item_loader: ProjectItemLoader
+      :param items_module_name: name of the Python module that contains the project items
+      :type items_module_name: str
+
+      :returns: mapping from item type to a dict that maps specification names to specification instances
+      :rtype: dict
+
+
+   .. py:method:: _make_item(self, item_name, direction)
+
+      Recreates item from project item dictionary. Note that all items are created twice.
+      One for the backward pipeline, the other one for the forward pipeline.
+
+
+   .. py:method:: get_event(self)
+
+      Returns the next event in the stream. Calling this after receiving the event of type "dag_exec_finished"
+      will raise StopIterationError.
+
+
+   .. py:method:: state(self)
+
+
+   .. py:method:: _get_event_stream(self)
+
+      Returns an iterator of tuples (event_type, event_data).
+
+      TODO: Describe the events in depth.
+
+
+   .. py:method:: answer_prompt(self, item_name, accepted)
+
+
+   .. py:method:: run(self)
+
+      Runs this engine.
+
+
+   .. py:method:: _process_event(self, event)
+
+      Processes events from a pipeline.
+
+      :param event: an event
+      :type event: DagsterEvent
+
+
+   .. py:method:: stop(self)
+
+      Stops the engine.
+
+
+   .. py:method:: _stop_item(self, item)
+
+
+   .. py:method:: _make_pipeline(self)
+
+      Returns a PipelineDefinition for executing this engine.
+
+      :returns: PipelineDefinition
+
+
+   .. py:method:: _make_backward_solid_def(self, item_name)
+
+      Returns a SolidDefinition for executing the given item in the backward sweep.
+
+      :param item_name: The project item that gets executed by the solid.
+      :type item_name: str
+
+
+   .. py:method:: _make_forward_solid_def(self, item_name)
+
+      Returns a SolidDefinition for executing the given item.
+
+      :param item_name:
+      :type item_name: str
+
+      :returns: SolidDefinition
+
+
+   .. py:method:: _execute_item(self, context, item_name, forward_resource_stacks, backward_resources)
+
+      Executes the given item using the given forward resource stacks and backward resources.
+      Returns list of output resource stacks.
+
+      Called by ``_make_forward_solid_def.compute_fn``.
+
+      For each element yielded by ``_filtered_resources_iterator``, spawns a thread that runs ``_execute_item_filtered``.
+
+      :param context:
+      :param item_name:
+      :type item_name: str
+      :param forward_resource_stacks:
+      :type forward_resource_stacks: list(tuple(ProjectItemResource))
+      :param backward_resources:
+      :type backward_resources: list(ProjectItemResource)
+
+      :returns: list(tuple(ProjectItemResource))
+
+
+   .. py:method:: _execute_item_filtered(self, item, filtered_forward_resources, filtered_backward_resources, output_resources_list, success)
+
+      Executes the given item using the given filtered resources. Target for threads in ``_execute_item``.
+
+      :param item:
+      :type item: ExecutableItemBase
+      :param filtered_forward_resources:
+      :type filtered_forward_resources: list(ProjectItemResource)
+      :param filtered_backward_resources:
+      :type filtered_backward_resources: list(ProjectItemResource)
+      :param output_resources_list: A list of lists, to append the
+                                    output resources generated by the item.
+      :type output_resources_list: list(list(ProjectItemResource))
+      :param success: A list of one element, to write the outcome of the execution.
+      :type success: list
+
+
+   .. py:method:: _filtered_resources_iterator(self, item_name, forward_resource_stacks, backward_resources, timestamp)
+
+      Yields tuples of (filtered forward resources, filtered backward resources, filter id).
+
+      Each tuple corresponds to a unique filter combination. Combinations are obtained by applying the cross-product
+      over forward resource stacks as yielded by ``_forward_resource_stacks_iterator``.
+
+      :param item_name:
+      :type item_name: str
+      :param forward_resource_stacks:
+      :type forward_resource_stacks: list(tuple(ProjectItemResource))
+      :param backward_resources:
+      :type backward_resources: list(ProjectItemResource)
+      :param timestamp: timestamp for the execution filter
+      :type timestamp: str
+
+      :returns: forward resources, backward resources, filter id
+      :rtype: Iterator(tuple(list,list,str))
+
+
+   .. py:method:: _expand_resource_stack(self, item_name, resource_stack)
+
+      Expands a resource stack if possible.
+
+      If the stack has more than one resource, returns the unaltered stack.
+
+      Otherwise, if the stack has only one resource but there are no filters defined for that resource,
+      again, returns the unaltered stack.
+
+      Otherwise, returns an expanded stack of as many resources as filter stacks defined for the only one resource.
+      Each resource in the expanded stack is a clone of the original, with one of the filter stacks
+      applied to the URL.
+
+      :param item_name: resource receiving item's name
+      :type item_name: str
+      :param resource_stack:
+      :type resource_stack: tuple(ProjectItemResource)
+
+      :returns: tuple(ProjectItemResource)
+
+
+   .. py:method:: _filter_stacks(self, item_name, resource_label)
+
+      Computes filter stacks.
+
+      Stacks are computed as the cross-product of all individual filters defined for a resource.
+
+      :param item_name: item's name
+      :type item_name: str
+      :param resource_label: resource's label
+      :type resource_label: str
+
+      :returns: filter stacks
+      :rtype: list of list
+
+
+   .. py:method:: _convert_forward_resources(self, item_name, resources)
+
+      Converts resources as they're being forwarded to given item.
+      The conversion is dictated by the connection the resources traverse in order to reach the item.
+
+      :param item_name: receiving item's name
+      :type item_name: str
+      :param resources: resources to convert
+      :type resources: list of ProjectItemResource
+
+      :returns: converted resources
+      :rtype: list of ProjectItemResource
+
+
+   .. py:method:: _make_dependencies(self)
+
+      Returns a dictionary of dependencies according to the given dictionaries of injectors.
+
+      :returns: a dictionary to pass to the PipelineDefinition constructor as dependencies
+      :rtype: dict
+
+
+
+.. py:function:: _shorten(resource)
+
+
+.. py:function:: _make_filter_id(resource_filter_stack)
+
+
+.. py:function:: _filter_names_from_stack(stack)
+
+
```

### Comparing `spine_engine-0.23.3/docs/source/autoapi/spine_engine/multithread_executor/thread_executor/index.rst` & `spine_engine-0.23.4/docs/source/autoapi/spine_engine/multithread_executor/thread_executor/index.rst`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,133 +1,133 @@
-:py:mod:`spine_engine.multithread_executor.thread_executor`
-===========================================================
-
-.. py:module:: spine_engine.multithread_executor.thread_executor
-
-.. autoapi-nested-parse::
-
-   Facilities for running arbitrary commands in child processes.
-
-
-
-Module Contents
----------------
-
-Classes
-~~~~~~~
-
-.. autoapisummary::
-
-   spine_engine.multithread_executor.thread_executor.ThreadEvent
-   spine_engine.multithread_executor.thread_executor.ThreadStartEvent
-   spine_engine.multithread_executor.thread_executor.ThreadDoneEvent
-   spine_engine.multithread_executor.thread_executor.ThreadSystemErrorEvent
-
-
-
-Functions
-~~~~~~~~~
-
-.. autoapisummary::
-
-   spine_engine.multithread_executor.thread_executor._execute_step_in_thread
-   spine_engine.multithread_executor.thread_executor._poll_for_event
-   spine_engine.multithread_executor.thread_executor.execute_thread_step
-
-
-
-Attributes
-~~~~~~~~~~
-
-.. autoapisummary::
-
-   spine_engine.multithread_executor.thread_executor.TICK
-   spine_engine.multithread_executor.thread_executor.THREAD_DEAD_AND_QUEUE_EMPTY
-
-
-.. py:class:: ThreadEvent
-
-
-.. py:class:: ThreadStartEvent
-
-   Bases: :py:obj:`namedtuple`\ (\ :py:obj:`'ThreadStartEvent'`\ , :py:obj:`'tid'`\ ), :py:obj:`ThreadEvent`
-
-   docstring
-
-   Initialize self.  See help(type(self)) for accurate signature.
-
-
-.. py:class:: ThreadDoneEvent
-
-   Bases: :py:obj:`namedtuple`\ (\ :py:obj:`'ThreadDoneEvent'`\ , :py:obj:`'tid'`\ ), :py:obj:`ThreadEvent`
-
-   docstring
-
-   Initialize self.  See help(type(self)) for accurate signature.
-
-
-.. py:class:: ThreadSystemErrorEvent
-
-   Bases: :py:obj:`namedtuple`\ (\ :py:obj:`'ThreadSystemErrorEvent'`\ , :py:obj:`'tid error_info'`\ ), :py:obj:`ThreadEvent`
-
-   docstring
-
-   Initialize self.  See help(type(self)) for accurate signature.
-
-
-.. py:exception:: ThreadCrashException
-
-   Bases: :py:obj:`Exception`
-
-   Thrown when the thread crashes.
-
-   Initialize self.  See help(type(self)) for accurate signature.
-
-
-.. py:function:: _execute_step_in_thread(event_queue, step_context, retries)
-
-   Wraps the execution of a step.
-
-   Handles errors and communicates across a queue with the parent process.
-
-
-.. py:data:: TICK
-   
-
-   The minimum interval at which to check for child process liveness -- default 20ms.
-
-.. py:data:: THREAD_DEAD_AND_QUEUE_EMPTY
-   :annotation: = THREAD_DEAD_AND_QUEUE_EMPTY
-
-   Sentinel value.
-
-.. py:function:: _poll_for_event(thread, event_queue)
-
-
-.. py:function:: execute_thread_step(step_context, retries)
-
-   Execute a step in a new thread.
-
-   This function starts a new thread whose execution target is the given step context wrapped by
-   _execute_step_in_thread; polls the queue for events yielded by the thread
-   until it dies and the queue is empty.
-
-   This function yields a complex set of objects to enable having multiple thread
-   executions in flight:
-
-       * None - nothing has happened, yielded to enable cooperative multitasking other iterators
-       * ThreadEvent - Family of objects that communicates state changes in the thread
-       * KeyboardInterrupt - Yielded in the case that an interrupt was recieved while
-           polling the thread. Yielded instead of raised to allow forwarding of the
-           interrupt to the thread and completion of the iterator for this thread and
-           any others that may be executing
-       * The actual values yielded by the thread execution
-
-   :param step_context: The step context to execute in the child process.
-   :type step_context: SystemStepExecutionContext
-   :param retries:
-   :type retries: Retries
-
-   Warning: if the thread is in an infinite loop, this will
-   also infinitely loop.
-
-
+:py:mod:`spine_engine.multithread_executor.thread_executor`
+===========================================================
+
+.. py:module:: spine_engine.multithread_executor.thread_executor
+
+.. autoapi-nested-parse::
+
+   Facilities for running arbitrary commands in child processes.
+
+
+
+Module Contents
+---------------
+
+Classes
+~~~~~~~
+
+.. autoapisummary::
+
+   spine_engine.multithread_executor.thread_executor.ThreadEvent
+   spine_engine.multithread_executor.thread_executor.ThreadStartEvent
+   spine_engine.multithread_executor.thread_executor.ThreadDoneEvent
+   spine_engine.multithread_executor.thread_executor.ThreadSystemErrorEvent
+
+
+
+Functions
+~~~~~~~~~
+
+.. autoapisummary::
+
+   spine_engine.multithread_executor.thread_executor._execute_step_in_thread
+   spine_engine.multithread_executor.thread_executor._poll_for_event
+   spine_engine.multithread_executor.thread_executor.execute_thread_step
+
+
+
+Attributes
+~~~~~~~~~~
+
+.. autoapisummary::
+
+   spine_engine.multithread_executor.thread_executor.TICK
+   spine_engine.multithread_executor.thread_executor.THREAD_DEAD_AND_QUEUE_EMPTY
+
+
+.. py:class:: ThreadEvent
+
+
+.. py:class:: ThreadStartEvent
+
+   Bases: :py:obj:`namedtuple`\ (\ :py:obj:`'ThreadStartEvent'`\ , :py:obj:`'tid'`\ ), :py:obj:`ThreadEvent`
+
+   docstring
+
+   Initialize self.  See help(type(self)) for accurate signature.
+
+
+.. py:class:: ThreadDoneEvent
+
+   Bases: :py:obj:`namedtuple`\ (\ :py:obj:`'ThreadDoneEvent'`\ , :py:obj:`'tid'`\ ), :py:obj:`ThreadEvent`
+
+   docstring
+
+   Initialize self.  See help(type(self)) for accurate signature.
+
+
+.. py:class:: ThreadSystemErrorEvent
+
+   Bases: :py:obj:`namedtuple`\ (\ :py:obj:`'ThreadSystemErrorEvent'`\ , :py:obj:`'tid error_info'`\ ), :py:obj:`ThreadEvent`
+
+   docstring
+
+   Initialize self.  See help(type(self)) for accurate signature.
+
+
+.. py:exception:: ThreadCrashException
+
+   Bases: :py:obj:`Exception`
+
+   Thrown when the thread crashes.
+
+   Initialize self.  See help(type(self)) for accurate signature.
+
+
+.. py:function:: _execute_step_in_thread(event_queue, step_context, retries)
+
+   Wraps the execution of a step.
+
+   Handles errors and communicates across a queue with the parent process.
+
+
+.. py:data:: TICK
+   
+
+   The minimum interval at which to check for child process liveness -- default 20ms.
+
+.. py:data:: THREAD_DEAD_AND_QUEUE_EMPTY
+   :annotation: = THREAD_DEAD_AND_QUEUE_EMPTY
+
+   Sentinel value.
+
+.. py:function:: _poll_for_event(thread, event_queue)
+
+
+.. py:function:: execute_thread_step(step_context, retries)
+
+   Execute a step in a new thread.
+
+   This function starts a new thread whose execution target is the given step context wrapped by
+   _execute_step_in_thread; polls the queue for events yielded by the thread
+   until it dies and the queue is empty.
+
+   This function yields a complex set of objects to enable having multiple thread
+   executions in flight:
+
+       * None - nothing has happened, yielded to enable cooperative multitasking other iterators
+       * ThreadEvent - Family of objects that communicates state changes in the thread
+       * KeyboardInterrupt - Yielded in the case that an interrupt was recieved while
+           polling the thread. Yielded instead of raised to allow forwarding of the
+           interrupt to the thread and completion of the iterator for this thread and
+           any others that may be executing
+       * The actual values yielded by the thread execution
+
+   :param step_context: The step context to execute in the child process.
+   :type step_context: SystemStepExecutionContext
+   :param retries:
+   :type retries: Retries
+
+   Warning: if the thread is in an infinite loop, this will
+   also infinitely loop.
+
+
```

### Comparing `spine_engine-0.23.3/docs/source/autoapi/spine_engine/project_item/connection/index.rst` & `spine_engine-0.23.4/docs/source/autoapi/spine_engine/project_item/connection/index.rst`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,165 +1,165 @@
-:py:mod:`spine_engine.project_item.connection`
-==============================================
-
-.. py:module:: spine_engine.project_item.connection
-
-.. autoapi-nested-parse::
-
-   Provides the :class:`Connection` class.
-
-   :authors: A. Soininen (VTT)
-   :date:    12.2.2021
-
-
-
-Module Contents
----------------
-
-Classes
-~~~~~~~
-
-.. autoapisummary::
-
-   spine_engine.project_item.connection.Connection
-
-
-
-
-.. py:class:: Connection(source_name, source_position, destination_name, destination_position, resource_filters=None, options=None)
-
-   Represents a connection between two project items.
-
-   :param source_name: source project item's name
-   :type source_name: str
-   :param source_position: source anchor's position
-   :type source_position: str
-   :param destination_name: destination project item's name
-   :type destination_name: str
-   :param destination_position: destination anchor's position
-   :type destination_position: str
-   :param resource_filters: mapping from resource labels and filter types to
-                            database ids and activity flags
-   :type resource_filters: dict, optional
-   :param options: any options, at the moment only has "use_datapackage"
-   :type options: dict, optional
-
-   .. py:method:: __eq__(self, other)
-
-      Return self==value.
-
-
-   .. py:method:: name(self)
-      :property:
-
-
-   .. py:method:: destination_position(self)
-      :property:
-
-      Anchor's position on destination item.
-
-
-   .. py:method:: source_position(self)
-      :property:
-
-      Anchor's position on source item.
-
-
-   .. py:method:: database_resources(self)
-      :property:
-
-      Connection's database resources
-
-
-   .. py:method:: has_filters(self)
-
-      Return True if connection has filters.
-
-      :returns: True if connection has filters, False otherwise
-      :rtype: bool
-
-
-   .. py:method:: resource_filters(self)
-      :property:
-
-      Connection's resource filters.
-
-
-   .. py:method:: use_datapackage(self)
-      :property:
-
-
-   .. py:method:: id_to_name(self, id_, filter_type)
-
-      Map from scenario/tool database id to name
-
-
-   .. py:method:: receive_resources_from_source(self, resources)
-
-      Receives resources from source item.
-
-      :param resources: source item's resources
-      :type resources: Iterable of ProjectItemResource
-
-
-   .. py:method:: replace_resource_from_source(self, old, new)
-
-      Replaces an existing resource.
-
-      :param old: old resource
-      :type old: ProjectItemResource
-      :param new: new resource
-      :type new: ProjectItemResource
-
-
-   .. py:method:: fetch_database_items(self)
-
-      Reads filter information from database.
-
-
-   .. py:method:: set_online(self, resource, filter_type, online)
-
-      Sets the given filters online or offline.
-
-      :param resource: Resource label
-      :type resource: str
-      :param filter_type: Either SCENARIO_FILTER_TYPE or TOOL_FILTER_TYPE, for now.
-      :type filter_type: str
-      :param online: mapping from scenario/tool id to online flag
-      :type online: dict
-
-
-   .. py:method:: convert_resources(self, resources)
-
-      Called when advertising resources through this connection *in the FORWARD direction*.
-      Takes the initial list of resources advertised by the source item and returns a new list,
-      which is the one finally advertised.
-
-      At the moment it only packs CSVs into datapackage (and again, it's only used in the FORWARD direction).
-
-      :param resources: Resources to convert
-      :type resources: list of ProjectItemResource
-
-      :returns: list of ProjectItemResource
-
-
-   .. py:method:: to_dict(self)
-
-      Returns a dictionary representation of this Connection.
-
-      :returns: serialized Connection
-      :rtype: dict
-
-
-   .. py:method:: from_dict(connection_dict)
-      :staticmethod:
-
-      Restores a connection from dictionary.
-
-      :param connection_dict: connection dictionary
-      :type connection_dict: dict
-
-      :returns: restored connection
-      :rtype: Connection
-
-
-
+:py:mod:`spine_engine.project_item.connection`
+==============================================
+
+.. py:module:: spine_engine.project_item.connection
+
+.. autoapi-nested-parse::
+
+   Provides the :class:`Connection` class.
+
+   :authors: A. Soininen (VTT)
+   :date:    12.2.2021
+
+
+
+Module Contents
+---------------
+
+Classes
+~~~~~~~
+
+.. autoapisummary::
+
+   spine_engine.project_item.connection.Connection
+
+
+
+
+.. py:class:: Connection(source_name, source_position, destination_name, destination_position, resource_filters=None, options=None)
+
+   Represents a connection between two project items.
+
+   :param source_name: source project item's name
+   :type source_name: str
+   :param source_position: source anchor's position
+   :type source_position: str
+   :param destination_name: destination project item's name
+   :type destination_name: str
+   :param destination_position: destination anchor's position
+   :type destination_position: str
+   :param resource_filters: mapping from resource labels and filter types to
+                            database ids and activity flags
+   :type resource_filters: dict, optional
+   :param options: any options, at the moment only has "use_datapackage"
+   :type options: dict, optional
+
+   .. py:method:: __eq__(self, other)
+
+      Return self==value.
+
+
+   .. py:method:: name(self)
+      :property:
+
+
+   .. py:method:: destination_position(self)
+      :property:
+
+      Anchor's position on destination item.
+
+
+   .. py:method:: source_position(self)
+      :property:
+
+      Anchor's position on source item.
+
+
+   .. py:method:: database_resources(self)
+      :property:
+
+      Connection's database resources
+
+
+   .. py:method:: has_filters(self)
+
+      Return True if connection has filters.
+
+      :returns: True if connection has filters, False otherwise
+      :rtype: bool
+
+
+   .. py:method:: resource_filters(self)
+      :property:
+
+      Connection's resource filters.
+
+
+   .. py:method:: use_datapackage(self)
+      :property:
+
+
+   .. py:method:: id_to_name(self, id_, filter_type)
+
+      Map from scenario/tool database id to name
+
+
+   .. py:method:: receive_resources_from_source(self, resources)
+
+      Receives resources from source item.
+
+      :param resources: source item's resources
+      :type resources: Iterable of ProjectItemResource
+
+
+   .. py:method:: replace_resource_from_source(self, old, new)
+
+      Replaces an existing resource.
+
+      :param old: old resource
+      :type old: ProjectItemResource
+      :param new: new resource
+      :type new: ProjectItemResource
+
+
+   .. py:method:: fetch_database_items(self)
+
+      Reads filter information from database.
+
+
+   .. py:method:: set_online(self, resource, filter_type, online)
+
+      Sets the given filters online or offline.
+
+      :param resource: Resource label
+      :type resource: str
+      :param filter_type: Either SCENARIO_FILTER_TYPE or TOOL_FILTER_TYPE, for now.
+      :type filter_type: str
+      :param online: mapping from scenario/tool id to online flag
+      :type online: dict
+
+
+   .. py:method:: convert_resources(self, resources)
+
+      Called when advertising resources through this connection *in the FORWARD direction*.
+      Takes the initial list of resources advertised by the source item and returns a new list,
+      which is the one finally advertised.
+
+      At the moment it only packs CSVs into datapackage (and again, it's only used in the FORWARD direction).
+
+      :param resources: Resources to convert
+      :type resources: list of ProjectItemResource
+
+      :returns: list of ProjectItemResource
+
+
+   .. py:method:: to_dict(self)
+
+      Returns a dictionary representation of this Connection.
+
+      :returns: serialized Connection
+      :rtype: dict
+
+
+   .. py:method:: from_dict(connection_dict)
+      :staticmethod:
+
+      Restores a connection from dictionary.
+
+      :param connection_dict: connection dictionary
+      :type connection_dict: dict
+
+      :returns: restored connection
+      :rtype: Connection
+
+
+
```

### Comparing `spine_engine-0.23.3/docs/source/autoapi/spine_engine/project_item/executable_item_base/index.rst` & `spine_engine-0.23.4/docs/source/autoapi/spine_engine/project_item/executable_item_base/index.rst`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,186 +1,186 @@
-:py:mod:`spine_engine.project_item.executable_item_base`
-========================================================
-
-.. py:module:: spine_engine.project_item.executable_item_base
-
-.. autoapi-nested-parse::
-
-   Contains ExecutableItem, a project item's counterpart in execution as well as support utilities.
-
-   :authors: A. Soininen (VTT)
-   :date:    30.3.2020
-
-
-
-Module Contents
----------------
-
-Classes
-~~~~~~~
-
-.. autoapisummary::
-
-   spine_engine.project_item.executable_item_base.ExecutableItemBase
-
-
-
-
-.. py:class:: ExecutableItemBase(name, project_dir, logger)
-
-   The part of a project item that is executed by the Spine Engine.
-
-   :param name: item's name
-   :type name: str
-   :param project_dir: absolute path to project directory
-   :type project_dir: str
-   :param logger: a logger
-   :type logger: LoggerInterface
-
-   .. py:method:: name(self)
-      :property:
-
-      Project item's name.
-
-
-   .. py:method:: group_id(self)
-      :property:
-
-      Returns the id for group-execution.
-      Items in the same group share a kernel, and also reuse the same kernel from past executions.
-      By default each item is its own group, so it executes in isolation.
-      NOTE: At the moment this is only used by Tool, but could be used by other items in the future?
-
-      :returns: item's id within an execution group
-      :rtype: str
-
-
-   .. py:method:: filter_id(self)
-      :property:
-
-
-   .. py:method:: ready_to_execute(self, settings)
-
-      Validates the internal state of this project item before execution.
-
-      Subclasses can implement this method to do the appropriate work.
-
-      :param settings: Application settings
-      :type settings: AppSettings
-
-      :returns: True if project item is ready for execution, False otherwise
-      :rtype: bool
-
-
-   .. py:method:: execute(self, forward_resources, backward_resources)
-
-      Executes this item using the given resources and returns a boolean indicating the outcome.
-
-      Subclasses can implement this method to do the appropriate work.
-
-      :param forward_resources: a list of ProjectItemResources from predecessors (forward)
-      :type forward_resources: list
-      :param backward_resources: a list of ProjectItemResources from successors (backward)
-      :type backward_resources: list
-
-      :returns: State depending on operation success
-      :rtype: ItemExecutionFinishState
-
-
-   .. py:method:: exclude_execution(self, forward_resources, backward_resources)
-
-      Excludes execution of this item.
-
-      This method is called when the item is not selected (i.e EXCLUDED) for execution.
-      Only lightweight bookkeeping or processing should be done in this case, e.g.
-      forward input resources.
-
-      Subclasses can implement this method to the appropriate work.
-
-      :param forward_resources: a list of ProjectItemResources from predecessors (forward)
-      :type forward_resources: list
-      :param backward_resources: a list of ProjectItemResources from successors (backward)
-      :type backward_resources: list
-
-
-   .. py:method:: finish_execution(self, state)
-
-      Does any work needed after execution given the execution success status.
-
-      :param state: Item execution finish state
-      :type state: ItemExecutionFinishState
-
-
-   .. py:method:: item_type()
-      :staticmethod:
-      :abstractmethod:
-
-      Returns the item's type identifier string.
-
-
-   .. py:method:: output_resources(self, direction)
-
-      Returns output resources in the given direction.
-
-      Subclasses need to implement _output_resources_backward and/or _output_resources_forward
-      if they want to provide resources in any direction.
-
-      :param direction: Direction where output resources are passed
-      :type direction: ExecutionDirection
-
-      :returns: a list of ProjectItemResources
-      :rtype: list
-
-
-   .. py:method:: stop_execution(self)
-
-      Stops executing this item.
-
-
-   .. py:method:: _output_resources_forward(self)
-
-      Returns output resources for forward execution.
-
-      The default implementation returns an empty list.
-
-      :returns: a list of ProjectItemResources
-      :rtype: list
-
-
-   .. py:method:: _output_resources_backward(self)
-
-      Returns output resources for backward execution.
-
-      The default implementation returns an empty list.
-
-      :returns: a list of ProjectItemResources
-      :rtype: list
-
-
-   .. py:method:: from_dict(cls, item_dict, name, project_dir, app_settings, specifications, logger)
-      :classmethod:
-      :abstractmethod:
-
-      Deserializes an executable item from item dictionary.
-
-      :param item_dict: serialized project item
-      :type item_dict: dict
-      :param name: item's name
-      :type name: str
-      :param project_dir: absolute path to the project directory
-      :type project_dir: str
-      :param app_settings: Toolbox settings
-      :type app_settings: QSettings
-      :param specifications: mapping from item type to specification name to :class:`ProjectItemSpecification`
-      :type specifications: dict
-      :param logger: a logger
-      :type logger: LoggingInterface
-
-      :returns: deserialized executable item
-      :rtype: ExecutableItemBase
-
-
-   .. py:method:: _get_specification(name, item_type, specification_name, specifications, logger)
-      :staticmethod:
-
-
-
+:py:mod:`spine_engine.project_item.executable_item_base`
+========================================================
+
+.. py:module:: spine_engine.project_item.executable_item_base
+
+.. autoapi-nested-parse::
+
+   Contains ExecutableItem, a project item's counterpart in execution as well as support utilities.
+
+   :authors: A. Soininen (VTT)
+   :date:    30.3.2020
+
+
+
+Module Contents
+---------------
+
+Classes
+~~~~~~~
+
+.. autoapisummary::
+
+   spine_engine.project_item.executable_item_base.ExecutableItemBase
+
+
+
+
+.. py:class:: ExecutableItemBase(name, project_dir, logger)
+
+   The part of a project item that is executed by the Spine Engine.
+
+   :param name: item's name
+   :type name: str
+   :param project_dir: absolute path to project directory
+   :type project_dir: str
+   :param logger: a logger
+   :type logger: LoggerInterface
+
+   .. py:method:: name(self)
+      :property:
+
+      Project item's name.
+
+
+   .. py:method:: group_id(self)
+      :property:
+
+      Returns the id for group-execution.
+      Items in the same group share a kernel, and also reuse the same kernel from past executions.
+      By default each item is its own group, so it executes in isolation.
+      NOTE: At the moment this is only used by Tool, but could be used by other items in the future?
+
+      :returns: item's id within an execution group
+      :rtype: str
+
+
+   .. py:method:: filter_id(self)
+      :property:
+
+
+   .. py:method:: ready_to_execute(self, settings)
+
+      Validates the internal state of this project item before execution.
+
+      Subclasses can implement this method to do the appropriate work.
+
+      :param settings: Application settings
+      :type settings: AppSettings
+
+      :returns: True if project item is ready for execution, False otherwise
+      :rtype: bool
+
+
+   .. py:method:: execute(self, forward_resources, backward_resources)
+
+      Executes this item using the given resources and returns a boolean indicating the outcome.
+
+      Subclasses can implement this method to do the appropriate work.
+
+      :param forward_resources: a list of ProjectItemResources from predecessors (forward)
+      :type forward_resources: list
+      :param backward_resources: a list of ProjectItemResources from successors (backward)
+      :type backward_resources: list
+
+      :returns: State depending on operation success
+      :rtype: ItemExecutionFinishState
+
+
+   .. py:method:: exclude_execution(self, forward_resources, backward_resources)
+
+      Excludes execution of this item.
+
+      This method is called when the item is not selected (i.e EXCLUDED) for execution.
+      Only lightweight bookkeeping or processing should be done in this case, e.g.
+      forward input resources.
+
+      Subclasses can implement this method to the appropriate work.
+
+      :param forward_resources: a list of ProjectItemResources from predecessors (forward)
+      :type forward_resources: list
+      :param backward_resources: a list of ProjectItemResources from successors (backward)
+      :type backward_resources: list
+
+
+   .. py:method:: finish_execution(self, state)
+
+      Does any work needed after execution given the execution success status.
+
+      :param state: Item execution finish state
+      :type state: ItemExecutionFinishState
+
+
+   .. py:method:: item_type()
+      :staticmethod:
+      :abstractmethod:
+
+      Returns the item's type identifier string.
+
+
+   .. py:method:: output_resources(self, direction)
+
+      Returns output resources in the given direction.
+
+      Subclasses need to implement _output_resources_backward and/or _output_resources_forward
+      if they want to provide resources in any direction.
+
+      :param direction: Direction where output resources are passed
+      :type direction: ExecutionDirection
+
+      :returns: a list of ProjectItemResources
+      :rtype: list
+
+
+   .. py:method:: stop_execution(self)
+
+      Stops executing this item.
+
+
+   .. py:method:: _output_resources_forward(self)
+
+      Returns output resources for forward execution.
+
+      The default implementation returns an empty list.
+
+      :returns: a list of ProjectItemResources
+      :rtype: list
+
+
+   .. py:method:: _output_resources_backward(self)
+
+      Returns output resources for backward execution.
+
+      The default implementation returns an empty list.
+
+      :returns: a list of ProjectItemResources
+      :rtype: list
+
+
+   .. py:method:: from_dict(cls, item_dict, name, project_dir, app_settings, specifications, logger)
+      :classmethod:
+      :abstractmethod:
+
+      Deserializes an executable item from item dictionary.
+
+      :param item_dict: serialized project item
+      :type item_dict: dict
+      :param name: item's name
+      :type name: str
+      :param project_dir: absolute path to the project directory
+      :type project_dir: str
+      :param app_settings: Toolbox settings
+      :type app_settings: QSettings
+      :param specifications: mapping from item type to specification name to :class:`ProjectItemSpecification`
+      :type specifications: dict
+      :param logger: a logger
+      :type logger: LoggingInterface
+
+      :returns: deserialized executable item
+      :rtype: ExecutableItemBase
+
+
+   .. py:method:: _get_specification(name, item_type, specification_name, specifications, logger)
+      :staticmethod:
+
+
+
```

### Comparing `spine_engine-0.23.3/docs/source/autoapi/spine_engine/project_item/project_item_resource/index.rst` & `spine_engine-0.23.4/docs/source/autoapi/spine_engine/project_item/project_item_resource/index.rst`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,198 +1,198 @@
-:py:mod:`spine_engine.project_item.project_item_resource`
-=========================================================
-
-.. py:module:: spine_engine.project_item.project_item_resource
-
-.. autoapi-nested-parse::
-
-   Provides the ProjectItemResource class.
-
-   :authors: M. Marin (KTH)
-   :date:   29.4.2020
-
-
-
-Module Contents
----------------
-
-Classes
-~~~~~~~
-
-.. autoapisummary::
-
-   spine_engine.project_item.project_item_resource.ProjectItemResource
-
-
-
-Functions
-~~~~~~~~~
-
-.. autoapisummary::
-
-   spine_engine.project_item.project_item_resource.database_resource
-   spine_engine.project_item.project_item_resource.file_resource
-   spine_engine.project_item.project_item_resource.transient_file_resource
-   spine_engine.project_item.project_item_resource.file_resource_in_pack
-   spine_engine.project_item.project_item_resource.extract_packs
-
-
-
-.. py:class:: ProjectItemResource(provider_name, type_, label, url=None, metadata=None)
-
-   Class to hold a resource made available by a project item and that may be consumed by another project item.
-
-   .. attribute:: provider_name
-
-      name of resource provider
-
-      :type: str
-
-   .. attribute:: type_
-
-      resource's type
-
-      :type: str
-
-   .. attribute:: label
-
-      an identifier string
-
-      :type: str
-
-   .. attribute:: metadata
-
-      resource's metadata
-
-      :type: dict
-
-   :param provider_name: The name of the item that provides the resource
-   :type provider_name: str
-   :param type_: The resource type, currently available types:
-
-                 - "file": url points to the file's path
-                 - "file_pack": resource is part of a pack; url points to the file's path
-                 - "database": url is the databases url
-   :type type_: str
-   :param label: A label that identifies the resource.
-   :type label: str
-   :param url: The url of the resource.
-   :type url: str, optional
-   :param metadata: Additional metadata providing extra information about the resource.
-                    Currently available keys:
-
-                    - filter_stack (str): resource's filter stack
-                    - filter_id (str): filter id
-   :type metadata: dict
-
-   .. py:method:: clone(self, additional_metadata=None)
-
-      Clones a resource and optionally updates the clone's metadata.
-
-      :param additional_metadata: metadata to add to the clone
-      :type additional_metadata: dict
-
-      :returns: cloned resource
-      :rtype: ProjectItemResource
-
-
-   .. py:method:: __eq__(self, other)
-
-      Return self==value.
-
-
-   .. py:method:: __hash__(self)
-
-      Return hash(self).
-
-
-   .. py:method:: __repr__(self)
-
-      Return repr(self).
-
-
-   .. py:method:: url(self)
-      :property:
-
-      Resource URL.
-
-
-   .. py:method:: path(self)
-      :property:
-
-      Returns the resource path in the local syntax, as obtained from parsing the url.
-
-
-   .. py:method:: scheme(self)
-      :property:
-
-      Returns the resource scheme, as obtained from parsing the url.
-
-
-   .. py:method:: hasfilepath(self)
-      :property:
-
-
-   .. py:method:: arg(self)
-      :property:
-
-
-
-.. py:function:: database_resource(provider_name, url, label=None)
-
-   Constructs a database resource.
-
-   :param provider_name: resource provider's name
-   :type provider_name: str
-   :param url: database URL
-   :type url: str
-   :param label: resource label
-   :type label: str, optional
-
-
-.. py:function:: file_resource(provider_name, file_path, label=None)
-
-   Constructs a file resource.
-
-   :param provider_name: resource provider's name
-   :type provider_name: str
-   :param file_path: path to file
-   :type file_path: str
-   :param label: resource label
-   :type label: str, optional
-
-
-.. py:function:: transient_file_resource(provider_name, label, file_path=None)
-
-   Constructs a transient file resource.
-
-   :param provider_name: resource provider's name
-   :type provider_name: str
-   :param label: resource label
-   :type label: str
-   :param file_path: file path if the file exists
-   :type file_path: str, optional
-
-
-.. py:function:: file_resource_in_pack(provider_name, label, file_path=None)
-
-   Constructs a file resource that is part of a resource pack.
-
-   :param provider_name: resource provider's name
-   :type provider_name: str
-   :param label: resource label
-   :type label: str
-   :param file_path: file path if the file exists
-   :type file_path: str, optional
-
-
-.. py:function:: extract_packs(resources)
-
-   Extracts file packs from resources.
-
-   :param resources: resources to process
-   :type resources: Iterable of ProjectItemResource
-
-   :returns: list of non-pack resources and dictionary of packs keyed by label
-   :rtype: tuple
-
-
+:py:mod:`spine_engine.project_item.project_item_resource`
+=========================================================
+
+.. py:module:: spine_engine.project_item.project_item_resource
+
+.. autoapi-nested-parse::
+
+   Provides the ProjectItemResource class.
+
+   :authors: M. Marin (KTH)
+   :date:   29.4.2020
+
+
+
+Module Contents
+---------------
+
+Classes
+~~~~~~~
+
+.. autoapisummary::
+
+   spine_engine.project_item.project_item_resource.ProjectItemResource
+
+
+
+Functions
+~~~~~~~~~
+
+.. autoapisummary::
+
+   spine_engine.project_item.project_item_resource.database_resource
+   spine_engine.project_item.project_item_resource.file_resource
+   spine_engine.project_item.project_item_resource.transient_file_resource
+   spine_engine.project_item.project_item_resource.file_resource_in_pack
+   spine_engine.project_item.project_item_resource.extract_packs
+
+
+
+.. py:class:: ProjectItemResource(provider_name, type_, label, url=None, metadata=None)
+
+   Class to hold a resource made available by a project item and that may be consumed by another project item.
+
+   .. attribute:: provider_name
+
+      name of resource provider
+
+      :type: str
+
+   .. attribute:: type_
+
+      resource's type
+
+      :type: str
+
+   .. attribute:: label
+
+      an identifier string
+
+      :type: str
+
+   .. attribute:: metadata
+
+      resource's metadata
+
+      :type: dict
+
+   :param provider_name: The name of the item that provides the resource
+   :type provider_name: str
+   :param type_: The resource type, currently available types:
+
+                 - "file": url points to the file's path
+                 - "file_pack": resource is part of a pack; url points to the file's path
+                 - "database": url is the databases url
+   :type type_: str
+   :param label: A label that identifies the resource.
+   :type label: str
+   :param url: The url of the resource.
+   :type url: str, optional
+   :param metadata: Additional metadata providing extra information about the resource.
+                    Currently available keys:
+
+                    - filter_stack (str): resource's filter stack
+                    - filter_id (str): filter id
+   :type metadata: dict
+
+   .. py:method:: clone(self, additional_metadata=None)
+
+      Clones a resource and optionally updates the clone's metadata.
+
+      :param additional_metadata: metadata to add to the clone
+      :type additional_metadata: dict
+
+      :returns: cloned resource
+      :rtype: ProjectItemResource
+
+
+   .. py:method:: __eq__(self, other)
+
+      Return self==value.
+
+
+   .. py:method:: __hash__(self)
+
+      Return hash(self).
+
+
+   .. py:method:: __repr__(self)
+
+      Return repr(self).
+
+
+   .. py:method:: url(self)
+      :property:
+
+      Resource URL.
+
+
+   .. py:method:: path(self)
+      :property:
+
+      Returns the resource path in the local syntax, as obtained from parsing the url.
+
+
+   .. py:method:: scheme(self)
+      :property:
+
+      Returns the resource scheme, as obtained from parsing the url.
+
+
+   .. py:method:: hasfilepath(self)
+      :property:
+
+
+   .. py:method:: arg(self)
+      :property:
+
+
+
+.. py:function:: database_resource(provider_name, url, label=None)
+
+   Constructs a database resource.
+
+   :param provider_name: resource provider's name
+   :type provider_name: str
+   :param url: database URL
+   :type url: str
+   :param label: resource label
+   :type label: str, optional
+
+
+.. py:function:: file_resource(provider_name, file_path, label=None)
+
+   Constructs a file resource.
+
+   :param provider_name: resource provider's name
+   :type provider_name: str
+   :param file_path: path to file
+   :type file_path: str
+   :param label: resource label
+   :type label: str, optional
+
+
+.. py:function:: transient_file_resource(provider_name, label, file_path=None)
+
+   Constructs a transient file resource.
+
+   :param provider_name: resource provider's name
+   :type provider_name: str
+   :param label: resource label
+   :type label: str
+   :param file_path: file path if the file exists
+   :type file_path: str, optional
+
+
+.. py:function:: file_resource_in_pack(provider_name, label, file_path=None)
+
+   Constructs a file resource that is part of a resource pack.
+
+   :param provider_name: resource provider's name
+   :type provider_name: str
+   :param label: resource label
+   :type label: str
+   :param file_path: file path if the file exists
+   :type file_path: str, optional
+
+
+.. py:function:: extract_packs(resources)
+
+   Extracts file packs from resources.
+
+   :param resources: resources to process
+   :type resources: Iterable of ProjectItemResource
+
+   :returns: list of non-pack resources and dictionary of packs keyed by label
+   :rtype: tuple
+
+
```

### Comparing `spine_engine-0.23.3/docs/source/autoapi/spine_engine/project_item/project_item_specification_factory/index.rst` & `spine_engine-0.23.4/docs/source/autoapi/spine_engine/project_item/project_item_specification_factory/index.rst`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,56 +1,56 @@
-:py:mod:`spine_engine.project_item.project_item_specification_factory`
-======================================================================
-
-.. py:module:: spine_engine.project_item.project_item_specification_factory
-
-.. autoapi-nested-parse::
-
-   Contains project item specification factory.
-
-   :authors: A. Soininen (VTT)
-   :date:   6.5.2020
-
-
-
-Module Contents
----------------
-
-Classes
-~~~~~~~
-
-.. autoapisummary::
-
-   spine_engine.project_item.project_item_specification_factory.ProjectItemSpecificationFactory
-
-
-
-
-.. py:class:: ProjectItemSpecificationFactory
-
-   A factory to make project item specifications.
-
-   .. py:method:: item_type()
-      :staticmethod:
-      :abstractmethod:
-
-      Returns the project item's type.
-
-
-   .. py:method:: make_specification(definition, app_settings, logger)
-      :staticmethod:
-      :abstractmethod:
-
-      Makes a project item specification.
-
-      :param definition: specification's definition dictionary
-      :type definition: dict
-      :param app_settings: Toolbox settings
-      :type app_settings: QSettings
-      :param logger: a logger
-      :type logger: LoggerInterface
-
-      :returns: a specification built from the given definition
-      :rtype: ProjectItemSpecification
-
-
-
+:py:mod:`spine_engine.project_item.project_item_specification_factory`
+======================================================================
+
+.. py:module:: spine_engine.project_item.project_item_specification_factory
+
+.. autoapi-nested-parse::
+
+   Contains project item specification factory.
+
+   :authors: A. Soininen (VTT)
+   :date:   6.5.2020
+
+
+
+Module Contents
+---------------
+
+Classes
+~~~~~~~
+
+.. autoapisummary::
+
+   spine_engine.project_item.project_item_specification_factory.ProjectItemSpecificationFactory
+
+
+
+
+.. py:class:: ProjectItemSpecificationFactory
+
+   A factory to make project item specifications.
+
+   .. py:method:: item_type()
+      :staticmethod:
+      :abstractmethod:
+
+      Returns the project item's type.
+
+
+   .. py:method:: make_specification(definition, app_settings, logger)
+      :staticmethod:
+      :abstractmethod:
+
+      Makes a project item specification.
+
+      :param definition: specification's definition dictionary
+      :type definition: dict
+      :param app_settings: Toolbox settings
+      :type app_settings: QSettings
+      :param logger: a logger
+      :type logger: LoggerInterface
+
+      :returns: a specification built from the given definition
+      :rtype: ProjectItemSpecification
+
+
+
```

### Comparing `spine_engine-0.23.3/docs/source/autoapi/spine_engine/project_item_loader/index.rst` & `spine_engine-0.23.4/docs/source/autoapi/spine_engine/project_item_loader/index.rst`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,74 +1,74 @@
-:py:mod:`spine_engine.project_item_loader`
-==========================================
-
-.. py:module:: spine_engine.project_item_loader
-
-.. autoapi-nested-parse::
-
-   Contains :class:`ProjectItemLoader`.
-
-   :author: A. Soininen (VTT)
-   :date:   11.2.2021
-
-
-
-Module Contents
----------------
-
-Classes
-~~~~~~~
-
-.. autoapisummary::
-
-   spine_engine.project_item_loader.ProjectItemLoader
-
-
-
-
-.. py:class:: ProjectItemLoader
-
-   A singleton class for loading project items from multiple processes simultaneously.
-
-   .. py:attribute:: _specification_factories
-      
-
-      
-
-   .. py:attribute:: _executable_item_classes
-      
-
-      
-
-   .. py:attribute:: _specification_factories_lock
-      
-
-      
-
-   .. py:attribute:: _executable_item_classes_lock
-      
-
-      
-
-   .. py:method:: load_item_specification_factories(self, items_module_name)
-
-      Loads the project item specification factories in the standard Toolbox package.
-
-      :param items_module_name: name of the Python module that contains the project items
-      :type items_module_name: str
-
-      :returns: a map from item type to specification factory
-      :rtype: dict
-
-
-   .. py:method:: load_executable_item_classes(self, items_module_name)
-
-      Loads the project item executable classes included in the standard Toolbox package.
-
-      :param items_module_name: name of the Python module that contains the project items
-      :type items_module_name: str
-
-      :returns: a map from item type to the executable item class
-      :rtype: dict
-
-
-
+:py:mod:`spine_engine.project_item_loader`
+==========================================
+
+.. py:module:: spine_engine.project_item_loader
+
+.. autoapi-nested-parse::
+
+   Contains :class:`ProjectItemLoader`.
+
+   :author: A. Soininen (VTT)
+   :date:   11.2.2021
+
+
+
+Module Contents
+---------------
+
+Classes
+~~~~~~~
+
+.. autoapisummary::
+
+   spine_engine.project_item_loader.ProjectItemLoader
+
+
+
+
+.. py:class:: ProjectItemLoader
+
+   A singleton class for loading project items from multiple processes simultaneously.
+
+   .. py:attribute:: _specification_factories
+      
+
+      
+
+   .. py:attribute:: _executable_item_classes
+      
+
+      
+
+   .. py:attribute:: _specification_factories_lock
+      
+
+      
+
+   .. py:attribute:: _executable_item_classes_lock
+      
+
+      
+
+   .. py:method:: load_item_specification_factories(self, items_module_name)
+
+      Loads the project item specification factories in the standard Toolbox package.
+
+      :param items_module_name: name of the Python module that contains the project items
+      :type items_module_name: str
+
+      :returns: a map from item type to specification factory
+      :rtype: dict
+
+
+   .. py:method:: load_executable_item_classes(self, items_module_name)
+
+      Loads the project item executable classes included in the standard Toolbox package.
+
+      :param items_module_name: name of the Python module that contains the project items
+      :type items_module_name: str
+
+      :returns: a map from item type to the executable item class
+      :rtype: dict
+
+
+
```

### Comparing `spine_engine-0.23.3/docs/source/autoapi/spine_engine/utils/command_line_arguments/index.rst` & `spine_engine-0.23.4/docs/source/autoapi/spine_engine/utils/command_line_arguments/index.rst`

 * *Ordering differences only*

 * *Files 25% similar despite different names*

```diff
@@ -1,42 +1,42 @@
-:py:mod:`spine_engine.utils.command_line_arguments`
-===================================================
-
-.. py:module:: spine_engine.utils.command_line_arguments
-
-.. autoapi-nested-parse::
-
-   Split command line arguments.
-
-   :authors: P. Savolainen (VTT)
-   :date:   10.1.2018
-
-
-
-Module Contents
----------------
-
-
-Functions
-~~~~~~~~~
-
-.. autoapisummary::
-
-   spine_engine.utils.command_line_arguments.split_cmdline_args
-
-
-
-.. py:function:: split_cmdline_args(arg_string)
-
-   Splits a string of command line arguments into a list of tokens.
-
-   Things in single ('') and double ("") quotes are kept as single tokens
-   while the quotes themselves are stripped away.
-   Thus, `--file="a long quoted 'file' name.txt` becomes ["--file=a long quoted 'file' name.txt"]
-
-   :param arg_string: command line arguments as a string
-   :type arg_string: str
-
-   :returns: a list of tokens
-   :rtype: list
-
-
+:py:mod:`spine_engine.utils.command_line_arguments`
+===================================================
+
+.. py:module:: spine_engine.utils.command_line_arguments
+
+.. autoapi-nested-parse::
+
+   Split command line arguments.
+
+   :authors: P. Savolainen (VTT)
+   :date:   10.1.2018
+
+
+
+Module Contents
+---------------
+
+
+Functions
+~~~~~~~~~
+
+.. autoapisummary::
+
+   spine_engine.utils.command_line_arguments.split_cmdline_args
+
+
+
+.. py:function:: split_cmdline_args(arg_string)
+
+   Splits a string of command line arguments into a list of tokens.
+
+   Things in single ('') and double ("") quotes are kept as single tokens
+   while the quotes themselves are stripped away.
+   Thus, `--file="a long quoted 'file' name.txt` becomes ["--file=a long quoted 'file' name.txt"]
+
+   :param arg_string: command line arguments as a string
+   :type arg_string: str
+
+   :returns: a list of tokens
+   :rtype: list
+
+
```

### Comparing `spine_engine-0.23.3/docs/source/autoapi/spine_engine/utils/helpers/index.rst` & `spine_engine-0.23.4/docs/source/autoapi/spine_engine/utils/helpers/index.rst`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,167 +1,167 @@
-:py:mod:`spine_engine.utils.helpers`
-====================================
-
-.. py:module:: spine_engine.utils.helpers
-
-.. autoapi-nested-parse::
-
-   Helpers functions and classes.
-
-   :authors: M. Marin (KTH)
-   :date:   20.11.2019
-
-
-
-Module Contents
----------------
-
-Classes
-~~~~~~~
-
-.. autoapisummary::
-
-   spine_engine.utils.helpers.Singleton
-   spine_engine.utils.helpers.AppSettings
-
-
-
-Functions
-~~~~~~~~~
-
-.. autoapisummary::
-
-   spine_engine.utils.helpers.shorten
-   spine_engine.utils.helpers.create_log_file_timestamp
-   spine_engine.utils.helpers.create_timestamp
-   spine_engine.utils.helpers.resolve_conda_executable
-   spine_engine.utils.helpers.resolve_python_interpreter
-   spine_engine.utils.helpers.resolve_julia_executable
-   spine_engine.utils.helpers.resolve_gams_executable
-   spine_engine.utils.helpers.resolve_executable_from_path
-   spine_engine.utils.helpers.inverted
-   spine_engine.utils.helpers.get_julia_command
-   spine_engine.utils.helpers.get_julia_env
-
-
-
-.. py:class:: Singleton
-
-   Bases: :py:obj:`type`
-
-   .. py:attribute:: _instances
-      
-
-      
-
-   .. py:method:: __call__(cls, *args, **kwargs)
-
-      Call self as a function.
-
-
-
-.. py:class:: AppSettings(settings)
-
-   A QSettings replacement.
-
-   Init.
-
-   :param settings:
-   :type settings: dict
-
-   .. py:method:: value(self, key, defaultValue='')
-
-
-
-.. py:function:: shorten(name)
-
-   Returns the 'short name' version of given name.
-
-
-.. py:function:: create_log_file_timestamp()
-
-   Creates a new timestamp string that is used as Data Store and Importer error log file.
-
-   :returns: Timestamp string or empty string if failed.
-
-
-.. py:function:: create_timestamp()
-
-
-.. py:function:: resolve_conda_executable(conda_path)
-
-   If given conda_path is an empty str, returns current Conda
-   executable from CONDA_EXE env variable if the app was started
-   on Conda, otherwise returns an empty string.
-
-
-.. py:function:: resolve_python_interpreter(python_path)
-
-   If given python_path is empty, returns the
-   full path to Python interpreter depending on user's
-   settings and whether the app is frozen or not.
-
-
-.. py:function:: resolve_julia_executable(julia_path)
-
-   if given julia_path is empty, tries to find the path to Julia
-   in user's PATH env variable. If Julia is not found in PATH,
-   returns an empty string.
-
-   Note: In the long run, we should decide whether this is something we want to do
-   because adding julia-x.x./bin/ dir to the PATH is not recommended because this
-   also exposes some .dlls to other programs on user's (windows) system. I.e. it
-   may break other programs, and this is why the Julia installer does not
-   add (and does not even offer the chance to add) Julia to PATH.
-
-
-.. py:function:: resolve_gams_executable(gams_path)
-
-   if given gams_path is empty, tries to find the path to Gams
-   in user's PATH env variable. If Gams is not found in PATH,
-   returns an empty string.
-
-
-.. py:function:: resolve_executable_from_path(executable_name)
-
-   Returns full path to executable name in user's
-   PATH env variable. If not found, returns an empty string.
-
-   Basically equivalent to 'where' and 'which' commands in
-   cmd.exe and bash respectively.
-
-   :param executable_name: Executable filename to find (e.g. python.exe, julia.exe)
-   :type executable_name: str
-
-   :returns: Full path or empty string
-   :rtype: str
-
-
-.. py:function:: inverted(input_)
-
-   Inverts a dictionary of list values.
-
-   :param input_:
-   :type input_: dict
-
-   :returns: keys are list items, and values are keys listing that item from the input dictionary
-   :rtype: dict
-
-
-.. py:function:: get_julia_command(settings)
-
-   :param settings:
-   :type settings: QSettings, AppSettings
-
-   :returns: e.g. ["path/to/julia", "--project=path/to/project/"]
-   :rtype: list
-
-
-.. py:function:: get_julia_env(settings)
-
-   :param settings:
-   :type settings: QSettings, AppSettings
-
-   :returns: (julia_exe, julia_project), or None if none found
-   :rtype: tuple, NoneType
-
-
+:py:mod:`spine_engine.utils.helpers`
+====================================
+
+.. py:module:: spine_engine.utils.helpers
+
+.. autoapi-nested-parse::
+
+   Helpers functions and classes.
+
+   :authors: M. Marin (KTH)
+   :date:   20.11.2019
+
+
+
+Module Contents
+---------------
+
+Classes
+~~~~~~~
+
+.. autoapisummary::
+
+   spine_engine.utils.helpers.Singleton
+   spine_engine.utils.helpers.AppSettings
+
+
+
+Functions
+~~~~~~~~~
+
+.. autoapisummary::
+
+   spine_engine.utils.helpers.shorten
+   spine_engine.utils.helpers.create_log_file_timestamp
+   spine_engine.utils.helpers.create_timestamp
+   spine_engine.utils.helpers.resolve_conda_executable
+   spine_engine.utils.helpers.resolve_python_interpreter
+   spine_engine.utils.helpers.resolve_julia_executable
+   spine_engine.utils.helpers.resolve_gams_executable
+   spine_engine.utils.helpers.resolve_executable_from_path
+   spine_engine.utils.helpers.inverted
+   spine_engine.utils.helpers.get_julia_command
+   spine_engine.utils.helpers.get_julia_env
+
+
+
+.. py:class:: Singleton
+
+   Bases: :py:obj:`type`
+
+   .. py:attribute:: _instances
+      
+
+      
+
+   .. py:method:: __call__(cls, *args, **kwargs)
+
+      Call self as a function.
+
+
+
+.. py:class:: AppSettings(settings)
+
+   A QSettings replacement.
+
+   Init.
+
+   :param settings:
+   :type settings: dict
+
+   .. py:method:: value(self, key, defaultValue='')
+
+
+
+.. py:function:: shorten(name)
+
+   Returns the 'short name' version of given name.
+
+
+.. py:function:: create_log_file_timestamp()
+
+   Creates a new timestamp string that is used as Data Store and Importer error log file.
+
+   :returns: Timestamp string or empty string if failed.
+
+
+.. py:function:: create_timestamp()
+
+
+.. py:function:: resolve_conda_executable(conda_path)
+
+   If given conda_path is an empty str, returns current Conda
+   executable from CONDA_EXE env variable if the app was started
+   on Conda, otherwise returns an empty string.
+
+
+.. py:function:: resolve_python_interpreter(python_path)
+
+   If given python_path is empty, returns the
+   full path to Python interpreter depending on user's
+   settings and whether the app is frozen or not.
+
+
+.. py:function:: resolve_julia_executable(julia_path)
+
+   if given julia_path is empty, tries to find the path to Julia
+   in user's PATH env variable. If Julia is not found in PATH,
+   returns an empty string.
+
+   Note: In the long run, we should decide whether this is something we want to do
+   because adding julia-x.x./bin/ dir to the PATH is not recommended because this
+   also exposes some .dlls to other programs on user's (windows) system. I.e. it
+   may break other programs, and this is why the Julia installer does not
+   add (and does not even offer the chance to add) Julia to PATH.
+
+
+.. py:function:: resolve_gams_executable(gams_path)
+
+   if given gams_path is empty, tries to find the path to Gams
+   in user's PATH env variable. If Gams is not found in PATH,
+   returns an empty string.
+
+
+.. py:function:: resolve_executable_from_path(executable_name)
+
+   Returns full path to executable name in user's
+   PATH env variable. If not found, returns an empty string.
+
+   Basically equivalent to 'where' and 'which' commands in
+   cmd.exe and bash respectively.
+
+   :param executable_name: Executable filename to find (e.g. python.exe, julia.exe)
+   :type executable_name: str
+
+   :returns: Full path or empty string
+   :rtype: str
+
+
+.. py:function:: inverted(input_)
+
+   Inverts a dictionary of list values.
+
+   :param input_:
+   :type input_: dict
+
+   :returns: keys are list items, and values are keys listing that item from the input dictionary
+   :rtype: dict
+
+
+.. py:function:: get_julia_command(settings)
+
+   :param settings:
+   :type settings: QSettings, AppSettings
+
+   :returns: e.g. ["path/to/julia", "--project=path/to/project/"]
+   :rtype: list
+
+
+.. py:function:: get_julia_env(settings)
+
+   :param settings:
+   :type settings: QSettings, AppSettings
+
+   :returns: (julia_exe, julia_project), or None if none found
+   :rtype: tuple, NoneType
+
+
```

### Comparing `spine_engine-0.23.3/docs/source/autoapi/spine_engine/utils/queue_logger/index.rst` & `spine_engine-0.23.4/docs/source/autoapi/spine_engine/utils/queue_logger/index.rst`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,69 +1,69 @@
-:py:mod:`spine_engine.utils.queue_logger`
-=========================================
-
-.. py:module:: spine_engine.utils.queue_logger
-
-.. autoapi-nested-parse::
-
-   The QueueLogger class.
-
-   :authors: M. Marin (KTH)
-   :date:   3.11.2020
-
-
-
-Module Contents
----------------
-
-Classes
-~~~~~~~
-
-.. autoapisummary::
-
-   spine_engine.utils.queue_logger._Message
-   spine_engine.utils.queue_logger._Prompt
-   spine_engine.utils.queue_logger._ExecutionMessage
-   spine_engine.utils.queue_logger.QueueLogger
-
-
-
-
-.. py:class:: _Message(queue, event_type, msg_type, item_name)
-
-   .. py:method:: filter_id(self)
-      :property:
-
-
-   .. py:method:: emit(self, msg_text)
-
-
-
-.. py:class:: _Prompt(queue, item_name, prompt_queue)
-
-   .. py:method:: filter_id(self)
-      :property:
-
-
-   .. py:method:: emit(self, prompt)
-
-
-
-.. py:class:: _ExecutionMessage(queue, event_type, item_name)
-
-   .. py:method:: filter_id(self)
-      :property:
-
-
-   .. py:method:: emit(self, msg)
-
-
-
-.. py:class:: QueueLogger(queue, item_name, prompt_queue)
-
-   A :class:`LoggerInterface` compliant logger that puts messages into a Queue.
-
-
-   .. py:method:: set_filter_id(self, filter_id)
-
-
-
+:py:mod:`spine_engine.utils.queue_logger`
+=========================================
+
+.. py:module:: spine_engine.utils.queue_logger
+
+.. autoapi-nested-parse::
+
+   The QueueLogger class.
+
+   :authors: M. Marin (KTH)
+   :date:   3.11.2020
+
+
+
+Module Contents
+---------------
+
+Classes
+~~~~~~~
+
+.. autoapisummary::
+
+   spine_engine.utils.queue_logger._Message
+   spine_engine.utils.queue_logger._Prompt
+   spine_engine.utils.queue_logger._ExecutionMessage
+   spine_engine.utils.queue_logger.QueueLogger
+
+
+
+
+.. py:class:: _Message(queue, event_type, msg_type, item_name)
+
+   .. py:method:: filter_id(self)
+      :property:
+
+
+   .. py:method:: emit(self, msg_text)
+
+
+
+.. py:class:: _Prompt(queue, item_name, prompt_queue)
+
+   .. py:method:: filter_id(self)
+      :property:
+
+
+   .. py:method:: emit(self, prompt)
+
+
+
+.. py:class:: _ExecutionMessage(queue, event_type, item_name)
+
+   .. py:method:: filter_id(self)
+      :property:
+
+
+   .. py:method:: emit(self, msg)
+
+
+
+.. py:class:: QueueLogger(queue, item_name, prompt_queue)
+
+   A :class:`LoggerInterface` compliant logger that puts messages into a Queue.
+
+
+   .. py:method:: set_filter_id(self, filter_id)
+
+
+
```

### Comparing `spine_engine-0.23.3/docs/source/autoapi/spine_engine/utils/returning_process/index.rst` & `spine_engine-0.23.4/docs/source/autoapi/spine_engine/utils/returning_process/index.rst`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,54 +1,54 @@
-:py:mod:`spine_engine.utils.returning_process`
-==============================================
-
-.. py:module:: spine_engine.utils.returning_process
-
-.. autoapi-nested-parse::
-
-   The ReturningProcess class.
-
-   :authors: M. Marin (KTH)
-   :date:    3.11.2020
-
-
-
-Module Contents
----------------
-
-Classes
-~~~~~~~
-
-.. autoapisummary::
-
-   spine_engine.utils.returning_process.ReturningProcess
-
-
-
-
-.. py:class:: ReturningProcess(*args, **kwargs)
-
-   Bases: :py:obj:`multiprocessing.Process`
-
-   Process objects represent activity that is run in a separate process
-
-   The class is analogous to `threading.Thread`
-
-   .. py:method:: run_until_complete(self)
-
-      Starts the process and joins it after it has finished.
-
-      :returns: Return value of the process where the first element is a status flag
-      :rtype: tuple
-
-
-   .. py:method:: run(self)
-
-      Method to be run in sub-process; can be overridden in sub-class
-
-
-   .. py:method:: terminate(self)
-
-      Terminate process; sends SIGTERM signal or uses TerminateProcess()
-
-
-
+:py:mod:`spine_engine.utils.returning_process`
+==============================================
+
+.. py:module:: spine_engine.utils.returning_process
+
+.. autoapi-nested-parse::
+
+   The ReturningProcess class.
+
+   :authors: M. Marin (KTH)
+   :date:    3.11.2020
+
+
+
+Module Contents
+---------------
+
+Classes
+~~~~~~~
+
+.. autoapisummary::
+
+   spine_engine.utils.returning_process.ReturningProcess
+
+
+
+
+.. py:class:: ReturningProcess(*args, **kwargs)
+
+   Bases: :py:obj:`multiprocessing.Process`
+
+   Process objects represent activity that is run in a separate process
+
+   The class is analogous to `threading.Thread`
+
+   .. py:method:: run_until_complete(self)
+
+      Starts the process and joins it after it has finished.
+
+      :returns: Return value of the process where the first element is a status flag
+      :rtype: tuple
+
+
+   .. py:method:: run(self)
+
+      Method to be run in sub-process; can be overridden in sub-class
+
+
+   .. py:method:: terminate(self)
+
+      Terminate process; sends SIGTERM signal or uses TerminateProcess()
+
+
+
```

### Comparing `spine_engine-0.23.3/docs/source/autoapi/spine_engine/utils/serialization/index.rst` & `spine_engine-0.23.4/docs/source/autoapi/spine_engine/utils/serialization/index.rst`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,82 +1,82 @@
-:py:mod:`spine_engine.utils.serialization`
-==========================================
-
-.. py:module:: spine_engine.utils.serialization
-
-.. autoapi-nested-parse::
-
-   Functions to (de)serialize stuff.
-
-   :authors: P. Savolainen (VTT)
-   :date:   10.1.2018
-
-
-
-Module Contents
----------------
-
-
-Functions
-~~~~~~~~~
-
-.. autoapisummary::
-
-   spine_engine.utils.serialization.path_in_dir
-   spine_engine.utils.serialization.serialize_path
-   spine_engine.utils.serialization.serialize_url
-   spine_engine.utils.serialization.deserialize_path
-   spine_engine.utils.serialization.deserialize_remote_path
-
-
-
-.. py:function:: path_in_dir(path, directory)
-
-   Returns True if the given path is in the given directory.
-
-
-.. py:function:: serialize_path(path, project_dir)
-
-   Returns a dict representation of the given path.
-
-   If path is in project_dir, converts the path to relative.
-
-   :param path: path to serialize
-   :type path: str
-   :param project_dir: path to the project directory
-   :type project_dir: str
-
-   :returns: Dictionary representing the given path
-   :rtype: dict
-
-
-.. py:function:: serialize_url(url, project_dir)
-
-   Return a dict representation of the given URL.
-
-   If the URL is a file that is in project dir, the URL is converted to a relative path.
-
-   :param url: a URL to serialize
-   :type url: str
-   :param project_dir: path to the project directory
-   :type project_dir: str
-
-   :returns: Dictionary representing the URL
-   :rtype: dict
-
-
-.. py:function:: deserialize_path(serialized, project_dir)
-
-   Returns a deserialized path or URL.
-
-   :param serialized: a serialized path or URL
-   :type serialized: dict
-   :param project_dir: path to the project directory
-   :type project_dir: str
-
-   :returns: Path or URL as string
-   :rtype: str
-
-
-.. py:function:: deserialize_remote_path(serialized, base_path)
-
-
+:py:mod:`spine_engine.utils.serialization`
+==========================================
+
+.. py:module:: spine_engine.utils.serialization
+
+.. autoapi-nested-parse::
+
+   Functions to (de)serialize stuff.
+
+   :authors: P. Savolainen (VTT)
+   :date:   10.1.2018
+
+
+
+Module Contents
+---------------
+
+
+Functions
+~~~~~~~~~
+
+.. autoapisummary::
+
+   spine_engine.utils.serialization.path_in_dir
+   spine_engine.utils.serialization.serialize_path
+   spine_engine.utils.serialization.serialize_url
+   spine_engine.utils.serialization.deserialize_path
+   spine_engine.utils.serialization.deserialize_remote_path
+
+
+
+.. py:function:: path_in_dir(path, directory)
+
+   Returns True if the given path is in the given directory.
+
+
+.. py:function:: serialize_path(path, project_dir)
+
+   Returns a dict representation of the given path.
+
+   If path is in project_dir, converts the path to relative.
+
+   :param path: path to serialize
+   :type path: str
+   :param project_dir: path to the project directory
+   :type project_dir: str
+
+   :returns: Dictionary representing the given path
+   :rtype: dict
+
+
+.. py:function:: serialize_url(url, project_dir)
+
+   Return a dict representation of the given URL.
+
+   If the URL is a file that is in project dir, the URL is converted to a relative path.
+
+   :param url: a URL to serialize
+   :type url: str
+   :param project_dir: path to the project directory
+   :type project_dir: str
+
+   :returns: Dictionary representing the URL
+   :rtype: dict
+
+
+.. py:function:: deserialize_path(serialized, project_dir)
+
+   Returns a deserialized path or URL.
+
+   :param serialized: a serialized path or URL
+   :type serialized: dict
+   :param project_dir: path to the project directory
+   :type project_dir: str
+
+   :returns: Path or URL as string
+   :rtype: str
+
+
+.. py:function:: deserialize_remote_path(serialized, base_path)
+
+
```

### Comparing `spine_engine-0.23.3/docs/source/conf.py` & `spine_engine-0.23.4/docs/source/conf.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,185 +1,185 @@
-# -*- coding: utf-8 -*-
-#
-# Configuration file for the Sphinx documentation builder.
-#
-# This file does only contain a selection of the most common options. For a
-# full list see the documentation:
-# http://www.sphinx-doc.org/en/master/config
-
-# -- Path setup --------------------------------------------------------------
-
-# If extensions (or modules to document with autodoc) are in another directory,
-# add these directories to sys.path here. If the directory is relative to the
-# documentation root, use os.path.abspath to make it absolute, like shown here.
-#
-
-import os
-import sys
-
-app_path = os.path.join(os.path.dirname(__file__), os.path.pardir, os.path.pardir)
-sys.path.insert(0, os.path.abspath(app_path))
-
-from spine_engine.version import __version__ as version
-
-# -- Project information -----------------------------------------------------
-
-project = 'Spine Engine'
-author = 'Spine project consortium'
-copyright = '2017-2021 {}'.format(author)
-
-
-# The full version, including alpha/beta/rc tags
-release = version
-
-
-# -- General configuration ---------------------------------------------------
-
-# If your documentation needs a minimal Sphinx version, state it here.
-#
-# needs_sphinx = '1.0'
-
-# Add any Sphinx extension module names here, as strings. They can be
-# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
-# ones.
-extensions = [
-    'sphinx.ext.todo',
-    'sphinx.ext.coverage',
-    'sphinx.ext.ifconfig',
-    'sphinx.ext.viewcode',
-    'sphinx.ext.githubpages',
-    'sphinx.ext.napoleon',
-    'sphinx.ext.intersphinx',
-    'recommonmark',
-    'autoapi.extension'
-]
-
-# Add any paths that contain templates here, relative to this directory.
-templates_path = ['_templates']
-
-# The suffix(es) of source filenames and their file types.
-source_suffix = {'.rst': 'restructuredtext', '.md': 'markdown'}
-
-# The master toctree document.
-master_doc = 'index'
-
-# The language for content autogenerated by Sphinx. Refer to documentation
-# for a list of supported languages.
-#
-# This is also used if you do content translation via gettext catalogs.
-# Usually you set "language" from the command line for these cases.
-language = None
-
-# List of patterns, relative to source directory, that match files and
-# directories to ignore when looking for source files.
-# This pattern also affects html_static_path and html_extra_path .
-exclude_patterns = []
-
-# The name of the Pygments (syntax highlighting) style to use.
-pygments_style = 'sphinx'
-
-# Settings for Sphinx AutoAPI
-autoapi_python_class_content = "both"
-autoapi_add_toctree_entry = True
-autoapi_root = "autoapi"
-autoapi_dirs = ['../../spine_engine']  # package to be documented
-autoapi_ignore = [
-]  # ignored modules
-autoapi_keep_files=True
-
-# -- Options for HTML output -------------------------------------------------
-
-# The theme to use for HTML and HTML Help pages.  See the documentation for
-# a list of builtin themes.
-#
-html_theme = 'sphinx_rtd_theme'
-
-# Theme options are theme-specific and customize the look and feel of a theme
-# further.  For a list of options available for each theme, see the
-# documentation.
-#
-# html_theme_options = {}
-
-# Add any paths that contain custom static files (such as style sheets) here,
-# relative to this directory. They are copied after the builtin static files,
-# so a file named "default.css" will overwrite the builtin "default.css".
-html_static_path = []
-
-# Custom sidebar templates, must be a dictionary that maps document names
-# to template names.
-#
-# The default sidebars (for documents that don't match any pattern) are
-# defined by theme itself.  Builtin themes are using these templates by
-# default: ``['localtoc.html', 'relations.html', 'sourcelink.html',
-# 'searchbox.html']``.
-# html_sidebars = {}
-
-# -- Options for HTMLHelp output ---------------------------------------------
-
-# Output file base name for HTML help builder.
-htmlhelp_basename = 'SpineEngineDoc'
-
-
-# -- Options for LaTeX output ------------------------------------------------
-
-latex_elements = {
-    # The paper size ('letterpaper' or 'a4paper').
-    #
-    # 'papersize': 'letterpaper',
-    # The font size ('10pt', '11pt' or '12pt').
-    #
-    # 'pointsize': '10pt',
-    # Additional stuff for the LaTeX preamble.
-    #
-    # 'preamble': '',
-    # Latex figure (float) alignment
-    #
-    # 'figure_align': 'htbp',
-}
-
-# Grouping the document tree into LaTeX files. List of tuples
-# (source start file, target name, title,
-#  author, documentclass [howto, manual, or own class]).
-latex_documents = [
-    (
-        master_doc,
-        'SpineEngine.tex',
-        'Spine Engine Documentation',
-        'Spine project consortium',
-        'manual',
-    )
-]
-
-
-# -- Options for manual page output ------------------------------------------
-
-# One entry per manual page. List of tuples
-# (source start file, name, description, authors, manual section).
-man_pages = [(master_doc, 'spine_engine', 'Spine Engine Documentation', [author], 1)]
-
-
-# -- Options for Texinfo output ----------------------------------------------
-
-# Grouping the document tree into Texinfo files. List of tuples
-# (source start file, target name, title, author,
-#  dir menu entry, description, category)
-texinfo_documents = [
-    (
-        master_doc,
-        'SpineEngine',
-        'Spine Engine Documentation',
-        author,
-        'SpineEngine',
-        'One line description of project.',
-        'Miscellaneous',
-    )
-]
-
-
-# -- Extension configuration -------------------------------------------------
-
-# -- Options for todo extension ----------------------------------------------
-
-# If true, `todo` and `todoList` produce output, else they produce nothing.
-todo_include_todos = True
-
-autodoc_member_order = "bysource"
+# -*- coding: utf-8 -*-
+#
+# Configuration file for the Sphinx documentation builder.
+#
+# This file does only contain a selection of the most common options. For a
+# full list see the documentation:
+# http://www.sphinx-doc.org/en/master/config
+
+# -- Path setup --------------------------------------------------------------
+
+# If extensions (or modules to document with autodoc) are in another directory,
+# add these directories to sys.path here. If the directory is relative to the
+# documentation root, use os.path.abspath to make it absolute, like shown here.
+#
+
+import os
+import sys
+
+app_path = os.path.join(os.path.dirname(__file__), os.path.pardir, os.path.pardir)
+sys.path.insert(0, os.path.abspath(app_path))
+
+from spine_engine.version import __version__ as version
+
+# -- Project information -----------------------------------------------------
+
+project = 'Spine Engine'
+author = 'Spine project consortium'
+copyright = '2017-2021 {}'.format(author)
+
+
+# The full version, including alpha/beta/rc tags
+release = version
+
+
+# -- General configuration ---------------------------------------------------
+
+# If your documentation needs a minimal Sphinx version, state it here.
+#
+# needs_sphinx = '1.0'
+
+# Add any Sphinx extension module names here, as strings. They can be
+# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
+# ones.
+extensions = [
+    'sphinx.ext.todo',
+    'sphinx.ext.coverage',
+    'sphinx.ext.ifconfig',
+    'sphinx.ext.viewcode',
+    'sphinx.ext.githubpages',
+    'sphinx.ext.napoleon',
+    'sphinx.ext.intersphinx',
+    'recommonmark',
+    'autoapi.extension'
+]
+
+# Add any paths that contain templates here, relative to this directory.
+templates_path = ['_templates']
+
+# The suffix(es) of source filenames and their file types.
+source_suffix = {'.rst': 'restructuredtext', '.md': 'markdown'}
+
+# The master toctree document.
+master_doc = 'index'
+
+# The language for content autogenerated by Sphinx. Refer to documentation
+# for a list of supported languages.
+#
+# This is also used if you do content translation via gettext catalogs.
+# Usually you set "language" from the command line for these cases.
+language = None
+
+# List of patterns, relative to source directory, that match files and
+# directories to ignore when looking for source files.
+# This pattern also affects html_static_path and html_extra_path .
+exclude_patterns = []
+
+# The name of the Pygments (syntax highlighting) style to use.
+pygments_style = 'sphinx'
+
+# Settings for Sphinx AutoAPI
+autoapi_python_class_content = "both"
+autoapi_add_toctree_entry = True
+autoapi_root = "autoapi"
+autoapi_dirs = ['../../spine_engine']  # package to be documented
+autoapi_ignore = [
+]  # ignored modules
+autoapi_keep_files=True
+
+# -- Options for HTML output -------------------------------------------------
+
+# The theme to use for HTML and HTML Help pages.  See the documentation for
+# a list of builtin themes.
+#
+html_theme = 'sphinx_rtd_theme'
+
+# Theme options are theme-specific and customize the look and feel of a theme
+# further.  For a list of options available for each theme, see the
+# documentation.
+#
+# html_theme_options = {}
+
+# Add any paths that contain custom static files (such as style sheets) here,
+# relative to this directory. They are copied after the builtin static files,
+# so a file named "default.css" will overwrite the builtin "default.css".
+html_static_path = []
+
+# Custom sidebar templates, must be a dictionary that maps document names
+# to template names.
+#
+# The default sidebars (for documents that don't match any pattern) are
+# defined by theme itself.  Builtin themes are using these templates by
+# default: ``['localtoc.html', 'relations.html', 'sourcelink.html',
+# 'searchbox.html']``.
+# html_sidebars = {}
+
+# -- Options for HTMLHelp output ---------------------------------------------
+
+# Output file base name for HTML help builder.
+htmlhelp_basename = 'SpineEngineDoc'
+
+
+# -- Options for LaTeX output ------------------------------------------------
+
+latex_elements = {
+    # The paper size ('letterpaper' or 'a4paper').
+    #
+    # 'papersize': 'letterpaper',
+    # The font size ('10pt', '11pt' or '12pt').
+    #
+    # 'pointsize': '10pt',
+    # Additional stuff for the LaTeX preamble.
+    #
+    # 'preamble': '',
+    # Latex figure (float) alignment
+    #
+    # 'figure_align': 'htbp',
+}
+
+# Grouping the document tree into LaTeX files. List of tuples
+# (source start file, target name, title,
+#  author, documentclass [howto, manual, or own class]).
+latex_documents = [
+    (
+        master_doc,
+        'SpineEngine.tex',
+        'Spine Engine Documentation',
+        'Spine project consortium',
+        'manual',
+    )
+]
+
+
+# -- Options for manual page output ------------------------------------------
+
+# One entry per manual page. List of tuples
+# (source start file, name, description, authors, manual section).
+man_pages = [(master_doc, 'spine_engine', 'Spine Engine Documentation', [author], 1)]
+
+
+# -- Options for Texinfo output ----------------------------------------------
+
+# Grouping the document tree into Texinfo files. List of tuples
+# (source start file, target name, title, author,
+#  dir menu entry, description, category)
+texinfo_documents = [
+    (
+        master_doc,
+        'SpineEngine',
+        'Spine Engine Documentation',
+        author,
+        'SpineEngine',
+        'One line description of project.',
+        'Miscellaneous',
+    )
+]
+
+
+# -- Extension configuration -------------------------------------------------
+
+# -- Options for todo extension ----------------------------------------------
+
+# If true, `todo` and `todoList` produce output, else they produce nothing.
+todo_include_todos = True
+
+autodoc_member_order = "bysource"
```

### Comparing `spine_engine-0.23.3/fig/eu-emblem-low-res.jpg` & `spine_engine-0.23.4/fig/eu-emblem-low-res.jpg`

 * *Files identical despite different names*

### Comparing `spine_engine-0.23.3/fig/spineengine_logo.svg` & `spine_engine-0.23.4/fig/spineengine_logo.svg`

 * *Files identical despite different names*

### Comparing `spine_engine-0.23.3/fig/spineengine_on_wht.svg` & `spine_engine-0.23.4/fig/spineengine_on_wht.svg`

 * *Files identical despite different names*

### Comparing `spine_engine-0.23.3/pylintrc` & `spine_engine-0.23.4/pylintrc`

 * *Files identical despite different names*

### Comparing `spine_engine-0.23.3/pyproject.toml` & `spine_engine-0.23.4/pyproject.toml`

 * *Files 17% similar despite different names*

```diff
@@ -1,70 +1,70 @@
-[project]
-name = "spine_engine"
-dynamic = ["version"]
-authors = [{name = "Spine Project consortium", email = "spine_info@vtt.fi"}]
-license = {text = "LGPL-3.0-or-later"}
-description = "A package to run Spine workflows."
-keywords = ["energy system modelling", "workflow", "optimisation", "database"]
-readme = {file = "README.md", content-type = "text/markdown"}
-classifiers = [
-	    "Programming Language :: Python :: 3",
-	    "License :: OSI Approved :: GNU Lesser General Public License v3 (LGPLv3)",
-	    "Operating System :: OS Independent",
-]
-requires-python = ">=3.8.1, <3.12"
-dependencies = [
-    # dagster >= 0.12.9 requires alembic that is incompatible with spinedb_api
-    "dagster>=0.12.6, <0.12.9",
-    # dagster versions lower that 1.5.7 do not support pendulum >= 3.0.0
-    "pendulum < 3.0.0",
-    # https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
-    "protobuf<3.21.0",
-    "networkx>2.5.1",
-    "datapackage>=1.15.2, <1.16",
-    "jupyter_client>=6.0",
-    "spinedb_api>=0.30.3",
-    "pyzmq >=21.0",
-    # dagster 0.12.8 requires Jinja2<3.0, which tries to import
-    # soft_unicode, which has been removed in markupsafe 2.1
-    "markupsafe < 2.1",
-]
-
-[project.urls]
-Repository = "https://github.com/spine-tools/spine-engine"
-
-[project.optional-dependencies]
-dev = ["coverage[toml]"]
-
-[build-system]
-requires = ["setuptools>=64", "setuptools_scm[toml]>=6.2", "wheel", "build"]
-build-backend = "setuptools.build_meta"
-
-[tool.setuptools_scm]
-write_to = "spine_engine/version.py"
-version_scheme = "release-branch-semver"
-
-[tool.setuptools]
-zip-safe = false
-
-[tool.setuptools.package-data]
-spine_engine = ["execution_managers/spine_repl.jl"]
-
-[tool.setuptools.packages.find]
-exclude = [
-	"bin*",
-	"docs*",
-	"fig*",
-	"tests*",
-]
-
-[tool.coverage.run]
-source = ["spine_engine"]
-branch = true
-
-[tool.coverage.report]
-ignore_errors = true
-
-[tool.black]
-line-length = 120
-skip-string-normalization = true
-exclude = '\.git'
+[project]
+name = "spine_engine"
+dynamic = ["version"]
+authors = [{name = "Spine Project consortium", email = "spine_info@vtt.fi"}]
+license = {text = "LGPL-3.0-or-later"}
+description = "A package to run Spine workflows."
+keywords = ["energy system modelling", "workflow", "optimisation", "database"]
+readme = {file = "README.md", content-type = "text/markdown"}
+classifiers = [
+	    "Programming Language :: Python :: 3",
+	    "License :: OSI Approved :: GNU Lesser General Public License v3 (LGPLv3)",
+	    "Operating System :: OS Independent",
+]
+requires-python = ">=3.8.1, <3.12"
+dependencies = [
+    # dagster >= 0.12.9 requires alembic that is incompatible with spinedb_api
+    "dagster>=0.12.6, <0.12.9",
+    # dagster versions lower that 1.5.7 do not support pendulum >= 3.0.0
+    "pendulum < 3.0.0",
+    # https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
+    "protobuf<3.21.0",
+    "networkx>2.5.1",
+    "datapackage>=1.15.2, <1.16",
+    "jupyter_client>=6.0",
+    "spinedb_api>=0.30.5",
+    "pyzmq >=21.0",
+    # dagster 0.12.8 requires Jinja2<3.0, which tries to import
+    # soft_unicode, which has been removed in markupsafe 2.1
+    "markupsafe < 2.1",
+]
+
+[project.urls]
+Repository = "https://github.com/spine-tools/spine-engine"
+
+[project.optional-dependencies]
+dev = ["coverage[toml]"]
+
+[build-system]
+requires = ["setuptools>=64", "setuptools_scm[toml]>=6.2", "wheel", "build"]
+build-backend = "setuptools.build_meta"
+
+[tool.setuptools_scm]
+write_to = "spine_engine/version.py"
+version_scheme = "release-branch-semver"
+
+[tool.setuptools]
+zip-safe = false
+
+[tool.setuptools.package-data]
+spine_engine = ["execution_managers/spine_repl.jl"]
+
+[tool.setuptools.packages.find]
+exclude = [
+	"bin*",
+	"docs*",
+	"fig*",
+	"tests*",
+]
+
+[tool.coverage.run]
+source = ["spine_engine"]
+branch = true
+
+[tool.coverage.report]
+ignore_errors = true
+
+[tool.black]
+line-length = 120
+skip-string-normalization = true
+exclude = '\.git'
```

### Comparing `spine_engine-0.23.3/spine_engine/__init__.py` & `spine_engine-0.23.4/spine_engine/__init__.py`

 * *Ordering differences only*

 * *Files 6% similar despite different names*

```diff
@@ -1,14 +1,14 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-from .spine_engine import SpineEngine, SpineEngineState
-from .utils.helpers import ExecutionDirection, ItemExecutionFinishState
-from .version import __version__
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+from .spine_engine import SpineEngine, SpineEngineState
+from .utils.helpers import ExecutionDirection, ItemExecutionFinishState
+from .version import __version__
```

### Comparing `spine_engine-0.23.3/spine_engine/config.py` & `spine_engine-0.23.4/spine_engine/config.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,45 +1,45 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Application constants.
-
-"""
-
-import os
-import sys
-
-_on_windows = sys.platform == "win32"
-
-
-def _executable(name):
-    """Appends a .exe extension to `name` on Windows platform."""
-    if _on_windows:
-        return name + ".exe"
-    return name
-
-
-# GAMS
-GAMS_EXECUTABLE = _executable("gams")
-
-# Julia
-JULIA_EXECUTABLE = _executable("julia")
-
-# Python
-PYTHON_EXECUTABLE = _executable("python" if _on_windows else "python3")
-_frozen = getattr(sys, "frozen", False)
-_path_to_executable = os.path.dirname(sys.executable if _frozen else __file__)
-APPLICATION_PATH = os.path.realpath(_path_to_executable)
-# Experimental Python interpreter shipped with Spine Toolbox installation bundle
-EMBEDDED_PYTHON = os.path.join(APPLICATION_PATH, "tools", "python.exe")
-
-# Tool output directory name
-TOOL_OUTPUT_DIR = "output"
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Application constants.
+
+"""
+
+import os
+import sys
+
+_on_windows = sys.platform == "win32"
+
+
+def _executable(name):
+    """Appends a .exe extension to `name` on Windows platform."""
+    if _on_windows:
+        return name + ".exe"
+    return name
+
+
+# GAMS
+GAMS_EXECUTABLE = _executable("gams")
+
+# Julia
+JULIA_EXECUTABLE = _executable("julia")
+
+# Python
+PYTHON_EXECUTABLE = _executable("python" if _on_windows else "python3")
+_frozen = getattr(sys, "frozen", False)
+_path_to_executable = os.path.dirname(sys.executable if _frozen else __file__)
+APPLICATION_PATH = os.path.realpath(_path_to_executable)
+# Experimental Python interpreter shipped with Spine Toolbox installation bundle
+EMBEDDED_PYTHON = os.path.join(APPLICATION_PATH, "tools", "python.exe")
+
+# Tool output directory name
+TOOL_OUTPUT_DIR = "output"
```

### Comparing `spine_engine-0.23.3/spine_engine/exception.py` & `spine_engine-0.23.4/spine_engine/exception.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,22 +1,22 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-"""
-Contains Engine's exceptions.
-
-"""
-
-
-class EngineInitFailed(Exception):
-    """Raised when :class:`SpineEngine` initialization fails."""
-
-
-class RemoteEngineInitFailed(Exception):
-    """Raised when initializing the remote server connection fails."""
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+"""
+Contains Engine's exceptions.
+
+"""
+
+
+class EngineInitFailed(Exception):
+    """Raised when :class:`SpineEngine` initialization fails."""
+
+
+class RemoteEngineInitFailed(Exception):
+    """Raised when initializing the remote server connection fails."""
```

### Comparing `spine_engine-0.23.3/spine_engine/execution_managers/__init__.py` & `spine_engine-0.23.4/spine_engine/execution_managers/__init__.py`

 * *Ordering differences only*

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
```

### Comparing `spine_engine-0.23.3/spine_engine/execution_managers/conda_kernel_spec_manager.py` & `spine_engine-0.23.4/spine_engine/execution_managers/conda_kernel_spec_manager.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,390 +1,390 @@
-# This is a fork of
-# https://github.com/Anaconda-Platform/nb_conda_kernels/blob/master/nb_conda_kernels/manager.py
-
-# -*- coding: utf-8 -*-
-import logging
-import json
-import re
-import shutil
-import subprocess
-import sys
-import time
-import glob
-
-import os
-from os.path import join, split, dirname, basename, abspath
-from traitlets import Bool, Unicode, TraitError, validate
-from jupyter_client.kernelspec import KernelSpecManager, KernelSpec, NoSuchKernel
-
-CACHE_TIMEOUT = 60
-
-RUNNER_COMMAND = ["python", "-m", "spine_engine.execution_managers.conda_kernel_spec_runner"]
-
-
-class CondaKernelSpecManager(KernelSpecManager):
-    """A custom KernelSpecManager able to search for conda environments and
-    create kernelspecs for them.
-    """
-
-    conda_only = Bool(
-        False,
-        config=True,
-        help="Include only the kernels not visible from Jupyter normally (True if kernelspec_path is not None)",
-    )
-
-    env_filter = Unicode(None, config=True, allow_none=True, help="Do not list environment names that match this regex")
-
-    kernelspec_path = Unicode(
-        None,
-        config=True,
-        allow_none=True,
-        help="""Path to install conda kernel specs to.
-
-        The acceptable values are:
-        - ``""`` (empty string): Install for all users
-        - ``--user``: Install for the current user instead of system-wide
-        - ``--sys-prefix``: Install to Python's sys.prefix
-        - ``PREFIX``: Specify an install prefix for the kernelspec. The kernel specs will be
-        written in ``PREFIX/share/jupyter/kernels``. Be careful that the PREFIX
-        may not be discoverable by Jupyter; set JUPYTER_DATA_DIR to force it or run
-        ``jupyter --paths`` to get the list of data directories.
-
-        If None, the conda kernel specs will only be available dynamically on notebook editors.
-        """,
-    )
-
-    @validate("kernelspec_path")
-    def _validate_kernelspec_path(self, proposal):
-        new_value = proposal["value"]
-        if new_value is not None:
-            if new_value not in ("", "--user", "--sys-prefix"):
-                if not os.path.isdir(self.kernelspec_path):
-                    raise TraitError("CondaKernelSpecManager.kernelspec_path is not a directory.")
-            self.log.debug("[nb_conda_kernels] Force conda_only=True as kernelspec_path is not None.")
-            self.conda_only = True
-
-        return new_value
-
-    name_format = Unicode(
-        '{language} [conda env:{environment}]',
-        config=True,
-        help="""String name format; available field names within the string:
-        '{0}' = Language
-        '{1}' = Environment name
-        '{conda_kernel}' = Dynamically built kernel name for conda environment
-        '{display_name}' = Kernel displayed name (as defined in the kernel spec)
-        '{environment}'  = Environment name (identical to '{1}')
-        '{kernel}' = Original kernel name (name of the folder containing the kernel spec)
-        '{language}'  = Language (identical to '{0}')
-        """,
-    )
-
-    def __init__(self, **kwargs):
-        self._conda_executable = kwargs.pop("conda_exe")
-        super(CondaKernelSpecManager, self).__init__(**kwargs)
-        self.log = logging.getLogger(__name__)
-        self.log.setLevel(logging.WARNING)
-        self._conda_info_cache = None
-        self._conda_info_cache_expiry = None
-
-        self._conda_kernels_cache = None
-        self._conda_kernels_cache_expiry = None
-
-        if self.env_filter is not None:
-            self._env_filter_regex = re.compile(self.env_filter)
-
-        self._kernel_user = self.kernelspec_path == "--user"
-        self._kernel_prefix = None
-        if not self._kernel_user:
-            self._kernel_prefix = sys.prefix if self.kernelspec_path == "--sys-prefix" else self.kernelspec_path
-
-        self.log.info("[nb_conda_kernels] enabled, %s kernels found", len(self._conda_kspecs))
-
-    @staticmethod
-    def clean_kernel_name(kname):
-        """Replaces invalid characters in the Jupyter kernelname, with
-        a bit of effort to preserve readability.
-        """
-        try:
-            kname.encode('ascii')
-        except UnicodeEncodeError:
-            # Replace accented characters with unaccented equivalents
-            import unicodedata
-
-            nfkd_form = unicodedata.normalize('NFKD', kname)
-            kname = u"".join([c for c in nfkd_form if not unicodedata.combining(c)])
-        # Replace anything else, including spaces, with underscores
-        kname = re.sub(r'[^a-zA-Z0-9._\-]', '_', kname)
-        return kname
-
-    @property
-    def _conda_info(self):
-        """Get and parse the whole conda information output
-
-        Caches the information for CACHE_TIMEOUT seconds, as this is
-        relatively expensive.
-        """
-
-        expiry = self._conda_info_cache_expiry
-        if expiry is None or expiry < time.time():
-            self.log.debug("[nb_conda_kernels] refreshing conda info")
-            # This is to make sure that subprocess can find 'conda' even if
-            # it is a Windows batch file---which is the case in non-root
-            # conda environments.
-            shell = self._conda_executable == 'conda' and sys.platform.startswith('win')
-            try:
-                # conda info --json uses the standard JSON escaping
-                # mechanism for non-ASCII characters. So it is always
-                # valid to decode here as 'ascii', since the JSON loads()
-                # method will recover any original Unicode for us.
-                p = subprocess.check_output([self._conda_executable, "info", "--json"], shell=shell).decode("ascii")
-                ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
-                result = ansi_escape.sub('', p)  # Remove ANSI Escape Sequences, such as ESC[0m
-                conda_info = json.loads(result)
-            except Exception as err:
-                conda_info = None
-                self.log.error("Obtaining 'conda info --json' failed")
-            self._conda_info_cache = conda_info
-            self._conda_info_cache_expiry = time.time() + CACHE_TIMEOUT
-        return self._conda_info_cache
-
-    def _all_envs(self):
-        """Find all of the environments we should be checking. We skip
-        environments in the conda-bld directory as well as environments
-        that match our env_filter regex. Returns a dict with canonical
-        environment names as keys, and full paths as values.
-        """
-        conda_info = self._conda_info
-        envs = conda_info['envs']
-        base_prefix = conda_info['conda_prefix']
-        envs_prefix = join(base_prefix, 'envs')
-        build_prefix = join(base_prefix, 'conda-bld', '')
-        # Older versions of conda do not seem to include the base prefix
-        # in the environment list, but we do want to scan that
-        if base_prefix not in envs:
-            envs.insert(0, base_prefix)
-        envs_dirs = conda_info['envs_dirs']
-        if not envs_dirs:
-            envs_dirs = [join(base_prefix, 'envs')]
-        all_envs = {}
-        for env_path in envs:
-            if self.env_filter is not None:
-                if self._env_filter_regex.search(env_path):
-                    continue
-            if env_path == base_prefix:
-                env_name = 'root'
-            elif env_path.startswith(build_prefix):
-                # Skip the conda-bld directory entirely
-                continue
-            else:
-                env_base, env_name = split(env_path)
-                # Add a prefix to environments not found in the default
-                # environment location. The assumed convention is that a
-                # directory named 'envs' is a collection of environments
-                # as created by, say, conda or anaconda-project. The name
-                # of the parent directory, then, provides useful context.
-                if basename(env_base) == 'envs' and (env_base != envs_prefix or env_name in all_envs):
-                    env_name = u'{}-{}'.format(basename(dirname(env_base)), env_name)
-            # Further disambiguate, if necessary, with a counter.
-            if env_name in all_envs:
-                base_name = env_name
-                for count in range(len(all_envs)):
-                    env_name = u'{}-{}'.format(base_name, count + 2)
-                    if env_name not in all_envs:
-                        break
-            all_envs[env_name] = env_path
-        return all_envs
-
-    def _all_specs(self):
-        """Find the all kernel specs in all environments.
-
-        Returns a dict with unique env names as keys, and the kernel.json
-        content as values, modified so that they can be run properly in
-        their native environments.
-
-        Caches the information for CACHE_TIMEOUT seconds, as this is
-        relatively expensive.
-        """
-
-        all_specs = {}
-        # We need to be able to find conda-run in the base conda environment
-        # even if this package is not running there
-        conda_prefix = self._conda_info['conda_prefix']
-        all_envs = self._all_envs()
-        for env_name, env_path in all_envs.items():
-            kspec_base = join(env_path, 'share', 'jupyter', 'kernels')
-            kspec_glob = glob.glob(join(kspec_base, '*', 'kernel.json'))
-            for spec_path in kspec_glob:
-                try:
-                    with open(spec_path, 'rb') as fp:
-                        data = fp.read()
-                    spec = json.loads(data.decode('utf-8'))
-                except Exception as err:
-                    self.log.error("[nb_conda_kernels] error loading %s:\n%s", spec_path, err)
-                    continue
-                kernel_dir = dirname(spec_path).lower()
-                kernel_name = raw_kernel_name = basename(kernel_dir)
-                if self.kernelspec_path is not None and kernel_name.startswith("conda-"):
-                    self.log.debug("[nb_conda_kernels] Skipping kernel spec %s", spec_path)
-                    continue  # Ensure to skip dynamically added kernel spec within the environment prefix
-                # We're doing a few of these adjustments here to ensure that
-                # the naming convention is as close as possible to the previous
-                # versions of this package; particularly so that the tests
-                # pass without change.
-                if kernel_name in ('python2', 'python3'):
-                    kernel_name = 'py'
-                elif kernel_name == 'ir':
-                    kernel_name = 'r'
-                kernel_prefix = '' if env_name == 'root' else 'env-'
-                kernel_name = u'conda-{}{}-{}'.format(kernel_prefix, env_name, kernel_name)
-                # Replace invalid characters with dashes
-                kernel_name = self.clean_kernel_name(kernel_name)
-
-                display_prefix = spec['display_name']
-                if display_prefix.startswith('Python'):
-                    display_prefix = 'Python'
-                display_name = self.name_format.format(
-                    display_prefix,
-                    env_name,
-                    conda_kernel=kernel_name,
-                    display_name=spec['display_name'],
-                    environment=env_name,
-                    kernel=raw_kernel_name,
-                    language=display_prefix,
-                )
-                if env_path == sys.prefix:
-                    display_name += ' *'
-                spec['display_name'] = display_name
-                if env_path != sys.prefix:
-                    spec['argv'] = RUNNER_COMMAND + [conda_prefix, env_path] + spec['argv']
-                metadata = spec.get('metadata', {})
-                metadata.update({'conda_env_name': env_name, 'conda_env_path': env_path})
-                spec['metadata'] = metadata
-
-                if self.kernelspec_path is not None:
-                    # Install the kernel spec
-                    try:
-                        destination = self.install_kernel_spec(
-                            kernel_dir, kernel_name=kernel_name, user=self._kernel_user, prefix=self._kernel_prefix
-                        )
-                        # Update the kernel spec
-                        kernel_spec = join(destination, "kernel.json")
-                        tmp_spec = spec.copy()
-                        if env_path == sys.prefix:  # Add the conda runner to the installed kernel spec
-                            tmp_spec['argv'] = RUNNER_COMMAND + [conda_prefix, env_path] + spec['argv']
-                        with open(kernel_spec, "w") as f:
-                            json.dump(tmp_spec, f)
-                    except OSError as error:
-                        self.log.warning(
-                            u"[nb_conda_kernels] Fail to install kernel '{}'.".format(kernel_dir), exc_info=error
-                        )
-
-                # resource_dir is not part of the spec file, so it is added at the latest time
-                spec['resource_dir'] = abspath(kernel_dir)
-
-                all_specs[kernel_name] = spec
-
-        # Remove non-existing conda environments
-        if self.kernelspec_path is not None:
-            kernels_destination = self._get_destination_dir("", user=self._kernel_user, prefix=self._kernel_prefix)
-            for folder in glob.glob(join(kernels_destination, "*", "kernel.json")):
-                kernel_dir = dirname(folder)
-                kernel_name = basename(kernel_dir)
-                if kernel_name.startswith("conda-") and kernel_name not in all_specs:
-                    self.log.info("Removing %s", kernel_dir)
-                    if os.path.islink(kernel_dir):
-                        os.remove(kernel_dir)
-                    else:
-                        shutil.rmtree(kernel_dir)
-
-        return all_specs
-
-    @property
-    def _conda_kspecs(self):
-        """Get (or refresh) the cache of conda kernels."""
-        if self._conda_info is None:
-            return {}
-
-        expiry = self._conda_kernels_cache_expiry
-        if expiry is not None and expiry >= time.time():
-            return self._conda_kernels_cache
-
-        kspecs = {}
-        for name, info in self._all_specs().items():
-            kspecs[name] = KernelSpec(**info)
-
-        self._conda_kernels_cache_expiry = time.time() + CACHE_TIMEOUT
-        self._conda_kernels_cache = kspecs
-
-        return kspecs
-
-    def find_kernel_specs(self):
-        """Returns a dict mapping kernel names to resource directories.
-
-        The update process also adds the resource dir for the conda
-        environments.
-        """
-        if self.conda_only:
-            kspecs = {}
-        else:
-            kspecs = super(CondaKernelSpecManager, self).find_kernel_specs()
-
-        # add conda envs kernelspecs
-        if self.whitelist:
-            kspecs.update(
-                {name: spec.resource_dir for name, spec in self._conda_kspecs.items() if name in self.whitelist}
-            )
-        else:
-            kspecs.update({name: spec.resource_dir for name, spec in self._conda_kspecs.items()})
-        return kspecs
-
-    def get_kernel_spec(self, kernel_name):
-        """Returns a :class:`KernelSpec` instance for the given kernel_name.
-
-        Additionally, conda kernelspecs are generated on the fly
-        accordingly with the detected environments.
-        """
-
-        res = self._conda_kspecs.get(kernel_name)
-        if not res:
-            # Happens when a Tool spec is trying to start a Conda kernel spec that doesn't exist
-            return None
-        self.log.info(f"res.argv:{res.argv}")
-        if res is None and not self.conda_only:
-            res = super(CondaKernelSpecManager, self).get_kernel_spec(kernel_name)
-        return res
-
-    def get_all_specs(self):
-        """Returns a dict mapping kernel names to dictionaries with two
-        entries: "resource_dir" and "spec". This was added to fill out
-        the full public interface to KernelManagerSpec.
-        """
-        res = {}
-        for name, resource_dir in self.find_kernel_specs().items():
-            try:
-                spec = self.get_kernel_spec(name)
-                res[name] = {'resource_dir': resource_dir, 'spec': spec.to_dict()}
-            except NoSuchKernel:
-                self.log.warning("Error loading kernelspec %r", name, exc_info=True)
-        return res
-
-    def remove_kernel_spec(self, name):
-        """Remove a kernel spec directory by name.
-
-        Returns the path that was deleted.
-        """
-        save_native = self.ensure_native_kernel
-        try:
-            self.ensure_native_kernel = False
-            # Conda environment kernelspec are only virtual, so remove can only be applied
-            # on non-virtual kernels.
-            specs = super(CondaKernelSpecManager, self).find_kernel_specs()
-        finally:
-            self.ensure_native_kernel = save_native
-        spec_dir = specs[name]
-        self.log.debug("Removing %s", spec_dir)
-        if os.path.islink(spec_dir):
-            os.remove(spec_dir)
-        else:
-            shutil.rmtree(spec_dir)
-        return spec_dir
+# This is a fork of
+# https://github.com/Anaconda-Platform/nb_conda_kernels/blob/master/nb_conda_kernels/manager.py
+
+# -*- coding: utf-8 -*-
+import logging
+import json
+import re
+import shutil
+import subprocess
+import sys
+import time
+import glob
+
+import os
+from os.path import join, split, dirname, basename, abspath
+from traitlets import Bool, Unicode, TraitError, validate
+from jupyter_client.kernelspec import KernelSpecManager, KernelSpec, NoSuchKernel
+
+CACHE_TIMEOUT = 60
+
+RUNNER_COMMAND = ["python", "-m", "spine_engine.execution_managers.conda_kernel_spec_runner"]
+
+
+class CondaKernelSpecManager(KernelSpecManager):
+    """A custom KernelSpecManager able to search for conda environments and
+    create kernelspecs for them.
+    """
+
+    conda_only = Bool(
+        False,
+        config=True,
+        help="Include only the kernels not visible from Jupyter normally (True if kernelspec_path is not None)",
+    )
+
+    env_filter = Unicode(None, config=True, allow_none=True, help="Do not list environment names that match this regex")
+
+    kernelspec_path = Unicode(
+        None,
+        config=True,
+        allow_none=True,
+        help="""Path to install conda kernel specs to.
+
+        The acceptable values are:
+        - ``""`` (empty string): Install for all users
+        - ``--user``: Install for the current user instead of system-wide
+        - ``--sys-prefix``: Install to Python's sys.prefix
+        - ``PREFIX``: Specify an install prefix for the kernelspec. The kernel specs will be
+        written in ``PREFIX/share/jupyter/kernels``. Be careful that the PREFIX
+        may not be discoverable by Jupyter; set JUPYTER_DATA_DIR to force it or run
+        ``jupyter --paths`` to get the list of data directories.
+
+        If None, the conda kernel specs will only be available dynamically on notebook editors.
+        """,
+    )
+
+    @validate("kernelspec_path")
+    def _validate_kernelspec_path(self, proposal):
+        new_value = proposal["value"]
+        if new_value is not None:
+            if new_value not in ("", "--user", "--sys-prefix"):
+                if not os.path.isdir(self.kernelspec_path):
+                    raise TraitError("CondaKernelSpecManager.kernelspec_path is not a directory.")
+            self.log.debug("[nb_conda_kernels] Force conda_only=True as kernelspec_path is not None.")
+            self.conda_only = True
+
+        return new_value
+
+    name_format = Unicode(
+        '{language} [conda env:{environment}]',
+        config=True,
+        help="""String name format; available field names within the string:
+        '{0}' = Language
+        '{1}' = Environment name
+        '{conda_kernel}' = Dynamically built kernel name for conda environment
+        '{display_name}' = Kernel displayed name (as defined in the kernel spec)
+        '{environment}'  = Environment name (identical to '{1}')
+        '{kernel}' = Original kernel name (name of the folder containing the kernel spec)
+        '{language}'  = Language (identical to '{0}')
+        """,
+    )
+
+    def __init__(self, **kwargs):
+        self._conda_executable = kwargs.pop("conda_exe")
+        super(CondaKernelSpecManager, self).__init__(**kwargs)
+        self.log = logging.getLogger(__name__)
+        self.log.setLevel(logging.WARNING)
+        self._conda_info_cache = None
+        self._conda_info_cache_expiry = None
+
+        self._conda_kernels_cache = None
+        self._conda_kernels_cache_expiry = None
+
+        if self.env_filter is not None:
+            self._env_filter_regex = re.compile(self.env_filter)
+
+        self._kernel_user = self.kernelspec_path == "--user"
+        self._kernel_prefix = None
+        if not self._kernel_user:
+            self._kernel_prefix = sys.prefix if self.kernelspec_path == "--sys-prefix" else self.kernelspec_path
+
+        self.log.info("[nb_conda_kernels] enabled, %s kernels found", len(self._conda_kspecs))
+
+    @staticmethod
+    def clean_kernel_name(kname):
+        """Replaces invalid characters in the Jupyter kernelname, with
+        a bit of effort to preserve readability.
+        """
+        try:
+            kname.encode('ascii')
+        except UnicodeEncodeError:
+            # Replace accented characters with unaccented equivalents
+            import unicodedata
+
+            nfkd_form = unicodedata.normalize('NFKD', kname)
+            kname = u"".join([c for c in nfkd_form if not unicodedata.combining(c)])
+        # Replace anything else, including spaces, with underscores
+        kname = re.sub(r'[^a-zA-Z0-9._\-]', '_', kname)
+        return kname
+
+    @property
+    def _conda_info(self):
+        """Get and parse the whole conda information output
+
+        Caches the information for CACHE_TIMEOUT seconds, as this is
+        relatively expensive.
+        """
+
+        expiry = self._conda_info_cache_expiry
+        if expiry is None or expiry < time.time():
+            self.log.debug("[nb_conda_kernels] refreshing conda info")
+            # This is to make sure that subprocess can find 'conda' even if
+            # it is a Windows batch file---which is the case in non-root
+            # conda environments.
+            shell = self._conda_executable == 'conda' and sys.platform.startswith('win')
+            try:
+                # conda info --json uses the standard JSON escaping
+                # mechanism for non-ASCII characters. So it is always
+                # valid to decode here as 'ascii', since the JSON loads()
+                # method will recover any original Unicode for us.
+                p = subprocess.check_output([self._conda_executable, "info", "--json"], shell=shell).decode("ascii")
+                ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
+                result = ansi_escape.sub('', p)  # Remove ANSI Escape Sequences, such as ESC[0m
+                conda_info = json.loads(result)
+            except Exception as err:
+                conda_info = None
+                self.log.error("Obtaining 'conda info --json' failed")
+            self._conda_info_cache = conda_info
+            self._conda_info_cache_expiry = time.time() + CACHE_TIMEOUT
+        return self._conda_info_cache
+
+    def _all_envs(self):
+        """Find all of the environments we should be checking. We skip
+        environments in the conda-bld directory as well as environments
+        that match our env_filter regex. Returns a dict with canonical
+        environment names as keys, and full paths as values.
+        """
+        conda_info = self._conda_info
+        envs = conda_info['envs']
+        base_prefix = conda_info['conda_prefix']
+        envs_prefix = join(base_prefix, 'envs')
+        build_prefix = join(base_prefix, 'conda-bld', '')
+        # Older versions of conda do not seem to include the base prefix
+        # in the environment list, but we do want to scan that
+        if base_prefix not in envs:
+            envs.insert(0, base_prefix)
+        envs_dirs = conda_info['envs_dirs']
+        if not envs_dirs:
+            envs_dirs = [join(base_prefix, 'envs')]
+        all_envs = {}
+        for env_path in envs:
+            if self.env_filter is not None:
+                if self._env_filter_regex.search(env_path):
+                    continue
+            if env_path == base_prefix:
+                env_name = 'root'
+            elif env_path.startswith(build_prefix):
+                # Skip the conda-bld directory entirely
+                continue
+            else:
+                env_base, env_name = split(env_path)
+                # Add a prefix to environments not found in the default
+                # environment location. The assumed convention is that a
+                # directory named 'envs' is a collection of environments
+                # as created by, say, conda or anaconda-project. The name
+                # of the parent directory, then, provides useful context.
+                if basename(env_base) == 'envs' and (env_base != envs_prefix or env_name in all_envs):
+                    env_name = u'{}-{}'.format(basename(dirname(env_base)), env_name)
+            # Further disambiguate, if necessary, with a counter.
+            if env_name in all_envs:
+                base_name = env_name
+                for count in range(len(all_envs)):
+                    env_name = u'{}-{}'.format(base_name, count + 2)
+                    if env_name not in all_envs:
+                        break
+            all_envs[env_name] = env_path
+        return all_envs
+
+    def _all_specs(self):
+        """Find the all kernel specs in all environments.
+
+        Returns a dict with unique env names as keys, and the kernel.json
+        content as values, modified so that they can be run properly in
+        their native environments.
+
+        Caches the information for CACHE_TIMEOUT seconds, as this is
+        relatively expensive.
+        """
+
+        all_specs = {}
+        # We need to be able to find conda-run in the base conda environment
+        # even if this package is not running there
+        conda_prefix = self._conda_info['conda_prefix']
+        all_envs = self._all_envs()
+        for env_name, env_path in all_envs.items():
+            kspec_base = join(env_path, 'share', 'jupyter', 'kernels')
+            kspec_glob = glob.glob(join(kspec_base, '*', 'kernel.json'))
+            for spec_path in kspec_glob:
+                try:
+                    with open(spec_path, 'rb') as fp:
+                        data = fp.read()
+                    spec = json.loads(data.decode('utf-8'))
+                except Exception as err:
+                    self.log.error("[nb_conda_kernels] error loading %s:\n%s", spec_path, err)
+                    continue
+                kernel_dir = dirname(spec_path).lower()
+                kernel_name = raw_kernel_name = basename(kernel_dir)
+                if self.kernelspec_path is not None and kernel_name.startswith("conda-"):
+                    self.log.debug("[nb_conda_kernels] Skipping kernel spec %s", spec_path)
+                    continue  # Ensure to skip dynamically added kernel spec within the environment prefix
+                # We're doing a few of these adjustments here to ensure that
+                # the naming convention is as close as possible to the previous
+                # versions of this package; particularly so that the tests
+                # pass without change.
+                if kernel_name in ('python2', 'python3'):
+                    kernel_name = 'py'
+                elif kernel_name == 'ir':
+                    kernel_name = 'r'
+                kernel_prefix = '' if env_name == 'root' else 'env-'
+                kernel_name = u'conda-{}{}-{}'.format(kernel_prefix, env_name, kernel_name)
+                # Replace invalid characters with dashes
+                kernel_name = self.clean_kernel_name(kernel_name)
+
+                display_prefix = spec['display_name']
+                if display_prefix.startswith('Python'):
+                    display_prefix = 'Python'
+                display_name = self.name_format.format(
+                    display_prefix,
+                    env_name,
+                    conda_kernel=kernel_name,
+                    display_name=spec['display_name'],
+                    environment=env_name,
+                    kernel=raw_kernel_name,
+                    language=display_prefix,
+                )
+                if env_path == sys.prefix:
+                    display_name += ' *'
+                spec['display_name'] = display_name
+                if env_path != sys.prefix:
+                    spec['argv'] = RUNNER_COMMAND + [conda_prefix, env_path] + spec['argv']
+                metadata = spec.get('metadata', {})
+                metadata.update({'conda_env_name': env_name, 'conda_env_path': env_path})
+                spec['metadata'] = metadata
+
+                if self.kernelspec_path is not None:
+                    # Install the kernel spec
+                    try:
+                        destination = self.install_kernel_spec(
+                            kernel_dir, kernel_name=kernel_name, user=self._kernel_user, prefix=self._kernel_prefix
+                        )
+                        # Update the kernel spec
+                        kernel_spec = join(destination, "kernel.json")
+                        tmp_spec = spec.copy()
+                        if env_path == sys.prefix:  # Add the conda runner to the installed kernel spec
+                            tmp_spec['argv'] = RUNNER_COMMAND + [conda_prefix, env_path] + spec['argv']
+                        with open(kernel_spec, "w") as f:
+                            json.dump(tmp_spec, f)
+                    except OSError as error:
+                        self.log.warning(
+                            u"[nb_conda_kernels] Fail to install kernel '{}'.".format(kernel_dir), exc_info=error
+                        )
+
+                # resource_dir is not part of the spec file, so it is added at the latest time
+                spec['resource_dir'] = abspath(kernel_dir)
+
+                all_specs[kernel_name] = spec
+
+        # Remove non-existing conda environments
+        if self.kernelspec_path is not None:
+            kernels_destination = self._get_destination_dir("", user=self._kernel_user, prefix=self._kernel_prefix)
+            for folder in glob.glob(join(kernels_destination, "*", "kernel.json")):
+                kernel_dir = dirname(folder)
+                kernel_name = basename(kernel_dir)
+                if kernel_name.startswith("conda-") and kernel_name not in all_specs:
+                    self.log.info("Removing %s", kernel_dir)
+                    if os.path.islink(kernel_dir):
+                        os.remove(kernel_dir)
+                    else:
+                        shutil.rmtree(kernel_dir)
+
+        return all_specs
+
+    @property
+    def _conda_kspecs(self):
+        """Get (or refresh) the cache of conda kernels."""
+        if self._conda_info is None:
+            return {}
+
+        expiry = self._conda_kernels_cache_expiry
+        if expiry is not None and expiry >= time.time():
+            return self._conda_kernels_cache
+
+        kspecs = {}
+        for name, info in self._all_specs().items():
+            kspecs[name] = KernelSpec(**info)
+
+        self._conda_kernels_cache_expiry = time.time() + CACHE_TIMEOUT
+        self._conda_kernels_cache = kspecs
+
+        return kspecs
+
+    def find_kernel_specs(self):
+        """Returns a dict mapping kernel names to resource directories.
+
+        The update process also adds the resource dir for the conda
+        environments.
+        """
+        if self.conda_only:
+            kspecs = {}
+        else:
+            kspecs = super(CondaKernelSpecManager, self).find_kernel_specs()
+
+        # add conda envs kernelspecs
+        if self.whitelist:
+            kspecs.update(
+                {name: spec.resource_dir for name, spec in self._conda_kspecs.items() if name in self.whitelist}
+            )
+        else:
+            kspecs.update({name: spec.resource_dir for name, spec in self._conda_kspecs.items()})
+        return kspecs
+
+    def get_kernel_spec(self, kernel_name):
+        """Returns a :class:`KernelSpec` instance for the given kernel_name.
+
+        Additionally, conda kernelspecs are generated on the fly
+        accordingly with the detected environments.
+        """
+
+        res = self._conda_kspecs.get(kernel_name)
+        if not res:
+            # Happens when a Tool spec is trying to start a Conda kernel spec that doesn't exist
+            return None
+        self.log.info(f"res.argv:{res.argv}")
+        if res is None and not self.conda_only:
+            res = super(CondaKernelSpecManager, self).get_kernel_spec(kernel_name)
+        return res
+
+    def get_all_specs(self):
+        """Returns a dict mapping kernel names to dictionaries with two
+        entries: "resource_dir" and "spec". This was added to fill out
+        the full public interface to KernelManagerSpec.
+        """
+        res = {}
+        for name, resource_dir in self.find_kernel_specs().items():
+            try:
+                spec = self.get_kernel_spec(name)
+                res[name] = {'resource_dir': resource_dir, 'spec': spec.to_dict()}
+            except NoSuchKernel:
+                self.log.warning("Error loading kernelspec %r", name, exc_info=True)
+        return res
+
+    def remove_kernel_spec(self, name):
+        """Remove a kernel spec directory by name.
+
+        Returns the path that was deleted.
+        """
+        save_native = self.ensure_native_kernel
+        try:
+            self.ensure_native_kernel = False
+            # Conda environment kernelspec are only virtual, so remove can only be applied
+            # on non-virtual kernels.
+            specs = super(CondaKernelSpecManager, self).find_kernel_specs()
+        finally:
+            self.ensure_native_kernel = save_native
+        spec_dir = specs[name]
+        self.log.debug("Removing %s", spec_dir)
+        if os.path.islink(spec_dir):
+            os.remove(spec_dir)
+        else:
+            shutil.rmtree(spec_dir)
+        return spec_dir
```

### Comparing `spine_engine-0.23.3/spine_engine/execution_managers/conda_kernel_spec_runner.py` & `spine_engine-0.23.4/spine_engine/execution_managers/conda_kernel_spec_runner.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,60 +1,60 @@
-# This is a fork of
-# https://github.com/Anaconda-Platform/nb_conda_kernels/blob/master/nb_conda_kernels/runner.py
-
-from __future__ import print_function
-
-import os
-import sys
-import subprocess
-import locale
-
-try:
-    from shlex import quote
-except ImportError:
-    from pipes import quote
-
-
-def exec_in_env(conda_prefix, env_path, *command):
-    # Run the standard conda activation script, and print the
-    # resulting environment variables to stdout for reading.
-    is_current_env = env_path == sys.prefix
-    if sys.platform.startswith('win'):
-        if is_current_env:
-            subprocess.Popen(list(command)).wait()
-        else:
-            activate = os.path.join(conda_prefix, 'Scripts', 'activate.bat')
-            ecomm = [
-                os.environ['COMSPEC'],
-                '/S',
-                '/U',
-                '/C',
-                '@echo',
-                'off',
-                '&&',
-                'chcp',
-                '65001',
-                '&&',
-                'call',
-                activate,
-                env_path,
-                '&&',
-                '@echo',
-                'CONDA_PREFIX=%CONDA_PREFIX%',
-                '&&',
-            ] + list(command)
-            subprocess.Popen(ecomm).wait()
-    else:
-        quoted_command = [quote(c) for c in command]
-        if is_current_env:
-            os.execvp(quoted_command[0], quoted_command)
-        else:
-            activate = os.path.join(conda_prefix, 'bin', 'activate')
-            ecomm = ". '{}' '{}' && echo CONDA_PREFIX=$CONDA_PREFIX && exec {}".format(
-                activate, env_path, ' '.join(quoted_command)
-            )
-            ecomm = ['sh' if 'bsd' in sys.platform else 'bash', '-c', ecomm]
-            os.execvp(ecomm[0], ecomm)
-
-
-if __name__ == '__main__':
-    exec_in_env(*(sys.argv[1:]))
+# This is a fork of
+# https://github.com/Anaconda-Platform/nb_conda_kernels/blob/master/nb_conda_kernels/runner.py
+
+from __future__ import print_function
+
+import os
+import sys
+import subprocess
+import locale
+
+try:
+    from shlex import quote
+except ImportError:
+    from pipes import quote
+
+
+def exec_in_env(conda_prefix, env_path, *command):
+    # Run the standard conda activation script, and print the
+    # resulting environment variables to stdout for reading.
+    is_current_env = env_path == sys.prefix
+    if sys.platform.startswith('win'):
+        if is_current_env:
+            subprocess.Popen(list(command)).wait()
+        else:
+            activate = os.path.join(conda_prefix, 'Scripts', 'activate.bat')
+            ecomm = [
+                os.environ['COMSPEC'],
+                '/S',
+                '/U',
+                '/C',
+                '@echo',
+                'off',
+                '&&',
+                'chcp',
+                '65001',
+                '&&',
+                'call',
+                activate,
+                env_path,
+                '&&',
+                '@echo',
+                'CONDA_PREFIX=%CONDA_PREFIX%',
+                '&&',
+            ] + list(command)
+            subprocess.Popen(ecomm).wait()
+    else:
+        quoted_command = [quote(c) for c in command]
+        if is_current_env:
+            os.execvp(quoted_command[0], quoted_command)
+        else:
+            activate = os.path.join(conda_prefix, 'bin', 'activate')
+            ecomm = ". '{}' '{}' && echo CONDA_PREFIX=$CONDA_PREFIX && exec {}".format(
+                activate, env_path, ' '.join(quoted_command)
+            )
+            ecomm = ['sh' if 'bsd' in sys.platform else 'bash', '-c', ecomm]
+            os.execvp(ecomm[0], ecomm)
+
+
+if __name__ == '__main__':
+    exec_in_env(*(sys.argv[1:]))
```

### Comparing `spine_engine-0.23.3/spine_engine/execution_managers/execution_manager_base.py` & `spine_engine-0.23.4/spine_engine/project_item/project_item_specification_factory.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,40 +1,39 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains the ExecutionManagerBase class.
-
-"""
-
-
-class ExecutionManagerBase:
-    """Base class for all tool instance execution managers."""
-
-    def __init__(self, logger):
-        """Class constructor.
-
-        Args:
-            logger (LoggerInterface): a logger instance
-        """
-        self._logger = logger
-        self.killed = False
-
-    def run_until_complete(self):
-        """Runs until completion.
-
-        Returns:
-            int: return code
-        """
-        raise NotImplementedError()
-
-    def stop_execution(self):
-        """Stops execution gracefully."""
-        raise NotImplementedError()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains project item specification factory.
+
+"""
+
+
+class ProjectItemSpecificationFactory:
+    """A factory to make project item specifications."""
+
+    @staticmethod
+    def item_type():
+        """Returns the project item's type."""
+        raise NotImplementedError()
+
+    @staticmethod
+    def make_specification(definition, app_settings, logger):
+        """
+        Makes a project item specification.
+
+        Args:
+            definition (dict): specification's definition dictionary
+            app_settings (QSettings): Toolbox settings
+            logger (LoggerInterface): a logger
+
+        Returns:
+            ProjectItemSpecification: a specification built from the given definition
+        """
+        raise NotImplementedError()
```

### Comparing `spine_engine-0.23.3/spine_engine/execution_managers/kernel_execution_manager.py` & `spine_engine-0.23.4/spine_engine/execution_managers/kernel_execution_manager.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,361 +1,361 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains the KernelExecutionManager class and subclasses, and some convenience functions.
-
-"""
-
-import os
-import sys
-import subprocess
-import uuid
-from jupyter_client.manager import KernelManager
-from jupyter_client.kernelspec import NoSuchKernel
-from ..utils.helpers import Singleton
-from .execution_manager_base import ExecutionManagerBase
-from spine_engine.execution_managers.conda_kernel_spec_manager import CondaKernelSpecManager
-
-
-class GroupedKernelManager(KernelManager):
-    """Kernel Manager that supports group ID's."""
-
-    def __init__(self, *args, **kwargs):
-        group_id = kwargs.pop("group_id", "")
-        super().__init__(*args, **kwargs)
-        self._group_id = group_id
-        self._is_busy = False
-
-    def group_id(self):
-        """Returns the group ID of this kernel manager."""
-        return self._group_id
-
-    def set_busy(self, f):
-        """Sets km busy. This is set according to the
-        status messages received from the IOPUB channel."""
-        self._is_busy = f
-
-    def is_busy(self):
-        """Returns whether km is busy or not."""
-        return self._is_busy
-
-
-class _KernelManagerFactory(metaclass=Singleton):
-    _kernel_managers = {}
-    """Maps filter_id (str) to associated KernelManager"""
-    _key_by_connection_file = {}
-    """Maps connection file path to filter_id (str)"""
-
-    def _make_kernel_manager(self, kernel_name, group_id, server_ip, filter_id):
-        """Creates a new kernel manager if necessary or returns an existing one.
-
-        Args:
-            kernel_name (str): The kernel
-            group_id (str): Item group that will execute using this kernel
-            server_ip (str): Engine Server IP address. '127.0.0.1' when execution happens locally
-            filter_id (str): Filter Id
-
-        Returns:
-            GroupedKernelManager
-        """
-        if not filter_id == "":
-            group_id = filter_id  # Ignore group ID in case filter ID exists
-        for k in self._kernel_managers:
-            # Reuse kernel manager if using same group id and kernel and it's idle
-            km = self._kernel_managers[k]
-            if km.group_id() == group_id and km.kernel_name == kernel_name:
-                if not km.is_busy():
-                    return km
-        key = uuid.uuid4().hex
-        # Spawn a new kernel manager
-        km = self._kernel_managers[key] = GroupedKernelManager(kernel_name=kernel_name, ip=server_ip, group_id=group_id)
-        return km
-
-    def new_kernel_manager(self, kernel_name, group_id, logger, extra_switches=None, environment="", **kwargs):
-        """Creates a new kernel manager for given kernel and group id if none exists.
-        Starts the kernel if not started, and returns it.
-
-        Args:
-            kernel_name (str): The kernel
-            group_id (str): Item group that will execute using this kernel
-            logger (LoggerInterface): For logging
-            extra_switches (list, optional): List of additional switches to julia or python.
-                These come before the 'programfile'.
-            environment (str): "conda" to launch a Conda kernel spec. "" for a regular kernel spec
-            `**kwargs`: optional. Keyword arguments passed to ``KernelManager.start_kernel()``
-
-        Returns:
-            KernelManager
-        """
-        server_ip = kwargs.pop("server_ip", "")
-        filter_id = logger.msg_kernel_execution.filter_id
-        km = self._make_kernel_manager(kernel_name, group_id, server_ip, filter_id)
-        conda_exe = kwargs.pop("conda_exe", "")
-        if environment == "conda":
-            if not os.path.exists(conda_exe):
-                logger.msg_kernel_execution.emit(msg=dict(type="conda_not_found"))
-                self._kernel_managers.pop(self.get_kernel_manager_key(km))
-                return None
-            km.kernel_spec_manager = CondaKernelSpecManager(conda_exe=conda_exe)
-        msg = dict(kernel_name=kernel_name)
-        if not km.is_alive():
-            try:
-                if not km.kernel_spec:
-                    # Happens when a conda kernel spec with the requested name cannot be dynamically created
-                    # i.e. the conda environment does not exist
-                    msg["type"] = "conda_kernel_spec_not_found"
-                    logger.msg_kernel_execution.emit(msg)
-                    self._kernel_managers.pop(self.get_kernel_manager_key(km))  # Delete failed kernel manager
-                    return None
-            except NoSuchKernel:
-                msg["type"] = "kernel_spec_not_found"
-                logger.msg_kernel_execution.emit(msg)
-                self._kernel_managers.pop(self.get_kernel_manager_key(km))
-                return None
-            # Check that kernel spec executable is referring to a file that actually exists
-            exe_path = km.kernel_spec.argv[0]
-            if not os.path.exists(exe_path) and os.path.isabs(exe_path):
-                msg["type"] = "kernel_spec_exe_not_found"
-                msg["kernel_exe_path"] = exe_path
-                logger.msg_kernel_execution.emit(msg)
-                self._kernel_managers.pop(self.get_kernel_manager_key(km))
-                return None
-            if extra_switches:
-                # Insert switches right after the julia program
-                km.kernel_spec.argv[1:1] = extra_switches
-            km.start_kernel(**kwargs)
-            key = self.get_kernel_manager_key(km)
-            if not key:
-                return None  # Logic error
-            self._key_by_connection_file[km.connection_file] = key
-        msg["type"] = "kernel_started"
-        msg["connection_file"] = km.connection_file
-        logger.msg_kernel_execution.emit(msg)
-        return km
-
-    def get_kernel_manager_key(self, km):
-        """Returns the key of the given kernel manager stored in this factory.
-
-        Args:
-            km (GroupedKernelManager): Kernel manager
-
-        Returns:
-            str: Kernel Manager's 32 character key
-        """
-        for key, kernman in self._kernel_managers.items():
-            if kernman == km:
-                return key
-        return None
-
-    def get_kernel_manager(self, connection_file):
-        """Returns a kernel manager for given connection file if any.
-
-        Args:
-            connection_file (str): Path of connection file
-
-        Returns:
-            GroupedKernelManager or None
-        """
-        key = self._key_by_connection_file.get(connection_file, None)
-        return self._kernel_managers.get(key, None)
-
-    def pop_kernel_manager(self, connection_file):
-        """Returns a kernel manager for given connection file if any.
-        It also removes it from cache.
-
-        Args:
-            connection_file (str): Path of connection file
-
-        Returns:
-            GroupedKernelManager or None
-        """
-        key = self._key_by_connection_file.pop(connection_file, None)
-        return self._kernel_managers.pop(key, None)
-
-    def shutdown_kernel_manager(self, connection_file):
-        """Pops a kernel manager from factory and shuts it down.
-
-        Args:
-            connection_file (str): Path of connection file
-
-        Returns:
-            bool: True if operation succeeded, False otherwise
-        """
-        km = self.pop_kernel_manager(connection_file)
-        if not km:
-            return False
-        if km.is_alive():
-            km.shutdown_kernel(now=True)
-            return True
-        return False
-
-    def restart_kernel_manager(self, connection_file):
-        """Restarts kernel manager.
-
-        Args:
-            connection_file (str): Path of connection file
-
-        Returns:
-            bool: True if operation succeeded, False otherwise
-        """
-        km = self.get_kernel_manager(connection_file)
-        if not km:
-            return False
-        if km.is_alive():
-            km.restart_kernel(now=True)
-            return True
-        return False
-
-    def kill_kernel_managers(self):
-        """Shuts down all kernel managers stored in the factory."""
-        while True:
-            try:
-                key, km = self._kernel_managers.popitem()
-                if km.is_alive():
-                    km.shutdown_kernel(now=True)
-            except KeyError:
-                break
-        self._key_by_connection_file.clear()
-
-    def n_kernel_managers(self):
-        """Returns the number of open kernel managers stored in the factory."""
-        return len(self._kernel_managers)
-
-
-_kernel_manager_factory = _KernelManagerFactory()
-
-
-def get_kernel_manager(connection_file):
-    return _kernel_manager_factory.get_kernel_manager(connection_file)
-
-
-def pop_kernel_manager(connection_file):
-    return _kernel_manager_factory.pop_kernel_manager(connection_file)
-
-
-def n_kernel_managers():
-    return _kernel_manager_factory.n_kernel_managers()
-
-
-def kill_all_kernel_managers():
-    _kernel_manager_factory.kill_kernel_managers()
-
-
-def shutdown_kernel_manager(connection_file):
-    return _kernel_manager_factory.shutdown_kernel_manager(connection_file)
-
-
-def restart_kernel_manager(connection_file):
-    return _kernel_manager_factory.restart_kernel_manager(connection_file)
-
-
-class KernelExecutionManager(ExecutionManagerBase):
-    def __init__(
-        self,
-        logger,
-        kernel_name,
-        *commands,
-        kill_completed=False,
-        group_id=None,
-        startup_timeout=60,
-        extra_switches=None,
-        environment="",
-        **kwargs,
-    ):
-        """
-        Args:
-            logger (LoggerInterface): For logging
-            kernel_name (str): The Kernel
-            *commands: Commands to execute in the kernel
-            kill_completed (bool): Whether to kill completed persistent processes
-            group_id (str, optional): Item group that will execute using this kernel
-            startup_timeout (int, optional): How much to wait for the kernel, used in ``KernelClient.wait_for_ready()``
-            extra_switches (list, optional): List of additional switches to launch julia.
-                These come before the 'programfile'.
-            environment (str): "conda" to launch a Conda kernel spec. "" for a regular kernel spec.
-            **kwargs (optional): Keyword arguments passed to ``KernelManager.start_kernel()``
-        """
-        super().__init__(logger)
-        self._msg_head = dict(kernel_name=kernel_name)
-        self._commands = commands
-        self._cmd_failed = False
-        self.std_out = kwargs["stdout"] = open(os.devnull, 'w')
-        self.std_err = kwargs["stderr"] = open(os.devnull, 'w')
-        # Don't show console when frozen
-        kwargs["creationflags"] = subprocess.CREATE_NO_WINDOW if sys.platform == "win32" else 0
-        self._kernel_manager = _kernel_manager_factory.new_kernel_manager(
-            kernel_name, group_id, logger, extra_switches=extra_switches, environment=environment, **kwargs
-        )
-        self._kernel_client = self._kernel_manager.client() if self._kernel_manager is not None else None
-        self._startup_timeout = startup_timeout
-        self._kill_completed = kill_completed
-
-    def run_until_complete(self):
-        if self._kernel_client is None:
-            return 0
-        self._kernel_client.start_channels()
-        run_succeeded = self._do_run()
-        self._kernel_client.stop_channels()
-        if self._kill_completed:
-            conn_file = self._kernel_manager.connection_file
-            shutdown_kernel_manager(conn_file)
-            self._logger.msg_kernel_execution.emit(dict(type="kernel_shutdown", **self._msg_head))
-        if self._cmd_failed or not run_succeeded:
-            return -1
-        return 0
-
-    def _do_run(self):
-        try:
-            self._kernel_client.wait_for_ready(timeout=self._startup_timeout)
-        except RuntimeError as e:
-            msg = dict(type="execution_failed_to_start", error=str(e), **self._msg_head)
-            self._logger.msg_kernel_execution.emit(msg)
-            self._kernel_client.stop_channels()
-            self._kernel_manager.shutdown_kernel(now=True)
-            return False
-        msg = dict(type="execution_started", **self._msg_head)
-        self._logger.msg_kernel_execution.emit(msg)
-        for cmd in self._commands:
-            self._cmd_failed = False
-            # 'reply' is an execute_reply msg coming from the shell (ROUTER/DEALER) channel, it's a response to
-            # an execute_request msg
-            reply = self._kernel_client.execute_interactive(cmd, output_hook=self._output_hook)
-            st = reply["content"]["status"]
-            if st != "ok":
-                return False  # This happens when execute_request fails
-        return True
-
-    def _output_hook(self, msg):
-        """Catches messages from the IOPUB (PUB/SUB) channel and
-        handles 'error' and 'status message tyoe cases. 'error'
-        msg is a response to an execute_input msg."""
-        if msg["header"]["msg_type"] == "error":
-            self._cmd_failed = True
-        elif msg["header"]["msg_type"] == "status":
-            # Set kernel manager busy if execution is starting or in progress
-            exec_state = msg["content"]["execution_state"]
-            if exec_state == "busy" or exec_state == "starting":
-                self._kernel_manager.set_busy(True)
-            else:  # exec_state == 'idle'
-                self._kernel_manager.set_busy(False)
-        elif msg["header"]["msg_type"] == "execute_input":
-            execution_count, code = msg["content"]["execution_count"], msg["content"]["code"]
-            self._logger.msg_kernel_execution.emit({"type": "stdin", "data": f"In [{execution_count}]: {code}"})
-        elif msg["header"]["msg_type"] == "stream":
-            self._logger.msg_kernel_execution.emit({"type": msg["content"]["name"], "data": msg["content"]["text"]})
-
-    def stop_execution(self):
-        if self._kernel_manager is not None:
-            self._kernel_manager.interrupt_kernel()
-            if self._kill_completed:
-                conn_file = self._kernel_manager.connection_file
-                shutdown_kernel_manager(conn_file)
-                self._logger.msg_kernel_execution.emit(dict(type="kernel_shutdown", **self._msg_head))
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains the KernelExecutionManager class and subclasses, and some convenience functions.
+
+"""
+
+import os
+import sys
+import subprocess
+import uuid
+from jupyter_client.manager import KernelManager
+from jupyter_client.kernelspec import NoSuchKernel
+from ..utils.helpers import Singleton
+from .execution_manager_base import ExecutionManagerBase
+from spine_engine.execution_managers.conda_kernel_spec_manager import CondaKernelSpecManager
+
+
+class GroupedKernelManager(KernelManager):
+    """Kernel Manager that supports group ID's."""
+
+    def __init__(self, *args, **kwargs):
+        group_id = kwargs.pop("group_id", "")
+        super().__init__(*args, **kwargs)
+        self._group_id = group_id
+        self._is_busy = False
+
+    def group_id(self):
+        """Returns the group ID of this kernel manager."""
+        return self._group_id
+
+    def set_busy(self, f):
+        """Sets km busy. This is set according to the
+        status messages received from the IOPUB channel."""
+        self._is_busy = f
+
+    def is_busy(self):
+        """Returns whether km is busy or not."""
+        return self._is_busy
+
+
+class _KernelManagerFactory(metaclass=Singleton):
+    _kernel_managers = {}
+    """Maps filter_id (str) to associated KernelManager"""
+    _key_by_connection_file = {}
+    """Maps connection file path to filter_id (str)"""
+
+    def _make_kernel_manager(self, kernel_name, group_id, server_ip, filter_id):
+        """Creates a new kernel manager if necessary or returns an existing one.
+
+        Args:
+            kernel_name (str): The kernel
+            group_id (str): Item group that will execute using this kernel
+            server_ip (str): Engine Server IP address. '127.0.0.1' when execution happens locally
+            filter_id (str): Filter Id
+
+        Returns:
+            GroupedKernelManager
+        """
+        if not filter_id == "":
+            group_id = filter_id  # Ignore group ID in case filter ID exists
+        for k in self._kernel_managers:
+            # Reuse kernel manager if using same group id and kernel and it's idle
+            km = self._kernel_managers[k]
+            if km.group_id() == group_id and km.kernel_name == kernel_name:
+                if not km.is_busy():
+                    return km
+        key = uuid.uuid4().hex
+        # Spawn a new kernel manager
+        km = self._kernel_managers[key] = GroupedKernelManager(kernel_name=kernel_name, ip=server_ip, group_id=group_id)
+        return km
+
+    def new_kernel_manager(self, kernel_name, group_id, logger, extra_switches=None, environment="", **kwargs):
+        """Creates a new kernel manager for given kernel and group id if none exists.
+        Starts the kernel if not started, and returns it.
+
+        Args:
+            kernel_name (str): The kernel
+            group_id (str): Item group that will execute using this kernel
+            logger (LoggerInterface): For logging
+            extra_switches (list, optional): List of additional switches to julia or python.
+                These come before the 'programfile'.
+            environment (str): "conda" to launch a Conda kernel spec. "" for a regular kernel spec
+            `**kwargs`: optional. Keyword arguments passed to ``KernelManager.start_kernel()``
+
+        Returns:
+            KernelManager
+        """
+        server_ip = kwargs.pop("server_ip", "")
+        filter_id = logger.msg_kernel_execution.filter_id
+        km = self._make_kernel_manager(kernel_name, group_id, server_ip, filter_id)
+        conda_exe = kwargs.pop("conda_exe", "")
+        if environment == "conda":
+            if not os.path.exists(conda_exe):
+                logger.msg_kernel_execution.emit(msg=dict(type="conda_not_found"))
+                self._kernel_managers.pop(self.get_kernel_manager_key(km))
+                return None
+            km.kernel_spec_manager = CondaKernelSpecManager(conda_exe=conda_exe)
+        msg = dict(kernel_name=kernel_name)
+        if not km.is_alive():
+            try:
+                if not km.kernel_spec:
+                    # Happens when a conda kernel spec with the requested name cannot be dynamically created
+                    # i.e. the conda environment does not exist
+                    msg["type"] = "conda_kernel_spec_not_found"
+                    logger.msg_kernel_execution.emit(msg)
+                    self._kernel_managers.pop(self.get_kernel_manager_key(km))  # Delete failed kernel manager
+                    return None
+            except NoSuchKernel:
+                msg["type"] = "kernel_spec_not_found"
+                logger.msg_kernel_execution.emit(msg)
+                self._kernel_managers.pop(self.get_kernel_manager_key(km))
+                return None
+            # Check that kernel spec executable is referring to a file that actually exists
+            exe_path = km.kernel_spec.argv[0]
+            if not os.path.exists(exe_path) and os.path.isabs(exe_path):
+                msg["type"] = "kernel_spec_exe_not_found"
+                msg["kernel_exe_path"] = exe_path
+                logger.msg_kernel_execution.emit(msg)
+                self._kernel_managers.pop(self.get_kernel_manager_key(km))
+                return None
+            if extra_switches:
+                # Insert switches right after the julia program
+                km.kernel_spec.argv[1:1] = extra_switches
+            km.start_kernel(**kwargs)
+            key = self.get_kernel_manager_key(km)
+            if not key:
+                return None  # Logic error
+            self._key_by_connection_file[km.connection_file] = key
+        msg["type"] = "kernel_started"
+        msg["connection_file"] = km.connection_file
+        logger.msg_kernel_execution.emit(msg)
+        return km
+
+    def get_kernel_manager_key(self, km):
+        """Returns the key of the given kernel manager stored in this factory.
+
+        Args:
+            km (GroupedKernelManager): Kernel manager
+
+        Returns:
+            str: Kernel Manager's 32 character key
+        """
+        for key, kernman in self._kernel_managers.items():
+            if kernman == km:
+                return key
+        return None
+
+    def get_kernel_manager(self, connection_file):
+        """Returns a kernel manager for given connection file if any.
+
+        Args:
+            connection_file (str): Path of connection file
+
+        Returns:
+            GroupedKernelManager or None
+        """
+        key = self._key_by_connection_file.get(connection_file, None)
+        return self._kernel_managers.get(key, None)
+
+    def pop_kernel_manager(self, connection_file):
+        """Returns a kernel manager for given connection file if any.
+        It also removes it from cache.
+
+        Args:
+            connection_file (str): Path of connection file
+
+        Returns:
+            GroupedKernelManager or None
+        """
+        key = self._key_by_connection_file.pop(connection_file, None)
+        return self._kernel_managers.pop(key, None)
+
+    def shutdown_kernel_manager(self, connection_file):
+        """Pops a kernel manager from factory and shuts it down.
+
+        Args:
+            connection_file (str): Path of connection file
+
+        Returns:
+            bool: True if operation succeeded, False otherwise
+        """
+        km = self.pop_kernel_manager(connection_file)
+        if not km:
+            return False
+        if km.is_alive():
+            km.shutdown_kernel(now=True)
+            return True
+        return False
+
+    def restart_kernel_manager(self, connection_file):
+        """Restarts kernel manager.
+
+        Args:
+            connection_file (str): Path of connection file
+
+        Returns:
+            bool: True if operation succeeded, False otherwise
+        """
+        km = self.get_kernel_manager(connection_file)
+        if not km:
+            return False
+        if km.is_alive():
+            km.restart_kernel(now=True)
+            return True
+        return False
+
+    def kill_kernel_managers(self):
+        """Shuts down all kernel managers stored in the factory."""
+        while True:
+            try:
+                key, km = self._kernel_managers.popitem()
+                if km.is_alive():
+                    km.shutdown_kernel(now=True)
+            except KeyError:
+                break
+        self._key_by_connection_file.clear()
+
+    def n_kernel_managers(self):
+        """Returns the number of open kernel managers stored in the factory."""
+        return len(self._kernel_managers)
+
+
+_kernel_manager_factory = _KernelManagerFactory()
+
+
+def get_kernel_manager(connection_file):
+    return _kernel_manager_factory.get_kernel_manager(connection_file)
+
+
+def pop_kernel_manager(connection_file):
+    return _kernel_manager_factory.pop_kernel_manager(connection_file)
+
+
+def n_kernel_managers():
+    return _kernel_manager_factory.n_kernel_managers()
+
+
+def kill_all_kernel_managers():
+    _kernel_manager_factory.kill_kernel_managers()
+
+
+def shutdown_kernel_manager(connection_file):
+    return _kernel_manager_factory.shutdown_kernel_manager(connection_file)
+
+
+def restart_kernel_manager(connection_file):
+    return _kernel_manager_factory.restart_kernel_manager(connection_file)
+
+
+class KernelExecutionManager(ExecutionManagerBase):
+    def __init__(
+        self,
+        logger,
+        kernel_name,
+        *commands,
+        kill_completed=False,
+        group_id=None,
+        startup_timeout=60,
+        extra_switches=None,
+        environment="",
+        **kwargs,
+    ):
+        """
+        Args:
+            logger (LoggerInterface): For logging
+            kernel_name (str): The Kernel
+            *commands: Commands to execute in the kernel
+            kill_completed (bool): Whether to kill completed persistent processes
+            group_id (str, optional): Item group that will execute using this kernel
+            startup_timeout (int, optional): How much to wait for the kernel, used in ``KernelClient.wait_for_ready()``
+            extra_switches (list, optional): List of additional switches to launch julia.
+                These come before the 'programfile'.
+            environment (str): "conda" to launch a Conda kernel spec. "" for a regular kernel spec.
+            **kwargs (optional): Keyword arguments passed to ``KernelManager.start_kernel()``
+        """
+        super().__init__(logger)
+        self._msg_head = dict(kernel_name=kernel_name)
+        self._commands = commands
+        self._cmd_failed = False
+        self.std_out = kwargs["stdout"] = open(os.devnull, 'w')
+        self.std_err = kwargs["stderr"] = open(os.devnull, 'w')
+        # Don't show console when frozen
+        kwargs["creationflags"] = subprocess.CREATE_NO_WINDOW if sys.platform == "win32" else 0
+        self._kernel_manager = _kernel_manager_factory.new_kernel_manager(
+            kernel_name, group_id, logger, extra_switches=extra_switches, environment=environment, **kwargs
+        )
+        self._kernel_client = self._kernel_manager.client() if self._kernel_manager is not None else None
+        self._startup_timeout = startup_timeout
+        self._kill_completed = kill_completed
+
+    def run_until_complete(self):
+        if self._kernel_client is None:
+            return 0
+        self._kernel_client.start_channels()
+        run_succeeded = self._do_run()
+        self._kernel_client.stop_channels()
+        if self._kill_completed:
+            conn_file = self._kernel_manager.connection_file
+            shutdown_kernel_manager(conn_file)
+            self._logger.msg_kernel_execution.emit(dict(type="kernel_shutdown", **self._msg_head))
+        if self._cmd_failed or not run_succeeded:
+            return -1
+        return 0
+
+    def _do_run(self):
+        try:
+            self._kernel_client.wait_for_ready(timeout=self._startup_timeout)
+        except RuntimeError as e:
+            msg = dict(type="execution_failed_to_start", error=str(e), **self._msg_head)
+            self._logger.msg_kernel_execution.emit(msg)
+            self._kernel_client.stop_channels()
+            self._kernel_manager.shutdown_kernel(now=True)
+            return False
+        msg = dict(type="execution_started", **self._msg_head)
+        self._logger.msg_kernel_execution.emit(msg)
+        for cmd in self._commands:
+            self._cmd_failed = False
+            # 'reply' is an execute_reply msg coming from the shell (ROUTER/DEALER) channel, it's a response to
+            # an execute_request msg
+            reply = self._kernel_client.execute_interactive(cmd, output_hook=self._output_hook)
+            st = reply["content"]["status"]
+            if st != "ok":
+                return False  # This happens when execute_request fails
+        return True
+
+    def _output_hook(self, msg):
+        """Catches messages from the IOPUB (PUB/SUB) channel and
+        handles 'error' and 'status message tyoe cases. 'error'
+        msg is a response to an execute_input msg."""
+        if msg["header"]["msg_type"] == "error":
+            self._cmd_failed = True
+        elif msg["header"]["msg_type"] == "status":
+            # Set kernel manager busy if execution is starting or in progress
+            exec_state = msg["content"]["execution_state"]
+            if exec_state == "busy" or exec_state == "starting":
+                self._kernel_manager.set_busy(True)
+            else:  # exec_state == 'idle'
+                self._kernel_manager.set_busy(False)
+        elif msg["header"]["msg_type"] == "execute_input":
+            execution_count, code = msg["content"]["execution_count"], msg["content"]["code"]
+            self._logger.msg_kernel_execution.emit({"type": "stdin", "data": f"In [{execution_count}]: {code}"})
+        elif msg["header"]["msg_type"] == "stream":
+            self._logger.msg_kernel_execution.emit({"type": msg["content"]["name"], "data": msg["content"]["text"]})
+
+    def stop_execution(self):
+        if self._kernel_manager is not None:
+            self._kernel_manager.interrupt_kernel()
+            if self._kill_completed:
+                conn_file = self._kernel_manager.connection_file
+                shutdown_kernel_manager(conn_file)
+                self._logger.msg_kernel_execution.emit(dict(type="kernel_shutdown", **self._msg_head))
```

### Comparing `spine_engine-0.23.3/spine_engine/execution_managers/process_execution_manager.py` & `spine_engine-0.23.4/spine_engine/execution_managers/process_execution_manager.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,83 +1,83 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains the ProcessExecutionManager class.
-
-"""
-
-import sys
-import subprocess
-from threading import Thread
-from .execution_manager_base import ExecutionManagerBase
-from ..utils.execution_resources import one_shot_process_semaphore
-
-
-class ProcessExecutionManager(ExecutionManagerBase):
-    def __init__(self, logger, program, *args, workdir=None):
-        """Class constructor.
-
-        Args:
-            logger (LoggerInterface): a logger instance
-            program (str): Path to program to run in the subprocess (e.g. julia.exe)
-            *args: List of arguments for the program (e.g. path to script file)
-        """
-        super().__init__(logger)
-        self._process = None
-        self._program = program
-        self._args = args
-        self._workdir = workdir
-        self._stopped = False
-
-    def run_until_complete(self):
-        self._stopped = False
-        cf = subprocess.CREATE_NO_WINDOW if sys.platform == "win32" else 0  # Don't show console when frozen
-        with one_shot_process_semaphore:
-            if self._stopped:
-                return 0
-            try:
-                self._process = subprocess.Popen(
-                    [self._program, *self._args],
-                    stdout=subprocess.PIPE,
-                    stderr=subprocess.PIPE,
-                    cwd=self._workdir,
-                    creationflags=cf,
-                )
-            except OSError as e:
-                msg = dict(type="execution_failed_to_start", error=str(e), program=self._program)
-                self._logger.msg_standard_execution.emit(msg)
-                return 1
-            msg = dict(type="execution_started", program=self._program, args=" ".join(self._args))
-            self._logger.msg_standard_execution.emit(msg)
-            running = "# Running" + " ".join([self._program, *self._args])
-            self._logger.msg_standard_execution.emit({"type": "stdin", "data": running})
-            Thread(target=self._log_stdout, args=(self._process.stdout,), daemon=True).start()
-            Thread(target=self._log_stderr, args=(self._process.stderr,), daemon=True).start()
-            return self._process.wait()
-
-    def stop_execution(self):
-        self._stopped = True
-        if self._process is not None:
-            self._process.terminate()
-
-    def _log_stdout(self, stdout):
-        for line in iter(stdout.readline, b''):
-            line = line.decode("UTF8", "replace").strip()
-            self._logger.msg_proc.emit(line)
-            self._logger.msg_standard_execution.emit({"type": "stdout", "data": line})
-        stdout.close()
-
-    def _log_stderr(self, stderr):
-        for line in iter(stderr.readline, b''):
-            line = line.decode("UTF8", "replace").strip()
-            self._logger.msg_proc_error.emit(line)
-            self._logger.msg_standard_execution.emit({"type": "stderr", "data": line})
-        stderr.close()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains the ProcessExecutionManager class.
+
+"""
+
+import sys
+import subprocess
+from threading import Thread
+from .execution_manager_base import ExecutionManagerBase
+from ..utils.execution_resources import one_shot_process_semaphore
+
+
+class ProcessExecutionManager(ExecutionManagerBase):
+    def __init__(self, logger, program, *args, workdir=None):
+        """Class constructor.
+
+        Args:
+            logger (LoggerInterface): a logger instance
+            program (str): Path to program to run in the subprocess (e.g. julia.exe)
+            *args: List of arguments for the program (e.g. path to script file)
+        """
+        super().__init__(logger)
+        self._process = None
+        self._program = program
+        self._args = args
+        self._workdir = workdir
+        self._stopped = False
+
+    def run_until_complete(self):
+        self._stopped = False
+        cf = subprocess.CREATE_NO_WINDOW if sys.platform == "win32" else 0  # Don't show console when frozen
+        with one_shot_process_semaphore:
+            if self._stopped:
+                return 0
+            try:
+                self._process = subprocess.Popen(
+                    [self._program, *self._args],
+                    stdout=subprocess.PIPE,
+                    stderr=subprocess.PIPE,
+                    cwd=self._workdir,
+                    creationflags=cf,
+                )
+            except OSError as e:
+                msg = dict(type="execution_failed_to_start", error=str(e), program=self._program)
+                self._logger.msg_standard_execution.emit(msg)
+                return 1
+            msg = dict(type="execution_started", program=self._program, args=" ".join(self._args))
+            self._logger.msg_standard_execution.emit(msg)
+            running = "# Running" + " ".join([self._program, *self._args])
+            self._logger.msg_standard_execution.emit({"type": "stdin", "data": running})
+            Thread(target=self._log_stdout, args=(self._process.stdout,), daemon=True).start()
+            Thread(target=self._log_stderr, args=(self._process.stderr,), daemon=True).start()
+            return self._process.wait()
+
+    def stop_execution(self):
+        self._stopped = True
+        if self._process is not None:
+            self._process.terminate()
+
+    def _log_stdout(self, stdout):
+        for line in iter(stdout.readline, b''):
+            line = line.decode("UTF8", "replace").strip()
+            self._logger.msg_proc.emit(line)
+            self._logger.msg_standard_execution.emit({"type": "stdout", "data": line})
+        stdout.close()
+
+    def _log_stderr(self, stderr):
+        for line in iter(stderr.readline, b''):
+            line = line.decode("UTF8", "replace").strip()
+            self._logger.msg_proc_error.emit(line)
+            self._logger.msg_standard_execution.emit({"type": "stderr", "data": line})
+        stderr.close()
```

### Comparing `spine_engine-0.23.3/spine_engine/execution_managers/spine_repl.jl` & `spine_engine-0.23.4/spine_engine/execution_managers/spine_repl.jl`

 * *Ordering differences only*

 * *Files 25% similar despite different names*

```diff
@@ -1,108 +1,108 @@
-######################################################################################################################
-# Copyright (C) 2017-2021 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-module SpineREPL
-
-using Sockets
-using REPL
-using REPL.REPLCompletions
-using REPL.Terminals
-using REPL.LineEdit
-
-_exception = false
-# History related stuff. This works with julia 1.0 to 1.6 at least
-term = TTYTerminal("", stdin, IOBuffer(), stderr)
-repl = LineEditREPL(term, false)
-repl.history_file = true
-interface = REPL.setup_interface(repl)
-mistate = LineEdit.init_state(term, interface)
-prompt_state = LineEdit.state(mistate)
-prompt = LineEdit.mode(prompt_state)
-hist = prompt.hist
-search_state = LineEdit.init_state(term, LineEdit.PrefixHistoryPrompt(hist, prompt))
-
-function set_exception(value)
-	global _exception = value
-end
-
-function ping(host, port)
-	s = connect(host, port)
-    write(s, _exception ? "error" : "ok")
-    close(s)
-    REPL.history_reset_state(hist)
-end	
-
-function completions(text)
-	text = string(text)
-    join(completion_text.(REPLCompletions.completions(text, length(text))[1]), " ")
-end
-
-function history_item(text, prefix, sense)
-	backwards = sense == "backwards"
-	take!(search_state.response_buffer)
-    write(search_state.response_buffer, text)
-	REPL.history_move_prefix(search_state, hist, prefix, backwards)
-	LineEdit.input_string(search_state)
-end
-
-function add_history(line)
-	line = string(line)
-    take!(prompt_state.input_buffer)
-    write(prompt_state.input_buffer, line)
-    REPL.add_history(hist, prompt_state)
-end
-
-function is_complete(cmd)
-	cmd = string(cmd)
-	try
-		_is_complete(Meta.parse(cmd))
-	catch
-		"true"
-	end
-end
-
-_is_complete(expr::Expr) = (expr.head === :incomplete) ? "false" : "true"
-_is_complete(other) = "true"
-
-function start_server(host, port)
-	handlers = Dict(
-		"completions" => completions,
-		"add_history" => add_history,
-		"history_item" => history_item,
-		"is_complete" => is_complete
-	)
-	req_args_sep = '\u1f'  # Unit separator
-	args_sep = '\u91'  # Private Use 1
-	@async begin
-		server = listen(getaddrinfo(host), port)
-		while true
-			sock = accept(server)
-			data = String(readavailable(sock))
-			request, args = split(data, req_args_sep; limit=2)
-			args = split(args, args_sep)	
-			handler = get(handlers, request, nothing)
-			if handler === nothing
-				close(sock)
-				continue
-			end
-			response = handler(args...)
-			try
-				write(sock, response * "\n")
-				flush(sock)
-			catch
-			end
-		end
-	end
-end
-
-end  # module
-
-
+######################################################################################################################
+# Copyright (C) 2017-2021 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+module SpineREPL
+
+using Sockets
+using REPL
+using REPL.REPLCompletions
+using REPL.Terminals
+using REPL.LineEdit
+
+_exception = false
+# History related stuff. This works with julia 1.0 to 1.6 at least
+term = TTYTerminal("", stdin, IOBuffer(), stderr)
+repl = LineEditREPL(term, false)
+repl.history_file = true
+interface = REPL.setup_interface(repl)
+mistate = LineEdit.init_state(term, interface)
+prompt_state = LineEdit.state(mistate)
+prompt = LineEdit.mode(prompt_state)
+hist = prompt.hist
+search_state = LineEdit.init_state(term, LineEdit.PrefixHistoryPrompt(hist, prompt))
+
+function set_exception(value)
+	global _exception = value
+end
+
+function ping(host, port)
+	s = connect(host, port)
+    write(s, _exception ? "error" : "ok")
+    close(s)
+    REPL.history_reset_state(hist)
+end	
+
+function completions(text)
+	text = string(text)
+    join(completion_text.(REPLCompletions.completions(text, length(text))[1]), " ")
+end
+
+function history_item(text, prefix, sense)
+	backwards = sense == "backwards"
+	take!(search_state.response_buffer)
+    write(search_state.response_buffer, text)
+	REPL.history_move_prefix(search_state, hist, prefix, backwards)
+	LineEdit.input_string(search_state)
+end
+
+function add_history(line)
+	line = string(line)
+    take!(prompt_state.input_buffer)
+    write(prompt_state.input_buffer, line)
+    REPL.add_history(hist, prompt_state)
+end
+
+function is_complete(cmd)
+	cmd = string(cmd)
+	try
+		_is_complete(Meta.parse(cmd))
+	catch
+		"true"
+	end
+end
+
+_is_complete(expr::Expr) = (expr.head === :incomplete) ? "false" : "true"
+_is_complete(other) = "true"
+
+function start_server(host, port)
+	handlers = Dict(
+		"completions" => completions,
+		"add_history" => add_history,
+		"history_item" => history_item,
+		"is_complete" => is_complete
+	)
+	req_args_sep = '\u1f'  # Unit separator
+	args_sep = '\u91'  # Private Use 1
+	@async begin
+		server = listen(getaddrinfo(host), port)
+		while true
+			sock = accept(server)
+			data = String(readavailable(sock))
+			request, args = split(data, req_args_sep; limit=2)
+			args = split(args, args_sep)	
+			handler = get(handlers, request, nothing)
+			if handler === nothing
+				close(sock)
+				continue
+			end
+			response = handler(args...)
+			try
+				write(sock, response * "\n")
+				flush(sock)
+			catch
+			end
+		end
+	end
+end
+
+end  # module
+
+
```

### Comparing `spine_engine-0.23.3/spine_engine/execution_managers/spine_repl.py` & `spine_engine-0.23.4/spine_engine/execution_managers/spine_repl.py`

 * *Ordering differences only*

 * *Files 27% similar despite different names*

```diff
@@ -1,116 +1,116 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-import socketserver
-import socket
-import threading
-import code
-
-try:
-    import readline
-    import itertools
-
-    _history_offset = -1
-    _history_saved_line = ""
-except ModuleNotFoundError:
-    readline = None
-
-
-class SpineDBServer(socketserver.ThreadingMixIn, socketserver.TCPServer):
-    allow_reuse_address = True
-
-
-class _RequestHandler(socketserver.BaseRequestHandler):
-    def handle(self):
-        data = self.request.recv(1024).decode("UTF8")
-        req_args_sep = '\u001f'  # Unit separator
-        args_sep = '\u0091'  # Private Use 1
-        request, args = data.split(req_args_sep)
-        args = args.split(args_sep)
-        handler = {
-            "completions": completions,
-            "add_history": add_history,
-            "history_item": history_item,
-            "is_complete": is_complete,
-        }.get(request)
-        if handler is None:
-            return
-        response = handler(*args)
-        try:
-            self.request.sendall(bytes(response, "UTF8"))
-        except:
-            pass
-
-
-def completions(text):
-    if not readline:
-        return ""
-    return " ".join(itertools.takewhile(bool, (readline.get_completer()(text, k) for k in range(100))))
-
-
-def add_history(line):
-    if not readline:
-        return
-    readline.add_history(line)
-
-
-def history_item(text, prefix, sense):
-    if not readline:
-        return ""
-    global _history_offset  # pylint: disable=global-statement
-    global _history_saved_line  # pylint: disable=global-statement
-    if _history_offset == -1:
-        _history_saved_line = text
-    step = 1 if sense == "backwards" else -1
-    cur_len = readline.get_current_history_length()
-    while -1 <= _history_offset + step < cur_len:
-        _history_offset += step
-        if _history_offset == -1:
-            return _history_saved_line
-        item = readline.get_history_item(cur_len - _history_offset)
-        if item.startswith(prefix):
-            return item
-    return ""
-
-
-def is_complete(cmd):
-    try:
-        if code.compile_command(cmd) is None:
-            return "false"
-    except (SyntaxError, OverflowError, ValueError):
-        pass
-    return "true"
-
-
-def start_server(address):
-    """
-    Args:
-        address (tuple(str,int)): Server address
-    """
-    server = SpineDBServer(address, _RequestHandler)
-    server_thread = threading.Thread(target=server.serve_forever)
-    server_thread.daemon = True
-    server_thread.start()
-
-
-_exception = [False]
-
-
-def set_exception(value):
-    _exception[0] = value
-
-
-def ping(host, port):
-    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
-        s.connect((host, port))
-        s.sendall(b"error" if _exception[0] else b"ok")
-    global _history_offset  # pylint: disable=global-statement
-    _history_offset = -1
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+import socketserver
+import socket
+import threading
+import code
+
+try:
+    import readline
+    import itertools
+
+    _history_offset = -1
+    _history_saved_line = ""
+except ModuleNotFoundError:
+    readline = None
+
+
+class SpineDBServer(socketserver.ThreadingMixIn, socketserver.TCPServer):
+    allow_reuse_address = True
+
+
+class _RequestHandler(socketserver.BaseRequestHandler):
+    def handle(self):
+        data = self.request.recv(1024).decode("UTF8")
+        req_args_sep = '\u001f'  # Unit separator
+        args_sep = '\u0091'  # Private Use 1
+        request, args = data.split(req_args_sep)
+        args = args.split(args_sep)
+        handler = {
+            "completions": completions,
+            "add_history": add_history,
+            "history_item": history_item,
+            "is_complete": is_complete,
+        }.get(request)
+        if handler is None:
+            return
+        response = handler(*args)
+        try:
+            self.request.sendall(bytes(response, "UTF8"))
+        except:
+            pass
+
+
+def completions(text):
+    if not readline:
+        return ""
+    return " ".join(itertools.takewhile(bool, (readline.get_completer()(text, k) for k in range(100))))
+
+
+def add_history(line):
+    if not readline:
+        return
+    readline.add_history(line)
+
+
+def history_item(text, prefix, sense):
+    if not readline:
+        return ""
+    global _history_offset  # pylint: disable=global-statement
+    global _history_saved_line  # pylint: disable=global-statement
+    if _history_offset == -1:
+        _history_saved_line = text
+    step = 1 if sense == "backwards" else -1
+    cur_len = readline.get_current_history_length()
+    while -1 <= _history_offset + step < cur_len:
+        _history_offset += step
+        if _history_offset == -1:
+            return _history_saved_line
+        item = readline.get_history_item(cur_len - _history_offset)
+        if item.startswith(prefix):
+            return item
+    return ""
+
+
+def is_complete(cmd):
+    try:
+        if code.compile_command(cmd) is None:
+            return "false"
+    except (SyntaxError, OverflowError, ValueError):
+        pass
+    return "true"
+
+
+def start_server(address):
+    """
+    Args:
+        address (tuple(str,int)): Server address
+    """
+    server = SpineDBServer(address, _RequestHandler)
+    server_thread = threading.Thread(target=server.serve_forever)
+    server_thread.daemon = True
+    server_thread.start()
+
+
+_exception = [False]
+
+
+def set_exception(value):
+    _exception[0] = value
+
+
+def ping(host, port):
+    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
+        s.connect((host, port))
+        s.sendall(b"error" if _exception[0] else b"ok")
+    global _history_offset  # pylint: disable=global-statement
+    _history_offset = -1
```

### Comparing `spine_engine-0.23.3/spine_engine/load_project_items.py` & `spine_engine-0.23.4/spine_engine/load_project_items.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,75 +1,75 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Functions to load project item modules.
-
-"""
-import pathlib
-import importlib
-import importlib.util
-
-
-def load_item_specification_factories(items_package_name):
-    """
-    Loads the project item specification factories in given project item package.
-
-    Args:
-        items_package_name (str): name of the package that contains the project items
-
-    Returns:
-        dict: a map from item type to specification factory
-    """
-    items = importlib.import_module(items_package_name)
-    items_root = pathlib.Path(items.__file__).parent
-    factories = dict()
-    for child in items_root.iterdir():
-        if child.is_dir() and (
-            child.joinpath("specification_factory.py").exists()
-            or child.is_dir()
-            and child.joinpath("specification_factory.pyc").exists()
-        ):
-            spec = importlib.util.find_spec(f"{items_package_name}.{child.stem}.specification_factory")
-            m = importlib.util.module_from_spec(spec)
-            spec.loader.exec_module(m)
-            if hasattr(m, "SpecificationFactory"):
-                item_type = m.SpecificationFactory.item_type()
-                factories[item_type] = m.SpecificationFactory
-    return factories
-
-
-def load_executable_item_classes(items_package_name):
-    """
-    Loads the project item executable classes included in given project item package.
-
-    Args:
-        items_package_name (str): name of the package that contains the project items
-
-    Returns:
-        dict: a map from item type to the executable item class
-    """
-    items = importlib.import_module(items_package_name)
-    items_root = pathlib.Path(items.__file__).parent
-    classes = dict()
-    for child in items_root.iterdir():
-        if (
-            child.is_dir()
-            and child.joinpath("executable_item.py").exists()
-            or (child.is_dir() and child.joinpath("executable_item.pyc").exists())
-        ):
-            spec = importlib.util.find_spec(f"{items_package_name}.{child.stem}.executable_item")
-            m = importlib.util.module_from_spec(spec)
-            spec.loader.exec_module(m)
-            if hasattr(m, "ExecutableItem"):
-                item_class = m.ExecutableItem
-                item_type = item_class.item_type()
-                classes[item_type] = item_class
-    return classes
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Functions to load project item modules.
+
+"""
+import pathlib
+import importlib
+import importlib.util
+
+
+def load_item_specification_factories(items_package_name):
+    """
+    Loads the project item specification factories in given project item package.
+
+    Args:
+        items_package_name (str): name of the package that contains the project items
+
+    Returns:
+        dict: a map from item type to specification factory
+    """
+    items = importlib.import_module(items_package_name)
+    items_root = pathlib.Path(items.__file__).parent
+    factories = dict()
+    for child in items_root.iterdir():
+        if child.is_dir() and (
+            child.joinpath("specification_factory.py").exists()
+            or child.is_dir()
+            and child.joinpath("specification_factory.pyc").exists()
+        ):
+            spec = importlib.util.find_spec(f"{items_package_name}.{child.stem}.specification_factory")
+            m = importlib.util.module_from_spec(spec)
+            spec.loader.exec_module(m)
+            if hasattr(m, "SpecificationFactory"):
+                item_type = m.SpecificationFactory.item_type()
+                factories[item_type] = m.SpecificationFactory
+    return factories
+
+
+def load_executable_item_classes(items_package_name):
+    """
+    Loads the project item executable classes included in given project item package.
+
+    Args:
+        items_package_name (str): name of the package that contains the project items
+
+    Returns:
+        dict: a map from item type to the executable item class
+    """
+    items = importlib.import_module(items_package_name)
+    items_root = pathlib.Path(items.__file__).parent
+    classes = dict()
+    for child in items_root.iterdir():
+        if (
+            child.is_dir()
+            and child.joinpath("executable_item.py").exists()
+            or (child.is_dir() and child.joinpath("executable_item.pyc").exists())
+        ):
+            spec = importlib.util.find_spec(f"{items_package_name}.{child.stem}.executable_item")
+            m = importlib.util.module_from_spec(spec)
+            spec.loader.exec_module(m)
+            if hasattr(m, "ExecutableItem"):
+                item_class = m.ExecutableItem
+                item_type = item_class.item_type()
+                classes[item_type] = item_class
+    return classes
```

### Comparing `spine_engine-0.23.3/spine_engine/multithread_executor/__init__.py` & `spine_engine-0.23.4/spine_engine/multithread_executor/__init__.py`

 * *Ordering differences only*

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
```

### Comparing `spine_engine-0.23.3/spine_engine/multithread_executor/executor.py` & `spine_engine-0.23.4/spine_engine/multithread_executor/executor.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,55 +1,55 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Module contains multithread_executor.
-
-"""
-
-from dagster.core.definitions.executor import Int, Field, RetryMode, check, executor, get_retries_config
-
-
-@executor(
-    name="multithread",
-    config_schema={"max_concurrent": Field(Int, is_required=False, default_value=0), "retries": get_retries_config()},
-)
-def multithread_executor(init_context):
-    """A custom multithread executor.
-
-    This simple multithread executor borrows almost all the code from dagster's builtin multiprocess executor,
-    but takes a twist to use threading instead of multiprocessing.
-    To select the multithread executor, include a fragment
-    such as the following in your config:
-
-    .. code-block:: yaml
-
-        execution:
-          multithread:
-            config:
-                max_concurrent: 4
-
-    The ``max_concurrent`` arg is optional and tells the execution engine how many threads may run
-    concurrently. By default, or if you set ``max_concurrent`` to be 0, this is 100
-    (in attendance of a better method).
-
-    Execution priority can be configured using the ``dagster/priority`` tag via solid metadata,
-    where the higher the number the higher the priority. 0 is the default and both positive
-    and negative numbers can be used.
-    """
-    from dagster.core.executor.init import InitExecutorContext
-    from spine_engine.multithread_executor.multithread import MultithreadExecutor
-
-    check.inst_param(init_context, "init_context", InitExecutorContext)
-
-    return MultithreadExecutor(
-        max_concurrent=init_context.executor_config["max_concurrent"],
-        retries=RetryMode.from_config(init_context.executor_config["retries"]),
-    )
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Module contains multithread_executor.
+
+"""
+
+from dagster.core.definitions.executor import Int, Field, RetryMode, check, executor, get_retries_config
+
+
+@executor(
+    name="multithread",
+    config_schema={"max_concurrent": Field(Int, is_required=False, default_value=0), "retries": get_retries_config()},
+)
+def multithread_executor(init_context):
+    """A custom multithread executor.
+
+    This simple multithread executor borrows almost all the code from dagster's builtin multiprocess executor,
+    but takes a twist to use threading instead of multiprocessing.
+    To select the multithread executor, include a fragment
+    such as the following in your config:
+
+    .. code-block:: yaml
+
+        execution:
+          multithread:
+            config:
+                max_concurrent: 4
+
+    The ``max_concurrent`` arg is optional and tells the execution engine how many threads may run
+    concurrently. By default, or if you set ``max_concurrent`` to be 0, this is 100
+    (in attendance of a better method).
+
+    Execution priority can be configured using the ``dagster/priority`` tag via solid metadata,
+    where the higher the number the higher the priority. 0 is the default and both positive
+    and negative numbers can be used.
+    """
+    from dagster.core.executor.init import InitExecutorContext
+    from spine_engine.multithread_executor.multithread import MultithreadExecutor
+
+    check.inst_param(init_context, "init_context", InitExecutorContext)
+
+    return MultithreadExecutor(
+        max_concurrent=init_context.executor_config["max_concurrent"],
+        retries=RetryMode.from_config(init_context.executor_config["retries"]),
+    )
```

### Comparing `spine_engine-0.23.3/spine_engine/multithread_executor/multithread.py` & `spine_engine-0.23.4/spine_engine/multithread_executor/multithread.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,318 +1,318 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Module contains MultithreadExecutor.
-
-"""
-
-import os
-import sys
-from dagster import check
-from dagster.core.execution.api import (
-    create_execution_plan,
-    inner_plan_execution_iterator,
-    ExecuteRunWithPlanIterable,
-    PlanExecutionContextManager,
-)
-from dagster.core.executor.base import Executor
-from dagster.core.execution.retries import RetryMode
-from dagster.core.execution.context.system import PlanOrchestrationContext
-from dagster.core.execution.plan.objects import StepFailureData
-from dagster.core.execution.plan.plan import ExecutionPlan
-from dagster.core.events import DagsterEvent, EngineEventData
-from dagster.core.errors import DagsterError
-from dagster.utils.timing import format_duration, time_execution_scope
-from dagster.utils.error import serializable_error_info_from_exc_info
-from .thread_executor import ThreadCrashException, ThreadEvent, ThreadSystemErrorEvent, execute_thread_command
-from ..spine_engine import ED
-
-DELEGATE_MARKER = "multithread_thread_init"
-
-
-class DagsterThreadError(DagsterError):
-    """An exception has occurred in one or more of the threads dagster manages.
-    This error forwards the message and stack trace for all of the collected errors.
-    """
-
-    def __init__(self, *args, **kwargs):
-        from dagster.utils.error import SerializableErrorInfo
-
-        self.thread_error_infos = check.list_param(
-            kwargs.pop("thread_error_infos"), "thread_error_infos", SerializableErrorInfo
-        )
-        super(DagsterThreadError, self).__init__(*args, **kwargs)
-
-
-class MultithreadExecutor(Executor):
-    def __init__(self, retries, max_concurrent=None):
-        self._retries = check.inst_param(retries, "retries", RetryMode)
-        max_concurrent = max_concurrent if max_concurrent else 100  # TODO: How to determine a good amount?
-        self.max_concurrent = check.int_param(max_concurrent, "max_concurrent")
-        self._forward_resources = {}
-        self._backward_resources = {}
-
-    @property
-    def retries(self):
-        return self._retries
-
-    def execute(self, pipeline_context, execution_plan):
-        check.inst_param(pipeline_context, "pipeline_context", PlanOrchestrationContext)
-        check.inst_param(execution_plan, "execution_plan", ExecutionPlan)
-
-        limit = self.max_concurrent
-
-        yield DagsterEvent.engine_event(
-            pipeline_context,
-            "Executing steps using multithread executor (pid: {pid})".format(pid=os.getpid()),
-            event_specific_data=EngineEventData.in_process(os.getpid(), execution_plan.step_keys_to_execute),
-        )
-
-        with time_execution_scope() as timer_result:
-            with execution_plan.start(retry_mode=self.retries) as active_execution:
-                active_iters = {}
-                errors = {}
-                waiting = {}
-                iterating = {}
-                iterating_active = set()
-                iterating_failed = set()
-                jumps = pipeline_context.pipeline.get_definition().jumps
-                jump_by_source = {}
-                jump_by_solid_name = {}
-                for jump in jumps:
-                    jump_by_source[jump.source_solid] = jump
-                    non_nested_solid_names = set(jump.solid_names)
-                    for other_jump in jumps:
-                        if jump is other_jump:
-                            continue
-                        if other_jump.solid_names > jump.solid_names:
-                            continue
-                        non_nested_solid_names -= other_jump.solid_names
-                    for solid_name in non_nested_solid_names:
-                        jump_by_solid_name[solid_name] = jump
-                unfinished_jumps = set(jumps)
-                loop_iteration_counters = {}
-                steps_by_key = {}
-                while not active_execution.is_complete or active_iters:
-                    # start iterators
-                    while len(active_iters) < limit:
-                        candidate_steps = active_execution.get_steps_to_execute(limit=(limit - len(active_iters)))
-                        steps_by_key.update({step.key: step for step in candidate_steps})
-                        # Add all waiting steps
-                        candidate_steps += list(waiting.values())
-                        # Add iterating steps that don't depend on other pending iterating
-                        iterating_skipped = set()
-                        for key, step in iterating.items():
-                            dependency_keys = step.get_execution_dependency_keys()
-                            if dependency_keys & (iterating_active | iterating_skipped | iterating_failed):
-                                iterating_skipped.add(key)
-                                continue
-                            iterating_active.add(key)
-                            candidate_steps.append(step)
-
-                        executable_steps = []
-                        for step in candidate_steps:
-                            # Check if the step depends on any jumps that don't contain it
-                            predecessor_jumps = (
-                                jump for jump in unfinished_jumps if step.solid_name not in jump.solid_names
-                            )
-                            predecessor_solid_names = {item for jump in predecessor_jumps for item in jump.solid_names}
-                            predecessor_keys = {
-                                key for key, step in steps_by_key.items() if step.solid_name in predecessor_solid_names
-                            }
-                            dependency_keys = step.get_execution_dependency_keys()
-                            if dependency_keys & predecessor_keys:
-                                if step.key not in iterating:
-                                    waiting[step.key] = step
-                                continue
-                            waiting.pop(step.key, None)
-                            iterating.pop(step.key, None)
-                            executable_steps.append(step)
-
-                        if not executable_steps:
-                            break
-
-                        for step in executable_steps:
-                            step_context = pipeline_context.for_step(step)
-                            active_iters[step.key] = self.execute_step_in_thread(
-                                pipeline_context.pipeline,
-                                step.key,
-                                step.solid_name,
-                                step_context,
-                                errors,
-                                active_execution.get_known_state(),
-                            )
-                    # process active iterators
-                    empty_iters = []
-                    for key, step_iter in active_iters.items():
-                        try:
-                            event_or_none = next(step_iter)
-                            if event_or_none is None:
-                                continue
-                            yield event_or_none
-                            try:
-                                active_execution.handle_event(event_or_none)
-                            except check.CheckError:
-                                # Bypass check errors on iterating steps
-                                if key in iterating_active:
-                                    pass
-                                else:
-                                    raise
-                            # Handle loops
-                            if event_or_none.is_step_failure:
-                                # Mark failed loops as finished
-                                iterating_active.discard(key)
-                                step = steps_by_key[key]
-                                failed_jump = jump_by_solid_name.get(step.solid_name)
-                                if failed_jump is None:
-                                    continue
-                                failed_solid_names = failed_jump.solid_names
-                                for jump in jumps:
-                                    if jump.solid_names & failed_solid_names:
-                                        unfinished_jumps.discard(jump)
-                                        loop_iteration_counters.pop(jump, None)
-                                iterating_failed.add(step)
-                            elif event_or_none.is_step_success:
-                                # Process loop condition
-                                iterating_active.discard(key)
-                                step = steps_by_key[key]
-                                jump = jump_by_source.get(step.solid_name)
-                                if jump is None:
-                                    continue
-                                forward_resources = self._forward_resources.get(jump.source_solid, [])
-                                backward_resources = self._backward_resources.get(jump.destination_solid, [])
-                                jump.receive_resources_from_source(forward_resources)
-                                jump.receive_resources_from_destination(backward_resources)
-                                iteration_counter = loop_iteration_counters.setdefault(jump, 1)
-                                if jump.is_condition_true(iteration_counter):
-                                    # Put all jump steps in the iterating bucket
-                                    for k, s in steps_by_key.items():
-                                        if s.solid_name in jump.solid_names:
-                                            iterating[k] = s
-                                    # Mark all nested jumps unfinished again
-                                    for solid_name in jump.solid_names:
-                                        nested_jump = jump_by_solid_name.get(solid_name)
-                                        if nested_jump is not None:
-                                            unfinished_jumps.add(nested_jump)
-
-                                    loop_iteration_counters[jump] += 1
-                                else:
-                                    unfinished_jumps.remove(jump)
-                                    del loop_iteration_counters[jump]
-
-                        except ThreadCrashException:
-                            serializable_error = serializable_error_info_from_exc_info(sys.exc_info())
-                            yield DagsterEvent.engine_event(
-                                pipeline_context,
-                                f"Multithread executor: thread for step {key} exited unexpectedly",
-                                EngineEventData.engine_error(serializable_error),
-                            )
-                            step_failure_event = DagsterEvent.step_failure_event(
-                                step_context=pipeline_context.for_step(active_execution.get_step_by_key(key)),
-                                step_failure_data=StepFailureData(error=serializable_error, user_failure_data=None),
-                            )
-                            active_execution.handle_event(step_failure_event)
-                            yield step_failure_event
-                            empty_iters.append(key)
-                        except StopIteration:
-                            empty_iters.append(key)
-                            # TODO: Anything about loops?
-                    # clear and mark complete finished iterators
-                    for key in empty_iters:
-                        del active_iters[key]
-                        active_execution.verify_complete(pipeline_context, key)
-
-                    # process skipped and abandoned steps
-                    for event in active_execution.plan_events_iterator(pipeline_context):
-                        yield event
-
-                errs = {tid: err for tid, err in errors.items() if err}
-                if errs:
-                    raise DagsterThreadError(
-                        "During multithread execution errors occurred in threads:\n{error_list}".format(
-                            error_list="\n".join(
-                                [
-                                    "In thread {tid}: {err}".format(tid=tid, err=err.to_string())
-                                    for tid, err in errs.items()
-                                ]
-                            )
-                        ),
-                        thread_error_infos=list(errs.values()),
-                    )
-
-        yield DagsterEvent.engine_event(
-            pipeline_context,
-            "Multithread executor: parent process exiting after {duration} (pid: {pid})".format(
-                duration=format_duration(timer_result.millis), pid=os.getpid()
-            ),
-            event_specific_data=EngineEventData.multiprocess(os.getpid()),
-        )
-
-    def execute_step_in_thread(self, pipeline, step_key, solid_name, step_context, errors, known_state):
-        yield DagsterEvent.engine_event(
-            step_context, f"Spawning thread for {step_key}", EngineEventData(marker_start=DELEGATE_MARKER)
-        )
-
-        command = ThreadExecutorChildThreadCommand(
-            step_context.run_config,
-            step_context.pipeline_run,
-            step_key,
-            step_context.instance,
-            pipeline,
-            self.retries,
-            known_state,
-        )
-        for ret in execute_thread_command(command):
-            if ret is None or isinstance(ret, DagsterEvent):
-                self._save_resources(command, solid_name)
-                yield ret
-            elif isinstance(ret, ThreadEvent):
-                if isinstance(ret, ThreadSystemErrorEvent):
-                    errors[ret.tid] = ret.error_info
-            else:
-                check.failed("Unexpected return value from thread {}".format(type(ret)))
-
-    def _save_resources(self, command, solid_name):
-        for output_handle, outputs in command.output_capture.items():
-            if output_handle.output_name == f"{ED.BACKWARD}_output":
-                self._backward_resources[solid_name] = outputs
-            elif output_handle.output_name == f"{ED.FORWARD}_output":
-                self._forward_resources[solid_name] = [r for stack in outputs for r in stack]
-
-
-class ThreadExecutorChildThreadCommand:
-    def __init__(self, run_config, pipeline_run, step_key, instance, pipeline, retry_mode, known_state):
-        self.output_capture = {}
-        self._execution_plan = create_execution_plan(
-            pipeline=pipeline,
-            run_config=run_config,
-            mode=pipeline_run.mode,
-            step_keys_to_execute=[step_key],
-            known_state=known_state,
-        )
-        self._execution_context_manager = PlanExecutionContextManager(
-            pipeline=pipeline,
-            retry_mode=retry_mode,
-            execution_plan=self._execution_plan,
-            run_config=run_config,
-            pipeline_run=pipeline_run,
-            instance=instance,
-            output_capture=self.output_capture,
-        )
-
-    def execute(self):
-        return iter(
-            ExecuteRunWithPlanIterable(
-                execution_plan=self._execution_plan,
-                iterator=inner_plan_execution_iterator,
-                execution_context_manager=self._execution_context_manager,
-            )
-        )
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Module contains MultithreadExecutor.
+
+"""
+
+import os
+import sys
+from dagster import check
+from dagster.core.execution.api import (
+    create_execution_plan,
+    inner_plan_execution_iterator,
+    ExecuteRunWithPlanIterable,
+    PlanExecutionContextManager,
+)
+from dagster.core.executor.base import Executor
+from dagster.core.execution.retries import RetryMode
+from dagster.core.execution.context.system import PlanOrchestrationContext
+from dagster.core.execution.plan.objects import StepFailureData
+from dagster.core.execution.plan.plan import ExecutionPlan
+from dagster.core.events import DagsterEvent, EngineEventData
+from dagster.core.errors import DagsterError
+from dagster.utils.timing import format_duration, time_execution_scope
+from dagster.utils.error import serializable_error_info_from_exc_info
+from .thread_executor import ThreadCrashException, ThreadEvent, ThreadSystemErrorEvent, execute_thread_command
+from ..spine_engine import ED
+
+DELEGATE_MARKER = "multithread_thread_init"
+
+
+class DagsterThreadError(DagsterError):
+    """An exception has occurred in one or more of the threads dagster manages.
+    This error forwards the message and stack trace for all of the collected errors.
+    """
+
+    def __init__(self, *args, **kwargs):
+        from dagster.utils.error import SerializableErrorInfo
+
+        self.thread_error_infos = check.list_param(
+            kwargs.pop("thread_error_infos"), "thread_error_infos", SerializableErrorInfo
+        )
+        super(DagsterThreadError, self).__init__(*args, **kwargs)
+
+
+class MultithreadExecutor(Executor):
+    def __init__(self, retries, max_concurrent=None):
+        self._retries = check.inst_param(retries, "retries", RetryMode)
+        max_concurrent = max_concurrent if max_concurrent else 100  # TODO: How to determine a good amount?
+        self.max_concurrent = check.int_param(max_concurrent, "max_concurrent")
+        self._forward_resources = {}
+        self._backward_resources = {}
+
+    @property
+    def retries(self):
+        return self._retries
+
+    def execute(self, pipeline_context, execution_plan):
+        check.inst_param(pipeline_context, "pipeline_context", PlanOrchestrationContext)
+        check.inst_param(execution_plan, "execution_plan", ExecutionPlan)
+
+        limit = self.max_concurrent
+
+        yield DagsterEvent.engine_event(
+            pipeline_context,
+            "Executing steps using multithread executor (pid: {pid})".format(pid=os.getpid()),
+            event_specific_data=EngineEventData.in_process(os.getpid(), execution_plan.step_keys_to_execute),
+        )
+
+        with time_execution_scope() as timer_result:
+            with execution_plan.start(retry_mode=self.retries) as active_execution:
+                active_iters = {}
+                errors = {}
+                waiting = {}
+                iterating = {}
+                iterating_active = set()
+                iterating_failed = set()
+                jumps = pipeline_context.pipeline.get_definition().jumps
+                jump_by_source = {}
+                jump_by_solid_name = {}
+                for jump in jumps:
+                    jump_by_source[jump.source_solid] = jump
+                    non_nested_solid_names = set(jump.solid_names)
+                    for other_jump in jumps:
+                        if jump is other_jump:
+                            continue
+                        if other_jump.solid_names > jump.solid_names:
+                            continue
+                        non_nested_solid_names -= other_jump.solid_names
+                    for solid_name in non_nested_solid_names:
+                        jump_by_solid_name[solid_name] = jump
+                unfinished_jumps = set(jumps)
+                loop_iteration_counters = {}
+                steps_by_key = {}
+                while not active_execution.is_complete or active_iters:
+                    # start iterators
+                    while len(active_iters) < limit:
+                        candidate_steps = active_execution.get_steps_to_execute(limit=(limit - len(active_iters)))
+                        steps_by_key.update({step.key: step for step in candidate_steps})
+                        # Add all waiting steps
+                        candidate_steps += list(waiting.values())
+                        # Add iterating steps that don't depend on other pending iterating
+                        iterating_skipped = set()
+                        for key, step in iterating.items():
+                            dependency_keys = step.get_execution_dependency_keys()
+                            if dependency_keys & (iterating_active | iterating_skipped | iterating_failed):
+                                iterating_skipped.add(key)
+                                continue
+                            iterating_active.add(key)
+                            candidate_steps.append(step)
+
+                        executable_steps = []
+                        for step in candidate_steps:
+                            # Check if the step depends on any jumps that don't contain it
+                            predecessor_jumps = (
+                                jump for jump in unfinished_jumps if step.solid_name not in jump.solid_names
+                            )
+                            predecessor_solid_names = {item for jump in predecessor_jumps for item in jump.solid_names}
+                            predecessor_keys = {
+                                key for key, step in steps_by_key.items() if step.solid_name in predecessor_solid_names
+                            }
+                            dependency_keys = step.get_execution_dependency_keys()
+                            if dependency_keys & predecessor_keys:
+                                if step.key not in iterating:
+                                    waiting[step.key] = step
+                                continue
+                            waiting.pop(step.key, None)
+                            iterating.pop(step.key, None)
+                            executable_steps.append(step)
+
+                        if not executable_steps:
+                            break
+
+                        for step in executable_steps:
+                            step_context = pipeline_context.for_step(step)
+                            active_iters[step.key] = self.execute_step_in_thread(
+                                pipeline_context.pipeline,
+                                step.key,
+                                step.solid_name,
+                                step_context,
+                                errors,
+                                active_execution.get_known_state(),
+                            )
+                    # process active iterators
+                    empty_iters = []
+                    for key, step_iter in active_iters.items():
+                        try:
+                            event_or_none = next(step_iter)
+                            if event_or_none is None:
+                                continue
+                            yield event_or_none
+                            try:
+                                active_execution.handle_event(event_or_none)
+                            except check.CheckError:
+                                # Bypass check errors on iterating steps
+                                if key in iterating_active:
+                                    pass
+                                else:
+                                    raise
+                            # Handle loops
+                            if event_or_none.is_step_failure:
+                                # Mark failed loops as finished
+                                iterating_active.discard(key)
+                                step = steps_by_key[key]
+                                failed_jump = jump_by_solid_name.get(step.solid_name)
+                                if failed_jump is None:
+                                    continue
+                                failed_solid_names = failed_jump.solid_names
+                                for jump in jumps:
+                                    if jump.solid_names & failed_solid_names:
+                                        unfinished_jumps.discard(jump)
+                                        loop_iteration_counters.pop(jump, None)
+                                iterating_failed.add(step)
+                            elif event_or_none.is_step_success:
+                                # Process loop condition
+                                iterating_active.discard(key)
+                                step = steps_by_key[key]
+                                jump = jump_by_source.get(step.solid_name)
+                                if jump is None:
+                                    continue
+                                forward_resources = self._forward_resources.get(jump.source_solid, [])
+                                backward_resources = self._backward_resources.get(jump.destination_solid, [])
+                                jump.receive_resources_from_source(forward_resources)
+                                jump.receive_resources_from_destination(backward_resources)
+                                iteration_counter = loop_iteration_counters.setdefault(jump, 1)
+                                if jump.is_condition_true(iteration_counter):
+                                    # Put all jump steps in the iterating bucket
+                                    for k, s in steps_by_key.items():
+                                        if s.solid_name in jump.solid_names:
+                                            iterating[k] = s
+                                    # Mark all nested jumps unfinished again
+                                    for solid_name in jump.solid_names:
+                                        nested_jump = jump_by_solid_name.get(solid_name)
+                                        if nested_jump is not None:
+                                            unfinished_jumps.add(nested_jump)
+
+                                    loop_iteration_counters[jump] += 1
+                                else:
+                                    unfinished_jumps.remove(jump)
+                                    del loop_iteration_counters[jump]
+
+                        except ThreadCrashException:
+                            serializable_error = serializable_error_info_from_exc_info(sys.exc_info())
+                            yield DagsterEvent.engine_event(
+                                pipeline_context,
+                                f"Multithread executor: thread for step {key} exited unexpectedly",
+                                EngineEventData.engine_error(serializable_error),
+                            )
+                            step_failure_event = DagsterEvent.step_failure_event(
+                                step_context=pipeline_context.for_step(active_execution.get_step_by_key(key)),
+                                step_failure_data=StepFailureData(error=serializable_error, user_failure_data=None),
+                            )
+                            active_execution.handle_event(step_failure_event)
+                            yield step_failure_event
+                            empty_iters.append(key)
+                        except StopIteration:
+                            empty_iters.append(key)
+                            # TODO: Anything about loops?
+                    # clear and mark complete finished iterators
+                    for key in empty_iters:
+                        del active_iters[key]
+                        active_execution.verify_complete(pipeline_context, key)
+
+                    # process skipped and abandoned steps
+                    for event in active_execution.plan_events_iterator(pipeline_context):
+                        yield event
+
+                errs = {tid: err for tid, err in errors.items() if err}
+                if errs:
+                    raise DagsterThreadError(
+                        "During multithread execution errors occurred in threads:\n{error_list}".format(
+                            error_list="\n".join(
+                                [
+                                    "In thread {tid}: {err}".format(tid=tid, err=err.to_string())
+                                    for tid, err in errs.items()
+                                ]
+                            )
+                        ),
+                        thread_error_infos=list(errs.values()),
+                    )
+
+        yield DagsterEvent.engine_event(
+            pipeline_context,
+            "Multithread executor: parent process exiting after {duration} (pid: {pid})".format(
+                duration=format_duration(timer_result.millis), pid=os.getpid()
+            ),
+            event_specific_data=EngineEventData.multiprocess(os.getpid()),
+        )
+
+    def execute_step_in_thread(self, pipeline, step_key, solid_name, step_context, errors, known_state):
+        yield DagsterEvent.engine_event(
+            step_context, f"Spawning thread for {step_key}", EngineEventData(marker_start=DELEGATE_MARKER)
+        )
+
+        command = ThreadExecutorChildThreadCommand(
+            step_context.run_config,
+            step_context.pipeline_run,
+            step_key,
+            step_context.instance,
+            pipeline,
+            self.retries,
+            known_state,
+        )
+        for ret in execute_thread_command(command):
+            if ret is None or isinstance(ret, DagsterEvent):
+                self._save_resources(command, solid_name)
+                yield ret
+            elif isinstance(ret, ThreadEvent):
+                if isinstance(ret, ThreadSystemErrorEvent):
+                    errors[ret.tid] = ret.error_info
+            else:
+                check.failed("Unexpected return value from thread {}".format(type(ret)))
+
+    def _save_resources(self, command, solid_name):
+        for output_handle, outputs in command.output_capture.items():
+            if output_handle.output_name == f"{ED.BACKWARD}_output":
+                self._backward_resources[solid_name] = outputs
+            elif output_handle.output_name == f"{ED.FORWARD}_output":
+                self._forward_resources[solid_name] = [r for stack in outputs for r in stack]
+
+
+class ThreadExecutorChildThreadCommand:
+    def __init__(self, run_config, pipeline_run, step_key, instance, pipeline, retry_mode, known_state):
+        self.output_capture = {}
+        self._execution_plan = create_execution_plan(
+            pipeline=pipeline,
+            run_config=run_config,
+            mode=pipeline_run.mode,
+            step_keys_to_execute=[step_key],
+            known_state=known_state,
+        )
+        self._execution_context_manager = PlanExecutionContextManager(
+            pipeline=pipeline,
+            retry_mode=retry_mode,
+            execution_plan=self._execution_plan,
+            run_config=run_config,
+            pipeline_run=pipeline_run,
+            instance=instance,
+            output_capture=self.output_capture,
+        )
+
+    def execute(self):
+        return iter(
+            ExecuteRunWithPlanIterable(
+                execution_plan=self._execution_plan,
+                iterator=inner_plan_execution_iterator,
+                execution_context_manager=self._execution_context_manager,
+            )
+        )
```

### Comparing `spine_engine-0.23.3/spine_engine/project_item/__init__.py` & `spine_engine-0.23.4/spine_engine/server/__init__.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,15 +1,15 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-This subpackage contains base classes for project items.
-
-"""
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+This package contains Remote Server classes of the Spine Engine.
+
+"""
```

### Comparing `spine_engine-0.23.3/spine_engine/project_item/connection.py` & `spine_engine-0.23.4/spine_engine/project_item/connection.py`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,686 +1,686 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-"""
-Provides connection classes for linking project items.
-
-"""
-from dataclasses import asdict, dataclass, field
-import os
-import subprocess
-import tempfile
-from contextlib import ExitStack
-from datapackage import Package
-from multiprocessing import Lock
-from spinedb_api import DatabaseMapping, SpineDBAPIError, SpineDBVersionError
-from spinedb_api.filters.scenario_filter import SCENARIO_FILTER_TYPE
-from spinedb_api.filters.tool_filter import TOOL_FILTER_TYPE
-from spinedb_api.purge import purge_url
-from spine_engine.project_item.project_item_resource import (
-    file_resource,
-    make_cmd_line_arg,
-    expand_cmd_line_args,
-    labelled_resource_args,
-)
-from spine_engine.utils.helpers import (
-    resolve_python_interpreter,
-    ItemExecutionFinishState,
-    PartCount,
-    ExecutionDirection as ED,
-)
-from spine_engine.utils.queue_logger import QueueLogger
-
-
-class ConnectionBase:
-    """Base class for connections between two project items."""
-
-    def __init__(self, source_name, source_position, destination_name, destination_position):
-        """
-        Args:
-            source_name (str): source project item's name
-            source_position (str): source anchor's position
-            destination_name (str): destination project item's name
-            destination_position (str): destination anchor's position
-        """
-        self.source = source_name
-        self._source_position = source_position
-        self.destination = destination_name
-        self._destination_position = destination_position
-        self._logger = None
-
-    def __eq__(self, other):
-        if not isinstance(other, ConnectionBase):
-            return NotImplemented
-        return (
-            self.source == other.source
-            and self._source_position == other._source_position
-            and self.destination == other.destination
-            and self._destination_position == other._destination_position
-        )
-
-    @property
-    def name(self):
-        return f"from {self.source} to {self.destination}"
-
-    @property
-    def destination_position(self):
-        """Anchor's position on destination item."""
-        return self._destination_position
-
-    @destination_position.setter
-    def destination_position(self, destination_position):
-        """Set anchor's position on destination item."""
-        self._destination_position = destination_position
-
-    @property
-    def source_position(self):
-        """Anchor's position on source item."""
-        return self._source_position
-
-    @source_position.setter
-    def source_position(self, source_position):
-        """Set anchor's position on source item."""
-        self._source_position = source_position
-
-    def __hash__(self):
-        return hash((self.source, self._source_position, self.destination, self._destination_position))
-
-    def ready_to_execute(self):
-        """Validates the internal state of connection before execution.
-
-        Subclasses can implement this method to do the appropriate work.
-
-        Returns:
-            bool: True if connection is ready for execution, False otherwise
-        """
-        return True
-
-    def notifications(self):
-        """Returns connection validation messages.
-
-        Returns:
-            list of str: notifications
-        """
-        return []
-
-    def to_dict(self):
-        """Returns a dictionary representation of this connection.
-
-        Returns:
-            dict: serialized Connection
-        """
-        return {
-            "name": self.name,
-            "from": [self.source, self._source_position],
-            "to": [self.destination, self._destination_position],
-        }
-
-    @staticmethod
-    def _constructor_args_from_dict(connection_dict):
-        """Parses __init__() arguments from serialized connection.
-
-        Args:
-            connection_dict (dict): serialized ConnectionBase
-
-        Returns:
-            dict: keyword arguments suitable for constructing ConnectionBase
-        """
-        source_name, source_anchor = connection_dict["from"]
-        destination_name, destination_anchor = connection_dict["to"]
-        return {
-            "source_name": source_name,
-            "source_position": source_anchor,
-            "destination_name": destination_name,
-            "destination_position": destination_anchor,
-        }
-
-    def receive_resources_from_source(self, resources):
-        """
-        Receives resources from source item.
-
-        Args:
-            resources (Iterable of ProjectItemResource): source item's resources
-        """
-
-    def receive_resources_from_destination(self, resources):
-        """
-        Receives resources from destination item.
-
-        Args:
-            resources (Iterable of ProjectItemResource): destination item's resources
-        """
-
-    def make_logger(self, queue):
-        self._logger = QueueLogger(queue, self.name, None, dict())
-
-    def emit_flash(self):
-        self._logger.flash.emit()
-
-
-@dataclass
-class FilterSettings:
-    """Filter settings for resource converting connections."""
-
-    known_filters: dict = field(default_factory=dict)
-    """mapping from resource labels and filter types to filter online statuses"""
-    auto_online: bool = True
-    """if True, set unknown filters automatically online"""
-
-    def has_filters(self):
-        """Tests if there are filters.
-
-        Returns:
-            bool: True if filters of any type exists, False otherwise
-        """
-        for filters_by_type in self.known_filters.values():
-            for filters in filters_by_type.values():
-                if filters:
-                    return True
-        return False
-
-    def has_any_filter_online(self):
-        """Tests in any filter is online.
-
-        Returns:
-            bool: True if any filter is online, False otherwise
-        """
-        for filters_by_type in self.known_filters.values():
-            for filters in filters_by_type.values():
-                if any(filters.values()):
-                    return True
-        return False
-
-    def has_filter_online(self, filter_type):
-        """Tests if any filter of given type is online.
-
-        Args:
-            filter_type (str): filter type to test
-
-        Returns:
-            bool: True if any filter of filter_type is online, False otherwise
-        """
-        for filters_by_type in self.known_filters.values():
-            if any(filters_by_type.get(filter_type, {}).values()):
-                return True
-        return False
-
-    def to_dict(self):
-        """Stores the settings to a dict.
-
-        Returns:
-            dict: serialized settings
-        """
-        return asdict(self)
-
-    @staticmethod
-    def from_dict(settings_dict):
-        """Restores the settings from a dict.
-
-        Args:
-            settings_dict (dict): serialized settings
-
-        Returns:
-            FilterSettings: restored settings
-        """
-        return FilterSettings(**settings_dict)
-
-
-class ResourceConvertingConnection(ConnectionBase):
-    def __init__(
-        self, source_name, source_position, destination_name, destination_position, options=None, filter_settings=None
-    ):
-        """
-        Args:
-            source_name (str): source project item's name
-            source_position (str): source anchor's position
-            destination_name (str): destination project item's name
-            destination_position (str): destination anchor's position
-            options (dict, optional): any additional options
-            filter_settings (FilterSettings, optional): filter settings
-        """
-        super().__init__(source_name, source_position, destination_name, destination_position)
-        self._filter_settings = filter_settings if filter_settings is not None else FilterSettings()
-        self.options = options if options is not None else dict()
-        self._resources = set()
-
-    def __eq__(self, other):
-        if not isinstance(other, ResourceConvertingConnection):
-            return NotImplemented
-        return (
-            super().__eq__(other) and self._filter_settings == other._filter_settings and self.options == other.options
-        )
-
-    @property
-    def use_datapackage(self):
-        """True if datapackage is used, False otherwise"""
-        return self.options.get("use_datapackage", False)
-
-    @property
-    def use_memory_db(self):
-        """True if in-memory database is used, False otherwise"""
-        return self.options.get("use_memory_db", False)
-
-    @property
-    def purge_before_writing(self):
-        """True if purge before writing is active, False otherwise"""
-        return self.options.get("purge_before_writing", False)
-
-    @property
-    def purge_settings(self):
-        """A dictionary mapping DB item types to a boolean value indicating whether to wipe them or not,
-        or None if the entire DB should suffer."""
-        return self.options.get("purge_settings")
-
-    @property
-    def write_index(self):
-        """The index this connection has in concurrent writing. Defaults to 1, lower writes earlier.
-        If two or more connections have the same, then no order is enforced among them.
-        """
-        return self.options.get("write_index", 1)
-
-    @property
-    def is_filter_online_by_default(self):
-        """True if new filters should be online by default."""
-        return self._filter_settings.auto_online
-
-    def has_filters_online(self):
-        """Tests if connection has any online filters.
-
-        Returns:
-            bool: True if there are online filters, False otherwise
-        """
-        return self._filter_settings.has_any_filter_online()
-
-    def require_filter_online(self, filter_type):
-        """Tests if online filters of given type are required for execution.
-
-        Args:
-            filter_type (str): filter type
-
-        Returns:
-            bool: True if online filters are required, False otherwise
-        """
-        return self.options.get("require_" + filter_type, False)
-
-    def notifications(self):
-        """See base class."""
-        notifications = []
-        for filter_type in (SCENARIO_FILTER_TYPE, TOOL_FILTER_TYPE):
-            filter_settings = self._filter_settings
-            if self.require_filter_online(filter_type) and (
-                not filter_settings.has_filter_online(filter_type)
-                if filter_settings.has_filters()
-                else not filter_settings.auto_online
-            ):
-                filter_name = {SCENARIO_FILTER_TYPE: "scenario", TOOL_FILTER_TYPE: "tool"}[filter_type]
-                notifications.append(f"At least one {filter_name} filter must be active.")
-        return notifications
-
-    def receive_resources_from_source(self, resources):
-        """See base class."""
-        self._resources = {r for r in resources if r.type_ == "database" and r.filterable}
-
-    def clean_up_backward_resources(self, resources):
-        self._do_purge_before_writing(resources)
-
-    def convert_backward_resources(self, resources, sibling_connections):
-        """Called when advertising resources through this connection *in the BACKWARD direction*.
-        Takes the initial list of resources advertised by the destination item and returns a new list,
-        which is the one finally advertised.
-
-        Args:
-            resources (list of ProjectItemResource): Resources to convert
-            sibling_connections (list of Connection): Sibling connections
-
-        Returns:
-            list of ProjectItemResource
-        """
-        return self._apply_use_memory_db(self._apply_write_index(resources, sibling_connections))
-
-    def convert_forward_resources(self, resources):
-        """Called when advertising resources through this connection *in the FORWARD direction*.
-        Takes the initial list of resources advertised by the source item and returns a new list,
-        which is the one finally advertised.
-
-        Args:
-            resources (list of ProjectItemResource): Resources to convert
-
-        Returns:
-            list of ProjectItemResource
-        """
-        return self._apply_use_memory_db(self._apply_use_datapackage(resources))
-
-    def _do_purge_before_writing(self, resources):
-        if self.purge_before_writing:
-            to_urls = (r.url for r in resources if r.type_ == "database")
-            for url in to_urls:
-                purge_url(url, self.purge_settings, self._logger)
-
-    def _apply_use_memory_db(self, resources):
-        if not self.use_memory_db:
-            return resources
-        final_resources = []
-        for r in resources:
-            if r.type_ == "database":
-                r = r.clone(additional_metadata={"memory": True})
-            final_resources.append(r)
-        return final_resources
-
-    def _apply_write_index(self, resources, sibling_connections):
-        final_resources = []
-        precursors = set(c.name for c in sibling_connections if c.write_index < self.write_index)
-        for r in resources:
-            if r.type_ == "database":
-                r = r.clone(
-                    additional_metadata={"current": self.name, "precursors": precursors, "part_count": PartCount()}
-                )
-            final_resources.append(r)
-        return final_resources
-
-    def _apply_use_datapackage(self, resources):
-        if not self.use_datapackage:
-            return resources
-        # Split CSVs from the rest of resources
-        final_resources = []
-        csv_filepaths = []
-        for r in resources:
-            if r.hasfilepath and os.path.splitext(r.path)[1].lower() == ".csv":
-                csv_filepaths.append(r.path)
-                continue
-            final_resources.append(r)
-        if not csv_filepaths:
-            return final_resources
-        # Build Package from CSVs and add it to the resources
-        base_path = os.path.dirname(os.path.commonpath(csv_filepaths))
-        package = Package(base_path=base_path)
-        for path in csv_filepaths:
-            package.add_resource({"path": os.path.relpath(path, base_path)})
-        package_path = os.path.join(base_path, "datapackage.json")
-        package.save(package_path)
-        provider = resources[0].provider_name
-        package_resource = file_resource(provider, package_path, label=f"datapackage@{provider}")
-        package_resource.metadata = resources[0].metadata
-        final_resources.append(package_resource)
-        return final_resources
-
-    def ready_to_execute(self):
-        """See base class."""
-        for filter_type in (SCENARIO_FILTER_TYPE, TOOL_FILTER_TYPE):
-            if self.require_filter_online(filter_type) and not self._filter_settings.has_filter_online(filter_type):
-                return False
-        return True
-
-    def to_dict(self):
-        """Returns a dictionary representation of this Connection.
-
-        Returns:
-            dict: serialized Connection
-        """
-        d = super().to_dict()
-        if self.options:
-            d["options"] = self.options.copy()
-        if self._filter_settings.has_filters():
-            d["filter_settings"] = self._filter_settings.to_dict()
-        return d
-
-    @staticmethod
-    def _constructor_args_from_dict(connection_dict):
-        """See base class."""
-        kw_args = ConnectionBase._constructor_args_from_dict(connection_dict)
-        kw_args["options"] = connection_dict.get("options")
-        filter_settings = connection_dict.get("filter_settings")
-        if filter_settings is not None:
-            kw_args["filter_settings"] = FilterSettings.from_dict(filter_settings)
-        else:
-            disabled_names = connection_dict.get("disabled_filters")
-            if disabled_names is not None:
-                known_filters = _restore_legacy_disabled_filters(disabled_names)
-                kw_args["filter_settings"] = FilterSettings(known_filters)
-        return kw_args
-
-
-class Connection(ResourceConvertingConnection):
-    """Represents a connection between two project items."""
-
-    def __init__(
-        self, source_name, source_position, destination_name, destination_position, options=None, filter_settings=None
-    ):
-        """
-        Args:
-            source_name (str): source project item's name
-            source_position (str): source anchor's position
-            destination_name (str): destination project item's name
-            destination_position (str): destination anchor's position
-            options (dict, optional): any additional options
-            filter_settings (FilterSettings, optional): filter settings
-        """
-        super().__init__(source_name, source_position, destination_name, destination_position, options, filter_settings)
-        self._enabled_filter_names = None
-        self._source_visited = False
-
-    def visit_source(self):
-        self._source_visited = True
-
-    def visit_destination(self):
-        if not self._source_visited:
-            # Can happen in loop execution
-            return
-        self._source_visited = False
-        self.emit_flash()
-
-    def enabled_filters(self, resource_label):
-        """Returns enabled filter names for given resource label.
-
-        Args:
-            resource_label (str): resource label
-
-        Returns:
-            dict: mapping from filter type to list of online filter names
-        """
-        if self._enabled_filter_names is None:
-            self._prepare_enabled_filter_names()
-        return self._enabled_filter_names.get(resource_label)
-
-    def _prepare_enabled_filter_names(self):
-        """Reads filter information from database."""
-        self._enabled_filter_names = {}
-        for resource in self._resources:
-            url = resource.url
-            if not url:
-                continue
-            try:
-                db_map = DatabaseMapping(url)
-            except (SpineDBAPIError, SpineDBVersionError):
-                continue
-            try:
-                scenario_filter_settings = self._filter_settings.known_filters.get(resource.label, {}).get(
-                    SCENARIO_FILTER_TYPE, {}
-                )
-                available_scenarios = {row.name for row in db_map.query(db_map.scenario_sq)}
-                enabled_scenarios = set()
-                for name in available_scenarios:
-                    if scenario_filter_settings.get(name, self._filter_settings.auto_online):
-                        enabled_scenarios.add(name)
-                if enabled_scenarios:
-                    self._enabled_filter_names.setdefault(resource.label, {})[SCENARIO_FILTER_TYPE] = sorted(
-                        list(enabled_scenarios)
-                    )
-                tool_filter_settings = self._filter_settings.known_filters.get(resource.label, {}).get(
-                    TOOL_FILTER_TYPE, {}
-                )
-                available_tools = {row.name for row in db_map.query(db_map.tool_sq)}
-                enabled_tools = set()
-                for name in available_tools:
-                    if tool_filter_settings.get(name, self._filter_settings.auto_online):
-                        enabled_tools.add(name)
-                if enabled_tools:
-                    self._enabled_filter_names.setdefault(resource.label, {})[TOOL_FILTER_TYPE] = sorted(enabled_tools)
-            finally:
-                db_map.connection.close()
-
-    @classmethod
-    def from_dict(cls, connection_dict):
-        """Restores a connection from dictionary.
-
-        Args:
-            connection_dict (dict): connection dictionary
-
-        Returns:
-            Connection: restored connection
-        """
-        kw_args_from_dict = cls._constructor_args_from_dict(connection_dict)
-        return cls(**kw_args_from_dict)
-
-
-class Jump(ConnectionBase):
-    """Represents a conditional jump between two project items."""
-
-    def __init__(
-        self, source_name, source_position, destination_name, destination_position, condition=None, cmd_line_args=()
-    ):
-        """
-        Args:
-            source_name (str): source project item's name
-            source_position (str): source anchor's position
-            destination_name (str): destination project item's name
-            destination_position (str): destination anchor's position
-            condition (dict): jump condition
-        """
-        super().__init__(source_name, source_position, destination_name, destination_position)
-        self.condition = condition if condition is not None else {"type": "python-script", "script": "exit(1)"}
-        self._resources_from_source = set()
-        self._resources_from_destination = set()
-        self.cmd_line_args = list(cmd_line_args)
-        self._engine = None
-        self.source_solid = None
-        self.destination_solid = None
-        self.item_names = set()
-        self.solid_names = set()
-
-    def set_engine(self, engine):
-        self._engine = engine
-
-    @property
-    def resources(self):
-        return self._resources_from_source | self._resources_from_destination
-
-    def update_cmd_line_args(self, cmd_line_args):
-        self.cmd_line_args = cmd_line_args
-
-    def receive_resources_from_source(self, resources):
-        """See base class."""
-        self._resources_from_source = set(resources)
-
-    def receive_resources_from_destination(self, resources):
-        """See base class."""
-        self._resources_from_destination = set(resources)
-
-    def is_condition_true(self, jump_counter):
-        """Evaluates jump condition.
-
-        Args:
-            jump_counter (int): how many times jump has been executed
-
-        Returns:
-            bool: True if jump should be executed, False otherwise
-        """
-        iterate = False
-        if self.condition["type"] == "python-script":
-            iterate = self._is_python_script_condition_true(jump_counter)
-        elif self.condition["type"] == "tool-specification":
-            iterate = self._is_tool_specification_condition_true(jump_counter)
-        if iterate:
-            self.emit_flash()
-            self._update_items()
-        return iterate
-
-    def _update_items(self):
-        for item_name in self.item_names:
-            item = self._engine.make_item(item_name, ED.NONE)
-            forward_resources, backward_resources = self._engine.resources_per_item[item_name]
-            item.update(forward_resources, backward_resources)
-
-    def _is_python_script_condition_true(self, jump_counter):
-        script = self.condition["script"]
-        if not script.strip():
-            return False
-        with ExitStack() as stack:
-            labelled_args = labelled_resource_args(self.resources, stack)
-            expanded_args = expand_cmd_line_args(self.cmd_line_args + [jump_counter], labelled_args, self._logger)
-            with tempfile.TemporaryFile("w+", encoding="utf-8") as script_file:
-                script_file.write(script)
-                script_file.seek(0)
-                python = resolve_python_interpreter("")
-                result = subprocess.run(
-                    [python, "-", *expanded_args], encoding="utf-8", stdin=script_file, capture_output=True
-                )
-                if result.stdout:
-                    self._logger.msg_proc.emit(result.stdout)
-                if result.stderr:
-                    self._logger.msg_proc_error.emit(result.stderr)
-                return result.returncode == 0
-
-    def _is_tool_specification_condition_true(self, jump_counter):
-        item_dict = {
-            "type": "Tool",
-            "execute_in_work": False,
-            "specification": self.condition["specification"],
-            "cmd_line_args": [arg.to_dict() for arg in self.cmd_line_args] + [str(jump_counter)],
-        }
-        condition_tool = self._engine.do_make_item(self.name, item_dict, self._logger)
-        return (
-            condition_tool.execute(list(self._resources_from_source), list(self._resources_from_destination), Lock())
-            == ItemExecutionFinishState.SUCCESS
-        )
-
-    @classmethod
-    def from_dict(cls, jump_dict, **kwargs):
-        """Restores a Jump from dictionary.
-
-        Args:
-            jump_dict (dict): serialized jump
-            **kwargs: extra keyword arguments passed to constructor
-
-        Returns:
-            Jump: restored jump
-        """
-        super_kw_ags = cls._constructor_args_from_dict(jump_dict)
-        condition = jump_dict["condition"]
-        cmd_line_args = jump_dict.get("cmd_line_args", [])
-        cmd_line_args = [make_cmd_line_arg(arg) for arg in cmd_line_args]
-        return cls(condition=condition, cmd_line_args=cmd_line_args, **super_kw_ags, **kwargs)
-
-    def to_dict(self):
-        """Returns a dictionary representation of this Jump.
-
-        Returns:
-            dict: serialized Jump
-        """
-        d = super().to_dict()
-        d["condition"] = self.condition
-        d["cmd_line_args"] = [arg.to_dict() for arg in self.cmd_line_args]
-        return d
-
-
-def _restore_legacy_disabled_filters(disabled_filter_names):
-    """Converts legacy serialized disabled filter names to known filters dict.
-
-    Args:
-        disabled_filter_names (dict): disabled filter names with names stored as lists
-
-    Returns:
-        dict: known filters
-    """
-    converted = {}
-    for label, names_by_type in disabled_filter_names.items():
-        converted_names_by_type = converted.setdefault(label, {})
-        for filter_type, names in names_by_type.items():
-            converted_names_by_type[filter_type] = {name: False for name in names}
-    return converted
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+"""
+Provides connection classes for linking project items.
+
+"""
+from dataclasses import asdict, dataclass, field
+import os
+import subprocess
+import tempfile
+from contextlib import ExitStack
+from datapackage import Package
+from multiprocessing import Lock
+from spinedb_api import DatabaseMapping, SpineDBAPIError, SpineDBVersionError
+from spinedb_api.filters.scenario_filter import SCENARIO_FILTER_TYPE
+from spinedb_api.filters.tool_filter import TOOL_FILTER_TYPE
+from spinedb_api.purge import purge_url
+from spine_engine.project_item.project_item_resource import (
+    file_resource,
+    make_cmd_line_arg,
+    expand_cmd_line_args,
+    labelled_resource_args,
+)
+from spine_engine.utils.helpers import (
+    resolve_python_interpreter,
+    ItemExecutionFinishState,
+    PartCount,
+    ExecutionDirection as ED,
+)
+from spine_engine.utils.queue_logger import QueueLogger
+
+
+class ConnectionBase:
+    """Base class for connections between two project items."""
+
+    def __init__(self, source_name, source_position, destination_name, destination_position):
+        """
+        Args:
+            source_name (str): source project item's name
+            source_position (str): source anchor's position
+            destination_name (str): destination project item's name
+            destination_position (str): destination anchor's position
+        """
+        self.source = source_name
+        self._source_position = source_position
+        self.destination = destination_name
+        self._destination_position = destination_position
+        self._logger = None
+
+    def __eq__(self, other):
+        if not isinstance(other, ConnectionBase):
+            return NotImplemented
+        return (
+            self.source == other.source
+            and self._source_position == other._source_position
+            and self.destination == other.destination
+            and self._destination_position == other._destination_position
+        )
+
+    @property
+    def name(self):
+        return f"from {self.source} to {self.destination}"
+
+    @property
+    def destination_position(self):
+        """Anchor's position on destination item."""
+        return self._destination_position
+
+    @destination_position.setter
+    def destination_position(self, destination_position):
+        """Set anchor's position on destination item."""
+        self._destination_position = destination_position
+
+    @property
+    def source_position(self):
+        """Anchor's position on source item."""
+        return self._source_position
+
+    @source_position.setter
+    def source_position(self, source_position):
+        """Set anchor's position on source item."""
+        self._source_position = source_position
+
+    def __hash__(self):
+        return hash((self.source, self._source_position, self.destination, self._destination_position))
+
+    def ready_to_execute(self):
+        """Validates the internal state of connection before execution.
+
+        Subclasses can implement this method to do the appropriate work.
+
+        Returns:
+            bool: True if connection is ready for execution, False otherwise
+        """
+        return True
+
+    def notifications(self):
+        """Returns connection validation messages.
+
+        Returns:
+            list of str: notifications
+        """
+        return []
+
+    def to_dict(self):
+        """Returns a dictionary representation of this connection.
+
+        Returns:
+            dict: serialized Connection
+        """
+        return {
+            "name": self.name,
+            "from": [self.source, self._source_position],
+            "to": [self.destination, self._destination_position],
+        }
+
+    @staticmethod
+    def _constructor_args_from_dict(connection_dict):
+        """Parses __init__() arguments from serialized connection.
+
+        Args:
+            connection_dict (dict): serialized ConnectionBase
+
+        Returns:
+            dict: keyword arguments suitable for constructing ConnectionBase
+        """
+        source_name, source_anchor = connection_dict["from"]
+        destination_name, destination_anchor = connection_dict["to"]
+        return {
+            "source_name": source_name,
+            "source_position": source_anchor,
+            "destination_name": destination_name,
+            "destination_position": destination_anchor,
+        }
+
+    def receive_resources_from_source(self, resources):
+        """
+        Receives resources from source item.
+
+        Args:
+            resources (Iterable of ProjectItemResource): source item's resources
+        """
+
+    def receive_resources_from_destination(self, resources):
+        """
+        Receives resources from destination item.
+
+        Args:
+            resources (Iterable of ProjectItemResource): destination item's resources
+        """
+
+    def make_logger(self, queue):
+        self._logger = QueueLogger(queue, self.name, None, dict())
+
+    def emit_flash(self):
+        self._logger.flash.emit()
+
+
+@dataclass
+class FilterSettings:
+    """Filter settings for resource converting connections."""
+
+    known_filters: dict = field(default_factory=dict)
+    """mapping from resource labels and filter types to filter online statuses"""
+    auto_online: bool = True
+    """if True, set unknown filters automatically online"""
+
+    def has_filters(self):
+        """Tests if there are filters.
+
+        Returns:
+            bool: True if filters of any type exists, False otherwise
+        """
+        for filters_by_type in self.known_filters.values():
+            for filters in filters_by_type.values():
+                if filters:
+                    return True
+        return False
+
+    def has_any_filter_online(self):
+        """Tests in any filter is online.
+
+        Returns:
+            bool: True if any filter is online, False otherwise
+        """
+        for filters_by_type in self.known_filters.values():
+            for filters in filters_by_type.values():
+                if any(filters.values()):
+                    return True
+        return False
+
+    def has_filter_online(self, filter_type):
+        """Tests if any filter of given type is online.
+
+        Args:
+            filter_type (str): filter type to test
+
+        Returns:
+            bool: True if any filter of filter_type is online, False otherwise
+        """
+        for filters_by_type in self.known_filters.values():
+            if any(filters_by_type.get(filter_type, {}).values()):
+                return True
+        return False
+
+    def to_dict(self):
+        """Stores the settings to a dict.
+
+        Returns:
+            dict: serialized settings
+        """
+        return asdict(self)
+
+    @staticmethod
+    def from_dict(settings_dict):
+        """Restores the settings from a dict.
+
+        Args:
+            settings_dict (dict): serialized settings
+
+        Returns:
+            FilterSettings: restored settings
+        """
+        return FilterSettings(**settings_dict)
+
+
+class ResourceConvertingConnection(ConnectionBase):
+    def __init__(
+        self, source_name, source_position, destination_name, destination_position, options=None, filter_settings=None
+    ):
+        """
+        Args:
+            source_name (str): source project item's name
+            source_position (str): source anchor's position
+            destination_name (str): destination project item's name
+            destination_position (str): destination anchor's position
+            options (dict, optional): any additional options
+            filter_settings (FilterSettings, optional): filter settings
+        """
+        super().__init__(source_name, source_position, destination_name, destination_position)
+        self._filter_settings = filter_settings if filter_settings is not None else FilterSettings()
+        self.options = options if options is not None else dict()
+        self._resources = set()
+
+    def __eq__(self, other):
+        if not isinstance(other, ResourceConvertingConnection):
+            return NotImplemented
+        return (
+            super().__eq__(other) and self._filter_settings == other._filter_settings and self.options == other.options
+        )
+
+    @property
+    def use_datapackage(self):
+        """True if datapackage is used, False otherwise"""
+        return self.options.get("use_datapackage", False)
+
+    @property
+    def use_memory_db(self):
+        """True if in-memory database is used, False otherwise"""
+        return self.options.get("use_memory_db", False)
+
+    @property
+    def purge_before_writing(self):
+        """True if purge before writing is active, False otherwise"""
+        return self.options.get("purge_before_writing", False)
+
+    @property
+    def purge_settings(self):
+        """A dictionary mapping DB item types to a boolean value indicating whether to wipe them or not,
+        or None if the entire DB should suffer."""
+        return self.options.get("purge_settings")
+
+    @property
+    def write_index(self):
+        """The index this connection has in concurrent writing. Defaults to 1, lower writes earlier.
+        If two or more connections have the same, then no order is enforced among them.
+        """
+        return self.options.get("write_index", 1)
+
+    @property
+    def is_filter_online_by_default(self):
+        """True if new filters should be online by default."""
+        return self._filter_settings.auto_online
+
+    def has_filters_online(self):
+        """Tests if connection has any online filters.
+
+        Returns:
+            bool: True if there are online filters, False otherwise
+        """
+        return self._filter_settings.has_any_filter_online()
+
+    def require_filter_online(self, filter_type):
+        """Tests if online filters of given type are required for execution.
+
+        Args:
+            filter_type (str): filter type
+
+        Returns:
+            bool: True if online filters are required, False otherwise
+        """
+        return self.options.get("require_" + filter_type, False)
+
+    def notifications(self):
+        """See base class."""
+        notifications = []
+        for filter_type in (SCENARIO_FILTER_TYPE, TOOL_FILTER_TYPE):
+            filter_settings = self._filter_settings
+            if self.require_filter_online(filter_type) and (
+                not filter_settings.has_filter_online(filter_type)
+                if filter_settings.has_filters()
+                else not filter_settings.auto_online
+            ):
+                filter_name = {SCENARIO_FILTER_TYPE: "scenario", TOOL_FILTER_TYPE: "tool"}[filter_type]
+                notifications.append(f"At least one {filter_name} filter must be active.")
+        return notifications
+
+    def receive_resources_from_source(self, resources):
+        """See base class."""
+        self._resources = {r for r in resources if r.type_ == "database" and r.filterable}
+
+    def clean_up_backward_resources(self, resources):
+        self._do_purge_before_writing(resources)
+
+    def convert_backward_resources(self, resources, sibling_connections):
+        """Called when advertising resources through this connection *in the BACKWARD direction*.
+        Takes the initial list of resources advertised by the destination item and returns a new list,
+        which is the one finally advertised.
+
+        Args:
+            resources (list of ProjectItemResource): Resources to convert
+            sibling_connections (list of Connection): Sibling connections
+
+        Returns:
+            list of ProjectItemResource
+        """
+        return self._apply_use_memory_db(self._apply_write_index(resources, sibling_connections))
+
+    def convert_forward_resources(self, resources):
+        """Called when advertising resources through this connection *in the FORWARD direction*.
+        Takes the initial list of resources advertised by the source item and returns a new list,
+        which is the one finally advertised.
+
+        Args:
+            resources (list of ProjectItemResource): Resources to convert
+
+        Returns:
+            list of ProjectItemResource
+        """
+        return self._apply_use_memory_db(self._apply_use_datapackage(resources))
+
+    def _do_purge_before_writing(self, resources):
+        if self.purge_before_writing:
+            to_urls = (r.url for r in resources if r.type_ == "database")
+            for url in to_urls:
+                purge_url(url, self.purge_settings, self._logger)
+
+    def _apply_use_memory_db(self, resources):
+        if not self.use_memory_db:
+            return resources
+        final_resources = []
+        for r in resources:
+            if r.type_ == "database":
+                r = r.clone(additional_metadata={"memory": True})
+            final_resources.append(r)
+        return final_resources
+
+    def _apply_write_index(self, resources, sibling_connections):
+        final_resources = []
+        precursors = set(c.name for c in sibling_connections if c.write_index < self.write_index)
+        for r in resources:
+            if r.type_ == "database":
+                r = r.clone(
+                    additional_metadata={"current": self.name, "precursors": precursors, "part_count": PartCount()}
+                )
+            final_resources.append(r)
+        return final_resources
+
+    def _apply_use_datapackage(self, resources):
+        if not self.use_datapackage:
+            return resources
+        # Split CSVs from the rest of resources
+        final_resources = []
+        csv_filepaths = []
+        for r in resources:
+            if r.hasfilepath and os.path.splitext(r.path)[1].lower() == ".csv":
+                csv_filepaths.append(r.path)
+                continue
+            final_resources.append(r)
+        if not csv_filepaths:
+            return final_resources
+        # Build Package from CSVs and add it to the resources
+        base_path = os.path.dirname(os.path.commonpath(csv_filepaths))
+        package = Package(base_path=base_path)
+        for path in csv_filepaths:
+            package.add_resource({"path": os.path.relpath(path, base_path)})
+        package_path = os.path.join(base_path, "datapackage.json")
+        package.save(package_path)
+        provider = resources[0].provider_name
+        package_resource = file_resource(provider, package_path, label=f"datapackage@{provider}")
+        package_resource.metadata = resources[0].metadata
+        final_resources.append(package_resource)
+        return final_resources
+
+    def ready_to_execute(self):
+        """See base class."""
+        for filter_type in (SCENARIO_FILTER_TYPE, TOOL_FILTER_TYPE):
+            if self.require_filter_online(filter_type) and not self._filter_settings.has_filter_online(filter_type):
+                return False
+        return True
+
+    def to_dict(self):
+        """Returns a dictionary representation of this Connection.
+
+        Returns:
+            dict: serialized Connection
+        """
+        d = super().to_dict()
+        if self.options:
+            d["options"] = self.options.copy()
+        if self._filter_settings.has_filters():
+            d["filter_settings"] = self._filter_settings.to_dict()
+        return d
+
+    @staticmethod
+    def _constructor_args_from_dict(connection_dict):
+        """See base class."""
+        kw_args = ConnectionBase._constructor_args_from_dict(connection_dict)
+        kw_args["options"] = connection_dict.get("options")
+        filter_settings = connection_dict.get("filter_settings")
+        if filter_settings is not None:
+            kw_args["filter_settings"] = FilterSettings.from_dict(filter_settings)
+        else:
+            disabled_names = connection_dict.get("disabled_filters")
+            if disabled_names is not None:
+                known_filters = _restore_legacy_disabled_filters(disabled_names)
+                kw_args["filter_settings"] = FilterSettings(known_filters)
+        return kw_args
+
+
+class Connection(ResourceConvertingConnection):
+    """Represents a connection between two project items."""
+
+    def __init__(
+        self, source_name, source_position, destination_name, destination_position, options=None, filter_settings=None
+    ):
+        """
+        Args:
+            source_name (str): source project item's name
+            source_position (str): source anchor's position
+            destination_name (str): destination project item's name
+            destination_position (str): destination anchor's position
+            options (dict, optional): any additional options
+            filter_settings (FilterSettings, optional): filter settings
+        """
+        super().__init__(source_name, source_position, destination_name, destination_position, options, filter_settings)
+        self._enabled_filter_names = None
+        self._source_visited = False
+
+    def visit_source(self):
+        self._source_visited = True
+
+    def visit_destination(self):
+        if not self._source_visited:
+            # Can happen in loop execution
+            return
+        self._source_visited = False
+        self.emit_flash()
+
+    def enabled_filters(self, resource_label):
+        """Returns enabled filter names for given resource label.
+
+        Args:
+            resource_label (str): resource label
+
+        Returns:
+            dict: mapping from filter type to list of online filter names
+        """
+        if self._enabled_filter_names is None:
+            self._prepare_enabled_filter_names()
+        return self._enabled_filter_names.get(resource_label)
+
+    def _prepare_enabled_filter_names(self):
+        """Reads filter information from database."""
+        self._enabled_filter_names = {}
+        for resource in self._resources:
+            url = resource.url
+            if not url:
+                continue
+            try:
+                db_map = DatabaseMapping(url)
+            except (SpineDBAPIError, SpineDBVersionError):
+                continue
+            try:
+                scenario_filter_settings = self._filter_settings.known_filters.get(resource.label, {}).get(
+                    SCENARIO_FILTER_TYPE, {}
+                )
+                available_scenarios = {row.name for row in db_map.query(db_map.scenario_sq)}
+                enabled_scenarios = set()
+                for name in available_scenarios:
+                    if scenario_filter_settings.get(name, self._filter_settings.auto_online):
+                        enabled_scenarios.add(name)
+                if enabled_scenarios:
+                    self._enabled_filter_names.setdefault(resource.label, {})[SCENARIO_FILTER_TYPE] = sorted(
+                        list(enabled_scenarios)
+                    )
+                tool_filter_settings = self._filter_settings.known_filters.get(resource.label, {}).get(
+                    TOOL_FILTER_TYPE, {}
+                )
+                available_tools = {row.name for row in db_map.query(db_map.tool_sq)}
+                enabled_tools = set()
+                for name in available_tools:
+                    if tool_filter_settings.get(name, self._filter_settings.auto_online):
+                        enabled_tools.add(name)
+                if enabled_tools:
+                    self._enabled_filter_names.setdefault(resource.label, {})[TOOL_FILTER_TYPE] = sorted(enabled_tools)
+            finally:
+                db_map.connection.close()
+
+    @classmethod
+    def from_dict(cls, connection_dict):
+        """Restores a connection from dictionary.
+
+        Args:
+            connection_dict (dict): connection dictionary
+
+        Returns:
+            Connection: restored connection
+        """
+        kw_args_from_dict = cls._constructor_args_from_dict(connection_dict)
+        return cls(**kw_args_from_dict)
+
+
+class Jump(ConnectionBase):
+    """Represents a conditional jump between two project items."""
+
+    def __init__(
+        self, source_name, source_position, destination_name, destination_position, condition=None, cmd_line_args=()
+    ):
+        """
+        Args:
+            source_name (str): source project item's name
+            source_position (str): source anchor's position
+            destination_name (str): destination project item's name
+            destination_position (str): destination anchor's position
+            condition (dict): jump condition
+        """
+        super().__init__(source_name, source_position, destination_name, destination_position)
+        self.condition = condition if condition is not None else {"type": "python-script", "script": "exit(1)"}
+        self._resources_from_source = set()
+        self._resources_from_destination = set()
+        self.cmd_line_args = list(cmd_line_args)
+        self._engine = None
+        self.source_solid = None
+        self.destination_solid = None
+        self.item_names = set()
+        self.solid_names = set()
+
+    def set_engine(self, engine):
+        self._engine = engine
+
+    @property
+    def resources(self):
+        return self._resources_from_source | self._resources_from_destination
+
+    def update_cmd_line_args(self, cmd_line_args):
+        self.cmd_line_args = cmd_line_args
+
+    def receive_resources_from_source(self, resources):
+        """See base class."""
+        self._resources_from_source = set(resources)
+
+    def receive_resources_from_destination(self, resources):
+        """See base class."""
+        self._resources_from_destination = set(resources)
+
+    def is_condition_true(self, jump_counter):
+        """Evaluates jump condition.
+
+        Args:
+            jump_counter (int): how many times jump has been executed
+
+        Returns:
+            bool: True if jump should be executed, False otherwise
+        """
+        iterate = False
+        if self.condition["type"] == "python-script":
+            iterate = self._is_python_script_condition_true(jump_counter)
+        elif self.condition["type"] == "tool-specification":
+            iterate = self._is_tool_specification_condition_true(jump_counter)
+        if iterate:
+            self.emit_flash()
+            self._update_items()
+        return iterate
+
+    def _update_items(self):
+        for item_name in self.item_names:
+            item = self._engine.make_item(item_name, ED.NONE)
+            forward_resources, backward_resources = self._engine.resources_per_item[item_name]
+            item.update(forward_resources, backward_resources)
+
+    def _is_python_script_condition_true(self, jump_counter):
+        script = self.condition["script"]
+        if not script.strip():
+            return False
+        with ExitStack() as stack:
+            labelled_args = labelled_resource_args(self.resources, stack)
+            expanded_args = expand_cmd_line_args(self.cmd_line_args + [jump_counter], labelled_args, self._logger)
+            with tempfile.TemporaryFile("w+", encoding="utf-8") as script_file:
+                script_file.write(script)
+                script_file.seek(0)
+                python = resolve_python_interpreter("")
+                result = subprocess.run(
+                    [python, "-", *expanded_args], encoding="utf-8", stdin=script_file, capture_output=True
+                )
+                if result.stdout:
+                    self._logger.msg_proc.emit(result.stdout)
+                if result.stderr:
+                    self._logger.msg_proc_error.emit(result.stderr)
+                return result.returncode == 0
+
+    def _is_tool_specification_condition_true(self, jump_counter):
+        item_dict = {
+            "type": "Tool",
+            "execute_in_work": False,
+            "specification": self.condition["specification"],
+            "cmd_line_args": [arg.to_dict() for arg in self.cmd_line_args] + [str(jump_counter)],
+        }
+        condition_tool = self._engine.do_make_item(self.name, item_dict, self._logger)
+        return (
+            condition_tool.execute(list(self._resources_from_source), list(self._resources_from_destination), Lock())
+            == ItemExecutionFinishState.SUCCESS
+        )
+
+    @classmethod
+    def from_dict(cls, jump_dict, **kwargs):
+        """Restores a Jump from dictionary.
+
+        Args:
+            jump_dict (dict): serialized jump
+            **kwargs: extra keyword arguments passed to constructor
+
+        Returns:
+            Jump: restored jump
+        """
+        super_kw_ags = cls._constructor_args_from_dict(jump_dict)
+        condition = jump_dict["condition"]
+        cmd_line_args = jump_dict.get("cmd_line_args", [])
+        cmd_line_args = [make_cmd_line_arg(arg) for arg in cmd_line_args]
+        return cls(condition=condition, cmd_line_args=cmd_line_args, **super_kw_ags, **kwargs)
+
+    def to_dict(self):
+        """Returns a dictionary representation of this Jump.
+
+        Returns:
+            dict: serialized Jump
+        """
+        d = super().to_dict()
+        d["condition"] = self.condition
+        d["cmd_line_args"] = [arg.to_dict() for arg in self.cmd_line_args]
+        return d
+
+
+def _restore_legacy_disabled_filters(disabled_filter_names):
+    """Converts legacy serialized disabled filter names to known filters dict.
+
+    Args:
+        disabled_filter_names (dict): disabled filter names with names stored as lists
+
+    Returns:
+        dict: known filters
+    """
+    converted = {}
+    for label, names_by_type in disabled_filter_names.items():
+        converted_names_by_type = converted.setdefault(label, {})
+        for filter_type, names in names_by_type.items():
+            converted_names_by_type[filter_type] = {name: False for name in names}
+    return converted
```

### Comparing `spine_engine-0.23.3/spine_engine/project_item/executable_item_base.py` & `spine_engine-0.23.4/spine_engine/project_item/executable_item_base.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,231 +1,231 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains ExecutableItem, a project item's counterpart in execution as well as support utilities.
-
-"""
-from hashlib import sha1
-from pathlib import Path
-from ..utils.helpers import ExecutionDirection, ItemExecutionFinishState, shorten
-
-
-class ExecutableItemBase:
-    """The part of a project item that is executed by the Spine Engine."""
-
-    def __init__(self, name, project_dir, logger, group_id=None):
-        """
-        Args:
-            name (str): item's name
-            project_dir (str): absolute path to project directory
-            logger (LoggerInterface): a logger
-        """
-        self._name = name
-        self._project_dir = project_dir
-        data_dir = Path(self._project_dir, ".spinetoolbox", "items", shorten(name))
-        data_dir.mkdir(parents=True, exist_ok=True)
-        self._data_dir = str(data_dir)
-        logs_dir = Path(self._data_dir, "logs")
-        logs_dir.mkdir(parents=True, exist_ok=True)
-        self._logs_dir = str(logs_dir)
-        self._logger = logger
-        self._group_id = name if group_id is None else group_id
-        self._filter_id = ""
-
-    @property
-    def name(self):
-        """Project item's name."""
-        return self._name
-
-    @property
-    def group_id(self):
-        """Returns the id for group-execution.
-        Items in the same group share a kernel, and also reuse the same kernel from past executions.
-        By default each item is its own group, so it executes in isolation.
-        NOTE: At the moment this is only used by Tool, but could be used by other items in the future?
-
-        Returns:
-            str: item's id within an execution group
-        """
-        return self._group_id
-
-    @property
-    def filter_id(self):
-        return self._filter_id
-
-    def hash_filter_id(self):
-        """Hashes filter id.
-
-        Returns:
-            str: hash
-        """
-        return sha1(bytes(self._filter_id, "utf8")).hexdigest() if self._filter_id else ""
-
-    @filter_id.setter
-    def filter_id(self, filter_id):
-        self._filter_id = filter_id
-        self._logger.set_filter_id(filter_id)
-
-    @property
-    def data_dir(self):
-        return self._data_dir
-
-    def ready_to_execute(self, settings):
-        """Validates the internal state of this project item before execution.
-
-        Subclasses can implement this method to do the appropriate work.
-
-        Args:
-            settings (AppSettings): Application settings
-
-        Returns:
-            bool: True if project item is ready for execution, False otherwise
-        """
-        return True
-
-    def update(self, forward_resources, backward_resources):
-        """Executes tasks that should be done before going into a next iteration of the loop."""
-        return True
-
-    def execute(self, forward_resources, backward_resources, lock):
-        """Executes this item using the given resources and returns a boolean indicating the outcome.
-
-        Subclasses can implement this method to do the appropriate work.
-
-        Args:
-            forward_resources (list): a list of ProjectItemResources from predecessors (forward)
-            backward_resources (list): a list of ProjectItemResources from successors (backward)
-            lock (Lock): shared lock for parallel executions
-
-        Returns:
-            ItemExecutionFinishState: State depending on operation success
-        """
-        self._logger.msg.emit(f"***Executing {self.item_type()} <b>{self._name}</b>***")
-        return ItemExecutionFinishState.SUCCESS
-
-    def exclude_execution(self, forward_resources, backward_resources, lock):
-        """Excludes execution of this item.
-
-        This method is called when the item is not selected (i.e EXCLUDED) for execution.
-        Only lightweight bookkeeping or processing should be done in this case, e.g.
-        forward input resources.
-
-        Subclasses can implement this method to do the appropriate work.
-
-        Args:
-            forward_resources (list): a list of ProjectItemResources from predecessors (forward)
-            backward_resources (list): a list of ProjectItemResources from successors (backward)
-            lock (Lock): shared lock for parallel executions
-        """
-
-    def finish_execution(self, state):
-        """Does any work needed after execution given the execution success status.
-
-        Args:
-            state (ItemExecutionFinishState): Item execution finish state
-        """
-        if state == ItemExecutionFinishState.SUCCESS:
-            self._logger.msg_success.emit(f"Executing {self.item_type()} {self.name} finished")
-        elif state == ItemExecutionFinishState.FAILURE:
-            self._logger.msg_error.emit(f"Executing {self.item_type()} {self.name} failed")
-        elif state == ItemExecutionFinishState.SKIPPED:
-            self._logger.msg_warning.emit(f"Executing {self.name} skipped")
-        elif state == ItemExecutionFinishState.STOPPED:
-            self._logger.msg_error.emit(f"Executing {self.name} stopped")
-        else:
-            self._logger.msg_error.emit(f"<b>{self.name}</b> finished execution in an unknown state")
-
-    @staticmethod
-    def item_type():
-        """Returns the item's type identifier string."""
-        raise NotImplementedError()
-
-    @staticmethod
-    def is_filter_terminus():
-        """Tests if the item 'terminates' a forked execution.
-
-        Returns:
-            bool: True if forked executions should be joined before the item, False otherwise
-        """
-        return False
-
-    def output_resources(self, direction):
-        """Returns output resources in the given direction.
-
-        Subclasses need to implement _output_resources_backward and/or _output_resources_forward
-        if they want to provide resources in any direction.
-
-        Args:
-            direction (ExecutionDirection): Direction where output resources are passed
-
-        Returns:
-            list: a list of ProjectItemResources
-        """
-        return {
-            ExecutionDirection.BACKWARD: self._output_resources_backward,
-            ExecutionDirection.FORWARD: self._output_resources_forward,
-        }[direction]()
-
-    def stop_execution(self):
-        """Stops executing this item."""
-        self._logger.msg.emit(f"Stopping {self._name}")
-
-    # pylint: disable=no-self-use
-    def _output_resources_forward(self):
-        """Returns output resources for forward execution.
-
-        The default implementation returns an empty list.
-
-        Returns:
-            list: a list of ProjectItemResources
-        """
-        return list()
-
-    # pylint: disable=no-self-use
-    def _output_resources_backward(self):
-        """Returns output resources for backward execution.
-
-        The default implementation returns an empty list.
-
-        Returns:
-            list: a list of ProjectItemResources
-        """
-        return list()
-
-    @classmethod
-    def from_dict(cls, item_dict, name, project_dir, app_settings, specifications, logger):
-        """Deserializes an executable item from item dictionary.
-
-        Args:
-            item_dict (dict): serialized project item
-            name (str): item's name
-            project_dir (str): absolute path to the project directory
-            app_settings (QSettings): Toolbox settings
-            specifications (dict): mapping from item type to specification name to :class:`ProjectItemSpecification`
-            logger (LoggingInterface): a logger
-
-        Returns:
-            ExecutableItemBase: deserialized executable item
-        """
-        raise NotImplementedError()
-
-    @staticmethod
-    def _get_specification(name, item_type, specification_name, specifications, logger):
-        if not specification_name:
-            return None
-        try:
-            return specifications[item_type][specification_name]
-        except KeyError as missing:
-            if missing == item_type:
-                logger.msg_error.emit(f"No specifications defined for item type '{item_type}'.")
-                return None
-            logger.msg_error.emit(f"Cannot find specification '{missing}'.")
-            return None
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains ExecutableItem, a project item's counterpart in execution as well as support utilities.
+
+"""
+from hashlib import sha1
+from pathlib import Path
+from ..utils.helpers import ExecutionDirection, ItemExecutionFinishState, shorten
+
+
+class ExecutableItemBase:
+    """The part of a project item that is executed by the Spine Engine."""
+
+    def __init__(self, name, project_dir, logger, group_id=None):
+        """
+        Args:
+            name (str): item's name
+            project_dir (str): absolute path to project directory
+            logger (LoggerInterface): a logger
+        """
+        self._name = name
+        self._project_dir = project_dir
+        data_dir = Path(self._project_dir, ".spinetoolbox", "items", shorten(name))
+        data_dir.mkdir(parents=True, exist_ok=True)
+        self._data_dir = str(data_dir)
+        logs_dir = Path(self._data_dir, "logs")
+        logs_dir.mkdir(parents=True, exist_ok=True)
+        self._logs_dir = str(logs_dir)
+        self._logger = logger
+        self._group_id = name if group_id is None else group_id
+        self._filter_id = ""
+
+    @property
+    def name(self):
+        """Project item's name."""
+        return self._name
+
+    @property
+    def group_id(self):
+        """Returns the id for group-execution.
+        Items in the same group share a kernel, and also reuse the same kernel from past executions.
+        By default each item is its own group, so it executes in isolation.
+        NOTE: At the moment this is only used by Tool, but could be used by other items in the future?
+
+        Returns:
+            str: item's id within an execution group
+        """
+        return self._group_id
+
+    @property
+    def filter_id(self):
+        return self._filter_id
+
+    def hash_filter_id(self):
+        """Hashes filter id.
+
+        Returns:
+            str: hash
+        """
+        return sha1(bytes(self._filter_id, "utf8")).hexdigest() if self._filter_id else ""
+
+    @filter_id.setter
+    def filter_id(self, filter_id):
+        self._filter_id = filter_id
+        self._logger.set_filter_id(filter_id)
+
+    @property
+    def data_dir(self):
+        return self._data_dir
+
+    def ready_to_execute(self, settings):
+        """Validates the internal state of this project item before execution.
+
+        Subclasses can implement this method to do the appropriate work.
+
+        Args:
+            settings (AppSettings): Application settings
+
+        Returns:
+            bool: True if project item is ready for execution, False otherwise
+        """
+        return True
+
+    def update(self, forward_resources, backward_resources):
+        """Executes tasks that should be done before going into a next iteration of the loop."""
+        return True
+
+    def execute(self, forward_resources, backward_resources, lock):
+        """Executes this item using the given resources and returns a boolean indicating the outcome.
+
+        Subclasses can implement this method to do the appropriate work.
+
+        Args:
+            forward_resources (list): a list of ProjectItemResources from predecessors (forward)
+            backward_resources (list): a list of ProjectItemResources from successors (backward)
+            lock (Lock): shared lock for parallel executions
+
+        Returns:
+            ItemExecutionFinishState: State depending on operation success
+        """
+        self._logger.msg.emit(f"***Executing {self.item_type()} <b>{self._name}</b>***")
+        return ItemExecutionFinishState.SUCCESS
+
+    def exclude_execution(self, forward_resources, backward_resources, lock):
+        """Excludes execution of this item.
+
+        This method is called when the item is not selected (i.e EXCLUDED) for execution.
+        Only lightweight bookkeeping or processing should be done in this case, e.g.
+        forward input resources.
+
+        Subclasses can implement this method to do the appropriate work.
+
+        Args:
+            forward_resources (list): a list of ProjectItemResources from predecessors (forward)
+            backward_resources (list): a list of ProjectItemResources from successors (backward)
+            lock (Lock): shared lock for parallel executions
+        """
+
+    def finish_execution(self, state):
+        """Does any work needed after execution given the execution success status.
+
+        Args:
+            state (ItemExecutionFinishState): Item execution finish state
+        """
+        if state == ItemExecutionFinishState.SUCCESS:
+            self._logger.msg_success.emit(f"Executing {self.item_type()} {self.name} finished")
+        elif state == ItemExecutionFinishState.FAILURE:
+            self._logger.msg_error.emit(f"Executing {self.item_type()} {self.name} failed")
+        elif state == ItemExecutionFinishState.SKIPPED:
+            self._logger.msg_warning.emit(f"Executing {self.name} skipped")
+        elif state == ItemExecutionFinishState.STOPPED:
+            self._logger.msg_error.emit(f"Executing {self.name} stopped")
+        else:
+            self._logger.msg_error.emit(f"<b>{self.name}</b> finished execution in an unknown state")
+
+    @staticmethod
+    def item_type():
+        """Returns the item's type identifier string."""
+        raise NotImplementedError()
+
+    @staticmethod
+    def is_filter_terminus():
+        """Tests if the item 'terminates' a forked execution.
+
+        Returns:
+            bool: True if forked executions should be joined before the item, False otherwise
+        """
+        return False
+
+    def output_resources(self, direction):
+        """Returns output resources in the given direction.
+
+        Subclasses need to implement _output_resources_backward and/or _output_resources_forward
+        if they want to provide resources in any direction.
+
+        Args:
+            direction (ExecutionDirection): Direction where output resources are passed
+
+        Returns:
+            list: a list of ProjectItemResources
+        """
+        return {
+            ExecutionDirection.BACKWARD: self._output_resources_backward,
+            ExecutionDirection.FORWARD: self._output_resources_forward,
+        }[direction]()
+
+    def stop_execution(self):
+        """Stops executing this item."""
+        self._logger.msg.emit(f"Stopping {self._name}")
+
+    # pylint: disable=no-self-use
+    def _output_resources_forward(self):
+        """Returns output resources for forward execution.
+
+        The default implementation returns an empty list.
+
+        Returns:
+            list: a list of ProjectItemResources
+        """
+        return list()
+
+    # pylint: disable=no-self-use
+    def _output_resources_backward(self):
+        """Returns output resources for backward execution.
+
+        The default implementation returns an empty list.
+
+        Returns:
+            list: a list of ProjectItemResources
+        """
+        return list()
+
+    @classmethod
+    def from_dict(cls, item_dict, name, project_dir, app_settings, specifications, logger):
+        """Deserializes an executable item from item dictionary.
+
+        Args:
+            item_dict (dict): serialized project item
+            name (str): item's name
+            project_dir (str): absolute path to the project directory
+            app_settings (QSettings): Toolbox settings
+            specifications (dict): mapping from item type to specification name to :class:`ProjectItemSpecification`
+            logger (LoggingInterface): a logger
+
+        Returns:
+            ExecutableItemBase: deserialized executable item
+        """
+        raise NotImplementedError()
+
+    @staticmethod
+    def _get_specification(name, item_type, specification_name, specifications, logger):
+        if not specification_name:
+            return None
+        try:
+            return specifications[item_type][specification_name]
+        except KeyError as missing:
+            if missing == item_type:
+                logger.msg_error.emit(f"No specifications defined for item type '{item_type}'.")
+                return None
+            logger.msg_error.emit(f"Cannot find specification '{missing}'.")
+            return None
```

### Comparing `spine_engine-0.23.3/spine_engine/project_item/project_item_info.py` & `spine_engine-0.23.4/spine_engine/project_item/project_item_info.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,36 +1,36 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-"""
-Provides the ProjectItemInfo class.
-
-"""
-
-
-class ProjectItemInfo:
-    @staticmethod
-    def item_category():
-        """
-        Returns the item category string, e.g., "Tools".
-
-        Returns:
-            str: item's category
-        """
-        raise NotImplementedError()
-
-    @staticmethod
-    def item_type():
-        """
-        Returns the item type string, e.g., "Importer".
-
-        Returns:
-            str: item's type
-        """
-        raise NotImplementedError()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+"""
+Provides the ProjectItemInfo class.
+
+"""
+
+
+class ProjectItemInfo:
+    @staticmethod
+    def item_category():
+        """
+        Returns the item category string, e.g., "Tools".
+
+        Returns:
+            str: item's category
+        """
+        raise NotImplementedError()
+
+    @staticmethod
+    def item_type():
+        """
+        Returns the item type string, e.g., "Importer".
+
+        Returns:
+            str: item's type
+        """
+        raise NotImplementedError()
```

### Comparing `spine_engine-0.23.3/spine_engine/project_item/project_item_resource.py` & `spine_engine-0.23.4/spine_engine/project_item/project_item_resource.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,440 +1,440 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-""" Provides the ProjectItemResource class. """
-import copy
-import uuid
-from contextlib import contextmanager
-from pathlib import Path
-from urllib.parse import urlparse
-from urllib.request import url2pathname
-from spinedb_api.filters.tools import clear_filter_configs
-from spinedb_api.spine_db_server import closing_spine_db_server, quick_db_checkout
-from spinedb_api.spine_db_client import SpineDBClient
-from ..utils.helpers import PartCount
-
-
-class ProjectItemResource:
-    """Class to hold a resource made available by a project item and that may be consumed by another project item.
-
-    Attributes:
-        provider_name (str): name of resource provider
-        type_ (str): resource's type
-        label (str): an identifier string
-        metadata (dict): resource's metadata
-    """
-
-    def __init__(self, provider_name, type_, label, url=None, metadata=None, filterable=False, identifier=None):
-        """
-        Args:
-            provider_name (str): The name of the item that provides the resource
-            type_ (str): The resource type, currently available types:
-
-                - "file": url points to a local file
-                - "file_pack": resource is part of a pack; url points to the file's path
-                - "database": url is a Spine database url
-                - "url": url is a generic URL
-            label (str): A label that identifies the resource.
-            url (str, optional): The url of the resource.
-            metadata (dict): Additional metadata providing extra information about the resource.
-                Currently available keys:
-
-                - filter_stack (str): resource's filter stack
-                - filter_id (str): filter id
-                - schema (str): database schema if resource is a database resource
-            filterable (bool): If True, the resource provides opportunity for filtering
-            identifier (str): an identifier of the original instance, shared also by all the clones
-        """
-        self.provider_name = provider_name
-        self.type_ = type_
-        self.label = label
-        self._url = url
-        self._filepath = None
-        self._parsed_url = urlparse(self._url)
-        self.metadata = metadata if metadata is not None else dict()
-        self._filterable = filterable
-        self._identifier = identifier if identifier is not None else uuid.uuid4().hex
-
-    @contextmanager
-    def open(self, db_checkin=False, db_checkout=False):
-        if self.type_ == "database":
-            ordering = {
-                "id": self._identifier,
-                "part_count": self.metadata.get("part_count", PartCount()),
-                "current": self.metadata.get("current"),
-                "precursors": self.metadata.get("precursors", set()),
-            }
-            db_server_manager_queue = self.metadata["db_server_manager_queue"]
-            with closing_spine_db_server(
-                db_server_manager_queue, self.url, memory=self.metadata.get("memory", False), ordering=ordering
-            ) as server_url:
-                if db_checkin:
-                    SpineDBClient.from_server_url(server_url).db_checkin()
-                try:
-                    yield server_url
-                finally:
-                    if db_checkout:
-                        SpineDBClient.from_server_url(server_url).db_checkout()
-        elif self.type_ == "url":
-            yield self.url
-        else:
-            yield self.path if self.hasfilepath else ""
-
-    def quick_db_checkout(self):
-        if self.type_ != "database":
-            return
-        db_server_manager_queue = self.metadata["db_server_manager_queue"]
-        ordering = {
-            "id": self._identifier,
-            "part_count": self.metadata.get("part_count", PartCount()),
-            "current": self.metadata.get("current"),
-            "precursors": self.metadata.get("precursors", set()),
-        }
-        quick_db_checkout(db_server_manager_queue, ordering)
-
-    def clone(self, additional_metadata=None):
-        """Clones this resource and optionally updates the clone's metadata.
-
-        Args:
-            additional_metadata (dict): metadata to add to the clone
-
-        Returns:
-            ProjectItemResource: cloned resource
-        """
-        if additional_metadata is None:
-            additional_metadata = {}
-        metadata = copy.deepcopy(self.metadata)
-        metadata.update(additional_metadata)
-        return ProjectItemResource(
-            self.provider_name,
-            self.type_,
-            label=self.label,
-            url=self._url,
-            metadata=metadata,
-            filterable=self._filterable,
-            identifier=self._identifier,
-        )
-
-    def __eq__(self, other):
-        if not isinstance(other, ProjectItemResource):
-            # don't attempt to compare against unrelated types
-            return NotImplemented
-        return (
-            self.provider_name == other.provider_name
-            and self.type_ == other.type_
-            and self._url == other._url
-            and self.metadata == other.metadata
-            and self._filterable == other._filterable
-        )
-
-    def __hash__(self):
-        return hash(repr(self))
-
-    def __repr__(self):
-        result = "ProjectItemResource("
-        result += f"provider={self.provider_name}, "
-        result += f"type_={self.type_}, "
-        result += f"url={self._url}, "
-        result += f"metadata={self.metadata}, "
-        result += f"filterable={self._filterable})"
-        return result
-
-    @property
-    def url(self):
-        """Resource URL."""
-        return self._url
-
-    @url.setter
-    def url(self, url):
-        self._url = url
-        self._parsed_url = urlparse(self._url)
-        self._filepath = url2pathname(self._parsed_url.path)
-
-    @property
-    def path(self):
-        """Returns the resource path in the local syntax, as obtained from parsing the url."""
-        if not self._filepath:
-            self._filepath = url2pathname(self._parsed_url.path)
-        return self._filepath
-
-    @property
-    def scheme(self):
-        """Returns the resource scheme, as obtained from parsing the url."""
-        return self._parsed_url.scheme
-
-    @property
-    def hasfilepath(self):
-        if not self._url:
-            return False
-        return self.type_ in ("file", "file_pack") or (self.type_ == "database" and self.scheme == "sqlite")
-
-    @property
-    def arg(self):
-        return self._url if self.type_ == "database" else self.path
-
-    @property
-    def filterable(self):
-        return self._filterable
-
-
-class CmdLineArg:
-    """Command line argument for items that execute shell commands."""
-
-    def __init__(self, arg):
-        """
-        Args:
-            arg (str): command line argument
-        """
-        self.arg = arg
-        self.missing = False
-
-    def __eq__(self, other):
-        if not isinstance(other, CmdLineArg):
-            return NotImplemented
-        return self.arg == other.arg
-
-    def __str__(self):
-        return self.arg
-
-    def to_dict(self):
-        """Serializes argument to JSON compatible dict.
-
-        Returns:
-            dict: serialized command line argument
-        """
-        return {"type": "literal", "arg": self.arg}
-
-
-class LabelArg(CmdLineArg):
-    """Command line argument that gets replaced by a project item's resource URL/file path."""
-
-    def to_dict(self):
-        """See base class."""
-        return {"type": "resource", "arg": self.arg}
-
-
-def database_resource(provider_name, url, label=None, filterable=False, schema=None):
-    """
-    Constructs a Spine database resource.
-
-    Args:
-        provider_name (str): resource provider's name
-        url (str): database URL
-        label (str, optional): resource label
-        filterable (bool): is resource filterable
-        schema (str, optional): database schema
-
-    Returns:
-        ProjectItemResources: Spine database resource
-    """
-    if label is None:
-        label = clear_filter_configs(url)
-    metadata = None if not schema else {"schema": schema}
-    return ProjectItemResource(provider_name, "database", label, url, metadata=metadata, filterable=filterable)
-
-
-def url_resource(provider_name, url, label, schema=None):
-    """
-    Constructs a generic URL resource.
-
-    Args:
-        provider_name (str): resource provider's name
-        url (str): database URL
-        label (str): resource label
-        schema (str, optional): database schema if URL is a database URL
-    """
-    metadata = None if not schema else {"schema": schema}
-    return ProjectItemResource(provider_name, "url", label, url, metadata=metadata)
-
-
-def file_resource(provider_name, file_path, label=None):
-    """
-    Constructs a file resource.
-
-    Args:
-        provider_name (str): resource provider's name
-        file_path (str): path to file
-        label (str, optional): resource label
-    """
-    if label is None:
-        label = file_path
-    url = Path(file_path).resolve().as_uri()
-    return ProjectItemResource(provider_name, "file", label, url)
-
-
-def transient_file_resource(provider_name, label, file_path=None):
-    """
-    Constructs a transient file resource.
-
-    Args:
-        provider_name (str): resource provider's name
-        label (str): resource label
-        file_path (str, optional): file path if the file exists
-    """
-    if file_path is not None:
-        url = Path(file_path).resolve().as_uri()
-    else:
-        url = None
-    return ProjectItemResource(provider_name, "file", label, url)
-
-
-def file_resource_in_pack(provider_name, label, file_path=None):
-    """
-    Constructs a file resource that is part of a resource pack.
-
-    Args:
-        provider_name (str): resource provider's name
-        label (str): resource label
-        file_path (str, optional): file path if the file exists
-    """
-    if file_path is not None:
-        url = Path(file_path).resolve().as_uri()
-    else:
-        url = None
-    return ProjectItemResource(provider_name, "file_pack", label, url)
-
-
-def extract_packs(resources):
-    """Extracts file packs from resources.
-
-    Args:
-        resources (Iterable of ProjectItemResource): resources to process
-
-    Returns:
-        tuple: list of non-pack resources and dictionary of packs keyed by label
-    """
-    singles = list()
-    packs = dict()
-    for resource in resources:
-        if resource.type_ != "file_pack":
-            singles.append(resource)
-        else:
-            packs.setdefault(resource.label, list()).append(resource)
-    return singles, packs
-
-
-def labelled_resource_filepaths(resources):
-    """Returns a dict mapping resource labels to file paths available in given resources.
-    The label acts as an identifier for a 'transient_file'.
-    """
-    return {resource.label: resource.path for resource in resources if resource.hasfilepath}
-
-
-_DATABASE_RESOURCE_TYPES = ("database", "url")
-
-
-def get_labelled_source_resources(resources):
-    """Collects URL and file resources and keys them by resource label.
-
-    Args:
-        resources (Iterable of ProjectItemResource): resources to organize
-
-    Returns:
-        dict: a mapping from resource label to list of URLs or file paths
-    """
-    d = {}
-    for resource in resources:
-        if resource.type_ in _DATABASE_RESOURCE_TYPES or resource.hasfilepath:
-            d.setdefault(resource.label, []).append(resource)
-    return d
-
-
-def get_source(resource):
-    """Gets source from resource.
-
-    Args:
-        resource (ProjectItemResource): resource
-
-    Returns:
-         str: source file path or URL or None if source is not available
-    """
-    if resource.type_ in _DATABASE_RESOURCE_TYPES:
-        return resource.url
-    elif resource.hasfilepath:
-        return resource.path
-    return None
-
-
-def get_source_extras(resource):
-    """Gets additional source settings from resource.
-
-    Args:
-        resource (ProjectItemResource): resource
-
-    Returns:
-        dict: additional source settings
-    """
-    if resource.type_ in _DATABASE_RESOURCE_TYPES:
-        return {"schema": resource.metadata.get("schema")}
-    return {}
-
-
-def make_cmd_line_arg(arg_spec):
-    """Deserializes argument from dictionary.
-
-    Args:
-        arg_spec (dict or str): serialized command line argument
-
-    Returns:
-        CmdLineArg: deserialized command line argument
-    """
-    if not isinstance(arg_spec, dict):
-        return CmdLineArg(arg_spec)
-    type_ = arg_spec["type"]
-    construct = {"literal": CmdLineArg, "resource": LabelArg}[type_]
-    return construct(arg_spec["arg"])
-
-
-def labelled_resource_args(resources, stack, db_checkin=False, db_checkout=False):
-    """Generates command line arguments for each resource.
-
-    Args:
-        resources (Iterable of ProjectItemResource): resources to process
-        stack (ExitStack): context manager to ensure resources get closed properly
-        db_checkin (bool): is database checkin required
-        db_checkout (bool): is database checkout required
-
-    Yields:
-        dict: mapping from resource label to a list of resource args.
-    """
-    result = {}
-    single_resources, pack_resources = extract_packs(resources)
-    for resource in single_resources:
-        result[resource.label] = [stack.enter_context(resource.open(db_checkin=db_checkin, db_checkout=db_checkout))]
-    for label, resources_ in pack_resources.items():
-        result[label] = [
-            stack.enter_context(r.open(db_checkin=db_checkin, db_checkout=db_checkout)) for r in resources_
-        ]
-    return result
-
-
-def expand_cmd_line_args(args, label_to_arg, logger):
-    """Expands command line arguments by replacing resource labels by URLs/paths.
-
-    Args:
-        args (list of CmdLineArg): command line arguments
-        label_to_arg (dict): a mapping from resource label to list of cmd line arguments
-        logger (LoggerInterface): a logger
-
-    Returns:
-        list of str: command line arguments as strings
-    """
-    expanded_args = list()
-    for arg in args:
-        if not isinstance(arg, LabelArg):
-            expanded_args.append(str(arg))
-            continue
-        expanded = label_to_arg.get(str(arg))
-        if expanded is None:
-            logger.msg_warning.emit(f"No resources matching argument '{arg}'.")
-            continue
-        if expanded:
-            expanded_args.extend(expanded)
-    return expanded_args
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+""" Provides the ProjectItemResource class. """
+import copy
+import uuid
+from contextlib import contextmanager
+from pathlib import Path
+from urllib.parse import urlparse
+from urllib.request import url2pathname
+from spinedb_api.filters.tools import clear_filter_configs
+from spinedb_api.spine_db_server import closing_spine_db_server, quick_db_checkout
+from spinedb_api.spine_db_client import SpineDBClient
+from ..utils.helpers import PartCount
+
+
+class ProjectItemResource:
+    """Class to hold a resource made available by a project item and that may be consumed by another project item.
+
+    Attributes:
+        provider_name (str): name of resource provider
+        type_ (str): resource's type
+        label (str): an identifier string
+        metadata (dict): resource's metadata
+    """
+
+    def __init__(self, provider_name, type_, label, url=None, metadata=None, filterable=False, identifier=None):
+        """
+        Args:
+            provider_name (str): The name of the item that provides the resource
+            type_ (str): The resource type, currently available types:
+
+                - "file": url points to a local file
+                - "file_pack": resource is part of a pack; url points to the file's path
+                - "database": url is a Spine database url
+                - "url": url is a generic URL
+            label (str): A label that identifies the resource.
+            url (str, optional): The url of the resource.
+            metadata (dict): Additional metadata providing extra information about the resource.
+                Currently available keys:
+
+                - filter_stack (str): resource's filter stack
+                - filter_id (str): filter id
+                - schema (str): database schema if resource is a database resource
+            filterable (bool): If True, the resource provides opportunity for filtering
+            identifier (str): an identifier of the original instance, shared also by all the clones
+        """
+        self.provider_name = provider_name
+        self.type_ = type_
+        self.label = label
+        self._url = url
+        self._filepath = None
+        self._parsed_url = urlparse(self._url)
+        self.metadata = metadata if metadata is not None else dict()
+        self._filterable = filterable
+        self._identifier = identifier if identifier is not None else uuid.uuid4().hex
+
+    @contextmanager
+    def open(self, db_checkin=False, db_checkout=False):
+        if self.type_ == "database":
+            ordering = {
+                "id": self._identifier,
+                "part_count": self.metadata.get("part_count", PartCount()),
+                "current": self.metadata.get("current"),
+                "precursors": self.metadata.get("precursors", set()),
+            }
+            db_server_manager_queue = self.metadata["db_server_manager_queue"]
+            with closing_spine_db_server(
+                db_server_manager_queue, self.url, memory=self.metadata.get("memory", False), ordering=ordering
+            ) as server_url:
+                if db_checkin:
+                    SpineDBClient.from_server_url(server_url).db_checkin()
+                try:
+                    yield server_url
+                finally:
+                    if db_checkout:
+                        SpineDBClient.from_server_url(server_url).db_checkout()
+        elif self.type_ == "url":
+            yield self.url
+        else:
+            yield self.path if self.hasfilepath else ""
+
+    def quick_db_checkout(self):
+        if self.type_ != "database":
+            return
+        db_server_manager_queue = self.metadata["db_server_manager_queue"]
+        ordering = {
+            "id": self._identifier,
+            "part_count": self.metadata.get("part_count", PartCount()),
+            "current": self.metadata.get("current"),
+            "precursors": self.metadata.get("precursors", set()),
+        }
+        quick_db_checkout(db_server_manager_queue, ordering)
+
+    def clone(self, additional_metadata=None):
+        """Clones this resource and optionally updates the clone's metadata.
+
+        Args:
+            additional_metadata (dict): metadata to add to the clone
+
+        Returns:
+            ProjectItemResource: cloned resource
+        """
+        if additional_metadata is None:
+            additional_metadata = {}
+        metadata = copy.deepcopy(self.metadata)
+        metadata.update(additional_metadata)
+        return ProjectItemResource(
+            self.provider_name,
+            self.type_,
+            label=self.label,
+            url=self._url,
+            metadata=metadata,
+            filterable=self._filterable,
+            identifier=self._identifier,
+        )
+
+    def __eq__(self, other):
+        if not isinstance(other, ProjectItemResource):
+            # don't attempt to compare against unrelated types
+            return NotImplemented
+        return (
+            self.provider_name == other.provider_name
+            and self.type_ == other.type_
+            and self._url == other._url
+            and self.metadata == other.metadata
+            and self._filterable == other._filterable
+        )
+
+    def __hash__(self):
+        return hash(repr(self))
+
+    def __repr__(self):
+        result = "ProjectItemResource("
+        result += f"provider={self.provider_name}, "
+        result += f"type_={self.type_}, "
+        result += f"url={self._url}, "
+        result += f"metadata={self.metadata}, "
+        result += f"filterable={self._filterable})"
+        return result
+
+    @property
+    def url(self):
+        """Resource URL."""
+        return self._url
+
+    @url.setter
+    def url(self, url):
+        self._url = url
+        self._parsed_url = urlparse(self._url)
+        self._filepath = url2pathname(self._parsed_url.path)
+
+    @property
+    def path(self):
+        """Returns the resource path in the local syntax, as obtained from parsing the url."""
+        if not self._filepath:
+            self._filepath = url2pathname(self._parsed_url.path)
+        return self._filepath
+
+    @property
+    def scheme(self):
+        """Returns the resource scheme, as obtained from parsing the url."""
+        return self._parsed_url.scheme
+
+    @property
+    def hasfilepath(self):
+        if not self._url:
+            return False
+        return self.type_ in ("file", "file_pack") or (self.type_ == "database" and self.scheme == "sqlite")
+
+    @property
+    def arg(self):
+        return self._url if self.type_ == "database" else self.path
+
+    @property
+    def filterable(self):
+        return self._filterable
+
+
+class CmdLineArg:
+    """Command line argument for items that execute shell commands."""
+
+    def __init__(self, arg):
+        """
+        Args:
+            arg (str): command line argument
+        """
+        self.arg = arg
+        self.missing = False
+
+    def __eq__(self, other):
+        if not isinstance(other, CmdLineArg):
+            return NotImplemented
+        return self.arg == other.arg
+
+    def __str__(self):
+        return self.arg
+
+    def to_dict(self):
+        """Serializes argument to JSON compatible dict.
+
+        Returns:
+            dict: serialized command line argument
+        """
+        return {"type": "literal", "arg": self.arg}
+
+
+class LabelArg(CmdLineArg):
+    """Command line argument that gets replaced by a project item's resource URL/file path."""
+
+    def to_dict(self):
+        """See base class."""
+        return {"type": "resource", "arg": self.arg}
+
+
+def database_resource(provider_name, url, label=None, filterable=False, schema=None):
+    """
+    Constructs a Spine database resource.
+
+    Args:
+        provider_name (str): resource provider's name
+        url (str): database URL
+        label (str, optional): resource label
+        filterable (bool): is resource filterable
+        schema (str, optional): database schema
+
+    Returns:
+        ProjectItemResources: Spine database resource
+    """
+    if label is None:
+        label = clear_filter_configs(url)
+    metadata = None if not schema else {"schema": schema}
+    return ProjectItemResource(provider_name, "database", label, url, metadata=metadata, filterable=filterable)
+
+
+def url_resource(provider_name, url, label, schema=None):
+    """
+    Constructs a generic URL resource.
+
+    Args:
+        provider_name (str): resource provider's name
+        url (str): database URL
+        label (str): resource label
+        schema (str, optional): database schema if URL is a database URL
+    """
+    metadata = None if not schema else {"schema": schema}
+    return ProjectItemResource(provider_name, "url", label, url, metadata=metadata)
+
+
+def file_resource(provider_name, file_path, label=None):
+    """
+    Constructs a file resource.
+
+    Args:
+        provider_name (str): resource provider's name
+        file_path (str): path to file
+        label (str, optional): resource label
+    """
+    if label is None:
+        label = file_path
+    url = Path(file_path).resolve().as_uri()
+    return ProjectItemResource(provider_name, "file", label, url)
+
+
+def transient_file_resource(provider_name, label, file_path=None):
+    """
+    Constructs a transient file resource.
+
+    Args:
+        provider_name (str): resource provider's name
+        label (str): resource label
+        file_path (str, optional): file path if the file exists
+    """
+    if file_path is not None:
+        url = Path(file_path).resolve().as_uri()
+    else:
+        url = None
+    return ProjectItemResource(provider_name, "file", label, url)
+
+
+def file_resource_in_pack(provider_name, label, file_path=None):
+    """
+    Constructs a file resource that is part of a resource pack.
+
+    Args:
+        provider_name (str): resource provider's name
+        label (str): resource label
+        file_path (str, optional): file path if the file exists
+    """
+    if file_path is not None:
+        url = Path(file_path).resolve().as_uri()
+    else:
+        url = None
+    return ProjectItemResource(provider_name, "file_pack", label, url)
+
+
+def extract_packs(resources):
+    """Extracts file packs from resources.
+
+    Args:
+        resources (Iterable of ProjectItemResource): resources to process
+
+    Returns:
+        tuple: list of non-pack resources and dictionary of packs keyed by label
+    """
+    singles = list()
+    packs = dict()
+    for resource in resources:
+        if resource.type_ != "file_pack":
+            singles.append(resource)
+        else:
+            packs.setdefault(resource.label, list()).append(resource)
+    return singles, packs
+
+
+def labelled_resource_filepaths(resources):
+    """Returns a dict mapping resource labels to file paths available in given resources.
+    The label acts as an identifier for a 'transient_file'.
+    """
+    return {resource.label: resource.path for resource in resources if resource.hasfilepath}
+
+
+_DATABASE_RESOURCE_TYPES = ("database", "url")
+
+
+def get_labelled_source_resources(resources):
+    """Collects URL and file resources and keys them by resource label.
+
+    Args:
+        resources (Iterable of ProjectItemResource): resources to organize
+
+    Returns:
+        dict: a mapping from resource label to list of URLs or file paths
+    """
+    d = {}
+    for resource in resources:
+        if resource.type_ in _DATABASE_RESOURCE_TYPES or resource.hasfilepath:
+            d.setdefault(resource.label, []).append(resource)
+    return d
+
+
+def get_source(resource):
+    """Gets source from resource.
+
+    Args:
+        resource (ProjectItemResource): resource
+
+    Returns:
+         str: source file path or URL or None if source is not available
+    """
+    if resource.type_ in _DATABASE_RESOURCE_TYPES:
+        return resource.url
+    elif resource.hasfilepath:
+        return resource.path
+    return None
+
+
+def get_source_extras(resource):
+    """Gets additional source settings from resource.
+
+    Args:
+        resource (ProjectItemResource): resource
+
+    Returns:
+        dict: additional source settings
+    """
+    if resource.type_ in _DATABASE_RESOURCE_TYPES:
+        return {"schema": resource.metadata.get("schema")}
+    return {}
+
+
+def make_cmd_line_arg(arg_spec):
+    """Deserializes argument from dictionary.
+
+    Args:
+        arg_spec (dict or str): serialized command line argument
+
+    Returns:
+        CmdLineArg: deserialized command line argument
+    """
+    if not isinstance(arg_spec, dict):
+        return CmdLineArg(arg_spec)
+    type_ = arg_spec["type"]
+    construct = {"literal": CmdLineArg, "resource": LabelArg}[type_]
+    return construct(arg_spec["arg"])
+
+
+def labelled_resource_args(resources, stack, db_checkin=False, db_checkout=False):
+    """Generates command line arguments for each resource.
+
+    Args:
+        resources (Iterable of ProjectItemResource): resources to process
+        stack (ExitStack): context manager to ensure resources get closed properly
+        db_checkin (bool): is database checkin required
+        db_checkout (bool): is database checkout required
+
+    Yields:
+        dict: mapping from resource label to a list of resource args.
+    """
+    result = {}
+    single_resources, pack_resources = extract_packs(resources)
+    for resource in single_resources:
+        result[resource.label] = [stack.enter_context(resource.open(db_checkin=db_checkin, db_checkout=db_checkout))]
+    for label, resources_ in pack_resources.items():
+        result[label] = [
+            stack.enter_context(r.open(db_checkin=db_checkin, db_checkout=db_checkout)) for r in resources_
+        ]
+    return result
+
+
+def expand_cmd_line_args(args, label_to_arg, logger):
+    """Expands command line arguments by replacing resource labels by URLs/paths.
+
+    Args:
+        args (list of CmdLineArg): command line arguments
+        label_to_arg (dict): a mapping from resource label to list of cmd line arguments
+        logger (LoggerInterface): a logger
+
+    Returns:
+        list of str: command line arguments as strings
+    """
+    expanded_args = list()
+    for arg in args:
+        if not isinstance(arg, LabelArg):
+            expanded_args.append(str(arg))
+            continue
+        expanded = label_to_arg.get(str(arg))
+        if expanded is None:
+            logger.msg_warning.emit(f"No resources matching argument '{arg}'.")
+            continue
+        if expanded:
+            expanded_args.extend(expanded)
+    return expanded_args
```

### Comparing `spine_engine-0.23.3/spine_engine/project_item/project_item_specification.py` & `spine_engine-0.23.4/spine_engine/project_item/project_item_specification.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,125 +1,125 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains project item specification class.
-
-"""
-import json
-
-from spine_engine.utils.helpers import shorten, gather_leaf_data
-
-
-class ProjectItemSpecification:
-    """
-    Class to hold a project item specification.
-
-    Attributes:
-        item_type (str): type of the project item the specification is compatible with
-        definition_file_path (str): specification's JSON file path
-    """
-
-    def __init__(self, name, description=None, item_type="", item_category=""):
-        """
-        Args:
-            name (str): specification name
-            description (str): description
-            item_type (str): Project item type
-            item_category (str): Project item category
-        """
-        self.name = name
-        self.short_name = shorten(name)
-        self.description = description
-        self.item_type = item_type
-        self.item_category = item_category
-        self.definition_file_path = ""
-        self.plugin = None
-
-    # TODO: Needed?
-    def set_name(self, name):
-        """Set object name and short name.
-        Note: Check conflicts (e.g. name already exists)
-        before calling this method.
-
-        Args:
-            name (str): New (long) name for this object
-        """
-        self.name = name
-        self.short_name = shorten(name)
-
-    # TODO:Needed?
-    def set_description(self, description):
-        """Set object description.
-
-        Args:
-            description (str): Object description
-        """
-        self.description = description
-
-    def save(self):
-        """
-        Writes the specification to the path given by ``self.definition_file_path``
-
-        Returns:
-            dict: definition's local entries
-        """
-        definition = self.to_dict()
-        local_entries = self._definition_local_entries()
-        popped = gather_leaf_data(definition, local_entries, pop=True)
-        with open(self.definition_file_path, "w") as fp:
-            json.dump(definition, fp, indent=4)
-        return popped
-
-    def to_dict(self):
-        """
-        Returns a dict for the specification.
-
-        Returns:
-            dict: specification dict
-        """
-        raise NotImplementedError()
-
-    def local_data(self):
-        """Makes a dict out of specification's local data.
-
-        Returns:
-            dict: local data
-        """
-        return gather_leaf_data(self.to_dict(), self._definition_local_entries())
-
-    def may_have_local_data(self):
-        """Tests if specification could have project specific local data.
-
-        Returns:
-            bool: True if specification may have local data, False otherwise
-        """
-        return bool(self._definition_local_entries())
-
-    @staticmethod
-    def _definition_local_entries():
-        """Returns entries or 'paths' in specification dict that should be stored in project's local data directory.
-
-        Returns:
-            list of tuple of str: local data item dict entries
-        """
-        return []
-
-    def is_equivalent(self, other):
-        """
-        Returns True if two specifications are essentially the same.
-
-        Args:
-            other (DataTransformerSpecification): specification to compare to
-
-        Returns:
-            bool: True if the specifications are equivalent, False otherwise
-        """
-        raise NotImplementedError()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains project item specification class.
+
+"""
+import json
+
+from spine_engine.utils.helpers import shorten, gather_leaf_data
+
+
+class ProjectItemSpecification:
+    """
+    Class to hold a project item specification.
+
+    Attributes:
+        item_type (str): type of the project item the specification is compatible with
+        definition_file_path (str): specification's JSON file path
+    """
+
+    def __init__(self, name, description=None, item_type="", item_category=""):
+        """
+        Args:
+            name (str): specification name
+            description (str): description
+            item_type (str): Project item type
+            item_category (str): Project item category
+        """
+        self.name = name
+        self.short_name = shorten(name)
+        self.description = description
+        self.item_type = item_type
+        self.item_category = item_category
+        self.definition_file_path = ""
+        self.plugin = None
+
+    # TODO: Needed?
+    def set_name(self, name):
+        """Set object name and short name.
+        Note: Check conflicts (e.g. name already exists)
+        before calling this method.
+
+        Args:
+            name (str): New (long) name for this object
+        """
+        self.name = name
+        self.short_name = shorten(name)
+
+    # TODO:Needed?
+    def set_description(self, description):
+        """Set object description.
+
+        Args:
+            description (str): Object description
+        """
+        self.description = description
+
+    def save(self):
+        """
+        Writes the specification to the path given by ``self.definition_file_path``
+
+        Returns:
+            dict: definition's local entries
+        """
+        definition = self.to_dict()
+        local_entries = self._definition_local_entries()
+        popped = gather_leaf_data(definition, local_entries, pop=True)
+        with open(self.definition_file_path, "w") as fp:
+            json.dump(definition, fp, indent=4)
+        return popped
+
+    def to_dict(self):
+        """
+        Returns a dict for the specification.
+
+        Returns:
+            dict: specification dict
+        """
+        raise NotImplementedError()
+
+    def local_data(self):
+        """Makes a dict out of specification's local data.
+
+        Returns:
+            dict: local data
+        """
+        return gather_leaf_data(self.to_dict(), self._definition_local_entries())
+
+    def may_have_local_data(self):
+        """Tests if specification could have project specific local data.
+
+        Returns:
+            bool: True if specification may have local data, False otherwise
+        """
+        return bool(self._definition_local_entries())
+
+    @staticmethod
+    def _definition_local_entries():
+        """Returns entries or 'paths' in specification dict that should be stored in project's local data directory.
+
+        Returns:
+            list of tuple of str: local data item dict entries
+        """
+        return []
+
+    def is_equivalent(self, other):
+        """
+        Returns True if two specifications are essentially the same.
+
+        Args:
+            other (DataTransformerSpecification): specification to compare to
+
+        Returns:
+            bool: True if the specifications are equivalent, False otherwise
+        """
+        raise NotImplementedError()
```

### Comparing `spine_engine-0.23.3/spine_engine/project_item_loader.py` & `spine_engine-0.23.4/spine_engine/project_item_loader.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,57 +1,57 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains :class:`ProjectItemLoader`.
-
-"""
-from multiprocessing import Lock
-from .load_project_items import load_executable_item_classes, load_item_specification_factories
-from .utils.helpers import Singleton
-
-
-class ProjectItemLoader(metaclass=Singleton):
-    """A singleton class for loading project items from multiple processes simultaneously."""
-
-    _specification_factories = {}
-    _executable_item_classes = {}
-    _specification_factories_lock = Lock()
-    _executable_item_classes_lock = Lock()
-
-    def load_item_specification_factories(self, items_module_name):
-        """
-        Loads the project item specification factories in the standard Toolbox package.
-
-        Args:
-            items_module_name (str): name of the Python module that contains the project items
-
-        Returns:
-            dict: a map from item type to specification factory
-        """
-        with self._specification_factories_lock:
-            if items_module_name not in self._specification_factories:
-                self._specification_factories[items_module_name] = load_item_specification_factories(items_module_name)
-        return self._specification_factories[items_module_name]
-
-    def load_executable_item_classes(self, items_module_name):
-        """
-        Loads the project item executable classes included in the standard Toolbox package.
-
-        Args:
-            items_module_name (str): name of the Python module that contains the project items
-
-        Returns:
-            dict: a map from item type to the executable item class
-        """
-        with self._executable_item_classes_lock:
-            if items_module_name not in self._executable_item_classes:
-                self._executable_item_classes[items_module_name] = load_executable_item_classes(items_module_name)
-        return self._executable_item_classes[items_module_name]
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains :class:`ProjectItemLoader`.
+
+"""
+from multiprocessing import Lock
+from .load_project_items import load_executable_item_classes, load_item_specification_factories
+from .utils.helpers import Singleton
+
+
+class ProjectItemLoader(metaclass=Singleton):
+    """A singleton class for loading project items from multiple processes simultaneously."""
+
+    _specification_factories = {}
+    _executable_item_classes = {}
+    _specification_factories_lock = Lock()
+    _executable_item_classes_lock = Lock()
+
+    def load_item_specification_factories(self, items_module_name):
+        """
+        Loads the project item specification factories in the standard Toolbox package.
+
+        Args:
+            items_module_name (str): name of the Python module that contains the project items
+
+        Returns:
+            dict: a map from item type to specification factory
+        """
+        with self._specification_factories_lock:
+            if items_module_name not in self._specification_factories:
+                self._specification_factories[items_module_name] = load_item_specification_factories(items_module_name)
+        return self._specification_factories[items_module_name]
+
+    def load_executable_item_classes(self, items_module_name):
+        """
+        Loads the project item executable classes included in the standard Toolbox package.
+
+        Args:
+            items_module_name (str): name of the Python module that contains the project items
+
+        Returns:
+            dict: a map from item type to the executable item class
+        """
+        with self._executable_item_classes_lock:
+            if items_module_name not in self._executable_item_classes:
+                self._executable_item_classes[items_module_name] = load_executable_item_classes(items_module_name)
+        return self._executable_item_classes[items_module_name]
```

### Comparing `spine_engine-0.23.3/spine_engine/server/__init__.py` & `spine_engine-0.23.4/spine_engine/utils/__init__.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,15 +1,10 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-This package contains Remote Server classes of the Spine Engine.
-
-"""
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
```

### Comparing `spine_engine-0.23.3/spine_engine/server/certificate_creator.py` & `spine_engine-0.23.4/spine_engine/server/certificate_creator.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,77 +1,77 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-The function of this class can be used for generation of keys for enabling security 
-of the remote spine_server. The code has been copied based on an example at (by Chris Laws):
-https://github.com/zeromq/pyzmq/blob/main/examples/security/generate_certificates.py
-
-"""
-
-import os
-import sys
-import shutil
-import zmq.auth
-
-
-class CertificateCreator:
-    @staticmethod
-    def generate_certificates(base_dir):
-        """Generates client and server keys for enabling security.
-
-        Args:
-            base_dir: folder where the files are created.
-        """
-        keys_dir = os.path.join(base_dir, "certificates")
-        public_keys_dir = os.path.join(base_dir, "public_keys")
-        secret_keys_dir = os.path.join(base_dir, "private_keys")
-
-        # Create directories for certificates, remove old content if necessary
-        for d in [base_dir, keys_dir, public_keys_dir, secret_keys_dir]:
-            if os.path.exists(d):
-                shutil.rmtree(d)
-            os.mkdir(d)
-
-        # create new keys in certificates dir
-        server_public_file, server_secret_file = zmq.auth.create_certificates(keys_dir, "server")
-        client_public_file, client_secret_file = zmq.auth.create_certificates(keys_dir, "client")
-        # move public keys to appropriate directory
-        for key_file in os.listdir(keys_dir):
-            if key_file.endswith(".key"):
-                shutil.move(os.path.join(keys_dir, key_file), os.path.join(public_keys_dir, "."))
-        # move secret keys to appropriate directory
-        for key_file in os.listdir(keys_dir):
-            if key_file.endswith(".key_secret"):
-                shutil.move(os.path.join(keys_dir, key_file), os.path.join(secret_keys_dir, "."))
-
-
-def main(args):
-    if len(args) < 2:
-        script_dir = os.path.dirname(os.path.abspath(__file__))
-        base_dir = os.path.join(script_dir, "certs")
-        cert_dir = os.path.join(base_dir, "certificates")
-        if os.path.exists(base_dir):
-            print(f"Directory {base_dir} already exists. Please remove it to recreate certificates.")
-            return 0
-    else:
-        print("Too many arguments")
-        return 0
-    try:
-        CertificateCreator.generate_certificates(base_dir)
-    except Exception as e:
-        print(f"Creating certificates failed. Error:{e}")
-        return 1
-    print(f"Certificates successfully created to: {base_dir}")
-    return 0
-
-
-if __name__ == "__main__":
-    main(sys.argv)
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+The function of this class can be used for generation of keys for enabling security 
+of the remote spine_server. The code has been copied based on an example at (by Chris Laws):
+https://github.com/zeromq/pyzmq/blob/main/examples/security/generate_certificates.py
+
+"""
+
+import os
+import sys
+import shutil
+import zmq.auth
+
+
+class CertificateCreator:
+    @staticmethod
+    def generate_certificates(base_dir):
+        """Generates client and server keys for enabling security.
+
+        Args:
+            base_dir: folder where the files are created.
+        """
+        keys_dir = os.path.join(base_dir, "certificates")
+        public_keys_dir = os.path.join(base_dir, "public_keys")
+        secret_keys_dir = os.path.join(base_dir, "private_keys")
+
+        # Create directories for certificates, remove old content if necessary
+        for d in [base_dir, keys_dir, public_keys_dir, secret_keys_dir]:
+            if os.path.exists(d):
+                shutil.rmtree(d)
+            os.mkdir(d)
+
+        # create new keys in certificates dir
+        server_public_file, server_secret_file = zmq.auth.create_certificates(keys_dir, "server")
+        client_public_file, client_secret_file = zmq.auth.create_certificates(keys_dir, "client")
+        # move public keys to appropriate directory
+        for key_file in os.listdir(keys_dir):
+            if key_file.endswith(".key"):
+                shutil.move(os.path.join(keys_dir, key_file), os.path.join(public_keys_dir, "."))
+        # move secret keys to appropriate directory
+        for key_file in os.listdir(keys_dir):
+            if key_file.endswith(".key_secret"):
+                shutil.move(os.path.join(keys_dir, key_file), os.path.join(secret_keys_dir, "."))
+
+
+def main(args):
+    if len(args) < 2:
+        script_dir = os.path.dirname(os.path.abspath(__file__))
+        base_dir = os.path.join(script_dir, "certs")
+        cert_dir = os.path.join(base_dir, "certificates")
+        if os.path.exists(base_dir):
+            print(f"Directory {base_dir} already exists. Please remove it to recreate certificates.")
+            return 0
+    else:
+        print("Too many arguments")
+        return 0
+    try:
+        CertificateCreator.generate_certificates(base_dir)
+    except Exception as e:
+        print(f"Creating certificates failed. Error:{e}")
+        return 1
+    print(f"Certificates successfully created to: {base_dir}")
+    return 0
+
+
+if __name__ == "__main__":
+    main(sys.argv)
```

### Comparing `spine_engine-0.23.3/spine_engine/server/engine_server.py` & `spine_engine-0.23.4/spine_engine/server/engine_server.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,361 +1,361 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains EngineServer class for running a Zero-MQ server with Spine Engine.
-"""
-import json.decoder
-import os
-import queue
-import time
-import threading
-import ipaddress
-import enum
-import uuid
-import zmq
-from zmq.auth.thread import ThreadAuthenticator
-from spine_engine.server.util.server_message import ServerMessage
-from spine_engine.server.request import Request
-from spine_engine.server.remote_execution_service import RemoteExecutionService
-from spine_engine.server.ping_service import PingService
-from spine_engine.server.project_extractor_service import ProjectExtractorService
-from spine_engine.server.project_retriever_service import ProjectRetrieverService
-from spine_engine.server.persistent_execution_service import PersistentExecutionService
-from spine_engine.server.project_remover_service import ProjectRemoverService
-
-
-class ServerSecurityModel(enum.Enum):
-    NONE = 0
-    STONEHOUSE = 1
-
-
-class EngineServer(threading.Thread):
-    """A server for receiving execution requests from Spine Toolbox."""
-
-    def __init__(self, protocol, port, sec_model, sec_folder):
-        """Initializes the server.
-
-        Args:
-            protocol (str): Protocol to be used by the server.
-            port (int): Port to bind the server to
-            sec_model (ServerSecurityModel): Security model state
-            sec_folder (str): Folder, where security files have been stored.
-        """
-        super().__init__(target=self.serve, name="EngineServerThread")
-        if sec_model == ServerSecurityModel.NONE:
-            self._sec_model_state = ServerSecurityModel.NONE
-        elif sec_model == ServerSecurityModel.STONEHOUSE:
-            if not sec_folder:
-                raise ValueError("Path to security folder missing")
-            base_dir = sec_folder
-            self.keys_dir = os.path.join(base_dir, 'certificates')
-            self.public_keys_dir = os.path.join(base_dir, 'public_keys')
-            self.secret_keys_dir = os.path.join(base_dir, 'private_keys')
-            if not os.path.exists(self.keys_dir):
-                raise ValueError(f"Security folder: {self.keys_dir} does not exist")
-            if not os.path.exists(self.public_keys_dir):
-                raise ValueError(f"Security folder: {self.public_keys_dir} does not exist")
-            if not os.path.exists(self.secret_keys_dir):
-                raise ValueError(f"Security folder: {self.secret_keys_dir} does not exist")
-            self._sec_folder = sec_folder
-            self._sec_model_state = ServerSecurityModel.STONEHOUSE
-        self.protocol = protocol
-        self.port = port
-        self.auth = None
-        # NOTE: Contexts are thread-safe! Sockets are NOT! Do not use or close
-        # sockets except in the thread that created them.
-        self._context = zmq.Context()
-        self.ctrl_msg_sender = self._context.socket(zmq.PAIR)
-        self.ctrl_msg_sender.bind("inproc://ctrl_msg")  # inproc:// transport requires a bind() before connect()
-        self.persistent_exec_mngrs = dict()
-        self.start()  # Start serving
-
-    def close(self):
-        """Closes the server by sending a KILL message to this thread using a PAIR socket."""
-        if self.auth is not None:
-            self.auth.stop()
-            time.sleep(0.2)  # wait a bit until authenticator has been closed
-        self.ctrl_msg_sender.send(b"KILL")
-        self.join()  # Wait for the thread to finish and sockets to close
-        self.ctrl_msg_sender.close()  # Close this in the same thread that it was created in
-        self._context.term()
-
-    def serve(self):
-        """Creates the required sockets, which are polled asynchronously. The ROUTER socket handles communicating
-        with clients, DEALER sockets are for communicating with the backend processes and PAIR sockets are for
-        internal server control messages."""
-        try:
-            frontend = self._context.socket(zmq.ROUTER)
-            backend = self._context.socket(zmq.DEALER)
-            backend.bind("inproc://backend")
-            # Socket for internal control input (i.e. killing the server)
-            ctrl_msg_listener = self._context.socket(zmq.PAIR)
-            ctrl_msg_listener.connect("inproc://ctrl_msg")
-            if self._sec_model_state == ServerSecurityModel.STONEHOUSE:
-                try:
-                    self.auth = self.enable_stonehouse_security(frontend)
-                except ValueError:
-                    raise
-            frontend.bind(self.protocol + "://*:" + str(self.port))
-            poller = zmq.Poller()
-            poller.register(frontend, zmq.POLLIN)
-            poller.register(backend, zmq.POLLIN)
-            poller.register(ctrl_msg_listener, zmq.POLLIN)
-        except Exception as e:
-            raise ValueError(f"Initializing serve() failed due to exception: {e}")
-        workers = dict()
-        project_dirs = dict()  # Mapping of job Id to an abs. path to a project directory ready for execution
-        persistent_exec_mngr_q = queue.Queue()
-        while True:
-            try:
-                socks = dict(poller.poll())
-                # Broker
-                if socks.get(frontend) == zmq.POLLIN:
-                    # Frontend received a message, send it to backend for processing
-                    msg = frontend.recv_multipart()
-                    request = self.handle_frontend_message_received(frontend, msg)
-                    if not request:
-                        print("Received request malformed. - continuing...")
-                        continue
-                    print(f"{request.cmd().upper()} request from client {request.connection_id()}")
-                    job_id = uuid.uuid4().hex  # Job Id for execution worker
-                    if request.cmd() == "ping":
-                        worker = PingService(self._context, request, job_id)
-                    elif request.cmd() == "prepare_execution":
-                        worker = ProjectExtractorService(self._context, request, job_id)
-                    elif request.cmd() == "start_execution":
-                        project_dir = project_dirs.get(request.request_id(), None)  # Get project dir based on job_id
-                        if not project_dir:
-                            print(f"Project for job_id:{request.request_id()} not found")
-                            msg = (
-                                f"Starting DAG execution failed. Project directory for "
-                                f"job_id:{request.request_id()} not found."
-                            )
-                            self.send_init_failed_reply(frontend, request.connection_id(), msg)
-                            continue
-                        worker = RemoteExecutionService(
-                            self._context, request, job_id, project_dir, persistent_exec_mngr_q
-                        )
-                    elif request.cmd() == "stop_execution":
-                        worker = workers.get(request.request_id(), None)  # Get DAG execution worker based on job Id
-                        if not worker:
-                            print(f"Worker for job_id:{request.request_id()} not found")
-                            msg = f"Stopping DAG execution failed. Worker for job_id:{request.request_id()} not found."
-                            self.send_init_failed_reply(frontend, request.connection_id(), msg)
-                            continue
-                        worker.stop_engine()
-                        request.send_response(frontend, ("server_status_msg", "DAG worker stopped"))
-                        continue
-                    elif request.cmd() == "answer_prompt":
-                        worker = workers.get(request.request_id(), None)  # Get DAG execution worker based on job Id
-                        if not worker:
-                            print(f"Worker for job_id:{request.request_id()} not found")
-                            msg = f"Answering prompt failed. Worker for job_id:{request.request_id()} not found."
-                            self.send_init_failed_reply(frontend, request.connection_id(), msg)
-                            continue
-                        item_name, accepted = request.data()
-                        worker.answer_prompt(item_name, accepted)
-                        continue
-                    elif request.cmd() == "retrieve_project":
-                        project_dir = project_dirs.get(request.request_id(), None)  # Get project dir based on job_id
-                        if not project_dir:
-                            print(f"Project for job_id:{request.request_id()} not found")
-                            msg = (
-                                f"Retrieving project for job_id {request.request_id()} failed. "
-                                f"Project directory not found."
-                            )
-                            self.send_init_failed_reply(frontend, request.connection_id(), msg)
-                            continue
-                        worker = ProjectRetrieverService(self._context, request, job_id, project_dir)
-                    elif request.cmd() == "execute_in_persistent":
-                        exec_mngr_key = request.data()[0]
-                        exec_mngr = self.persistent_exec_mngrs.get(exec_mngr_key, None)
-                        if not exec_mngr:
-                            print(f"Persistent exec. mngr for key:{exec_mngr_key} not found.")
-                            msg = (
-                                f"Executing command:{request.data()[1]} - {request.data()[2]} in persistent "
-                                f"manager failed. Persistent execution manager for key {exec_mngr_key} not found."
-                            )
-                            self.send_init_failed_reply(frontend, request.connection_id(), msg)
-                            continue
-                        worker = PersistentExecutionService(self._context, request, job_id, exec_mngr)
-                    elif request.cmd() == "remove_project":
-                        project_dir = project_dirs.get(request.request_id(), None)  # Get project dir based on job_id
-                        if not project_dir:
-                            print(f"Project for job_id:{request.request_id()} not found")
-                            msg = f"Project directory for job_id {request.request_id()} not found"
-                            self.send_init_failed_reply(frontend, request.connection_id(), msg)
-                            continue
-                        worker = ProjectRemoverService(self._context, request, job_id, project_dir)
-                    else:
-                        print(f"Unknown command {request.cmd()} requested")
-                        msg = f"Server error: Unknown command '{request.cmd()}' requested"
-                        self.send_init_failed_reply(frontend, request.connection_id(), msg)
-                        continue
-                    worker.start()
-                    workers[job_id] = worker
-                if socks.get(backend) == zmq.POLLIN:
-                    # Worker has finished execution. Relay reply from backend back to client using the frontend socket
-                    message = backend.recv_multipart()
-                    internal_msg = json.loads(message.pop().decode("utf-8"))
-                    if internal_msg[1] != "in_progress":
-                        finished_worker = workers.pop(internal_msg[0])
-                        if isinstance(finished_worker, ProjectExtractorService):
-                            project_dirs[internal_msg[0]] = internal_msg[1]
-                        if isinstance(finished_worker, RemoteExecutionService):
-                            # Store refs to exec. managers
-                            try:
-                                new_exec_mngrs = persistent_exec_mngr_q.get_nowait()
-                                for k, v in new_exec_mngrs.items():
-                                    self.persistent_exec_mngrs[k] = v
-                            except queue.Empty:
-                                pass
-                        finished_worker.close()
-                        finished_worker.join()
-                    print(f"Sending msg to client {message[0]}")
-                    frontend.send_multipart(message)
-                if socks.get(ctrl_msg_listener) == zmq.POLLIN:
-                    print("Closing server...")
-                    self.kill_persistent_exec_mngrs()
-                    if len(workers) > 0:
-                        print(f"WARNING: Some workers still running:{workers.keys()}")
-                    break
-            except Exception as e:
-                print(f"[DEBUG] EngineServer.serve() exception: {type(e)} serving failed, exception: {e}")
-                break
-        # Close sockets
-        ctrl_msg_listener.close()
-        frontend.close()
-        backend.close()
-
-    def kill_persistent_exec_mngrs(self):
-        """Kills all persistent (execution) manager processes."""
-        n_exec_mngrs = len(self.persistent_exec_mngrs)
-        if n_exec_mngrs > 0:
-            print(f"Closing {len(self.persistent_exec_mngrs)} persistent execution manager processes")
-            for k, exec_mngr in self.persistent_exec_mngrs.items():
-                exec_mngr._persistent_manager.kill_process()
-            self.persistent_exec_mngrs.clear()
-
-    def handle_frontend_message_received(self, socket, msg):
-        """Check received message integrity.
-
-        msg for ping is eg.
-        [b'\x00k\x8bEg', b'{\n   "command": "ping",\n   "id":"4773735",\n   "data":"",\n   "files": {}\n}']
-        where,
-        msg[0] - Frame 1, is the connection identity (or an address). Unique binary string handle to the connection.
-        msg[1] - Frame 2, data frame. This is the request from client as a binary
-            JSON string containing a server message dictionary
-
-        Returns:
-            None if something went wrong or a new Request instance
-        """
-        b_json_str_server_msg = msg[1]  # binary string
-        if len(b_json_str_server_msg) <= 10:  # Message size too small
-            print(f"User data frame too small [{len(msg[1])}]. msg:{msg}")
-            self.send_init_failed_reply(socket, msg[0], "User data frame too small. Malformed message sent to server.")
-            return None
-        try:
-            json_str_server_msg = b_json_str_server_msg.decode("utf-8")  # json string
-        except UnicodeDecodeError as e:
-            print(f"Decoding received msg '{msg[1]} ' failed. \nUnicodeDecodeError: {e}")
-            self.send_init_failed_reply(
-                socket, msg[0], f"UnicodeDecodeError: {e}. " f"- Malformed message sent to server."
-            )
-            return None
-        # Load JSON string into dictionary
-        try:
-            server_msg = json.loads(json_str_server_msg)  # dictionary
-        except json.decoder.JSONDecodeError as e:
-            self.send_init_failed_reply(
-                socket, msg[0], f"json.decoder.JSONDecodeError: {e}. " f"- Message parsing error at server."
-            )
-            return None
-        # server_msg is now a dict with keys: 'command', 'id', 'data', and 'files'
-        data_str = server_msg["data"]  # String
-        files = server_msg["files"]  # Dictionary. TODO: Should this be a list?
-        files_list = []
-        if len(files) > 0:
-            for f in files:
-                files_list.append(files[f])
-        return Request(msg, server_msg["command"], server_msg["id"], data_str, files_list)
-
-    @staticmethod
-    def send_init_failed_reply(socket, connection_id, error_msg):
-        """Sends an error reply to client when request is malformed.
-
-        Args:
-            socket (ZMQSocket): Socket for sending the reply
-            connection_id (bytes): Client Id. Assigned by the frontend ROUTER socket when a request is received.
-            error_msg (str): Error message to client
-        """
-        error_msg_tuple = ("server_init_failed", error_msg)
-        err_msg_as_json = json.dumps(error_msg_tuple)
-        reply_msg = ServerMessage("", "", err_msg_as_json, [])
-        frame = [connection_id, b"", reply_msg.to_bytes()]
-        socket.send_multipart(frame)
-        print("\nClient has been notified. Moving on...")
-
-    def enable_stonehouse_security(self, frontend):
-        """Enables Stonehouse security by starting an authenticator and configuring
-        the frontend socket with authenticator.
-
-        implementation based on https://github.com/zeromq/pyzmq/blob/main/examples/security/stonehouse.py
-
-        Args:
-            frontend (zmq.Socket): Frontend socket
-        """
-        auth = ThreadAuthenticator(self._context)  # Start an authenticator for this context
-        auth.start()
-        endpoints_file = os.path.join(self._sec_folder, "allowEndpoints.txt")
-        if not os.path.exists(endpoints_file):
-            raise ValueError(
-                f"File allowEndpoints.txt missing. Please create the file into directory: "
-                f"{self._sec_folder} and add allowed IP's there"
-            )
-        endpoints = self._read_end_points(endpoints_file)
-        if not endpoints:
-            raise ValueError("No endpoints configured. Please add allowed IP's into allowEndPoints.txt")
-        # Allow configured endpoints
-        allowed = list()
-        for ep in endpoints:
-            try:
-                ep = ep.strip()
-                ipaddress.ip_address(ep)
-                auth.allow(ep)
-                allowed.append(ep)  # Just for printing
-            except:
-                raise ValueError(f"Invalid IP address in allowEndpoints.txt:'{ep}'")
-        allowed_str = "\n".join(allowed)
-        print(f"StoneHouse security activated. Allowed endpoints ({len(allowed)}):\n{allowed_str}")
-        # Tell the authenticator how to handle CURVE requests
-        auth.configure_curve(domain='*', location=zmq.auth.CURVE_ALLOW_ANY)
-        server_secret_file = os.path.join(self.secret_keys_dir, "server.key_secret")
-        server_public, server_secret = zmq.auth.load_certificate(server_secret_file)
-        frontend.curve_secretkey = server_secret
-        frontend.curve_publickey = server_public
-        frontend.curve_server = True  # must come before bind
-        return auth
-
-    @staticmethod
-    def _read_end_points(config_file_location):
-        """Reads all lines from a text file and returns them in a list. Empty strings are removed from the list.
-
-        Args:
-            config_file_location (str): Full path to some text file
-
-        Returns:
-            list: Lines of the given file in a list
-        """
-        with open(config_file_location, "r") as f:
-            all_lines = f.read().splitlines()
-            lines = [x for x in all_lines if x]
-            return lines
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains EngineServer class for running a Zero-MQ server with Spine Engine.
+"""
+import json.decoder
+import os
+import queue
+import time
+import threading
+import ipaddress
+import enum
+import uuid
+import zmq
+from zmq.auth.thread import ThreadAuthenticator
+from spine_engine.server.util.server_message import ServerMessage
+from spine_engine.server.request import Request
+from spine_engine.server.remote_execution_service import RemoteExecutionService
+from spine_engine.server.ping_service import PingService
+from spine_engine.server.project_extractor_service import ProjectExtractorService
+from spine_engine.server.project_retriever_service import ProjectRetrieverService
+from spine_engine.server.persistent_execution_service import PersistentExecutionService
+from spine_engine.server.project_remover_service import ProjectRemoverService
+
+
+class ServerSecurityModel(enum.Enum):
+    NONE = 0
+    STONEHOUSE = 1
+
+
+class EngineServer(threading.Thread):
+    """A server for receiving execution requests from Spine Toolbox."""
+
+    def __init__(self, protocol, port, sec_model, sec_folder):
+        """Initializes the server.
+
+        Args:
+            protocol (str): Protocol to be used by the server.
+            port (int): Port to bind the server to
+            sec_model (ServerSecurityModel): Security model state
+            sec_folder (str): Folder, where security files have been stored.
+        """
+        super().__init__(target=self.serve, name="EngineServerThread")
+        if sec_model == ServerSecurityModel.NONE:
+            self._sec_model_state = ServerSecurityModel.NONE
+        elif sec_model == ServerSecurityModel.STONEHOUSE:
+            if not sec_folder:
+                raise ValueError("Path to security folder missing")
+            base_dir = sec_folder
+            self.keys_dir = os.path.join(base_dir, 'certificates')
+            self.public_keys_dir = os.path.join(base_dir, 'public_keys')
+            self.secret_keys_dir = os.path.join(base_dir, 'private_keys')
+            if not os.path.exists(self.keys_dir):
+                raise ValueError(f"Security folder: {self.keys_dir} does not exist")
+            if not os.path.exists(self.public_keys_dir):
+                raise ValueError(f"Security folder: {self.public_keys_dir} does not exist")
+            if not os.path.exists(self.secret_keys_dir):
+                raise ValueError(f"Security folder: {self.secret_keys_dir} does not exist")
+            self._sec_folder = sec_folder
+            self._sec_model_state = ServerSecurityModel.STONEHOUSE
+        self.protocol = protocol
+        self.port = port
+        self.auth = None
+        # NOTE: Contexts are thread-safe! Sockets are NOT! Do not use or close
+        # sockets except in the thread that created them.
+        self._context = zmq.Context()
+        self.ctrl_msg_sender = self._context.socket(zmq.PAIR)
+        self.ctrl_msg_sender.bind("inproc://ctrl_msg")  # inproc:// transport requires a bind() before connect()
+        self.persistent_exec_mngrs = dict()
+        self.start()  # Start serving
+
+    def close(self):
+        """Closes the server by sending a KILL message to this thread using a PAIR socket."""
+        if self.auth is not None:
+            self.auth.stop()
+            time.sleep(0.2)  # wait a bit until authenticator has been closed
+        self.ctrl_msg_sender.send(b"KILL")
+        self.join()  # Wait for the thread to finish and sockets to close
+        self.ctrl_msg_sender.close()  # Close this in the same thread that it was created in
+        self._context.term()
+
+    def serve(self):
+        """Creates the required sockets, which are polled asynchronously. The ROUTER socket handles communicating
+        with clients, DEALER sockets are for communicating with the backend processes and PAIR sockets are for
+        internal server control messages."""
+        try:
+            frontend = self._context.socket(zmq.ROUTER)
+            backend = self._context.socket(zmq.DEALER)
+            backend.bind("inproc://backend")
+            # Socket for internal control input (i.e. killing the server)
+            ctrl_msg_listener = self._context.socket(zmq.PAIR)
+            ctrl_msg_listener.connect("inproc://ctrl_msg")
+            if self._sec_model_state == ServerSecurityModel.STONEHOUSE:
+                try:
+                    self.auth = self.enable_stonehouse_security(frontend)
+                except ValueError:
+                    raise
+            frontend.bind(self.protocol + "://*:" + str(self.port))
+            poller = zmq.Poller()
+            poller.register(frontend, zmq.POLLIN)
+            poller.register(backend, zmq.POLLIN)
+            poller.register(ctrl_msg_listener, zmq.POLLIN)
+        except Exception as e:
+            raise ValueError(f"Initializing serve() failed due to exception: {e}")
+        workers = dict()
+        project_dirs = dict()  # Mapping of job Id to an abs. path to a project directory ready for execution
+        persistent_exec_mngr_q = queue.Queue()
+        while True:
+            try:
+                socks = dict(poller.poll())
+                # Broker
+                if socks.get(frontend) == zmq.POLLIN:
+                    # Frontend received a message, send it to backend for processing
+                    msg = frontend.recv_multipart()
+                    request = self.handle_frontend_message_received(frontend, msg)
+                    if not request:
+                        print("Received request malformed. - continuing...")
+                        continue
+                    print(f"{request.cmd().upper()} request from client {request.connection_id()}")
+                    job_id = uuid.uuid4().hex  # Job Id for execution worker
+                    if request.cmd() == "ping":
+                        worker = PingService(self._context, request, job_id)
+                    elif request.cmd() == "prepare_execution":
+                        worker = ProjectExtractorService(self._context, request, job_id)
+                    elif request.cmd() == "start_execution":
+                        project_dir = project_dirs.get(request.request_id(), None)  # Get project dir based on job_id
+                        if not project_dir:
+                            print(f"Project for job_id:{request.request_id()} not found")
+                            msg = (
+                                f"Starting DAG execution failed. Project directory for "
+                                f"job_id:{request.request_id()} not found."
+                            )
+                            self.send_init_failed_reply(frontend, request.connection_id(), msg)
+                            continue
+                        worker = RemoteExecutionService(
+                            self._context, request, job_id, project_dir, persistent_exec_mngr_q
+                        )
+                    elif request.cmd() == "stop_execution":
+                        worker = workers.get(request.request_id(), None)  # Get DAG execution worker based on job Id
+                        if not worker:
+                            print(f"Worker for job_id:{request.request_id()} not found")
+                            msg = f"Stopping DAG execution failed. Worker for job_id:{request.request_id()} not found."
+                            self.send_init_failed_reply(frontend, request.connection_id(), msg)
+                            continue
+                        worker.stop_engine()
+                        request.send_response(frontend, ("server_status_msg", "DAG worker stopped"))
+                        continue
+                    elif request.cmd() == "answer_prompt":
+                        worker = workers.get(request.request_id(), None)  # Get DAG execution worker based on job Id
+                        if not worker:
+                            print(f"Worker for job_id:{request.request_id()} not found")
+                            msg = f"Answering prompt failed. Worker for job_id:{request.request_id()} not found."
+                            self.send_init_failed_reply(frontend, request.connection_id(), msg)
+                            continue
+                        item_name, accepted = request.data()
+                        worker.answer_prompt(item_name, accepted)
+                        continue
+                    elif request.cmd() == "retrieve_project":
+                        project_dir = project_dirs.get(request.request_id(), None)  # Get project dir based on job_id
+                        if not project_dir:
+                            print(f"Project for job_id:{request.request_id()} not found")
+                            msg = (
+                                f"Retrieving project for job_id {request.request_id()} failed. "
+                                f"Project directory not found."
+                            )
+                            self.send_init_failed_reply(frontend, request.connection_id(), msg)
+                            continue
+                        worker = ProjectRetrieverService(self._context, request, job_id, project_dir)
+                    elif request.cmd() == "execute_in_persistent":
+                        exec_mngr_key = request.data()[0]
+                        exec_mngr = self.persistent_exec_mngrs.get(exec_mngr_key, None)
+                        if not exec_mngr:
+                            print(f"Persistent exec. mngr for key:{exec_mngr_key} not found.")
+                            msg = (
+                                f"Executing command:{request.data()[1]} - {request.data()[2]} in persistent "
+                                f"manager failed. Persistent execution manager for key {exec_mngr_key} not found."
+                            )
+                            self.send_init_failed_reply(frontend, request.connection_id(), msg)
+                            continue
+                        worker = PersistentExecutionService(self._context, request, job_id, exec_mngr)
+                    elif request.cmd() == "remove_project":
+                        project_dir = project_dirs.get(request.request_id(), None)  # Get project dir based on job_id
+                        if not project_dir:
+                            print(f"Project for job_id:{request.request_id()} not found")
+                            msg = f"Project directory for job_id {request.request_id()} not found"
+                            self.send_init_failed_reply(frontend, request.connection_id(), msg)
+                            continue
+                        worker = ProjectRemoverService(self._context, request, job_id, project_dir)
+                    else:
+                        print(f"Unknown command {request.cmd()} requested")
+                        msg = f"Server error: Unknown command '{request.cmd()}' requested"
+                        self.send_init_failed_reply(frontend, request.connection_id(), msg)
+                        continue
+                    worker.start()
+                    workers[job_id] = worker
+                if socks.get(backend) == zmq.POLLIN:
+                    # Worker has finished execution. Relay reply from backend back to client using the frontend socket
+                    message = backend.recv_multipart()
+                    internal_msg = json.loads(message.pop().decode("utf-8"))
+                    if internal_msg[1] != "in_progress":
+                        finished_worker = workers.pop(internal_msg[0])
+                        if isinstance(finished_worker, ProjectExtractorService):
+                            project_dirs[internal_msg[0]] = internal_msg[1]
+                        if isinstance(finished_worker, RemoteExecutionService):
+                            # Store refs to exec. managers
+                            try:
+                                new_exec_mngrs = persistent_exec_mngr_q.get_nowait()
+                                for k, v in new_exec_mngrs.items():
+                                    self.persistent_exec_mngrs[k] = v
+                            except queue.Empty:
+                                pass
+                        finished_worker.close()
+                        finished_worker.join()
+                    print(f"Sending msg to client {message[0]}")
+                    frontend.send_multipart(message)
+                if socks.get(ctrl_msg_listener) == zmq.POLLIN:
+                    print("Closing server...")
+                    self.kill_persistent_exec_mngrs()
+                    if len(workers) > 0:
+                        print(f"WARNING: Some workers still running:{workers.keys()}")
+                    break
+            except Exception as e:
+                print(f"[DEBUG] EngineServer.serve() exception: {type(e)} serving failed, exception: {e}")
+                break
+        # Close sockets
+        ctrl_msg_listener.close()
+        frontend.close()
+        backend.close()
+
+    def kill_persistent_exec_mngrs(self):
+        """Kills all persistent (execution) manager processes."""
+        n_exec_mngrs = len(self.persistent_exec_mngrs)
+        if n_exec_mngrs > 0:
+            print(f"Closing {len(self.persistent_exec_mngrs)} persistent execution manager processes")
+            for k, exec_mngr in self.persistent_exec_mngrs.items():
+                exec_mngr._persistent_manager.kill_process()
+            self.persistent_exec_mngrs.clear()
+
+    def handle_frontend_message_received(self, socket, msg):
+        """Check received message integrity.
+
+        msg for ping is eg.
+        [b'\x00k\x8bEg', b'{\n   "command": "ping",\n   "id":"4773735",\n   "data":"",\n   "files": {}\n}']
+        where,
+        msg[0] - Frame 1, is the connection identity (or an address). Unique binary string handle to the connection.
+        msg[1] - Frame 2, data frame. This is the request from client as a binary
+            JSON string containing a server message dictionary
+
+        Returns:
+            None if something went wrong or a new Request instance
+        """
+        b_json_str_server_msg = msg[1]  # binary string
+        if len(b_json_str_server_msg) <= 10:  # Message size too small
+            print(f"User data frame too small [{len(msg[1])}]. msg:{msg}")
+            self.send_init_failed_reply(socket, msg[0], "User data frame too small. Malformed message sent to server.")
+            return None
+        try:
+            json_str_server_msg = b_json_str_server_msg.decode("utf-8")  # json string
+        except UnicodeDecodeError as e:
+            print(f"Decoding received msg '{msg[1]} ' failed. \nUnicodeDecodeError: {e}")
+            self.send_init_failed_reply(
+                socket, msg[0], f"UnicodeDecodeError: {e}. " f"- Malformed message sent to server."
+            )
+            return None
+        # Load JSON string into dictionary
+        try:
+            server_msg = json.loads(json_str_server_msg)  # dictionary
+        except json.decoder.JSONDecodeError as e:
+            self.send_init_failed_reply(
+                socket, msg[0], f"json.decoder.JSONDecodeError: {e}. " f"- Message parsing error at server."
+            )
+            return None
+        # server_msg is now a dict with keys: 'command', 'id', 'data', and 'files'
+        data_str = server_msg["data"]  # String
+        files = server_msg["files"]  # Dictionary. TODO: Should this be a list?
+        files_list = []
+        if len(files) > 0:
+            for f in files:
+                files_list.append(files[f])
+        return Request(msg, server_msg["command"], server_msg["id"], data_str, files_list)
+
+    @staticmethod
+    def send_init_failed_reply(socket, connection_id, error_msg):
+        """Sends an error reply to client when request is malformed.
+
+        Args:
+            socket (ZMQSocket): Socket for sending the reply
+            connection_id (bytes): Client Id. Assigned by the frontend ROUTER socket when a request is received.
+            error_msg (str): Error message to client
+        """
+        error_msg_tuple = ("server_init_failed", error_msg)
+        err_msg_as_json = json.dumps(error_msg_tuple)
+        reply_msg = ServerMessage("", "", err_msg_as_json, [])
+        frame = [connection_id, b"", reply_msg.to_bytes()]
+        socket.send_multipart(frame)
+        print("\nClient has been notified. Moving on...")
+
+    def enable_stonehouse_security(self, frontend):
+        """Enables Stonehouse security by starting an authenticator and configuring
+        the frontend socket with authenticator.
+
+        implementation based on https://github.com/zeromq/pyzmq/blob/main/examples/security/stonehouse.py
+
+        Args:
+            frontend (zmq.Socket): Frontend socket
+        """
+        auth = ThreadAuthenticator(self._context)  # Start an authenticator for this context
+        auth.start()
+        endpoints_file = os.path.join(self._sec_folder, "allowEndpoints.txt")
+        if not os.path.exists(endpoints_file):
+            raise ValueError(
+                f"File allowEndpoints.txt missing. Please create the file into directory: "
+                f"{self._sec_folder} and add allowed IP's there"
+            )
+        endpoints = self._read_end_points(endpoints_file)
+        if not endpoints:
+            raise ValueError("No endpoints configured. Please add allowed IP's into allowEndPoints.txt")
+        # Allow configured endpoints
+        allowed = list()
+        for ep in endpoints:
+            try:
+                ep = ep.strip()
+                ipaddress.ip_address(ep)
+                auth.allow(ep)
+                allowed.append(ep)  # Just for printing
+            except:
+                raise ValueError(f"Invalid IP address in allowEndpoints.txt:'{ep}'")
+        allowed_str = "\n".join(allowed)
+        print(f"StoneHouse security activated. Allowed endpoints ({len(allowed)}):\n{allowed_str}")
+        # Tell the authenticator how to handle CURVE requests
+        auth.configure_curve(domain='*', location=zmq.auth.CURVE_ALLOW_ANY)
+        server_secret_file = os.path.join(self.secret_keys_dir, "server.key_secret")
+        server_public, server_secret = zmq.auth.load_certificate(server_secret_file)
+        frontend.curve_secretkey = server_secret
+        frontend.curve_publickey = server_public
+        frontend.curve_server = True  # must come before bind
+        return auth
+
+    @staticmethod
+    def _read_end_points(config_file_location):
+        """Reads all lines from a text file and returns them in a list. Empty strings are removed from the list.
+
+        Args:
+            config_file_location (str): Full path to some text file
+
+        Returns:
+            list: Lines of the given file in a list
+        """
+        with open(config_file_location, "r") as f:
+            all_lines = f.read().splitlines()
+            lines = [x for x in all_lines if x]
+            return lines
```

### Comparing `spine_engine-0.23.3/spine_engine/server/persistent_execution_service.py` & `spine_engine-0.23.4/spine_engine/server/persistent_execution_service.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,83 +1,83 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains a class for remote persistent execution manager related actions.
-"""
-
-import threading
-import json
-import zmq
-from spine_engine.server.service_base import ServiceBase
-
-
-class PersistentExecutionService(threading.Thread, ServiceBase):
-    """Class for interacting with a persistent execution manager running on server."""
-
-    def __init__(self, context, request, job_id, persistent_exec_mngr):
-        """
-        Args:
-            context (zmq.Context): Context for this handler.
-            request (Request): Client request
-            job_id (str): Worker thread Id
-            persistent_exec_mngr (PersistentExecutionManagerBase): Persistent execution manager
-        """
-        super(PersistentExecutionService, self).__init__(name="PersistentExecutionService")
-        ServiceBase.__init__(self, context, request, job_id)
-        self.persistent_exec_mngr = persistent_exec_mngr
-        self.push_socket = self.context.socket(zmq.PUSH)
-
-    def run(self):
-        """Executes client's command in execution service and returns the response back to client."""
-        self.worker_socket.connect("inproc://backend")
-        pub_port = self.push_socket.bind_to_random_port("tcp://*")
-        pm = self.persistent_exec_mngr._persistent_manager
-        cmd_type = self.request.data()[1]  # Command type for persistent manager, e.g. 'is_complete'
-        cmd = self.request.data()[2]  # Command to process in persistent manager
-        if cmd_type == "is_complete":
-            retval = pm.make_complete_command(cmd) is not None
-            retval_tuple = cmd_type, retval
-        elif cmd_type == "issue_persistent_command":
-            self.request.send_response(self.worker_socket, (cmd_type, str(pub_port)), (self.job_id, "in_progress"))
-            for msg in pm.issue_command(cmd, add_history=True, catch_exception=False):
-                json_msg = json.dumps(msg)
-                self.push_socket.send(json_msg.encode("utf-8"))  # This blocks until somebody is pulling (receiving)
-            self.push_socket.send(b'END')
-            retval_tuple = cmd_type, "ok"
-        elif cmd_type == "get_completions":
-            retval = pm.get_completions(cmd)
-            retval_tuple = cmd_type, retval
-        elif cmd_type == "get_history_item":
-            text, prefix, backwards = cmd
-            retval = pm.get_history_item(text, prefix, backwards)
-            retval_tuple = cmd_type, retval
-        elif cmd_type == "restart_persistent":
-            self.request.send_response(self.worker_socket, (cmd_type, str(pub_port)), (self.job_id, "in_progress"))
-            for msg in pm.restart_persistent():
-                json_msg = json.dumps(msg)
-                self.push_socket.send(json_msg.encode("utf-8"))
-            self.push_socket.send(b'END')
-            retval_tuple = cmd_type, "ok"
-        elif cmd_type == "interrupt_persistent":
-            pm.interrupt_persistent()
-            retval_tuple = cmd_type, "ok"
-        elif cmd_type == "kill_persistent":
-            pm.kill_process()
-            retval_tuple = cmd_type, "ok"
-        else:
-            print(f"Command type {cmd_type} does not have a handler. cmd:{cmd}")
-            retval_tuple = cmd_type, "No handler for command"
-        self.request.send_response(self.worker_socket, retval_tuple, (self.job_id, "completed"))
-
-    def close(self):
-        """Cleans up after thread closes."""
-        super().close()
-        self.push_socket.close()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains a class for remote persistent execution manager related actions.
+"""
+
+import threading
+import json
+import zmq
+from spine_engine.server.service_base import ServiceBase
+
+
+class PersistentExecutionService(threading.Thread, ServiceBase):
+    """Class for interacting with a persistent execution manager running on server."""
+
+    def __init__(self, context, request, job_id, persistent_exec_mngr):
+        """
+        Args:
+            context (zmq.Context): Context for this handler.
+            request (Request): Client request
+            job_id (str): Worker thread Id
+            persistent_exec_mngr (PersistentExecutionManagerBase): Persistent execution manager
+        """
+        super(PersistentExecutionService, self).__init__(name="PersistentExecutionService")
+        ServiceBase.__init__(self, context, request, job_id)
+        self.persistent_exec_mngr = persistent_exec_mngr
+        self.push_socket = self.context.socket(zmq.PUSH)
+
+    def run(self):
+        """Executes client's command in execution service and returns the response back to client."""
+        self.worker_socket.connect("inproc://backend")
+        pub_port = self.push_socket.bind_to_random_port("tcp://*")
+        pm = self.persistent_exec_mngr._persistent_manager
+        cmd_type = self.request.data()[1]  # Command type for persistent manager, e.g. 'is_complete'
+        cmd = self.request.data()[2]  # Command to process in persistent manager
+        if cmd_type == "is_complete":
+            retval = pm.make_complete_command(cmd) is not None
+            retval_tuple = cmd_type, retval
+        elif cmd_type == "issue_persistent_command":
+            self.request.send_response(self.worker_socket, (cmd_type, str(pub_port)), (self.job_id, "in_progress"))
+            for msg in pm.issue_command(cmd, add_history=True, catch_exception=False):
+                json_msg = json.dumps(msg)
+                self.push_socket.send(json_msg.encode("utf-8"))  # This blocks until somebody is pulling (receiving)
+            self.push_socket.send(b'END')
+            retval_tuple = cmd_type, "ok"
+        elif cmd_type == "get_completions":
+            retval = pm.get_completions(cmd)
+            retval_tuple = cmd_type, retval
+        elif cmd_type == "get_history_item":
+            text, prefix, backwards = cmd
+            retval = pm.get_history_item(text, prefix, backwards)
+            retval_tuple = cmd_type, retval
+        elif cmd_type == "restart_persistent":
+            self.request.send_response(self.worker_socket, (cmd_type, str(pub_port)), (self.job_id, "in_progress"))
+            for msg in pm.restart_persistent():
+                json_msg = json.dumps(msg)
+                self.push_socket.send(json_msg.encode("utf-8"))
+            self.push_socket.send(b'END')
+            retval_tuple = cmd_type, "ok"
+        elif cmd_type == "interrupt_persistent":
+            pm.interrupt_persistent()
+            retval_tuple = cmd_type, "ok"
+        elif cmd_type == "kill_persistent":
+            pm.kill_process()
+            retval_tuple = cmd_type, "ok"
+        else:
+            print(f"Command type {cmd_type} does not have a handler. cmd:{cmd}")
+            retval_tuple = cmd_type, "No handler for command"
+        self.request.send_response(self.worker_socket, retval_tuple, (self.job_id, "completed"))
+
+    def close(self):
+        """Cleans up after thread closes."""
+        super().close()
+        self.push_socket.close()
```

### Comparing `spine_engine-0.23.3/spine_engine/server/ping_service.py` & `spine_engine-0.23.4/spine_engine/server/ping_service.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,44 +1,44 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains a class for handling ping requests.
-"""
-
-import threading
-import json
-import zmq
-from spine_engine.server.service_base import ServiceBase
-from spine_engine.server.util.server_message import ServerMessage
-
-
-class PingService(threading.Thread, ServiceBase):
-    """Class for handling ping requests."""
-
-    def __init__(self, context, request, job_id):
-        """Initializes instance.
-
-        Args:
-            context (zmq.Context): Server context
-            request (Request): Client request
-            job_id (str): Worker thread Id
-        """
-        super(PingService, self).__init__(name="PingServiceThread")
-        ServiceBase.__init__(self, context, request, job_id)
-
-    def run(self):
-        """Replies to a ping command."""
-        self.worker_socket.connect("inproc://backend")
-        reply_msg = ServerMessage("ping", self.request.request_id(), "", None)
-        internal_msg = json.dumps((self.job_id, "completed"))
-        conn_id = self.request.connection_id()
-        rep = reply_msg.to_bytes()
-        self.request.send_multipart_reply(self.worker_socket, conn_id, rep, internal_msg)
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains a class for handling ping requests.
+"""
+
+import threading
+import json
+import zmq
+from spine_engine.server.service_base import ServiceBase
+from spine_engine.server.util.server_message import ServerMessage
+
+
+class PingService(threading.Thread, ServiceBase):
+    """Class for handling ping requests."""
+
+    def __init__(self, context, request, job_id):
+        """Initializes instance.
+
+        Args:
+            context (zmq.Context): Server context
+            request (Request): Client request
+            job_id (str): Worker thread Id
+        """
+        super(PingService, self).__init__(name="PingServiceThread")
+        ServiceBase.__init__(self, context, request, job_id)
+
+    def run(self):
+        """Replies to a ping command."""
+        self.worker_socket.connect("inproc://backend")
+        reply_msg = ServerMessage("ping", self.request.request_id(), "", None)
+        internal_msg = json.dumps((self.job_id, "completed"))
+        conn_id = self.request.connection_id()
+        rep = reply_msg.to_bytes()
+        self.request.send_multipart_reply(self.worker_socket, conn_id, rep, internal_msg)
```

### Comparing `spine_engine-0.23.3/spine_engine/server/project_extractor_service.py` & `spine_engine-0.23.4/spine_engine/server/project_extractor_service.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,112 +1,112 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains a class for extracting a project ZIP file into a new directory on server.
-"""
-
-import os
-import json
-import threading
-import uuid
-import zmq
-from spine_engine.utils.helpers import get_file_size
-from spine_engine.server.service_base import ServiceBase
-from spine_engine.server.util.server_message import ServerMessage
-from spine_engine.server.util.zip_handler import ZipHandler
-
-
-class ProjectExtractorService(threading.Thread, ServiceBase):
-    """Class for handling 'prepare_execution' requests."""
-
-    # Root directory, where all projects will be extracted and executed
-    INTERNAL_PROJECT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "received_projects")
-
-    def __init__(self, context, request, job_id):
-        """Initializes instance.
-
-        Args:
-            context (zmq.Context): Server context
-            request (Request): Client request
-            job_id (str): Worker thread Id
-        """
-        super(ProjectExtractorService, self).__init__(name="ProjectExtractorServiceThread")
-        ServiceBase.__init__(self, context, request, job_id)
-
-    def run(self):
-        """Extracts the project into a new directory and sends a response back to client."""
-        self.worker_socket.connect("inproc://backend")
-        dir_name = self.request.data()
-        file_names = self.request.filenames()
-        if not len(file_names) == 1:  # No file name included
-            print("Received msg contained no file name for the ZIP file")
-            self.send_completed_with_error("Project ZIP file name missing")
-            return
-        if not dir_name:
-            print("Project name missing from request. Cannot create a local project directory.")
-            self.send_completed_with_error("Project name missing")
-            return
-        if not self.request.zip_file():
-            print("Project ZIP file missing from request")
-            self.send_completed_with_error("Project ZIP file missing")
-            return
-        # Make a new local project directory based on project name in request
-        local_project_dir = os.path.join(
-            ProjectExtractorService.INTERNAL_PROJECT_DIR, dir_name + "__" + uuid.uuid4().hex
-        )
-        # Create project directory
-        try:
-            os.makedirs(local_project_dir)
-        except OSError:
-            print(f"Creating project directory '{local_project_dir}' failed")
-            msg = f"Server failed in creating a project directory for the received project '{local_project_dir}'"
-            self.send_completed_with_error(msg)
-            return
-        # Save the received ZIP file
-        zip_path = os.path.join(local_project_dir, file_names[0])
-        try:
-            with open(zip_path, "wb") as f:
-                f.write(self.request.zip_file())
-        except Exception as e:
-            print(f"Saving the received file to '{zip_path}' failed. [{type(e).__name__}: {e}")
-            msg = f" [{type(e).__name__}] Server failed in saving the received file to '{zip_path}'"
-            self.send_completed_with_error(msg)
-            return
-        # Check that the size of received bytes and the saved ZIP file match
-        if not len(self.request.zip_file()) == os.path.getsize(zip_path):
-            print(
-                f"Error: Size mismatch in saving ZIP file. Received bytes:{len(self.request.zip_file())}. "
-                f"ZIP file size:{os.path.getsize(zip_path)}"
-            )
-        # Extract the saved file
-        print(f"Extracting {file_names[0]} [{get_file_size(os.path.getsize(zip_path))}] to: {local_project_dir}")
-        try:
-            ZipHandler.extract(zip_path, local_project_dir)
-        except Exception as e:
-            print(f"File extraction failed: {type(e).__name__}: {e}")
-            msg = f"{type(e).__name__}: {e}. - File extraction failed on Server"
-            self.send_completed_with_error(msg)
-            return
-        # Remove extracted ZIP file
-        try:
-            os.remove(zip_path)
-        except OSError:
-            print(f"[OSError] File: {zip_path} was not removed")
-        reply_msg = ServerMessage(self.request.cmd(), self.job_id, "", None)
-        internal_msg = json.dumps((self.job_id, local_project_dir))
-        self.request.send_multipart_reply(
-            self.worker_socket, self.request.connection_id(), reply_msg.to_bytes(), internal_msg
-        )
-
-    def send_completed_with_error(self, msg):
-        """Sends completed message to frontend for relaying to client when something goes wrong."""
-        error_event = "remote_execution_init_failed", msg
-        self.request.send_response(self.worker_socket, error_event, (self.job_id, "completed"))
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains a class for extracting a project ZIP file into a new directory on server.
+"""
+
+import os
+import json
+import threading
+import uuid
+import zmq
+from spine_engine.utils.helpers import get_file_size
+from spine_engine.server.service_base import ServiceBase
+from spine_engine.server.util.server_message import ServerMessage
+from spine_engine.server.util.zip_handler import ZipHandler
+
+
+class ProjectExtractorService(threading.Thread, ServiceBase):
+    """Class for handling 'prepare_execution' requests."""
+
+    # Root directory, where all projects will be extracted and executed
+    INTERNAL_PROJECT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "received_projects")
+
+    def __init__(self, context, request, job_id):
+        """Initializes instance.
+
+        Args:
+            context (zmq.Context): Server context
+            request (Request): Client request
+            job_id (str): Worker thread Id
+        """
+        super(ProjectExtractorService, self).__init__(name="ProjectExtractorServiceThread")
+        ServiceBase.__init__(self, context, request, job_id)
+
+    def run(self):
+        """Extracts the project into a new directory and sends a response back to client."""
+        self.worker_socket.connect("inproc://backend")
+        dir_name = self.request.data()
+        file_names = self.request.filenames()
+        if not len(file_names) == 1:  # No file name included
+            print("Received msg contained no file name for the ZIP file")
+            self.send_completed_with_error("Project ZIP file name missing")
+            return
+        if not dir_name:
+            print("Project name missing from request. Cannot create a local project directory.")
+            self.send_completed_with_error("Project name missing")
+            return
+        if not self.request.zip_file():
+            print("Project ZIP file missing from request")
+            self.send_completed_with_error("Project ZIP file missing")
+            return
+        # Make a new local project directory based on project name in request
+        local_project_dir = os.path.join(
+            ProjectExtractorService.INTERNAL_PROJECT_DIR, dir_name + "__" + uuid.uuid4().hex
+        )
+        # Create project directory
+        try:
+            os.makedirs(local_project_dir)
+        except OSError:
+            print(f"Creating project directory '{local_project_dir}' failed")
+            msg = f"Server failed in creating a project directory for the received project '{local_project_dir}'"
+            self.send_completed_with_error(msg)
+            return
+        # Save the received ZIP file
+        zip_path = os.path.join(local_project_dir, file_names[0])
+        try:
+            with open(zip_path, "wb") as f:
+                f.write(self.request.zip_file())
+        except Exception as e:
+            print(f"Saving the received file to '{zip_path}' failed. [{type(e).__name__}: {e}")
+            msg = f" [{type(e).__name__}] Server failed in saving the received file to '{zip_path}'"
+            self.send_completed_with_error(msg)
+            return
+        # Check that the size of received bytes and the saved ZIP file match
+        if not len(self.request.zip_file()) == os.path.getsize(zip_path):
+            print(
+                f"Error: Size mismatch in saving ZIP file. Received bytes:{len(self.request.zip_file())}. "
+                f"ZIP file size:{os.path.getsize(zip_path)}"
+            )
+        # Extract the saved file
+        print(f"Extracting {file_names[0]} [{get_file_size(os.path.getsize(zip_path))}] to: {local_project_dir}")
+        try:
+            ZipHandler.extract(zip_path, local_project_dir)
+        except Exception as e:
+            print(f"File extraction failed: {type(e).__name__}: {e}")
+            msg = f"{type(e).__name__}: {e}. - File extraction failed on Server"
+            self.send_completed_with_error(msg)
+            return
+        # Remove extracted ZIP file
+        try:
+            os.remove(zip_path)
+        except OSError:
+            print(f"[OSError] File: {zip_path} was not removed")
+        reply_msg = ServerMessage(self.request.cmd(), self.job_id, "", None)
+        internal_msg = json.dumps((self.job_id, local_project_dir))
+        self.request.send_multipart_reply(
+            self.worker_socket, self.request.connection_id(), reply_msg.to_bytes(), internal_msg
+        )
+
+    def send_completed_with_error(self, msg):
+        """Sends completed message to frontend for relaying to client when something goes wrong."""
+        error_event = "remote_execution_init_failed", msg
+        self.request.send_response(self.worker_socket, error_event, (self.job_id, "completed"))
```

### Comparing `spine_engine-0.23.3/spine_engine/server/project_remover_service.py` & `spine_engine-0.23.4/spine_engine/server/project_remover_service.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,53 +1,53 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains a class for sending a finished project back to client.
-"""
-
-import os
-import shutil
-import threading
-import zmq
-from spine_engine.server.service_base import ServiceBase
-
-
-class ProjectRemoverService(threading.Thread, ServiceBase):
-    """Class for removing a project folder from server."""
-
-    def __init__(self, context, request, job_id, project_dir):
-        """Initializes instance.
-
-        Args:
-            context (zmq.Context): Server context
-            request (Request): Client request
-            job_id (str): Thread job Id
-            project_dir (str): Absolute path to project directory
-        """
-        super(ProjectRemoverService, self).__init__(name="ProjectRemoverServiceThread")
-        ServiceBase.__init__(self, context, request, job_id)
-        self.project_dir = project_dir
-
-    def run(self):
-        """Removes a used project directory from server."""
-        self.worker_socket.connect("inproc://backend")
-        int_msg = (self.job_id, "completed")
-        if not os.path.isdir(self.project_dir):
-            print(f"Project dir {self.project_dir} has been removed already.")
-            self.request.send_response(self.worker_socket, ("server_event", "ok"), int_msg)
-            return
-        try:
-            shutil.rmtree(self.project_dir)
-        except OSError as e:
-            print(f"[OSError]: {e}\nRemoving project dir failed. Moving on...")
-            self.request.send_response(self.worker_socket, ("server_event", "fine"), int_msg)
-            return
-        self.request.send_response(self.worker_socket, ("server_event", "completed"), int_msg)
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains a class for sending a finished project back to client.
+"""
+
+import os
+import shutil
+import threading
+import zmq
+from spine_engine.server.service_base import ServiceBase
+
+
+class ProjectRemoverService(threading.Thread, ServiceBase):
+    """Class for removing a project folder from server."""
+
+    def __init__(self, context, request, job_id, project_dir):
+        """Initializes instance.
+
+        Args:
+            context (zmq.Context): Server context
+            request (Request): Client request
+            job_id (str): Thread job Id
+            project_dir (str): Absolute path to project directory
+        """
+        super(ProjectRemoverService, self).__init__(name="ProjectRemoverServiceThread")
+        ServiceBase.__init__(self, context, request, job_id)
+        self.project_dir = project_dir
+
+    def run(self):
+        """Removes a used project directory from server."""
+        self.worker_socket.connect("inproc://backend")
+        int_msg = (self.job_id, "completed")
+        if not os.path.isdir(self.project_dir):
+            print(f"Project dir {self.project_dir} has been removed already.")
+            self.request.send_response(self.worker_socket, ("server_event", "ok"), int_msg)
+            return
+        try:
+            shutil.rmtree(self.project_dir)
+        except OSError as e:
+            print(f"[OSError]: {e}\nRemoving project dir failed. Moving on...")
+            self.request.send_response(self.worker_socket, ("server_event", "fine"), int_msg)
+            return
+        self.request.send_response(self.worker_socket, ("server_event", "completed"), int_msg)
```

### Comparing `spine_engine-0.23.3/spine_engine/server/project_retriever_service.py` & `spine_engine-0.23.4/spine_engine/server/project_retriever_service.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,75 +1,75 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains a class for sending a finished project back to client.
-"""
-
-import os
-import shutil
-import threading
-import json
-import zmq
-from spine_engine.utils.helpers import get_file_size
-from spine_engine.server.service_base import ServiceBase
-from spine_engine.server.util.server_message import ServerMessage
-
-
-class ProjectRetrieverService(threading.Thread, ServiceBase):
-    """Class for transmitting a project back to client."""
-
-    def __init__(self, context, request, job_id, project_dir):
-        """Initializes instance.
-
-        Args:
-            context (zmq.Context): Server context
-            request (Request): Client request
-            job_id (str): Thread job Id
-            project_dir (str): Absolute path to project directory
-        """
-        super(ProjectRetrieverService, self).__init__(name="ProjectRetrieverServiceThread")
-        ServiceBase.__init__(self, context, request, job_id)
-        self.project_dir = project_dir
-
-    def run(self):
-        """Replies to a retrieve_project command."""
-        self.worker_socket.connect("inproc://backend")
-        zip_fname = "project_package"  # make_archive() adds extension (.zip)
-        dest_dir = os.path.join(self.project_dir, os.pardir)  # Parent dir of project_dir
-        zip_path = os.path.join(dest_dir, zip_fname)
-        try:
-            shutil.make_archive(zip_path, "zip", self.project_dir)
-        except OSError:
-            msg = f"Zipping project {self.project_dir} to {zip_path} failed"
-            print(msg)
-            self.send_completed_with_error(msg)
-            return
-        zip_fpath = os.path.abspath(os.path.join(self.project_dir, os.pardir, zip_fname + ".zip"))
-        if not os.path.isfile(zip_fpath):
-            msg = f"Zip file {zip_fpath} does not exist"
-            print(msg)
-            self.send_completed_with_error(msg)
-            return
-        file_size = get_file_size(os.path.getsize(zip_fpath))
-        print(f"Transmitting file [{file_size}]: {zip_fpath} to client")
-        # Read file into bytes string and transmit
-        with open(zip_fpath, "rb") as f:
-            file_data = f.read()
-        reply_msg = ServerMessage(self.request.cmd(), self.request.request_id(), "", ["project_package.zip"])
-        internal_msg = json.dumps((self.job_id, "completed"))
-        self.request.send_multipart_reply_with_file(
-            self.worker_socket, self.request.connection_id(), reply_msg.to_bytes(), file_data, internal_msg
-        )
-
-    def send_completed_with_error(self, msg):
-        """Sends completed message to frontend for relaying to client when something goes wrong."""
-        error_event = "project_retriever_service_failed", msg
-        self.request.send_response(self.worker_socket, error_event, (self.job_id, "completed"))
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains a class for sending a finished project back to client.
+"""
+
+import os
+import shutil
+import threading
+import json
+import zmq
+from spine_engine.utils.helpers import get_file_size
+from spine_engine.server.service_base import ServiceBase
+from spine_engine.server.util.server_message import ServerMessage
+
+
+class ProjectRetrieverService(threading.Thread, ServiceBase):
+    """Class for transmitting a project back to client."""
+
+    def __init__(self, context, request, job_id, project_dir):
+        """Initializes instance.
+
+        Args:
+            context (zmq.Context): Server context
+            request (Request): Client request
+            job_id (str): Thread job Id
+            project_dir (str): Absolute path to project directory
+        """
+        super(ProjectRetrieverService, self).__init__(name="ProjectRetrieverServiceThread")
+        ServiceBase.__init__(self, context, request, job_id)
+        self.project_dir = project_dir
+
+    def run(self):
+        """Replies to a retrieve_project command."""
+        self.worker_socket.connect("inproc://backend")
+        zip_fname = "project_package"  # make_archive() adds extension (.zip)
+        dest_dir = os.path.join(self.project_dir, os.pardir)  # Parent dir of project_dir
+        zip_path = os.path.join(dest_dir, zip_fname)
+        try:
+            shutil.make_archive(zip_path, "zip", self.project_dir)
+        except OSError:
+            msg = f"Zipping project {self.project_dir} to {zip_path} failed"
+            print(msg)
+            self.send_completed_with_error(msg)
+            return
+        zip_fpath = os.path.abspath(os.path.join(self.project_dir, os.pardir, zip_fname + ".zip"))
+        if not os.path.isfile(zip_fpath):
+            msg = f"Zip file {zip_fpath} does not exist"
+            print(msg)
+            self.send_completed_with_error(msg)
+            return
+        file_size = get_file_size(os.path.getsize(zip_fpath))
+        print(f"Transmitting file [{file_size}]: {zip_fpath} to client")
+        # Read file into bytes string and transmit
+        with open(zip_fpath, "rb") as f:
+            file_data = f.read()
+        reply_msg = ServerMessage(self.request.cmd(), self.request.request_id(), "", ["project_package.zip"])
+        internal_msg = json.dumps((self.job_id, "completed"))
+        self.request.send_multipart_reply_with_file(
+            self.worker_socket, self.request.connection_id(), reply_msg.to_bytes(), file_data, internal_msg
+        )
+
+    def send_completed_with_error(self, msg):
+        """Sends completed message to frontend for relaying to client when something goes wrong."""
+        error_event = "project_retriever_service_failed", msg
+        self.request.send_response(self.worker_socket, error_event, (self.job_id, "completed"))
```

### Comparing `spine_engine-0.23.3/spine_engine/server/remote_execution_service.py` & `spine_engine-0.23.4/spine_engine/server/remote_execution_service.py`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,219 +1,219 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains RemoteExecutionService class that executes a single DAG on the Spine Engine Server.
-"""
-
-import os
-import threading
-import zmq
-from spine_engine import SpineEngine
-from spine_engine.utils.helpers import get_file_size
-from spine_engine.server.service_base import ServiceBase
-from spine_engine.server.util.event_data_converter import EventDataConverter
-
-
-class RemoteExecutionService(threading.Thread, ServiceBase):
-    """Executes a DAG contained in the client request. Project must
-    be on server before running this service."""
-
-    def __init__(self, context, request, job_id, project_dir, persistent_exec_mngr_q):
-        """
-        Args:
-            context (zmq.Context): Context for this handler.
-            request (Request): Client request
-            job_id (str): Worker thread Id
-            project_dir (str): Absolute path to a server directory where the project has been extracted to
-            persistent_exec_mngr_q (queue.Queue): Queue for storing persistent exec. managers (consumed in frontend)
-        """
-        super().__init__(name="RemoteExecutionServiceThread")
-        ServiceBase.__init__(self, context, request, job_id)
-        self.engine = None
-        self.push_socket = self.context.socket(zmq.PUSH)  # Transmits events and files directly to client
-        self.local_project_dir = project_dir
-        self.persistent_keys = dict()  # Mapping of item_name to a persistent execution manager key
-        self.persistent_exec_mngrs = dict()  # Mapping of per. execution manager key to per. execution manager
-        self.persist_q = persistent_exec_mngr_q
-        self.items = list()
-
-    def collect_persistent_keys(self, event_type, data):
-        """Collects the keys used in identifying persistent execution managers
-        The key is in a persistent_execution_msg when the type is persistent_started."""
-        if event_type == "persistent_execution_msg" and data["type"] == "persistent_started":
-            self.persistent_keys[data["item_name"]] = data["key"]
-
-    def collect_persistent_console_managers(self, event_type, data, running_items):
-        """Collects a persistent execution manager from a tool item that is being
-        executed in engine (running). Matches the key (collected earlier), with the
-        persistent execution manager and inserts them into a dict."""
-        if event_type == "persistent_execution_msg" and data["type"] == "execution_started":
-            persistent_owner = data["item_name"]
-            if len(running_items) > 0:
-                for item in running_items:
-                    if item.name == persistent_owner:
-                        ref = item._tool_instance.exec_mngr
-                        k = self.persistent_keys[persistent_owner]
-                        self.persistent_exec_mngrs[k] = ref
-            else:  # If this happens regularly, we have a problem
-                print(f"[DEBUG] Collecting {persistent_owner}'s persistent exec. manager failed. Item not running.")
-
-    def collect_running_items(self, running_items):
-        """Collects executed items into a list."""
-        if len(running_items) > 0:
-            for running_item in running_items:
-                if running_item not in self.items:
-                    self.items.append(running_item)
-
-    def collect_resources(self):
-        """Returns a dictionary containing items name, type and ProjectItemResources."""
-        resources = dict()
-        for item in self.items:
-            resources[item.name] = [item.item_type(), item._output_resources_forward()]
-        return resources
-
-    def run(self):
-        """Sends an execution started response to start execution request. Runs Spine Engine
-        and sends the events to the client using a publish socket."""
-        self.worker_socket.connect("inproc://backend")
-        push_port = self.push_socket.bind_to_random_port("tcp://*")
-        engine_data = self.request.data()
-        print(f"Executing DAG [{self.job_id}] ...")
-        # Send reply to 'start_execution' request to client with the push socket port for
-        # pulling events and worker job id for stopping execution
-        self.request.send_response(
-            self.worker_socket, ("remote_execution_started", str(push_port), self.job_id), (self.job_id, "in_progress")
-        )
-        converted_data = self.convert_input(engine_data, self.local_project_dir)
-        self.engine = SpineEngine(**converted_data)
-        try:
-            while True:
-                event_type, data = self.engine.get_event()  # Get next event and associated data from spine engine
-                self.collect_persistent_keys(event_type, data)
-                self.collect_persistent_console_managers(event_type, data, self.engine._running_items)
-                self.collect_running_items(self.engine._running_items)
-                json_event = EventDataConverter.convert(event_type, data)
-                self.push_socket.send_multipart([json_event.encode("utf-8")])  # Blocks until the client pulls
-                if data == "COMPLETED" or data == "FAILED" or data == "USER_STOPPED":
-                    break
-        except StopIteration:
-            # Raised by SpineEngine._get_event_stream() generator if we try to get_event() after
-            # "dag_exec_finished" has been processed
-            print("[DEBUG] Handled StopIteration exception")
-            self.send_completed()
-            return
-        except Exception as e:
-            print(f"Execution failed: {type(e).__name__}: {e}")
-            json_error_event = EventDataConverter.convert(
-                "server_execution_error", f"{type(e).__name__}: {e}. - Project execution failed on Server"
-            )
-            self.push_socket.send_multipart([json_error_event.encode("utf-8")])
-            self.send_completed()
-            return
-        finally:
-            self.engine.wait()
-        if data != "USER_STOPPED":
-            resources = self.collect_resources()
-            self.persist_q.put(self.persistent_exec_mngrs)  # Put new persistent execution managers to queue
-            # Send file resources back to client except for Data Connections
-            for type_and_pir in resources.values():
-                if type_and_pir[0] == "Data Connection":
-                    continue
-                for resource in type_and_pir[1]:
-                    if resource.hasfilepath:
-                        with open(resource.path, "rb") as f:
-                            file_data = f.read()
-                        _, fname = os.path.split(resource.path)
-                        fsize = get_file_size(os.path.getsize(resource.path))
-                        self.push_socket.send_multipart([b"incoming_file", f"{fname} [{fsize}]".encode("utf-8")])
-                        path_rel_to_project_dir = os.path.relpath(resource.path, self.local_project_dir)
-                        b_fpath = path_rel_to_project_dir.replace(os.sep, "/").encode("utf-8")  # Replace "\" with "/"
-                        self.push_socket.send_multipart([b_fpath, file_data])
-            self.push_socket.send_multipart([b"END", b""])
-            print(f"Executing DAG [{self.job_id}] completed")
-        else:
-            print(f"Executing DAG [{self.job_id}] stopped")
-        self.send_completed()
-
-    def send_completed(self):
-        """Sends a 'completed' message to frontend to notify that this worker has finished and it can be cleaned up.
-        This message should not to be relayed to client.
-        """
-        self.request.send_response(
-            self.worker_socket, ("remote_execution_event", "completed"), (self.job_id, "completed")
-        )
-
-    def stop_engine(self):
-        """Stops DAG execution."""
-        self.engine.stop()
-
-    def answer_prompt(self, item_name, accepted):
-        """Answers prompt."""
-        self.engine.answer_prompt(item_name, accepted)
-
-    def close(self):
-        """Cleans up sockets after worker is finished."""
-        super().close()
-        self.push_socket.close()
-
-    @staticmethod
-    def convert_input(input_data, local_project_dir):
-        """Converts received input data for execution in a local folder.
-
-        Args:
-            input_data (dict): Input data as a dict.
-            local_project_dir (str): Local (on server) project directory.
-
-        Returns:
-            dict: Converted input data
-        """
-        # Adjust project_dir to point to the local folder
-        remote_folder = input_data["project_dir"]  # Project directory on client
-        input_data["project_dir"] = local_project_dir  # Project directory on server
-        # Loop specs
-        specs_keys = input_data["specifications"].keys()
-        for specs_key in specs_keys:
-            spec_item = input_data["specifications"][specs_key]
-            i = 0
-            for specItemInfo in spec_item:
-                # Adjust definition_file_path in specs to point to the server folder
-                if "definition_file_path" in specItemInfo:
-                    original_def_file_path = specItemInfo["definition_file_path"]  # Absolute path on client machine
-                    # Make sure path separators match the OS separator
-                    original_def_file_path = original_def_file_path.replace("\\", os.path.sep)
-                    # Remove part of definition file path that references client machine path to get
-                    # a relative definition file path. Note: os.path.relpath() does not work because the output
-                    # depends on OS. Note2: '/' must be added to remote folder here.
-                    rel_def_file_path = original_def_file_path.replace(remote_folder + "/", "")
-                    modified = os.path.join(local_project_dir, rel_def_file_path)  # Absolute path on server machine
-                    input_data["specifications"][specs_key][i]["definition_file_path"] = modified
-                i += 1
-                # Modify Python executable path and kernel spec because those refer to paths on client's machine
-                if "execution_settings" in specItemInfo and specItemInfo["tooltype"] == "python":
-                    if specItemInfo["execution_settings"]["use_jupyter_console"]:
-                        # Replace kernel_spec_name with the default kernel spec 'python3' (must be available on server)
-                        specItemInfo["execution_settings"]["kernel_spec_name"] = "python3"
-                    else:
-                        # Replace Python executable exec with "" because client's Python is not available on server
-                        specItemInfo["execution_settings"]["executable"] = ""
-        # Loop items
-        items_keys = input_data["items"].keys()
-        for items_key in items_keys:
-            # Force execute in source dir
-            if "execute_in_work" in input_data["items"][items_key]:
-                input_data["items"][items_key]["execute_in_work"] = False
-        # Edit app settings dictionary
-        # Replace Julia path and Julia project path with an empty string so that the server uses the Julia in PATH
-        input_data["settings"]["appSettings/juliaPath"] = ""
-        input_data["settings"]["appSettings/juliaProjectPath"] = ""
-        # Replace Julia kernel
-        input_data["settings"]["appSettings/juliaKernel"] = "julia-1.8"  # (must be available on server)
-        return input_data
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains RemoteExecutionService class that executes a single DAG on the Spine Engine Server.
+"""
+
+import os
+import threading
+import zmq
+from spine_engine import SpineEngine
+from spine_engine.utils.helpers import get_file_size
+from spine_engine.server.service_base import ServiceBase
+from spine_engine.server.util.event_data_converter import EventDataConverter
+
+
+class RemoteExecutionService(threading.Thread, ServiceBase):
+    """Executes a DAG contained in the client request. Project must
+    be on server before running this service."""
+
+    def __init__(self, context, request, job_id, project_dir, persistent_exec_mngr_q):
+        """
+        Args:
+            context (zmq.Context): Context for this handler.
+            request (Request): Client request
+            job_id (str): Worker thread Id
+            project_dir (str): Absolute path to a server directory where the project has been extracted to
+            persistent_exec_mngr_q (queue.Queue): Queue for storing persistent exec. managers (consumed in frontend)
+        """
+        super().__init__(name="RemoteExecutionServiceThread")
+        ServiceBase.__init__(self, context, request, job_id)
+        self.engine = None
+        self.push_socket = self.context.socket(zmq.PUSH)  # Transmits events and files directly to client
+        self.local_project_dir = project_dir
+        self.persistent_keys = dict()  # Mapping of item_name to a persistent execution manager key
+        self.persistent_exec_mngrs = dict()  # Mapping of per. execution manager key to per. execution manager
+        self.persist_q = persistent_exec_mngr_q
+        self.items = list()
+
+    def collect_persistent_keys(self, event_type, data):
+        """Collects the keys used in identifying persistent execution managers
+        The key is in a persistent_execution_msg when the type is persistent_started."""
+        if event_type == "persistent_execution_msg" and data["type"] == "persistent_started":
+            self.persistent_keys[data["item_name"]] = data["key"]
+
+    def collect_persistent_console_managers(self, event_type, data, running_items):
+        """Collects a persistent execution manager from a tool item that is being
+        executed in engine (running). Matches the key (collected earlier), with the
+        persistent execution manager and inserts them into a dict."""
+        if event_type == "persistent_execution_msg" and data["type"] == "execution_started":
+            persistent_owner = data["item_name"]
+            if len(running_items) > 0:
+                for item in running_items:
+                    if item.name == persistent_owner:
+                        ref = item._tool_instance.exec_mngr
+                        k = self.persistent_keys[persistent_owner]
+                        self.persistent_exec_mngrs[k] = ref
+            else:  # If this happens regularly, we have a problem
+                print(f"[DEBUG] Collecting {persistent_owner}'s persistent exec. manager failed. Item not running.")
+
+    def collect_running_items(self, running_items):
+        """Collects executed items into a list."""
+        if len(running_items) > 0:
+            for running_item in running_items:
+                if running_item not in self.items:
+                    self.items.append(running_item)
+
+    def collect_resources(self):
+        """Returns a dictionary containing items name, type and ProjectItemResources."""
+        resources = dict()
+        for item in self.items:
+            resources[item.name] = [item.item_type(), item._output_resources_forward()]
+        return resources
+
+    def run(self):
+        """Sends an execution started response to start execution request. Runs Spine Engine
+        and sends the events to the client using a publish socket."""
+        self.worker_socket.connect("inproc://backend")
+        push_port = self.push_socket.bind_to_random_port("tcp://*")
+        engine_data = self.request.data()
+        print(f"Executing DAG [{self.job_id}] ...")
+        # Send reply to 'start_execution' request to client with the push socket port for
+        # pulling events and worker job id for stopping execution
+        self.request.send_response(
+            self.worker_socket, ("remote_execution_started", str(push_port), self.job_id), (self.job_id, "in_progress")
+        )
+        converted_data = self.convert_input(engine_data, self.local_project_dir)
+        self.engine = SpineEngine(**converted_data)
+        try:
+            while True:
+                event_type, data = self.engine.get_event()  # Get next event and associated data from spine engine
+                self.collect_persistent_keys(event_type, data)
+                self.collect_persistent_console_managers(event_type, data, self.engine._running_items)
+                self.collect_running_items(self.engine._running_items)
+                json_event = EventDataConverter.convert(event_type, data)
+                self.push_socket.send_multipart([json_event.encode("utf-8")])  # Blocks until the client pulls
+                if data == "COMPLETED" or data == "FAILED" or data == "USER_STOPPED":
+                    break
+        except StopIteration:
+            # Raised by SpineEngine._get_event_stream() generator if we try to get_event() after
+            # "dag_exec_finished" has been processed
+            print("[DEBUG] Handled StopIteration exception")
+            self.send_completed()
+            return
+        except Exception as e:
+            print(f"Execution failed: {type(e).__name__}: {e}")
+            json_error_event = EventDataConverter.convert(
+                "server_execution_error", f"{type(e).__name__}: {e}. - Project execution failed on Server"
+            )
+            self.push_socket.send_multipart([json_error_event.encode("utf-8")])
+            self.send_completed()
+            return
+        finally:
+            self.engine.wait()
+        if data != "USER_STOPPED":
+            resources = self.collect_resources()
+            self.persist_q.put(self.persistent_exec_mngrs)  # Put new persistent execution managers to queue
+            # Send file resources back to client except for Data Connections
+            for type_and_pir in resources.values():
+                if type_and_pir[0] == "Data Connection":
+                    continue
+                for resource in type_and_pir[1]:
+                    if resource.hasfilepath:
+                        with open(resource.path, "rb") as f:
+                            file_data = f.read()
+                        _, fname = os.path.split(resource.path)
+                        fsize = get_file_size(os.path.getsize(resource.path))
+                        self.push_socket.send_multipart([b"incoming_file", f"{fname} [{fsize}]".encode("utf-8")])
+                        path_rel_to_project_dir = os.path.relpath(resource.path, self.local_project_dir)
+                        b_fpath = path_rel_to_project_dir.replace(os.sep, "/").encode("utf-8")  # Replace "\" with "/"
+                        self.push_socket.send_multipart([b_fpath, file_data])
+            self.push_socket.send_multipart([b"END", b""])
+            print(f"Executing DAG [{self.job_id}] completed")
+        else:
+            print(f"Executing DAG [{self.job_id}] stopped")
+        self.send_completed()
+
+    def send_completed(self):
+        """Sends a 'completed' message to frontend to notify that this worker has finished and it can be cleaned up.
+        This message should not to be relayed to client.
+        """
+        self.request.send_response(
+            self.worker_socket, ("remote_execution_event", "completed"), (self.job_id, "completed")
+        )
+
+    def stop_engine(self):
+        """Stops DAG execution."""
+        self.engine.stop()
+
+    def answer_prompt(self, item_name, accepted):
+        """Answers prompt."""
+        self.engine.answer_prompt(item_name, accepted)
+
+    def close(self):
+        """Cleans up sockets after worker is finished."""
+        super().close()
+        self.push_socket.close()
+
+    @staticmethod
+    def convert_input(input_data, local_project_dir):
+        """Converts received input data for execution in a local folder.
+
+        Args:
+            input_data (dict): Input data as a dict.
+            local_project_dir (str): Local (on server) project directory.
+
+        Returns:
+            dict: Converted input data
+        """
+        # Adjust project_dir to point to the local folder
+        remote_folder = input_data["project_dir"]  # Project directory on client
+        input_data["project_dir"] = local_project_dir  # Project directory on server
+        # Loop specs
+        specs_keys = input_data["specifications"].keys()
+        for specs_key in specs_keys:
+            spec_item = input_data["specifications"][specs_key]
+            i = 0
+            for specItemInfo in spec_item:
+                # Adjust definition_file_path in specs to point to the server folder
+                if "definition_file_path" in specItemInfo:
+                    original_def_file_path = specItemInfo["definition_file_path"]  # Absolute path on client machine
+                    # Make sure path separators match the OS separator
+                    original_def_file_path = original_def_file_path.replace("\\", os.path.sep)
+                    # Remove part of definition file path that references client machine path to get
+                    # a relative definition file path. Note: os.path.relpath() does not work because the output
+                    # depends on OS. Note2: '/' must be added to remote folder here.
+                    rel_def_file_path = original_def_file_path.replace(remote_folder + "/", "")
+                    modified = os.path.join(local_project_dir, rel_def_file_path)  # Absolute path on server machine
+                    input_data["specifications"][specs_key][i]["definition_file_path"] = modified
+                i += 1
+                # Modify Python executable path and kernel spec because those refer to paths on client's machine
+                if "execution_settings" in specItemInfo and specItemInfo["tooltype"] == "python":
+                    if specItemInfo["execution_settings"]["use_jupyter_console"]:
+                        # Replace kernel_spec_name with the default kernel spec 'python3' (must be available on server)
+                        specItemInfo["execution_settings"]["kernel_spec_name"] = "python3"
+                    else:
+                        # Replace Python executable exec with "" because client's Python is not available on server
+                        specItemInfo["execution_settings"]["executable"] = ""
+        # Loop items
+        items_keys = input_data["items"].keys()
+        for items_key in items_keys:
+            # Force execute in source dir
+            if "execute_in_work" in input_data["items"][items_key]:
+                input_data["items"][items_key]["execute_in_work"] = False
+        # Edit app settings dictionary
+        # Replace Julia path and Julia project path with an empty string so that the server uses the Julia in PATH
+        input_data["settings"]["appSettings/juliaPath"] = ""
+        input_data["settings"]["appSettings/juliaProjectPath"] = ""
+        # Replace Julia kernel
+        input_data["settings"]["appSettings/juliaKernel"] = "julia-1.8"  # (must be available on server)
+        return input_data
```

### Comparing `spine_engine-0.23.3/spine_engine/server/request.py` & `spine_engine-0.23.4/spine_engine/server/request.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,123 +1,123 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains a class for client requests received at server.
-"""
-
-import json
-from spine_engine.server.util.server_message import ServerMessage
-
-
-class Request:
-    """Class for bundling the received request and associated data together."""
-
-    def __init__(self, msg, cmd, request_id, data, filenames):
-        """Init class.
-
-        Args:
-            msg (list): List of three or four binary frames (conn id, empty frame and user data frame,
-                and possibly zip-file)
-            cmd (str): Command associated with the request
-            request_id (str): Client request Id
-            data (bytes): Zip-file
-            filenames (list): List of associated filenames
-        """
-        self._msg = msg
-        self._cmd = cmd
-        self._request_id = request_id
-        self._data = data
-        self._filenames = filenames
-        self._connection_id = msg[0]  # Assigned by the frontend (ROUTER) socket that received the message
-        self._zip_file = None
-        if len(msg) == 3:
-            self._zip_file = msg[2]
-
-    def msg(self):
-        """Returns a list containing three binary frames.
-        First frame is the connection id (added by the frontend ROUTER socket at receiver).
-        The second frame is empty (added by the frontend ROUTER socket at receiver).
-        The third frame contains the data sent by client."""
-        return self._msg
-
-    def cmd(self):
-        """Returns the command as string (eg. 'execute' or 'ping' associated to this request)."""
-        return self._cmd
-
-    def request_id(self):
-        """Returns the request id as string of the received ServerMessage.
-        Assigned by client when the request was made."""
-        return self._request_id
-
-    def data(self):
-        """Returns the parsed msg associated to this request."""
-        return self._data
-
-    def filenames(self):
-        """Returns associated filenames if any."""
-        return self._filenames
-
-    def connection_id(self):
-        """Returns the connection Id as binary string. Assigned by the frontend
-        ROUTER socket when the message was received at server."""
-        return self._connection_id
-
-    def zip_file(self):
-        """Returns the binary zip file (zipped project dir) associated with the message
-        or None if the message did not contain a zip-file."""
-        return self._zip_file
-
-    def send_response(self, socket, info, internal_msg=None, dump_to_json=True):
-        """Sends a response message to client. Do not use \n in the internal_msg (not allowed in JSON).
-
-        Args:
-            socket (ZMQSocket): Socket for sending the reply
-            info (tuple): Message (data) tuple for client [event_type, msg]
-            internal_msg (tuple): Internal server message, [job_id, msg]
-            dump_to_json (bool): If True, info is dumped to a JSON str. When False, info must be a JSON str already.
-        """
-        info_as_json = json.dumps(info) if dump_to_json else info
-        reply_msg = ServerMessage(self._cmd, self._request_id, info_as_json, [])
-        if not internal_msg:
-            self.send_multipart_reply(socket, self.connection_id(), reply_msg.to_bytes())
-        else:
-            internal_msg_json = json.dumps(internal_msg)
-            self.send_multipart_reply(socket, self.connection_id(), reply_msg.to_bytes(), internal_msg_json)
-
-    @staticmethod
-    def send_multipart_reply(socket, connection_id, data, internal_msg=None):
-        """Sends a multi-part (multi-frame) response.
-
-        Args:
-            socket (ZMQSocket): Socket for sending the reply
-            connection_id (bytes): Client Id. Assigned by the frontend ROUTER socket when a request is received.
-            data (bytes): User data to be sent
-            internal_msg (str): Internal server message as JSON string
-        """
-        if not internal_msg:
-            frame = [connection_id, b"", data]
-        else:
-            frame = [connection_id, b"", data, internal_msg.encode("utf-8")]
-        socket.send_multipart(frame)
-
-    @staticmethod
-    def send_multipart_reply_with_file(socket, connection_id, data, file, internal_msg):
-        """Sends a multi-part (multi-frame) response.
-
-        Args:
-            socket (ZMQSocket): Socket for sending the reply
-            connection_id (bytes): Client Id. Assigned by the frontend ROUTER socket when a request is received.
-            data (bytes): User data to be sent
-            file (bytes): File to transmit to client
-            internal_msg (str): Internal server message as JSON string
-        """
-        frame = [connection_id, b"", data, file, internal_msg.encode("utf-8")]
-        socket.send_multipart(frame)
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains a class for client requests received at server.
+"""
+
+import json
+from spine_engine.server.util.server_message import ServerMessage
+
+
+class Request:
+    """Class for bundling the received request and associated data together."""
+
+    def __init__(self, msg, cmd, request_id, data, filenames):
+        """Init class.
+
+        Args:
+            msg (list): List of three or four binary frames (conn id, empty frame and user data frame,
+                and possibly zip-file)
+            cmd (str): Command associated with the request
+            request_id (str): Client request Id
+            data (bytes): Zip-file
+            filenames (list): List of associated filenames
+        """
+        self._msg = msg
+        self._cmd = cmd
+        self._request_id = request_id
+        self._data = data
+        self._filenames = filenames
+        self._connection_id = msg[0]  # Assigned by the frontend (ROUTER) socket that received the message
+        self._zip_file = None
+        if len(msg) == 3:
+            self._zip_file = msg[2]
+
+    def msg(self):
+        """Returns a list containing three binary frames.
+        First frame is the connection id (added by the frontend ROUTER socket at receiver).
+        The second frame is empty (added by the frontend ROUTER socket at receiver).
+        The third frame contains the data sent by client."""
+        return self._msg
+
+    def cmd(self):
+        """Returns the command as string (eg. 'execute' or 'ping' associated to this request)."""
+        return self._cmd
+
+    def request_id(self):
+        """Returns the request id as string of the received ServerMessage.
+        Assigned by client when the request was made."""
+        return self._request_id
+
+    def data(self):
+        """Returns the parsed msg associated to this request."""
+        return self._data
+
+    def filenames(self):
+        """Returns associated filenames if any."""
+        return self._filenames
+
+    def connection_id(self):
+        """Returns the connection Id as binary string. Assigned by the frontend
+        ROUTER socket when the message was received at server."""
+        return self._connection_id
+
+    def zip_file(self):
+        """Returns the binary zip file (zipped project dir) associated with the message
+        or None if the message did not contain a zip-file."""
+        return self._zip_file
+
+    def send_response(self, socket, info, internal_msg=None, dump_to_json=True):
+        """Sends a response message to client. Do not use \n in the internal_msg (not allowed in JSON).
+
+        Args:
+            socket (ZMQSocket): Socket for sending the reply
+            info (tuple): Message (data) tuple for client [event_type, msg]
+            internal_msg (tuple): Internal server message, [job_id, msg]
+            dump_to_json (bool): If True, info is dumped to a JSON str. When False, info must be a JSON str already.
+        """
+        info_as_json = json.dumps(info) if dump_to_json else info
+        reply_msg = ServerMessage(self._cmd, self._request_id, info_as_json, [])
+        if not internal_msg:
+            self.send_multipart_reply(socket, self.connection_id(), reply_msg.to_bytes())
+        else:
+            internal_msg_json = json.dumps(internal_msg)
+            self.send_multipart_reply(socket, self.connection_id(), reply_msg.to_bytes(), internal_msg_json)
+
+    @staticmethod
+    def send_multipart_reply(socket, connection_id, data, internal_msg=None):
+        """Sends a multi-part (multi-frame) response.
+
+        Args:
+            socket (ZMQSocket): Socket for sending the reply
+            connection_id (bytes): Client Id. Assigned by the frontend ROUTER socket when a request is received.
+            data (bytes): User data to be sent
+            internal_msg (str): Internal server message as JSON string
+        """
+        if not internal_msg:
+            frame = [connection_id, b"", data]
+        else:
+            frame = [connection_id, b"", data, internal_msg.encode("utf-8")]
+        socket.send_multipart(frame)
+
+    @staticmethod
+    def send_multipart_reply_with_file(socket, connection_id, data, file, internal_msg):
+        """Sends a multi-part (multi-frame) response.
+
+        Args:
+            socket (ZMQSocket): Socket for sending the reply
+            connection_id (bytes): Client Id. Assigned by the frontend ROUTER socket when a request is received.
+            data (bytes): User data to be sent
+            file (bytes): File to transmit to client
+            internal_msg (str): Internal server message as JSON string
+        """
+        frame = [connection_id, b"", data, file, internal_msg.encode("utf-8")]
+        socket.send_multipart(frame)
```

### Comparing `spine_engine-0.23.3/spine_engine/server/service_base.py` & `spine_engine-0.23.4/spine_engine/server/service_base.py`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,37 +1,37 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains a base class for different services provided by the Spine Engine Server.
-"""
-
-import zmq
-
-
-class ServiceBase:
-    """Service base class."""
-
-    def __init__(self, context, request, job_id):
-        """Initializes instance.
-
-        Args:
-            context (zmq.Context): Context for this handler.
-            request (Request): Client request
-            job_id (str): Worker thread Id
-        """
-        self.context = context
-        self.request = request
-        self.job_id = job_id
-        self.worker_socket = self.context.socket(zmq.DEALER)
-
-    def close(self):
-        """Closes socket after thread has finished."""
-        self.worker_socket.close()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains a base class for different services provided by the Spine Engine Server.
+"""
+
+import zmq
+
+
+class ServiceBase:
+    """Service base class."""
+
+    def __init__(self, context, request, job_id):
+        """Initializes instance.
+
+        Args:
+            context (zmq.Context): Context for this handler.
+            request (Request): Client request
+            job_id (str): Worker thread Id
+        """
+        self.context = context
+        self.request = request
+        self.job_id = job_id
+        self.worker_socket = self.context.socket(zmq.DEALER)
+
+    def close(self):
+        """Closes socket after thread has finished."""
+        self.worker_socket.close()
```

### Comparing `spine_engine-0.23.3/spine_engine/server/start_server.py` & `spine_engine-0.23.4/spine_engine/server/start_server.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,62 +1,62 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Start script for Spine Engine Server.
-
-"""
-
-import sys
-import time
-from spine_engine.server.engine_server import EngineServer, ServerSecurityModel
-
-
-def main(argv):
-    """Spine Engine server main."""
-    if len(argv) != 2 and len(argv) != 4:
-        print(
-            f"Spine Engine Server\n\nUsage:\n  python {argv[0]} <port>\n"
-            f"or\n  python {argv[0]} <port> stonehouse <path_to_security_folder>\n"
-            f"to enable security."
-        )
-        return
-    server = None
-    try:
-        port = int(argv[1])
-        if len(argv) == 2:
-            server = EngineServer("tcp", port, ServerSecurityModel.NONE, "")
-        elif len(argv) == 4:
-            server = EngineServer("tcp", port, ServerSecurityModel.STONEHOUSE, argv[3])
-    except Exception as e:
-        print(f"{type(e).__name__}: {e}")
-        return
-    # Block main thread until user closes it
-    print("Press c or Ctrl-c to close the server")
-    print("\nListening...")
-    kb_input = ""
-    while kb_input != "c":
-        try:
-            kb_input = input()
-        except (KeyboardInterrupt, EOFError):
-            kb_input = "c"
-        if kb_input == "c":
-            try:
-                server.close()
-            except Exception as e:
-                print(f"start_server.main(): {type(e).__name__}: {e}")
-            break
-        else:
-            time.sleep(0.1)
-    return
-
-
-if __name__ == "__main__":
-    main(sys.argv)
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Start script for Spine Engine Server.
+
+"""
+
+import sys
+import time
+from spine_engine.server.engine_server import EngineServer, ServerSecurityModel
+
+
+def main(argv):
+    """Spine Engine server main."""
+    if len(argv) != 2 and len(argv) != 4:
+        print(
+            f"Spine Engine Server\n\nUsage:\n  python {argv[0]} <port>\n"
+            f"or\n  python {argv[0]} <port> stonehouse <path_to_security_folder>\n"
+            f"to enable security."
+        )
+        return
+    server = None
+    try:
+        port = int(argv[1])
+        if len(argv) == 2:
+            server = EngineServer("tcp", port, ServerSecurityModel.NONE, "")
+        elif len(argv) == 4:
+            server = EngineServer("tcp", port, ServerSecurityModel.STONEHOUSE, argv[3])
+    except Exception as e:
+        print(f"{type(e).__name__}: {e}")
+        return
+    # Block main thread until user closes it
+    print("Press c or Ctrl-c to close the server")
+    print("\nListening...")
+    kb_input = ""
+    while kb_input != "c":
+        try:
+            kb_input = input()
+        except (KeyboardInterrupt, EOFError):
+            kb_input = "c"
+        if kb_input == "c":
+            try:
+                server.close()
+            except Exception as e:
+                print(f"start_server.main(): {type(e).__name__}: {e}")
+            break
+        else:
+            time.sleep(0.1)
+    return
+
+
+if __name__ == "__main__":
+    main(sys.argv)
```

### Comparing `spine_engine-0.23.3/spine_engine/server/util/__init__.py` & `spine_engine-0.23.4/tests/server/util/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,15 +1,10 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-This package contains utility classes of remote server part of the Spine Engine.
-
-"""
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
```

### Comparing `spine_engine-0.23.3/spine_engine/server/util/event_data_converter.py` & `spine_engine-0.23.4/spine_engine/server/util/event_data_converter.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,139 +1,139 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains static methods for converting event and data information to JSON format and back.
-"""
-
-import base64
-import json
-from spine_engine.spine_engine import ItemExecutionFinishState
-
-
-class EventDataConverter:
-    @staticmethod
-    def convert(event_type, data, b64encoding=False):
-        """Converts a single event_type, data pair into a JSON string.
-         Optionally, encodes data as base64.
-
-        Args:
-            event_type: (str): Event type (e.g. exec_started, dag_exec_finished, etc.)
-            data (dict or str): Data associated with the event_type
-            b64encoding (bool): True encodes data as base64, False does not
-
-        Returns:
-            str: JSON string
-        """
-        if b64encoding:
-            msg_b = str(data).encode("ascii")
-            base64_b = base64.b64encode(msg_b)
-            data = base64_b.decode("ascii")
-        data = break_event_data(event_type, data)
-        event_dict = {"event_type": event_type, "data": data}
-        json_event_data = json.dumps(event_dict)
-        return json_event_data
-
-    @staticmethod
-    def deconvert(event_data, b64decoding=False):
-        """Decodes a bytes object into a JSON string, then converts it to a
-        dictionary with a single event_type, data pair into a tuple
-        containing the same. Optionally, decodes data field from base64
-        back to ascii.
-
-        Args:
-            event_data (bytes): Event type and data as bytes
-            b64decoding (bool): Flag indicating, whether data is decoded from base64
-
-        Returns:
-            (tuple): Event type and data pair
-        """
-        event_dict = json.loads(event_data.decode("utf-8"))
-        if b64decoding:
-            base64_bytes = event_dict["data"].encode("ascii")
-            message_bytes = base64.b64decode(base64_bytes)
-            data = message_bytes.decode("ascii")
-        else:
-            data = event_dict["data"]
-        fixed_event = fix_event_data((event_dict["event_type"], data))
-        return fixed_event
-
-
-def break_event_data(event_type, data):
-    """Makes values in data dictionary suitable for converting them to JSON strings.
-
-    Args:
-        event_type (str): Event type
-        data (dict or str): Data
-
-    Returns:
-        dict or str: Edited data dictionary or data string as it was.
-    """
-    if type(data) != str:
-        if "item_state" in data.keys():
-            data["item_state"] = str(data["item_state"])  # Cast ItemExecutionFinishState instance to string
-        if "url" in data.keys():
-            data["url"] = str(data["url"])  # Cast URL instances to string
-        if "connection_file" in data.keys():
-            # When the item requires a Jupyter Console for execution, this is the message that the client uses
-            # to connect to the kernel manager running on server
-            # kernel_execution_msg: {'item_name': 'T2', 'filter_id': '', 'type': 'kernel_started',
-            #                        'connection_file': '/tmp/tmp_ve9ohel.json', 'kernel_name': 'python3'}
-            # We need to read the connection file to a JSON dictionary and insert that to data as a new key
-            with open(data["connection_file"], "r") as fh:
-                try:
-                    connection_file_dict = json.load(fh)
-                    data["connection_file_dict"] = connection_file_dict
-                except json.decoder.JSONDecodeError:
-                    print(f"Error loading connection file {data['connection_file']}. Invalid JSON.")
-        for key in data.keys():
-            # Print warning if there are any tuples used as keys in the data dictionary.
-            # Tuples are converted to lists by json.dumps(). Lists must be converted back to tuples
-            # on client side (in fix_event_data()).
-            if type(data[key]) == tuple:
-                print(f"[WARNING] Found tuple in message {event_type}: {data}. Fix this on client side.")
-    return data
-
-
-def fix_event_data(event):
-    """Does the opposite of break_event_data(). Converts values in data dictionary back to original.
-
-    Args:
-        event (tuple): (event_type, data). event_type is str, data is dict or str.
-
-    Returns:
-        tuple: Fixed event_type: data tuple
-    """
-    # Convert item_state str back to ItemExecutionFinishState. This was converted to str on server because
-    # it is not JSON serializable
-    if type(event[1]) == str:
-        return event
-    if "item_state" in event[1].keys():
-        event[1]["item_state"] = convert_execution_finish_state(event[1]["item_state"])
-    return event
-
-
-def convert_execution_finish_state(state):
-    """Transforms state string into an ItemExecutionFinishState enum.
-
-    Args:
-        state (str): State as string
-
-    Returns:
-        ItemExecutionFinishState: Enum if given str is valid, None otherwise.
-    """
-    states = dict()
-    states["SUCCESS"] = ItemExecutionFinishState.SUCCESS
-    states["FAILURE"] = ItemExecutionFinishState.FAILURE
-    states["SKIPPED"] = ItemExecutionFinishState.SKIPPED
-    states["EXCLUDED"] = ItemExecutionFinishState.EXCLUDED
-    states["STOPPED"] = ItemExecutionFinishState.STOPPED
-    states["NEVER_FINISHED"] = ItemExecutionFinishState.NEVER_FINISHED
-    return states.get(state, None)
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains static methods for converting event and data information to JSON format and back.
+"""
+
+import base64
+import json
+from spine_engine.spine_engine import ItemExecutionFinishState
+
+
+class EventDataConverter:
+    @staticmethod
+    def convert(event_type, data, b64encoding=False):
+        """Converts a single event_type, data pair into a JSON string.
+         Optionally, encodes data as base64.
+
+        Args:
+            event_type: (str): Event type (e.g. exec_started, dag_exec_finished, etc.)
+            data (dict or str): Data associated with the event_type
+            b64encoding (bool): True encodes data as base64, False does not
+
+        Returns:
+            str: JSON string
+        """
+        if b64encoding:
+            msg_b = str(data).encode("ascii")
+            base64_b = base64.b64encode(msg_b)
+            data = base64_b.decode("ascii")
+        data = break_event_data(event_type, data)
+        event_dict = {"event_type": event_type, "data": data}
+        json_event_data = json.dumps(event_dict)
+        return json_event_data
+
+    @staticmethod
+    def deconvert(event_data, b64decoding=False):
+        """Decodes a bytes object into a JSON string, then converts it to a
+        dictionary with a single event_type, data pair into a tuple
+        containing the same. Optionally, decodes data field from base64
+        back to ascii.
+
+        Args:
+            event_data (bytes): Event type and data as bytes
+            b64decoding (bool): Flag indicating, whether data is decoded from base64
+
+        Returns:
+            (tuple): Event type and data pair
+        """
+        event_dict = json.loads(event_data.decode("utf-8"))
+        if b64decoding:
+            base64_bytes = event_dict["data"].encode("ascii")
+            message_bytes = base64.b64decode(base64_bytes)
+            data = message_bytes.decode("ascii")
+        else:
+            data = event_dict["data"]
+        fixed_event = fix_event_data((event_dict["event_type"], data))
+        return fixed_event
+
+
+def break_event_data(event_type, data):
+    """Makes values in data dictionary suitable for converting them to JSON strings.
+
+    Args:
+        event_type (str): Event type
+        data (dict or str): Data
+
+    Returns:
+        dict or str: Edited data dictionary or data string as it was.
+    """
+    if type(data) != str:
+        if "item_state" in data.keys():
+            data["item_state"] = str(data["item_state"])  # Cast ItemExecutionFinishState instance to string
+        if "url" in data.keys():
+            data["url"] = str(data["url"])  # Cast URL instances to string
+        if "connection_file" in data.keys():
+            # When the item requires a Jupyter Console for execution, this is the message that the client uses
+            # to connect to the kernel manager running on server
+            # kernel_execution_msg: {'item_name': 'T2', 'filter_id': '', 'type': 'kernel_started',
+            #                        'connection_file': '/tmp/tmp_ve9ohel.json', 'kernel_name': 'python3'}
+            # We need to read the connection file to a JSON dictionary and insert that to data as a new key
+            with open(data["connection_file"], "r") as fh:
+                try:
+                    connection_file_dict = json.load(fh)
+                    data["connection_file_dict"] = connection_file_dict
+                except json.decoder.JSONDecodeError:
+                    print(f"Error loading connection file {data['connection_file']}. Invalid JSON.")
+        for key in data.keys():
+            # Print warning if there are any tuples used as keys in the data dictionary.
+            # Tuples are converted to lists by json.dumps(). Lists must be converted back to tuples
+            # on client side (in fix_event_data()).
+            if type(data[key]) == tuple:
+                print(f"[WARNING] Found tuple in message {event_type}: {data}. Fix this on client side.")
+    return data
+
+
+def fix_event_data(event):
+    """Does the opposite of break_event_data(). Converts values in data dictionary back to original.
+
+    Args:
+        event (tuple): (event_type, data). event_type is str, data is dict or str.
+
+    Returns:
+        tuple: Fixed event_type: data tuple
+    """
+    # Convert item_state str back to ItemExecutionFinishState. This was converted to str on server because
+    # it is not JSON serializable
+    if type(event[1]) == str:
+        return event
+    if "item_state" in event[1].keys():
+        event[1]["item_state"] = convert_execution_finish_state(event[1]["item_state"])
+    return event
+
+
+def convert_execution_finish_state(state):
+    """Transforms state string into an ItemExecutionFinishState enum.
+
+    Args:
+        state (str): State as string
+
+    Returns:
+        ItemExecutionFinishState: Enum if given str is valid, None otherwise.
+    """
+    states = dict()
+    states["SUCCESS"] = ItemExecutionFinishState.SUCCESS
+    states["FAILURE"] = ItemExecutionFinishState.FAILURE
+    states["SKIPPED"] = ItemExecutionFinishState.SKIPPED
+    states["EXCLUDED"] = ItemExecutionFinishState.EXCLUDED
+    states["STOPPED"] = ItemExecutionFinishState.STOPPED
+    states["NEVER_FINISHED"] = ItemExecutionFinishState.NEVER_FINISHED
+    return states.get(state, None)
```

### Comparing `spine_engine-0.23.3/spine_engine/server/util/server_message.py` & `spine_engine-0.23.4/spine_engine/server/util/server_message.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,129 +1,129 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains a helper class for JSON-based messages exchanged between server and clients.
-"""
-
-import json
-
-
-class ServerMessage:
-    """Class for communicating requests and replies between the client and the server."""
-
-    def __init__(self, command, req_id, data, files=None):
-        """
-        Supported requests and expected server responses
-        - Ping
-            request: ServerMessage("ping", id, "", None)
-            response: ServerMessage("ping", id, "", None)
-        - Prepare project execution
-            request: ServerMessage("prepare_execution", "1", <project_name>, [project_package.zip]) + file as bytes
-            response: ServerMessage("prepare_execution, job_id, "", None)
-        - Start DAG execution
-            request: ServerMessage("start_execution", job_id, engine_data, None)
-            response: ServerMessage("start_execution", job_id, ("remote_execution_started", publish_port), [])
-        - Retrieve finished project
-            request: ServerMessage("retrieve_project", job_id, "", [])
-            response ServerMessage("retrieve_project, job_id, "", ["project_package.zip"]) + file as bytes
-
-        Args:
-            command (str): Command to be executed at the server
-            req_id (str): Identifier associated with the command
-            data (str): Data associated to the command. In an execute request, this is the engine
-            data as a JSON string. In an execute reply, this is an event_type:data (str:str) tuple.
-            files (list[str], None): List of file names to be associated with the message (optional)
-        """
-        self._command = command
-        self._id = req_id
-        self._data = data
-        if not files:
-            self._files = list()
-        else:
-            self._files = files  # Name of the file where zip-file is saved to. Does not need to be the same as original
-
-    def getCommand(self):
-        return self._command
-
-    def getId(self):
-        return self._id
-
-    def getData(self):
-        return self._data
-
-    def getFileNames(self):
-        return self._files
-
-    def toJSON(self):
-        """Converts this instance into a JSON string.
-
-        Returns:
-            str: The instance as a JSON string
-        """
-        jsonFileNames = self._getJSONFileNames()
-        retStr = ""
-        retStr += "{\n"
-        retStr += "   \"command\": \"" + self._command + "\",\n"
-        retStr += "   \"id\":\"" + self._id + "\",\n"
-
-        if len(self._data) == 0:
-            retStr += "   \"data\":\"\",\n"
-        else:
-            retStr += "   \"data\":" + self._data + ",\n"
-        retStr += "   \"files\": " + jsonFileNames
-        retStr += "}"
-        return retStr
-
-    def _getJSONFileNames(self):
-        fileNameCount = len(self._files)
-        if fileNameCount == 0:
-            return "{}\n"
-        retStr = '{\n'
-        i = 0
-        for fName in self._files:
-            if i + 1 < fileNameCount:
-                retStr = retStr + "    \"name-" + str(i) + "\": \"" + fName + "\",\n"
-            else:
-                retStr = retStr + "    \"name-" + str(i) + "\": \"" + fName + "\"\n"
-            i += 1
-        retStr = retStr + "    }\n"
-        return retStr
-
-    def to_bytes(self):
-        """Converts this ServerMessage instance to a JSON and then to a bytes string.
-
-        Returns:
-            bytes: ServerMessage instance as a UTF-8 bytes JSON string.
-        """
-        as_json = self.toJSON()
-        return bytes(as_json, "utf-8")
-
-    @classmethod
-    def parse(cls, message):
-        """Makes a ServerMessage instance from a received bytes object.
-
-        Args:
-            message (bytes): JSON message as bytes
-
-        Returns:
-            ServerMessage: Parsed message
-        """
-        parsed_msg = json.loads(message.decode("utf-8"))  # Load JSON string into dictionary
-        filenames = parsed_msg["files"]  # dict
-        data = parsed_msg["data"]  # list
-        parsed_filenames = list()
-        if len(filenames) > 0:
-            for f in filenames:
-                parsed_filenames.append(filenames[f])
-            msg = cls(parsed_msg['command'], parsed_msg['id'], data, parsed_filenames)
-        else:
-            msg = cls(parsed_msg['command'], parsed_msg['id'], data, None)
-        return msg
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains a helper class for JSON-based messages exchanged between server and clients.
+"""
+
+import json
+
+
+class ServerMessage:
+    """Class for communicating requests and replies between the client and the server."""
+
+    def __init__(self, command, req_id, data, files=None):
+        """
+        Supported requests and expected server responses
+        - Ping
+            request: ServerMessage("ping", id, "", None)
+            response: ServerMessage("ping", id, "", None)
+        - Prepare project execution
+            request: ServerMessage("prepare_execution", "1", <project_name>, [project_package.zip]) + file as bytes
+            response: ServerMessage("prepare_execution, job_id, "", None)
+        - Start DAG execution
+            request: ServerMessage("start_execution", job_id, engine_data, None)
+            response: ServerMessage("start_execution", job_id, ("remote_execution_started", publish_port), [])
+        - Retrieve finished project
+            request: ServerMessage("retrieve_project", job_id, "", [])
+            response ServerMessage("retrieve_project, job_id, "", ["project_package.zip"]) + file as bytes
+
+        Args:
+            command (str): Command to be executed at the server
+            req_id (str): Identifier associated with the command
+            data (str): Data associated to the command. In an execute request, this is the engine
+            data as a JSON string. In an execute reply, this is an event_type:data (str:str) tuple.
+            files (list[str], None): List of file names to be associated with the message (optional)
+        """
+        self._command = command
+        self._id = req_id
+        self._data = data
+        if not files:
+            self._files = list()
+        else:
+            self._files = files  # Name of the file where zip-file is saved to. Does not need to be the same as original
+
+    def getCommand(self):
+        return self._command
+
+    def getId(self):
+        return self._id
+
+    def getData(self):
+        return self._data
+
+    def getFileNames(self):
+        return self._files
+
+    def toJSON(self):
+        """Converts this instance into a JSON string.
+
+        Returns:
+            str: The instance as a JSON string
+        """
+        jsonFileNames = self._getJSONFileNames()
+        retStr = ""
+        retStr += "{\n"
+        retStr += "   \"command\": \"" + self._command + "\",\n"
+        retStr += "   \"id\":\"" + self._id + "\",\n"
+
+        if len(self._data) == 0:
+            retStr += "   \"data\":\"\",\n"
+        else:
+            retStr += "   \"data\":" + self._data + ",\n"
+        retStr += "   \"files\": " + jsonFileNames
+        retStr += "}"
+        return retStr
+
+    def _getJSONFileNames(self):
+        fileNameCount = len(self._files)
+        if fileNameCount == 0:
+            return "{}\n"
+        retStr = '{\n'
+        i = 0
+        for fName in self._files:
+            if i + 1 < fileNameCount:
+                retStr = retStr + "    \"name-" + str(i) + "\": \"" + fName + "\",\n"
+            else:
+                retStr = retStr + "    \"name-" + str(i) + "\": \"" + fName + "\"\n"
+            i += 1
+        retStr = retStr + "    }\n"
+        return retStr
+
+    def to_bytes(self):
+        """Converts this ServerMessage instance to a JSON and then to a bytes string.
+
+        Returns:
+            bytes: ServerMessage instance as a UTF-8 bytes JSON string.
+        """
+        as_json = self.toJSON()
+        return bytes(as_json, "utf-8")
+
+    @classmethod
+    def parse(cls, message):
+        """Makes a ServerMessage instance from a received bytes object.
+
+        Args:
+            message (bytes): JSON message as bytes
+
+        Returns:
+            ServerMessage: Parsed message
+        """
+        parsed_msg = json.loads(message.decode("utf-8"))  # Load JSON string into dictionary
+        filenames = parsed_msg["files"]  # dict
+        data = parsed_msg["data"]  # list
+        parsed_filenames = list()
+        if len(filenames) > 0:
+            for f in filenames:
+                parsed_filenames.append(filenames[f])
+            msg = cls(parsed_msg['command'], parsed_msg['id'], data, parsed_filenames)
+        else:
+            msg = cls(parsed_msg['command'], parsed_msg['id'], data, None)
+        return msg
```

### Comparing `spine_engine-0.23.3/spine_engine/server/util/zip_handler.py` & `spine_engine-0.23.4/spine_engine/server/util/zip_handler.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,82 +1,82 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains static methods for handling ZIP files.
-"""
-
-import os
-import shutil
-from zipfile import ZipFile
-from spine_engine.utils.helpers import get_file_size
-
-
-class ZipHandler:
-    """ZIP file handler."""
-
-    @staticmethod
-    def package(src_folder, dst_folder, fname):
-        """Packages a directory into a ZIP-file.
-
-        NOTE: Do not use the src_folder as the dst_folder. If the ZIP file is
-        created to the same directory as root_dir, there will be a corrupted
-        project_package.zip file INSIDE the actual project_package.zip file, which
-        makes unzipping the file fail.
-
-        Args:
-            src_folder (str): Folder to be included to the ZIP file
-            dst_folder (str): Destination folder for the ZIP file
-            fname (str): Name of the ZIP-file without extension (it's added by shutil.make_archive())
-        """
-        zip_path = os.path.join(dst_folder, fname)
-        try:
-            shutil.make_archive(zip_path, "zip", src_folder)
-        except OSError:
-            raise
-
-    @staticmethod
-    def extract(zip_file, output_folder):
-        """Extracts the contents of a ZIP file to the provided folder.
-
-        Args:
-            zip_file (str): Absolute path to ZIP file to be extracted.
-            output_folder (str): Absolute path to destination directory
-        """
-        if not os.path.exists(zip_file):
-            raise ValueError(f"ZIP file '{zip_file}' does not exist")
-        file_size = os.path.getsize(zip_file)
-        if file_size < 100:
-            raise ValueError(f"'{zip_file}' possibly corrupted. File size too small [{get_file_size(file_size)}]")
-        with ZipFile(zip_file, "r") as zip_obj:
-            try:
-                first_bad_file = zip_obj.testzip()  # Test ZIP file integrity before extraction (debugging)
-                if not first_bad_file:
-                    zip_obj.extractall(output_folder)
-                else:
-                    print(f"'{zip_file}' integrity test failure. First bad file: {first_bad_file}")
-            except Exception as e:
-                raise e
-
-    @staticmethod
-    def delete_folder(folder):
-        """Removes the provided directory and all it's contents.
-
-        Args:
-            folder: Directory to be removed
-        """
-        if not folder:
-            raise ValueError("Invalid input. No folder given.")
-        if not os.path.isdir(folder):
-            raise ValueError(f"Given dir:{folder} does not exist.")
-        try:
-            shutil.rmtree(folder)
-        except OSError:
-            raise
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains static methods for handling ZIP files.
+"""
+
+import os
+import shutil
+from zipfile import ZipFile
+from spine_engine.utils.helpers import get_file_size
+
+
+class ZipHandler:
+    """ZIP file handler."""
+
+    @staticmethod
+    def package(src_folder, dst_folder, fname):
+        """Packages a directory into a ZIP-file.
+
+        NOTE: Do not use the src_folder as the dst_folder. If the ZIP file is
+        created to the same directory as root_dir, there will be a corrupted
+        project_package.zip file INSIDE the actual project_package.zip file, which
+        makes unzipping the file fail.
+
+        Args:
+            src_folder (str): Folder to be included to the ZIP file
+            dst_folder (str): Destination folder for the ZIP file
+            fname (str): Name of the ZIP-file without extension (it's added by shutil.make_archive())
+        """
+        zip_path = os.path.join(dst_folder, fname)
+        try:
+            shutil.make_archive(zip_path, "zip", src_folder)
+        except OSError:
+            raise
+
+    @staticmethod
+    def extract(zip_file, output_folder):
+        """Extracts the contents of a ZIP file to the provided folder.
+
+        Args:
+            zip_file (str): Absolute path to ZIP file to be extracted.
+            output_folder (str): Absolute path to destination directory
+        """
+        if not os.path.exists(zip_file):
+            raise ValueError(f"ZIP file '{zip_file}' does not exist")
+        file_size = os.path.getsize(zip_file)
+        if file_size < 100:
+            raise ValueError(f"'{zip_file}' possibly corrupted. File size too small [{get_file_size(file_size)}]")
+        with ZipFile(zip_file, "r") as zip_obj:
+            try:
+                first_bad_file = zip_obj.testzip()  # Test ZIP file integrity before extraction (debugging)
+                if not first_bad_file:
+                    zip_obj.extractall(output_folder)
+                else:
+                    print(f"'{zip_file}' integrity test failure. First bad file: {first_bad_file}")
+            except Exception as e:
+                raise e
+
+    @staticmethod
+    def delete_folder(folder):
+        """Removes the provided directory and all it's contents.
+
+        Args:
+            folder: Directory to be removed
+        """
+        if not folder:
+            raise ValueError("Invalid input. No folder given.")
+        if not os.path.isdir(folder):
+            raise ValueError(f"Given dir:{folder} does not exist.")
+        try:
+            shutil.rmtree(folder)
+        except OSError:
+            raise
```

### Comparing `spine_engine-0.23.3/spine_engine/shared_memory_io_manager.py` & `spine_engine-0.23.4/spine_engine/shared_memory_io_manager.py`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,36 +1,36 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-"""
-Contains Engine's IO manager.
-
-"""
-from dagster import IOManager, io_manager
-
-
-@io_manager
-def shared_memory_io_manager(init_context):
-    return SharedMemoryIOManager()
-
-
-class SharedMemoryIOManager(IOManager):
-    """An IO manager that stores values in shared storage."""
-
-    _shared_values = dict()
-
-    def handle_output(self, context, obj):
-        """Stores values in memory."""
-        keys = tuple(context.get_output_identifier())
-        self._shared_values[keys] = obj
-
-    def load_input(self, context):
-        """Loads value from memory."""
-        keys = tuple(context.upstream_output.get_output_identifier())
-        return self._shared_values[keys]
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+"""
+Contains Engine's IO manager.
+
+"""
+from dagster import IOManager, io_manager
+
+
+@io_manager
+def shared_memory_io_manager(init_context):
+    return SharedMemoryIOManager()
+
+
+class SharedMemoryIOManager(IOManager):
+    """An IO manager that stores values in shared storage."""
+
+    _shared_values = dict()
+
+    def handle_output(self, context, obj):
+        """Stores values in memory."""
+        keys = tuple(context.get_output_identifier())
+        self._shared_values[keys] = obj
+
+    def load_input(self, context):
+        """Loads value from memory."""
+        keys = tuple(context.upstream_output.get_output_identifier())
+        return self._shared_values[keys]
```

### Comparing `spine_engine-0.23.3/spine_engine/spine_engine.py` & `spine_engine-0.23.4/spine_engine/spine_engine.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,967 +1,967 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains the SpineEngine class for running Spine Toolbox DAGs.
-
-"""
-from enum import Enum, unique
-import os
-import threading
-import multiprocessing as mp
-from itertools import product
-import networkx as nx
-from dagster import (
-    PipelineDefinition,
-    SolidDefinition,
-    InputDefinition,
-    OutputDefinition,
-    DependencyDefinition,
-    ModeDefinition,
-    Output,
-    Failure,
-    DagsterEventType,
-    default_executors,
-    AssetMaterialization,
-    execute_pipeline_iterator,
-)
-from spinedb_api import append_filter_config, name_from_dict
-from spinedb_api.spine_db_server import db_server_manager
-from spinedb_api.filters.tools import filter_config
-from spinedb_api.filters.scenario_filter import scenario_name_from_dict
-from spinedb_api.filters.execution_filter import execution_filter_config
-from .exception import EngineInitFailed
-from .execution_managers.persistent_execution_manager import (
-    disable_persistent_process_creation,
-    enable_persistent_process_creation,
-)
-from .utils.helpers import (
-    AppSettings,
-    required_items_for_execution,
-    inverted,
-    create_timestamp,
-    make_dag,
-    ExecutionDirection as ED,
-    ItemExecutionFinishState,
-    dag_edges,
-    make_connections,
-)
-from .utils.execution_resources import one_shot_process_semaphore, persistent_process_semaphore
-from .utils.queue_logger import QueueLogger
-from .project_item_loader import ProjectItemLoader
-from .multithread_executor.executor import multithread_executor
-from .project_item.connection import Connection, Jump
-from .shared_memory_io_manager import shared_memory_io_manager
-
-
-@unique
-class SpineEngineState(Enum):
-    SLEEPING = 1
-    """Dare to wake it?"""
-    RUNNING = 2
-    USER_STOPPED = 3
-    FAILED = 4
-    COMPLETED = 5
-
-    def __str__(self):
-        return str(self.name)
-
-
-class _JumpPipelineDefinition(PipelineDefinition):
-    def __init__(self, *args, jumps=None, **kwargs):
-        super().__init__(*args, **kwargs)
-        self.jumps = jumps if jumps is not None else []
-
-
-class SpineEngine:
-    """
-    An engine for executing a Spine Toolbox DAG-workflow.
-    """
-
-    _resource_limit_lock = threading.Lock()
-
-    def __init__(
-        self,
-        items=None,
-        specifications=None,
-        connections=None,
-        jumps=None,
-        items_module_name="spine_items",
-        settings=None,
-        project_dir=None,
-        execution_permits=None,
-        debug=False,
-    ):
-        """
-        Args:
-            items (dict): A mapping from item name to item dict
-            specifications (dict(str,list(dict))): A mapping from item type to list of specification dicts.
-            connections (list of dict): List of connection dicts
-            jumps (list of dict, optional): List of jump dicts
-            items_module_name (str): name of the Python module that contains project items
-            settings (dict): Toolbox execution settings.
-            project_dir (str): Path to project directory.
-            execution_permits (dict(str,bool)): A mapping from item name to a boolean value, False indicating that
-                the item is not executed
-            debug (bool): Whether debug mode is active or not.
-
-        Raises:
-            EngineInitFailed: Raised if initialization fails
-        """
-        super().__init__()
-        self._queue = mp.Queue()
-        if items is None:
-            items = {}
-        self._items = items
-        if execution_permits is None:
-            execution_permits = {}
-        self._execution_permits = execution_permits
-        connections = list(map(Connection.from_dict, connections))  # Deserialize connections
-        project_item_loader = ProjectItemLoader()
-        self._executable_item_classes = project_item_loader.load_executable_item_classes(items_module_name)
-        required_items = required_items_for_execution(
-            self._items, connections, self._executable_item_classes, self._execution_permits
-        )
-        self._connections = make_connections(connections, required_items)
-        self._connections_by_source = dict()
-        self._connections_by_destination = dict()
-        self._validate_and_sort_connections()
-        edges = dag_edges(self._connections)  # Mapping of a source node (item) to a list of destination nodes (items)
-        self._check_write_index()
-        self._settings = AppSettings(settings if settings is not None else {})
-        _set_resource_limits(self._settings, SpineEngine._resource_limit_lock)
-        enable_persistent_process_creation()
-        self._project_dir = project_dir
-        if specifications is None:
-            specifications = {}
-        self._item_specifications = self._make_item_specifications(
-            specifications, project_item_loader, items_module_name
-        )
-        self._dag = make_dag(edges, self._execution_permits)
-        _validate_dag(self._dag)
-        self._dag_nodes = list(self._dag)  # Names of permitted items and their neighbors
-        if jumps is None:
-            jumps = []
-        self._jumps = list(map(Jump.from_dict, jumps))
-        validate_jumps(self._jumps, self._dag)
-        for x in self._connections + self._jumps:
-            x.make_logger(self._queue)
-        for x in self._jumps:
-            x.set_engine(self)
-        # Mapping of item name to solid name
-        self._solids_by_items = {item_name: str(i) for i, item_name in enumerate(self._dag_nodes)}
-        # Mapping of solid name to item name
-        self._items_by_solids = {solid_name: item_name for item_name, solid_name in self._solids_by_items.items()}
-        # Same as edges but item names are swapped to solid names
-        self._back_injectors = {
-            self._solids_by_items[key]: [self._solids_by_items[x] for x in value] for key, value in edges.items()
-        }
-        self._forth_injectors = inverted(self._back_injectors)
-        self._pipeline = self._make_pipeline()
-        self._state = SpineEngineState.SLEEPING
-        self._debug = debug
-        self._running_items = []
-        self._prompt_queues = {}
-        self._answered_prompts = {}
-        self.resources_per_item = {}  # Tuples of (forward resources, backward resources) from last execution
-        self._timestamp = create_timestamp()
-        self._db_server_manager_queue = None
-        self._thread = threading.Thread(target=self.run)
-        self._event_stream = self._get_event_stream()
-
-    def _descendants(self, name):
-        """Yields descendant item names.
-
-        Args:
-            name (str): name of the project item whose descendants to collect
-
-        Yields:
-            str: descendant name
-        """
-        for c in self._connections_by_source.get(name, ()):
-            yield c.destination
-            yield from self._descendants(c.destination)
-
-    def _check_write_index(self):
-        """Checks if write indexes are valid."""
-        conflicting_by_item = {}
-        for item_name in self._items:
-            conflicting = {}
-            descendants = self._descendants(item_name)
-            for conn in self._connections_by_source.get(item_name, ()):
-                sibling_connections = [
-                    x for x in self._connections_by_destination.get(conn.destination, []) if x != conn
-                ]
-                conflicting.update(
-                    {
-                        c.source: c.destination
-                        for c in sibling_connections
-                        if c.write_index < conn.write_index and c.source in descendants
-                    }
-                )
-            if conflicting:
-                conflicting_by_item[item_name] = conflicting
-        rows = []
-        for item_name, conflicting in conflicting_by_item.items():
-            row = []
-            for other_item_name, dest in conflicting.items():
-                row.append(f"{other_item_name}, but {other_item_name} is set to write ealier to {dest}")
-            if row:
-                rows.append(f"Item {item_name} cannot execute because it is a dependency to " + ", ".join(row))
-        msg = "\n".join(rows)
-        if msg:
-            raise EngineInitFailed(msg)
-
-    def _validate_and_sort_connections(self):
-        """Checks and sorts Connections by source and destination.
-
-        Raises:
-            EngineInitFailed: If connection is not ready
-        """
-        for connection in self._connections:
-            if not connection.ready_to_execute():
-                notifications = " ".join(connection.notifications())
-                raise EngineInitFailed(f"Link {connection.name} is not ready for execution. {notifications}")
-            source, destination = connection.source, connection.destination
-            self._connections_by_source.setdefault(source, list()).append(connection)
-            self._connections_by_destination.setdefault(destination, list()).append(connection)
-
-    def _make_item_specifications(self, specifications, project_item_loader, items_module_name):
-        """Instantiates item specifications.
-
-        Args:
-            specifications (dict): A mapping from item type to list of specification dicts.
-            project_item_loader (ProjectItemLoader): loader instance
-            items_module_name (str): name of the Python module that contains the project items
-
-        Returns:
-            dict: Mapping from item type to a dict that maps specification names to specification instances
-        """
-        specification_factories = project_item_loader.load_item_specification_factories(items_module_name)
-        item_specifications = {}
-        for item_type, spec_dicts in specifications.items():
-            factory = specification_factories.get(item_type)
-            if factory is None:
-                continue
-            item_specifications[item_type] = dict()
-            for spec_dict in spec_dicts:
-                spec = factory.make_specification(spec_dict, self._settings, None)
-                item_specifications[item_type][spec.name] = spec
-        return item_specifications
-
-    def make_item(self, item_name, direction):
-        """Recreates item from project item dictionary for a particular execution.
-        Note that this method is called multiple times for each item:
-        Once for the backward pipeline, and once for each filtered execution in the forward pipeline."""
-        item_dict = self._items[item_name]
-        prompt_queue = self._prompt_queues[item_name] = mp.Queue()
-        logger = QueueLogger(
-            self._queue, item_name, prompt_queue, self._answered_prompts, silent=direction is ED.BACKWARD
-        )
-        return self.do_make_item(item_name, item_dict, logger)
-
-    def do_make_item(self, item_name, item_dict, logger):
-        item_type = item_dict["type"]
-        executable_item_class = self._executable_item_classes[item_type]
-        return executable_item_class.from_dict(
-            item_dict, item_name, self._project_dir, self._settings, self._item_specifications, logger
-        )
-
-    def get_event(self):
-        """Returns the next event in the stream. Calling this after receiving the event of type "dag_exec_finished"
-        will raise StopIterationError."""
-        return next(self._event_stream)
-
-    def state(self):
-        """Returns Spine Engine state."""
-        return self._state
-
-    def _get_event_stream(self):
-        """Yields events (event_type, event_data).
-
-        TODO: Describe the events in depth.
-
-        Yields:
-            tuple: event type and data
-        """
-        self._thread.start()
-        while True:
-            msg = self._queue.get()
-            yield msg
-            if msg[0] == "dag_exec_finished":
-                break
-        self._thread.join()
-
-    def answer_prompt(self, item_name, accepted):
-        """Answers the prompt for the specified item, either accepting or rejecting it."""
-        self._prompt_queues[item_name].put(accepted)
-
-    def wait(self):
-        """Waits until engine execution has finished."""
-        if self._thread.is_alive():
-            self._thread.join()
-
-    def run(self):
-        """Starts db server manager the engine."""
-        with db_server_manager() as self._db_server_manager_queue:
-            self._do_run()
-
-    def _do_run(self):
-        """Runs this engine."""
-        self._state = SpineEngineState.RUNNING
-        run_config = {
-            "loggers": {"console": {"config": {"log_level": "CRITICAL"}}},
-            "execution": {"multithread": {"config": {}}},
-        }
-        for event in execute_pipeline_iterator(self._pipeline, run_config=run_config):
-            self._process_event(event)
-        if self._state == SpineEngineState.RUNNING:
-            self._state = SpineEngineState.COMPLETED
-        self._queue.put(("dag_exec_finished", str(self._state)))
-
-    def _process_event(self, event):
-        """Processes events from a pipeline.
-
-        Args:
-            event (DagsterEvent): an event
-        """
-        if event.event_type == DagsterEventType.STEP_START:
-            direction, _, solid_name = event.solid_name.partition("_")
-            item_name = self._items_by_solids[solid_name]
-            self._queue.put(('exec_started', {"item_name": item_name, "direction": direction}))
-        elif event.event_type == DagsterEventType.STEP_FAILURE and self._state != SpineEngineState.USER_STOPPED:
-            direction, _, solid_name = event.solid_name.partition("_")
-            item_name = self._items_by_solids[solid_name]
-            self._state = SpineEngineState.FAILED
-            self._queue.put(
-                (
-                    'exec_finished',
-                    {
-                        "item_name": item_name,
-                        "direction": direction,
-                        "state": str(self._state),
-                        "item_state": ItemExecutionFinishState.FAILURE,
-                    },
-                )
-            )
-            if self._debug:
-                error = event.event_specific_data.error
-                print("Traceback (most recent call last):")
-                print("".join(error.stack + [error.message]))
-                print("(reported by SpineEngine in debug mode)")
-        elif event.event_type == DagsterEventType.STEP_SUCCESS:
-            # Notify Toolbox here when BACKWARD execution has finished
-            direction, _, solid_name = event.solid_name.partition("_")
-            if direction != "BACKWARD":
-                return
-            item_name = self._items_by_solids[solid_name]
-            if not self._execution_permits[item_name]:
-                item_finish_state = ItemExecutionFinishState.EXCLUDED
-            else:
-                item_finish_state = ItemExecutionFinishState.SUCCESS
-            self._queue.put(
-                (
-                    'exec_finished',
-                    {
-                        "item_name": item_name,
-                        "direction": direction,
-                        "state": str(self._state),
-                        "item_state": item_finish_state,
-                    },
-                )
-            )
-        elif event.event_type == DagsterEventType.ASSET_MATERIALIZATION:
-            # Notify Toolbox here when FORWARD execution has finished
-            direction, _, solid_name = event.solid_name.partition("_")
-            if direction != "FORWARD":
-                return
-            item_name = self._items_by_solids[solid_name]
-            state_value = event.asset_key.path[0]
-            item_finish_state = ItemExecutionFinishState[state_value]
-            self._queue.put(
-                (
-                    'exec_finished',
-                    {
-                        "item_name": item_name,
-                        "direction": direction,
-                        "state": str(self._state),
-                        "item_state": item_finish_state,
-                    },
-                )
-            )
-
-    def stop(self):
-        """Stops the engine."""
-        self._state = SpineEngineState.USER_STOPPED
-        disable_persistent_process_creation()
-        for item in self._running_items:
-            self._stop_item(item)
-        self._queue.put(("dag_exec_finished", str(self._state)))
-
-    def _stop_item(self, item):
-        """Stops given project item."""
-        item.stop_execution()
-        self._queue.put(
-            (
-                'exec_finished',
-                {
-                    "item_name": item.name,
-                    "direction": str(ED.FORWARD),
-                    "state": str(self._state),
-                    "item_state": ItemExecutionFinishState.STOPPED,
-                },
-            )
-        )
-
-    def _make_pipeline(self):
-        """Returns a _JumpPipelineDefinition for executing this engine.
-
-        Returns:
-            _JumpPipelineDefinition
-        """
-        solid_defs = [
-            make_solid_def(item_name)
-            for item_name in self._dag_nodes
-            for make_solid_def in (self._make_forward_solid_def, self._make_backward_solid_def)
-        ]
-        dependencies = self._make_dependencies()
-        mode_defs = [
-            ModeDefinition(
-                executor_defs=default_executors + [multithread_executor],
-                resource_defs={"io_manager": shared_memory_io_manager},
-            )
-        ]
-        self._complete_jumps()
-        return _JumpPipelineDefinition(
-            name="pipeline", solid_defs=solid_defs, dependencies=dependencies, mode_defs=mode_defs, jumps=self._jumps
-        )
-
-    def _complete_jumps(self):
-        """Updates jumps with item and corresponding solid information."""
-        for jump in self._jumps:
-            src, dst = jump.source, jump.destination
-            jump.item_names = {dst, src}
-            for path in nx.all_simple_paths(self._dag, dst, src):
-                jump.item_names.update(path)
-            jump.solid_names = {f"{ED.FORWARD}_{self._solids_by_items[n]}" for n in jump.item_names}
-            jump.source_solid = f"{ED.FORWARD}_{self._solids_by_items[src]}"
-            jump.destination_solid = f"{ED.BACKWARD}_{self._solids_by_items[dst]}"
-
-    def _make_backward_solid_def(self, item_name):
-        """Returns a SolidDefinition for executing the given item in the backward sweep.
-
-        Args:
-            item_name (str): The project item that gets executed by the solid.
-
-        Returns:
-            SolidDefinition: solid's definition
-        """
-
-        def compute_fn(context, inputs):
-            if self.state() == SpineEngineState.USER_STOPPED:
-                context.log.error(f"compute_fn() FAILURE with item: {item_name} stopped by the user")
-                raise Failure()
-            context.log.info(f"Item Name: {item_name}")
-            item = self.make_item(item_name, ED.BACKWARD)
-            resources = item.output_resources(ED.BACKWARD)
-            for r in resources:
-                r.metadata["db_server_manager_queue"] = self._db_server_manager_queue
-            yield Output(value=resources, output_name=f"{ED.BACKWARD}_output")
-
-        input_defs = []
-        output_defs = [OutputDefinition(name=f"{ED.BACKWARD}_output")]
-        return SolidDefinition(
-            name=f"{ED.BACKWARD}_{self._solids_by_items[item_name]}",
-            input_defs=input_defs,
-            compute_fn=compute_fn,
-            output_defs=output_defs,
-        )
-
-    def _make_forward_solid_def(self, item_name):
-        """Returns a SolidDefinition for executing the given item.
-
-        Args:
-            item_name (str)
-
-        Returns:
-            SolidDefinition
-        """
-
-        def compute_fn(context, inputs):
-            if self.state() == SpineEngineState.USER_STOPPED:
-                context.log.error(f"compute_fn() FAILURE with item: {item_name} stopped by the user")
-                raise Failure()
-            context.log.info(f"Item Name: {item_name}")
-            for conn in self._connections_by_destination.get(item_name, []):
-                conn.visit_destination()
-            # Split inputs into forward and backward resources based on prefix
-            forward_resource_stacks = []
-            backward_resources = []
-            for name, values in inputs.items():
-                if name.startswith(f"{ED.FORWARD}"):
-                    forward_resource_stacks += values
-                elif name.startswith(f"{ED.BACKWARD}"):
-                    backward_resources += values
-            item_finish_state, output_resource_stacks = self._execute_item(
-                context, item_name, forward_resource_stacks, backward_resources
-            )
-            yield AssetMaterialization(asset_key=str(item_finish_state))
-            if output_resource_stacks:
-                yield Output(value=output_resource_stacks, output_name=f"{ED.FORWARD}_output")
-            for conn in self._connections_by_source.get(item_name, []):
-                conn.visit_source()
-
-        input_defs = [
-            InputDefinition(name=f"{ED.FORWARD}_input_from_{inj}")
-            for inj in self._forth_injectors.get(self._solids_by_items[item_name], [])
-        ] + [
-            InputDefinition(name=f"{ED.BACKWARD}_input_from_{inj}")
-            for inj in self._back_injectors.get(self._solids_by_items[item_name], [])
-        ]
-        output_defs = [OutputDefinition(name=f"{ED.FORWARD}_output")]
-        return SolidDefinition(
-            name=f"{ED.FORWARD}_{self._solids_by_items[item_name]}",
-            input_defs=input_defs,
-            compute_fn=compute_fn,
-            output_defs=output_defs,
-        )
-
-    def _execute_item(self, context, item_name, forward_resource_stacks, backward_resources):
-        """Executes the given item using the given forward resource stacks and backward resources.
-        Returns list of output resource stacks.
-
-        Called by ``_make_forward_solid_def.compute_fn``.
-
-        For each element yielded by ``_filtered_resources_iterator``, spawns a thread that runs
-        ``_execute_item_filtered``.
-
-        Args:
-            context
-            item_name (str)
-            forward_resource_stacks (list(tuple(ProjectItemResource))): resources coming from predecessor items -
-                one tuple of ProjectItemResource per item, where each element in the tuple corresponds to a filtered
-                execution of the item.
-            backward_resources (list(ProjectItemResource)): resources coming from successor items - just one
-                resource per item.
-
-        Returns:
-            ItemExecutionFinishState
-            list(tuple(ProjectItemResource))
-        """
-        item = self.make_item(item_name, ED.NONE)
-        if not item.ready_to_execute(self._settings):
-            if not self._execution_permits[item_name]:
-                return ItemExecutionFinishState.EXCLUDED, []
-            context.log.error(f"compute_fn() FAILURE with '{item_name}', not ready for forward execution")
-            return ItemExecutionFinishState.FAILURE, []
-        success = [ItemExecutionFinishState.NEVER_FINISHED]
-        output_resources_list = []
-        threads = []
-        resources_iterator = self._filtered_resources_iterator(
-            item_name, forward_resource_stacks, backward_resources, self._timestamp
-        )
-        with mp.Manager() as multiprocess_manager:
-            item_lock = multiprocess_manager.Lock()
-            for flt_fwd_resources, flt_bwd_resources, filter_id in resources_iterator:
-                self.resources_per_item[item_name] = (flt_fwd_resources, flt_bwd_resources)
-                item = self.make_item(item_name, ED.FORWARD)
-                item.filter_id = filter_id
-                thread = threading.Thread(
-                    target=self._execute_item_filtered,
-                    args=(item, flt_fwd_resources, flt_bwd_resources, output_resources_list, item_lock, success),
-                )
-                threads.append(thread)
-            for thread in threads:
-                thread.start()
-            for thread in threads:
-                thread.join()
-        if success[0] == ItemExecutionFinishState.FAILURE:
-            context.log.error(f"compute_fn() FAILURE with {item_name}, failed to execute")
-            raise Failure()
-        for resources in output_resources_list:
-            for connection in self._connections_by_source.get(item_name, []):
-                connection.receive_resources_from_source(resources)
-        return success[0], output_resources_list
-
-    def _execute_item_filtered(
-        self, item, filtered_forward_resources, filtered_backward_resources, output_resources_list, item_lock, success
-    ):
-        """Executes the given item using the given filtered resources. Target for threads in ``_execute_item``.
-
-        Args:
-            item (ExecutableItemBase)
-            filtered_forward_resources (list(ProjectItemResource))
-            filtered_backward_resources (list(ProjectItemResource))
-            output_resources_list (list(list(ProjectItemResource))): A list to append the output resources
-                generated by the item.
-            item_lock (mp.Lock): Shared lock for parallel executions.
-            success (list): A list of one element, to write the outcome of the execution.
-        """
-        self._running_items.append(item)
-        if self._execution_permits[item.name]:
-            item_finish_state = item.execute(filtered_forward_resources, filtered_backward_resources, item_lock)
-            item.finish_execution(item_finish_state)
-        else:
-            item.exclude_execution(filtered_forward_resources, filtered_backward_resources, item_lock)
-            item_finish_state = ItemExecutionFinishState.EXCLUDED
-        filter_stack = sum((r.metadata.get("filter_stack", ()) for r in filtered_forward_resources), ())
-        output_resources = item.output_resources(ED.FORWARD)
-        for resource in output_resources:
-            resource.metadata["filter_stack"] = filter_stack
-            resource.metadata["filter_id"] = item.filter_id
-            resource.metadata["db_server_manager_queue"] = self._db_server_manager_queue
-        output_resources_list.append(output_resources)
-        success[0] = item_finish_state  # FIXME: We need a Lock here
-        self._running_items.remove(item)
-
-    def _filtered_resources_iterator(self, item_name, forward_resource_stacks, backward_resources, timestamp):
-        """Yields tuples of (filtered forward resources, filtered backward resources, filter id).
-
-        Each tuple corresponds to a unique filter combination. Combinations are obtained by applying the cross-product
-        over forward resource stacks.
-
-        Args:
-            item_name (str)
-            forward_resource_stacks (list(tuple(ProjectItemResource))): resources coming from predecessor items -
-                one tuple of ProjectItemResource per item, where each element in the tuple corresponds to a filtered
-                execution of the item.
-            backward_resources (list(ProjectItemResource)): resources coming from successor items - just one
-                resource per item.
-            timestamp (str): timestamp for the execution filter
-
-        Yields:
-            tuple(list,list,str): forward resources, backward resources, filter id
-        """
-
-        def check_resource_affinity(filtered_forward_resources):
-            filter_ids_by_provider = dict()
-            for r in filtered_forward_resources:
-                filter_ids_by_provider.setdefault(r.provider_name, set()).add(r.metadata.get("filter_id"))
-            return all(len(filter_ids) == 1 for filter_ids in filter_ids_by_provider.values())
-
-        resource_filter_stacks = dict()
-        unfiltered_resource_lists = dict()
-        for stack in forward_resource_stacks:
-            if not stack:
-                continue
-            unfiltered = list()
-            for resource in stack:
-                filter_stacks = self._filter_stacks(item_name, resource.provider_name, resource.label)
-                if not filter_stacks:
-                    unfiltered.append(resource)
-                else:
-                    resource_filter_stacks[resource] = filter_stacks
-            if unfiltered:
-                unfiltered_resource_lists.setdefault(stack[0].provider_name, list()).append(unfiltered)
-        forward_resource_stacks_iterator = (
-            self._expand_resource_stack(resource, filter_stacks)
-            for resource, filter_stacks in resource_filter_stacks.items()
-        )
-        backward_resources = self._convert_backward_resources(item_name, backward_resources)
-        for resources_or_lists in product(*unfiltered_resource_lists.values(), *forward_resource_stacks_iterator):
-            filtered_forward_resources = list()
-            for item in resources_or_lists:
-                if isinstance(item, list):
-                    filtered_forward_resources += item
-                else:
-                    filtered_forward_resources.append(item)
-            if not check_resource_affinity(filtered_forward_resources):
-                continue
-            filtered_forward_resources = self._convert_forward_resources(item_name, filtered_forward_resources)
-            resource_filter_stack = {r: r.metadata.get("filter_stack", ()) for r in filtered_forward_resources}
-            scenarios = {scenario_name_from_dict(cfg) for stack in resource_filter_stack.values() for cfg in stack}
-            scenarios.discard(None)
-            execution = {"execution_item": item_name, "scenarios": list(scenarios), "timestamp": timestamp}
-            config = execution_filter_config(execution)
-            filtered_backward_resources = []
-            for resource in backward_resources:
-                if "part_count" in resource.metadata:
-                    resource.metadata["part_count"] += 1
-                clone = resource.clone(additional_metadata={"filter_stack": (config,)})
-                clone.url = append_filter_config(clone.url, config)
-                filtered_backward_resources.append(clone)
-            filter_id = _make_filter_id(resource_filter_stack)
-            yield list(filtered_forward_resources), filtered_backward_resources, filter_id
-
-    @staticmethod
-    def _expand_resource_stack(resource, filter_stacks):
-        """Expands a resource according to filters defined for that resource.
-
-        Returns an expanded stack of as many resources as filter stacks defined for the resource.
-        Each resource in the expanded stack is a clone of the original, with one of the filter stacks
-        applied to the URL.
-
-        Args:
-            resource (ProjectItemResource): resource to expand
-            filter_stacks (list): resource's filter stacks
-
-        Returns:
-            tuple(ProjectItemResource): expanded resources
-        """
-        expanded_stack = ()
-        for filter_stack in filter_stacks:
-            filtered_clone = resource.clone(additional_metadata={"filter_stack": filter_stack})
-            for config in filter_stack:
-                filtered_clone.url = append_filter_config(filtered_clone.url, config)
-            expanded_stack += (filtered_clone,)
-        return expanded_stack
-
-    def _filter_stacks(self, item_name, provider_name, resource_label):
-        """Computes filter stacks.
-
-        Stacks are computed as the cross-product of all individual filters defined for a resource.
-
-        Args:
-            item_name (str): item's name
-            provider_name (str): resource provider's name
-            resource_label (str): resource's label
-
-        Returns:
-            list of list: filter stacks
-        """
-        connections = self._connections_by_destination.get(item_name, [])
-        connection = next(iter(c for c in connections if c.source == provider_name), None)
-        if connection is None:
-            raise RuntimeError("Logic error: no connection from resource provider")
-        filters = connection.enabled_filters(resource_label)
-        if filters is None:
-            return []
-        filter_configs_list = []
-        for filter_type, names in filters.items():
-            filter_configs = [filter_config(filter_type, name) for name in names]
-            if not filter_configs:
-                continue
-            filter_configs_list.append(filter_configs)
-        return list(product(*filter_configs_list))
-
-    def _convert_backward_resources(self, item_name, resources):
-        """Converts resources as they're being passed backwards to given item.
-        The conversion is dictated by the connection the resources traverse in order to reach the item.
-
-        Args:
-            item_name (str): receiving item's name
-            resources (Iterable of ProjectItemResource): resources to convert
-
-        Returns:
-            list of ProjectItemResource: converted resources
-        """
-        connections = self._connections_by_source.get(item_name, [])
-        resources_by_provider = {}
-        for r in resources:
-            resources_by_provider.setdefault(r.provider_name, list()).append(r)
-        for c in connections:
-            resources_from_destination = resources_by_provider.get(c.destination)
-            if resources_from_destination is None:
-                continue
-            if self._execution_permits[item_name]:
-                c.clean_up_backward_resources(resources_from_destination)
-            sibling_connections = [x for x in self._connections_by_destination.get(c.destination, []) if x != c]
-            resources_by_provider[c.destination] = c.convert_backward_resources(
-                resources_from_destination, sibling_connections
-            )
-        return [r for resources in resources_by_provider.values() for r in resources]
-
-    def _convert_forward_resources(self, item_name, resources):
-        """Converts resources as they're being passed forwards to given item.
-        The conversion is dictated by the connection the resources traverse in order to reach the item.
-
-        Args:
-            item_name (str): receiving item's name
-            resources (Iterable of ProjectItemResource): resources to convert
-
-        Returns:
-            list of ProjectItemResource: converted resources
-        """
-        connections = self._connections_by_destination.get(item_name, [])
-        resources_by_provider = {}
-        for r in resources:
-            resources_by_provider.setdefault(r.provider_name, list()).append(r)
-        for c in connections:
-            resources_from_source = resources_by_provider.get(c.source)
-            if resources_from_source is None:
-                continue
-            resources_by_provider[c.source] = c.convert_forward_resources(resources_from_source)
-        return [r for resources in resources_by_provider.values() for r in resources]
-
-    def _make_dependencies(self):
-        """
-        Returns a dictionary of dependencies according to the given dictionaries of injectors.
-
-        Returns:
-            dict: a dictionary to pass to the PipelineDefinition constructor as dependencies
-        """
-        forward_deps = {
-            f"{ED.FORWARD}_{n}": {
-                f"{ED.FORWARD}_input_from_{inj}": DependencyDefinition(f"{ED.FORWARD}_{inj}", f"{ED.FORWARD}_output")
-                for inj in injs
-            }
-            for n, injs in self._forth_injectors.items()
-        }
-        backward_deps = {
-            f"{ED.FORWARD}_{n}": {
-                f"{ED.BACKWARD}_input_from_{inj}": DependencyDefinition(f"{ED.BACKWARD}_{inj}", f"{ED.BACKWARD}_output")
-                for inj in injs
-            }
-            for n, injs in self._back_injectors.items()
-        }
-        deps = {}
-        for n in forward_deps.keys() | backward_deps.keys():
-            deps[n] = forward_deps.get(n, {})
-            deps[n].update(backward_deps.get(n, {}))
-        return deps
-
-
-def _make_filter_id(resource_filter_stack):
-    """Builds filter id from resource filter stack.
-
-    Args:
-        resource_filter_stack (dict): mapping from resource to filter stack
-
-    Returns:
-        str: filter id
-    """
-    provider_filters = set()
-    for resource, stack in resource_filter_stack.items():
-        if resource.type_ != "database":
-            filter_id = resource.metadata.get("filter_id")
-            if filter_id is None:
-                continue
-            provider_filters.add(filter_id)
-        else:
-            filter_names = sorted(_filter_names_from_stack(stack))
-            if not filter_names:
-                continue
-            provider_filters.add(", ".join(filter_names) + " - " + resource.provider_name)
-    return " & ".join(sorted(provider_filters))
-
-
-def _filter_names_from_stack(stack):
-    """Yields filter names from filter stack.
-
-    Args:
-        stack (Iterable of dict): filter stack
-
-    Yields:
-        str: filter name
-    """
-    for config in stack:
-        if not config:
-            continue
-        filter_name = name_from_dict(config)
-        if filter_name is not None:
-            yield filter_name
-
-
-def _validate_dag(dag):
-    """Raises an exception in case DAG is not valid.
-
-    Args:
-        dag (networkx.DiGraph): mapping from node name to list of direct successor nodes
-    """
-    if not nx.is_directed_acyclic_graph(dag):
-        raise EngineInitFailed("Invalid DAG")
-    if len(list(nx.weakly_connected_components(dag))) > 1:
-        raise EngineInitFailed("DAG contains unconnected items.")
-
-
-def validate_jumps(jumps, dag):
-    """Raises an exception in case jumps are not valid.
-
-    Args:
-        jumps (list of Jump): jumps
-        dag (DiGraph): jumps' DAG
-    """
-    items_by_jump = _get_items_by_jump(jumps, dag)
-    for jump in jumps:
-        validate_single_jump(jump, jumps, dag, items_by_jump)
-
-
-def validate_single_jump(jump, jumps, dag, items_by_jump=None):
-    """Raises an exception in case one jump is not valid.
-
-    Args:
-        jump (Jump): the jump to check
-        jumps (list of Jump): all jumps in dag
-        dag (DiGraph): jumps' DAG
-        items_by_jump (dict, optional): mapping jumps to a set of items in between destination and source
-    """
-    if not jump.ready_to_execute():
-        raise EngineInitFailed(f"Jump {jump.name} is not ready for execution.")
-    if items_by_jump is None:
-        items_by_jump = _get_items_by_jump(jumps, dag)
-    for other in jumps:
-        if other is jump:
-            continue
-        if other.source == jump.source:
-            raise EngineInitFailed(f"{jump.name} cannot have the same source as {other.name}.")
-        jump_items = items_by_jump[jump]
-        other_items = items_by_jump[other]
-        intersection = jump_items & other_items
-        if intersection not in (set(), jump_items, other_items):
-            raise EngineInitFailed(f"{jump.name} cannot partially overlap {other.name}.")
-    if not dag.has_node(jump.destination):
-        raise EngineInitFailed(f"Loop destination '{jump.destination}' not found in DAG")
-    if not dag.has_node(jump.source):
-        raise EngineInitFailed(f"Loop source '{jump.source}' not found in DAG")
-    if jump.source == jump.destination:
-        return
-    if nx.has_path(dag, jump.source, jump.destination):
-        raise EngineInitFailed("Cannot loop in forward direction.")
-    if not nx.has_path(nx.reverse_view(dag), jump.source, jump.destination):
-        raise EngineInitFailed("Cannot loop between DAG branches.")
-
-
-def _get_items_by_jump(jumps, dag):
-    """Returns a dict mapping jumps to a set of items between destination and source.
-
-    Args:
-        jumps (list of Jump): all jumps in dag
-        dag (DiGraph): jumps' DAG
-
-    Returns:
-        dict
-    """
-    items_by_jump = {}
-    for jump in jumps:
-        try:
-            items_by_jump[jump] = {
-                item for path in nx.all_simple_paths(dag, jump.destination, jump.source) for item in path
-            }
-        except nx.NodeNotFound:
-            items_by_jump[jump] = set()
-    return items_by_jump
-
-
-def _set_resource_limits(settings, lock):
-    """Sets limits for simultaneous single-shot and persistent processes.
-
-    May potentially kill existing persistent processes.
-
-    Args:
-        settings (AppSettings): Engine settings
-    """
-    with lock:
-        process_limiter = settings.value("engineSettings/processLimiter", "auto")
-        if process_limiter == "unlimited":
-            limit = "unlimited"
-        elif process_limiter == "auto":
-            limit = os.cpu_count()
-        else:
-            limit = int(settings.value("engineSettings/maxProcesses", os.cpu_count()))
-        one_shot_process_semaphore.set_limit(limit)
-        persistent_limiter = settings.value("engineSettings/persistentLimiter", "unlimited")
-        if persistent_limiter == "unlimited":
-            limit = "unlimited"
-        elif persistent_limiter == "auto":
-            limit = os.cpu_count()
-        else:
-            limit = int(settings.value("engineSettings/maxPersistentProcesses", os.cpu_count()))
-        persistent_process_semaphore.set_limit(limit)
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains the SpineEngine class for running Spine Toolbox DAGs.
+
+"""
+from enum import Enum, unique
+import os
+import threading
+import multiprocessing as mp
+from itertools import product
+import networkx as nx
+from dagster import (
+    PipelineDefinition,
+    SolidDefinition,
+    InputDefinition,
+    OutputDefinition,
+    DependencyDefinition,
+    ModeDefinition,
+    Output,
+    Failure,
+    DagsterEventType,
+    default_executors,
+    AssetMaterialization,
+    execute_pipeline_iterator,
+)
+from spinedb_api import append_filter_config, name_from_dict
+from spinedb_api.spine_db_server import db_server_manager
+from spinedb_api.filters.tools import filter_config
+from spinedb_api.filters.scenario_filter import scenario_name_from_dict
+from spinedb_api.filters.execution_filter import execution_filter_config
+from .exception import EngineInitFailed
+from .execution_managers.persistent_execution_manager import (
+    disable_persistent_process_creation,
+    enable_persistent_process_creation,
+)
+from .utils.helpers import (
+    AppSettings,
+    required_items_for_execution,
+    inverted,
+    create_timestamp,
+    make_dag,
+    ExecutionDirection as ED,
+    ItemExecutionFinishState,
+    dag_edges,
+    make_connections,
+)
+from .utils.execution_resources import one_shot_process_semaphore, persistent_process_semaphore
+from .utils.queue_logger import QueueLogger
+from .project_item_loader import ProjectItemLoader
+from .multithread_executor.executor import multithread_executor
+from .project_item.connection import Connection, Jump
+from .shared_memory_io_manager import shared_memory_io_manager
+
+
+@unique
+class SpineEngineState(Enum):
+    SLEEPING = 1
+    """Dare to wake it?"""
+    RUNNING = 2
+    USER_STOPPED = 3
+    FAILED = 4
+    COMPLETED = 5
+
+    def __str__(self):
+        return str(self.name)
+
+
+class _JumpPipelineDefinition(PipelineDefinition):
+    def __init__(self, *args, jumps=None, **kwargs):
+        super().__init__(*args, **kwargs)
+        self.jumps = jumps if jumps is not None else []
+
+
+class SpineEngine:
+    """
+    An engine for executing a Spine Toolbox DAG-workflow.
+    """
+
+    _resource_limit_lock = threading.Lock()
+
+    def __init__(
+        self,
+        items=None,
+        specifications=None,
+        connections=None,
+        jumps=None,
+        items_module_name="spine_items",
+        settings=None,
+        project_dir=None,
+        execution_permits=None,
+        debug=False,
+    ):
+        """
+        Args:
+            items (dict): A mapping from item name to item dict
+            specifications (dict(str,list(dict))): A mapping from item type to list of specification dicts.
+            connections (list of dict): List of connection dicts
+            jumps (list of dict, optional): List of jump dicts
+            items_module_name (str): name of the Python module that contains project items
+            settings (dict): Toolbox execution settings.
+            project_dir (str): Path to project directory.
+            execution_permits (dict(str,bool)): A mapping from item name to a boolean value, False indicating that
+                the item is not executed
+            debug (bool): Whether debug mode is active or not.
+
+        Raises:
+            EngineInitFailed: Raised if initialization fails
+        """
+        super().__init__()
+        self._queue = mp.Queue()
+        if items is None:
+            items = {}
+        self._items = items
+        if execution_permits is None:
+            execution_permits = {}
+        self._execution_permits = execution_permits
+        connections = list(map(Connection.from_dict, connections))  # Deserialize connections
+        project_item_loader = ProjectItemLoader()
+        self._executable_item_classes = project_item_loader.load_executable_item_classes(items_module_name)
+        required_items = required_items_for_execution(
+            self._items, connections, self._executable_item_classes, self._execution_permits
+        )
+        self._connections = make_connections(connections, required_items)
+        self._connections_by_source = dict()
+        self._connections_by_destination = dict()
+        self._validate_and_sort_connections()
+        edges = dag_edges(self._connections)  # Mapping of a source node (item) to a list of destination nodes (items)
+        self._check_write_index()
+        self._settings = AppSettings(settings if settings is not None else {})
+        _set_resource_limits(self._settings, SpineEngine._resource_limit_lock)
+        enable_persistent_process_creation()
+        self._project_dir = project_dir
+        if specifications is None:
+            specifications = {}
+        self._item_specifications = self._make_item_specifications(
+            specifications, project_item_loader, items_module_name
+        )
+        self._dag = make_dag(edges, self._execution_permits)
+        _validate_dag(self._dag)
+        self._dag_nodes = list(self._dag)  # Names of permitted items and their neighbors
+        if jumps is None:
+            jumps = []
+        self._jumps = list(map(Jump.from_dict, jumps))
+        validate_jumps(self._jumps, self._dag)
+        for x in self._connections + self._jumps:
+            x.make_logger(self._queue)
+        for x in self._jumps:
+            x.set_engine(self)
+        # Mapping of item name to solid name
+        self._solids_by_items = {item_name: str(i) for i, item_name in enumerate(self._dag_nodes)}
+        # Mapping of solid name to item name
+        self._items_by_solids = {solid_name: item_name for item_name, solid_name in self._solids_by_items.items()}
+        # Same as edges but item names are swapped to solid names
+        self._back_injectors = {
+            self._solids_by_items[key]: [self._solids_by_items[x] for x in value] for key, value in edges.items()
+        }
+        self._forth_injectors = inverted(self._back_injectors)
+        self._pipeline = self._make_pipeline()
+        self._state = SpineEngineState.SLEEPING
+        self._debug = debug
+        self._running_items = []
+        self._prompt_queues = {}
+        self._answered_prompts = {}
+        self.resources_per_item = {}  # Tuples of (forward resources, backward resources) from last execution
+        self._timestamp = create_timestamp()
+        self._db_server_manager_queue = None
+        self._thread = threading.Thread(target=self.run)
+        self._event_stream = self._get_event_stream()
+
+    def _descendants(self, name):
+        """Yields descendant item names.
+
+        Args:
+            name (str): name of the project item whose descendants to collect
+
+        Yields:
+            str: descendant name
+        """
+        for c in self._connections_by_source.get(name, ()):
+            yield c.destination
+            yield from self._descendants(c.destination)
+
+    def _check_write_index(self):
+        """Checks if write indexes are valid."""
+        conflicting_by_item = {}
+        for item_name in self._items:
+            conflicting = {}
+            descendants = self._descendants(item_name)
+            for conn in self._connections_by_source.get(item_name, ()):
+                sibling_connections = [
+                    x for x in self._connections_by_destination.get(conn.destination, []) if x != conn
+                ]
+                conflicting.update(
+                    {
+                        c.source: c.destination
+                        for c in sibling_connections
+                        if c.write_index < conn.write_index and c.source in descendants
+                    }
+                )
+            if conflicting:
+                conflicting_by_item[item_name] = conflicting
+        rows = []
+        for item_name, conflicting in conflicting_by_item.items():
+            row = []
+            for other_item_name, dest in conflicting.items():
+                row.append(f"{other_item_name}, but {other_item_name} is set to write ealier to {dest}")
+            if row:
+                rows.append(f"Item {item_name} cannot execute because it is a dependency to " + ", ".join(row))
+        msg = "\n".join(rows)
+        if msg:
+            raise EngineInitFailed(msg)
+
+    def _validate_and_sort_connections(self):
+        """Checks and sorts Connections by source and destination.
+
+        Raises:
+            EngineInitFailed: If connection is not ready
+        """
+        for connection in self._connections:
+            if not connection.ready_to_execute():
+                notifications = " ".join(connection.notifications())
+                raise EngineInitFailed(f"Link {connection.name} is not ready for execution. {notifications}")
+            source, destination = connection.source, connection.destination
+            self._connections_by_source.setdefault(source, list()).append(connection)
+            self._connections_by_destination.setdefault(destination, list()).append(connection)
+
+    def _make_item_specifications(self, specifications, project_item_loader, items_module_name):
+        """Instantiates item specifications.
+
+        Args:
+            specifications (dict): A mapping from item type to list of specification dicts.
+            project_item_loader (ProjectItemLoader): loader instance
+            items_module_name (str): name of the Python module that contains the project items
+
+        Returns:
+            dict: Mapping from item type to a dict that maps specification names to specification instances
+        """
+        specification_factories = project_item_loader.load_item_specification_factories(items_module_name)
+        item_specifications = {}
+        for item_type, spec_dicts in specifications.items():
+            factory = specification_factories.get(item_type)
+            if factory is None:
+                continue
+            item_specifications[item_type] = dict()
+            for spec_dict in spec_dicts:
+                spec = factory.make_specification(spec_dict, self._settings, None)
+                item_specifications[item_type][spec.name] = spec
+        return item_specifications
+
+    def make_item(self, item_name, direction):
+        """Recreates item from project item dictionary for a particular execution.
+        Note that this method is called multiple times for each item:
+        Once for the backward pipeline, and once for each filtered execution in the forward pipeline."""
+        item_dict = self._items[item_name]
+        prompt_queue = self._prompt_queues[item_name] = mp.Queue()
+        logger = QueueLogger(
+            self._queue, item_name, prompt_queue, self._answered_prompts, silent=direction is ED.BACKWARD
+        )
+        return self.do_make_item(item_name, item_dict, logger)
+
+    def do_make_item(self, item_name, item_dict, logger):
+        item_type = item_dict["type"]
+        executable_item_class = self._executable_item_classes[item_type]
+        return executable_item_class.from_dict(
+            item_dict, item_name, self._project_dir, self._settings, self._item_specifications, logger
+        )
+
+    def get_event(self):
+        """Returns the next event in the stream. Calling this after receiving the event of type "dag_exec_finished"
+        will raise StopIterationError."""
+        return next(self._event_stream)
+
+    def state(self):
+        """Returns Spine Engine state."""
+        return self._state
+
+    def _get_event_stream(self):
+        """Yields events (event_type, event_data).
+
+        TODO: Describe the events in depth.
+
+        Yields:
+            tuple: event type and data
+        """
+        self._thread.start()
+        while True:
+            msg = self._queue.get()
+            yield msg
+            if msg[0] == "dag_exec_finished":
+                break
+        self._thread.join()
+
+    def answer_prompt(self, item_name, accepted):
+        """Answers the prompt for the specified item, either accepting or rejecting it."""
+        self._prompt_queues[item_name].put(accepted)
+
+    def wait(self):
+        """Waits until engine execution has finished."""
+        if self._thread.is_alive():
+            self._thread.join()
+
+    def run(self):
+        """Starts db server manager the engine."""
+        with db_server_manager() as self._db_server_manager_queue:
+            self._do_run()
+
+    def _do_run(self):
+        """Runs this engine."""
+        self._state = SpineEngineState.RUNNING
+        run_config = {
+            "loggers": {"console": {"config": {"log_level": "CRITICAL"}}},
+            "execution": {"multithread": {"config": {}}},
+        }
+        for event in execute_pipeline_iterator(self._pipeline, run_config=run_config):
+            self._process_event(event)
+        if self._state == SpineEngineState.RUNNING:
+            self._state = SpineEngineState.COMPLETED
+        self._queue.put(("dag_exec_finished", str(self._state)))
+
+    def _process_event(self, event):
+        """Processes events from a pipeline.
+
+        Args:
+            event (DagsterEvent): an event
+        """
+        if event.event_type == DagsterEventType.STEP_START:
+            direction, _, solid_name = event.solid_name.partition("_")
+            item_name = self._items_by_solids[solid_name]
+            self._queue.put(('exec_started', {"item_name": item_name, "direction": direction}))
+        elif event.event_type == DagsterEventType.STEP_FAILURE and self._state != SpineEngineState.USER_STOPPED:
+            direction, _, solid_name = event.solid_name.partition("_")
+            item_name = self._items_by_solids[solid_name]
+            self._state = SpineEngineState.FAILED
+            self._queue.put(
+                (
+                    'exec_finished',
+                    {
+                        "item_name": item_name,
+                        "direction": direction,
+                        "state": str(self._state),
+                        "item_state": ItemExecutionFinishState.FAILURE,
+                    },
+                )
+            )
+            if self._debug:
+                error = event.event_specific_data.error
+                print("Traceback (most recent call last):")
+                print("".join(error.stack + [error.message]))
+                print("(reported by SpineEngine in debug mode)")
+        elif event.event_type == DagsterEventType.STEP_SUCCESS:
+            # Notify Toolbox here when BACKWARD execution has finished
+            direction, _, solid_name = event.solid_name.partition("_")
+            if direction != "BACKWARD":
+                return
+            item_name = self._items_by_solids[solid_name]
+            if not self._execution_permits[item_name]:
+                item_finish_state = ItemExecutionFinishState.EXCLUDED
+            else:
+                item_finish_state = ItemExecutionFinishState.SUCCESS
+            self._queue.put(
+                (
+                    'exec_finished',
+                    {
+                        "item_name": item_name,
+                        "direction": direction,
+                        "state": str(self._state),
+                        "item_state": item_finish_state,
+                    },
+                )
+            )
+        elif event.event_type == DagsterEventType.ASSET_MATERIALIZATION:
+            # Notify Toolbox here when FORWARD execution has finished
+            direction, _, solid_name = event.solid_name.partition("_")
+            if direction != "FORWARD":
+                return
+            item_name = self._items_by_solids[solid_name]
+            state_value = event.asset_key.path[0]
+            item_finish_state = ItemExecutionFinishState[state_value]
+            self._queue.put(
+                (
+                    'exec_finished',
+                    {
+                        "item_name": item_name,
+                        "direction": direction,
+                        "state": str(self._state),
+                        "item_state": item_finish_state,
+                    },
+                )
+            )
+
+    def stop(self):
+        """Stops the engine."""
+        self._state = SpineEngineState.USER_STOPPED
+        disable_persistent_process_creation()
+        for item in self._running_items:
+            self._stop_item(item)
+        self._queue.put(("dag_exec_finished", str(self._state)))
+
+    def _stop_item(self, item):
+        """Stops given project item."""
+        item.stop_execution()
+        self._queue.put(
+            (
+                'exec_finished',
+                {
+                    "item_name": item.name,
+                    "direction": str(ED.FORWARD),
+                    "state": str(self._state),
+                    "item_state": ItemExecutionFinishState.STOPPED,
+                },
+            )
+        )
+
+    def _make_pipeline(self):
+        """Returns a _JumpPipelineDefinition for executing this engine.
+
+        Returns:
+            _JumpPipelineDefinition
+        """
+        solid_defs = [
+            make_solid_def(item_name)
+            for item_name in self._dag_nodes
+            for make_solid_def in (self._make_forward_solid_def, self._make_backward_solid_def)
+        ]
+        dependencies = self._make_dependencies()
+        mode_defs = [
+            ModeDefinition(
+                executor_defs=default_executors + [multithread_executor],
+                resource_defs={"io_manager": shared_memory_io_manager},
+            )
+        ]
+        self._complete_jumps()
+        return _JumpPipelineDefinition(
+            name="pipeline", solid_defs=solid_defs, dependencies=dependencies, mode_defs=mode_defs, jumps=self._jumps
+        )
+
+    def _complete_jumps(self):
+        """Updates jumps with item and corresponding solid information."""
+        for jump in self._jumps:
+            src, dst = jump.source, jump.destination
+            jump.item_names = {dst, src}
+            for path in nx.all_simple_paths(self._dag, dst, src):
+                jump.item_names.update(path)
+            jump.solid_names = {f"{ED.FORWARD}_{self._solids_by_items[n]}" for n in jump.item_names}
+            jump.source_solid = f"{ED.FORWARD}_{self._solids_by_items[src]}"
+            jump.destination_solid = f"{ED.BACKWARD}_{self._solids_by_items[dst]}"
+
+    def _make_backward_solid_def(self, item_name):
+        """Returns a SolidDefinition for executing the given item in the backward sweep.
+
+        Args:
+            item_name (str): The project item that gets executed by the solid.
+
+        Returns:
+            SolidDefinition: solid's definition
+        """
+
+        def compute_fn(context, inputs):
+            if self.state() == SpineEngineState.USER_STOPPED:
+                context.log.error(f"compute_fn() FAILURE with item: {item_name} stopped by the user")
+                raise Failure()
+            context.log.info(f"Item Name: {item_name}")
+            item = self.make_item(item_name, ED.BACKWARD)
+            resources = item.output_resources(ED.BACKWARD)
+            for r in resources:
+                r.metadata["db_server_manager_queue"] = self._db_server_manager_queue
+            yield Output(value=resources, output_name=f"{ED.BACKWARD}_output")
+
+        input_defs = []
+        output_defs = [OutputDefinition(name=f"{ED.BACKWARD}_output")]
+        return SolidDefinition(
+            name=f"{ED.BACKWARD}_{self._solids_by_items[item_name]}",
+            input_defs=input_defs,
+            compute_fn=compute_fn,
+            output_defs=output_defs,
+        )
+
+    def _make_forward_solid_def(self, item_name):
+        """Returns a SolidDefinition for executing the given item.
+
+        Args:
+            item_name (str)
+
+        Returns:
+            SolidDefinition
+        """
+
+        def compute_fn(context, inputs):
+            if self.state() == SpineEngineState.USER_STOPPED:
+                context.log.error(f"compute_fn() FAILURE with item: {item_name} stopped by the user")
+                raise Failure()
+            context.log.info(f"Item Name: {item_name}")
+            for conn in self._connections_by_destination.get(item_name, []):
+                conn.visit_destination()
+            # Split inputs into forward and backward resources based on prefix
+            forward_resource_stacks = []
+            backward_resources = []
+            for name, values in inputs.items():
+                if name.startswith(f"{ED.FORWARD}"):
+                    forward_resource_stacks += values
+                elif name.startswith(f"{ED.BACKWARD}"):
+                    backward_resources += values
+            item_finish_state, output_resource_stacks = self._execute_item(
+                context, item_name, forward_resource_stacks, backward_resources
+            )
+            yield AssetMaterialization(asset_key=str(item_finish_state))
+            if output_resource_stacks:
+                yield Output(value=output_resource_stacks, output_name=f"{ED.FORWARD}_output")
+            for conn in self._connections_by_source.get(item_name, []):
+                conn.visit_source()
+
+        input_defs = [
+            InputDefinition(name=f"{ED.FORWARD}_input_from_{inj}")
+            for inj in self._forth_injectors.get(self._solids_by_items[item_name], [])
+        ] + [
+            InputDefinition(name=f"{ED.BACKWARD}_input_from_{inj}")
+            for inj in self._back_injectors.get(self._solids_by_items[item_name], [])
+        ]
+        output_defs = [OutputDefinition(name=f"{ED.FORWARD}_output")]
+        return SolidDefinition(
+            name=f"{ED.FORWARD}_{self._solids_by_items[item_name]}",
+            input_defs=input_defs,
+            compute_fn=compute_fn,
+            output_defs=output_defs,
+        )
+
+    def _execute_item(self, context, item_name, forward_resource_stacks, backward_resources):
+        """Executes the given item using the given forward resource stacks and backward resources.
+        Returns list of output resource stacks.
+
+        Called by ``_make_forward_solid_def.compute_fn``.
+
+        For each element yielded by ``_filtered_resources_iterator``, spawns a thread that runs
+        ``_execute_item_filtered``.
+
+        Args:
+            context
+            item_name (str)
+            forward_resource_stacks (list(tuple(ProjectItemResource))): resources coming from predecessor items -
+                one tuple of ProjectItemResource per item, where each element in the tuple corresponds to a filtered
+                execution of the item.
+            backward_resources (list(ProjectItemResource)): resources coming from successor items - just one
+                resource per item.
+
+        Returns:
+            ItemExecutionFinishState
+            list(tuple(ProjectItemResource))
+        """
+        item = self.make_item(item_name, ED.NONE)
+        if not item.ready_to_execute(self._settings):
+            if not self._execution_permits[item_name]:
+                return ItemExecutionFinishState.EXCLUDED, []
+            context.log.error(f"compute_fn() FAILURE with '{item_name}', not ready for forward execution")
+            return ItemExecutionFinishState.FAILURE, []
+        success = [ItemExecutionFinishState.NEVER_FINISHED]
+        output_resources_list = []
+        threads = []
+        resources_iterator = self._filtered_resources_iterator(
+            item_name, forward_resource_stacks, backward_resources, self._timestamp
+        )
+        with mp.Manager() as multiprocess_manager:
+            item_lock = multiprocess_manager.Lock()
+            for flt_fwd_resources, flt_bwd_resources, filter_id in resources_iterator:
+                self.resources_per_item[item_name] = (flt_fwd_resources, flt_bwd_resources)
+                item = self.make_item(item_name, ED.FORWARD)
+                item.filter_id = filter_id
+                thread = threading.Thread(
+                    target=self._execute_item_filtered,
+                    args=(item, flt_fwd_resources, flt_bwd_resources, output_resources_list, item_lock, success),
+                )
+                threads.append(thread)
+            for thread in threads:
+                thread.start()
+            for thread in threads:
+                thread.join()
+        if success[0] == ItemExecutionFinishState.FAILURE:
+            context.log.error(f"compute_fn() FAILURE with {item_name}, failed to execute")
+            raise Failure()
+        for resources in output_resources_list:
+            for connection in self._connections_by_source.get(item_name, []):
+                connection.receive_resources_from_source(resources)
+        return success[0], output_resources_list
+
+    def _execute_item_filtered(
+        self, item, filtered_forward_resources, filtered_backward_resources, output_resources_list, item_lock, success
+    ):
+        """Executes the given item using the given filtered resources. Target for threads in ``_execute_item``.
+
+        Args:
+            item (ExecutableItemBase)
+            filtered_forward_resources (list(ProjectItemResource))
+            filtered_backward_resources (list(ProjectItemResource))
+            output_resources_list (list(list(ProjectItemResource))): A list to append the output resources
+                generated by the item.
+            item_lock (mp.Lock): Shared lock for parallel executions.
+            success (list): A list of one element, to write the outcome of the execution.
+        """
+        self._running_items.append(item)
+        if self._execution_permits[item.name]:
+            item_finish_state = item.execute(filtered_forward_resources, filtered_backward_resources, item_lock)
+            item.finish_execution(item_finish_state)
+        else:
+            item.exclude_execution(filtered_forward_resources, filtered_backward_resources, item_lock)
+            item_finish_state = ItemExecutionFinishState.EXCLUDED
+        filter_stack = sum((r.metadata.get("filter_stack", ()) for r in filtered_forward_resources), ())
+        output_resources = item.output_resources(ED.FORWARD)
+        for resource in output_resources:
+            resource.metadata["filter_stack"] = filter_stack
+            resource.metadata["filter_id"] = item.filter_id
+            resource.metadata["db_server_manager_queue"] = self._db_server_manager_queue
+        output_resources_list.append(output_resources)
+        success[0] = item_finish_state  # FIXME: We need a Lock here
+        self._running_items.remove(item)
+
+    def _filtered_resources_iterator(self, item_name, forward_resource_stacks, backward_resources, timestamp):
+        """Yields tuples of (filtered forward resources, filtered backward resources, filter id).
+
+        Each tuple corresponds to a unique filter combination. Combinations are obtained by applying the cross-product
+        over forward resource stacks.
+
+        Args:
+            item_name (str)
+            forward_resource_stacks (list(tuple(ProjectItemResource))): resources coming from predecessor items -
+                one tuple of ProjectItemResource per item, where each element in the tuple corresponds to a filtered
+                execution of the item.
+            backward_resources (list(ProjectItemResource)): resources coming from successor items - just one
+                resource per item.
+            timestamp (str): timestamp for the execution filter
+
+        Yields:
+            tuple(list,list,str): forward resources, backward resources, filter id
+        """
+
+        def check_resource_affinity(filtered_forward_resources):
+            filter_ids_by_provider = dict()
+            for r in filtered_forward_resources:
+                filter_ids_by_provider.setdefault(r.provider_name, set()).add(r.metadata.get("filter_id"))
+            return all(len(filter_ids) == 1 for filter_ids in filter_ids_by_provider.values())
+
+        resource_filter_stacks = dict()
+        unfiltered_resource_lists = dict()
+        for stack in forward_resource_stacks:
+            if not stack:
+                continue
+            unfiltered = list()
+            for resource in stack:
+                filter_stacks = self._filter_stacks(item_name, resource.provider_name, resource.label)
+                if not filter_stacks:
+                    unfiltered.append(resource)
+                else:
+                    resource_filter_stacks[resource] = filter_stacks
+            if unfiltered:
+                unfiltered_resource_lists.setdefault(stack[0].provider_name, list()).append(unfiltered)
+        forward_resource_stacks_iterator = (
+            self._expand_resource_stack(resource, filter_stacks)
+            for resource, filter_stacks in resource_filter_stacks.items()
+        )
+        backward_resources = self._convert_backward_resources(item_name, backward_resources)
+        for resources_or_lists in product(*unfiltered_resource_lists.values(), *forward_resource_stacks_iterator):
+            filtered_forward_resources = list()
+            for item in resources_or_lists:
+                if isinstance(item, list):
+                    filtered_forward_resources += item
+                else:
+                    filtered_forward_resources.append(item)
+            if not check_resource_affinity(filtered_forward_resources):
+                continue
+            filtered_forward_resources = self._convert_forward_resources(item_name, filtered_forward_resources)
+            resource_filter_stack = {r: r.metadata.get("filter_stack", ()) for r in filtered_forward_resources}
+            scenarios = {scenario_name_from_dict(cfg) for stack in resource_filter_stack.values() for cfg in stack}
+            scenarios.discard(None)
+            execution = {"execution_item": item_name, "scenarios": list(scenarios), "timestamp": timestamp}
+            config = execution_filter_config(execution)
+            filtered_backward_resources = []
+            for resource in backward_resources:
+                if "part_count" in resource.metadata:
+                    resource.metadata["part_count"] += 1
+                clone = resource.clone(additional_metadata={"filter_stack": (config,)})
+                clone.url = append_filter_config(clone.url, config)
+                filtered_backward_resources.append(clone)
+            filter_id = _make_filter_id(resource_filter_stack)
+            yield list(filtered_forward_resources), filtered_backward_resources, filter_id
+
+    @staticmethod
+    def _expand_resource_stack(resource, filter_stacks):
+        """Expands a resource according to filters defined for that resource.
+
+        Returns an expanded stack of as many resources as filter stacks defined for the resource.
+        Each resource in the expanded stack is a clone of the original, with one of the filter stacks
+        applied to the URL.
+
+        Args:
+            resource (ProjectItemResource): resource to expand
+            filter_stacks (list): resource's filter stacks
+
+        Returns:
+            tuple(ProjectItemResource): expanded resources
+        """
+        expanded_stack = ()
+        for filter_stack in filter_stacks:
+            filtered_clone = resource.clone(additional_metadata={"filter_stack": filter_stack})
+            for config in filter_stack:
+                filtered_clone.url = append_filter_config(filtered_clone.url, config)
+            expanded_stack += (filtered_clone,)
+        return expanded_stack
+
+    def _filter_stacks(self, item_name, provider_name, resource_label):
+        """Computes filter stacks.
+
+        Stacks are computed as the cross-product of all individual filters defined for a resource.
+
+        Args:
+            item_name (str): item's name
+            provider_name (str): resource provider's name
+            resource_label (str): resource's label
+
+        Returns:
+            list of list: filter stacks
+        """
+        connections = self._connections_by_destination.get(item_name, [])
+        connection = next(iter(c for c in connections if c.source == provider_name), None)
+        if connection is None:
+            raise RuntimeError("Logic error: no connection from resource provider")
+        filters = connection.enabled_filters(resource_label)
+        if filters is None:
+            return []
+        filter_configs_list = []
+        for filter_type, names in filters.items():
+            filter_configs = [filter_config(filter_type, name) for name in names]
+            if not filter_configs:
+                continue
+            filter_configs_list.append(filter_configs)
+        return list(product(*filter_configs_list))
+
+    def _convert_backward_resources(self, item_name, resources):
+        """Converts resources as they're being passed backwards to given item.
+        The conversion is dictated by the connection the resources traverse in order to reach the item.
+
+        Args:
+            item_name (str): receiving item's name
+            resources (Iterable of ProjectItemResource): resources to convert
+
+        Returns:
+            list of ProjectItemResource: converted resources
+        """
+        connections = self._connections_by_source.get(item_name, [])
+        resources_by_provider = {}
+        for r in resources:
+            resources_by_provider.setdefault(r.provider_name, list()).append(r)
+        for c in connections:
+            resources_from_destination = resources_by_provider.get(c.destination)
+            if resources_from_destination is None:
+                continue
+            if self._execution_permits[item_name]:
+                c.clean_up_backward_resources(resources_from_destination)
+            sibling_connections = [x for x in self._connections_by_destination.get(c.destination, []) if x != c]
+            resources_by_provider[c.destination] = c.convert_backward_resources(
+                resources_from_destination, sibling_connections
+            )
+        return [r for resources in resources_by_provider.values() for r in resources]
+
+    def _convert_forward_resources(self, item_name, resources):
+        """Converts resources as they're being passed forwards to given item.
+        The conversion is dictated by the connection the resources traverse in order to reach the item.
+
+        Args:
+            item_name (str): receiving item's name
+            resources (Iterable of ProjectItemResource): resources to convert
+
+        Returns:
+            list of ProjectItemResource: converted resources
+        """
+        connections = self._connections_by_destination.get(item_name, [])
+        resources_by_provider = {}
+        for r in resources:
+            resources_by_provider.setdefault(r.provider_name, list()).append(r)
+        for c in connections:
+            resources_from_source = resources_by_provider.get(c.source)
+            if resources_from_source is None:
+                continue
+            resources_by_provider[c.source] = c.convert_forward_resources(resources_from_source)
+        return [r for resources in resources_by_provider.values() for r in resources]
+
+    def _make_dependencies(self):
+        """
+        Returns a dictionary of dependencies according to the given dictionaries of injectors.
+
+        Returns:
+            dict: a dictionary to pass to the PipelineDefinition constructor as dependencies
+        """
+        forward_deps = {
+            f"{ED.FORWARD}_{n}": {
+                f"{ED.FORWARD}_input_from_{inj}": DependencyDefinition(f"{ED.FORWARD}_{inj}", f"{ED.FORWARD}_output")
+                for inj in injs
+            }
+            for n, injs in self._forth_injectors.items()
+        }
+        backward_deps = {
+            f"{ED.FORWARD}_{n}": {
+                f"{ED.BACKWARD}_input_from_{inj}": DependencyDefinition(f"{ED.BACKWARD}_{inj}", f"{ED.BACKWARD}_output")
+                for inj in injs
+            }
+            for n, injs in self._back_injectors.items()
+        }
+        deps = {}
+        for n in forward_deps.keys() | backward_deps.keys():
+            deps[n] = forward_deps.get(n, {})
+            deps[n].update(backward_deps.get(n, {}))
+        return deps
+
+
+def _make_filter_id(resource_filter_stack):
+    """Builds filter id from resource filter stack.
+
+    Args:
+        resource_filter_stack (dict): mapping from resource to filter stack
+
+    Returns:
+        str: filter id
+    """
+    provider_filters = set()
+    for resource, stack in resource_filter_stack.items():
+        if resource.type_ != "database":
+            filter_id = resource.metadata.get("filter_id")
+            if filter_id is None:
+                continue
+            provider_filters.add(filter_id)
+        else:
+            filter_names = sorted(_filter_names_from_stack(stack))
+            if not filter_names:
+                continue
+            provider_filters.add(", ".join(filter_names) + " - " + resource.provider_name)
+    return " & ".join(sorted(provider_filters))
+
+
+def _filter_names_from_stack(stack):
+    """Yields filter names from filter stack.
+
+    Args:
+        stack (Iterable of dict): filter stack
+
+    Yields:
+        str: filter name
+    """
+    for config in stack:
+        if not config:
+            continue
+        filter_name = name_from_dict(config)
+        if filter_name is not None:
+            yield filter_name
+
+
+def _validate_dag(dag):
+    """Raises an exception in case DAG is not valid.
+
+    Args:
+        dag (networkx.DiGraph): mapping from node name to list of direct successor nodes
+    """
+    if not nx.is_directed_acyclic_graph(dag):
+        raise EngineInitFailed("Invalid DAG")
+    if len(list(nx.weakly_connected_components(dag))) > 1:
+        raise EngineInitFailed("DAG contains unconnected items.")
+
+
+def validate_jumps(jumps, dag):
+    """Raises an exception in case jumps are not valid.
+
+    Args:
+        jumps (list of Jump): jumps
+        dag (DiGraph): jumps' DAG
+    """
+    items_by_jump = _get_items_by_jump(jumps, dag)
+    for jump in jumps:
+        validate_single_jump(jump, jumps, dag, items_by_jump)
+
+
+def validate_single_jump(jump, jumps, dag, items_by_jump=None):
+    """Raises an exception in case one jump is not valid.
+
+    Args:
+        jump (Jump): the jump to check
+        jumps (list of Jump): all jumps in dag
+        dag (DiGraph): jumps' DAG
+        items_by_jump (dict, optional): mapping jumps to a set of items in between destination and source
+    """
+    if not jump.ready_to_execute():
+        raise EngineInitFailed(f"Jump {jump.name} is not ready for execution.")
+    if items_by_jump is None:
+        items_by_jump = _get_items_by_jump(jumps, dag)
+    for other in jumps:
+        if other is jump:
+            continue
+        if other.source == jump.source:
+            raise EngineInitFailed(f"{jump.name} cannot have the same source as {other.name}.")
+        jump_items = items_by_jump[jump]
+        other_items = items_by_jump[other]
+        intersection = jump_items & other_items
+        if intersection not in (set(), jump_items, other_items):
+            raise EngineInitFailed(f"{jump.name} cannot partially overlap {other.name}.")
+    if not dag.has_node(jump.destination):
+        raise EngineInitFailed(f"Loop destination '{jump.destination}' not found in DAG")
+    if not dag.has_node(jump.source):
+        raise EngineInitFailed(f"Loop source '{jump.source}' not found in DAG")
+    if jump.source == jump.destination:
+        return
+    if nx.has_path(dag, jump.source, jump.destination):
+        raise EngineInitFailed("Cannot loop in forward direction.")
+    if not nx.has_path(nx.reverse_view(dag), jump.source, jump.destination):
+        raise EngineInitFailed("Cannot loop between DAG branches.")
+
+
+def _get_items_by_jump(jumps, dag):
+    """Returns a dict mapping jumps to a set of items between destination and source.
+
+    Args:
+        jumps (list of Jump): all jumps in dag
+        dag (DiGraph): jumps' DAG
+
+    Returns:
+        dict
+    """
+    items_by_jump = {}
+    for jump in jumps:
+        try:
+            items_by_jump[jump] = {
+                item for path in nx.all_simple_paths(dag, jump.destination, jump.source) for item in path
+            }
+        except nx.NodeNotFound:
+            items_by_jump[jump] = set()
+    return items_by_jump
+
+
+def _set_resource_limits(settings, lock):
+    """Sets limits for simultaneous single-shot and persistent processes.
+
+    May potentially kill existing persistent processes.
+
+    Args:
+        settings (AppSettings): Engine settings
+    """
+    with lock:
+        process_limiter = settings.value("engineSettings/processLimiter", "auto")
+        if process_limiter == "unlimited":
+            limit = "unlimited"
+        elif process_limiter == "auto":
+            limit = os.cpu_count()
+        else:
+            limit = int(settings.value("engineSettings/maxProcesses", os.cpu_count()))
+        one_shot_process_semaphore.set_limit(limit)
+        persistent_limiter = settings.value("engineSettings/persistentLimiter", "unlimited")
+        if persistent_limiter == "unlimited":
+            limit = "unlimited"
+        elif persistent_limiter == "auto":
+            limit = os.cpu_count()
+        else:
+            limit = int(settings.value("engineSettings/maxPersistentProcesses", os.cpu_count()))
+        persistent_process_semaphore.set_limit(limit)
```

### Comparing `spine_engine-0.23.3/spine_engine/utils/__init__.py` & `spine_engine-0.23.4/tests/server/__init__.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,14 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Init file for spine_engine.tests.server package.
+"""
```

### Comparing `spine_engine-0.23.3/spine_engine/utils/helpers.py` & `spine_engine-0.23.4/spine_engine/utils/helpers.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,533 +1,533 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Helpers functions and classes.
-
-"""
-import collections
-import os
-import sys
-import datetime
-import itertools
-import time
-import json
-from pathlib import Path
-from enum import Enum, auto, unique
-import networkx
-from jupyter_client.kernelspec import find_kernel_specs
-from spinedb_api.spine_io.gdx_utils import find_gams_directory
-from ..config import PYTHON_EXECUTABLE, JULIA_EXECUTABLE, GAMS_EXECUTABLE, EMBEDDED_PYTHON
-
-
-@unique
-class ExecutionDirection(Enum):
-    FORWARD = auto()
-    BACKWARD = auto()
-    NONE = auto()
-
-    def __str__(self):
-        return str(self.name)
-
-
-@unique
-class ItemExecutionFinishState(Enum):
-    SUCCESS = 1
-    FAILURE = 2
-    SKIPPED = 3
-    EXCLUDED = 4
-    STOPPED = 5
-    NEVER_FINISHED = 6
-
-    def __str__(self):
-        return str(self.name)
-
-
-class Singleton(type):
-    _instances = {}
-
-    def __call__(cls, *args, **kwargs):
-        if cls not in cls._instances:
-            cls._instances[cls] = super().__call__(*args, **kwargs)
-        return cls._instances[cls]
-
-
-class AppSettings:
-    """
-    A QSettings replacement.
-    """
-
-    def __init__(self, settings):
-        """
-        Init.
-
-        Args:
-            settings (dict)
-        """
-        self._settings = settings
-
-    def value(self, key, defaultValue=""):
-        return self._settings.get(key, defaultValue)
-
-
-def shorten(name):
-    """Returns the 'short name' version of given name."""
-    return name.lower().replace(" ", "_")
-
-
-def create_log_file_timestamp():
-    """Creates a new timestamp string that is used as Data Store and Importer error log file.
-
-    Returns:
-        Timestamp string or empty string if failed.
-    """
-    try:
-        # Create timestamp
-        stamp = datetime.datetime.fromtimestamp(time.time())
-    except OverflowError:
-        return ""
-    extension = stamp.strftime("%Y%m%dT%H%M%S")
-    return extension
-
-
-def create_timestamp():
-    return datetime.datetime.now().strftime("%Y-%m-%dT%H:%M:%S")
-
-
-def resolve_conda_executable(conda_path):
-    """If given conda_path is an empty str, returns current Conda
-    executable from CONDA_EXE env variable if the app was started
-    on Conda, otherwise returns an empty string.
-    """
-    if conda_path != "":
-        return conda_path
-    conda_exe = os.environ.get("CONDA_EXE", "")
-    return conda_exe
-
-
-def resolve_python_interpreter(python_path):
-    """If given python_path is empty, returns the
-    full path to Python interpreter depending on user's
-    settings and whether the app is frozen or not.
-    """
-    if python_path != "":
-        return python_path
-    if not getattr(sys, "frozen", False):
-        return sys.executable  # Use current Python
-    # We are frozen
-    path = resolve_executable_from_path(PYTHON_EXECUTABLE)
-    if path != "":
-        return path  # Use Python from PATH
-    return EMBEDDED_PYTHON  # Use embedded <app_install_dir>/Tools/python.exe
-
-
-def resolve_julia_executable(julia_path):
-    """if given julia_path is empty, tries to find the path to Julia
-    in user's PATH env variable. If Julia is not found in PATH,
-    returns an empty string.
-
-    Note: In the long run, we should decide whether this is something we want to do
-    because adding julia-x.x./bin/ dir to the PATH is not recommended because this
-    also exposes some .dlls to other programs on user's (windows) system. I.e. it
-    may break other programs, and this is why the Julia installer does not
-    add (and does not even offer the chance to add) Julia to PATH.
-    """
-    if julia_path != "":
-        return julia_path
-    return resolve_executable_from_path(JULIA_EXECUTABLE)
-
-
-def resolve_gams_executable(gams_path):
-    """If given gams_path is empty, tries to find the path to GAMS executable.
-
-    If GAMS is not found, returns an empty string.
-
-    Args:
-        gams_path (str): current path to GAMS executable
-
-    Returns:
-        str: resolved path to GAMS executable
-    """
-    if gams_path != "":
-        return gams_path
-    gams_dir = find_gams_directory()
-    if gams_dir is None:
-        return ""
-    return os.path.join(gams_dir, GAMS_EXECUTABLE)
-
-
-def resolve_executable_from_path(executable_name):
-    """Returns full path to executable name in user's
-    PATH env variable. If not found, returns an empty string.
-
-    Basically equivalent to 'where' and 'which' commands in
-    cmd.exe and bash respectively.
-
-    Args:
-        executable_name (str): Executable filename to find (e.g. python.exe, julia.exe)
-
-    Returns:
-        str: Full path or empty string
-    """
-    executable_paths = os.get_exec_path()
-    for path in executable_paths:
-        candidate = os.path.join(path, executable_name)
-        if os.path.isfile(candidate):
-            return candidate
-    return ""
-
-
-def inverted(input_):
-    """Inverts a dictionary of list values.
-
-    Args:
-        input_ (dict)
-
-    Returns:
-        dict: keys are list items, and values are keys listing that item from the input dictionary
-    """
-    output = dict()
-    for key, value_list in input_.items():
-        for value in value_list:
-            output.setdefault(value, list()).append(key)
-    return output
-
-
-def get_julia_command(settings):
-    """
-    Args:
-        settings (QSettings, AppSettings)
-
-    Returns:
-        list of str: e.g. ["path/to/julia", "--project=path/to/project/"]
-    """
-    env = get_julia_env(settings)
-    if env is None:
-        return None
-    julia, project = env
-    command = [julia]
-    if project:
-        command.append(f"--project={project}")
-    return command
-
-
-def get_julia_env(settings):
-    """
-    Args:
-        settings (QSettings, AppSettings)
-
-    Returns:
-        tuple, NoneType: (julia_exe, julia_project), or None if none found
-    """
-    use_julia_kernel = settings.value("appSettings/useJuliaKernel", defaultValue="2") == "2"
-    if use_julia_kernel:
-        kernel_name = settings.value("appSettings/juliaKernel", defaultValue="")
-        resource_dir = find_kernel_specs().get(kernel_name)
-        if resource_dir is None:
-            return None
-        filepath = os.path.join(resource_dir, "kernel.json")
-        with open(filepath, "r") as fh:
-            try:
-                kernel_spec = json.load(fh)
-            except json.decoder.JSONDecodeError:
-                return None
-        julia = kernel_spec["argv"].pop(0)
-        project_arg = next((arg for arg in kernel_spec["argv"] if arg.startswith("--project=")), None)
-        project = "" if project_arg is None else project_arg.split("--project=")[1]
-        return julia, project
-    julia = settings.value("appSettings/juliaPath", defaultValue="")
-    if julia == "":
-        julia = resolve_executable_from_path(JULIA_EXECUTABLE)
-        if julia == "":
-            return None
-    project = settings.value("appSettings/juliaProjectPath", defaultValue="")
-    return julia, project
-
-
-def required_items_for_execution(items, connections, executable_item_classes, execution_permits):
-    """Builds a list of names of items that are required for execution.
-
-    An item is required if
-
-    - it has an execution permit
-    - the item is part of a filtered fork that contains an item that has an execution permit
-
-    Args:
-        items (dict): mapping from item name to item dict
-        connections (list of Connection): connections
-        executable_item_classes (dict): mapping from item type to its executable class
-        execution_permits (dict): item execution permits
-
-    Returns:
-        set of str: names of required items
-    """
-    first_filter_fork_nodes = _first_filter_fork_nodes(connections)
-    filter_fork_terminus_nodes = _filter_fork_termini(items, executable_item_classes)
-    dependent_paths = _filtered_fork_paths(
-        make_dag(dag_edges(connections), execution_permits), first_filter_fork_nodes, filter_fork_terminus_nodes
-    )
-    items_required_predecessors = _dependent_items_per_item(dependent_paths)
-    required_items = {item for item, is_permitted in execution_permits.items() if is_permitted}
-    for item in list(required_items):
-        required_items |= items_required_predecessors.get(item, set())
-    return required_items
-
-
-def _first_filter_fork_nodes(connections):
-    """Collects nodes that start a filtered fork.
-
-    Args:
-        connections (Iterable of Connection): connections
-
-    Returns:
-        set of str: item names
-    """
-    nodes = set()
-    for connection in connections:
-        if connection.has_filters_online():
-            nodes.add(connection.destination)
-    return nodes
-
-
-def _filter_fork_termini(items, executable_item_classes):
-    """Collects nodes that terminate a filtered fork.
-
-    Args:
-        items (dict): mapping from item name to item dict
-        executable_item_classes (dict): mapping from item type to corresponding executable item class
-
-    Returns:
-        set of str: item names
-    """
-    termini = set()
-    for name, item_dict in items.items():
-        if executable_item_classes[item_dict["type"]].is_filter_terminus():
-            termini.add(name)
-    return termini
-
-
-def _filtered_fork_paths(dag, first_filter_fork_nodes, filter_fork_terminus_nodes):
-    """Collects all simple paths within given DAG that will be forked.
-
-    Args:
-        dag (DiGraph): DAG
-        first_filter_fork_nodes (set of str): names of fork staring items
-        filter_fork_terminus_nodes (set of str): names of fork ending items
-
-    Returns:
-        list of list of str: items names along the paths
-    """
-    sources = [node for node, in_degree in dag.in_degree if in_degree == 0]
-    targets = [node for node, out_degree in dag.out_degree if out_degree == 0]
-    paths = []
-    for source, target in itertools.product(sources, targets):
-        for path in networkx.all_simple_paths(dag, source, target):
-            gather = False
-            gathered = []
-            for node in path:
-                if node in first_filter_fork_nodes:
-                    gather = True
-                if node in filter_fork_terminus_nodes and gather:
-                    if gathered:
-                        paths.append(gathered)
-                        gathered = []
-                    gather = False
-                if gather:
-                    gathered.append(node)
-            if gathered:
-                paths.append(gathered)
-    return paths
-
-
-def _dependent_items_per_item(fork_paths):
-    """Collects dependent items for each project item.
-
-    Args:
-        fork_paths (list of list of str): item names along fork paths
-
-    Returns:
-        dict: mapping from item name to a set of the names of its dependant items
-    """
-    items_dependent_nodes = collections.defaultdict(set)
-    for path in fork_paths:
-        for i, node in enumerate(path[1:]):
-            items_dependent_nodes[node] |= set(path[: i + 1])
-    return items_dependent_nodes
-
-
-def make_connections(connections, permitted_items):
-    """Returns a list of Connections based on permitted
-    items. Creates Connections only for connections that
-    are coming from permitted items or leaving from
-    permitted items.
-
-    Args:
-        connections (list of Connection): connections in the DAG
-        permitted_items (set of str): names of permitted items
-
-    Returns:
-        list of Connection: List of permitted Connections or an empty list if the DAG contains no connections
-    """
-    if not connections:
-        return list()
-    connections = connections_to_selected_items(connections, permitted_items)
-    return connections
-
-
-def connections_to_selected_items(connections, selected_items):
-    """Returns a list of Connections that have a permitted item
-    as its source or destination item.
-
-    Args:
-        connections (list(Connection): List of Connections
-        selected_items (set of str): names of permitted items
-
-    Returns:
-        list of Connection: Connections allowed in the current DAG
-    """
-    return [conn for conn in connections if conn.source in selected_items or conn.destination in selected_items]
-
-
-def dag_edges(connections):
-    """Collects DAG edges based on Connection instances.
-
-    Args:
-        connections (list(Connection): Connections
-
-    Returns:
-        dict: DAG edges. Mapping of source item (node) to a list of destination items (nodes)
-    """
-    edges = dict()
-    for connection in connections:
-        source, destination = connection.source, connection.destination
-        edges.setdefault(source, list()).append(destination)
-    return edges
-
-
-def make_dag(edges, permitted_nodes=None):
-    """Builds a DAG from edges or if no edges exist, from permitted_nodes.
-
-    Args:
-        edges (dict): Mapping from item name to list of its successors' names
-        permitted_nodes (dict, optional): Mapping from item name to boolean value indicating if item is selected
-
-    Returns:
-        DiGraph: Directed acyclic graph
-    """
-    graph = networkx.DiGraph()
-    if not edges:
-        # Make a single node DAG with no edges
-        nodes = [node_name for node_name, permitted in permitted_nodes.items() if permitted]
-        graph.add_nodes_from(nodes)
-    else:
-        graph.add_nodes_from(edges)
-        for node, successors in edges.items():
-            if successors is None:
-                continue
-            for successor in successors:
-                graph.add_edge(node, successor)
-    return graph
-
-
-def write_filter_id_file(filter_id, path):
-    """Writes filter id to disk.
-
-    Args:
-        filter_id (str): filter id
-        path (Path or str): full path to directory where the filter id file will be written
-    """
-    with Path(path, ".filter_id").open("w") as filter_id_file:
-        filter_id_file.writelines([filter_id + "\n"])
-
-
-def gather_leaf_data(input_dict, paths, pop=False):
-    """Gathers data defined by 'paths' of keys from nested dicts.
-
-    Args:
-        input_dict (dict): dict to pop from
-        paths (list of tuple): 'paths' of dict keys to leaf entries
-        pop (bool): if True, pops the leaf data modifying ''input_dict''
-
-    Returns:
-        dict: popped data
-    """
-
-    def travel_to_leaf(dict_to_travel, path_to_leaf):
-        traveller = dict_to_travel
-        for part in path_to_leaf[:-1]:
-            traveller = traveller.get(part)
-            if traveller is None:
-                return None
-        return traveller
-
-    def build_to_leaf(base_dict, path_to_leaf):
-        builder = base_dict
-        for part in path_to_leaf[:-1]:
-            builder = builder.setdefault(part, {})
-        return builder
-
-    gather = "pop" if pop else "get"
-    output_dict = {}
-    for prefix in paths:
-        leaf_dict = travel_to_leaf(input_dict, prefix)
-        if leaf_dict is None:
-            continue
-        value = getattr(leaf_dict, gather)(prefix[-1], None)
-        if value is None:
-            continue
-        leaf_dict = build_to_leaf(output_dict, prefix)
-        leaf_dict[prefix[-1]] = value
-    return output_dict
-
-
-def get_file_size(size_in_bytes):
-    """Returns a human readable string of the size of a file. Given size_in_bytes arg
-    is designed as the output of os.path.getsize().
-
-    1 KB = 1024 bytes
-    1 MB = 1024*1024 bytes
-    1 GB = 1024*1024*1024 bytes
-
-    Args:
-        size_in_bytes (int): Size in bytes [B]
-
-    Returns:
-        str: Human readable file size
-    """
-    kb = 1024
-    mb = 1024 * 1024
-    gb = 1024 * 1024 * 1024
-    if size_in_bytes <= kb:
-        return str(size_in_bytes) + " B"
-    if kb < size_in_bytes <= mb:
-        return str(round(size_in_bytes / kb, 1)) + " KB"
-    elif mb < size_in_bytes < gb:
-        return str(round(size_in_bytes / mb, 1)) + " MB"
-    else:
-        return str(round(size_in_bytes / gb, 1)) + " GB"
-
-
-class PartCount:
-    def __init__(self):
-        self._count = 0
-
-    def __iadd__(self, number):
-        self._count += number
-        return self
-
-    def __eq__(self, number):
-        return self._count == number
-
-    def __deepcopy__(self, memo):
-        return self
-
-    def __repr__(self):
-        return str(self._count)
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Helpers functions and classes.
+
+"""
+import collections
+import os
+import sys
+import datetime
+import itertools
+import time
+import json
+from pathlib import Path
+from enum import Enum, auto, unique
+import networkx
+from jupyter_client.kernelspec import find_kernel_specs
+from spinedb_api.spine_io.gdx_utils import find_gams_directory
+from ..config import PYTHON_EXECUTABLE, JULIA_EXECUTABLE, GAMS_EXECUTABLE, EMBEDDED_PYTHON
+
+
+@unique
+class ExecutionDirection(Enum):
+    FORWARD = auto()
+    BACKWARD = auto()
+    NONE = auto()
+
+    def __str__(self):
+        return str(self.name)
+
+
+@unique
+class ItemExecutionFinishState(Enum):
+    SUCCESS = 1
+    FAILURE = 2
+    SKIPPED = 3
+    EXCLUDED = 4
+    STOPPED = 5
+    NEVER_FINISHED = 6
+
+    def __str__(self):
+        return str(self.name)
+
+
+class Singleton(type):
+    _instances = {}
+
+    def __call__(cls, *args, **kwargs):
+        if cls not in cls._instances:
+            cls._instances[cls] = super().__call__(*args, **kwargs)
+        return cls._instances[cls]
+
+
+class AppSettings:
+    """
+    A QSettings replacement.
+    """
+
+    def __init__(self, settings):
+        """
+        Init.
+
+        Args:
+            settings (dict)
+        """
+        self._settings = settings
+
+    def value(self, key, defaultValue=""):
+        return self._settings.get(key, defaultValue)
+
+
+def shorten(name):
+    """Returns the 'short name' version of given name."""
+    return name.lower().replace(" ", "_")
+
+
+def create_log_file_timestamp():
+    """Creates a new timestamp string that is used as Data Store and Importer error log file.
+
+    Returns:
+        Timestamp string or empty string if failed.
+    """
+    try:
+        # Create timestamp
+        stamp = datetime.datetime.fromtimestamp(time.time())
+    except OverflowError:
+        return ""
+    extension = stamp.strftime("%Y%m%dT%H%M%S")
+    return extension
+
+
+def create_timestamp():
+    return datetime.datetime.now().strftime("%Y-%m-%dT%H:%M:%S")
+
+
+def resolve_conda_executable(conda_path):
+    """If given conda_path is an empty str, returns current Conda
+    executable from CONDA_EXE env variable if the app was started
+    on Conda, otherwise returns an empty string.
+    """
+    if conda_path != "":
+        return conda_path
+    conda_exe = os.environ.get("CONDA_EXE", "")
+    return conda_exe
+
+
+def resolve_python_interpreter(python_path):
+    """If given python_path is empty, returns the
+    full path to Python interpreter depending on user's
+    settings and whether the app is frozen or not.
+    """
+    if python_path != "":
+        return python_path
+    if not getattr(sys, "frozen", False):
+        return sys.executable  # Use current Python
+    # We are frozen
+    path = resolve_executable_from_path(PYTHON_EXECUTABLE)
+    if path != "":
+        return path  # Use Python from PATH
+    return EMBEDDED_PYTHON  # Use embedded <app_install_dir>/Tools/python.exe
+
+
+def resolve_julia_executable(julia_path):
+    """if given julia_path is empty, tries to find the path to Julia
+    in user's PATH env variable. If Julia is not found in PATH,
+    returns an empty string.
+
+    Note: In the long run, we should decide whether this is something we want to do
+    because adding julia-x.x./bin/ dir to the PATH is not recommended because this
+    also exposes some .dlls to other programs on user's (windows) system. I.e. it
+    may break other programs, and this is why the Julia installer does not
+    add (and does not even offer the chance to add) Julia to PATH.
+    """
+    if julia_path != "":
+        return julia_path
+    return resolve_executable_from_path(JULIA_EXECUTABLE)
+
+
+def resolve_gams_executable(gams_path):
+    """If given gams_path is empty, tries to find the path to GAMS executable.
+
+    If GAMS is not found, returns an empty string.
+
+    Args:
+        gams_path (str): current path to GAMS executable
+
+    Returns:
+        str: resolved path to GAMS executable
+    """
+    if gams_path != "":
+        return gams_path
+    gams_dir = find_gams_directory()
+    if gams_dir is None:
+        return ""
+    return os.path.join(gams_dir, GAMS_EXECUTABLE)
+
+
+def resolve_executable_from_path(executable_name):
+    """Returns full path to executable name in user's
+    PATH env variable. If not found, returns an empty string.
+
+    Basically equivalent to 'where' and 'which' commands in
+    cmd.exe and bash respectively.
+
+    Args:
+        executable_name (str): Executable filename to find (e.g. python.exe, julia.exe)
+
+    Returns:
+        str: Full path or empty string
+    """
+    executable_paths = os.get_exec_path()
+    for path in executable_paths:
+        candidate = os.path.join(path, executable_name)
+        if os.path.isfile(candidate):
+            return candidate
+    return ""
+
+
+def inverted(input_):
+    """Inverts a dictionary of list values.
+
+    Args:
+        input_ (dict)
+
+    Returns:
+        dict: keys are list items, and values are keys listing that item from the input dictionary
+    """
+    output = dict()
+    for key, value_list in input_.items():
+        for value in value_list:
+            output.setdefault(value, list()).append(key)
+    return output
+
+
+def get_julia_command(settings):
+    """
+    Args:
+        settings (QSettings, AppSettings)
+
+    Returns:
+        list of str: e.g. ["path/to/julia", "--project=path/to/project/"]
+    """
+    env = get_julia_env(settings)
+    if env is None:
+        return None
+    julia, project = env
+    command = [julia]
+    if project:
+        command.append(f"--project={project}")
+    return command
+
+
+def get_julia_env(settings):
+    """
+    Args:
+        settings (QSettings, AppSettings)
+
+    Returns:
+        tuple, NoneType: (julia_exe, julia_project), or None if none found
+    """
+    use_julia_kernel = settings.value("appSettings/useJuliaKernel", defaultValue="2") == "2"
+    if use_julia_kernel:
+        kernel_name = settings.value("appSettings/juliaKernel", defaultValue="")
+        resource_dir = find_kernel_specs().get(kernel_name)
+        if resource_dir is None:
+            return None
+        filepath = os.path.join(resource_dir, "kernel.json")
+        with open(filepath, "r") as fh:
+            try:
+                kernel_spec = json.load(fh)
+            except json.decoder.JSONDecodeError:
+                return None
+        julia = kernel_spec["argv"].pop(0)
+        project_arg = next((arg for arg in kernel_spec["argv"] if arg.startswith("--project=")), None)
+        project = "" if project_arg is None else project_arg.split("--project=")[1]
+        return julia, project
+    julia = settings.value("appSettings/juliaPath", defaultValue="")
+    if julia == "":
+        julia = resolve_executable_from_path(JULIA_EXECUTABLE)
+        if julia == "":
+            return None
+    project = settings.value("appSettings/juliaProjectPath", defaultValue="")
+    return julia, project
+
+
+def required_items_for_execution(items, connections, executable_item_classes, execution_permits):
+    """Builds a list of names of items that are required for execution.
+
+    An item is required if
+
+    - it has an execution permit
+    - the item is part of a filtered fork that contains an item that has an execution permit
+
+    Args:
+        items (dict): mapping from item name to item dict
+        connections (list of Connection): connections
+        executable_item_classes (dict): mapping from item type to its executable class
+        execution_permits (dict): item execution permits
+
+    Returns:
+        set of str: names of required items
+    """
+    first_filter_fork_nodes = _first_filter_fork_nodes(connections)
+    filter_fork_terminus_nodes = _filter_fork_termini(items, executable_item_classes)
+    dependent_paths = _filtered_fork_paths(
+        make_dag(dag_edges(connections), execution_permits), first_filter_fork_nodes, filter_fork_terminus_nodes
+    )
+    items_required_predecessors = _dependent_items_per_item(dependent_paths)
+    required_items = {item for item, is_permitted in execution_permits.items() if is_permitted}
+    for item in list(required_items):
+        required_items |= items_required_predecessors.get(item, set())
+    return required_items
+
+
+def _first_filter_fork_nodes(connections):
+    """Collects nodes that start a filtered fork.
+
+    Args:
+        connections (Iterable of Connection): connections
+
+    Returns:
+        set of str: item names
+    """
+    nodes = set()
+    for connection in connections:
+        if connection.has_filters_online():
+            nodes.add(connection.destination)
+    return nodes
+
+
+def _filter_fork_termini(items, executable_item_classes):
+    """Collects nodes that terminate a filtered fork.
+
+    Args:
+        items (dict): mapping from item name to item dict
+        executable_item_classes (dict): mapping from item type to corresponding executable item class
+
+    Returns:
+        set of str: item names
+    """
+    termini = set()
+    for name, item_dict in items.items():
+        if executable_item_classes[item_dict["type"]].is_filter_terminus():
+            termini.add(name)
+    return termini
+
+
+def _filtered_fork_paths(dag, first_filter_fork_nodes, filter_fork_terminus_nodes):
+    """Collects all simple paths within given DAG that will be forked.
+
+    Args:
+        dag (DiGraph): DAG
+        first_filter_fork_nodes (set of str): names of fork staring items
+        filter_fork_terminus_nodes (set of str): names of fork ending items
+
+    Returns:
+        list of list of str: items names along the paths
+    """
+    sources = [node for node, in_degree in dag.in_degree if in_degree == 0]
+    targets = [node for node, out_degree in dag.out_degree if out_degree == 0]
+    paths = []
+    for source, target in itertools.product(sources, targets):
+        for path in networkx.all_simple_paths(dag, source, target):
+            gather = False
+            gathered = []
+            for node in path:
+                if node in first_filter_fork_nodes:
+                    gather = True
+                if node in filter_fork_terminus_nodes and gather:
+                    if gathered:
+                        paths.append(gathered)
+                        gathered = []
+                    gather = False
+                if gather:
+                    gathered.append(node)
+            if gathered:
+                paths.append(gathered)
+    return paths
+
+
+def _dependent_items_per_item(fork_paths):
+    """Collects dependent items for each project item.
+
+    Args:
+        fork_paths (list of list of str): item names along fork paths
+
+    Returns:
+        dict: mapping from item name to a set of the names of its dependant items
+    """
+    items_dependent_nodes = collections.defaultdict(set)
+    for path in fork_paths:
+        for i, node in enumerate(path[1:]):
+            items_dependent_nodes[node] |= set(path[: i + 1])
+    return items_dependent_nodes
+
+
+def make_connections(connections, permitted_items):
+    """Returns a list of Connections based on permitted
+    items. Creates Connections only for connections that
+    are coming from permitted items or leaving from
+    permitted items.
+
+    Args:
+        connections (list of Connection): connections in the DAG
+        permitted_items (set of str): names of permitted items
+
+    Returns:
+        list of Connection: List of permitted Connections or an empty list if the DAG contains no connections
+    """
+    if not connections:
+        return list()
+    connections = connections_to_selected_items(connections, permitted_items)
+    return connections
+
+
+def connections_to_selected_items(connections, selected_items):
+    """Returns a list of Connections that have a permitted item
+    as its source or destination item.
+
+    Args:
+        connections (list(Connection): List of Connections
+        selected_items (set of str): names of permitted items
+
+    Returns:
+        list of Connection: Connections allowed in the current DAG
+    """
+    return [conn for conn in connections if conn.source in selected_items or conn.destination in selected_items]
+
+
+def dag_edges(connections):
+    """Collects DAG edges based on Connection instances.
+
+    Args:
+        connections (list(Connection): Connections
+
+    Returns:
+        dict: DAG edges. Mapping of source item (node) to a list of destination items (nodes)
+    """
+    edges = dict()
+    for connection in connections:
+        source, destination = connection.source, connection.destination
+        edges.setdefault(source, list()).append(destination)
+    return edges
+
+
+def make_dag(edges, permitted_nodes=None):
+    """Builds a DAG from edges or if no edges exist, from permitted_nodes.
+
+    Args:
+        edges (dict): Mapping from item name to list of its successors' names
+        permitted_nodes (dict, optional): Mapping from item name to boolean value indicating if item is selected
+
+    Returns:
+        DiGraph: Directed acyclic graph
+    """
+    graph = networkx.DiGraph()
+    if not edges:
+        # Make a single node DAG with no edges
+        nodes = [node_name for node_name, permitted in permitted_nodes.items() if permitted]
+        graph.add_nodes_from(nodes)
+    else:
+        graph.add_nodes_from(edges)
+        for node, successors in edges.items():
+            if successors is None:
+                continue
+            for successor in successors:
+                graph.add_edge(node, successor)
+    return graph
+
+
+def write_filter_id_file(filter_id, path):
+    """Writes filter id to disk.
+
+    Args:
+        filter_id (str): filter id
+        path (Path or str): full path to directory where the filter id file will be written
+    """
+    with Path(path, ".filter_id").open("w") as filter_id_file:
+        filter_id_file.writelines([filter_id + "\n"])
+
+
+def gather_leaf_data(input_dict, paths, pop=False):
+    """Gathers data defined by 'paths' of keys from nested dicts.
+
+    Args:
+        input_dict (dict): dict to pop from
+        paths (list of tuple): 'paths' of dict keys to leaf entries
+        pop (bool): if True, pops the leaf data modifying ''input_dict''
+
+    Returns:
+        dict: popped data
+    """
+
+    def travel_to_leaf(dict_to_travel, path_to_leaf):
+        traveller = dict_to_travel
+        for part in path_to_leaf[:-1]:
+            traveller = traveller.get(part)
+            if traveller is None:
+                return None
+        return traveller
+
+    def build_to_leaf(base_dict, path_to_leaf):
+        builder = base_dict
+        for part in path_to_leaf[:-1]:
+            builder = builder.setdefault(part, {})
+        return builder
+
+    gather = "pop" if pop else "get"
+    output_dict = {}
+    for prefix in paths:
+        leaf_dict = travel_to_leaf(input_dict, prefix)
+        if leaf_dict is None:
+            continue
+        value = getattr(leaf_dict, gather)(prefix[-1], None)
+        if value is None:
+            continue
+        leaf_dict = build_to_leaf(output_dict, prefix)
+        leaf_dict[prefix[-1]] = value
+    return output_dict
+
+
+def get_file_size(size_in_bytes):
+    """Returns a human readable string of the size of a file. Given size_in_bytes arg
+    is designed as the output of os.path.getsize().
+
+    1 KB = 1024 bytes
+    1 MB = 1024*1024 bytes
+    1 GB = 1024*1024*1024 bytes
+
+    Args:
+        size_in_bytes (int): Size in bytes [B]
+
+    Returns:
+        str: Human readable file size
+    """
+    kb = 1024
+    mb = 1024 * 1024
+    gb = 1024 * 1024 * 1024
+    if size_in_bytes <= kb:
+        return str(size_in_bytes) + " B"
+    if kb < size_in_bytes <= mb:
+        return str(round(size_in_bytes / kb, 1)) + " KB"
+    elif mb < size_in_bytes < gb:
+        return str(round(size_in_bytes / mb, 1)) + " MB"
+    else:
+        return str(round(size_in_bytes / gb, 1)) + " GB"
+
+
+class PartCount:
+    def __init__(self):
+        self._count = 0
+
+    def __iadd__(self, number):
+        self._count += number
+        return self
+
+    def __eq__(self, number):
+        return self._count == number
+
+    def __deepcopy__(self, memo):
+        return self
+
+    def __repr__(self):
+        return str(self._count)
```

### Comparing `spine_engine-0.23.3/spine_engine/utils/returning_process.py` & `spine_engine-0.23.4/spine_engine/utils/returning_process.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,104 +1,104 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-The ReturningProcess class.
-
-"""
-
-import multiprocessing as mp
-import threading
-from contextlib import contextmanager
-from enum import Enum, auto, unique
-from .execution_resources import one_shot_process_semaphore
-
-
-class ReturningProcess(mp.Process):
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        self._queue = mp.Queue()
-        self._terminated = False
-        self.maybe_idle = _MaybeIdle()  # target functions should enter this context whenever they may become idle
-
-    def run_until_complete(self):
-        """Starts the process and joins it after it has finished.
-
-        Returns:
-            tuple: Return value of the process where the first element is a status flag
-        """
-        with one_shot_process_semaphore:
-            if self._terminated:
-                return (False,)
-            with self.maybe_idle.listen():
-                self.start()
-                return_value = self._queue.get()
-                self.join()
-            return return_value
-
-    def run(self):
-        if not self._target:
-            self._queue.put((False,))
-            return
-        return_value = self._target(self, *self._args, **self._kwargs)
-        self._queue.put(return_value)
-
-    def terminate(self):
-        self._terminated = True
-        if self.is_alive():
-            super().terminate()
-        self._queue.put((False,))
-
-
-class _MaybeIdle:
-    """A context manager to temporarily release the one shot process semaphore.
-    This allows us to prevent deadlocks when processes need to wait for others to do certain actions in order to
-    proceed (e.g., when executable items need to write to a DB in a certain order and we can't predict in which order
-    their processes will start).
-    """
-
-    @unique
-    class _Event(Enum):
-        ENTER = auto()
-        EXIT = auto()
-
-    def __init__(self):
-        self._queue = mp.Queue()
-
-    def __enter__(self):
-        self._queue.put(self._Event.ENTER)
-
-    def __exit__(self, exc_type, exc_val, exc_tb):
-        self._queue.put(self._Event.EXIT)
-
-    @contextmanager
-    def listen(self):
-        thread = threading.Thread(target=self._do_listen)
-        thread.start()
-        try:
-            yield
-        finally:
-            self._queue.put(None)
-            thread.join()
-
-    def _do_listen(self):
-        while True:
-            event = self._queue.get()
-            if event == self._Event.ENTER:
-                one_shot_process_semaphore.release()
-            elif event == self._Event.EXIT:
-                one_shot_process_semaphore.acquire()
-            elif event is None:
-                break
-
-
-if __name__ == "__main__":
-    # https://docs.python.org/3.7/library/multiprocessing.html?highlight=freeze_support#multiprocessing.freeze_support
-    mp.freeze_support()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+The ReturningProcess class.
+
+"""
+
+import multiprocessing as mp
+import threading
+from contextlib import contextmanager
+from enum import Enum, auto, unique
+from .execution_resources import one_shot_process_semaphore
+
+
+class ReturningProcess(mp.Process):
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self._queue = mp.Queue()
+        self._terminated = False
+        self.maybe_idle = _MaybeIdle()  # target functions should enter this context whenever they may become idle
+
+    def run_until_complete(self):
+        """Starts the process and joins it after it has finished.
+
+        Returns:
+            tuple: Return value of the process where the first element is a status flag
+        """
+        with one_shot_process_semaphore:
+            if self._terminated:
+                return (False,)
+            with self.maybe_idle.listen():
+                self.start()
+                return_value = self._queue.get()
+                self.join()
+            return return_value
+
+    def run(self):
+        if not self._target:
+            self._queue.put((False,))
+            return
+        return_value = self._target(self, *self._args, **self._kwargs)
+        self._queue.put(return_value)
+
+    def terminate(self):
+        self._terminated = True
+        if self.is_alive():
+            super().terminate()
+        self._queue.put((False,))
+
+
+class _MaybeIdle:
+    """A context manager to temporarily release the one shot process semaphore.
+    This allows us to prevent deadlocks when processes need to wait for others to do certain actions in order to
+    proceed (e.g., when executable items need to write to a DB in a certain order and we can't predict in which order
+    their processes will start).
+    """
+
+    @unique
+    class _Event(Enum):
+        ENTER = auto()
+        EXIT = auto()
+
+    def __init__(self):
+        self._queue = mp.Queue()
+
+    def __enter__(self):
+        self._queue.put(self._Event.ENTER)
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        self._queue.put(self._Event.EXIT)
+
+    @contextmanager
+    def listen(self):
+        thread = threading.Thread(target=self._do_listen)
+        thread.start()
+        try:
+            yield
+        finally:
+            self._queue.put(None)
+            thread.join()
+
+    def _do_listen(self):
+        while True:
+            event = self._queue.get()
+            if event == self._Event.ENTER:
+                one_shot_process_semaphore.release()
+            elif event == self._Event.EXIT:
+                one_shot_process_semaphore.acquire()
+            elif event is None:
+                break
+
+
+if __name__ == "__main__":
+    # https://docs.python.org/3.7/library/multiprocessing.html?highlight=freeze_support#multiprocessing.freeze_support
+    mp.freeze_support()
```

### Comparing `spine_engine-0.23.3/spine_engine/utils/serialization.py` & `spine_engine-0.23.4/spine_engine/utils/serialization.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,144 +1,144 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Functions to (de)serialize stuff.
-
-"""
-
-import os
-import sys
-import urllib
-from pathlib import Path
-from urllib.parse import urljoin
-
-
-def path_in_dir(path, directory):
-    """Returns True if the given path is in the given directory.
-
-    Args:
-        path (str): path to test
-        directory (str): directory to test against
-
-    Returns:
-        bool: True if path is in directory, False otherwise
-    """
-    path = Path(path)
-    directory = Path(directory)
-    if path.drive.casefold() != directory.drive.casefold():
-        return False
-    return Path(os.path.commonpath((path, directory))) == Path(directory)
-
-
-def serialize_path(path, project_dir):
-    """
-    Returns a dict representation of the given path.
-
-    If path is in project_dir, converts the path to relative.
-
-    Args:
-        path (str): path to serialize
-        project_dir (str): path to the project directory
-
-    Returns:
-        dict: Dictionary representing the given path
-    """
-    is_relative = path_in_dir(path, project_dir)
-    serialized = {
-        "type": "path",
-        "relative": is_relative,
-        "path": os.path.relpath(path, project_dir).replace(os.sep, "/") if is_relative else path.replace(os.sep, "/"),
-    }
-    return serialized
-
-
-def serialize_url(url, project_dir):
-    """
-    Return a dict representation of the given URL.
-
-    If the URL is a file that is in project dir, the URL is converted to a relative path.
-
-    Args:
-        url (str): a URL to serialize
-        project_dir (str): path to the project directory
-
-    Returns:
-        dict: Dictionary representing the URL
-    """
-    parsed = urllib.parse.urlparse(url)
-    path = urllib.parse.unquote(parsed.path)
-    if sys.platform == "win32":
-        path = path[1:]  # Remove extra '/' from the beginning
-    if os.path.isfile(path):
-        is_relative = path_in_dir(path, project_dir)
-        serialized = {
-            "type": "file_url",
-            "relative": is_relative,
-            "path": os.path.relpath(path, project_dir).replace(os.sep, "/")
-            if is_relative
-            else path.replace(os.sep, "/"),
-            "scheme": parsed.scheme,
-        }
-        if parsed.query:
-            serialized["query"] = parsed.query
-    else:
-        serialized = {"type": "url", "relative": False, "path": url}
-    return serialized
-
-
-def deserialize_path(serialized, project_dir):
-    """
-    Returns a deserialized path or URL.
-
-    Args:
-        serialized (dict): a serialized path or URL
-        project_dir (str): path to the project directory
-
-    Returns:
-        str: Path or URL as string
-    """
-    if not isinstance(serialized, dict):
-        return serialized
-    try:
-        path_type = serialized["type"]
-        if path_type == "path":
-            path = serialized["path"]
-            return os.path.normpath(os.path.join(project_dir, path) if serialized["relative"] else path)
-        if path_type == "file_url":
-            path = serialized["path"]
-            if serialized["relative"]:
-                path = os.path.join(project_dir, path)
-            path = os.path.normpath(path)
-            query = serialized.get("query", "")
-            if query:
-                query = "?" + query
-            return serialized["scheme"] + ":///" + path + query
-        if path_type == "url":
-            return serialized["path"]
-    except KeyError as error:
-        raise RuntimeError(f"Key '{error}' missing from serialized path")
-    raise RuntimeError(f"Cannot deserialize: unknown path type '{path_type}'")
-
-
-def deserialize_remote_path(serialized, base_path):
-    if not isinstance(serialized, dict):
-        return serialized
-    try:
-        path_type = serialized["type"]
-        if path_type != "path":
-            raise RuntimeError(f"Cannot deserialize remote path type '{path_type}'")
-        relative = serialized["relative"]
-        if not relative:
-            raise RuntimeError("Cannot deserialize non-relative remote path")
-        path = serialized["path"]
-        return urljoin(base_path, path)
-    except KeyError as error:
-        raise RuntimeError(f"Key '{error}' missing from serialized url")
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Functions to (de)serialize stuff.
+
+"""
+
+import os
+import sys
+import urllib
+from pathlib import Path
+from urllib.parse import urljoin
+
+
+def path_in_dir(path, directory):
+    """Returns True if the given path is in the given directory.
+
+    Args:
+        path (str): path to test
+        directory (str): directory to test against
+
+    Returns:
+        bool: True if path is in directory, False otherwise
+    """
+    path = Path(path)
+    directory = Path(directory)
+    if path.drive.casefold() != directory.drive.casefold():
+        return False
+    return Path(os.path.commonpath((path, directory))) == Path(directory)
+
+
+def serialize_path(path, project_dir):
+    """
+    Returns a dict representation of the given path.
+
+    If path is in project_dir, converts the path to relative.
+
+    Args:
+        path (str): path to serialize
+        project_dir (str): path to the project directory
+
+    Returns:
+        dict: Dictionary representing the given path
+    """
+    is_relative = path_in_dir(path, project_dir)
+    serialized = {
+        "type": "path",
+        "relative": is_relative,
+        "path": os.path.relpath(path, project_dir).replace(os.sep, "/") if is_relative else path.replace(os.sep, "/"),
+    }
+    return serialized
+
+
+def serialize_url(url, project_dir):
+    """
+    Return a dict representation of the given URL.
+
+    If the URL is a file that is in project dir, the URL is converted to a relative path.
+
+    Args:
+        url (str): a URL to serialize
+        project_dir (str): path to the project directory
+
+    Returns:
+        dict: Dictionary representing the URL
+    """
+    parsed = urllib.parse.urlparse(url)
+    path = urllib.parse.unquote(parsed.path)
+    if sys.platform == "win32":
+        path = path[1:]  # Remove extra '/' from the beginning
+    if os.path.isfile(path):
+        is_relative = path_in_dir(path, project_dir)
+        serialized = {
+            "type": "file_url",
+            "relative": is_relative,
+            "path": os.path.relpath(path, project_dir).replace(os.sep, "/")
+            if is_relative
+            else path.replace(os.sep, "/"),
+            "scheme": parsed.scheme,
+        }
+        if parsed.query:
+            serialized["query"] = parsed.query
+    else:
+        serialized = {"type": "url", "relative": False, "path": url}
+    return serialized
+
+
+def deserialize_path(serialized, project_dir):
+    """
+    Returns a deserialized path or URL.
+
+    Args:
+        serialized (dict): a serialized path or URL
+        project_dir (str): path to the project directory
+
+    Returns:
+        str: Path or URL as string
+    """
+    if not isinstance(serialized, dict):
+        return serialized
+    try:
+        path_type = serialized["type"]
+        if path_type == "path":
+            path = serialized["path"]
+            return os.path.normpath(os.path.join(project_dir, path) if serialized["relative"] else path)
+        if path_type == "file_url":
+            path = serialized["path"]
+            if serialized["relative"]:
+                path = os.path.join(project_dir, path)
+            path = os.path.normpath(path)
+            query = serialized.get("query", "")
+            if query:
+                query = "?" + query
+            return serialized["scheme"] + ":///" + path + query
+        if path_type == "url":
+            return serialized["path"]
+    except KeyError as error:
+        raise RuntimeError(f"Key '{error}' missing from serialized path")
+    raise RuntimeError(f"Cannot deserialize: unknown path type '{path_type}'")
+
+
+def deserialize_remote_path(serialized, base_path):
+    if not isinstance(serialized, dict):
+        return serialized
+    try:
+        path_type = serialized["type"]
+        if path_type != "path":
+            raise RuntimeError(f"Cannot deserialize remote path type '{path_type}'")
+        relative = serialized["relative"]
+        if not relative:
+            raise RuntimeError("Cannot deserialize non-relative remote path")
+        path = serialized["path"]
+        return urljoin(base_path, path)
+    except KeyError as error:
+        raise RuntimeError(f"Key '{error}' missing from serialized url")
```

### Comparing `spine_engine-0.23.3/spine_engine.egg-info/PKG-INFO` & `spine_engine-0.23.4/spine_engine.egg-info/PKG-INFO`

 * *Files 12% similar despite different names*

```diff
@@ -1,74 +1,74 @@
-Metadata-Version: 2.1
-Name: spine_engine
-Version: 0.23.3
-Summary: A package to run Spine workflows.
-Author-email: Spine Project consortium <spine_info@vtt.fi>
-License: LGPL-3.0-or-later
-Project-URL: Repository, https://github.com/spine-tools/spine-engine
-Keywords: energy system modelling,workflow,optimisation,database
-Classifier: Programming Language :: Python :: 3
-Classifier: License :: OSI Approved :: GNU Lesser General Public License v3 (LGPLv3)
-Classifier: Operating System :: OS Independent
-Requires-Python: <3.12,>=3.8.1
-Description-Content-Type: text/markdown
-License-File: COPYING
-License-File: COPYING.LESSER
-Requires-Dist: dagster<0.12.9,>=0.12.6
-Requires-Dist: pendulum<3.0.0
-Requires-Dist: protobuf<3.21.0
-Requires-Dist: networkx>2.5.1
-Requires-Dist: datapackage<1.16,>=1.15.2
-Requires-Dist: jupyter_client>=6.0
-Requires-Dist: spinedb_api>=0.30.3
-Requires-Dist: pyzmq>=21.0
-Requires-Dist: markupsafe<2.1
-Provides-Extra: dev
-Requires-Dist: coverage[toml]; extra == "dev"
-
-# Spine Engine
-
-[![Python](https://img.shields.io/badge/python-3.8%20|%203.9%20|%203.10%20|%203.11-blue.svg)](https://www.python.org/downloads/release/python-379/)
-[![Unit tests](https://github.com/spine-tools/spine-engine/workflows/Unit%20tests/badge.svg)](https://github.com/spine-tools/spine-engine/actions?query=workflow%3A"Unit+tests")
-[![codecov](https://codecov.io/gh/spine-tools/spine-engine/branch/master/graph/badge.svg)](https://codecov.io/gh/spine-tools/spine-engine)
-[![PyPI version](https://badge.fury.io/py/spine-engine.svg)](https://badge.fury.io/py/spine-engine)
-
-A Python package to coordinate the execution of [Spine Toolbox](https://github.com/spine-tools/Spine-Toolbox) workflows.
-
-<p align="center" width="100%">
-  <picture>
-    <source media="(prefers-color-scheme: dark)" srcset="./fig/spineengine_logo.svg" width="50%">
-    <img alt="Spine Engine" src="./fig/spineengine_on_wht.svg" width="50%">
-  </picture>
-</p>
-
-## License
-
-Spine Engine is released under the GNU Lesser General Public License (LGPL) license. All accompanying
-documentation and manual are released under the [Creative Commons BY-SA 4.0 license](https://creativecommons.org/licenses/by-sa/4.0/).
-
-## Getting started
-
-### Installation
-
-To install Spine Engine into an existing Python environment, run
-
-    $ pip install spine_engine
-
-### Dependencies
-
-Spine Engine installation will install [dagster](https://dagster.readthedocs.io/en/master/index.html).
-
-&nbsp;
-<hr>
-<center>
-<table width=500px frame="none">
-<tr>
-<td valign="middle" width=100px>
-<img src=fig/eu-emblem-low-res.jpg alt="EU emblem" width=100%></td>
-<td valign="middle">This project has received funding from European Climate, Infrastructure and Environment Executive Agency under the European Union’s HORIZON Research and Innovation Actions under grant agreement N°101095998.</td>
-<tr>
-<td valign="middle" width=100px>
-<img src=fig/eu-emblem-low-res.jpg alt="EU emblem" width=100%></td>
-<td valign="middle">This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 774629.</td>
-</table>
-</center>
+Metadata-Version: 2.1
+Name: spine_engine
+Version: 0.23.4
+Summary: A package to run Spine workflows.
+Author-email: Spine Project consortium <spine_info@vtt.fi>
+License: LGPL-3.0-or-later
+Project-URL: Repository, https://github.com/spine-tools/spine-engine
+Keywords: energy system modelling,workflow,optimisation,database
+Classifier: Programming Language :: Python :: 3
+Classifier: License :: OSI Approved :: GNU Lesser General Public License v3 (LGPLv3)
+Classifier: Operating System :: OS Independent
+Requires-Python: <3.12,>=3.8.1
+Description-Content-Type: text/markdown
+License-File: COPYING
+License-File: COPYING.LESSER
+Requires-Dist: dagster<0.12.9,>=0.12.6
+Requires-Dist: pendulum<3.0.0
+Requires-Dist: protobuf<3.21.0
+Requires-Dist: networkx>2.5.1
+Requires-Dist: datapackage<1.16,>=1.15.2
+Requires-Dist: jupyter_client>=6.0
+Requires-Dist: spinedb_api>=0.30.5
+Requires-Dist: pyzmq>=21.0
+Requires-Dist: markupsafe<2.1
+Provides-Extra: dev
+Requires-Dist: coverage[toml]; extra == "dev"
+
+# Spine Engine
+
+[![Python](https://img.shields.io/badge/python-3.8%20|%203.9%20|%203.10%20|%203.11-blue.svg)](https://www.python.org/downloads/release/python-379/)
+[![Unit tests](https://github.com/spine-tools/spine-engine/workflows/Unit%20tests/badge.svg)](https://github.com/spine-tools/spine-engine/actions?query=workflow%3A"Unit+tests")
+[![codecov](https://codecov.io/gh/spine-tools/spine-engine/branch/master/graph/badge.svg)](https://codecov.io/gh/spine-tools/spine-engine)
+[![PyPI version](https://badge.fury.io/py/spine-engine.svg)](https://badge.fury.io/py/spine-engine)
+
+A Python package to coordinate the execution of [Spine Toolbox](https://github.com/spine-tools/Spine-Toolbox) workflows.
+
+<p align="center" width="100%">
+  <picture>
+    <source media="(prefers-color-scheme: dark)" srcset="./fig/spineengine_logo.svg" width="50%">
+    <img alt="Spine Engine" src="./fig/spineengine_on_wht.svg" width="50%">
+  </picture>
+</p>
+
+## License
+
+Spine Engine is released under the GNU Lesser General Public License (LGPL) license. All accompanying
+documentation and manual are released under the [Creative Commons BY-SA 4.0 license](https://creativecommons.org/licenses/by-sa/4.0/).
+
+## Getting started
+
+### Installation
+
+To install Spine Engine into an existing Python environment, run
+
+    $ pip install spine_engine
+
+### Dependencies
+
+Spine Engine installation will install [dagster](https://dagster.readthedocs.io/en/master/index.html).
+
+&nbsp;
+<hr>
+<center>
+<table width=500px frame="none">
+<tr>
+<td valign="middle" width=100px>
+<img src=fig/eu-emblem-low-res.jpg alt="EU emblem" width=100%></td>
+<td valign="middle">This project has received funding from European Climate, Infrastructure and Environment Executive Agency under the European Union’s HORIZON Research and Innovation Actions under grant agreement N°101095998.</td>
+<tr>
+<td valign="middle" width=100px>
+<img src=fig/eu-emblem-low-res.jpg alt="EU emblem" width=100%></td>
+<td valign="middle">This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 774629.</td>
+</table>
+</center>
```

#### html2text {}

```diff
@@ -1,19 +1,19 @@
-Metadata-Version: 2.1 Name: spine_engine Version: 0.23.3 Summary: A package to
+Metadata-Version: 2.1 Name: spine_engine Version: 0.23.4 Summary: A package to
 run Spine workflows. Author-email: Spine Project consortium
 vtt.fi> License: LGPL-3.0-or-later Project-URL: Repository, https://github.com/
 spine-tools/spine-engine Keywords: energy system
 modelling,workflow,optimisation,database Classifier: Programming Language ::
 Python :: 3 Classifier: License :: OSI Approved :: GNU Lesser General Public
 License v3 (LGPLv3) Classifier: Operating System :: OS Independent Requires-
 Python: <3.12,>=3.8.1 Description-Content-Type: text/markdown License-File:
 COPYING License-File: COPYING.LESSER Requires-Dist: dagster<0.12.9,>=0.12.6
 Requires-Dist: pendulum<3.0.0 Requires-Dist: protobuf<3.21.0 Requires-Dist:
 networkx>2.5.1 Requires-Dist: datapackage<1.16,>=1.15.2 Requires-Dist:
-jupyter_client>=6.0 Requires-Dist: spinedb_api>=0.30.3 Requires-Dist:
+jupyter_client>=6.0 Requires-Dist: spinedb_api>=0.30.5 Requires-Dist:
 pyzmq>=21.0 Requires-Dist: markupsafe<2.1 Provides-Extra: dev Requires-Dist:
 coverage[toml]; extra == "dev" # Spine Engine [![Python](https://
 img.shields.io/badge/python-3.8%20|%203.9%20|%203.10%20|%203.11-blue.svg)]
 (https://www.python.org/downloads/release/python-379/) [![Unit tests](https://
 github.com/spine-tools/spine-engine/workflows/Unit%20tests/badge.svg)](https://
 github.com/spine-tools/spine-engine/actions?query=workflow%3A"Unit+tests") [!
 [codecov](https://codecov.io/gh/spine-tools/spine-engine/branch/master/graph/
```

### Comparing `spine_engine-0.23.3/spine_engine.egg-info/SOURCES.txt` & `spine_engine-0.23.4/spine_engine.egg-info/SOURCES.txt`

 * *Files 17% similar despite different names*

```diff
@@ -96,34 +96,14 @@
 spine_engine/server/project_extractor_service.py
 spine_engine/server/project_remover_service.py
 spine_engine/server/project_retriever_service.py
 spine_engine/server/remote_execution_service.py
 spine_engine/server/request.py
 spine_engine/server/service_base.py
 spine_engine/server/start_server.py
-spine_engine/server/received_projects/hello_world_on_server__49e90ea3f66842bf9eb455448c363cdc/__init__.py
-spine_engine/server/received_projects/hello_world_on_server__49e90ea3f66842bf9eb455448c363cdc/execution_test.py
-spine_engine/server/received_projects/hello_world_on_server__49e90ea3f66842bf9eb455448c363cdc/simple_script.py
-spine_engine/server/received_projects/hello_world_on_server__75bf5d45bfea433d9f7e37ee93724313/__init__.py
-spine_engine/server/received_projects/hello_world_on_server__75bf5d45bfea433d9f7e37ee93724313/execution_test.py
-spine_engine/server/received_projects/hello_world_on_server__75bf5d45bfea433d9f7e37ee93724313/simple_script.py
-spine_engine/server/received_projects/hello_world_on_server__772c2f8565a04e439f7e6ccfc383221c/__init__.py
-spine_engine/server/received_projects/hello_world_on_server__772c2f8565a04e439f7e6ccfc383221c/execution_test.py
-spine_engine/server/received_projects/hello_world_on_server__772c2f8565a04e439f7e6ccfc383221c/simple_script.py
-spine_engine/server/received_projects/hello_world_on_server__8421db3482fc4ce5aa4fb5e1e1212874/__init__.py
-spine_engine/server/received_projects/hello_world_on_server__8421db3482fc4ce5aa4fb5e1e1212874/execution_test.py
-spine_engine/server/received_projects/hello_world_on_server__8421db3482fc4ce5aa4fb5e1e1212874/simple_script.py
-spine_engine/server/received_projects/hello_world_on_server__aabd50f3fb9541c7806fafb9b56578cf/__init__.py
-spine_engine/server/received_projects/hello_world_on_server__aabd50f3fb9541c7806fafb9b56578cf/execution_test.py
-spine_engine/server/received_projects/hello_world_on_server__aabd50f3fb9541c7806fafb9b56578cf/simple_script.py
-spine_engine/server/received_projects/hello_world_on_server__d8b047f1708b40a2a28d580737de834c/__init__.py
-spine_engine/server/received_projects/hello_world_on_server__d8b047f1708b40a2a28d580737de834c/execution_test.py
-spine_engine/server/received_projects/hello_world_on_server__d8b047f1708b40a2a28d580737de834c/simple_script.py
-spine_engine/server/received_projects/simple_importer_on_server__b1a6cf81eded41138838387d6a0dca62/__init__.py
-spine_engine/server/received_projects/simple_importer_on_server__b1a6cf81eded41138838387d6a0dca62/execution_test.py
 spine_engine/server/util/__init__.py
 spine_engine/server/util/event_data_converter.py
 spine_engine/server/util/server_message.py
 spine_engine/server/util/zip_handler.py
 spine_engine/utils/__init__.py
 spine_engine/utils/command_line_arguments.py
 spine_engine/utils/execution_resources.py
```

### Comparing `spine_engine-0.23.3/tests/__init__.py` & `spine_engine-0.23.4/tests/utils/__init__.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,15 +1,15 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Init file for tests package. Intentionally empty.
-
-"""
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Init file for utils package. Intentionally empty.
+
+"""
```

### Comparing `spine_engine-0.23.3/tests/execution_managers/__init__.py` & `spine_engine-0.23.4/tests/project_item/__init__.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,11 +1,15 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-"""Unit tests for ``execution_managers`` package."""
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Init file for tests.project_item package. Intentionally empty.
+
+"""
```

### Comparing `spine_engine-0.23.3/tests/execution_managers/test_kernel_execution_manager.py` & `spine_engine-0.23.4/tests/execution_managers/test_kernel_execution_manager.py`

 * *Ordering differences only*

 * *Files 9% similar despite different names*

```diff
@@ -1,252 +1,252 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-"""Unit tests for ``kernel_execution_manager`` module."""
-import os
-import unittest
-import tempfile
-from unittest.mock import MagicMock
-from spine_engine.execution_managers.kernel_execution_manager import KernelExecutionManager, _kernel_manager_factory
-from jupyter_client.kernelspec import NATIVE_KERNEL_NAME  # =='python3'
-
-
-class TestKernelExecutionManager(unittest.TestCase):
-    @staticmethod
-    def release_exec_mngr_resources(mngr):
-        """Frees resources after exec_mngr has been used. Consider putting
-        this to KernelExecutionManager.close() or something."""
-        if not mngr._kernel_client.context.closed:
-            mngr._kernel_client.context.term()  # ResourceWarning: Unclosed <zmq.Context() happens without this
-        mngr.std_out.close()  # Prevents ResourceWarning: unclosed file <_io.TextIOWrapper name='nul' ...
-        mngr.std_err.close()  # Prevents ResourceWarning: unclosed file <_io.TextIOWrapper name='nul' ...
-
-    def test_kernel_execution_manager(self):
-        logger = MagicMock()
-        logger.msg_kernel_execution.filter_id = ""
-        with tempfile.NamedTemporaryFile("w+", encoding="utf-8") as script_file:
-            script_file.write('print("hello")')
-            script_file.seek(0)
-            d, fname = os.path.split(script_file.name)
-            cmds = [f"%cd -q {d}", f"%run {fname}"]
-            # exec_mngr represents the manager on spine-items side
-            exec_mngr = KernelExecutionManager(logger, NATIVE_KERNEL_NAME, *cmds, group_id="SomeGroup")
-            self.assertTrue(exec_mngr._kernel_manager.is_alive())
-            exec_mngr = self.replace_client(exec_mngr)
-            retval = exec_mngr.run_until_complete()  # Run commands
-            self.assertEqual(0, retval)
-            self.assertTrue(exec_mngr._kernel_manager.is_alive())
-            connection_file = exec_mngr._kernel_manager.connection_file
-            self.release_exec_mngr_resources(exec_mngr)
-            self.assertEqual(1, _kernel_manager_factory.n_kernel_managers())
-            _kernel_manager_factory.shutdown_kernel_manager(connection_file)
-            self.assertEqual(0, _kernel_manager_factory.n_kernel_managers())
-            (
-                kernel_started_messages,
-                stdin_messages,
-                execution_started_messages,
-                kernel_shutdown_messages,
-            ) = self._collect_messages_per_type(logger)
-            self.assertEqual(len(kernel_started_messages), 1)
-            self.assertEqual(kernel_started_messages[0]["kernel_name"], NATIVE_KERNEL_NAME)
-            self.assertEqual(len(stdin_messages), 2)
-            expected_msg = {"type": "execution_started", "kernel_name": NATIVE_KERNEL_NAME}
-            self.assertEqual([expected_msg], execution_started_messages)
-            self.assertEqual(len(kernel_shutdown_messages), 0)
-
-    def test_kernel_execution_manager_kill_completed(self):
-        logger = MagicMock()
-        logger.msg_kernel_execution.filter_id = ""
-        with tempfile.NamedTemporaryFile("w+", encoding="utf-8") as script_file:
-            script_file.write('print("hello")')
-            script_file.seek(0)
-            d, fname = os.path.split(script_file.name)
-            cmds = [f"%cd -q {d}", f"%run {fname}"]
-            # exec_mngr represents the manager on spine-items side
-            exec_mngr = KernelExecutionManager(logger, NATIVE_KERNEL_NAME, *cmds, kill_completed=True, group_id="a")
-            self.assertTrue(exec_mngr._kernel_manager.is_alive())
-            exec_mngr = self.replace_client(exec_mngr)
-            retval = exec_mngr.run_until_complete()  # Run commands
-            self.assertEqual(0, retval)
-            self.assertFalse(exec_mngr._kernel_manager.is_alive())
-            self.release_exec_mngr_resources(exec_mngr)
-            self.assertEqual(0, _kernel_manager_factory.n_kernel_managers())
-            (
-                kernel_started_messages,
-                stdin_messages,
-                execution_started_messages,
-                kernel_shutdown_messages,
-            ) = self._collect_messages_per_type(logger)
-            self.assertEqual(len(kernel_started_messages), 1)
-            self.assertEqual(kernel_started_messages[0]["kernel_name"], NATIVE_KERNEL_NAME)
-            self.assertEqual(len(stdin_messages), 2)
-            expected_msg = {"type": "execution_started", "kernel_name": NATIVE_KERNEL_NAME}
-            self.assertEqual([expected_msg], execution_started_messages)
-            self.assertEqual(len(kernel_shutdown_messages), 1)
-            self.assertEqual(kernel_shutdown_messages[0]["kernel_name"], NATIVE_KERNEL_NAME)
-
-    def test_kernel_manager_sharing(self):
-        logger1 = MagicMock()
-        logger1.msg_kernel_execution.filter_id = ""
-        logger2 = MagicMock()
-        logger2.msg_kernel_execution.filter_id = ""
-        with tempfile.NamedTemporaryFile("w+", encoding="utf-8") as script_file1, tempfile.NamedTemporaryFile(
-            "w+", encoding="utf-8"
-        ) as script_file2:
-            script_file1.write('print("hello")')
-            script_file1.seek(0)
-            script_file2.write('print("hello again")')
-            script_file2.seek(0)
-            d1, fname1 = os.path.split(script_file1.name)
-            d2, fname2 = os.path.split(script_file2.name)
-            exec_mngr1_cmds = [f"%cd -q {d1}", f"%run {fname1}"]
-            exec_mngr2_cmds = [f"%cd -q {d2}", f"%run {fname2}"]
-            kernel_name = NATIVE_KERNEL_NAME
-            exec_mngr1 = KernelExecutionManager(logger1, kernel_name, *exec_mngr1_cmds, group_id="SomeGroup")
-            exec_mngr1 = self.replace_client(exec_mngr1)
-            retval1 = exec_mngr1.run_until_complete()  # Run commands
-            self.assertEqual(0, retval1)
-            exec_mngr2 = KernelExecutionManager(logger2, kernel_name, *exec_mngr2_cmds, group_id="SomeGroup")
-            exec_mngr2 = self.replace_client(exec_mngr2)
-            self.assertEqual(1, _kernel_manager_factory.n_kernel_managers())
-            self.assertEqual(exec_mngr1._kernel_manager, exec_mngr2._kernel_manager)
-            retval2 = exec_mngr2.run_until_complete()  # Run commands
-            self.assertEqual(0, retval2)
-            # Close
-            self.release_exec_mngr_resources(exec_mngr1)
-            self.release_exec_mngr_resources(exec_mngr2)
-            _kernel_manager_factory.kill_kernel_managers()
-            self.assertEqual(0, _kernel_manager_factory.n_kernel_managers())
-            # Check emitted messages
-            (
-                kernel_started_messages,
-                stdin_messages,
-                execution_started_messages,
-                kernel_shutdown_messages,
-            ) = self._collect_messages_per_type(logger1)
-            self.assertEqual(len(kernel_started_messages), 1)
-            self.assertEqual(kernel_started_messages[0]["kernel_name"], NATIVE_KERNEL_NAME)
-            self.assertEqual(len(stdin_messages), 2)
-            expected_msg = {"type": "execution_started", "kernel_name": NATIVE_KERNEL_NAME}
-            self.assertEqual([expected_msg], execution_started_messages)
-            self.assertEqual(len(kernel_shutdown_messages), 0)
-            (
-                kernel_started_messages,
-                stdin_messages,
-                execution_started_messages,
-                kernel_shutdown_messages,
-            ) = self._collect_messages_per_type(logger2)
-            self.assertEqual(len(kernel_started_messages), 1)
-            self.assertEqual(kernel_started_messages[0]["kernel_name"], NATIVE_KERNEL_NAME)
-            self.assertEqual(len(stdin_messages), 2)
-            expected_msg = {"type": "execution_started", "kernel_name": NATIVE_KERNEL_NAME}
-            self.assertEqual([expected_msg], execution_started_messages)
-            self.assertEqual(len(kernel_shutdown_messages), 0)
-
-    def test_two_kernel_managers(self):
-        logger1 = MagicMock()
-        logger1.msg_kernel_execution.filter_id = ""
-        logger2 = MagicMock()
-        logger2.msg_kernel_execution.filter_id = ""
-        with tempfile.NamedTemporaryFile("w+", encoding="utf-8") as script_file1, tempfile.NamedTemporaryFile(
-            "w+", encoding="utf-8"
-        ) as script_file2:
-            script_file1.write('print("hello")')
-            script_file1.seek(0)
-            script_file2.write('print("hello again")')
-            script_file2.seek(0)
-            d1, fname1 = os.path.split(script_file1.name)
-            d2, fname2 = os.path.split(script_file2.name)
-            exec_mngr1_cmds = [f"%cd -q {d1}", f"%run {fname1}"]
-            exec_mngr2_cmds = [f"%cd -q {d2}", f"%run {fname2}"]
-            kernel_name = NATIVE_KERNEL_NAME
-            exec_mngr1 = KernelExecutionManager(logger1, kernel_name, *exec_mngr1_cmds, group_id="SomeGroup")
-            exec_mngr1 = self.replace_client(exec_mngr1)
-            retval1 = exec_mngr1.run_until_complete()  # Run commands
-            self.assertEqual(0, retval1)
-            exec_mngr2 = KernelExecutionManager(logger2, kernel_name, *exec_mngr2_cmds, group_id="AnotherGroup")
-            exec_mngr2 = self.replace_client(exec_mngr2)
-            self.assertEqual(2, _kernel_manager_factory.n_kernel_managers())
-            self.assertNotEqual(exec_mngr1._kernel_manager, exec_mngr2._kernel_manager)
-            retval2 = exec_mngr2.run_until_complete()  # Run commands
-            self.assertEqual(0, retval2)
-            # Close
-            self.release_exec_mngr_resources(exec_mngr1)
-            self.release_exec_mngr_resources(exec_mngr2)
-            _kernel_manager_factory.kill_kernel_managers()
-            self.assertEqual(0, _kernel_manager_factory.n_kernel_managers())
-            # Check emitted messages
-            (
-                kernel_started_messages,
-                stdin_messages,
-                execution_started_messages,
-                kernel_shutdown_messages,
-            ) = self._collect_messages_per_type(logger1)
-            self.assertEqual(len(kernel_started_messages), 1)
-            self.assertEqual(kernel_started_messages[0]["kernel_name"], NATIVE_KERNEL_NAME)
-            self.assertEqual(len(stdin_messages), 2)
-            expected_msg = {"type": "execution_started", "kernel_name": NATIVE_KERNEL_NAME}
-            self.assertEqual([expected_msg], execution_started_messages)
-            self.assertEqual(len(kernel_shutdown_messages), 0)
-            (
-                kernel_started_messages,
-                stdin_messages,
-                execution_started_messages,
-                kernel_shutdown_messages,
-            ) = self._collect_messages_per_type(logger2)
-            self.assertEqual(len(kernel_started_messages), 1)
-            self.assertEqual(kernel_started_messages[0]["kernel_name"], NATIVE_KERNEL_NAME)
-            self.assertEqual(len(stdin_messages), 2)
-            expected_msg = {"type": "execution_started", "kernel_name": NATIVE_KERNEL_NAME}
-            self.assertEqual([expected_msg], execution_started_messages)
-            self.assertEqual(len(kernel_shutdown_messages), 0)
-
-    @staticmethod
-    def replace_client(exec_mngr):
-        """Reloads the connection file, and replaces the kernel client with a new one, just like
-        we do on Toolbox side. Don't really understand why we need to do this, but I think
-        it's because of the 'classic' race condition in jupyter-client < 7.0.
-        This maybe fixed in a more recent version of jupyter-client."""
-        # exec_mngr._kernel_client.stop_channels()
-        # exec_mngr._kernel_client.context.term()  # ResourceWarning: Unclosed <zmq.Context() happens without this
-        exec_mngr._kernel_manager.load_connection_file()
-        kc = exec_mngr._kernel_manager.client()  # Make new client
-        # kc.start_channels()
-        exec_mngr._kernel_client = kc  # Replace the original client
-        return exec_mngr
-
-    def _collect_messages_per_type(self, logger):
-        stdin_messages = []
-        execution_started_messages = []
-        kernel_started_messages = []
-        kernel_shutdown_messages = []
-        for i, call in enumerate(logger.msg_kernel_execution.emit.call_args_list):
-            with self.subTest(call_number=i):
-                self.assertEqual(len(call.kwargs), 0)
-                self.assertEqual(len(call.args), 1)
-            message = call.args[0]
-            msg_type = message["type"]
-            if msg_type == "stdin":
-                stdin_messages.append(message)
-            elif msg_type == "execution_started":
-                execution_started_messages.append(message)
-            elif msg_type == "stderr":
-                # UserWarnings are harmless, everything else is suspicious
-                self.assertIn("UserWarning", message["data"])
-            elif msg_type == "kernel_started":
-                kernel_started_messages.append(message)
-            elif msg_type == "kernel_shutdown":
-                kernel_shutdown_messages.append(message)
-            elif msg_type == "stdout":
-                # On Linux, script file output is echoed in stdout
-                # This doesn't happen on Windows for some reason.
-                pass
-            else:
-                self.fail(f"unexpected message type {msg_type}")
-        return kernel_started_messages, stdin_messages, execution_started_messages, kernel_shutdown_messages
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+"""Unit tests for ``kernel_execution_manager`` module."""
+import os
+import unittest
+import tempfile
+from unittest.mock import MagicMock
+from spine_engine.execution_managers.kernel_execution_manager import KernelExecutionManager, _kernel_manager_factory
+from jupyter_client.kernelspec import NATIVE_KERNEL_NAME  # =='python3'
+
+
+class TestKernelExecutionManager(unittest.TestCase):
+    @staticmethod
+    def release_exec_mngr_resources(mngr):
+        """Frees resources after exec_mngr has been used. Consider putting
+        this to KernelExecutionManager.close() or something."""
+        if not mngr._kernel_client.context.closed:
+            mngr._kernel_client.context.term()  # ResourceWarning: Unclosed <zmq.Context() happens without this
+        mngr.std_out.close()  # Prevents ResourceWarning: unclosed file <_io.TextIOWrapper name='nul' ...
+        mngr.std_err.close()  # Prevents ResourceWarning: unclosed file <_io.TextIOWrapper name='nul' ...
+
+    def test_kernel_execution_manager(self):
+        logger = MagicMock()
+        logger.msg_kernel_execution.filter_id = ""
+        with tempfile.NamedTemporaryFile("w+", encoding="utf-8") as script_file:
+            script_file.write('print("hello")')
+            script_file.seek(0)
+            d, fname = os.path.split(script_file.name)
+            cmds = [f"%cd -q {d}", f"%run {fname}"]
+            # exec_mngr represents the manager on spine-items side
+            exec_mngr = KernelExecutionManager(logger, NATIVE_KERNEL_NAME, *cmds, group_id="SomeGroup")
+            self.assertTrue(exec_mngr._kernel_manager.is_alive())
+            exec_mngr = self.replace_client(exec_mngr)
+            retval = exec_mngr.run_until_complete()  # Run commands
+            self.assertEqual(0, retval)
+            self.assertTrue(exec_mngr._kernel_manager.is_alive())
+            connection_file = exec_mngr._kernel_manager.connection_file
+            self.release_exec_mngr_resources(exec_mngr)
+            self.assertEqual(1, _kernel_manager_factory.n_kernel_managers())
+            _kernel_manager_factory.shutdown_kernel_manager(connection_file)
+            self.assertEqual(0, _kernel_manager_factory.n_kernel_managers())
+            (
+                kernel_started_messages,
+                stdin_messages,
+                execution_started_messages,
+                kernel_shutdown_messages,
+            ) = self._collect_messages_per_type(logger)
+            self.assertEqual(len(kernel_started_messages), 1)
+            self.assertEqual(kernel_started_messages[0]["kernel_name"], NATIVE_KERNEL_NAME)
+            self.assertEqual(len(stdin_messages), 2)
+            expected_msg = {"type": "execution_started", "kernel_name": NATIVE_KERNEL_NAME}
+            self.assertEqual([expected_msg], execution_started_messages)
+            self.assertEqual(len(kernel_shutdown_messages), 0)
+
+    def test_kernel_execution_manager_kill_completed(self):
+        logger = MagicMock()
+        logger.msg_kernel_execution.filter_id = ""
+        with tempfile.NamedTemporaryFile("w+", encoding="utf-8") as script_file:
+            script_file.write('print("hello")')
+            script_file.seek(0)
+            d, fname = os.path.split(script_file.name)
+            cmds = [f"%cd -q {d}", f"%run {fname}"]
+            # exec_mngr represents the manager on spine-items side
+            exec_mngr = KernelExecutionManager(logger, NATIVE_KERNEL_NAME, *cmds, kill_completed=True, group_id="a")
+            self.assertTrue(exec_mngr._kernel_manager.is_alive())
+            exec_mngr = self.replace_client(exec_mngr)
+            retval = exec_mngr.run_until_complete()  # Run commands
+            self.assertEqual(0, retval)
+            self.assertFalse(exec_mngr._kernel_manager.is_alive())
+            self.release_exec_mngr_resources(exec_mngr)
+            self.assertEqual(0, _kernel_manager_factory.n_kernel_managers())
+            (
+                kernel_started_messages,
+                stdin_messages,
+                execution_started_messages,
+                kernel_shutdown_messages,
+            ) = self._collect_messages_per_type(logger)
+            self.assertEqual(len(kernel_started_messages), 1)
+            self.assertEqual(kernel_started_messages[0]["kernel_name"], NATIVE_KERNEL_NAME)
+            self.assertEqual(len(stdin_messages), 2)
+            expected_msg = {"type": "execution_started", "kernel_name": NATIVE_KERNEL_NAME}
+            self.assertEqual([expected_msg], execution_started_messages)
+            self.assertEqual(len(kernel_shutdown_messages), 1)
+            self.assertEqual(kernel_shutdown_messages[0]["kernel_name"], NATIVE_KERNEL_NAME)
+
+    def test_kernel_manager_sharing(self):
+        logger1 = MagicMock()
+        logger1.msg_kernel_execution.filter_id = ""
+        logger2 = MagicMock()
+        logger2.msg_kernel_execution.filter_id = ""
+        with tempfile.NamedTemporaryFile("w+", encoding="utf-8") as script_file1, tempfile.NamedTemporaryFile(
+            "w+", encoding="utf-8"
+        ) as script_file2:
+            script_file1.write('print("hello")')
+            script_file1.seek(0)
+            script_file2.write('print("hello again")')
+            script_file2.seek(0)
+            d1, fname1 = os.path.split(script_file1.name)
+            d2, fname2 = os.path.split(script_file2.name)
+            exec_mngr1_cmds = [f"%cd -q {d1}", f"%run {fname1}"]
+            exec_mngr2_cmds = [f"%cd -q {d2}", f"%run {fname2}"]
+            kernel_name = NATIVE_KERNEL_NAME
+            exec_mngr1 = KernelExecutionManager(logger1, kernel_name, *exec_mngr1_cmds, group_id="SomeGroup")
+            exec_mngr1 = self.replace_client(exec_mngr1)
+            retval1 = exec_mngr1.run_until_complete()  # Run commands
+            self.assertEqual(0, retval1)
+            exec_mngr2 = KernelExecutionManager(logger2, kernel_name, *exec_mngr2_cmds, group_id="SomeGroup")
+            exec_mngr2 = self.replace_client(exec_mngr2)
+            self.assertEqual(1, _kernel_manager_factory.n_kernel_managers())
+            self.assertEqual(exec_mngr1._kernel_manager, exec_mngr2._kernel_manager)
+            retval2 = exec_mngr2.run_until_complete()  # Run commands
+            self.assertEqual(0, retval2)
+            # Close
+            self.release_exec_mngr_resources(exec_mngr1)
+            self.release_exec_mngr_resources(exec_mngr2)
+            _kernel_manager_factory.kill_kernel_managers()
+            self.assertEqual(0, _kernel_manager_factory.n_kernel_managers())
+            # Check emitted messages
+            (
+                kernel_started_messages,
+                stdin_messages,
+                execution_started_messages,
+                kernel_shutdown_messages,
+            ) = self._collect_messages_per_type(logger1)
+            self.assertEqual(len(kernel_started_messages), 1)
+            self.assertEqual(kernel_started_messages[0]["kernel_name"], NATIVE_KERNEL_NAME)
+            self.assertEqual(len(stdin_messages), 2)
+            expected_msg = {"type": "execution_started", "kernel_name": NATIVE_KERNEL_NAME}
+            self.assertEqual([expected_msg], execution_started_messages)
+            self.assertEqual(len(kernel_shutdown_messages), 0)
+            (
+                kernel_started_messages,
+                stdin_messages,
+                execution_started_messages,
+                kernel_shutdown_messages,
+            ) = self._collect_messages_per_type(logger2)
+            self.assertEqual(len(kernel_started_messages), 1)
+            self.assertEqual(kernel_started_messages[0]["kernel_name"], NATIVE_KERNEL_NAME)
+            self.assertEqual(len(stdin_messages), 2)
+            expected_msg = {"type": "execution_started", "kernel_name": NATIVE_KERNEL_NAME}
+            self.assertEqual([expected_msg], execution_started_messages)
+            self.assertEqual(len(kernel_shutdown_messages), 0)
+
+    def test_two_kernel_managers(self):
+        logger1 = MagicMock()
+        logger1.msg_kernel_execution.filter_id = ""
+        logger2 = MagicMock()
+        logger2.msg_kernel_execution.filter_id = ""
+        with tempfile.NamedTemporaryFile("w+", encoding="utf-8") as script_file1, tempfile.NamedTemporaryFile(
+            "w+", encoding="utf-8"
+        ) as script_file2:
+            script_file1.write('print("hello")')
+            script_file1.seek(0)
+            script_file2.write('print("hello again")')
+            script_file2.seek(0)
+            d1, fname1 = os.path.split(script_file1.name)
+            d2, fname2 = os.path.split(script_file2.name)
+            exec_mngr1_cmds = [f"%cd -q {d1}", f"%run {fname1}"]
+            exec_mngr2_cmds = [f"%cd -q {d2}", f"%run {fname2}"]
+            kernel_name = NATIVE_KERNEL_NAME
+            exec_mngr1 = KernelExecutionManager(logger1, kernel_name, *exec_mngr1_cmds, group_id="SomeGroup")
+            exec_mngr1 = self.replace_client(exec_mngr1)
+            retval1 = exec_mngr1.run_until_complete()  # Run commands
+            self.assertEqual(0, retval1)
+            exec_mngr2 = KernelExecutionManager(logger2, kernel_name, *exec_mngr2_cmds, group_id="AnotherGroup")
+            exec_mngr2 = self.replace_client(exec_mngr2)
+            self.assertEqual(2, _kernel_manager_factory.n_kernel_managers())
+            self.assertNotEqual(exec_mngr1._kernel_manager, exec_mngr2._kernel_manager)
+            retval2 = exec_mngr2.run_until_complete()  # Run commands
+            self.assertEqual(0, retval2)
+            # Close
+            self.release_exec_mngr_resources(exec_mngr1)
+            self.release_exec_mngr_resources(exec_mngr2)
+            _kernel_manager_factory.kill_kernel_managers()
+            self.assertEqual(0, _kernel_manager_factory.n_kernel_managers())
+            # Check emitted messages
+            (
+                kernel_started_messages,
+                stdin_messages,
+                execution_started_messages,
+                kernel_shutdown_messages,
+            ) = self._collect_messages_per_type(logger1)
+            self.assertEqual(len(kernel_started_messages), 1)
+            self.assertEqual(kernel_started_messages[0]["kernel_name"], NATIVE_KERNEL_NAME)
+            self.assertEqual(len(stdin_messages), 2)
+            expected_msg = {"type": "execution_started", "kernel_name": NATIVE_KERNEL_NAME}
+            self.assertEqual([expected_msg], execution_started_messages)
+            self.assertEqual(len(kernel_shutdown_messages), 0)
+            (
+                kernel_started_messages,
+                stdin_messages,
+                execution_started_messages,
+                kernel_shutdown_messages,
+            ) = self._collect_messages_per_type(logger2)
+            self.assertEqual(len(kernel_started_messages), 1)
+            self.assertEqual(kernel_started_messages[0]["kernel_name"], NATIVE_KERNEL_NAME)
+            self.assertEqual(len(stdin_messages), 2)
+            expected_msg = {"type": "execution_started", "kernel_name": NATIVE_KERNEL_NAME}
+            self.assertEqual([expected_msg], execution_started_messages)
+            self.assertEqual(len(kernel_shutdown_messages), 0)
+
+    @staticmethod
+    def replace_client(exec_mngr):
+        """Reloads the connection file, and replaces the kernel client with a new one, just like
+        we do on Toolbox side. Don't really understand why we need to do this, but I think
+        it's because of the 'classic' race condition in jupyter-client < 7.0.
+        This maybe fixed in a more recent version of jupyter-client."""
+        # exec_mngr._kernel_client.stop_channels()
+        # exec_mngr._kernel_client.context.term()  # ResourceWarning: Unclosed <zmq.Context() happens without this
+        exec_mngr._kernel_manager.load_connection_file()
+        kc = exec_mngr._kernel_manager.client()  # Make new client
+        # kc.start_channels()
+        exec_mngr._kernel_client = kc  # Replace the original client
+        return exec_mngr
+
+    def _collect_messages_per_type(self, logger):
+        stdin_messages = []
+        execution_started_messages = []
+        kernel_started_messages = []
+        kernel_shutdown_messages = []
+        for i, call in enumerate(logger.msg_kernel_execution.emit.call_args_list):
+            with self.subTest(call_number=i):
+                self.assertEqual(len(call.kwargs), 0)
+                self.assertEqual(len(call.args), 1)
+            message = call.args[0]
+            msg_type = message["type"]
+            if msg_type == "stdin":
+                stdin_messages.append(message)
+            elif msg_type == "execution_started":
+                execution_started_messages.append(message)
+            elif msg_type == "stderr":
+                # UserWarnings are harmless, everything else is suspicious
+                self.assertIn("UserWarning", message["data"])
+            elif msg_type == "kernel_started":
+                kernel_started_messages.append(message)
+            elif msg_type == "kernel_shutdown":
+                kernel_shutdown_messages.append(message)
+            elif msg_type == "stdout":
+                # On Linux, script file output is echoed in stdout
+                # This doesn't happen on Windows for some reason.
+                pass
+            else:
+                self.fail(f"unexpected message type {msg_type}")
+        return kernel_started_messages, stdin_messages, execution_started_messages, kernel_shutdown_messages
```

### Comparing `spine_engine-0.23.3/tests/execution_managers/test_persistent_execution_manager.py` & `spine_engine-0.23.4/tests/execution_managers/test_persistent_execution_manager.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,76 +1,76 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-"""Unit tests for ``persistent_execution_manager`` module."""
-import unittest
-from unittest.mock import MagicMock
-from spine_engine.execution_managers.persistent_execution_manager import PythonPersistentExecutionManager
-from spine_engine.utils.execution_resources import persistent_process_semaphore
-
-
-class TestPythonPersistentExecutionManager(unittest.TestCase):
-    def test_reuse_process(self):
-        logger = MagicMock()
-        exec_mngr1 = PythonPersistentExecutionManager(
-            logger, ["python"], ['print("hello")'], "alias", kill_completed_processes=False, group_id="SomeGroup"
-        )
-        exec_mngr1.run_until_complete()
-        exec_mngr2 = PythonPersistentExecutionManager(
-            logger,
-            ["python"],
-            ['print("hello again")'],
-            "another alias",
-            kill_completed_processes=False,
-            group_id="SomeGroup",
-        )
-        self.assertEqual(exec_mngr1._persistent_manager, exec_mngr2._persistent_manager)
-        logger.msg_warning.emit.assert_called_once()
-        exec_mngr1._persistent_manager.kill_process()
-
-    def test_do_not_reuse_unfinished_process(self):
-        persistent_process_semaphore.set_limit(2)
-        logger = MagicMock()
-        exec_mngr1 = PythonPersistentExecutionManager(
-            logger, ["python"], ['print("hello")'], "alias", kill_completed_processes=False, group_id="SomeGroup"
-        )
-        exec_mngr2 = PythonPersistentExecutionManager(
-            logger,
-            ["python"],
-            ['print("hello again")'],
-            "another alias",
-            kill_completed_processes=False,
-            group_id="SomeGroup",
-        )
-        self.assertNotEqual(exec_mngr1._persistent_manager, exec_mngr2._persistent_manager)
-        exec_mngr1._persistent_manager.kill_process()
-        exec_mngr2._persistent_manager.kill_process()
-
-    def test_failing_process(self):
-        logger = MagicMock()
-        exec_mngr = PythonPersistentExecutionManager(
-            logger, ["python"], ["exit(666)"], "my execution", kill_completed_processes=False, group_id="SomeGroup"
-        )
-        self.assertEqual(exec_mngr.run_until_complete(), -1)
-        self.assertTrue(exec_mngr.killed)
-        expected_messages = [
-            {"type": "persistent_started", "language": "python"},
-            {"type": "execution_started", "args": "python"},
-            {"type": "stdin", "data": "# Running my execution"},
-            {"type": "stdout", "data": "Kernel died (×_×)"},
-        ]
-        message_emits = logger.msg_persistent_execution.emit.call_args_list
-        self.assertEqual(len(message_emits), len(expected_messages))
-        for call, expected_message in zip(message_emits, expected_messages):
-            for key, expected in expected_message.items():
-                self.assertEqual(call[0][0][key], expected)
-
-
-if __name__ == "__main__":
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+"""Unit tests for ``persistent_execution_manager`` module."""
+import unittest
+from unittest.mock import MagicMock
+from spine_engine.execution_managers.persistent_execution_manager import PythonPersistentExecutionManager
+from spine_engine.utils.execution_resources import persistent_process_semaphore
+
+
+class TestPythonPersistentExecutionManager(unittest.TestCase):
+    def test_reuse_process(self):
+        logger = MagicMock()
+        exec_mngr1 = PythonPersistentExecutionManager(
+            logger, ["python"], ['print("hello")'], "alias", kill_completed_processes=False, group_id="SomeGroup"
+        )
+        exec_mngr1.run_until_complete()
+        exec_mngr2 = PythonPersistentExecutionManager(
+            logger,
+            ["python"],
+            ['print("hello again")'],
+            "another alias",
+            kill_completed_processes=False,
+            group_id="SomeGroup",
+        )
+        self.assertEqual(exec_mngr1._persistent_manager, exec_mngr2._persistent_manager)
+        logger.msg_warning.emit.assert_called_once()
+        exec_mngr1._persistent_manager.kill_process()
+
+    def test_do_not_reuse_unfinished_process(self):
+        persistent_process_semaphore.set_limit(2)
+        logger = MagicMock()
+        exec_mngr1 = PythonPersistentExecutionManager(
+            logger, ["python"], ['print("hello")'], "alias", kill_completed_processes=False, group_id="SomeGroup"
+        )
+        exec_mngr2 = PythonPersistentExecutionManager(
+            logger,
+            ["python"],
+            ['print("hello again")'],
+            "another alias",
+            kill_completed_processes=False,
+            group_id="SomeGroup",
+        )
+        self.assertNotEqual(exec_mngr1._persistent_manager, exec_mngr2._persistent_manager)
+        exec_mngr1._persistent_manager.kill_process()
+        exec_mngr2._persistent_manager.kill_process()
+
+    def test_failing_process(self):
+        logger = MagicMock()
+        exec_mngr = PythonPersistentExecutionManager(
+            logger, ["python"], ["exit(666)"], "my execution", kill_completed_processes=False, group_id="SomeGroup"
+        )
+        self.assertEqual(exec_mngr.run_until_complete(), -1)
+        self.assertTrue(exec_mngr.killed)
+        expected_messages = [
+            {"type": "persistent_started", "language": "python"},
+            {"type": "execution_started", "args": "python"},
+            {"type": "stdin", "data": "# Running my execution"},
+            {"type": "stdout", "data": "Kernel died (×_×)"},
+        ]
+        message_emits = logger.msg_persistent_execution.emit.call_args_list
+        self.assertEqual(len(message_emits), len(expected_messages))
+        for call, expected_message in zip(message_emits, expected_messages):
+            for key, expected in expected_message.items():
+                self.assertEqual(call[0][0][key], expected)
+
+
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `spine_engine-0.23.3/tests/project_item/__init__.py` & `spine_engine-0.23.4/spine_engine/server/util/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,15 +1,15 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Init file for tests.project_item package. Intentionally empty.
-
-"""
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+This package contains utility classes of remote server part of the Spine Engine.
+
+"""
```

### Comparing `spine_engine-0.23.3/tests/project_item/test_ExecutableItem.py` & `spine_engine-0.23.4/tests/project_item/test_ExecutableItem.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,52 +1,52 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Toolbox.
-# Spine Toolbox is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Unit tests for ExecutableItem.
-
-"""
-from tempfile import TemporaryDirectory
-import unittest
-from unittest import mock
-from spine_engine import ExecutionDirection
-from spine_engine.project_item.executable_item_base import ExecutableItemBase
-
-
-class TestExecutableItem(unittest.TestCase):
-    def setUp(self):
-        self._temp_dir = TemporaryDirectory()
-
-    def tearDown(self):
-        self._temp_dir.cleanup()
-
-    def test_name(self):
-        item = ExecutableItemBase("name", self._temp_dir.name, mock.MagicMock())
-        self.assertEqual(item.name, "name")
-
-    def test_output_resources_backward(self):
-        item = ExecutableItemBase("name", self._temp_dir.name, mock.MagicMock())
-        item._output_resources_backward = mock.MagicMock(return_value=[3, 5, 7])
-        item._output_resources_forward = mock.MagicMock()
-        self.assertEqual(item.output_resources(ExecutionDirection.BACKWARD), [3, 5, 7])
-        item._output_resources_backward.assert_called_once_with()
-        item._output_resources_forward.assert_not_called()
-
-    def test_output_resources_forward(self):
-        item = ExecutableItemBase("name", self._temp_dir.name, mock.MagicMock())
-        item._output_resources_backward = mock.MagicMock()
-        item._output_resources_forward = mock.MagicMock(return_value=[3, 5, 7])
-        self.assertEqual(item.output_resources(ExecutionDirection.FORWARD), [3, 5, 7])
-        item._output_resources_backward.assert_not_called()
-        item._output_resources_forward.assert_called_once_with()
-
-
-if __name__ == "__main__":
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Toolbox.
+# Spine Toolbox is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Unit tests for ExecutableItem.
+
+"""
+from tempfile import TemporaryDirectory
+import unittest
+from unittest import mock
+from spine_engine import ExecutionDirection
+from spine_engine.project_item.executable_item_base import ExecutableItemBase
+
+
+class TestExecutableItem(unittest.TestCase):
+    def setUp(self):
+        self._temp_dir = TemporaryDirectory()
+
+    def tearDown(self):
+        self._temp_dir.cleanup()
+
+    def test_name(self):
+        item = ExecutableItemBase("name", self._temp_dir.name, mock.MagicMock())
+        self.assertEqual(item.name, "name")
+
+    def test_output_resources_backward(self):
+        item = ExecutableItemBase("name", self._temp_dir.name, mock.MagicMock())
+        item._output_resources_backward = mock.MagicMock(return_value=[3, 5, 7])
+        item._output_resources_forward = mock.MagicMock()
+        self.assertEqual(item.output_resources(ExecutionDirection.BACKWARD), [3, 5, 7])
+        item._output_resources_backward.assert_called_once_with()
+        item._output_resources_forward.assert_not_called()
+
+    def test_output_resources_forward(self):
+        item = ExecutableItemBase("name", self._temp_dir.name, mock.MagicMock())
+        item._output_resources_backward = mock.MagicMock()
+        item._output_resources_forward = mock.MagicMock(return_value=[3, 5, 7])
+        self.assertEqual(item.output_resources(ExecutionDirection.FORWARD), [3, 5, 7])
+        item._output_resources_backward.assert_not_called()
+        item._output_resources_forward.assert_called_once_with()
+
+
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `spine_engine-0.23.3/tests/project_item/test_connection.py` & `spine_engine-0.23.4/tests/project_item/test_connection.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,281 +1,281 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-"""
-Uni tests for the ``connection`` module.
-
-"""
-import os.path
-from tempfile import TemporaryDirectory
-import unittest
-from unittest.mock import Mock
-from spinedb_api import DatabaseMapping, import_scenarios, import_tools, import_alternatives, import_object_classes
-from spine_engine.project_item.connection import Connection, FilterSettings, Jump, ResourceConvertingConnection
-from spine_engine.project_item.project_item_resource import database_resource
-from spine_engine.spine_engine import SpineEngine
-from spinedb_api.filters.scenario_filter import SCENARIO_FILTER_TYPE
-from spinedb_api.filters.tool_filter import TOOL_FILTER_TYPE
-
-
-class TestConnection(unittest.TestCase):
-    def test_serialization_without_filters(self):
-        connection = Connection("source", "bottom", "destination", "top", {"option": 23})
-        connection_dict = connection.to_dict()
-        restored = Connection.from_dict(connection_dict)
-        self.assertEqual(restored.source, "source")
-        self.assertEqual(restored.source_position, "bottom")
-        self.assertEqual(restored.destination, "destination")
-        self.assertEqual(restored.destination_position, "top")
-        self.assertEqual(restored.options, {"option": 23})
-
-    def test_ready_to_execute_returns_false_when_required_filters_do_not_exist(self):
-        options = {"require_" + SCENARIO_FILTER_TYPE: True}
-        connection = Connection("source", "bottom", "destination", "top", options)
-        self.assertFalse(connection.ready_to_execute())
-
-    def test_ready_to_execute_returns_false_when_required_filters_are_offline(self):
-        options = {"require_" + SCENARIO_FILTER_TYPE: True}
-        filter_settings = FilterSettings({"database@Data Store": {SCENARIO_FILTER_TYPE: {"scenario_1": False}}})
-        connection = Connection("source", "bottom", "destination", "top", options, filter_settings)
-        self.assertFalse(connection.ready_to_execute())
-
-    def test_ready_to_execute_returns_true_when_required_filters_are_online(self):
-        options = {"require_" + SCENARIO_FILTER_TYPE: True}
-        filter_settings = FilterSettings({"database@Data Store": {SCENARIO_FILTER_TYPE: {"scenario_1": True}}})
-        connection = Connection("source", "bottom", "destination", "top", options, filter_settings)
-        self.assertTrue(connection.ready_to_execute())
-
-    def test_require_filter_online(self):
-        options = {"require_" + SCENARIO_FILTER_TYPE: True}
-        filter_settings = FilterSettings({"database@Data Store": {SCENARIO_FILTER_TYPE: {"scenario_1": True}}})
-        connection = Connection("source", "bottom", "destination", "top", options, filter_settings)
-        self.assertTrue(connection.require_filter_online(SCENARIO_FILTER_TYPE))
-        self.assertFalse(connection.require_filter_online(TOOL_FILTER_TYPE))
-
-    def test_require_filter_online_default_value(self):
-        filter_settings = FilterSettings({"database@Data Store": {SCENARIO_FILTER_TYPE: {"scenario_1": True}}})
-        connection = Connection("source", "bottom", "destination", "top", {}, filter_settings)
-        self.assertIsNotNone(connection.require_filter_online(SCENARIO_FILTER_TYPE))
-        self.assertIsNotNone(connection.require_filter_online(TOOL_FILTER_TYPE))
-        self.assertFalse(connection.require_filter_online(SCENARIO_FILTER_TYPE))
-        self.assertFalse(connection.require_filter_online(TOOL_FILTER_TYPE))
-
-    def test_notification_when_filter_validation_fails(self):
-        options = {"require_" + SCENARIO_FILTER_TYPE: True, "require_" + TOOL_FILTER_TYPE: True}
-        filter_settings = FilterSettings(auto_online=False)
-        connection = Connection("source", "bottom", "destination", "top", options, filter_settings)
-        self.assertEqual(
-            connection.notifications(),
-            ["At least one scenario filter must be active.", "At least one tool filter must be active."],
-        )
-
-    def test_nothing_to_notify(self):
-        filter_settings = FilterSettings(
-            {"database@Data Store": {SCENARIO_FILTER_TYPE: {"scenario_1": True}, TOOL_FILTER_TYPE: {"tool_1": True}}}
-        )
-        options = {"require_" + SCENARIO_FILTER_TYPE: True, "require_" + TOOL_FILTER_TYPE: True}
-        connection = Connection("source", "bottom", "destination", "top", options, filter_settings)
-        self.assertEqual(connection.notifications(), [])
-
-    def test_emtpy_notifications_when_scenario_filter_is_automatically_online(self):
-        options = {"require_scenario_filter": True}
-        connection = Connection("A", "bottom", "B", "top", options)
-        notifications = connection.notifications()
-        self.assertEqual(notifications, [])
-
-
-class TestConnectionWithDatabase(unittest.TestCase):
-    def setUp(self):
-        self._temp_dir = TemporaryDirectory()
-        self._url = "sqlite:///" + os.path.join(self._temp_dir.name, "db.sqlite")
-        self._db_map = DatabaseMapping(self._url, create=True)
-
-    def tearDown(self):
-        self._db_map.connection.close()
-        self._temp_dir.cleanup()
-
-    def test_serialization_with_filters(self):
-        import_scenarios(self._db_map, ("my_scenario",))
-        self._db_map.commit_session("Add test data.")
-        filter_settings = FilterSettings(
-            {"my_database": {"scenario_filter": {"my_scenario": False}}}, auto_online=False
-        )
-        connection = Connection("source", "bottom", "destination", "top", filter_settings=filter_settings)
-        connection.receive_resources_from_source([database_resource("unit_test", self._url, "my_database")])
-        connection_dict = connection.to_dict()
-        restored = Connection.from_dict(connection_dict)
-        self.assertEqual(restored.source, "source")
-        self.assertEqual(restored.source_position, "bottom")
-        self.assertEqual(restored.destination, "destination")
-        self.assertEqual(restored.destination_position, "top")
-        self.assertEqual(restored.options, {})
-        self.assertEqual(restored._filter_settings, filter_settings)
-
-    def test_enabled_scenarios_with_auto_enable_on(self):
-        import_scenarios(self._db_map, ("scenario_1", "scenario_2"))
-        self._db_map.commit_session("Add test data.")
-        filter_settings = FilterSettings({"my_database": {"scenario_filter": {"scenario_1": False}}})
-        connection = Connection("source", "bottom", "destination", "top", filter_settings=filter_settings)
-        resources = [database_resource("unit_test", self._url, "my_database", filterable=True)]
-        connection.receive_resources_from_source(resources)
-        self.assertEqual(connection.enabled_filters("my_database"), {"scenario_filter": ["scenario_2"]})
-
-    def test_enabled_scenarios_with_auto_enable_off(self):
-        import_scenarios(self._db_map, ("scenario_1", "scenario_2"))
-        self._db_map.commit_session("Add test data.")
-        filter_settings = FilterSettings({"my_database": {"scenario_filter": {"scenario_1": True}}}, auto_online=False)
-        connection = Connection("source", "bottom", "destination", "top", filter_settings=filter_settings)
-        resources = [database_resource("unit_test", self._url, "my_database", filterable=True)]
-        connection.receive_resources_from_source(resources)
-        self.assertEqual(connection.enabled_filters("my_database"), {"scenario_filter": ["scenario_1"]})
-
-    def test_enabled_tools_with_auto_enable_on(self):
-        import_tools(self._db_map, ("tool_1", "tool_2"))
-        self._db_map.commit_session("Add test data.")
-        filter_settings = FilterSettings({"my_database": {"tool_filter": {"tool_1": False}}})
-        connection = Connection("source", "bottom", "destination", "top", filter_settings=filter_settings)
-        resources = [database_resource("unit_test", self._url, "my_database", filterable=True)]
-        connection.receive_resources_from_source(resources)
-        self.assertEqual(connection.enabled_filters("my_database"), {"tool_filter": ["tool_2"]})
-
-    def test_enabled_tools_with_auto_enable_off(self):
-        import_tools(self._db_map, ("tool_1", "tool_2"))
-        self._db_map.commit_session("Add test data.")
-        filter_settings = FilterSettings({"my_database": {"tool_filter": {"tool_1": True}}}, auto_online=False)
-        connection = Connection("source", "bottom", "destination", "top", filter_settings=filter_settings)
-        resources = [database_resource("unit_test", self._url, "my_database", filterable=True)]
-        connection.receive_resources_from_source(resources)
-        self.assertEqual(connection.enabled_filters("my_database"), {"tool_filter": ["tool_1"]})
-
-    def test_purge_data_before_writing(self):
-        import_alternatives(self._db_map, ("my_alternative",))
-        import_object_classes(self._db_map, ("my_object_class",))
-        self._db_map.commit_session("Add test data.")
-        self._db_map.connection.close()
-        connection = Connection(
-            "source",
-            "bottom",
-            "destination",
-            "top",
-            options={"purge_before_writing": True, "purge_settings": {"object_class": True}},
-        )
-        resources = [database_resource("unit_test", self._url, "my_database")]
-        connection.clean_up_backward_resources(resources)
-        database_map = DatabaseMapping(self._url)
-        object_class_list = database_map.query(database_map.object_class_sq).all()
-        self.assertEqual(len(object_class_list), 0)
-        alternative_list = database_map.query(database_map.alternative_sq).all()
-        self.assertEqual(len(alternative_list), 2)
-        self.assertEqual(alternative_list[0].name, "Base")
-        self.assertEqual(alternative_list[1].name, "my_alternative")
-        database_map.connection.close()
-
-
-class TestJump(unittest.TestCase):
-    def test_default_condition_prevents_jump(self):
-        jump = Jump("source", "bottom", "destination", "top")
-        self.assertFalse(jump.is_condition_true(1))
-
-    def test_empty_condition_prevents_jump(self):
-        jump = Jump("source", "bottom", "destination", "top", {"type": "python-script", "script": ""})
-        self.assertFalse(jump.is_condition_true(1))
-
-    def test_counter_passed_to_condition(self):
-        condition = {
-            "type": "python-script",
-            "script": "\n".join(("import sys", "counter = int(sys.argv[1])", "exit(0 if counter == 23 else 1)")),
-        }
-        jump = Jump("source", "bottom", "destination", "top", condition)
-        jump.make_logger(Mock())
-        self.assertTrue(jump.is_condition_true(23))
-
-    @unittest.skip("Doesn't work in github actions: ModuleNotFoundError: No module named 'spine_items'")
-    def test_tool_spec_condition(self):
-        condition = {"type": "tool-specification", "specification": "loop_twice"}
-        jump = Jump("source", "bottom", "destination", "top", condition)
-        jump.make_logger(Mock())
-        with TemporaryDirectory() as temp_dir:
-            main_program_file_path = "script"
-            temp_file_path = os.path.join(temp_dir, main_program_file_path)
-            with open(temp_file_path, "w+") as program_file:
-                program_file.write('if ARGS[1] == "23" exit(0) else exit(1) end')
-            engine = SpineEngine(
-                settings={"appSettings/useJuliaKernel": "1"},
-                project_dir=temp_dir,
-                specifications={
-                    "Tool": [
-                        {
-                            "name": "loop_twice",
-                            "tooltype": "julia",
-                            "includes_main_path": temp_dir,
-                            "includes": [main_program_file_path],
-                        }
-                    ]
-                },
-            )
-            jump.prepare_condition(engine)
-            self.assertTrue(jump.is_condition_true(23))
-
-    def test_dictionary(self):
-        jump = Jump("source", "bottom", "destination", "top", {"type": "python-script", "script": "exit(23)"})
-        jump_dict = jump.to_dict()
-        new_jump = Jump.from_dict(jump_dict)
-        self.assertEqual(new_jump.source, jump.source)
-        self.assertEqual(new_jump.destination, jump.destination)
-        self.assertEqual(new_jump.source_position, jump.source_position)
-        self.assertEqual(new_jump.destination_position, jump.destination_position)
-        self.assertEqual(new_jump.condition, jump.condition)
-
-
-class TestFilterSettings(unittest.TestCase):
-    def test_has_filters_returns_false_when_no_filters_exist(self):
-        settings = FilterSettings()
-        self.assertFalse(settings.has_filters())
-
-    def test_has_filters_returns_true_when_filters_exist(self):
-        settings = FilterSettings({"database@Data Store": {TOOL_FILTER_TYPE: {"tool_1": True}}})
-        self.assertTrue(settings.has_filters())
-
-    def test_has_filters_online_returns_false_when_no_filters_exist(self):
-        settings = FilterSettings()
-        self.assertFalse(settings.has_filter_online(SCENARIO_FILTER_TYPE))
-
-    def test_has_filters_online_returns_false_when_filters_are_offline(self):
-        settings = FilterSettings(
-            {"database@Data Store": {SCENARIO_FILTER_TYPE: {"scenario_1": True}, TOOL_FILTER_TYPE: {"tool_1": False}}}
-        )
-        self.assertFalse(settings.has_filter_online(TOOL_FILTER_TYPE))
-
-    def test_has_filters_online_returns_true_when_filters_are_online(self):
-        settings = FilterSettings(
-            {"database@Data Store": {SCENARIO_FILTER_TYPE: {"scenario_1": False}, TOOL_FILTER_TYPE: {"tool_1": True}}}
-        )
-        self.assertTrue(settings.has_filter_online(TOOL_FILTER_TYPE))
-
-    def test_has_filter_online_works_when_there_are_no_known_filters(self):
-        settings = FilterSettings()
-        self.assertFalse(settings.has_filter_online(SCENARIO_FILTER_TYPE))
-        self.assertFalse(settings.has_filter_online(TOOL_FILTER_TYPE))
-
-    def test_has_any_filter_online_returns_true_when_filters_are_online(self):
-        settings = FilterSettings(
-            {"database@Data Store": {SCENARIO_FILTER_TYPE: {"scenario_1": False}, TOOL_FILTER_TYPE: {"tool_1": True}}}
-        )
-        self.assertTrue(settings.has_any_filter_online())
-
-    def test_has_any_filter_online_returns_false_when_all_filters_are_offline(self):
-        settings = FilterSettings(
-            {"database@Data Store": {SCENARIO_FILTER_TYPE: {"scenario_1": False}, TOOL_FILTER_TYPE: {"tool_1": False}}}
-        )
-        self.assertFalse(settings.has_any_filter_online())
-
-
-if __name__ == "__main__":
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+"""
+Uni tests for the ``connection`` module.
+
+"""
+import os.path
+from tempfile import TemporaryDirectory
+import unittest
+from unittest.mock import Mock
+from spinedb_api import DatabaseMapping, import_scenarios, import_tools, import_alternatives, import_object_classes
+from spine_engine.project_item.connection import Connection, FilterSettings, Jump, ResourceConvertingConnection
+from spine_engine.project_item.project_item_resource import database_resource
+from spine_engine.spine_engine import SpineEngine
+from spinedb_api.filters.scenario_filter import SCENARIO_FILTER_TYPE
+from spinedb_api.filters.tool_filter import TOOL_FILTER_TYPE
+
+
+class TestConnection(unittest.TestCase):
+    def test_serialization_without_filters(self):
+        connection = Connection("source", "bottom", "destination", "top", {"option": 23})
+        connection_dict = connection.to_dict()
+        restored = Connection.from_dict(connection_dict)
+        self.assertEqual(restored.source, "source")
+        self.assertEqual(restored.source_position, "bottom")
+        self.assertEqual(restored.destination, "destination")
+        self.assertEqual(restored.destination_position, "top")
+        self.assertEqual(restored.options, {"option": 23})
+
+    def test_ready_to_execute_returns_false_when_required_filters_do_not_exist(self):
+        options = {"require_" + SCENARIO_FILTER_TYPE: True}
+        connection = Connection("source", "bottom", "destination", "top", options)
+        self.assertFalse(connection.ready_to_execute())
+
+    def test_ready_to_execute_returns_false_when_required_filters_are_offline(self):
+        options = {"require_" + SCENARIO_FILTER_TYPE: True}
+        filter_settings = FilterSettings({"database@Data Store": {SCENARIO_FILTER_TYPE: {"scenario_1": False}}})
+        connection = Connection("source", "bottom", "destination", "top", options, filter_settings)
+        self.assertFalse(connection.ready_to_execute())
+
+    def test_ready_to_execute_returns_true_when_required_filters_are_online(self):
+        options = {"require_" + SCENARIO_FILTER_TYPE: True}
+        filter_settings = FilterSettings({"database@Data Store": {SCENARIO_FILTER_TYPE: {"scenario_1": True}}})
+        connection = Connection("source", "bottom", "destination", "top", options, filter_settings)
+        self.assertTrue(connection.ready_to_execute())
+
+    def test_require_filter_online(self):
+        options = {"require_" + SCENARIO_FILTER_TYPE: True}
+        filter_settings = FilterSettings({"database@Data Store": {SCENARIO_FILTER_TYPE: {"scenario_1": True}}})
+        connection = Connection("source", "bottom", "destination", "top", options, filter_settings)
+        self.assertTrue(connection.require_filter_online(SCENARIO_FILTER_TYPE))
+        self.assertFalse(connection.require_filter_online(TOOL_FILTER_TYPE))
+
+    def test_require_filter_online_default_value(self):
+        filter_settings = FilterSettings({"database@Data Store": {SCENARIO_FILTER_TYPE: {"scenario_1": True}}})
+        connection = Connection("source", "bottom", "destination", "top", {}, filter_settings)
+        self.assertIsNotNone(connection.require_filter_online(SCENARIO_FILTER_TYPE))
+        self.assertIsNotNone(connection.require_filter_online(TOOL_FILTER_TYPE))
+        self.assertFalse(connection.require_filter_online(SCENARIO_FILTER_TYPE))
+        self.assertFalse(connection.require_filter_online(TOOL_FILTER_TYPE))
+
+    def test_notification_when_filter_validation_fails(self):
+        options = {"require_" + SCENARIO_FILTER_TYPE: True, "require_" + TOOL_FILTER_TYPE: True}
+        filter_settings = FilterSettings(auto_online=False)
+        connection = Connection("source", "bottom", "destination", "top", options, filter_settings)
+        self.assertEqual(
+            connection.notifications(),
+            ["At least one scenario filter must be active.", "At least one tool filter must be active."],
+        )
+
+    def test_nothing_to_notify(self):
+        filter_settings = FilterSettings(
+            {"database@Data Store": {SCENARIO_FILTER_TYPE: {"scenario_1": True}, TOOL_FILTER_TYPE: {"tool_1": True}}}
+        )
+        options = {"require_" + SCENARIO_FILTER_TYPE: True, "require_" + TOOL_FILTER_TYPE: True}
+        connection = Connection("source", "bottom", "destination", "top", options, filter_settings)
+        self.assertEqual(connection.notifications(), [])
+
+    def test_emtpy_notifications_when_scenario_filter_is_automatically_online(self):
+        options = {"require_scenario_filter": True}
+        connection = Connection("A", "bottom", "B", "top", options)
+        notifications = connection.notifications()
+        self.assertEqual(notifications, [])
+
+
+class TestConnectionWithDatabase(unittest.TestCase):
+    def setUp(self):
+        self._temp_dir = TemporaryDirectory()
+        self._url = "sqlite:///" + os.path.join(self._temp_dir.name, "db.sqlite")
+        self._db_map = DatabaseMapping(self._url, create=True)
+
+    def tearDown(self):
+        self._db_map.connection.close()
+        self._temp_dir.cleanup()
+
+    def test_serialization_with_filters(self):
+        import_scenarios(self._db_map, ("my_scenario",))
+        self._db_map.commit_session("Add test data.")
+        filter_settings = FilterSettings(
+            {"my_database": {"scenario_filter": {"my_scenario": False}}}, auto_online=False
+        )
+        connection = Connection("source", "bottom", "destination", "top", filter_settings=filter_settings)
+        connection.receive_resources_from_source([database_resource("unit_test", self._url, "my_database")])
+        connection_dict = connection.to_dict()
+        restored = Connection.from_dict(connection_dict)
+        self.assertEqual(restored.source, "source")
+        self.assertEqual(restored.source_position, "bottom")
+        self.assertEqual(restored.destination, "destination")
+        self.assertEqual(restored.destination_position, "top")
+        self.assertEqual(restored.options, {})
+        self.assertEqual(restored._filter_settings, filter_settings)
+
+    def test_enabled_scenarios_with_auto_enable_on(self):
+        import_scenarios(self._db_map, ("scenario_1", "scenario_2"))
+        self._db_map.commit_session("Add test data.")
+        filter_settings = FilterSettings({"my_database": {"scenario_filter": {"scenario_1": False}}})
+        connection = Connection("source", "bottom", "destination", "top", filter_settings=filter_settings)
+        resources = [database_resource("unit_test", self._url, "my_database", filterable=True)]
+        connection.receive_resources_from_source(resources)
+        self.assertEqual(connection.enabled_filters("my_database"), {"scenario_filter": ["scenario_2"]})
+
+    def test_enabled_scenarios_with_auto_enable_off(self):
+        import_scenarios(self._db_map, ("scenario_1", "scenario_2"))
+        self._db_map.commit_session("Add test data.")
+        filter_settings = FilterSettings({"my_database": {"scenario_filter": {"scenario_1": True}}}, auto_online=False)
+        connection = Connection("source", "bottom", "destination", "top", filter_settings=filter_settings)
+        resources = [database_resource("unit_test", self._url, "my_database", filterable=True)]
+        connection.receive_resources_from_source(resources)
+        self.assertEqual(connection.enabled_filters("my_database"), {"scenario_filter": ["scenario_1"]})
+
+    def test_enabled_tools_with_auto_enable_on(self):
+        import_tools(self._db_map, ("tool_1", "tool_2"))
+        self._db_map.commit_session("Add test data.")
+        filter_settings = FilterSettings({"my_database": {"tool_filter": {"tool_1": False}}})
+        connection = Connection("source", "bottom", "destination", "top", filter_settings=filter_settings)
+        resources = [database_resource("unit_test", self._url, "my_database", filterable=True)]
+        connection.receive_resources_from_source(resources)
+        self.assertEqual(connection.enabled_filters("my_database"), {"tool_filter": ["tool_2"]})
+
+    def test_enabled_tools_with_auto_enable_off(self):
+        import_tools(self._db_map, ("tool_1", "tool_2"))
+        self._db_map.commit_session("Add test data.")
+        filter_settings = FilterSettings({"my_database": {"tool_filter": {"tool_1": True}}}, auto_online=False)
+        connection = Connection("source", "bottom", "destination", "top", filter_settings=filter_settings)
+        resources = [database_resource("unit_test", self._url, "my_database", filterable=True)]
+        connection.receive_resources_from_source(resources)
+        self.assertEqual(connection.enabled_filters("my_database"), {"tool_filter": ["tool_1"]})
+
+    def test_purge_data_before_writing(self):
+        import_alternatives(self._db_map, ("my_alternative",))
+        import_object_classes(self._db_map, ("my_object_class",))
+        self._db_map.commit_session("Add test data.")
+        self._db_map.connection.close()
+        connection = Connection(
+            "source",
+            "bottom",
+            "destination",
+            "top",
+            options={"purge_before_writing": True, "purge_settings": {"object_class": True}},
+        )
+        resources = [database_resource("unit_test", self._url, "my_database")]
+        connection.clean_up_backward_resources(resources)
+        database_map = DatabaseMapping(self._url)
+        object_class_list = database_map.query(database_map.object_class_sq).all()
+        self.assertEqual(len(object_class_list), 0)
+        alternative_list = database_map.query(database_map.alternative_sq).all()
+        self.assertEqual(len(alternative_list), 2)
+        self.assertEqual(alternative_list[0].name, "Base")
+        self.assertEqual(alternative_list[1].name, "my_alternative")
+        database_map.connection.close()
+
+
+class TestJump(unittest.TestCase):
+    def test_default_condition_prevents_jump(self):
+        jump = Jump("source", "bottom", "destination", "top")
+        self.assertFalse(jump.is_condition_true(1))
+
+    def test_empty_condition_prevents_jump(self):
+        jump = Jump("source", "bottom", "destination", "top", {"type": "python-script", "script": ""})
+        self.assertFalse(jump.is_condition_true(1))
+
+    def test_counter_passed_to_condition(self):
+        condition = {
+            "type": "python-script",
+            "script": "\n".join(("import sys", "counter = int(sys.argv[1])", "exit(0 if counter == 23 else 1)")),
+        }
+        jump = Jump("source", "bottom", "destination", "top", condition)
+        jump.make_logger(Mock())
+        self.assertTrue(jump.is_condition_true(23))
+
+    @unittest.skip("Doesn't work in github actions: ModuleNotFoundError: No module named 'spine_items'")
+    def test_tool_spec_condition(self):
+        condition = {"type": "tool-specification", "specification": "loop_twice"}
+        jump = Jump("source", "bottom", "destination", "top", condition)
+        jump.make_logger(Mock())
+        with TemporaryDirectory() as temp_dir:
+            main_program_file_path = "script"
+            temp_file_path = os.path.join(temp_dir, main_program_file_path)
+            with open(temp_file_path, "w+") as program_file:
+                program_file.write('if ARGS[1] == "23" exit(0) else exit(1) end')
+            engine = SpineEngine(
+                settings={"appSettings/useJuliaKernel": "1"},
+                project_dir=temp_dir,
+                specifications={
+                    "Tool": [
+                        {
+                            "name": "loop_twice",
+                            "tooltype": "julia",
+                            "includes_main_path": temp_dir,
+                            "includes": [main_program_file_path],
+                        }
+                    ]
+                },
+            )
+            jump.prepare_condition(engine)
+            self.assertTrue(jump.is_condition_true(23))
+
+    def test_dictionary(self):
+        jump = Jump("source", "bottom", "destination", "top", {"type": "python-script", "script": "exit(23)"})
+        jump_dict = jump.to_dict()
+        new_jump = Jump.from_dict(jump_dict)
+        self.assertEqual(new_jump.source, jump.source)
+        self.assertEqual(new_jump.destination, jump.destination)
+        self.assertEqual(new_jump.source_position, jump.source_position)
+        self.assertEqual(new_jump.destination_position, jump.destination_position)
+        self.assertEqual(new_jump.condition, jump.condition)
+
+
+class TestFilterSettings(unittest.TestCase):
+    def test_has_filters_returns_false_when_no_filters_exist(self):
+        settings = FilterSettings()
+        self.assertFalse(settings.has_filters())
+
+    def test_has_filters_returns_true_when_filters_exist(self):
+        settings = FilterSettings({"database@Data Store": {TOOL_FILTER_TYPE: {"tool_1": True}}})
+        self.assertTrue(settings.has_filters())
+
+    def test_has_filters_online_returns_false_when_no_filters_exist(self):
+        settings = FilterSettings()
+        self.assertFalse(settings.has_filter_online(SCENARIO_FILTER_TYPE))
+
+    def test_has_filters_online_returns_false_when_filters_are_offline(self):
+        settings = FilterSettings(
+            {"database@Data Store": {SCENARIO_FILTER_TYPE: {"scenario_1": True}, TOOL_FILTER_TYPE: {"tool_1": False}}}
+        )
+        self.assertFalse(settings.has_filter_online(TOOL_FILTER_TYPE))
+
+    def test_has_filters_online_returns_true_when_filters_are_online(self):
+        settings = FilterSettings(
+            {"database@Data Store": {SCENARIO_FILTER_TYPE: {"scenario_1": False}, TOOL_FILTER_TYPE: {"tool_1": True}}}
+        )
+        self.assertTrue(settings.has_filter_online(TOOL_FILTER_TYPE))
+
+    def test_has_filter_online_works_when_there_are_no_known_filters(self):
+        settings = FilterSettings()
+        self.assertFalse(settings.has_filter_online(SCENARIO_FILTER_TYPE))
+        self.assertFalse(settings.has_filter_online(TOOL_FILTER_TYPE))
+
+    def test_has_any_filter_online_returns_true_when_filters_are_online(self):
+        settings = FilterSettings(
+            {"database@Data Store": {SCENARIO_FILTER_TYPE: {"scenario_1": False}, TOOL_FILTER_TYPE: {"tool_1": True}}}
+        )
+        self.assertTrue(settings.has_any_filter_online())
+
+    def test_has_any_filter_online_returns_false_when_all_filters_are_offline(self):
+        settings = FilterSettings(
+            {"database@Data Store": {SCENARIO_FILTER_TYPE: {"scenario_1": False}, TOOL_FILTER_TYPE: {"tool_1": False}}}
+        )
+        self.assertFalse(settings.has_any_filter_online())
+
+
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `spine_engine-0.23.3/tests/project_item/test_project_item_resource.py` & `spine_engine-0.23.4/tests/project_item/test_project_item_resource.py`

 * *Ordering differences only*

 * *Files 9% similar despite different names*

```diff
@@ -1,178 +1,178 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-""" Unit tests for ``project_item_resource`` module. """
-from contextlib import ExitStack
-from pathlib import Path
-import unittest
-from unittest import mock
-from spine_engine.project_item.project_item_resource import (
-    CmdLineArg,
-    database_resource,
-    expand_cmd_line_args,
-    file_resource,
-    file_resource_in_pack,
-    get_labelled_source_resources,
-    get_source,
-    get_source_extras,
-    LabelArg,
-    labelled_resource_args,
-    transient_file_resource,
-    url_resource,
-)
-from spinedb_api import append_filter_config
-from spinedb_api.filters.scenario_filter import scenario_filter_config
-from spinedb_api.spine_db_server import db_server_manager
-
-
-class TestGetLabelledSourceResources(unittest.TestCase):
-    def test_empty_input_produces_empty_output(self):
-        self.assertEqual(get_labelled_source_resources([]), {})
-
-    def test_single_database_resource_gets_collected(self):
-        resources = [database_resource("provider", "sqlite:///file.sqlite", "my database")]
-        expected = {"my database": [resources[0]]}
-        self.assertEqual(get_labelled_source_resources(resources), expected)
-
-    def test_url_resource_gets_collected(self):
-        resources = [url_resource("provider", "sqlite:///file.sqlite", "my non-Spine database")]
-        expected = {"my non-Spine database": [resources[0]]}
-        self.assertEqual(get_labelled_source_resources(resources), expected)
-
-    def test_single_file_resource_gets_collected(self):
-        resources = [file_resource("provider", "/path/to/file", "my file")]
-        expected = {"my file": [resources[0]]}
-        self.assertEqual(get_labelled_source_resources(resources), expected)
-
-    def test_multiple_file_resources_in_packs_get_collected(self):
-        resources = [
-            file_resource_in_pack("provider", "*.dat", "/path/file1.dat"),
-            file_resource_in_pack("provider", "*.dat", "/path/file2.dat"),
-        ]
-        expected = {"*.dat": [resources[0], resources[1]]}
-        self.assertEqual(get_labelled_source_resources(resources), expected)
-
-
-class TestLabelledResourceArgs(unittest.TestCase):
-    def test_empty_resources(self):
-        resources = []
-        with ExitStack() as exit_stack:
-            labelled_args = labelled_resource_args(resources, exit_stack)
-        self.assertEqual(labelled_args, {})
-
-    def test_single_resources(self):
-        single_file_resource = file_resource("my provider", "/path/to/file")
-        remote_resource = url_resource("so-called cloud", "ftp://ftp.sausage.org/wurst", "food-url")
-        with db_server_manager() as db_server_manager_queue:
-            db_resource = database_resource("db provider", "sqlite://")
-            db_resource.metadata["db_server_manager_queue"] = db_server_manager_queue
-            resources = [single_file_resource, db_resource, remote_resource]
-            with ExitStack() as exit_stack:
-                labelled_args = labelled_resource_args(resources, exit_stack)
-            self.assertEqual(len(labelled_args), 3)
-            self.assertEqual(labelled_args["/path/to/file"], [single_file_resource.path])
-            self.assertEqual(len(labelled_args["sqlite://"]), 1)
-            self.assertTrue(labelled_args["sqlite://"][0].startswith("http://127.0.0.1:"))
-            self.assertEqual(labelled_args["food-url"], ["ftp://ftp.sausage.org/wurst"])
-
-    def test_pack_resource(self):
-        file_1 = file_resource_in_pack("my provider", "pack", "/path/to/file1")
-        file_2 = file_resource_in_pack("my provider", "pack", "/path/to/file2")
-        resources = [file_1, file_2]
-        with ExitStack() as exit_stack:
-            labelled_args = labelled_resource_args(resources, exit_stack)
-        self.assertEqual(labelled_args, {"pack": [file_1.path, file_2.path]})
-
-
-class TestExpandCmdLineArgs(unittest.TestCase):
-    def setUp(self):
-        self._logger = mock.MagicMock()
-
-    def test_empty_args(self):
-        self.assertEqual(expand_cmd_line_args([], {}, self._logger), [])
-
-    def test_single_label_arg(self):
-        args = [LabelArg("file label")]
-        label_to_arg = {"file label": ["/path/to/file"]}
-        expanded_args = expand_cmd_line_args(args, label_to_arg, self._logger)
-        self.assertEqual(expanded_args, ["/path/to/file"])
-
-    def test_multiple_args_for_label(self):
-        args = [LabelArg("file label")]
-        label_to_arg = {"file label": ["/path/to/file1", "/path/to/file2"]}
-        expanded_args = expand_cmd_line_args(args, label_to_arg, self._logger)
-        self.assertEqual(expanded_args, ["/path/to/file1", "/path/to/file2"])
-
-    def test_non_label_arg(self):
-        args = [CmdLineArg("--no-worries")]
-        label_to_arg = {"--no-worries": ["/path/to/file1", "/path/to/file2"]}
-        expanded_args = expand_cmd_line_args(args, label_to_arg, self._logger)
-        self.assertEqual(expanded_args, ["--no-worries"])
-
-
-class TestDatabaseResource(unittest.TestCase):
-    def test_schema_is_stored_in_metadata(self):
-        resource = database_resource("project item", "sqlite:///path/to/db.sqlite", schema="my_schema")
-        self.assertEqual(resource.metadata, {"schema": "my_schema"})
-
-    def test_empty_label_is_replaced_by_url_without_filter_configs(self):
-        url = "sqlite:///path/to/db.sqlite"
-        filter_config = scenario_filter_config("my_scenario")
-        filtered_url = append_filter_config(url, filter_config)
-        self.assertNotEqual(url, filtered_url)
-        resource = database_resource("project item", filtered_url)
-        self.assertEqual(resource.url, filtered_url)
-        self.assertEqual(resource.label, url)
-
-
-class TestURLResource(unittest.TestCase):
-    def test_schema_is_stored_in_metadata(self):
-        resource = url_resource("project item", "sqlite:///path/do/db.sqlite", "my database", schema="my_schema")
-        self.assertEqual(resource.metadata, {"schema": "my_schema"})
-
-
-class TestGetSource(unittest.TestCase):
-    def test_file_resource(self):
-        resource = file_resource("project item", "/path/to/file")
-        self.assertEqual(get_source(resource), str(Path("/", "path", "to", "file").resolve()))
-
-    def test_url_resource(self):
-        resource = url_resource("project item", "mysql://user:psw@localhost/db", "database")
-        self.assertEqual(get_source(resource), "mysql://user:psw@localhost/db")
-
-    def test_database_resource(self):
-        resource = database_resource("project item", "mysql://user:psw@localhost/db")
-        self.assertEqual(get_source(resource), "mysql://user:psw@localhost/db")
-
-    def test_transient_file_resource(self):
-        resource = transient_file_resource("project item", "non-existent")
-        self.assertIsNone(get_source(resource))
-
-
-class TestGetSourceExtras(unittest.TestCase):
-    def test_file_resource(self):
-        resource = file_resource("project item", "/path/to/file")
-        self.assertEqual(get_source_extras(resource), {})
-
-    def test_url_resource(self):
-        resource = url_resource("project item", "mysql://user:psw@localhost/db", "database", "myschema")
-        self.assertEqual(get_source_extras(resource), {"schema": "myschema"})
-
-    def test_database_resource(self):
-        resource = database_resource("project item", "mysql://user:psw@localhost/db")
-        self.assertEqual(get_source_extras(resource), {"schema": None})
-
-    def test_transient_file_resource(self):
-        resource = transient_file_resource("project item", "non-existent")
-        self.assertEqual(get_source_extras(resource), {})
-
-
-if __name__ == '__main__':
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+""" Unit tests for ``project_item_resource`` module. """
+from contextlib import ExitStack
+from pathlib import Path
+import unittest
+from unittest import mock
+from spine_engine.project_item.project_item_resource import (
+    CmdLineArg,
+    database_resource,
+    expand_cmd_line_args,
+    file_resource,
+    file_resource_in_pack,
+    get_labelled_source_resources,
+    get_source,
+    get_source_extras,
+    LabelArg,
+    labelled_resource_args,
+    transient_file_resource,
+    url_resource,
+)
+from spinedb_api import append_filter_config
+from spinedb_api.filters.scenario_filter import scenario_filter_config
+from spinedb_api.spine_db_server import db_server_manager
+
+
+class TestGetLabelledSourceResources(unittest.TestCase):
+    def test_empty_input_produces_empty_output(self):
+        self.assertEqual(get_labelled_source_resources([]), {})
+
+    def test_single_database_resource_gets_collected(self):
+        resources = [database_resource("provider", "sqlite:///file.sqlite", "my database")]
+        expected = {"my database": [resources[0]]}
+        self.assertEqual(get_labelled_source_resources(resources), expected)
+
+    def test_url_resource_gets_collected(self):
+        resources = [url_resource("provider", "sqlite:///file.sqlite", "my non-Spine database")]
+        expected = {"my non-Spine database": [resources[0]]}
+        self.assertEqual(get_labelled_source_resources(resources), expected)
+
+    def test_single_file_resource_gets_collected(self):
+        resources = [file_resource("provider", "/path/to/file", "my file")]
+        expected = {"my file": [resources[0]]}
+        self.assertEqual(get_labelled_source_resources(resources), expected)
+
+    def test_multiple_file_resources_in_packs_get_collected(self):
+        resources = [
+            file_resource_in_pack("provider", "*.dat", "/path/file1.dat"),
+            file_resource_in_pack("provider", "*.dat", "/path/file2.dat"),
+        ]
+        expected = {"*.dat": [resources[0], resources[1]]}
+        self.assertEqual(get_labelled_source_resources(resources), expected)
+
+
+class TestLabelledResourceArgs(unittest.TestCase):
+    def test_empty_resources(self):
+        resources = []
+        with ExitStack() as exit_stack:
+            labelled_args = labelled_resource_args(resources, exit_stack)
+        self.assertEqual(labelled_args, {})
+
+    def test_single_resources(self):
+        single_file_resource = file_resource("my provider", "/path/to/file")
+        remote_resource = url_resource("so-called cloud", "ftp://ftp.sausage.org/wurst", "food-url")
+        with db_server_manager() as db_server_manager_queue:
+            db_resource = database_resource("db provider", "sqlite://")
+            db_resource.metadata["db_server_manager_queue"] = db_server_manager_queue
+            resources = [single_file_resource, db_resource, remote_resource]
+            with ExitStack() as exit_stack:
+                labelled_args = labelled_resource_args(resources, exit_stack)
+            self.assertEqual(len(labelled_args), 3)
+            self.assertEqual(labelled_args["/path/to/file"], [single_file_resource.path])
+            self.assertEqual(len(labelled_args["sqlite://"]), 1)
+            self.assertTrue(labelled_args["sqlite://"][0].startswith("http://127.0.0.1:"))
+            self.assertEqual(labelled_args["food-url"], ["ftp://ftp.sausage.org/wurst"])
+
+    def test_pack_resource(self):
+        file_1 = file_resource_in_pack("my provider", "pack", "/path/to/file1")
+        file_2 = file_resource_in_pack("my provider", "pack", "/path/to/file2")
+        resources = [file_1, file_2]
+        with ExitStack() as exit_stack:
+            labelled_args = labelled_resource_args(resources, exit_stack)
+        self.assertEqual(labelled_args, {"pack": [file_1.path, file_2.path]})
+
+
+class TestExpandCmdLineArgs(unittest.TestCase):
+    def setUp(self):
+        self._logger = mock.MagicMock()
+
+    def test_empty_args(self):
+        self.assertEqual(expand_cmd_line_args([], {}, self._logger), [])
+
+    def test_single_label_arg(self):
+        args = [LabelArg("file label")]
+        label_to_arg = {"file label": ["/path/to/file"]}
+        expanded_args = expand_cmd_line_args(args, label_to_arg, self._logger)
+        self.assertEqual(expanded_args, ["/path/to/file"])
+
+    def test_multiple_args_for_label(self):
+        args = [LabelArg("file label")]
+        label_to_arg = {"file label": ["/path/to/file1", "/path/to/file2"]}
+        expanded_args = expand_cmd_line_args(args, label_to_arg, self._logger)
+        self.assertEqual(expanded_args, ["/path/to/file1", "/path/to/file2"])
+
+    def test_non_label_arg(self):
+        args = [CmdLineArg("--no-worries")]
+        label_to_arg = {"--no-worries": ["/path/to/file1", "/path/to/file2"]}
+        expanded_args = expand_cmd_line_args(args, label_to_arg, self._logger)
+        self.assertEqual(expanded_args, ["--no-worries"])
+
+
+class TestDatabaseResource(unittest.TestCase):
+    def test_schema_is_stored_in_metadata(self):
+        resource = database_resource("project item", "sqlite:///path/to/db.sqlite", schema="my_schema")
+        self.assertEqual(resource.metadata, {"schema": "my_schema"})
+
+    def test_empty_label_is_replaced_by_url_without_filter_configs(self):
+        url = "sqlite:///path/to/db.sqlite"
+        filter_config = scenario_filter_config("my_scenario")
+        filtered_url = append_filter_config(url, filter_config)
+        self.assertNotEqual(url, filtered_url)
+        resource = database_resource("project item", filtered_url)
+        self.assertEqual(resource.url, filtered_url)
+        self.assertEqual(resource.label, url)
+
+
+class TestURLResource(unittest.TestCase):
+    def test_schema_is_stored_in_metadata(self):
+        resource = url_resource("project item", "sqlite:///path/do/db.sqlite", "my database", schema="my_schema")
+        self.assertEqual(resource.metadata, {"schema": "my_schema"})
+
+
+class TestGetSource(unittest.TestCase):
+    def test_file_resource(self):
+        resource = file_resource("project item", "/path/to/file")
+        self.assertEqual(get_source(resource), str(Path("/", "path", "to", "file").resolve()))
+
+    def test_url_resource(self):
+        resource = url_resource("project item", "mysql://user:psw@localhost/db", "database")
+        self.assertEqual(get_source(resource), "mysql://user:psw@localhost/db")
+
+    def test_database_resource(self):
+        resource = database_resource("project item", "mysql://user:psw@localhost/db")
+        self.assertEqual(get_source(resource), "mysql://user:psw@localhost/db")
+
+    def test_transient_file_resource(self):
+        resource = transient_file_resource("project item", "non-existent")
+        self.assertIsNone(get_source(resource))
+
+
+class TestGetSourceExtras(unittest.TestCase):
+    def test_file_resource(self):
+        resource = file_resource("project item", "/path/to/file")
+        self.assertEqual(get_source_extras(resource), {})
+
+    def test_url_resource(self):
+        resource = url_resource("project item", "mysql://user:psw@localhost/db", "database", "myschema")
+        self.assertEqual(get_source_extras(resource), {"schema": "myschema"})
+
+    def test_database_resource(self):
+        resource = database_resource("project item", "mysql://user:psw@localhost/db")
+        self.assertEqual(get_source_extras(resource), {"schema": None})
+
+    def test_transient_file_resource(self):
+        resource = transient_file_resource("project item", "non-existent")
+        self.assertEqual(get_source_extras(resource), {})
+
+
+if __name__ == '__main__':
+    unittest.main()
```

### Comparing `spine_engine-0.23.3/tests/server/__init__.py` & `spine_engine-0.23.4/tests/__init__.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,14 +1,15 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Init file for spine_engine.tests.server package.
-"""
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Init file for tests package. Intentionally empty.
+
+"""
```

### Comparing `spine_engine-0.23.3/tests/server/helloworld.zip` & `spine_engine-0.23.4/tests/server/helloworld.zip`

 * *Files identical despite different names*

### Comparing `spine_engine-0.23.3/tests/server/simple_importer.zip` & `spine_engine-0.23.4/tests/server/simple_importer.zip`

 * *Files identical despite different names*

### Comparing `spine_engine-0.23.3/tests/server/test_EngineServer.py` & `spine_engine-0.23.4/tests/server/test_EngineServer.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,181 +1,181 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Unit tests for EngineServer class.
-"""
-
-import threading
-import unittest
-import zmq
-import os
-import pathlib
-from spine_engine.server.engine_server import EngineServer, ServerSecurityModel
-from spine_engine.server.util.server_message import ServerMessage
-
-base_dir = os.path.join(str(pathlib.Path(__file__).parent), "secfolder_for_tests")
-
-
-def _security_folder_exists():
-    """Security folder and allowEndpoints.txt must exist to test security."""
-    endpointfile = os.path.join(base_dir, "allowEndpoints.txt")
-    return os.path.exists(base_dir) and os.path.exists(endpointfile)
-
-
-class TestEngineServer(unittest.TestCase):
-    def setUp(self):
-        """Sets up client context and socket."""
-        self.client_context = zmq.Context()
-        self.req_socket = self.client_context.socket(zmq.DEALER)
-        self.base_dir = base_dir
-
-    def tearDown(self):
-        """Closes client socket and context if open."""
-        if not self.req_socket.closed:
-            self.req_socket.close()
-        if not self.client_context.closed:
-            self.client_context.term()
-
-    def test_missing_secfolder(self):
-        """Tests the starting/stopping of the ZMQ server without proper sec folder"""
-        server = None
-        with self.assertRaises(ValueError):
-            server = EngineServer("tcp", 6002, ServerSecurityModel.STONEHOUSE, "")
-        self.assertIsNone(server)
-
-    def test_invalid_secfolder(self):
-        """Tests the starting the ZMQ server without a proper sec folder."""
-        server = None
-        with self.assertRaises(ValueError):
-            server = EngineServer("tcp", 6003, ServerSecurityModel.STONEHOUSE, "/fwhkjfnsefkjnselk")
-        self.assertIsNone(server)
-
-    def test_starting_server(self):
-        """Tests starting a tcp ZMQ server without security and pinging it."""
-        server = EngineServer("tcp", 5556, ServerSecurityModel.NONE, "")
-        self.req_socket.connect("tcp://localhost:5556")
-        # Ping the server
-        ping_msg = ServerMessage("ping", "123", "", None)
-        self.req_socket.send_multipart([ping_msg.to_bytes()])
-        response = self.req_socket.recv_multipart()
-        response_str = response[1].decode("utf-8")
-        ping_as_json = ping_msg.toJSON()
-        self.assertEqual(response_str, ping_as_json)  # check that echoed content is as expected
-        server.close()
-
-    @unittest.skipIf(not _security_folder_exists(), "Test requires a security folder")
-    def test_starting_server_with_security(self):
-        """Tests starting a tcp ZMQ server with StoneHouse Security and pinging it."""
-        server = EngineServer("tcp", 6006, ServerSecurityModel.STONEHOUSE, base_dir)
-        # Configure client security
-        secret_keys_dir = os.path.join(base_dir, "private_keys")
-        keys_dir = os.path.join(base_dir, "certificates")
-        public_keys_dir = os.path.join(base_dir, "public_keys")
-        # We need two certificates, one for the client and one for the server.
-        client_secret_file = os.path.join(secret_keys_dir, "client.key_secret")
-        client_public, client_secret = zmq.auth.load_certificate(client_secret_file)
-        self.req_socket.curve_secretkey = client_secret
-        self.req_socket.curve_publickey = client_public
-        server_public_file = os.path.join(public_keys_dir, "server.key")
-        server_public, _ = zmq.auth.load_certificate(server_public_file)
-        # The client must know the server's public key to make a CURVE connection.
-        self.req_socket.curve_serverkey = server_public
-        self.req_socket.connect("tcp://localhost:6006")
-        # Ping the server
-        ping_msg = ServerMessage("ping", "123", "", None)
-        self.req_socket.send_multipart([ping_msg.to_bytes()])
-        response = self.req_socket.recv_multipart()
-        response_str = response[1].decode("utf-8")
-        ping_as_json = ping_msg.toJSON()
-        self.assertEqual(response_str, ping_as_json)  # check that echoed content is as expected
-        server.close()
-
-    def test_malformed_server_message(self):
-        """Tests what happens when the sent request is not valid."""
-        server = EngineServer("tcp", 5556, ServerSecurityModel.NONE, "")
-        self.req_socket.connect("tcp://localhost:5556")
-        file_like_object = bytearray([1, 2, 3, 4, 5])
-        req = b"feiofnoknfsdnoiknsmd"
-        self.req_socket.send_multipart([req, file_like_object])
-        response = self.req_socket.recv_multipart()
-        server_msg = ServerMessage.parse(response[1])
-        msg_data = server_msg.getData()
-        self.assertEqual("server_init_failed", msg_data[0])
-        self.assertTrue(msg_data[1].startswith("json.decoder.JSONDecodeError:"))
-        server.close()
-
-    def test_multiple_client_sockets_sync(self):
-        """Tests multiple client sockets pinging the server synchronously (sequentially)."""
-        server = EngineServer("tcp", 5558, ServerSecurityModel.NONE, "")
-        socket1 = self.client_context.socket(zmq.DEALER)
-        socket2 = self.client_context.socket(zmq.DEALER)
-        socket3 = self.client_context.socket(zmq.DEALER)
-        socket1.connect("tcp://localhost:5558")
-        socket2.connect("tcp://localhost:5558")
-        socket3.connect("tcp://localhost:5558")
-        ping_msg1 = ServerMessage("ping", "1", "", None).to_bytes()
-        ping_msg2 = ServerMessage("ping", "2", "", None).to_bytes()
-        ping_msg3 = ServerMessage("ping", "3", "", None).to_bytes()
-        socket1.send_multipart([ping_msg1])
-        response1 = socket1.recv_multipart()
-        self.assertEqual(response1[1], ping_msg1)
-        socket2.send_multipart([ping_msg2])
-        response2 = socket2.recv_multipart()
-        self.assertEqual(response2[1], ping_msg2)
-        socket3.send_multipart([ping_msg3])
-        response3 = socket3.recv_multipart()
-        self.assertEqual(response3[1], ping_msg3)
-        socket1.close()
-        socket2.close()
-        socket3.close()
-        server.close()
-
-    def test_multiple_client_sockets_async(self):
-        """Tests multiple client sockets pinging the server asynchronously."""
-        server = EngineServer("tcp", 5559, ServerSecurityModel.NONE, "")
-        socket1 = self.client_context.socket(zmq.DEALER)
-        socket2 = self.client_context.socket(zmq.DEALER)
-        socket3 = self.client_context.socket(zmq.DEALER)
-        socket1.connect("tcp://localhost:5559")
-        socket2.connect("tcp://localhost:5559")
-        socket3.connect("tcp://localhost:5559")
-        ping_msg1 = ServerMessage("ping", "1", "", None).to_bytes()
-        ping_msg2 = ServerMessage("ping", "2", "", None).to_bytes()
-        ping_msg3 = ServerMessage("ping", "3", "", None).to_bytes()
-        socket1.send_multipart([ping_msg1])
-        socket2.send_multipart([ping_msg2])
-        socket3.send_multipart([ping_msg3])
-        response1 = socket1.recv_multipart()
-        response2 = socket2.recv_multipart()
-        response3 = socket3.recv_multipart()
-        self.assertEqual(response1[1], ping_msg1)
-        self.assertEqual(response2[1], ping_msg2)
-        self.assertEqual(response3[1], ping_msg3)
-        socket1.close()
-        socket2.close()
-        socket3.close()
-        server.close()
-
-    def test_engineserver_close(self):
-        """Tests thread, socket, and context states after server has been closed."""
-        server = EngineServer("tcp", 5555, ServerSecurityModel.NONE, "")
-        self.assertFalse(server.ctrl_msg_sender.closed)
-        self.assertFalse(server._context.closed)
-        self.assertTrue(server.is_alive())
-        server.close()
-        self.assertTrue(server.ctrl_msg_sender.closed)  # PAIR socket should be closed
-        self.assertTrue(server._context.closed)  # Context should be closed
-        self.assertFalse(server.is_alive())  # server thread should not be alive
-        self.assertEqual(threading.active_count(), 1)  # Only one thread running after close
-
-
-if __name__ == "__main__":
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Unit tests for EngineServer class.
+"""
+
+import threading
+import unittest
+import zmq
+import os
+import pathlib
+from spine_engine.server.engine_server import EngineServer, ServerSecurityModel
+from spine_engine.server.util.server_message import ServerMessage
+
+base_dir = os.path.join(str(pathlib.Path(__file__).parent), "secfolder_for_tests")
+
+
+def _security_folder_exists():
+    """Security folder and allowEndpoints.txt must exist to test security."""
+    endpointfile = os.path.join(base_dir, "allowEndpoints.txt")
+    return os.path.exists(base_dir) and os.path.exists(endpointfile)
+
+
+class TestEngineServer(unittest.TestCase):
+    def setUp(self):
+        """Sets up client context and socket."""
+        self.client_context = zmq.Context()
+        self.req_socket = self.client_context.socket(zmq.DEALER)
+        self.base_dir = base_dir
+
+    def tearDown(self):
+        """Closes client socket and context if open."""
+        if not self.req_socket.closed:
+            self.req_socket.close()
+        if not self.client_context.closed:
+            self.client_context.term()
+
+    def test_missing_secfolder(self):
+        """Tests the starting/stopping of the ZMQ server without proper sec folder"""
+        server = None
+        with self.assertRaises(ValueError):
+            server = EngineServer("tcp", 6002, ServerSecurityModel.STONEHOUSE, "")
+        self.assertIsNone(server)
+
+    def test_invalid_secfolder(self):
+        """Tests the starting the ZMQ server without a proper sec folder."""
+        server = None
+        with self.assertRaises(ValueError):
+            server = EngineServer("tcp", 6003, ServerSecurityModel.STONEHOUSE, "/fwhkjfnsefkjnselk")
+        self.assertIsNone(server)
+
+    def test_starting_server(self):
+        """Tests starting a tcp ZMQ server without security and pinging it."""
+        server = EngineServer("tcp", 5556, ServerSecurityModel.NONE, "")
+        self.req_socket.connect("tcp://localhost:5556")
+        # Ping the server
+        ping_msg = ServerMessage("ping", "123", "", None)
+        self.req_socket.send_multipart([ping_msg.to_bytes()])
+        response = self.req_socket.recv_multipart()
+        response_str = response[1].decode("utf-8")
+        ping_as_json = ping_msg.toJSON()
+        self.assertEqual(response_str, ping_as_json)  # check that echoed content is as expected
+        server.close()
+
+    @unittest.skipIf(not _security_folder_exists(), "Test requires a security folder")
+    def test_starting_server_with_security(self):
+        """Tests starting a tcp ZMQ server with StoneHouse Security and pinging it."""
+        server = EngineServer("tcp", 6006, ServerSecurityModel.STONEHOUSE, base_dir)
+        # Configure client security
+        secret_keys_dir = os.path.join(base_dir, "private_keys")
+        keys_dir = os.path.join(base_dir, "certificates")
+        public_keys_dir = os.path.join(base_dir, "public_keys")
+        # We need two certificates, one for the client and one for the server.
+        client_secret_file = os.path.join(secret_keys_dir, "client.key_secret")
+        client_public, client_secret = zmq.auth.load_certificate(client_secret_file)
+        self.req_socket.curve_secretkey = client_secret
+        self.req_socket.curve_publickey = client_public
+        server_public_file = os.path.join(public_keys_dir, "server.key")
+        server_public, _ = zmq.auth.load_certificate(server_public_file)
+        # The client must know the server's public key to make a CURVE connection.
+        self.req_socket.curve_serverkey = server_public
+        self.req_socket.connect("tcp://localhost:6006")
+        # Ping the server
+        ping_msg = ServerMessage("ping", "123", "", None)
+        self.req_socket.send_multipart([ping_msg.to_bytes()])
+        response = self.req_socket.recv_multipart()
+        response_str = response[1].decode("utf-8")
+        ping_as_json = ping_msg.toJSON()
+        self.assertEqual(response_str, ping_as_json)  # check that echoed content is as expected
+        server.close()
+
+    def test_malformed_server_message(self):
+        """Tests what happens when the sent request is not valid."""
+        server = EngineServer("tcp", 5556, ServerSecurityModel.NONE, "")
+        self.req_socket.connect("tcp://localhost:5556")
+        file_like_object = bytearray([1, 2, 3, 4, 5])
+        req = b"feiofnoknfsdnoiknsmd"
+        self.req_socket.send_multipart([req, file_like_object])
+        response = self.req_socket.recv_multipart()
+        server_msg = ServerMessage.parse(response[1])
+        msg_data = server_msg.getData()
+        self.assertEqual("server_init_failed", msg_data[0])
+        self.assertTrue(msg_data[1].startswith("json.decoder.JSONDecodeError:"))
+        server.close()
+
+    def test_multiple_client_sockets_sync(self):
+        """Tests multiple client sockets pinging the server synchronously (sequentially)."""
+        server = EngineServer("tcp", 5558, ServerSecurityModel.NONE, "")
+        socket1 = self.client_context.socket(zmq.DEALER)
+        socket2 = self.client_context.socket(zmq.DEALER)
+        socket3 = self.client_context.socket(zmq.DEALER)
+        socket1.connect("tcp://localhost:5558")
+        socket2.connect("tcp://localhost:5558")
+        socket3.connect("tcp://localhost:5558")
+        ping_msg1 = ServerMessage("ping", "1", "", None).to_bytes()
+        ping_msg2 = ServerMessage("ping", "2", "", None).to_bytes()
+        ping_msg3 = ServerMessage("ping", "3", "", None).to_bytes()
+        socket1.send_multipart([ping_msg1])
+        response1 = socket1.recv_multipart()
+        self.assertEqual(response1[1], ping_msg1)
+        socket2.send_multipart([ping_msg2])
+        response2 = socket2.recv_multipart()
+        self.assertEqual(response2[1], ping_msg2)
+        socket3.send_multipart([ping_msg3])
+        response3 = socket3.recv_multipart()
+        self.assertEqual(response3[1], ping_msg3)
+        socket1.close()
+        socket2.close()
+        socket3.close()
+        server.close()
+
+    def test_multiple_client_sockets_async(self):
+        """Tests multiple client sockets pinging the server asynchronously."""
+        server = EngineServer("tcp", 5559, ServerSecurityModel.NONE, "")
+        socket1 = self.client_context.socket(zmq.DEALER)
+        socket2 = self.client_context.socket(zmq.DEALER)
+        socket3 = self.client_context.socket(zmq.DEALER)
+        socket1.connect("tcp://localhost:5559")
+        socket2.connect("tcp://localhost:5559")
+        socket3.connect("tcp://localhost:5559")
+        ping_msg1 = ServerMessage("ping", "1", "", None).to_bytes()
+        ping_msg2 = ServerMessage("ping", "2", "", None).to_bytes()
+        ping_msg3 = ServerMessage("ping", "3", "", None).to_bytes()
+        socket1.send_multipart([ping_msg1])
+        socket2.send_multipart([ping_msg2])
+        socket3.send_multipart([ping_msg3])
+        response1 = socket1.recv_multipart()
+        response2 = socket2.recv_multipart()
+        response3 = socket3.recv_multipart()
+        self.assertEqual(response1[1], ping_msg1)
+        self.assertEqual(response2[1], ping_msg2)
+        self.assertEqual(response3[1], ping_msg3)
+        socket1.close()
+        socket2.close()
+        socket3.close()
+        server.close()
+
+    def test_engineserver_close(self):
+        """Tests thread, socket, and context states after server has been closed."""
+        server = EngineServer("tcp", 5555, ServerSecurityModel.NONE, "")
+        self.assertFalse(server.ctrl_msg_sender.closed)
+        self.assertFalse(server._context.closed)
+        self.assertTrue(server.is_alive())
+        server.close()
+        self.assertTrue(server.ctrl_msg_sender.closed)  # PAIR socket should be closed
+        self.assertTrue(server._context.closed)  # Context should be closed
+        self.assertFalse(server.is_alive())  # server thread should not be alive
+        self.assertEqual(threading.active_count(), 1)  # Only one thread running after close
+
+
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `spine_engine-0.23.3/tests/server/test_PingService.py` & `spine_engine-0.23.4/tests/server/test_PingService.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,59 +1,59 @@
-#####################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Unit tests for PingService class.
-"""
-
-import unittest
-import zmq
-from spine_engine.server.util.server_message import ServerMessage
-from spine_engine.server.engine_server import EngineServer, ServerSecurityModel
-
-
-class TestPingService(unittest.TestCase):
-    def setUp(self):
-        self.context = zmq.Context()
-        self.socket = self.context.socket(zmq.DEALER)
-
-    def tearDown(self):
-        if not self.socket.closed:
-            self.socket.close()
-        if not self.context.closed:
-            self.context.term()
-
-    def test_ping_tcp(self):
-        service = EngineServer("tcp", 5558, ServerSecurityModel.NONE, "")
-        self.socket.connect("tcp://localhost:5558")
-        i = 0
-        while i < 10:
-            ping_msg = ServerMessage("ping", str(i), "", None)
-            self.socket.send_multipart([ping_msg.to_bytes()])
-            response = self.socket.recv_multipart()
-            response_str = response[1].decode("utf-8")
-            ping_as_json = ping_msg.toJSON()
-            self.assertEqual(response_str, ping_as_json)  # Check that echoed content is as expected
-            i = i + 1
-        service.close()
-
-    def test_no_connection(self):
-        self.socket.setsockopt(zmq.LINGER, 0)
-        self.socket.connect("tcp://localhost:7002")  # Connect socket somewhere that does not exist
-        msg_parts = []
-        ping_msg = ServerMessage("ping", "2", "", None)
-        msg_parts.append(ping_msg.to_bytes())
-        self.socket.send_multipart(msg_parts, flags=zmq.NOBLOCK)
-        event = self.socket.poll(timeout=1000)
-        self.assertEqual(0, event)
-
-
-if __name__ == "__main__":
-    unittest.main()
+#####################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Unit tests for PingService class.
+"""
+
+import unittest
+import zmq
+from spine_engine.server.util.server_message import ServerMessage
+from spine_engine.server.engine_server import EngineServer, ServerSecurityModel
+
+
+class TestPingService(unittest.TestCase):
+    def setUp(self):
+        self.context = zmq.Context()
+        self.socket = self.context.socket(zmq.DEALER)
+
+    def tearDown(self):
+        if not self.socket.closed:
+            self.socket.close()
+        if not self.context.closed:
+            self.context.term()
+
+    def test_ping_tcp(self):
+        service = EngineServer("tcp", 5558, ServerSecurityModel.NONE, "")
+        self.socket.connect("tcp://localhost:5558")
+        i = 0
+        while i < 10:
+            ping_msg = ServerMessage("ping", str(i), "", None)
+            self.socket.send_multipart([ping_msg.to_bytes()])
+            response = self.socket.recv_multipart()
+            response_str = response[1].decode("utf-8")
+            ping_as_json = ping_msg.toJSON()
+            self.assertEqual(response_str, ping_as_json)  # Check that echoed content is as expected
+            i = i + 1
+        service.close()
+
+    def test_no_connection(self):
+        self.socket.setsockopt(zmq.LINGER, 0)
+        self.socket.connect("tcp://localhost:7002")  # Connect socket somewhere that does not exist
+        msg_parts = []
+        ping_msg = ServerMessage("ping", "2", "", None)
+        msg_parts.append(ping_msg.to_bytes())
+        self.socket.send_multipart(msg_parts, flags=zmq.NOBLOCK)
+        event = self.socket.poll(timeout=1000)
+        self.assertEqual(0, event)
+
+
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `spine_engine-0.23.3/tests/server/test_ProjectExtractorService.py` & `spine_engine-0.23.4/tests/server/test_ProjectExtractorService.py`

 * *Ordering differences only*

 * *Files 8% similar despite different names*

```diff
@@ -1,94 +1,94 @@
-#####################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Unit tests for ProjectExtractorService class.
-"""
-
-import unittest
-import os
-import json
-from pathlib import Path
-from unittest import mock
-from tempfile import TemporaryDirectory
-import zmq
-from spine_engine.server.util.server_message import ServerMessage
-from spine_engine.server.engine_server import EngineServer, ServerSecurityModel
-
-
-class TestProjectExtractorService(unittest.TestCase):
-    def setUp(self):
-        self._temp_dir = TemporaryDirectory()
-        self.service = EngineServer("tcp", 5559, ServerSecurityModel.NONE, "")
-        self.context = zmq.Context()
-        self.socket = self.context.socket(zmq.DEALER)
-        self.socket.connect("tcp://localhost:5559")
-
-    def tearDown(self):
-        self._temp_dir.cleanup()
-        self.service.close()
-        if not self.socket.closed:
-            self.socket.close()
-        if not self.context.closed:
-            self.context.term()
-
-    @mock.patch(
-        "spine_engine.server.project_extractor_service.ProjectExtractorService.INTERNAL_PROJECT_DIR",
-        new_callable=mock.PropertyMock,
-    )
-    def test_project_extraction(self, mock_proj_dir):
-        mock_proj_dir.return_value = self._temp_dir.name
-        with open(os.path.join(str(Path(__file__).parent), "helloworld.zip"), "rb") as f:
-            file_data = f.read()
-        msg = ServerMessage("prepare_execution", "1", json.dumps("helloworld"), ["helloworld.zip"])
-        self.socket.send_multipart([msg.to_bytes(), file_data])
-        response = self.socket.recv_multipart()
-        response_msg = ServerMessage.parse(response[1])
-        self.assertEqual("prepare_execution", response_msg.getCommand())
-        self.assertTrue(len(response_msg.getId()) == 32)
-        self.assertEqual("", response_msg.getData())
-
-    def test_project_file_name_missing(self):
-        """File name list in request is empty."""
-        with open(os.path.join(str(Path(__file__).parent), "helloworld.zip"), "rb") as f:
-            file_data = f.read()
-        msg = ServerMessage("prepare_execution", "1", json.dumps("helloworld"), [])
-        self.socket.send_multipart([msg.to_bytes(), file_data])
-        response = self.socket.recv_multipart()
-        self.check_correct_error_response(response[1], "Project ZIP file name missing")
-
-    def test_project_name_missing(self):
-        """Project name missing from request."""
-        with open(os.path.join(str(Path(__file__).parent), "helloworld.zip"), "rb") as f:
-            data_file = f.read()
-        msg = ServerMessage("prepare_execution", "1", "", ["helloworld.zip"])
-        self.socket.send_multipart([msg.to_bytes(), data_file])
-        response = self.socket.recv_multipart()
-        self.check_correct_error_response(response[1], "Project name missing")
-
-    def test_zip_file_missing(self):
-        """Project zip-file missing from request."""
-        msg = ServerMessage("prepare_execution", "1", json.dumps("projekti"), ["helloworld.zip"])
-        self.socket.send_multipart([msg.to_bytes()])
-        response = self.socket.recv_multipart()
-        self.check_correct_error_response(response[1], "Project ZIP file missing")
-
-    def check_correct_error_response(self, response, expected_err_str):
-        response_msg = ServerMessage.parse(response)
-        self.assertEqual("prepare_execution", response_msg.getCommand())
-        self.assertEqual("1", response_msg.getId())
-        data = response_msg.getData()
-        self.assertEqual("remote_execution_init_failed", data[0])
-        self.assertEqual(expected_err_str, data[1])
-
-
-if __name__ == "__main__":
-    unittest.main()
+#####################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Unit tests for ProjectExtractorService class.
+"""
+
+import unittest
+import os
+import json
+from pathlib import Path
+from unittest import mock
+from tempfile import TemporaryDirectory
+import zmq
+from spine_engine.server.util.server_message import ServerMessage
+from spine_engine.server.engine_server import EngineServer, ServerSecurityModel
+
+
+class TestProjectExtractorService(unittest.TestCase):
+    def setUp(self):
+        self._temp_dir = TemporaryDirectory()
+        self.service = EngineServer("tcp", 5559, ServerSecurityModel.NONE, "")
+        self.context = zmq.Context()
+        self.socket = self.context.socket(zmq.DEALER)
+        self.socket.connect("tcp://localhost:5559")
+
+    def tearDown(self):
+        self._temp_dir.cleanup()
+        self.service.close()
+        if not self.socket.closed:
+            self.socket.close()
+        if not self.context.closed:
+            self.context.term()
+
+    @mock.patch(
+        "spine_engine.server.project_extractor_service.ProjectExtractorService.INTERNAL_PROJECT_DIR",
+        new_callable=mock.PropertyMock,
+    )
+    def test_project_extraction(self, mock_proj_dir):
+        mock_proj_dir.return_value = self._temp_dir.name
+        with open(os.path.join(str(Path(__file__).parent), "helloworld.zip"), "rb") as f:
+            file_data = f.read()
+        msg = ServerMessage("prepare_execution", "1", json.dumps("helloworld"), ["helloworld.zip"])
+        self.socket.send_multipart([msg.to_bytes(), file_data])
+        response = self.socket.recv_multipart()
+        response_msg = ServerMessage.parse(response[1])
+        self.assertEqual("prepare_execution", response_msg.getCommand())
+        self.assertTrue(len(response_msg.getId()) == 32)
+        self.assertEqual("", response_msg.getData())
+
+    def test_project_file_name_missing(self):
+        """File name list in request is empty."""
+        with open(os.path.join(str(Path(__file__).parent), "helloworld.zip"), "rb") as f:
+            file_data = f.read()
+        msg = ServerMessage("prepare_execution", "1", json.dumps("helloworld"), [])
+        self.socket.send_multipart([msg.to_bytes(), file_data])
+        response = self.socket.recv_multipart()
+        self.check_correct_error_response(response[1], "Project ZIP file name missing")
+
+    def test_project_name_missing(self):
+        """Project name missing from request."""
+        with open(os.path.join(str(Path(__file__).parent), "helloworld.zip"), "rb") as f:
+            data_file = f.read()
+        msg = ServerMessage("prepare_execution", "1", "", ["helloworld.zip"])
+        self.socket.send_multipart([msg.to_bytes(), data_file])
+        response = self.socket.recv_multipart()
+        self.check_correct_error_response(response[1], "Project name missing")
+
+    def test_zip_file_missing(self):
+        """Project zip-file missing from request."""
+        msg = ServerMessage("prepare_execution", "1", json.dumps("projekti"), ["helloworld.zip"])
+        self.socket.send_multipart([msg.to_bytes()])
+        response = self.socket.recv_multipart()
+        self.check_correct_error_response(response[1], "Project ZIP file missing")
+
+    def check_correct_error_response(self, response, expected_err_str):
+        response_msg = ServerMessage.parse(response)
+        self.assertEqual("prepare_execution", response_msg.getCommand())
+        self.assertEqual("1", response_msg.getId())
+        data = response_msg.getData()
+        self.assertEqual("remote_execution_init_failed", data[0])
+        self.assertEqual(expected_err_str, data[1])
+
+
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `spine_engine-0.23.3/tests/server/test_ProjectRemoverService.py` & `spine_engine-0.23.4/tests/server/test_ProjectRemoverService.py`

 * *Ordering differences only*

 * *Files 8% similar despite different names*

```diff
@@ -1,68 +1,68 @@
-#####################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Unit tests for ProjectRemoverService class.
-"""
-
-import unittest
-import os
-import json
-from pathlib import Path
-import zmq
-from spine_engine.server.util.server_message import ServerMessage
-from spine_engine.server.engine_server import EngineServer, ServerSecurityModel
-
-
-class TestProjectRemoverService(unittest.TestCase):
-    def setUp(self):
-        self.service = EngineServer("tcp", 5559, ServerSecurityModel.NONE, "")
-        self.context = zmq.Context()
-        self.socket = self.context.socket(zmq.DEALER)
-        self.socket.connect("tcp://localhost:5559")
-
-    def tearDown(self):
-        self.service.close()
-        if not self.socket.closed:
-            self.socket.close()
-        if not self.context.closed:
-            self.context.term()
-
-    def test_remove_project_service(self):
-        """Extracts a project into received_projects folder, then removes it using the ProjectRemoverService."""
-        with open(os.path.join(str(Path(__file__).parent), "helloworld.zip"), "rb") as f:
-            file_data = f.read()
-        msg = ServerMessage("prepare_execution", "1", json.dumps("helloworld"), ["helloworld.zip"])
-        self.socket.send_multipart([msg.to_bytes(), file_data])
-        r1 = self.socket.recv_multipart()
-        r1_msg = ServerMessage.parse(r1[1])
-        self.assertEqual("prepare_execution", r1_msg.getCommand())
-        self.assertTrue(len(r1_msg.getId()) == 32)
-        self.assertEqual("", r1_msg.getData())
-        job_id = r1_msg.getId()
-        req = ServerMessage("remove_project", job_id, "")
-        self.socket.send_multipart([req.to_bytes()])
-        r2 = self.socket.recv_multipart()
-        r2_msg = ServerMessage.parse(r2[1])
-        self.assertEqual("remove_project", r2_msg.getCommand())
-        self.assertEqual(["server_event", "completed"], r2_msg.getData())
-
-    def test_project_dir_not_found(self):
-        """Sends an invalid job_id to server and check that response is as expected."""
-        msg = ServerMessage("remove_project", "12345", "")  # Invalid job_id on purpose
-        self.socket.send_multipart([msg.to_bytes()])
-        r = self.socket.recv_multipart()
-        r_msg = ServerMessage.parse(r[1])
-        self.assertEqual("server_init_failed", r_msg.getData()[0])
-
-
-if __name__ == "__main__":
-    unittest.main()
+#####################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Unit tests for ProjectRemoverService class.
+"""
+
+import unittest
+import os
+import json
+from pathlib import Path
+import zmq
+from spine_engine.server.util.server_message import ServerMessage
+from spine_engine.server.engine_server import EngineServer, ServerSecurityModel
+
+
+class TestProjectRemoverService(unittest.TestCase):
+    def setUp(self):
+        self.service = EngineServer("tcp", 5559, ServerSecurityModel.NONE, "")
+        self.context = zmq.Context()
+        self.socket = self.context.socket(zmq.DEALER)
+        self.socket.connect("tcp://localhost:5559")
+
+    def tearDown(self):
+        self.service.close()
+        if not self.socket.closed:
+            self.socket.close()
+        if not self.context.closed:
+            self.context.term()
+
+    def test_remove_project_service(self):
+        """Extracts a project into received_projects folder, then removes it using the ProjectRemoverService."""
+        with open(os.path.join(str(Path(__file__).parent), "helloworld.zip"), "rb") as f:
+            file_data = f.read()
+        msg = ServerMessage("prepare_execution", "1", json.dumps("helloworld"), ["helloworld.zip"])
+        self.socket.send_multipart([msg.to_bytes(), file_data])
+        r1 = self.socket.recv_multipart()
+        r1_msg = ServerMessage.parse(r1[1])
+        self.assertEqual("prepare_execution", r1_msg.getCommand())
+        self.assertTrue(len(r1_msg.getId()) == 32)
+        self.assertEqual("", r1_msg.getData())
+        job_id = r1_msg.getId()
+        req = ServerMessage("remove_project", job_id, "")
+        self.socket.send_multipart([req.to_bytes()])
+        r2 = self.socket.recv_multipart()
+        r2_msg = ServerMessage.parse(r2[1])
+        self.assertEqual("remove_project", r2_msg.getCommand())
+        self.assertEqual(["server_event", "completed"], r2_msg.getData())
+
+    def test_project_dir_not_found(self):
+        """Sends an invalid job_id to server and check that response is as expected."""
+        msg = ServerMessage("remove_project", "12345", "")  # Invalid job_id on purpose
+        self.socket.send_multipart([msg.to_bytes()])
+        r = self.socket.recv_multipart()
+        r_msg = ServerMessage.parse(r[1])
+        self.assertEqual("server_init_failed", r_msg.getData()[0])
+
+
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `spine_engine-0.23.3/tests/server/test_RemoteExecutionService.py` & `spine_engine-0.23.4/tests/server/test_RemoteExecutionService.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,352 +1,352 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Unit tests for RemoteExecutionService class.
-"""
-
-import unittest
-import json
-import os
-import random
-from pathlib import Path
-from unittest import mock
-from tempfile import TemporaryDirectory
-import zmq
-from spine_engine.server.engine_server import EngineServer, ServerSecurityModel
-from spine_engine.server.util.server_message import ServerMessage
-from spine_engine.server.util.event_data_converter import EventDataConverter
-
-
-class TestRemoteExecutionService(unittest.TestCase):
-    def setUp(self):
-        self._temp_dir = TemporaryDirectory()
-        self.service = EngineServer("tcp", 5559, ServerSecurityModel.NONE, "")
-        self.context = zmq.Context()
-        self.socket = self.context.socket(zmq.DEALER)
-        self.socket.identity = "Worker1".encode("ascii")
-        self.socket.connect("tcp://localhost:5559")
-        self.pull_socket = self.context.socket(zmq.PULL)
-        self.poller = zmq.Poller()
-        self.poller.register(self.socket, zmq.POLLIN)
-        self.poller.register(self.pull_socket, zmq.POLLIN)
-
-    def tearDown(self):
-        self.service.close()
-        if not self.socket.closed:
-            self.socket.close()
-        if not self.pull_socket.closed:
-            self.pull_socket.close()
-        if not self.context.closed:
-            self.context.term()
-        try:
-            self._temp_dir.cleanup()
-        except RecursionError:
-            print("RecursionError due to a PermissionError on Windows. Server resources not cleaned up properly.")
-
-    @mock.patch(
-        "spine_engine.server.project_extractor_service.ProjectExtractorService.INTERNAL_PROJECT_DIR",
-        new_callable=mock.PropertyMock,
-    )
-    def test_remote_execution1(self, mock_proj_dir):
-        """Tests executing a Hello World (DC -> Python Tool) project."""
-        mock_proj_dir.return_value = self._temp_dir.name
-        with open(os.path.join(str(Path(__file__).parent), "helloworld.zip"), "rb") as f:
-            file_data = f.read()
-        prepare_msg = ServerMessage("prepare_execution", "1", json.dumps("Hello World"), ["helloworld.zip"])
-        self.socket.send_multipart([prepare_msg.to_bytes(), file_data])
-        prepare_response = self.socket.recv_multipart()
-        prepare_response_msg = ServerMessage.parse(prepare_response[1])
-        job_id = prepare_response_msg.getId()
-        self.assertEqual("prepare_execution", prepare_response_msg.getCommand())
-        self.assertTrue(len(job_id) == 32)
-        self.assertEqual("", prepare_response_msg.getData())
-        # Send start_execution request
-        engine_data = self.make_engine_data_for_helloworld_project()
-        engine_data_json = json.dumps(engine_data)
-        start_msg = ServerMessage("start_execution", job_id, engine_data_json, None)
-        self.socket.send_multipart([start_msg.to_bytes()])
-        start_response = self.socket.recv_multipart()
-        start_response_msg = ServerMessage.parse(start_response[1])
-        start_response_msg_data = start_response_msg.getData()
-        self.assertEqual("remote_execution_started", start_response_msg_data[0])
-        self.receive_events(start_response_msg_data[1], start_response_msg_data[2])
-
-    @mock.patch(
-        "spine_engine.server.project_extractor_service.ProjectExtractorService.INTERNAL_PROJECT_DIR",
-        new_callable=mock.PropertyMock,
-    )
-    def test_remote_execution2(self, mock_project_dir):
-        """Tests executing a single DAG project with 3 items (DC -> Importer -> DS)."""
-        mock_project_dir.return_value = self._temp_dir.name
-        with open(os.path.join(str(Path(__file__).parent), "simple_importer.zip"), "rb") as f:
-            file_data = f.read()
-        prepare_msg = ServerMessage("prepare_execution", "1", json.dumps("Simple Importer"), ["simple_importer.zip"])
-        self.socket.send_multipart([prepare_msg.to_bytes(), file_data])
-        prepare_response = self.socket.recv_multipart()
-        prepare_response_msg = ServerMessage.parse(prepare_response[1])
-        job_id = prepare_response_msg.getId()
-        self.assertEqual("prepare_execution", prepare_response_msg.getCommand())
-        self.assertTrue(len(job_id) == 32)
-        self.assertEqual("", prepare_response_msg.getData())
-        # Send start_execution request
-        engine_data = self.make_engine_data_for_simple_importer_project()
-        engine_data_json = json.dumps(engine_data)
-        start_msg = ServerMessage("start_execution", job_id, engine_data_json, None)
-        self.socket.send_multipart([start_msg.to_bytes()])
-        start_response = self.socket.recv_multipart()
-        start_response_msg = ServerMessage.parse(start_response[1])
-        start_response_msg_data = start_response_msg.getData()
-        self.assertEqual("remote_execution_started", start_response_msg_data[0])
-        self.receive_events(start_response_msg_data[1], start_response_msg_data[2])
-
-    @mock.patch(
-        "spine_engine.server.project_extractor_service.ProjectExtractorService.INTERNAL_PROJECT_DIR",
-        new_callable=mock.PropertyMock,
-    )
-    def test_loop_calls(self, mock_proj_dir):
-        """Tests executing the Hellow World project (DC -> Tool) five times in a row."""
-        mock_proj_dir.return_value = self._temp_dir.name
-        engine_data = self.make_engine_data_for_helloworld_project()
-        engine_data_json = json.dumps(engine_data)
-        with open(os.path.join(str(Path(__file__).parent), "helloworld.zip"), "rb") as f:
-            file_data = f.read()
-        for i in range(5):
-            # Change project dir on each iteration
-            project_name = "loop_test_project_" + str(i)
-            prepare_msg = ServerMessage("prepare_execution", "1", json.dumps(project_name), ["helloworld.zip"])
-            self.socket.send_multipart([prepare_msg.to_bytes(), file_data])
-            prepare_response = self.socket.recv_multipart()
-            prepare_response_msg = ServerMessage.parse(prepare_response[1])
-            job_id = prepare_response_msg.getId()
-            self.assertEqual("prepare_execution", prepare_response_msg.getCommand())
-            # Send start_execution request
-            start_msg = ServerMessage("start_execution", job_id, engine_data_json, None)
-            self.socket.send_multipart([start_msg.to_bytes()])
-            start_response = self.socket.recv_multipart()
-            start_response_msg = ServerMessage.parse(start_response[1])
-            start_response_msg_data = start_response_msg.getData()
-            self.assertEqual("remote_execution_started", start_response_msg_data[0])
-            self.receive_events(start_response_msg_data[1], start_response_msg_data[2])
-            self.service.kill_persistent_exec_mngrs()
-            # TODO: Check that we use the same persistent exec manager in all 5 execution iterations
-
-    def receive_events(self, publish_port, job_id):
-        """Receives events from server until DAG execution has finished.
-
-        Args:
-            publish_port (str): Publish socket port
-        """
-        self.pull_socket.connect("tcp://localhost:" + publish_port)
-        while True:
-            socks = dict(self.poller.poll())
-            if socks.get(self.pull_socket) == zmq.POLLIN:
-                # Pull events
-                rcv = self.pull_socket.recv_multipart()
-                if len(rcv) > 1:  # Discard 'incoming_file', 'END' and file data messages
-                    continue
-                event_deconverted = EventDataConverter.deconvert(*rcv)
-                if event_deconverted[0] == "prompt":
-                    # Accept whatever prompt
-                    item_name = event_deconverted[1]["item_name"]
-                    accept_prompt_msg = ServerMessage("answer_prompt", job_id, json.dumps((item_name, True)), None)
-                    self.socket.send_multipart([accept_prompt_msg.to_bytes()])
-                if event_deconverted[0] == "dag_exec_finished":
-                    self.assertEqual("COMPLETED", event_deconverted[1])
-            if socks.get(self.socket) == zmq.POLLIN:
-                # Wait for the 'completed' msg to make sure that the server can be closed
-                resp = self.socket.recv_multipart()
-                resp_msg = ServerMessage.parse(resp[1])
-                resp_msg_data = resp_msg.getData()
-                self.assertEqual("remote_execution_event", resp_msg_data[0])
-                self.assertEqual("completed", resp_msg_data[1])
-                break
-
-    def assert_error_response(self, expected_event_type, expected_start_of_error_msg):
-        """Waits for a response from server and checks that the error msg is as expected."""
-        response = self.socket.recv_multipart()
-        server_msg = ServerMessage.parse(response[1])
-        msg_data = server_msg.getData()
-        self.assertEqual(expected_event_type, msg_data[0])
-        self.assertTrue(msg_data[1].startswith(expected_start_of_error_msg))
-
-    def make_default_item_dict(self, item_type):
-        """Keep up-to-date with spinetoolbox.project_item.project_item.item_dict()."""
-        return {"type": item_type, "description": "", "x": random.uniform(0, 100), "y": random.uniform(0, 100)}
-
-    def make_dc_item_dict(self, file_ref=None):
-        """Make a Data Connection item_dict.
-        Keep up-to-date with spine_items.data_connection.data_connection.item_dict()."""
-        d = self.make_default_item_dict("Data Connection")
-        d["file_references"] = file_ref if file_ref is not None else list()
-        d["db_references"] = []
-        d["db_credentials"] = {}
-        return d
-
-    def make_tool_item_dict(self, spec_name, exec_in_work, options=None, group_id=None):
-        """Make a Tool item_dict.
-        Keep up-to-date with spine_items.tool.tool.item_dict()."""
-        d = self.make_default_item_dict("Tool")
-        d["specification"] = spec_name
-        d["execute_in_work"] = exec_in_work
-        d["cmd_line_args"] = []
-        if options is not None:
-            d["options"] = options
-        if group_id is not None:
-            d["group_id"] = group_id
-        return d
-
-    def make_importer_item_dict(self):
-        d = self.make_default_item_dict("Importer")
-        d["specification"] = "Importer 1 - units.xlsx"
-        d["cancel_on_error"] = True
-        d["on_conflict"] = "replace"
-        d["file_selection"] = [["<Raw data>/units.xlsx", True]]
-        return d
-
-    def make_ds_item_dict(self):
-        d = self.make_default_item_dict("Data Store")
-        d["url"] = {
-            "dialect": "sqlite",
-            "username": "",
-            "password": "",
-            "host": "",
-            "port": "",
-            "database": {"type": "path", "relative": True, "path": ".spinetoolbox/items/ds1/DS1.sqlite"},
-        }
-        return d
-
-    @staticmethod
-    def make_python_tool_spec_dict(name, includes, input_files, includes_main_path, def_file_path, exec_settings=None):
-        return {
-            "name": name,
-            "tooltype": "python",
-            "includes": includes,
-            "description": "",
-            "inputfiles": input_files,
-            "inputfiles_opt": [],
-            "outputfiles": [],
-            "cmdline_args": [],
-            "includes_main_path": includes_main_path,
-            "execution_settings": exec_settings if exec_settings is not None else dict(),
-            "definition_file_path": def_file_path,
-        }
-
-    @staticmethod
-    def make_importer_spec_dict():
-        d = dict()
-        d["name"] = "Importer 1 - units.xlsx"
-        d["item_type"] = "Importer"
-        d["mapping"] = {
-            "table_mappings": {
-                "Sheet1": [
-                    {
-                        "": {
-                            "mapping": [
-                                {"map_type": "ObjectClass", "position": 0},
-                                {"map_type": "Object", "position": 1},
-                                {"map_type": "ObjectMetadata", "position": "hidden"},
-                            ]
-                        }
-                    }
-                ]
-            },
-            "selected_tables": ["Sheet1"],
-            "table_options": {"Sheet1": {}},
-            "table_types": {"Sheet1": {"0": "string", "1": "string"}},
-            "table_default_column_type": {},
-            "table_row_types": {},
-            "source_type": "ExcelConnector",
-        }
-        d["description"] = ("",)
-        d[
-            "definition_file_path"
-        ] = "C:\\data\\SpineToolboxData\\Projects\\Simple Importer\\Importer 1 - units.xlsx.json"
-        return d
-
-    def make_engine_data_for_helloworld_project(self):
-        """Returns an engine data dictionary for SpineEngine() for the project in file helloworld.zip.
-
-        engine_data dict must be the same as what is passed to SpineEngineWorker() in
-        spinetoolbox.project.create_engine_worker()
-        """
-        tool_item_dict = self.make_tool_item_dict("Simple Tool Spec", False)
-        dc_item_dict = self.make_dc_item_dict(file_ref=[{"type": "path", "relative": True, "path": "input_file.txt"}])
-        spec_dict = self.make_python_tool_spec_dict(
-            name="Simple Tool Spec",
-            includes=["simple_script.py"],
-            input_files=["input_file.txt"],
-            includes_main_path="../../..",
-            def_file_path="C:/data/temp/.spinetoolbox/specifications/Tool/simple_tool_spec.json",
-            exec_settings={"env": "", "kernel_spec_name": "py38", "use_jupyter_console": False, "executable": ""},
-        )
-        item_dicts = dict()
-        item_dicts["Data Connection 1"] = dc_item_dict
-        item_dicts["Simple Tool"] = tool_item_dict
-        specification_dicts = dict()
-        specification_dicts["Tool"] = [spec_dict]
-        engine_data = {
-            "items": item_dicts,
-            "specifications": specification_dicts,
-            "connections": [
-                {
-                    "name": "from Data Connection 1 to Simple Tool",
-                    "from": ["Data Connection 1", "right"],
-                    "to": ["Simple Tool", "left"],
-                }
-            ],
-            "jumps": [],
-            "execution_permits": {"Data Connection 1": True, "Simple Tool": True},
-            "items_module_name": "spine_items",
-            "settings": {},
-            "project_dir": "C:/data/temp",
-        }
-        return engine_data
-
-    def make_engine_data_for_simple_importer_project(self):
-        """Returns an engine data dictionary for SpineEngine() for the project in file simple_importer.zip.
-
-        engine_data dict must be the same as what is passed to SpineEngineWorker() in
-        spinetoolbox.project.create_engine_worker()
-        """
-        dc_item_dict = self.make_dc_item_dict()
-        importer_item_dict = self.make_importer_item_dict()
-        ds_item_dict = self.make_ds_item_dict()
-        importer_spec_dict = self.make_importer_spec_dict()
-        item_dicts = dict()
-        item_dicts["Raw data"] = dc_item_dict
-        item_dicts["Importer 1"] = importer_item_dict
-        item_dicts["DS1"] = ds_item_dict
-        specification_dicts = dict()
-        specification_dicts["Importer"] = [importer_spec_dict]
-        engine_data = {
-            "items": item_dicts,
-            "specifications": specification_dicts,
-            "connections": [
-                {"name": "from Raw data to Importer 1", "from": ["Raw data", "right"], "to": ["Importer 1", "left"]},
-                {
-                    "name": "from Importer 1 to DS1",
-                    "from": ["Importer 1", "right"],
-                    "to": ["DS1", "left"],
-                    "options": {"purge_before_writing": True, "purge_settings": None},
-                },
-            ],
-            "jumps": [],
-            "execution_permits": {"Importer 1": True, "DS1": True, "Raw data": True},
-            "items_module_name": "spine_items",
-            "settings": {},
-            "project_dir": "C:/data/temp",
-        }
-        return engine_data
-
-
-if __name__ == "__main__":
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Unit tests for RemoteExecutionService class.
+"""
+
+import unittest
+import json
+import os
+import random
+from pathlib import Path
+from unittest import mock
+from tempfile import TemporaryDirectory
+import zmq
+from spine_engine.server.engine_server import EngineServer, ServerSecurityModel
+from spine_engine.server.util.server_message import ServerMessage
+from spine_engine.server.util.event_data_converter import EventDataConverter
+
+
+class TestRemoteExecutionService(unittest.TestCase):
+    def setUp(self):
+        self._temp_dir = TemporaryDirectory()
+        self.service = EngineServer("tcp", 5559, ServerSecurityModel.NONE, "")
+        self.context = zmq.Context()
+        self.socket = self.context.socket(zmq.DEALER)
+        self.socket.identity = "Worker1".encode("ascii")
+        self.socket.connect("tcp://localhost:5559")
+        self.pull_socket = self.context.socket(zmq.PULL)
+        self.poller = zmq.Poller()
+        self.poller.register(self.socket, zmq.POLLIN)
+        self.poller.register(self.pull_socket, zmq.POLLIN)
+
+    def tearDown(self):
+        self.service.close()
+        if not self.socket.closed:
+            self.socket.close()
+        if not self.pull_socket.closed:
+            self.pull_socket.close()
+        if not self.context.closed:
+            self.context.term()
+        try:
+            self._temp_dir.cleanup()
+        except RecursionError:
+            print("RecursionError due to a PermissionError on Windows. Server resources not cleaned up properly.")
+
+    @mock.patch(
+        "spine_engine.server.project_extractor_service.ProjectExtractorService.INTERNAL_PROJECT_DIR",
+        new_callable=mock.PropertyMock,
+    )
+    def test_remote_execution1(self, mock_proj_dir):
+        """Tests executing a Hello World (DC -> Python Tool) project."""
+        mock_proj_dir.return_value = self._temp_dir.name
+        with open(os.path.join(str(Path(__file__).parent), "helloworld.zip"), "rb") as f:
+            file_data = f.read()
+        prepare_msg = ServerMessage("prepare_execution", "1", json.dumps("Hello World"), ["helloworld.zip"])
+        self.socket.send_multipart([prepare_msg.to_bytes(), file_data])
+        prepare_response = self.socket.recv_multipart()
+        prepare_response_msg = ServerMessage.parse(prepare_response[1])
+        job_id = prepare_response_msg.getId()
+        self.assertEqual("prepare_execution", prepare_response_msg.getCommand())
+        self.assertTrue(len(job_id) == 32)
+        self.assertEqual("", prepare_response_msg.getData())
+        # Send start_execution request
+        engine_data = self.make_engine_data_for_helloworld_project()
+        engine_data_json = json.dumps(engine_data)
+        start_msg = ServerMessage("start_execution", job_id, engine_data_json, None)
+        self.socket.send_multipart([start_msg.to_bytes()])
+        start_response = self.socket.recv_multipart()
+        start_response_msg = ServerMessage.parse(start_response[1])
+        start_response_msg_data = start_response_msg.getData()
+        self.assertEqual("remote_execution_started", start_response_msg_data[0])
+        self.receive_events(start_response_msg_data[1], start_response_msg_data[2])
+
+    @mock.patch(
+        "spine_engine.server.project_extractor_service.ProjectExtractorService.INTERNAL_PROJECT_DIR",
+        new_callable=mock.PropertyMock,
+    )
+    def test_remote_execution2(self, mock_project_dir):
+        """Tests executing a single DAG project with 3 items (DC -> Importer -> DS)."""
+        mock_project_dir.return_value = self._temp_dir.name
+        with open(os.path.join(str(Path(__file__).parent), "simple_importer.zip"), "rb") as f:
+            file_data = f.read()
+        prepare_msg = ServerMessage("prepare_execution", "1", json.dumps("Simple Importer"), ["simple_importer.zip"])
+        self.socket.send_multipart([prepare_msg.to_bytes(), file_data])
+        prepare_response = self.socket.recv_multipart()
+        prepare_response_msg = ServerMessage.parse(prepare_response[1])
+        job_id = prepare_response_msg.getId()
+        self.assertEqual("prepare_execution", prepare_response_msg.getCommand())
+        self.assertTrue(len(job_id) == 32)
+        self.assertEqual("", prepare_response_msg.getData())
+        # Send start_execution request
+        engine_data = self.make_engine_data_for_simple_importer_project()
+        engine_data_json = json.dumps(engine_data)
+        start_msg = ServerMessage("start_execution", job_id, engine_data_json, None)
+        self.socket.send_multipart([start_msg.to_bytes()])
+        start_response = self.socket.recv_multipart()
+        start_response_msg = ServerMessage.parse(start_response[1])
+        start_response_msg_data = start_response_msg.getData()
+        self.assertEqual("remote_execution_started", start_response_msg_data[0])
+        self.receive_events(start_response_msg_data[1], start_response_msg_data[2])
+
+    @mock.patch(
+        "spine_engine.server.project_extractor_service.ProjectExtractorService.INTERNAL_PROJECT_DIR",
+        new_callable=mock.PropertyMock,
+    )
+    def test_loop_calls(self, mock_proj_dir):
+        """Tests executing the Hellow World project (DC -> Tool) five times in a row."""
+        mock_proj_dir.return_value = self._temp_dir.name
+        engine_data = self.make_engine_data_for_helloworld_project()
+        engine_data_json = json.dumps(engine_data)
+        with open(os.path.join(str(Path(__file__).parent), "helloworld.zip"), "rb") as f:
+            file_data = f.read()
+        for i in range(5):
+            # Change project dir on each iteration
+            project_name = "loop_test_project_" + str(i)
+            prepare_msg = ServerMessage("prepare_execution", "1", json.dumps(project_name), ["helloworld.zip"])
+            self.socket.send_multipart([prepare_msg.to_bytes(), file_data])
+            prepare_response = self.socket.recv_multipart()
+            prepare_response_msg = ServerMessage.parse(prepare_response[1])
+            job_id = prepare_response_msg.getId()
+            self.assertEqual("prepare_execution", prepare_response_msg.getCommand())
+            # Send start_execution request
+            start_msg = ServerMessage("start_execution", job_id, engine_data_json, None)
+            self.socket.send_multipart([start_msg.to_bytes()])
+            start_response = self.socket.recv_multipart()
+            start_response_msg = ServerMessage.parse(start_response[1])
+            start_response_msg_data = start_response_msg.getData()
+            self.assertEqual("remote_execution_started", start_response_msg_data[0])
+            self.receive_events(start_response_msg_data[1], start_response_msg_data[2])
+            self.service.kill_persistent_exec_mngrs()
+            # TODO: Check that we use the same persistent exec manager in all 5 execution iterations
+
+    def receive_events(self, publish_port, job_id):
+        """Receives events from server until DAG execution has finished.
+
+        Args:
+            publish_port (str): Publish socket port
+        """
+        self.pull_socket.connect("tcp://localhost:" + publish_port)
+        while True:
+            socks = dict(self.poller.poll())
+            if socks.get(self.pull_socket) == zmq.POLLIN:
+                # Pull events
+                rcv = self.pull_socket.recv_multipart()
+                if len(rcv) > 1:  # Discard 'incoming_file', 'END' and file data messages
+                    continue
+                event_deconverted = EventDataConverter.deconvert(*rcv)
+                if event_deconverted[0] == "prompt":
+                    # Accept whatever prompt
+                    item_name = event_deconverted[1]["item_name"]
+                    accept_prompt_msg = ServerMessage("answer_prompt", job_id, json.dumps((item_name, True)), None)
+                    self.socket.send_multipart([accept_prompt_msg.to_bytes()])
+                if event_deconverted[0] == "dag_exec_finished":
+                    self.assertEqual("COMPLETED", event_deconverted[1])
+            if socks.get(self.socket) == zmq.POLLIN:
+                # Wait for the 'completed' msg to make sure that the server can be closed
+                resp = self.socket.recv_multipart()
+                resp_msg = ServerMessage.parse(resp[1])
+                resp_msg_data = resp_msg.getData()
+                self.assertEqual("remote_execution_event", resp_msg_data[0])
+                self.assertEqual("completed", resp_msg_data[1])
+                break
+
+    def assert_error_response(self, expected_event_type, expected_start_of_error_msg):
+        """Waits for a response from server and checks that the error msg is as expected."""
+        response = self.socket.recv_multipart()
+        server_msg = ServerMessage.parse(response[1])
+        msg_data = server_msg.getData()
+        self.assertEqual(expected_event_type, msg_data[0])
+        self.assertTrue(msg_data[1].startswith(expected_start_of_error_msg))
+
+    def make_default_item_dict(self, item_type):
+        """Keep up-to-date with spinetoolbox.project_item.project_item.item_dict()."""
+        return {"type": item_type, "description": "", "x": random.uniform(0, 100), "y": random.uniform(0, 100)}
+
+    def make_dc_item_dict(self, file_ref=None):
+        """Make a Data Connection item_dict.
+        Keep up-to-date with spine_items.data_connection.data_connection.item_dict()."""
+        d = self.make_default_item_dict("Data Connection")
+        d["file_references"] = file_ref if file_ref is not None else list()
+        d["db_references"] = []
+        d["db_credentials"] = {}
+        return d
+
+    def make_tool_item_dict(self, spec_name, exec_in_work, options=None, group_id=None):
+        """Make a Tool item_dict.
+        Keep up-to-date with spine_items.tool.tool.item_dict()."""
+        d = self.make_default_item_dict("Tool")
+        d["specification"] = spec_name
+        d["execute_in_work"] = exec_in_work
+        d["cmd_line_args"] = []
+        if options is not None:
+            d["options"] = options
+        if group_id is not None:
+            d["group_id"] = group_id
+        return d
+
+    def make_importer_item_dict(self):
+        d = self.make_default_item_dict("Importer")
+        d["specification"] = "Importer 1 - units.xlsx"
+        d["cancel_on_error"] = True
+        d["on_conflict"] = "replace"
+        d["file_selection"] = [["<Raw data>/units.xlsx", True]]
+        return d
+
+    def make_ds_item_dict(self):
+        d = self.make_default_item_dict("Data Store")
+        d["url"] = {
+            "dialect": "sqlite",
+            "username": "",
+            "password": "",
+            "host": "",
+            "port": "",
+            "database": {"type": "path", "relative": True, "path": ".spinetoolbox/items/ds1/DS1.sqlite"},
+        }
+        return d
+
+    @staticmethod
+    def make_python_tool_spec_dict(name, includes, input_files, includes_main_path, def_file_path, exec_settings=None):
+        return {
+            "name": name,
+            "tooltype": "python",
+            "includes": includes,
+            "description": "",
+            "inputfiles": input_files,
+            "inputfiles_opt": [],
+            "outputfiles": [],
+            "cmdline_args": [],
+            "includes_main_path": includes_main_path,
+            "execution_settings": exec_settings if exec_settings is not None else dict(),
+            "definition_file_path": def_file_path,
+        }
+
+    @staticmethod
+    def make_importer_spec_dict():
+        d = dict()
+        d["name"] = "Importer 1 - units.xlsx"
+        d["item_type"] = "Importer"
+        d["mapping"] = {
+            "table_mappings": {
+                "Sheet1": [
+                    {
+                        "": {
+                            "mapping": [
+                                {"map_type": "ObjectClass", "position": 0},
+                                {"map_type": "Object", "position": 1},
+                                {"map_type": "ObjectMetadata", "position": "hidden"},
+                            ]
+                        }
+                    }
+                ]
+            },
+            "selected_tables": ["Sheet1"],
+            "table_options": {"Sheet1": {}},
+            "table_types": {"Sheet1": {"0": "string", "1": "string"}},
+            "table_default_column_type": {},
+            "table_row_types": {},
+            "source_type": "ExcelConnector",
+        }
+        d["description"] = ("",)
+        d[
+            "definition_file_path"
+        ] = "C:\\data\\SpineToolboxData\\Projects\\Simple Importer\\Importer 1 - units.xlsx.json"
+        return d
+
+    def make_engine_data_for_helloworld_project(self):
+        """Returns an engine data dictionary for SpineEngine() for the project in file helloworld.zip.
+
+        engine_data dict must be the same as what is passed to SpineEngineWorker() in
+        spinetoolbox.project.create_engine_worker()
+        """
+        tool_item_dict = self.make_tool_item_dict("Simple Tool Spec", False)
+        dc_item_dict = self.make_dc_item_dict(file_ref=[{"type": "path", "relative": True, "path": "input_file.txt"}])
+        spec_dict = self.make_python_tool_spec_dict(
+            name="Simple Tool Spec",
+            includes=["simple_script.py"],
+            input_files=["input_file.txt"],
+            includes_main_path="../../..",
+            def_file_path="C:/data/temp/.spinetoolbox/specifications/Tool/simple_tool_spec.json",
+            exec_settings={"env": "", "kernel_spec_name": "py38", "use_jupyter_console": False, "executable": ""},
+        )
+        item_dicts = dict()
+        item_dicts["Data Connection 1"] = dc_item_dict
+        item_dicts["Simple Tool"] = tool_item_dict
+        specification_dicts = dict()
+        specification_dicts["Tool"] = [spec_dict]
+        engine_data = {
+            "items": item_dicts,
+            "specifications": specification_dicts,
+            "connections": [
+                {
+                    "name": "from Data Connection 1 to Simple Tool",
+                    "from": ["Data Connection 1", "right"],
+                    "to": ["Simple Tool", "left"],
+                }
+            ],
+            "jumps": [],
+            "execution_permits": {"Data Connection 1": True, "Simple Tool": True},
+            "items_module_name": "spine_items",
+            "settings": {},
+            "project_dir": "C:/data/temp",
+        }
+        return engine_data
+
+    def make_engine_data_for_simple_importer_project(self):
+        """Returns an engine data dictionary for SpineEngine() for the project in file simple_importer.zip.
+
+        engine_data dict must be the same as what is passed to SpineEngineWorker() in
+        spinetoolbox.project.create_engine_worker()
+        """
+        dc_item_dict = self.make_dc_item_dict()
+        importer_item_dict = self.make_importer_item_dict()
+        ds_item_dict = self.make_ds_item_dict()
+        importer_spec_dict = self.make_importer_spec_dict()
+        item_dicts = dict()
+        item_dicts["Raw data"] = dc_item_dict
+        item_dicts["Importer 1"] = importer_item_dict
+        item_dicts["DS1"] = ds_item_dict
+        specification_dicts = dict()
+        specification_dicts["Importer"] = [importer_spec_dict]
+        engine_data = {
+            "items": item_dicts,
+            "specifications": specification_dicts,
+            "connections": [
+                {"name": "from Raw data to Importer 1", "from": ["Raw data", "right"], "to": ["Importer 1", "left"]},
+                {
+                    "name": "from Importer 1 to DS1",
+                    "from": ["Importer 1", "right"],
+                    "to": ["DS1", "left"],
+                    "options": {"purge_before_writing": True, "purge_settings": None},
+                },
+            ],
+            "jumps": [],
+            "execution_permits": {"Importer 1": True, "DS1": True, "Raw data": True},
+            "items_module_name": "spine_items",
+            "settings": {},
+            "project_dir": "C:/data/temp",
+        }
+        return engine_data
+
+
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `spine_engine-0.23.3/tests/server/test_start_server.py` & `spine_engine-0.23.4/tests/server/test_start_server.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,47 +1,47 @@
-#####################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Unit tests for start_server.py script.
-"""
-import unittest
-import time
-from multiprocessing import Process
-from spine_engine.server.start_server import main
-
-
-class TestStartServer(unittest.TestCase):
-    def test_start_server(self):
-        server = Process(target=main, args=(["", "5001"],))
-        server.start()
-        self.assertTrue(server.is_alive())
-        time.sleep(5)  # Starting the server takes some time (in tests)
-        server.terminate()
-        server.join()  # process is still alive after terminate if we don't join()
-        self.assertFalse(server.is_alive())
-        self.assertTrue(server.exitcode == 0)
-
-    def test_start_server_with_security_folder_missing(self):
-        # Expected behaviour is to exit immediately and print an error message
-        server = Process(target=main, args=(["", "5001", "stonehouse", ""],))
-        server.start()
-        server.join()
-        self.assertTrue(server.exitcode == 0)
-
-    def test_invalid_number_of_args(self):
-        server = Process(target=main, args=(["", "a", "b"],))
-        server.start()
-        server.join()
-        self.assertTrue(server.exitcode == 0)
-
-
-if __name__ == "__main__":
-    unittest.main()
+#####################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Unit tests for start_server.py script.
+"""
+import unittest
+import time
+from multiprocessing import Process
+from spine_engine.server.start_server import main
+
+
+class TestStartServer(unittest.TestCase):
+    def test_start_server(self):
+        server = Process(target=main, args=(["", "5001"],))
+        server.start()
+        self.assertTrue(server.is_alive())
+        time.sleep(5)  # Starting the server takes some time (in tests)
+        server.terminate()
+        server.join()  # process is still alive after terminate if we don't join()
+        self.assertFalse(server.is_alive())
+        self.assertTrue(server.exitcode == 0)
+
+    def test_start_server_with_security_folder_missing(self):
+        # Expected behaviour is to exit immediately and print an error message
+        server = Process(target=main, args=(["", "5001", "stonehouse", ""],))
+        server.start()
+        server.join()
+        self.assertTrue(server.exitcode == 0)
+
+    def test_invalid_number_of_args(self):
+        server = Process(target=main, args=(["", "a", "b"],))
+        server.start()
+        server.join()
+        self.assertTrue(server.exitcode == 0)
+
+
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `spine_engine-0.23.3/tests/server/util/__init__.py` & `spine_engine-0.23.4/tests/execution_managers/__init__.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+"""Unit tests for ``execution_managers`` package."""
```

### Comparing `spine_engine-0.23.3/tests/server/util/projectforpackagingtests/.spinetoolbox/project.json` & `spine_engine-0.23.4/tests/server/util/projectforpackagingtests/.spinetoolbox/project.json`

 * *Format-specific differences are supported for JSON files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: JSON text data*

 * *Files 27% similar despite different names*

```diff
@@ -1,105 +1,102 @@
-00000000: 7b0d 0a20 2020 2022 7072 6f6a 6563 7422  {..    "project"
-00000010: 3a20 7b0d 0a20 2020 2020 2020 2022 7665  : {..        "ve
-00000020: 7273 696f 6e22 3a20 3130 2c0d 0a20 2020  rsion": 10,..   
-00000030: 2020 2020 2022 6465 7363 7269 7074 696f       "descriptio
-00000040: 6e22 3a20 2222 2c0d 0a20 2020 2020 2020  n": "",..       
-00000050: 2022 7370 6563 6966 6963 6174 696f 6e73   "specifications
-00000060: 223a 207b 0d0a 2020 2020 2020 2020 2020  ": {..          
-00000070: 2020 2254 6f6f 6c22 3a20 5b0d 0a20 2020    "Tool": [..   
-00000080: 2020 2020 2020 2020 2020 2020 207b 0d0a               {..
-00000090: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000000a0: 2020 2020 2274 7970 6522 3a20 2270 6174      "type": "pat
-000000b0: 6822 2c0d 0a20 2020 2020 2020 2020 2020  h",..           
-000000c0: 2020 2020 2020 2020 2022 7265 6c61 7469           "relati
-000000d0: 7665 223a 2074 7275 652c 0d0a 2020 2020  ve": true,..    
-000000e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000000f0: 2270 6174 6822 3a20 222e 7370 696e 6574  "path": ".spinet
-00000100: 6f6f 6c62 6f78 2f73 7065 6369 6669 6361  oolbox/specifica
-00000110: 7469 6f6e 732f 546f 6f6c 2f68 656c 6c6f  tions/Tool/hello
-00000120: 776f 726c 642e 6a73 6f6e 220d 0a20 2020  world.json"..   
-00000130: 2020 2020 2020 2020 2020 2020 207d 2c0d               },.
-00000140: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00000150: 207b 0d0a 2020 2020 2020 2020 2020 2020   {..            
-00000160: 2020 2020 2020 2020 2274 7970 6522 3a20          "type": 
-00000170: 2270 6174 6822 2c0d 0a20 2020 2020 2020  "path",..       
-00000180: 2020 2020 2020 2020 2020 2020 2022 7265               "re
-00000190: 6c61 7469 7665 223a 2074 7275 652c 0d0a  lative": true,..
-000001a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000001b0: 2020 2020 2270 6174 6822 3a20 222e 7370      "path": ".sp
-000001c0: 696e 6574 6f6f 6c62 6f78 2f73 7065 6369  inetoolbox/speci
-000001d0: 6669 6361 7469 6f6e 732f 546f 6f6c 2f68  fications/Tool/h
-000001e0: 656c 6c6f 776f 726c 6432 2e6a 736f 6e22  elloworld2.json"
-000001f0: 0d0a 2020 2020 2020 2020 2020 2020 2020  ..              
-00000200: 2020 7d0d 0a20 2020 2020 2020 2020 2020    }..           
-00000210: 205d 0d0a 2020 2020 2020 2020 7d2c 0d0a   ]..        },..
-00000220: 2020 2020 2020 2020 2263 6f6e 6e65 6374          "connect
-00000230: 696f 6e73 223a 205b 0d0a 2020 2020 2020  ions": [..      
-00000240: 2020 2020 2020 7b0d 0a20 2020 2020 2020        {..       
-00000250: 2020 2020 2020 2020 2022 6e61 6d65 223a           "name":
-00000260: 2022 6672 6f6d 2044 6174 6120 436f 6e6e   "from Data Conn
-00000270: 6563 7469 6f6e 2031 2074 6f20 6865 6c6c  ection 1 to hell
-00000280: 6f77 6f72 6c64 222c 0d0a 2020 2020 2020  oworld",..      
-00000290: 2020 2020 2020 2020 2020 2266 726f 6d22            "from"
-000002a0: 3a20 5b0d 0a20 2020 2020 2020 2020 2020  : [..           
-000002b0: 2020 2020 2020 2020 2022 4461 7461 2043           "Data C
-000002c0: 6f6e 6e65 6374 696f 6e20 3122 2c0d 0a20  onnection 1",.. 
-000002d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000002e0: 2020 2022 7269 6768 7422 0d0a 2020 2020     "right"..    
-000002f0: 2020 2020 2020 2020 2020 2020 5d2c 0d0a              ],..
+00000000: 7b0a 2020 2020 2270 726f 6a65 6374 223a  {.    "project":
+00000010: 207b 0a20 2020 2020 2020 2022 7665 7273   {.        "vers
+00000020: 696f 6e22 3a20 3130 2c0a 2020 2020 2020  ion": 10,.      
+00000030: 2020 2264 6573 6372 6970 7469 6f6e 223a    "description":
+00000040: 2022 222c 0a20 2020 2020 2020 2022 7370   "",.        "sp
+00000050: 6563 6966 6963 6174 696f 6e73 223a 207b  ecifications": {
+00000060: 0a20 2020 2020 2020 2020 2020 2022 546f  .            "To
+00000070: 6f6c 223a 205b 0a20 2020 2020 2020 2020  ol": [.         
+00000080: 2020 2020 2020 207b 0a20 2020 2020 2020         {.       
+00000090: 2020 2020 2020 2020 2020 2020 2022 7479               "ty
+000000a0: 7065 223a 2022 7061 7468 222c 0a20 2020  pe": "path",.   
+000000b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000000c0: 2022 7265 6c61 7469 7665 223a 2074 7275   "relative": tru
+000000d0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+000000e0: 2020 2020 2020 2022 7061 7468 223a 2022         "path": "
+000000f0: 2e73 7069 6e65 746f 6f6c 626f 782f 7370  .spinetoolbox/sp
+00000100: 6563 6966 6963 6174 696f 6e73 2f54 6f6f  ecifications/Too
+00000110: 6c2f 6865 6c6c 6f77 6f72 6c64 2e6a 736f  l/helloworld.jso
+00000120: 6e22 0a20 2020 2020 2020 2020 2020 2020  n".             
+00000130: 2020 207d 2c0a 2020 2020 2020 2020 2020     },.          
+00000140: 2020 2020 2020 7b0a 2020 2020 2020 2020        {.        
+00000150: 2020 2020 2020 2020 2020 2020 2274 7970              "typ
+00000160: 6522 3a20 2270 6174 6822 2c0a 2020 2020  e": "path",.    
+00000170: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000180: 2272 656c 6174 6976 6522 3a20 7472 7565  "relative": true
+00000190: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+000001a0: 2020 2020 2020 2270 6174 6822 3a20 222e        "path": ".
+000001b0: 7370 696e 6574 6f6f 6c62 6f78 2f73 7065  spinetoolbox/spe
+000001c0: 6369 6669 6361 7469 6f6e 732f 546f 6f6c  cifications/Tool
+000001d0: 2f68 656c 6c6f 776f 726c 6432 2e6a 736f  /helloworld2.jso
+000001e0: 6e22 0a20 2020 2020 2020 2020 2020 2020  n".             
+000001f0: 2020 207d 0a20 2020 2020 2020 2020 2020     }.           
+00000200: 205d 0a20 2020 2020 2020 207d 2c0a 2020   ].        },.  
+00000210: 2020 2020 2020 2263 6f6e 6e65 6374 696f        "connectio
+00000220: 6e73 223a 205b 0a20 2020 2020 2020 2020  ns": [.         
+00000230: 2020 207b 0a20 2020 2020 2020 2020 2020     {.           
+00000240: 2020 2020 2022 6e61 6d65 223a 2022 6672       "name": "fr
+00000250: 6f6d 2044 6174 6120 436f 6e6e 6563 7469  om Data Connecti
+00000260: 6f6e 2031 2074 6f20 6865 6c6c 6f77 6f72  on 1 to hellowor
+00000270: 6c64 222c 0a20 2020 2020 2020 2020 2020  ld",.           
+00000280: 2020 2020 2022 6672 6f6d 223a 205b 0a20       "from": [. 
+00000290: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000002a0: 2020 2022 4461 7461 2043 6f6e 6e65 6374     "Data Connect
+000002b0: 696f 6e20 3122 2c0a 2020 2020 2020 2020  ion 1",.        
+000002c0: 2020 2020 2020 2020 2020 2020 2272 6967              "rig
+000002d0: 6874 220a 2020 2020 2020 2020 2020 2020  ht".            
+000002e0: 2020 2020 5d2c 0a20 2020 2020 2020 2020      ],.         
+000002f0: 2020 2020 2020 2022 746f 223a 205b 0a20         "to": [. 
 00000300: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000310: 2274 6f22 3a20 5b0d 0a20 2020 2020 2020  "to": [..       
-00000320: 2020 2020 2020 2020 2020 2020 2022 6865               "he
-00000330: 6c6c 6f77 6f72 6c64 222c 0d0a 2020 2020  lloworld",..    
-00000340: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000350: 226c 6566 7422 0d0a 2020 2020 2020 2020  "left"..        
-00000360: 2020 2020 2020 2020 5d0d 0a20 2020 2020          ]..     
-00000370: 2020 2020 2020 207d 0d0a 2020 2020 2020         }..      
-00000380: 2020 5d2c 0d0a 2020 2020 2020 2020 226a    ],..        "j
-00000390: 756d 7073 223a 205b 5d0d 0a20 2020 207d  umps": []..    }
-000003a0: 2c0d 0a20 2020 2022 6974 656d 7322 3a20  ,..    "items": 
-000003b0: 7b0d 0a20 2020 2020 2020 2022 6865 6c6c  {..        "hell
-000003c0: 6f77 6f72 6c64 223a 207b 0d0a 2020 2020  oworld": {..    
-000003d0: 2020 2020 2020 2020 2274 7970 6522 3a20          "type": 
-000003e0: 2254 6f6f 6c22 2c0d 0a20 2020 2020 2020  "Tool",..       
-000003f0: 2020 2020 2022 6465 7363 7269 7074 696f       "descriptio
-00000400: 6e22 3a20 2222 2c0d 0a20 2020 2020 2020  n": "",..       
-00000410: 2020 2020 2022 7822 3a20 3535 2e38 3839       "x": 55.889
-00000420: 3433 3037 3332 3438 3430 372c 0d0a 2020  43073248407,..  
-00000430: 2020 2020 2020 2020 2020 2279 223a 2032            "y": 2
-00000440: 332e 3439 3938 3137 3534 3234 3632 3834  3.49981754246284
-00000450: 332c 0d0a 2020 2020 2020 2020 2020 2020  3,..            
-00000460: 2273 7065 6369 6669 6361 7469 6f6e 223a  "specification":
-00000470: 2022 6865 6c6c 6f77 6f72 6c64 3222 2c0d   "helloworld2",.
-00000480: 0a20 2020 2020 2020 2020 2020 2022 6578  .            "ex
-00000490: 6563 7574 655f 696e 5f77 6f72 6b22 3a20  ecute_in_work": 
-000004a0: 7472 7565 2c0d 0a20 2020 2020 2020 2020  true,..         
-000004b0: 2020 2022 636d 645f 6c69 6e65 5f61 7267     "cmd_line_arg
-000004c0: 7322 3a20 5b5d 0d0a 2020 2020 2020 2020  s": []..        
-000004d0: 7d2c 0d0a 2020 2020 2020 2020 2244 6174  },..        "Dat
-000004e0: 6120 436f 6e6e 6563 7469 6f6e 2031 223a  a Connection 1":
-000004f0: 207b 0d0a 2020 2020 2020 2020 2020 2020   {..            
-00000500: 2274 7970 6522 3a20 2244 6174 6120 436f  "type": "Data Co
-00000510: 6e6e 6563 7469 6f6e 222c 0d0a 2020 2020  nnection",..    
-00000520: 2020 2020 2020 2020 2264 6573 6372 6970          "descrip
-00000530: 7469 6f6e 223a 2022 222c 0d0a 2020 2020  tion": "",..    
-00000540: 2020 2020 2020 2020 2278 223a 202d 3130          "x": -10
-00000550: 332e 3931 3430 3935 3637 3430 3937 3634  3.91409567409764
-00000560: 2c0d 0a20 2020 2020 2020 2020 2020 2022  ,..            "
-00000570: 7922 3a20 3536 2e37 3930 3130 3734 3834  y": 56.790107484
-00000580: 3037 3634 3434 2c0d 0a20 2020 2020 2020  076444,..       
-00000590: 2020 2020 2022 6669 6c65 5f72 6566 6572       "file_refer
-000005a0: 656e 6365 7322 3a20 5b0d 0a20 2020 2020  ences": [..     
-000005b0: 2020 2020 2020 2020 2020 207b 0d0a 2020             {..  
-000005c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000005d0: 2020 2274 7970 6522 3a20 2270 6174 6822    "type": "path"
-000005e0: 2c0d 0a20 2020 2020 2020 2020 2020 2020  ,..             
-000005f0: 2020 2020 2020 2022 7265 6c61 7469 7665         "relative
-00000600: 223a 2074 7275 652c 0d0a 2020 2020 2020  ": true,..      
-00000610: 2020 2020 2020 2020 2020 2020 2020 2270                "p
-00000620: 6174 6822 3a20 2269 6e70 7574 322e 7478  ath": "input2.tx
-00000630: 7422 0d0a 2020 2020 2020 2020 2020 2020  t"..            
-00000640: 2020 2020 7d0d 0a20 2020 2020 2020 2020      }..         
-00000650: 2020 205d 2c0d 0a20 2020 2020 2020 2020     ],..         
-00000660: 2020 2022 6462 5f72 6566 6572 656e 6365     "db_reference
-00000670: 7322 3a20 5b5d 0d0a 2020 2020 2020 2020  s": []..        
-00000680: 7d0d 0a20 2020 207d 0d0a 7d              }..    }..}
+00000310: 2020 2022 6865 6c6c 6f77 6f72 6c64 222c     "helloworld",
+00000320: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00000330: 2020 2020 2022 6c65 6674 220a 2020 2020       "left".    
+00000340: 2020 2020 2020 2020 2020 2020 5d0a 2020              ].  
+00000350: 2020 2020 2020 2020 2020 7d0a 2020 2020            }.    
+00000360: 2020 2020 5d2c 0a20 2020 2020 2020 2022      ],.        "
+00000370: 6a75 6d70 7322 3a20 5b5d 0a20 2020 207d  jumps": [].    }
+00000380: 2c0a 2020 2020 2269 7465 6d73 223a 207b  ,.    "items": {
+00000390: 0a20 2020 2020 2020 2022 6865 6c6c 6f77  .        "hellow
+000003a0: 6f72 6c64 223a 207b 0a20 2020 2020 2020  orld": {.       
+000003b0: 2020 2020 2022 7479 7065 223a 2022 546f       "type": "To
+000003c0: 6f6c 222c 0a20 2020 2020 2020 2020 2020  ol",.           
+000003d0: 2022 6465 7363 7269 7074 696f 6e22 3a20   "description": 
+000003e0: 2222 2c0a 2020 2020 2020 2020 2020 2020  "",.            
+000003f0: 2278 223a 2035 352e 3838 3934 3330 3733  "x": 55.88943073
+00000400: 3234 3834 3037 2c0a 2020 2020 2020 2020  248407,.        
+00000410: 2020 2020 2279 223a 2032 332e 3439 3938      "y": 23.4998
+00000420: 3137 3534 3234 3632 3834 332c 0a20 2020  17542462843,.   
+00000430: 2020 2020 2020 2020 2022 7370 6563 6966           "specif
+00000440: 6963 6174 696f 6e22 3a20 2268 656c 6c6f  ication": "hello
+00000450: 776f 726c 6432 222c 0a20 2020 2020 2020  world2",.       
+00000460: 2020 2020 2022 6578 6563 7574 655f 696e       "execute_in
+00000470: 5f77 6f72 6b22 3a20 7472 7565 2c0a 2020  _work": true,.  
+00000480: 2020 2020 2020 2020 2020 2263 6d64 5f6c            "cmd_l
+00000490: 696e 655f 6172 6773 223a 205b 5d0a 2020  ine_args": [].  
+000004a0: 2020 2020 2020 7d2c 0a20 2020 2020 2020        },.       
+000004b0: 2022 4461 7461 2043 6f6e 6e65 6374 696f   "Data Connectio
+000004c0: 6e20 3122 3a20 7b0a 2020 2020 2020 2020  n 1": {.        
+000004d0: 2020 2020 2274 7970 6522 3a20 2244 6174      "type": "Dat
+000004e0: 6120 436f 6e6e 6563 7469 6f6e 222c 0a20  a Connection",. 
+000004f0: 2020 2020 2020 2020 2020 2022 6465 7363             "desc
+00000500: 7269 7074 696f 6e22 3a20 2222 2c0a 2020  ription": "",.  
+00000510: 2020 2020 2020 2020 2020 2278 223a 202d            "x": -
+00000520: 3130 332e 3931 3430 3935 3637 3430 3937  103.914095674097
+00000530: 3634 2c0a 2020 2020 2020 2020 2020 2020  64,.            
+00000540: 2279 223a 2035 362e 3739 3031 3037 3438  "y": 56.79010748
+00000550: 3430 3736 3434 342c 0a20 2020 2020 2020  4076444,.       
+00000560: 2020 2020 2022 6669 6c65 5f72 6566 6572       "file_refer
+00000570: 656e 6365 7322 3a20 5b0a 2020 2020 2020  ences": [.      
+00000580: 2020 2020 2020 2020 2020 7b0a 2020 2020            {.    
+00000590: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000005a0: 2274 7970 6522 3a20 2270 6174 6822 2c0a  "type": "path",.
+000005b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000005c0: 2020 2020 2272 656c 6174 6976 6522 3a20      "relative": 
+000005d0: 7472 7565 2c0a 2020 2020 2020 2020 2020  true,.          
+000005e0: 2020 2020 2020 2020 2020 2270 6174 6822            "path"
+000005f0: 3a20 2269 6e70 7574 322e 7478 7422 0a20  : "input2.txt". 
+00000600: 2020 2020 2020 2020 2020 2020 2020 207d                 }
+00000610: 0a20 2020 2020 2020 2020 2020 205d 2c0a  .            ],.
+00000620: 2020 2020 2020 2020 2020 2020 2264 625f              "db_
+00000630: 7265 6665 7265 6e63 6573 223a 205b 5d0a  references": [].
+00000640: 2020 2020 2020 2020 7d0a 2020 2020 7d0a          }.    }.
+00000650: 7d                                       }
```

### Comparing `spine_engine-0.23.3/tests/server/util/test_EventDataConverter.py` & `spine_engine-0.23.4/tests/server/util/test_EventDataConverter.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,149 +1,149 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Unit tests for EventDataConverter class.
-"""
-
-import unittest
-from copy import deepcopy
-from spine_engine.spine_engine import ItemExecutionFinishState
-from spine_engine.server.util.event_data_converter import EventDataConverter
-
-
-class TestEventDataConverter(unittest.TestCase):
-    def make_event_data(self):
-        test_events = [
-            ('exec_started', {'item_name': 'helloworld', 'direction': 'BACKWARD'}),
-            ('exec_started', {'item_name': 'Data Connection 1', 'direction': 'BACKWARD'}),
-            (
-                'exec_finished',
-                {
-                    'item_name': 'helloworld',
-                    'direction': 'BACKWARD',
-                    'state': 'RUNNING',
-                    'item_state': ItemExecutionFinishState.SUCCESS,
-                },
-            ),
-            (
-                'exec_finished',
-                {
-                    'item_name': 'Data Connection 1',
-                    'direction': 'BACKWARD',
-                    'state': 'RUNNING',
-                    'item_state': ItemExecutionFinishState.SUCCESS,
-                },
-            ),
-            ('exec_started', {'item_name': 'Data Connection 1', 'direction': 'FORWARD'}),
-            (
-                'event_msg',
-                {
-                    'item_name': 'Data Connection 1',
-                    'filter_id': '',
-                    'msg_type': 'msg_success',
-                    'msg_text': 'Executing Data Connection Data Connection 1 finished',
-                },
-            ),
-            (
-                'exec_finished',
-                {
-                    'item_name': 'Data Connection 1',
-                    'direction': 'FORWARD',
-                    'state': 'RUNNING',
-                    'item_state': ItemExecutionFinishState.SUCCESS,
-                },
-            ),
-            ('flash', {'item_name': 'from Data Connection 1 to helloworld'}),
-            ('exec_started', {'item_name': 'helloworld', 'direction': 'FORWARD'}),
-            (
-                'event_msg',
-                {
-                    'item_name': 'helloworld',
-                    'filter_id': '',
-                    'msg_type': 'msg',
-                    'msg_text': "*** Executing Tool specification <b>helloworld2</b> in <a style='color:#99CCFF;' title='C:\\data\\GIT\\SPINEENGINE\\spine_engine\\server\\received_projects\\helloworld__35bc62cea0324e8788144ce81342f4f1'href='file:///C:\\data\\GIT\\SPINEENGINE\\spine_engine\\server\\received_projects\\helloworld__35bc62cea0324e8788144ce81342f4f1'>source directory</a> ***",
-                },
-            ),
-            (
-                'persistent_execution_msg',
-                {
-                    'item_name': 'helloworld',
-                    'filter_id': '',
-                    'type': 'persistent_started',
-                    'key': '6ceeb59271114fc2a0f787266f72dedc',
-                    'language': 'python',
-                },
-            ),
-            (
-                'persistent_execution_msg',
-                {'item_name': 'helloworld', 'filter_id': '', 'type': 'stdin', 'data': '# Running python helloworld.py'},
-            ),
-            (
-                'persistent_execution_msg',
-                {'item_name': 'helloworld', 'filter_id': '', 'type': 'stdout', 'data': 'helloo'},
-            ),
-            (
-                'event_msg',
-                {
-                    'item_name': 'helloworld',
-                    'filter_id': '',
-                    'msg_type': 'msg',
-                    'msg_text': "*** Archiving output files to <a style='color:#BB99FF;' title='C:\\data\\GIT\\SPINEENGINE\\spine_engine\\server\\received_projects\\helloworld__35bc62cea0324e8788144ce81342f4f1\\.spinetoolbox\\items\\helloworld\\output\\2022-08-19T13.03.13' href='file:///C:\\data\\GIT\\SPINEENGINE\\spine_engine\\server\\received_projects\\helloworld__35bc62cea0324e8788144ce81342f4f1\\.spinetoolbox\\items\\helloworld\\output\\2022-08-19T13.03.13'>results directory</a> ***",
-                },
-            ),
-            (
-                'exec_finished',
-                {
-                    'item_name': 'helloworld',
-                    'direction': 'FORWARD',
-                    'state': 'RUNNING',
-                    'item_state': ItemExecutionFinishState.SUCCESS,
-                },
-            ),
-            ('dag_exec_finished', 'COMPLETED'),
-        ]
-        return test_events
-
-    def test_convert(self):
-        event_data = self.make_event_data()
-        converted_events = list()
-        for event in event_data:
-            json_str = EventDataConverter.convert(event[0], event[1])
-            converted_events.append(json_str)
-        self.assertEqual(16, len(converted_events))
-        # Check that item_state values are cast to strings i.e. ItemExecutionFinishState.SUCCESS -> "SUCCESS"
-        n = 0  # Counter for how many ItemExecutionFinishStates were cast to strings
-        for converted_event in converted_events:
-            if converted_event.startswith('{"event_type": "exec_finished"'):
-                self.assertTrue('"item_state": "SUCCESS"' in converted_event)
-                n += 1
-        self.assertEqual(4, n)
-
-    def test_convert_deconvert(self):
-        """Converts events, then deconverts them back."""
-        event_data = self.make_event_data()
-        expected_data = deepcopy(event_data)
-        converted_events = list()
-        for event in event_data:
-            json_str = EventDataConverter.convert(event[0], event[1])
-            converted_events.append(json_str)
-        self.assertEqual(16, len(converted_events))
-        deconverted_events = list()
-        for conv_event in converted_events:
-            event_tuple = EventDataConverter.deconvert(conv_event.encode("utf-8"))
-            deconverted_events.append(event_tuple)
-        self.assertEqual(16, len(deconverted_events))
-        # The converted & deconverted list must be equal to the original
-        self.assertEqual(expected_data, deconverted_events)
-
-
-if __name__ == '__main__':
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Unit tests for EventDataConverter class.
+"""
+
+import unittest
+from copy import deepcopy
+from spine_engine.spine_engine import ItemExecutionFinishState
+from spine_engine.server.util.event_data_converter import EventDataConverter
+
+
+class TestEventDataConverter(unittest.TestCase):
+    def make_event_data(self):
+        test_events = [
+            ('exec_started', {'item_name': 'helloworld', 'direction': 'BACKWARD'}),
+            ('exec_started', {'item_name': 'Data Connection 1', 'direction': 'BACKWARD'}),
+            (
+                'exec_finished',
+                {
+                    'item_name': 'helloworld',
+                    'direction': 'BACKWARD',
+                    'state': 'RUNNING',
+                    'item_state': ItemExecutionFinishState.SUCCESS,
+                },
+            ),
+            (
+                'exec_finished',
+                {
+                    'item_name': 'Data Connection 1',
+                    'direction': 'BACKWARD',
+                    'state': 'RUNNING',
+                    'item_state': ItemExecutionFinishState.SUCCESS,
+                },
+            ),
+            ('exec_started', {'item_name': 'Data Connection 1', 'direction': 'FORWARD'}),
+            (
+                'event_msg',
+                {
+                    'item_name': 'Data Connection 1',
+                    'filter_id': '',
+                    'msg_type': 'msg_success',
+                    'msg_text': 'Executing Data Connection Data Connection 1 finished',
+                },
+            ),
+            (
+                'exec_finished',
+                {
+                    'item_name': 'Data Connection 1',
+                    'direction': 'FORWARD',
+                    'state': 'RUNNING',
+                    'item_state': ItemExecutionFinishState.SUCCESS,
+                },
+            ),
+            ('flash', {'item_name': 'from Data Connection 1 to helloworld'}),
+            ('exec_started', {'item_name': 'helloworld', 'direction': 'FORWARD'}),
+            (
+                'event_msg',
+                {
+                    'item_name': 'helloworld',
+                    'filter_id': '',
+                    'msg_type': 'msg',
+                    'msg_text': "*** Executing Tool specification <b>helloworld2</b> in <a style='color:#99CCFF;' title='C:\\data\\GIT\\SPINEENGINE\\spine_engine\\server\\received_projects\\helloworld__35bc62cea0324e8788144ce81342f4f1'href='file:///C:\\data\\GIT\\SPINEENGINE\\spine_engine\\server\\received_projects\\helloworld__35bc62cea0324e8788144ce81342f4f1'>source directory</a> ***",
+                },
+            ),
+            (
+                'persistent_execution_msg',
+                {
+                    'item_name': 'helloworld',
+                    'filter_id': '',
+                    'type': 'persistent_started',
+                    'key': '6ceeb59271114fc2a0f787266f72dedc',
+                    'language': 'python',
+                },
+            ),
+            (
+                'persistent_execution_msg',
+                {'item_name': 'helloworld', 'filter_id': '', 'type': 'stdin', 'data': '# Running python helloworld.py'},
+            ),
+            (
+                'persistent_execution_msg',
+                {'item_name': 'helloworld', 'filter_id': '', 'type': 'stdout', 'data': 'helloo'},
+            ),
+            (
+                'event_msg',
+                {
+                    'item_name': 'helloworld',
+                    'filter_id': '',
+                    'msg_type': 'msg',
+                    'msg_text': "*** Archiving output files to <a style='color:#BB99FF;' title='C:\\data\\GIT\\SPINEENGINE\\spine_engine\\server\\received_projects\\helloworld__35bc62cea0324e8788144ce81342f4f1\\.spinetoolbox\\items\\helloworld\\output\\2022-08-19T13.03.13' href='file:///C:\\data\\GIT\\SPINEENGINE\\spine_engine\\server\\received_projects\\helloworld__35bc62cea0324e8788144ce81342f4f1\\.spinetoolbox\\items\\helloworld\\output\\2022-08-19T13.03.13'>results directory</a> ***",
+                },
+            ),
+            (
+                'exec_finished',
+                {
+                    'item_name': 'helloworld',
+                    'direction': 'FORWARD',
+                    'state': 'RUNNING',
+                    'item_state': ItemExecutionFinishState.SUCCESS,
+                },
+            ),
+            ('dag_exec_finished', 'COMPLETED'),
+        ]
+        return test_events
+
+    def test_convert(self):
+        event_data = self.make_event_data()
+        converted_events = list()
+        for event in event_data:
+            json_str = EventDataConverter.convert(event[0], event[1])
+            converted_events.append(json_str)
+        self.assertEqual(16, len(converted_events))
+        # Check that item_state values are cast to strings i.e. ItemExecutionFinishState.SUCCESS -> "SUCCESS"
+        n = 0  # Counter for how many ItemExecutionFinishStates were cast to strings
+        for converted_event in converted_events:
+            if converted_event.startswith('{"event_type": "exec_finished"'):
+                self.assertTrue('"item_state": "SUCCESS"' in converted_event)
+                n += 1
+        self.assertEqual(4, n)
+
+    def test_convert_deconvert(self):
+        """Converts events, then deconverts them back."""
+        event_data = self.make_event_data()
+        expected_data = deepcopy(event_data)
+        converted_events = list()
+        for event in event_data:
+            json_str = EventDataConverter.convert(event[0], event[1])
+            converted_events.append(json_str)
+        self.assertEqual(16, len(converted_events))
+        deconverted_events = list()
+        for conv_event in converted_events:
+            event_tuple = EventDataConverter.deconvert(conv_event.encode("utf-8"))
+            deconverted_events.append(event_tuple)
+        self.assertEqual(16, len(deconverted_events))
+        # The converted & deconverted list must be equal to the original
+        self.assertEqual(expected_data, deconverted_events)
+
+
+if __name__ == '__main__':
+    unittest.main()
```

### Comparing `spine_engine-0.23.3/tests/server/util/test_ServerMessage.py` & `spine_engine-0.23.4/tests/server/util/test_ServerMessage.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,229 +1,229 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Unit tests for ServerMessage class.
-"""
-
-import unittest
-import json
-from spine_engine.server.util.server_message import ServerMessage
-
-
-class TestServerMessage(unittest.TestCase):
-    def make_engine_data1(self):
-        engine_data = {
-            'items': {
-                'T1': {
-                    'type': 'Tool',
-                    'description': '',
-                    'x': -81.09856329675559,
-                    'y': -7.8289158512399695,
-                    'specification': 'a',
-                    'execute_in_work': True,
-                    'cmd_line_args': [],
-                },
-                'DC1': {
-                    'type': 'Data Connection',
-                    'description': '',
-                    'x': -249.4143275244174,
-                    'y': -122.2554094109619,
-                    'file_references': [],
-                    'db_references': [],
-                    'db_credentials': {},
-                },
-            },
-            'specifications': {
-                'Tool': [
-                    {
-                        'name': 'a',
-                        'tooltype': 'python',
-                        'includes': ['a.py'],
-                        'description': '',
-                        'inputfiles': ['a.txt'],
-                        'inputfiles_opt': [],
-                        'outputfiles': [],
-                        'cmdline_args': [],
-                        'includes_main_path': '.',
-                        'execution_settings': {
-                            'env': '',
-                            'kernel_spec_name': 'python38',
-                            'use_jupyter_console': False,
-                            'executable': '',
-                        },
-                        'definition_file_path': 'C:\\Users\\ttepsa\\OneDrive - Teknologian Tutkimuskeskus VTT\\Documents\\SpineToolboxProjects\\remote test 2 dags\\.spinetoolbox\\specifications\\Tool\\a.json',
-                    }
-                ]
-            },
-            'connections': [{'name': 'from DC1 to T1', 'from': ['DC1', 'right'], 'to': ['T1', 'left']}],
-            'jumps': [],
-            'execution_permits': {'T1': True, 'DC1': True},
-            'items_module_name': 'spine_items',
-            'settings': {
-                'engineSettings/remoteExecutionEnabled': 'true',
-                'engineSettings/remoteHost': '192.168.56.69',
-                'engineSettings/remotePort': 50001,
-                'engineSettings/remoteSecurityFolder': '',
-                'engineSettings/remoteSecurityModel': '',
-            },
-            'project_dir': 'C:/Users/ttepsa/OneDrive - Teknologian Tutkimuskeskus VTT/Documents/SpineToolboxProjects/remote test 2 dags',
-        }
-        return engine_data
-
-    def make_engine_data2(self):
-        engine_data = {
-            'items': {
-                'Importer 1': {
-                    'type': 'Importer',
-                    'description': '',
-                    'x': 72.45758726309028,
-                    'y': -80.20321040425301,
-                    'specification': 'Importer 1 - pekka_units.xlsx - 0',
-                    'cancel_on_error': True,
-                    'purge_before_writing': True,
-                    'on_conflict': 'replace',
-                    'file_selection': [['<pekka data>/pekka_units.xlsx', True]],
-                },
-                'DS1': {
-                    'type': 'Data Store',
-                    'description': '',
-                    'x': 211.34193262411353,
-                    'y': -152.99750295508272,
-                    'url': {
-                        'dialect': 'sqlite',
-                        'username': '',
-                        'password': '',
-                        'host': '',
-                        'port': '',
-                        'database': {'type': 'path', 'relative': True, 'path': '.spinetoolbox/items/ds1/DS1.sqlite'},
-                    },
-                },
-                'pekka data': {
-                    'type': 'Data Connection',
-                    'description': '',
-                    'x': -78.23453014184398,
-                    'y': -145.98354018912534,
-                    'file_references': [],
-                    'db_references': [],
-                    'db_credentials': {},
-                },
-            },
-            'specifications': {
-                'Importer': [
-                    {
-                        'name': 'Importer 1 - pekka_units.xlsx - 0',
-                        'item_type': 'Importer',
-                        'mapping': {
-                            'table_mappings': {
-                                'Sheet1': [
-                                    {
-                                        'map_type': 'ObjectClass',
-                                        'name': {'map_type': 'column', 'reference': 0},
-                                        'parameters': {'map_type': 'None'},
-                                        'skip_columns': [],
-                                        'read_start_row': 0,
-                                        'objects': {'map_type': 'column', 'reference': 1},
-                                    }
-                                ]
-                            },
-                            'table_options': {},
-                            'table_types': {'Sheet1': {'0': 'string', '1': 'string'}},
-                            'table_row_types': {},
-                            'selected_tables': ['Sheet1'],
-                            'source_type': 'ExcelConnector',
-                        },
-                        'description': None,
-                        'definition_file_path': 'C:\\Users\\ttepsa\\OneDrive - Teknologian Tutkimuskeskus VTT\\Documents\\SpineToolboxProjects\\Simple Importer\\Importer 1 - pekka_units.xlsx - 0.json',
-                    }
-                ]
-            },
-            'connections': [
-                {
-                    'name': 'from pekka data to Importer 1',
-                    'from': ['pekka data', 'right'],
-                    'to': ['Importer 1', 'left'],
-                },
-                {'name': 'from Importer 1 to DS1', 'from': ['Importer 1', 'right'], 'to': ['DS1', 'left']},
-            ],
-            'jumps': [],
-            'execution_permits': {'Importer 1': True, 'DS1': True, 'pekka data': True},
-            'items_module_name': 'spine_items',
-            'settings': {
-                'engineSettings/remoteExecutionEnabled': 'true',
-                'engineSettings/remoteHost': '192.168.56.69',
-                'engineSettings/remotePort': 50001,
-                'engineSettings/remoteSecurityFolder': '',
-                'engineSettings/remoteSecurityModel': '',
-            },
-            'project_dir': 'C:/Users/ttepsa/OneDrive - Teknologian Tutkimuskeskus VTT/Documents/SpineToolboxProjects/Simple Importer',
-        }
-        return engine_data
-
-    def test_make_server_msg_and_parse1(self):
-        """Engine data of DC -> Tool DAG"""
-        engine_data = self.make_engine_data1()
-        msg_data_json = json.dumps(engine_data)
-        msg = ServerMessage("start_execution", "1", msg_data_json, ["not_needed.zip"])
-        msg_as_bytes = msg.to_bytes()
-        parsed_msg = ServerMessage.parse(msg_as_bytes)
-        parsed_engine_data = parsed_msg.getData()
-        self.assertEqual(engine_data, parsed_engine_data)
-
-    def test_make_server_msg_and_parse2(self):
-        """Engine data of DC -> Importer -> DS DAG"""
-        engine_data = self.make_engine_data2()
-        msg_data_json = json.dumps(engine_data)
-        msg = ServerMessage("start_execution", "1", msg_data_json, ["not_needed.zip"])
-        msg_as_bytes = msg.to_bytes()
-        parsed_msg = ServerMessage.parse(msg_as_bytes)
-        parsed_engine_data = parsed_msg.getData()
-        self.assertEqual(engine_data, parsed_engine_data)
-
-    def test_msg_creation(self):
-        list_files = ["dffd.zip", "fdeef.zip"]
-        msg = ServerMessage("execute", "4", "[]", list_files)
-        self.assertEqual("execute", msg.getCommand())
-        self.assertEqual("4", msg.getId())
-        self.assertEqual(list_files, msg.getFileNames())
-        self.assertEqual("[]", msg.getData())
-
-    def test_msg_creation_nofilenames(self):
-        msg = ServerMessage("execute", "4", "[]", None)
-        self.assertEqual("execute", msg.getCommand())
-        self.assertEqual("4", msg.getId())
-        self.assertEqual("[]", msg.getData())
-
-    def test_msg_nodata(self):
-        listFiles = ["dffd.zip", "fdeef.zip"]
-        msg = ServerMessage("execute", "4", "", listFiles)
-        self.assertEqual("execute", msg.getCommand())
-        self.assertEqual("4", msg.getId())
-
-    def test_invalid_input1(self):
-        with self.assertRaises(ValueError):  # json.decoder.JSONDecodeError is a ValueError
-            ServerMessage.parse(b"")
-
-    def test_invalid_input2(self):
-        with self.assertRaises(ValueError):
-            ServerMessage.parse(b"fiuoehfoiewjkfdewjiofdj{")
-
-    def test_msg_nofiles(self):
-        msg = ServerMessage("execute", "4", "{}", None)
-        jsonStr = msg.to_bytes()
-        msg = ServerMessage.parse(jsonStr)
-        self.assertEqual(msg.getCommand(), "execute")
-        self.assertEqual(msg.getId(), "4")
-        self.assertEqual(msg.getData(), {})
-        self.assertEqual(len(msg.getFileNames()), 0)
-
-
-if __name__ == '__main__':
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Unit tests for ServerMessage class.
+"""
+
+import unittest
+import json
+from spine_engine.server.util.server_message import ServerMessage
+
+
+class TestServerMessage(unittest.TestCase):
+    def make_engine_data1(self):
+        engine_data = {
+            'items': {
+                'T1': {
+                    'type': 'Tool',
+                    'description': '',
+                    'x': -81.09856329675559,
+                    'y': -7.8289158512399695,
+                    'specification': 'a',
+                    'execute_in_work': True,
+                    'cmd_line_args': [],
+                },
+                'DC1': {
+                    'type': 'Data Connection',
+                    'description': '',
+                    'x': -249.4143275244174,
+                    'y': -122.2554094109619,
+                    'file_references': [],
+                    'db_references': [],
+                    'db_credentials': {},
+                },
+            },
+            'specifications': {
+                'Tool': [
+                    {
+                        'name': 'a',
+                        'tooltype': 'python',
+                        'includes': ['a.py'],
+                        'description': '',
+                        'inputfiles': ['a.txt'],
+                        'inputfiles_opt': [],
+                        'outputfiles': [],
+                        'cmdline_args': [],
+                        'includes_main_path': '.',
+                        'execution_settings': {
+                            'env': '',
+                            'kernel_spec_name': 'python38',
+                            'use_jupyter_console': False,
+                            'executable': '',
+                        },
+                        'definition_file_path': 'C:\\Users\\ttepsa\\OneDrive - Teknologian Tutkimuskeskus VTT\\Documents\\SpineToolboxProjects\\remote test 2 dags\\.spinetoolbox\\specifications\\Tool\\a.json',
+                    }
+                ]
+            },
+            'connections': [{'name': 'from DC1 to T1', 'from': ['DC1', 'right'], 'to': ['T1', 'left']}],
+            'jumps': [],
+            'execution_permits': {'T1': True, 'DC1': True},
+            'items_module_name': 'spine_items',
+            'settings': {
+                'engineSettings/remoteExecutionEnabled': 'true',
+                'engineSettings/remoteHost': '192.168.56.69',
+                'engineSettings/remotePort': 50001,
+                'engineSettings/remoteSecurityFolder': '',
+                'engineSettings/remoteSecurityModel': '',
+            },
+            'project_dir': 'C:/Users/ttepsa/OneDrive - Teknologian Tutkimuskeskus VTT/Documents/SpineToolboxProjects/remote test 2 dags',
+        }
+        return engine_data
+
+    def make_engine_data2(self):
+        engine_data = {
+            'items': {
+                'Importer 1': {
+                    'type': 'Importer',
+                    'description': '',
+                    'x': 72.45758726309028,
+                    'y': -80.20321040425301,
+                    'specification': 'Importer 1 - pekka_units.xlsx - 0',
+                    'cancel_on_error': True,
+                    'purge_before_writing': True,
+                    'on_conflict': 'replace',
+                    'file_selection': [['<pekka data>/pekka_units.xlsx', True]],
+                },
+                'DS1': {
+                    'type': 'Data Store',
+                    'description': '',
+                    'x': 211.34193262411353,
+                    'y': -152.99750295508272,
+                    'url': {
+                        'dialect': 'sqlite',
+                        'username': '',
+                        'password': '',
+                        'host': '',
+                        'port': '',
+                        'database': {'type': 'path', 'relative': True, 'path': '.spinetoolbox/items/ds1/DS1.sqlite'},
+                    },
+                },
+                'pekka data': {
+                    'type': 'Data Connection',
+                    'description': '',
+                    'x': -78.23453014184398,
+                    'y': -145.98354018912534,
+                    'file_references': [],
+                    'db_references': [],
+                    'db_credentials': {},
+                },
+            },
+            'specifications': {
+                'Importer': [
+                    {
+                        'name': 'Importer 1 - pekka_units.xlsx - 0',
+                        'item_type': 'Importer',
+                        'mapping': {
+                            'table_mappings': {
+                                'Sheet1': [
+                                    {
+                                        'map_type': 'ObjectClass',
+                                        'name': {'map_type': 'column', 'reference': 0},
+                                        'parameters': {'map_type': 'None'},
+                                        'skip_columns': [],
+                                        'read_start_row': 0,
+                                        'objects': {'map_type': 'column', 'reference': 1},
+                                    }
+                                ]
+                            },
+                            'table_options': {},
+                            'table_types': {'Sheet1': {'0': 'string', '1': 'string'}},
+                            'table_row_types': {},
+                            'selected_tables': ['Sheet1'],
+                            'source_type': 'ExcelConnector',
+                        },
+                        'description': None,
+                        'definition_file_path': 'C:\\Users\\ttepsa\\OneDrive - Teknologian Tutkimuskeskus VTT\\Documents\\SpineToolboxProjects\\Simple Importer\\Importer 1 - pekka_units.xlsx - 0.json',
+                    }
+                ]
+            },
+            'connections': [
+                {
+                    'name': 'from pekka data to Importer 1',
+                    'from': ['pekka data', 'right'],
+                    'to': ['Importer 1', 'left'],
+                },
+                {'name': 'from Importer 1 to DS1', 'from': ['Importer 1', 'right'], 'to': ['DS1', 'left']},
+            ],
+            'jumps': [],
+            'execution_permits': {'Importer 1': True, 'DS1': True, 'pekka data': True},
+            'items_module_name': 'spine_items',
+            'settings': {
+                'engineSettings/remoteExecutionEnabled': 'true',
+                'engineSettings/remoteHost': '192.168.56.69',
+                'engineSettings/remotePort': 50001,
+                'engineSettings/remoteSecurityFolder': '',
+                'engineSettings/remoteSecurityModel': '',
+            },
+            'project_dir': 'C:/Users/ttepsa/OneDrive - Teknologian Tutkimuskeskus VTT/Documents/SpineToolboxProjects/Simple Importer',
+        }
+        return engine_data
+
+    def test_make_server_msg_and_parse1(self):
+        """Engine data of DC -> Tool DAG"""
+        engine_data = self.make_engine_data1()
+        msg_data_json = json.dumps(engine_data)
+        msg = ServerMessage("start_execution", "1", msg_data_json, ["not_needed.zip"])
+        msg_as_bytes = msg.to_bytes()
+        parsed_msg = ServerMessage.parse(msg_as_bytes)
+        parsed_engine_data = parsed_msg.getData()
+        self.assertEqual(engine_data, parsed_engine_data)
+
+    def test_make_server_msg_and_parse2(self):
+        """Engine data of DC -> Importer -> DS DAG"""
+        engine_data = self.make_engine_data2()
+        msg_data_json = json.dumps(engine_data)
+        msg = ServerMessage("start_execution", "1", msg_data_json, ["not_needed.zip"])
+        msg_as_bytes = msg.to_bytes()
+        parsed_msg = ServerMessage.parse(msg_as_bytes)
+        parsed_engine_data = parsed_msg.getData()
+        self.assertEqual(engine_data, parsed_engine_data)
+
+    def test_msg_creation(self):
+        list_files = ["dffd.zip", "fdeef.zip"]
+        msg = ServerMessage("execute", "4", "[]", list_files)
+        self.assertEqual("execute", msg.getCommand())
+        self.assertEqual("4", msg.getId())
+        self.assertEqual(list_files, msg.getFileNames())
+        self.assertEqual("[]", msg.getData())
+
+    def test_msg_creation_nofilenames(self):
+        msg = ServerMessage("execute", "4", "[]", None)
+        self.assertEqual("execute", msg.getCommand())
+        self.assertEqual("4", msg.getId())
+        self.assertEqual("[]", msg.getData())
+
+    def test_msg_nodata(self):
+        listFiles = ["dffd.zip", "fdeef.zip"]
+        msg = ServerMessage("execute", "4", "", listFiles)
+        self.assertEqual("execute", msg.getCommand())
+        self.assertEqual("4", msg.getId())
+
+    def test_invalid_input1(self):
+        with self.assertRaises(ValueError):  # json.decoder.JSONDecodeError is a ValueError
+            ServerMessage.parse(b"")
+
+    def test_invalid_input2(self):
+        with self.assertRaises(ValueError):
+            ServerMessage.parse(b"fiuoehfoiewjkfdewjiofdj{")
+
+    def test_msg_nofiles(self):
+        msg = ServerMessage("execute", "4", "{}", None)
+        jsonStr = msg.to_bytes()
+        msg = ServerMessage.parse(jsonStr)
+        self.assertEqual(msg.getCommand(), "execute")
+        self.assertEqual(msg.getId(), "4")
+        self.assertEqual(msg.getData(), {})
+        self.assertEqual(len(msg.getFileNames()), 0)
+
+
+if __name__ == '__main__':
+    unittest.main()
```

### Comparing `spine_engine-0.23.3/tests/server/util/test_ZipHandler.py` & `spine_engine-0.23.4/tests/server/util/test_ZipHandler.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,54 +1,54 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Unit tests for FileExtractor class.
-"""
-
-import unittest
-import os
-from pathlib import Path
-from spine_engine.server.util.zip_handler import ZipHandler
-
-
-class TestZipHandler(unittest.TestCase):
-    def test_package(self):
-        src = os.path.join(str(Path(__file__).parent), "projectforpackagingtests")
-        dst = os.path.join(src, os.pardir)
-        ZipHandler.package(src, dst, "packager_test_zip")
-        zip_file = os.path.join(dst, "packager_test_zip.zip")
-        self.assertTrue(os.path.isfile(zip_file))
-        os.remove(zip_file)
-        self.assertFalse(os.path.isfile(zip_file))
-
-    def test_extract_and_delete_folder(self):
-        zip_file_path = str(Path(__file__).parent.parent / "helloworld.zip")
-        output_dir_path = str(Path(__file__).parent / "output")
-        ZipHandler.extract(zip_file_path, output_dir_path)
-        self.assertEqual(os.path.isdir(output_dir_path), True)
-        ZipHandler.delete_folder(output_dir_path)
-        self.assertEqual(os.path.isdir(output_dir_path), False)
-
-    def test_extract_invalid_input(self):
-        with self.assertRaises(ValueError):
-            ZipHandler.extract("", "./output")
-        with self.assertRaises(ValueError):
-            ZipHandler.extract("does_not_exist.zip", "")
-
-    def test_delete_folder_invalid_input(self):
-        with self.assertRaises(ValueError):
-            ZipHandler.delete_folder("./output2")
-        with self.assertRaises(ValueError):
-            ZipHandler.delete_folder("")
-
-
-if __name__ == "__main__":
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Unit tests for FileExtractor class.
+"""
+
+import unittest
+import os
+from pathlib import Path
+from spine_engine.server.util.zip_handler import ZipHandler
+
+
+class TestZipHandler(unittest.TestCase):
+    def test_package(self):
+        src = os.path.join(str(Path(__file__).parent), "projectforpackagingtests")
+        dst = os.path.join(src, os.pardir)
+        ZipHandler.package(src, dst, "packager_test_zip")
+        zip_file = os.path.join(dst, "packager_test_zip.zip")
+        self.assertTrue(os.path.isfile(zip_file))
+        os.remove(zip_file)
+        self.assertFalse(os.path.isfile(zip_file))
+
+    def test_extract_and_delete_folder(self):
+        zip_file_path = str(Path(__file__).parent.parent / "helloworld.zip")
+        output_dir_path = str(Path(__file__).parent / "output")
+        ZipHandler.extract(zip_file_path, output_dir_path)
+        self.assertEqual(os.path.isdir(output_dir_path), True)
+        ZipHandler.delete_folder(output_dir_path)
+        self.assertEqual(os.path.isdir(output_dir_path), False)
+
+    def test_extract_invalid_input(self):
+        with self.assertRaises(ValueError):
+            ZipHandler.extract("", "./output")
+        with self.assertRaises(ValueError):
+            ZipHandler.extract("does_not_exist.zip", "")
+
+    def test_delete_folder_invalid_input(self):
+        with self.assertRaises(ValueError):
+            ZipHandler.delete_folder("./output2")
+        with self.assertRaises(ValueError):
+            ZipHandler.delete_folder("")
+
+
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `spine_engine-0.23.3/tests/test_load_project_items.py` & `spine_engine-0.23.4/tests/test_load_project_items.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,48 +1,48 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Unit tests for `load_project_items` module.
-
-"""
-import os.path
-import unittest
-import sys
-from spine_engine.load_project_items import load_executable_item_classes, load_item_specification_factories
-from spine_engine.project_item.executable_item_base import ExecutableItemBase
-from spine_engine.project_item.project_item_specification_factory import ProjectItemSpecificationFactory
-
-
-class TestLoadProjectItems(unittest.TestCase):
-    def setUp(self):
-        sys.path.insert(0, os.path.join(os.path.dirname(__file__), "mock_project_items"))
-
-    def tearDown(self):
-        sys.path.pop(0)
-
-    def test_load_executable_items(self):
-        path = sys.path
-        item_classes = load_executable_item_classes("items_module")
-        item_types = ("TestItem",)
-        for item_type in item_types:
-            self.assertIn(item_type, item_classes)
-        for item_class in item_classes.values():
-            self.assertTrue(issubclass(item_class, ExecutableItemBase))
-
-    def test_load_item_specification_factories(self):
-        factories = load_item_specification_factories("items_module")
-        self.assertEqual(len(factories), 1)
-        self.assertIn("TestItem", factories)
-        self.assertTrue(issubclass(factories["TestItem"], ProjectItemSpecificationFactory))
-
-
-if __name__ == '__main__':
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Unit tests for `load_project_items` module.
+
+"""
+import os.path
+import unittest
+import sys
+from spine_engine.load_project_items import load_executable_item_classes, load_item_specification_factories
+from spine_engine.project_item.executable_item_base import ExecutableItemBase
+from spine_engine.project_item.project_item_specification_factory import ProjectItemSpecificationFactory
+
+
+class TestLoadProjectItems(unittest.TestCase):
+    def setUp(self):
+        sys.path.insert(0, os.path.join(os.path.dirname(__file__), "mock_project_items"))
+
+    def tearDown(self):
+        sys.path.pop(0)
+
+    def test_load_executable_items(self):
+        path = sys.path
+        item_classes = load_executable_item_classes("items_module")
+        item_types = ("TestItem",)
+        for item_type in item_types:
+            self.assertIn(item_type, item_classes)
+        for item_class in item_classes.values():
+            self.assertTrue(issubclass(item_class, ExecutableItemBase))
+
+    def test_load_item_specification_factories(self):
+        factories = load_item_specification_factories("items_module")
+        self.assertEqual(len(factories), 1)
+        self.assertIn("TestItem", factories)
+        self.assertTrue(issubclass(factories["TestItem"], ProjectItemSpecificationFactory))
+
+
+if __name__ == '__main__':
+    unittest.main()
```

### Comparing `spine_engine-0.23.3/tests/test_spine_engine.py` & `spine_engine-0.23.4/tests/test_spine_engine.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,871 +1,871 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Unit tests for `spine_engine` module.
-
-Inspired from tests for spinetoolbox.ExecutionInstance and spinetoolbox.ResourceMap,
-and intended to supersede them.
-
-"""
-import os.path
-import sys
-from tempfile import TemporaryDirectory
-import unittest
-from unittest.mock import MagicMock, NonCallableMagicMock, call, patch
-
-from spinedb_api import DatabaseMapping, import_scenarios, import_tools
-from spinedb_api.filters.scenario_filter import scenario_filter_config
-from spinedb_api.filters.tool_filter import tool_filter_config
-from spinedb_api.filters.execution_filter import execution_filter_config
-from spinedb_api.filters.tools import clear_filter_configs
-from spine_engine.exception import EngineInitFailed
-from spine_engine.spine_engine import validate_single_jump
-from spine_engine.utils.helpers import make_dag
-from spine_engine import ExecutionDirection, SpineEngine, SpineEngineState, ItemExecutionFinishState
-from spine_engine.project_item.connection import Jump, Connection
-from spine_engine.project_item.project_item_resource import ProjectItemResource
-
-
-def _make_url_resource(url):
-    return ProjectItemResource("name", "database", "label", url, filterable=True)
-
-
-class TestSpineEngine(unittest.TestCase):
-    _LOOP_TWICE = {
-        "type": "python-script",
-        "script": "\n".join(["import sys", "loop_counter = int(sys.argv[1])", "exit(0 if loop_counter < 2 else 1)"]),
-    }
-
-    @staticmethod
-    def _mock_item(
-        name, resources_forward=None, resources_backward=None, execute_outcome=ItemExecutionFinishState.SUCCESS
-    ):
-        """Returns a mock project item.
-
-        Args:
-            name (str)
-            resources_forward (list): Forward output_resources return value.
-            resources_backward (list): Backward output_resources return value.
-            execute_outcome (ItemExecutionFinishState): Execution return state.
-
-        Returns:
-            NonCallableMagicMock`
-        """
-        if resources_forward is None:
-            resources_forward = []
-        if resources_backward is None:
-            resources_backward = []
-        item = NonCallableMagicMock()
-        item.name = name
-        item.short_name = name.lower().replace(' ', '_')
-        item.execute.return_value = execute_outcome
-        item.exclude_execution = MagicMock()
-        for r in resources_forward + resources_backward:
-            r.provider_name = item.name
-        item.output_resources.side_effect = lambda direction: {
-            ExecutionDirection.FORWARD: resources_forward,
-            ExecutionDirection.BACKWARD: resources_backward,
-        }[direction]
-        return item
-
-    @staticmethod
-    def _default_forward_url_resource(url, predecessor_name):
-        resource = _make_url_resource(url)
-        resource.provider_name = predecessor_name
-        resource.metadata = {'filter_id': '', 'filter_stack': ()}
-        return resource
-
-    @staticmethod
-    def _default_backward_url_resource(url, item_name, successor_name, scenarios=None):
-        resource = _make_url_resource(url)
-        resource.provider_name = successor_name
-        if scenarios is None:
-            scenarios = list()
-        resource.metadata = {
-            "filter_stack": (
-                execution_filter_config(
-                    {"execution_item": item_name, "scenarios": scenarios, "timestamp": "timestamp"}
-                ),
-            )
-        }
-        return resource
-
-    def _run_engine(self, items, connections, item_instances, execution_permits=None, jumps=None):
-        if execution_permits is None:
-            execution_permits = {item_name: True for item_name in items}
-        with patch("spine_engine.spine_engine.create_timestamp") as mock_create_timestamp:
-            mock_create_timestamp.return_value = "timestamp"
-            engine = SpineEngine(
-                items=items,
-                connections=connections,
-                jumps=jumps,
-                execution_permits=execution_permits,
-                items_module_name="items_module",
-            )
-
-        def make_item(name, direction):
-            if direction == ExecutionDirection.FORWARD:
-                return item_instances[name].pop(0)
-            return item_instances[name][0]
-
-        engine.make_item = make_item
-        engine.run()
-        self.assertEqual(engine.state(), SpineEngineState.COMPLETED)
-
-    def setUp(self):
-        sys.path.insert(0, os.path.join(os.path.dirname(__file__), "mock_project_items"))
-
-    def tearDown(self):
-        sys.path.pop(0)
-
-    def test_single_item_execution(self):
-        """Test execution of a single item."""
-        url_a_fw = _make_url_resource("db:///url_a_fw")
-        url_a_bw = _make_url_resource("db:///url_b_fw")
-        mock_item_a = self._mock_item("item_a", resources_forward=[url_a_fw], resources_backward=[url_a_bw])
-        item_instances = {"item_a": [mock_item_a]}
-        items = {"item_a": {"type": "TestItem"}}
-        connections = []
-        self._run_engine(items, connections, item_instances)
-        item_a_execute_args = [[[], []]]
-        self._assert_resource_args(mock_item_a.execute.call_args_list, item_a_execute_args)
-        mock_item_a.exclude_execution.assert_not_called()
-        self.assertEqual(mock_item_a.filter_id, "")
-
-    def test_linear_execution(self):
-        """Test execution with items a-b-c in a line."""
-        url_a_fw = _make_url_resource("db:///url_a_fw")
-        url_b_fw = _make_url_resource("db:///url_b_fw")
-        url_c_fw = _make_url_resource("db:///url_c_fw")
-        url_a_bw = _make_url_resource("db:///url_a_bw")
-        url_b_bw = _make_url_resource("db:///url_b_bw")
-        url_c_bw = _make_url_resource("db:///url_c_bw")
-        mock_item_a = self._mock_item("item_a", resources_forward=[url_a_fw], resources_backward=[url_a_bw])
-        mock_item_b = self._mock_item("item_b", resources_forward=[url_b_fw], resources_backward=[url_b_bw])
-        mock_item_c = self._mock_item("item_c", resources_forward=[url_c_fw], resources_backward=[url_c_bw])
-        item_instances = {"item_a": [mock_item_a], "item_b": [mock_item_b], "item_c": [mock_item_c]}
-        items = {"item_a": {"type": "TestItem"}, "item_b": {"type": "TestItem"}, "item_c": {"type": "TestItem"}}
-        connections = [
-            {"from": ("item_a", "right"), "to": ("item_b", "left")},
-            {"from": ("item_b", "bottom"), "to": ("item_c", "left")},
-        ]
-        self._run_engine(items, connections, item_instances)
-        expected_bw_resource = self._default_backward_url_resource("db:///url_b_bw", "item_a", "item_b")
-        item_a_execute_args = [[[], [expected_bw_resource]]]
-        self._assert_resource_args(mock_item_a.execute.call_args_list, item_a_execute_args)
-        self.assertEqual(mock_item_a.filter_id, "")
-        mock_item_a.exclude_execution.assert_not_called()
-        expected_fw_resource = self._default_forward_url_resource("db:///url_a_fw", "item_a")
-        expected_bw_resource = self._default_backward_url_resource("db:///url_c_bw", "item_b", "item_c")
-        item_b_execute_args = [[[expected_fw_resource], [expected_bw_resource]]]
-        self._assert_resource_args(mock_item_b.execute.call_args_list, item_b_execute_args)
-        self.assertEqual(mock_item_b.filter_id, "")
-        mock_item_b.exclude_execution.assert_not_called()
-        expected_fw_resource = self._default_forward_url_resource("db:///url_b_fw", "item_b")
-        item_c_execute_args = [[[expected_fw_resource], []]]
-        self._assert_resource_args(mock_item_c.execute.call_args_list, item_c_execute_args)
-        mock_item_c.exclude_execution.assert_not_called()
-        self.assertEqual(mock_item_c.filter_id, "")
-
-    def test_fork_execution(self):
-        """Test execution that forks from item a to items b and c."""
-        url_a_fw = _make_url_resource("db:///url_a_fw")
-        url_b_fw = _make_url_resource("db:///url_b_fw")
-        url_c_fw = _make_url_resource("db:///url_c_fw")
-        url_a_bw = _make_url_resource("db:///url_a_bw")
-        url_b_bw = _make_url_resource("db:///url_b_bw")
-        url_c_bw = _make_url_resource("db:///url_c_bw")
-        mock_item_a = self._mock_item("item_a", resources_forward=[url_a_fw], resources_backward=[url_a_bw])
-        mock_item_b = self._mock_item("item_b", resources_forward=[url_b_fw], resources_backward=[url_b_bw])
-        mock_item_c = self._mock_item("item_c", resources_forward=[url_c_fw], resources_backward=[url_c_bw])
-        item_instances = {"item_a": [mock_item_a], "item_b": [mock_item_b], "item_c": [mock_item_c]}
-        items = {"item_a": {"type": "TestItem"}, "item_b": {"type": "TestItem"}, "item_c": {"type": "TestItem"}}
-        connections = [
-            {"from": ("item_a", "right"), "to": ("item_b", "left")},
-            {"from": ("item_a", "bottom"), "to": ("item_c", "left")},
-        ]
-        self._run_engine(items, connections, item_instances)
-        expected_bw_resource1 = self._default_backward_url_resource("db:///url_b_bw", "item_a", "item_b")
-        expected_bw_resource2 = self._default_backward_url_resource("db:///url_c_bw", "item_a", "item_c")
-        item_a_execute_args = [[[], [expected_bw_resource1, expected_bw_resource2]]]
-        self._assert_resource_args(mock_item_a.execute.call_args_list, item_a_execute_args)
-        self.assertEqual(mock_item_a.filter_id, "")
-        mock_item_a.exclude_execution.assert_not_called()
-        expected_fw_resource = self._default_forward_url_resource("db:///url_a_fw", "item_a")
-        item_b_execute_calls = [[[expected_fw_resource], []]]
-        self._assert_resource_args(mock_item_b.execute.call_args_list, item_b_execute_calls)
-        mock_item_b.exclude_execution.assert_not_called()
-        self.assertEqual(mock_item_b.filter_id, "")
-        item_c_execute_calls = [[[expected_fw_resource], []]]
-        self._assert_resource_args(mock_item_c.execute.call_args_list, item_c_execute_calls)
-        mock_item_c.exclude_execution.assert_not_called()
-        self.assertEqual(mock_item_c.filter_id, "")
-
-    def test_branch_merge_execution(self):
-        """Tests execution with items a and b as direct successors for c."""
-        url_a_fw = _make_url_resource("db:///url_a_fw")
-        url_b_fw = _make_url_resource("db:///url_b_fw")
-        url_c_fw = _make_url_resource("db:///url_c_fw")
-        url_a_bw = _make_url_resource("db:///url_a_bw")
-        url_b_bw = _make_url_resource("db:///url_b_bw")
-        url_c_bw = _make_url_resource("db:///url_c_bw")
-        mock_item_a = self._mock_item("item_a", resources_forward=[url_a_fw], resources_backward=[url_a_bw])
-        mock_item_b = self._mock_item("item_b", resources_forward=[url_b_fw], resources_backward=[url_b_bw])
-        mock_item_c = self._mock_item("item_c", resources_forward=[url_c_fw], resources_backward=[url_c_bw])
-        item_instances = {"item_a": [mock_item_a], "item_b": [mock_item_b], "item_c": [mock_item_c]}
-        items = {"item_a": {"type": "TestItem"}, "item_b": {"type": "TestItem"}, "item_c": {"type": "TestItem"}}
-        connections = [
-            {"from": ("item_a", "right"), "to": ("item_c", "left")},
-            {"from": ("item_b", "bottom"), "to": ("item_c", "left")},
-        ]
-        self._run_engine(items, connections, item_instances)
-        expected_bw_resource = self._default_backward_url_resource("db:///url_c_bw", "item_a", "item_c")
-        item_a_execute_args = [[[], [expected_bw_resource]]]
-        self._assert_resource_args(mock_item_a.execute.call_args_list, item_a_execute_args)
-        self.assertEqual(mock_item_a.filter_id, "")
-        expected_bw_resource = self._default_backward_url_resource("db:///url_c_bw", "item_b", "item_c")
-        item_b_execute_calls = [[[], [expected_bw_resource]]]
-        self._assert_resource_args(mock_item_b.execute.call_args_list, item_b_execute_calls)
-        self.assertEqual(mock_item_b.filter_id, "")
-        expected_fw_resource1 = self._default_forward_url_resource("db:///url_a_fw", "item_a")
-        expected_fw_resource2 = self._default_forward_url_resource("db:///url_b_fw", "item_b")
-        item_c_execute_calls = [[[expected_fw_resource1, expected_fw_resource2], []]]
-        self._assert_resource_args(mock_item_c.execute.call_args_list, item_c_execute_calls)
-        self.assertEqual(mock_item_c.filter_id, "")
-
-    def test_execution_permits(self):
-        """Tests that the middle item of an item triplet is not executed when its execution permit is False."""
-        url_a_fw = _make_url_resource("db:///url_a_fw")
-        url_b_fw = _make_url_resource("db:///url_b_fw")
-        url_c_fw = _make_url_resource("db:///url_c_fw")
-        url_a_bw = _make_url_resource("db:///url_a_bw")
-        url_b_bw = _make_url_resource("db:///url_b_bw")
-        url_c_bw = _make_url_resource("db:///url_c_bw")
-        mock_item_a = self._mock_item("item_a", resources_forward=[url_a_fw], resources_backward=[url_a_bw])
-        mock_item_b = self._mock_item("item_b", resources_forward=[url_b_fw], resources_backward=[url_b_bw])
-        mock_item_c = self._mock_item("item_c", resources_forward=[url_c_fw], resources_backward=[url_c_bw])
-        item_instances = {"item_a": [mock_item_a], "item_b": [mock_item_b], "item_c": [mock_item_c]}
-        items = {"item_a": {"type": "TestItem"}, "item_b": {"type": "TestItem"}, "item_c": {"type": "TestItem"}}
-        connections = [
-            {"from": ("item_a", "right"), "to": ("item_b", "left")},
-            {"from": ("item_b", "bottom"), "to": ("item_c", "left")},
-        ]
-        execution_permits = {"item_a": True, "item_b": False, "item_c": True}
-        self._run_engine(items, connections, item_instances, execution_permits=execution_permits)
-        expected_bw_resource = self._default_backward_url_resource("db:///url_b_bw", "item_a", "item_b")
-        item_a_execute_args = [[[], [expected_bw_resource]]]
-        self._assert_resource_args(mock_item_a.execute.call_args_list, item_a_execute_args)
-        mock_item_a.exclude_execution.assert_not_called()
-        mock_item_b.execute.assert_not_called()
-        expected_fw_resource = self._default_forward_url_resource("db:///url_a_fw", "item_a")
-        expected_bw_resource = self._default_backward_url_resource("db:///url_c_bw", "item_b", "item_c")
-        item_b_skip_execution_args = [[[expected_fw_resource], [expected_bw_resource]]]
-        self._assert_resource_args(mock_item_b.exclude_execution.call_args_list, item_b_skip_execution_args)
-        mock_item_b.output_resources.assert_called()
-        expected_fw_resource = self._default_forward_url_resource("db:///url_b_fw", "item_b")
-        item_c_execute_calls = [[[expected_fw_resource], []]]
-        self._assert_resource_args(mock_item_c.execute.call_args_list, item_c_execute_calls)
-        mock_item_c.exclude_execution.assert_not_called()
-
-    def test_filter_stacks(self):
-        """Tests filter stacks are properly applied."""
-        with TemporaryDirectory() as temp_dir:
-            url = "sqlite:///" + os.path.join(temp_dir, "db.sqlite")
-            db_map = DatabaseMapping(url, create=True)
-            import_scenarios(db_map, (("scen1", True), ("scen2", True)))
-            import_tools(db_map, ("toolA",))
-            db_map.commit_session("Add test data.")
-            db_map.connection.close()
-            url_a_fw = _make_url_resource(url)
-            url_b_fw1 = _make_url_resource("db:///url_b_fw")
-            url_b_fw2 = _make_url_resource("db:///url_b_fw")
-            url_c_bw = _make_url_resource("db:///url_c_bw")
-            mock_item_a = self._mock_item("item_a", resources_forward=[url_a_fw], resources_backward=[])
-            mock_item_b1 = self._mock_item("item_b", resources_forward=[url_b_fw1], resources_backward=[])
-            mock_item_b2 = self._mock_item("item_b", resources_forward=[url_b_fw2], resources_backward=[])
-            mock_item_c1 = self._mock_item("item_c", resources_forward=[], resources_backward=[url_c_bw])
-            mock_item_c2 = self._mock_item("item_c", resources_forward=[], resources_backward=[url_c_bw])
-            item_instances = {
-                "item_a": [mock_item_a],
-                "item_b": [mock_item_b1, mock_item_b2],
-                "item_c": [mock_item_c1, mock_item_c2],
-            }
-            items = {
-                "item_a": {"type": "TestItem"},
-                "item_b": {"type": "TestItem"},
-                "item_c": {"type": "TestItem"},
-            }
-            connections = [
-                {
-                    "from": ("item_a", "right"),
-                    "to": ("item_b", "left"),
-                    "disabled_filters": {url_a_fw.label: {"scenario_filter": [], "tool_filter": []}},
-                },
-                {"from": ("item_b", "bottom"), "to": ("item_c", "left")},
-            ]
-            self._run_engine(items, connections, item_instances)
-            item_a_execution_args = [[[], []]]
-            self._assert_resource_args(mock_item_a.execute.call_args_list, item_a_execution_args)
-            self.assertEqual(mock_item_a.filter_id, "")
-            # Check that item_b has been executed two times, with the right filters
-            expected_fw_resource1 = ProjectItemResource("item_a", "database", "label", url)
-            expected_filter_stack1 = (scenario_filter_config("scen1"), tool_filter_config("toolA"))
-            expected_fw_resource1.metadata = {"filter_stack": expected_filter_stack1, "filter_id": ""}
-            expected_bw_resource1 = self._default_backward_url_resource("db:///url_c_bw", "item_b", "item_c", ["scen1"])
-            item_b_execution_args = [[[expected_fw_resource1], [expected_bw_resource1]]]
-            self._assert_resource_args(mock_item_b1.execute.call_args_list, item_b_execution_args)
-            self.assertEqual(mock_item_b1.filter_id, "scen1, toolA - item_a")
-            expected_fw_resource2 = ProjectItemResource("item_a", "database", "label", url)
-            expected_filter_stack2 = (scenario_filter_config("scen2"), tool_filter_config("toolA"))
-            expected_fw_resource2.metadata = {"filter_stack": expected_filter_stack2, "filter_id": ""}
-            expected_bw_resource2 = self._default_backward_url_resource("db:///url_c_bw", "item_b", "item_c", ["scen2"])
-            item_b_execution_args = [[[expected_fw_resource2], [expected_bw_resource2]]]
-            self._assert_resource_args(mock_item_b2.execute.call_args_list, item_b_execution_args)
-            self.assertEqual(mock_item_b2.filter_id, "scen2, toolA - item_a")
-            # Check that item_c has been executed twice, with the right filters
-            expected_fw_resource1 = ProjectItemResource("item_b", "database", "label", "db:///url_b_fw")
-            expected_fw_resource1.metadata = {
-                "filter_stack": expected_filter_stack1,
-                "filter_id": "scen1, toolA - item_a",
-            }
-            item_c_execution_args = [[[expected_fw_resource1], []]]
-            self._assert_resource_args(mock_item_c1.execute.call_args_list, item_c_execution_args)
-            self.assertEqual(mock_item_c1.filter_id, "scen1, toolA - item_b")
-            expected_fw_resource2 = ProjectItemResource("item_b", "database", "label", "db:///url_b_fw")
-            expected_fw_resource2.metadata = {
-                "filter_stack": expected_filter_stack2,
-                "filter_id": "scen2, toolA - item_a",
-            }
-            item_c_execution_args = [[[expected_fw_resource2], []]]
-            self._assert_resource_args(mock_item_c2.execute.call_args_list, item_c_execution_args)
-            self.assertEqual(mock_item_c2.filter_id, "scen2, toolA - item_b")
-
-    def test_parallel_execution_ends_when_no_output_resources_are_generated(self):
-        with TemporaryDirectory() as temp_dir:
-            url = "sqlite:///" + os.path.join(temp_dir, "db.sqlite")
-            db_map = DatabaseMapping(url, create=True)
-            import_scenarios(db_map, (("scen1", True), ("scen2", True)))
-            import_tools(db_map, ("toolA",))
-            db_map.commit_session("Add test data.")
-            db_map.connection.close()
-            url_a_fw = _make_url_resource(url)
-            url_c_bw = _make_url_resource("db:///url_c_bw")
-            mock_item_a = self._mock_item("item_a", resources_forward=[url_a_fw], resources_backward=[])
-            mock_item_b1 = self._mock_item("item_b", resources_forward=[], resources_backward=[])
-            mock_item_b2 = self._mock_item("item_b", resources_forward=[], resources_backward=[])
-            mock_item_c = self._mock_item("item_c", resources_forward=[], resources_backward=[url_c_bw])
-            item_instances = {"item_a": [mock_item_a], "item_b": [mock_item_b1, mock_item_b2], "item_c": [mock_item_c]}
-            items = {
-                "item_a": {"type": "TestItem"},
-                "item_b": {"type": "TestItem"},
-                "item_c": {"type": "TestItem"},
-            }
-            connections = [
-                {
-                    "from": ("item_a", "right"),
-                    "to": ("item_b", "left"),
-                    "resource_filters": {url_a_fw.label: {"scenario_filter": [1, 2], "tool_filter": [1]}},
-                },
-                {"from": ("item_b", "bottom"), "to": ("item_c", "left")},
-            ]
-            self._run_engine(items, connections, item_instances)
-            item_a_execution_args = [[[], []]]
-            self._assert_resource_args(mock_item_a.execute.call_args_list, item_a_execution_args)
-            self.assertEqual(mock_item_a.filter_id, "")
-            # Check that item_b has been executed two times, with the right filters
-            expected_fw_resource1 = ProjectItemResource("item_a", "database", "label", url)
-            expected_filter_stack1 = (scenario_filter_config("scen1"), tool_filter_config("toolA"))
-            expected_fw_resource1.metadata = {"filter_stack": expected_filter_stack1, "filter_id": ""}
-            expected_bw_resource1 = self._default_backward_url_resource("db:///url_c_bw", "item_b", "item_c", ["scen1"])
-            item_b_execution_args = [[[expected_fw_resource1], [expected_bw_resource1]]]
-            self._assert_resource_args(mock_item_b1.execute.call_args_list, item_b_execution_args)
-            self.assertEqual(mock_item_b1.filter_id, "scen1, toolA - item_a")
-            expected_fw_resource2 = ProjectItemResource("item_a", "database", "label", url)
-            expected_filter_stack2 = (scenario_filter_config("scen2"), tool_filter_config("toolA"))
-            expected_fw_resource2.metadata = {"filter_stack": expected_filter_stack2, "filter_id": ""}
-            expected_bw_resource2 = self._default_backward_url_resource("db:///url_c_bw", "item_b", "item_c", ["scen2"])
-            item_b_execution_args = [[[expected_fw_resource2], [expected_bw_resource2]]]
-            self._assert_resource_args(mock_item_b2.execute.call_args_list, item_b_execution_args)
-            self.assertEqual(mock_item_b2.filter_id, "scen2, toolA - item_a")
-            # Check that item_c has been executed only once
-            self._assert_resource_args(mock_item_c.execute.call_args_list, [[[], []]])
-            self.assertEqual(mock_item_c.filter_id, "")
-
-    def test_filter_stacks_and_multiple_file_output_resources(self):
-        """Multiple file output resources should be combined correctly for a successor"""
-        with TemporaryDirectory() as temp_dir:
-            url = "sqlite:///" + os.path.join(temp_dir, "db.sqlite")
-            db_map = DatabaseMapping(url, create=True)
-            import_scenarios(db_map, (("scen1", True), ("scen2", True)))
-            import_tools(db_map, ("toolA",))
-            db_map.commit_session("Add test data.")
-            db_map.connection.close()
-            url_a_fw = _make_url_resource(url)
-            file_b_fw_11 = ProjectItemResource("item_b", "file", "label_1")
-            file_b_fw_12 = ProjectItemResource("item_b", "file", "label_1")
-            file_b_fw_21 = ProjectItemResource("item_b", "file", "label_2")
-            file_b_fw_22 = ProjectItemResource("item_b", "file", "label_2")
-            url_c_bw = _make_url_resource("db:///url_c_bw")
-            mock_item_a = self._mock_item("item_a", resources_forward=[url_a_fw], resources_backward=[])
-            mock_item_b1 = self._mock_item(
-                "item_b", resources_forward=[file_b_fw_11, file_b_fw_21], resources_backward=[]
-            )
-            mock_item_b2 = self._mock_item(
-                "item_b", resources_forward=[file_b_fw_12, file_b_fw_22], resources_backward=[]
-            )
-            mock_item_c1 = self._mock_item("item_c", resources_forward=[], resources_backward=[url_c_bw])
-            mock_item_c2 = self._mock_item("item_c", resources_forward=[], resources_backward=[url_c_bw])
-            item_instances = {
-                "item_a": [mock_item_a],
-                "item_b": [mock_item_b1, mock_item_b2],
-                "item_c": [mock_item_c1, mock_item_c2],
-            }
-            items = {
-                "item_a": {"type": "TestItem"},
-                "item_b": {"type": "TestItem"},
-                "item_c": {"type": "TestItem"},
-            }
-            connections = [
-                {
-                    "from": ("item_a", "right"),
-                    "to": ("item_b", "left"),
-                    "disabled_filters": {url_a_fw.label: {"scenario_filter": [], "tool_filter": []}},
-                },
-                {"from": ("item_b", "bottom"), "to": ("item_c", "left")},
-            ]
-            self._run_engine(items, connections, item_instances)
-            item_a_execution_args = [[[], []]]
-            self._assert_resource_args(mock_item_a.execute.call_args_list, item_a_execution_args)
-            self.assertEqual(mock_item_a.filter_id, "")
-            # Check that item_b has been executed two times, with the right filters
-            expected_fw_resource1 = ProjectItemResource("item_a", "database", "label", url)
-            expected_filter_stack1 = (scenario_filter_config("scen1"), tool_filter_config("toolA"))
-            expected_fw_resource1.metadata = {"filter_stack": expected_filter_stack1, "filter_id": ""}
-            expected_bw_resource1 = self._default_backward_url_resource("db:///url_c_bw", "item_b", "item_c", ["scen1"])
-            item_b_execution_args = [[[expected_fw_resource1], [expected_bw_resource1]]]
-            self._assert_resource_args(mock_item_b1.execute.call_args_list, item_b_execution_args)
-            self.assertEqual(mock_item_b1.filter_id, "scen1, toolA - item_a")
-            expected_fw_resource2 = ProjectItemResource("item_a", "database", "label", url)
-            expected_filter_stack2 = (scenario_filter_config("scen2"), tool_filter_config("toolA"))
-            expected_fw_resource2.metadata = {"filter_stack": expected_filter_stack2, "filter_id": ""}
-            expected_bw_resource2 = self._default_backward_url_resource("db:///url_c_bw", "item_b", "item_c", ["scen2"])
-            item_b_execution_args = [[[expected_fw_resource2], [expected_bw_resource2]]]
-            self._assert_resource_args(mock_item_b2.execute.call_args_list, item_b_execution_args)
-            self.assertEqual(mock_item_b2.filter_id, "scen2, toolA - item_a")
-            # Check that item_c has been executed twice, with the right filters
-            expected_fw_resource1 = ProjectItemResource("item_b", "file", "label_1")
-            expected_fw_resource1.metadata = {
-                "filter_stack": expected_filter_stack1,
-                "filter_id": "scen1, toolA - item_a",
-            }
-            expected_fw_resource2 = ProjectItemResource("item_b", "file", "label_2")
-            expected_fw_resource2.metadata = {
-                "filter_stack": expected_filter_stack1,
-                "filter_id": "scen1, toolA - item_a",
-            }
-            item_c_execution_args = [[[expected_fw_resource1, expected_fw_resource2], []]]
-            self._assert_resource_args(mock_item_c1.execute.call_args_list, item_c_execution_args)
-            self.assertEqual(mock_item_c1.filter_id, "scen1, toolA - item_a")
-            expected_fw_resource3 = ProjectItemResource("item_b", "file", "label_1")
-            expected_fw_resource3.metadata = {
-                "filter_stack": expected_filter_stack2,
-                "filter_id": "scen2, toolA - item_a",
-            }
-            expected_fw_resource4 = ProjectItemResource("item_b", "file", "label_2")
-            expected_fw_resource4.metadata = {
-                "filter_stack": expected_filter_stack2,
-                "filter_id": "scen2, toolA - item_a",
-            }
-            item_c_execution_args = [[[expected_fw_resource3, expected_fw_resource4], []]]
-            self._assert_resource_args(mock_item_c2.execute.call_args_list, item_c_execution_args)
-            self.assertEqual(mock_item_c2.filter_id, "scen2, toolA - item_a")
-
-    def test_merge_two_filtered_database_branches(self):
-        with TemporaryDirectory() as temp_dir:
-            urlA = "sqlite:///" + os.path.join(temp_dir, "dbA.sqlite")
-            db_map = DatabaseMapping(urlA, create=True)
-            import_scenarios(db_map, (("scenA1", True), ("scenA2", True)))
-            import_tools(db_map, ("toolA",))
-            db_map.commit_session("Add test data.")
-            db_map.connection.close()
-            urlB = "sqlite:///" + os.path.join(temp_dir, "dbB.sqlite")
-            db_map = DatabaseMapping(urlB, create=True)
-            import_scenarios(db_map, (("scenB1", True), ("scenB2", True)))
-            import_tools(db_map, ("toolB",))
-            db_map.commit_session("Add test data.")
-            db_map.connection.close()
-            url_a_fw = _make_url_resource(urlA)
-            url_b_fw = _make_url_resource(urlB)
-            mock_item_a = self._mock_item("item_a", resources_forward=[url_a_fw], resources_backward=[])
-            mock_item_b = self._mock_item("item_b", resources_forward=[url_b_fw], resources_backward=[])
-            mock_item_c1 = self._mock_item("item_c", resources_forward=[], resources_backward=[])
-            mock_item_c2 = self._mock_item("item_c", resources_forward=[], resources_backward=[])
-            mock_item_c3 = self._mock_item("item_c", resources_forward=[], resources_backward=[])
-            mock_item_c4 = self._mock_item("item_c", resources_forward=[], resources_backward=[])
-            item_instances = {
-                "item_a": [mock_item_a],
-                "item_b": [mock_item_b],
-                "item_c": [mock_item_c1, mock_item_c2, mock_item_c3, mock_item_c4],
-            }
-            items = {
-                "item_a": {"type": "TestItem"},
-                "item_b": {"type": "TestItem"},
-                "item_c": {"type": "TestItem"},
-            }
-            connections = [
-                {
-                    "from": ("item_a", "right"),
-                    "to": ("item_c", "left"),
-                    "disabled_filters": {url_a_fw.label: {"scenario_filter": [], "tool_filter": []}},
-                },
-                {
-                    "from": ("item_b", "right"),
-                    "to": ("item_c", "left"),
-                    "disabled_filters": {url_b_fw.label: {"scenario_filter": [], "tool_filter": []}},
-                },
-            ]
-            self._run_engine(items, connections, item_instances)
-            item_a_execution_args = [[[], []]]
-            self._assert_resource_args(mock_item_a.execute.call_args_list, item_a_execution_args)
-            item_b_execution_args = [[[], []]]
-            self._assert_resource_args(mock_item_b.execute.call_args_list, item_b_execution_args)
-            # Check that item_c has been executed four times, with all combinations of item_a and item_b filters
-            expected_fw_resource1 = ProjectItemResource("item_a", "database", "label", urlA)
-            expected_filter_stack1 = (scenario_filter_config("scenA1"), tool_filter_config("toolA"))
-            expected_fw_resource1.metadata = {"filter_stack": expected_filter_stack1, "filter_id": ""}
-            expected_fw_resource2 = ProjectItemResource("item_b", "database", "label", urlB)
-            expected_filter_stack2 = (scenario_filter_config("scenB1"), tool_filter_config("toolB"))
-            expected_fw_resource2.metadata = {"filter_stack": expected_filter_stack2, "filter_id": ""}
-            item_c_execution_args = [[[expected_fw_resource1, expected_fw_resource2], []]]
-            self._assert_resource_args(mock_item_c1.execute.call_args_list, item_c_execution_args)
-            expected_fw_resource3 = ProjectItemResource("item_b", "database", "label", urlB)
-            expected_filter_stack3 = (scenario_filter_config("scenB2"), tool_filter_config("toolB"))
-            expected_fw_resource3.metadata = {"filter_stack": expected_filter_stack3, "filter_id": ""}
-            item_c_execution_args = [[[expected_fw_resource1, expected_fw_resource3], []]]
-            self._assert_resource_args(mock_item_c2.execute.call_args_list, item_c_execution_args)
-            expected_fw_resource4 = ProjectItemResource("item_a", "database", "label", urlA)
-            expected_filter_stack4 = (scenario_filter_config("scenA2"), tool_filter_config("toolA"))
-            expected_fw_resource4.metadata = {"filter_stack": expected_filter_stack4, "filter_id": ""}
-            item_c_execution_args = [[[expected_fw_resource4, expected_fw_resource2], []]]
-            self._assert_resource_args(mock_item_c3.execute.call_args_list, item_c_execution_args)
-            item_c_execution_args = [[[expected_fw_resource4, expected_fw_resource3], []]]
-            self._assert_resource_args(mock_item_c4.execute.call_args_list, item_c_execution_args)
-
-    def test_merge_two_filtered_file_resource_branches(self):
-        with TemporaryDirectory() as temp_dir:
-            urlA = "sqlite:///" + os.path.join(temp_dir, "dbA.sqlite")
-            db_map = DatabaseMapping(urlA, create=True)
-            import_scenarios(db_map, (("scenA1", True), ("scenA2", True)))
-            import_tools(db_map, ("toolA",))
-            db_map.commit_session("Add test data.")
-            db_map.connection.close()
-            urlB = "sqlite:///" + os.path.join(temp_dir, "dbB.sqlite")
-            db_map = DatabaseMapping(urlB, create=True)
-            import_scenarios(db_map, (("scenB1", True), ("scenB2", True)))
-            import_tools(db_map, ("toolB",))
-            db_map.commit_session("Add test data.")
-            db_map.connection.close()
-            url_a_fw = _make_url_resource(urlA)
-            url_b_fw = _make_url_resource(urlB)
-            file_c_fw_11 = ProjectItemResource("item_c", "file", "label_1")
-            file_c_fw_12 = ProjectItemResource("item_c", "file", "label_1")
-            file_c_fw_21 = ProjectItemResource("item_c", "file", "label_2")
-            file_c_fw_22 = ProjectItemResource("item_c", "file", "label_2")
-            file_d_fw_11 = ProjectItemResource("item_d", "file", "label_3")
-            file_d_fw_12 = ProjectItemResource("item_d", "file", "label_3")
-            file_d_fw_21 = ProjectItemResource("item_d", "file", "label_4")
-            file_d_fw_22 = ProjectItemResource("item_d", "file", "label_4")
-            mock_item_a = self._mock_item("item_a", resources_forward=[url_a_fw], resources_backward=[])
-            mock_item_b = self._mock_item("item_b", resources_forward=[url_b_fw], resources_backward=[])
-            mock_item_c1 = self._mock_item(
-                "item_c", resources_forward=[file_c_fw_11, file_c_fw_21], resources_backward=[]
-            )
-            mock_item_c2 = self._mock_item(
-                "item_c", resources_forward=[file_c_fw_12, file_c_fw_22], resources_backward=[]
-            )
-            mock_item_d1 = self._mock_item(
-                "item_d", resources_forward=[file_d_fw_11, file_d_fw_21], resources_backward=[]
-            )
-            mock_item_d2 = self._mock_item(
-                "item_d", resources_forward=[file_d_fw_12, file_d_fw_22], resources_backward=[]
-            )
-            mock_item_e1 = self._mock_item("item_e", resources_forward=[], resources_backward=[])
-            mock_item_e2 = self._mock_item("item_e", resources_forward=[], resources_backward=[])
-            mock_item_e3 = self._mock_item("item_e", resources_forward=[], resources_backward=[])
-            mock_item_e4 = self._mock_item("item_e", resources_forward=[], resources_backward=[])
-            item_instances = {
-                "item_a": [mock_item_a],
-                "item_b": [mock_item_b],
-                "item_c": [mock_item_c1, mock_item_c2],
-                "item_d": [mock_item_d1, mock_item_d2],
-                "item_e": [mock_item_e1, mock_item_e2, mock_item_e3, mock_item_e4],
-            }
-            items = {
-                "item_a": {"type": "TestItem"},
-                "item_b": {"type": "TestItem"},
-                "item_c": {"type": "TestItem"},
-                "item_d": {"type": "TestItem"},
-                "item_e": {"type": "TestItem"},
-            }
-            connections = [
-                {
-                    "from": ("item_a", "right"),
-                    "to": ("item_c", "left"),
-                    "resource_filters": {url_a_fw.label: {"scenario_filter": [1, 2], "tool_filter": [1]}},
-                },
-                {
-                    "from": ("item_b", "right"),
-                    "to": ("item_d", "left"),
-                    "resource_filters": {url_b_fw.label: {"scenario_filter": [1, 2], "tool_filter": [1]}},
-                },
-                {"from": ("item_c", "right"), "to": ("item_e", "left")},
-                {"from": ("item_d", "bottom"), "to": ("item_e", "top")},
-            ]
-            self._run_engine(items, connections, item_instances)
-            item_a_execution_args = [[[], []]]
-            self._assert_resource_args(mock_item_a.execute.call_args_list, item_a_execution_args)
-            self.assertEqual(mock_item_a.filter_id, "")
-            item_b_execution_args = [[[], []]]
-            self._assert_resource_args(mock_item_b.execute.call_args_list, item_b_execution_args)
-            self.assertEqual(mock_item_b.filter_id, "")
-            # Check that item_c has been executed twice, with all combinations of item_a's filters
-            expected_fw_resource1 = ProjectItemResource("item_a", "database", "label", urlA)
-            expected_filter_stack1 = (scenario_filter_config("scenA1"), tool_filter_config("toolA"))
-            expected_fw_resource1.metadata = {"filter_stack": expected_filter_stack1, "filter_id": ""}
-            item_c_execution_args = [[[expected_fw_resource1], []]]
-            self._assert_resource_args(mock_item_c1.execute.call_args_list, item_c_execution_args)
-            self.assertEqual(mock_item_c1.filter_id, "scenA1, toolA - item_a")
-            expected_fw_resource2 = ProjectItemResource("item_a", "database", "label", urlA)
-            expected_filter_stack2 = (scenario_filter_config("scenA2"), tool_filter_config("toolA"))
-            expected_fw_resource2.metadata = {"filter_stack": expected_filter_stack2, "filter_id": ""}
-            item_c_execution_args = [[[expected_fw_resource2], []]]
-            self._assert_resource_args(mock_item_c2.execute.call_args_list, item_c_execution_args)
-            self.assertEqual(mock_item_c2.filter_id, "scenA2, toolA - item_a")
-            # Check that item_d has been executed twice, with all combinations of item_b's filters
-            expected_fw_resource3 = ProjectItemResource("item_b", "database", "label", urlB)
-            expected_filter_stack3 = (scenario_filter_config("scenB1"), tool_filter_config("toolB"))
-            expected_fw_resource3.metadata = {"filter_stack": expected_filter_stack3, "filter_id": ""}
-            item_d_execution_args = [[[expected_fw_resource3], []]]
-            self._assert_resource_args(mock_item_d1.execute.call_args_list, item_d_execution_args)
-            self.assertEqual(mock_item_d1.filter_id, "scenB1, toolB - item_b")
-            expected_fw_resource4 = ProjectItemResource("item_b", "database", "label", urlB)
-            expected_filter_stack4 = (scenario_filter_config("scenB2"), tool_filter_config("toolB"))
-            expected_fw_resource4.metadata = {"filter_stack": expected_filter_stack4, "filter_id": ""}
-            item_d_execution_args = [[[expected_fw_resource4], []]]
-            self._assert_resource_args(mock_item_d2.execute.call_args_list, item_d_execution_args)
-            self.assertEqual(mock_item_d2.filter_id, "scenB2, toolB - item_b")
-            # Check that item_e has been executed four times, with all combinations of item_c's and item_d's resources.
-            expected_fw_resource5 = ProjectItemResource("item_c", "file", "label_1")
-            expected_filter_stack5 = (scenario_filter_config("scenA1"), tool_filter_config("toolA"))
-            expected_fw_resource5.metadata = {
-                "filter_stack": expected_filter_stack5,
-                "filter_id": "scenA1, toolA - item_a",
-            }
-            expected_fw_resource6 = ProjectItemResource("item_c", "file", "label_2")
-            expected_fw_resource6.metadata = {
-                "filter_stack": expected_filter_stack5,
-                "filter_id": "scenA1, toolA - item_a",
-            }
-            expected_fw_resource7 = ProjectItemResource("item_d", "file", "label_3")
-            expected_filter_stack7 = (scenario_filter_config("scenB1"), tool_filter_config("toolB"))
-            expected_fw_resource7.metadata = {
-                "filter_stack": expected_filter_stack7,
-                "filter_id": "scenB1, toolB - item_b",
-            }
-            expected_fw_resource8 = ProjectItemResource("item_d", "file", "label_4")
-            expected_fw_resource8.metadata = {
-                "filter_stack": expected_filter_stack7,
-                "filter_id": "scenB1, toolB - item_b",
-            }
-            item_e_execution_args = [
-                [[expected_fw_resource5, expected_fw_resource6, expected_fw_resource7, expected_fw_resource8], []]
-            ]
-            self._assert_resource_args(mock_item_e1.execute.call_args_list, item_e_execution_args)
-            self.assertEqual(mock_item_e1.filter_id, "scenA1, toolA - item_a & scenB1, toolB - item_b")
-            expected_fw_resource9 = ProjectItemResource("item_d", "file", "label_3")
-            expected_filter_stack9 = (scenario_filter_config("scenB2"), tool_filter_config("toolB"))
-            expected_fw_resource9.metadata = {
-                "filter_stack": expected_filter_stack9,
-                "filter_id": "scenB2, toolB - item_b",
-            }
-            expected_fw_resource10 = ProjectItemResource("item_d", "file", "label_4")
-            expected_fw_resource10.metadata = {
-                "filter_stack": expected_filter_stack9,
-                "filter_id": "scenB2, toolB - item_b",
-            }
-            item_e_execution_args = [
-                [[expected_fw_resource5, expected_fw_resource6, expected_fw_resource9, expected_fw_resource10], []]
-            ]
-            self._assert_resource_args(mock_item_e2.execute.call_args_list, item_e_execution_args)
-            self.assertEqual(mock_item_e2.filter_id, "scenA1, toolA - item_a & scenB2, toolB - item_b")
-            expected_fw_resource11 = ProjectItemResource("item_c", "file", "label_1")
-            expected_filter_stack11 = (scenario_filter_config("scenA2"), tool_filter_config("toolA"))
-            expected_fw_resource11.metadata = {
-                "filter_stack": expected_filter_stack11,
-                "filter_id": "scenA2, toolA - item_a",
-            }
-            expected_fw_resource12 = ProjectItemResource("item_c", "file", "label_2")
-            expected_fw_resource12.metadata = {
-                "filter_stack": expected_filter_stack11,
-                "filter_id": "scenA2, toolA - item_a",
-            }
-            item_e_execution_args = [
-                [[expected_fw_resource11, expected_fw_resource12, expected_fw_resource7, expected_fw_resource8], []]
-            ]
-            self._assert_resource_args(mock_item_e3.execute.call_args_list, item_e_execution_args)
-            self.assertEqual(mock_item_e3.filter_id, "scenA2, toolA - item_a & scenB1, toolB - item_b")
-            item_e_execution_args = [
-                [[expected_fw_resource11, expected_fw_resource12, expected_fw_resource9, expected_fw_resource10], []]
-            ]
-            self._assert_resource_args(mock_item_e4.execute.call_args_list, item_e_execution_args)
-            self.assertEqual(mock_item_e4.filter_id, "scenA2, toolA - item_a & scenB2, toolB - item_b")
-
-    def test_self_jump_succeeds(self):
-        mock_item = self._mock_item("item", resources_forward=[], resources_backward=[])
-        items = {"item": {"type": "TestItem"}}
-        connections = []
-        jumps = [Jump("item", "bottom", "item", "top", self._LOOP_TWICE).to_dict()]
-        execution_permits = {"item": True}
-
-        engine = SpineEngine(
-            items=items,
-            connections=connections,
-            jumps=jumps,
-            execution_permits=execution_permits,
-            items_module_name="items_module",
-        )
-        engine.make_item = lambda name, direction: mock_item
-        engine.run()
-        lock_1 = mock_item.execute.call_args_list[0].args[-1]
-        lock_2 = mock_item.execute.call_args_list[1].args[-1]
-        self.assertEqual(mock_item.execute.call_args_list, [call([], [], lock_1), call([], [], lock_2)])
-        self.assertEqual(engine.state(), SpineEngineState.COMPLETED)
-
-    @unittest.skip("Hangs because something's not right in SpineDBServer")
-    def test_jump_resources_get_passed_correctly(self):
-        resource_fw_a = _make_url_resource("db:///fw_a")
-        resource_bw_a = _make_url_resource("db:///bw_a")
-        item_a = self._mock_item("a", resources_forward=[resource_fw_a], resources_backward=[resource_bw_a])
-        resource_fw_b = _make_url_resource("db:///fw_b")
-        resource_bw_b = _make_url_resource("db:///bw_b")
-        item_b = self._mock_item("b", resources_forward=[resource_fw_b], resources_backward=[resource_bw_b])
-        resource_fw_c = _make_url_resource("db:///fw_c")
-        resource_bw_c = _make_url_resource("db:///bw_c")
-        item_c = self._mock_item("c", resources_forward=[resource_fw_c], resources_backward=[resource_bw_c])
-        resource_fw_d = _make_url_resource("db:///fw_d")
-        resource_bw_d = _make_url_resource("db:///bw_d")
-        item_d = self._mock_item("d", resources_forward=[resource_fw_d], resources_backward=[resource_bw_d])
-        item_instances = {"a": [item_a], "b": [item_b, item_b], "c": [item_c, item_c], "d": [item_d]}
-        items = {
-            "a": {"type": "TestItem"},
-            "b": {"type": "TestItem"},
-            "c": {"type": "TestItem"},
-            "d": {"type": "TestItem"},
-        }
-        connections = [
-            c.to_dict()
-            for c in (
-                Connection("a", "right", "b", "left"),
-                Connection("b", "bottom", "c", "top"),
-                Connection("c", "left", "d", "right"),
-            )
-        ]
-        jumps = [Jump("c", "right", "b", "right", self._LOOP_TWICE).to_dict()]
-        self._run_engine(items, connections, item_instances, jumps=jumps)
-        self._assert_resource_args(
-            item_a.execute.call_args_list, [[[], [self._default_backward_url_resource("db:///bw_b", "a", "b")]]]
-        )
-        self._assert_resource_args(
-            item_b.execute.call_args_list,
-            2
-            * [
-                [
-                    [self._default_forward_url_resource("db:///fw_a", "a")],
-                    [self._default_backward_url_resource("db:///bw_c", "b", "c")],
-                ]
-            ],
-        )
-        self._assert_resource_args(
-            item_c.execute.call_args_list,
-            2
-            * [
-                [
-                    [self._default_forward_url_resource("db:///fw_b", "b")],
-                    [self._default_backward_url_resource("db:///bw_d", "c", "d")],
-                ]
-            ],
-        )
-        self._assert_resource_args(
-            item_d.execute.call_args_list, [[[self._default_forward_url_resource("db:///fw_c", "c")], []]]
-        )
-
-    @unittest.skip("Hangs because something's not right in SpineDBServer")
-    def test_nested_jump_with_inner_self_jump(self):
-        resource_fw_a = _make_url_resource("db:///fw_a")
-        resource_bw_a = _make_url_resource("db:///bw_a")
-        item_a = self._mock_item("a", resources_forward=[resource_fw_a], resources_backward=[resource_bw_a])
-        resource_fw_b = _make_url_resource("db:///fw_b")
-        resource_bw_b = _make_url_resource("db:///bw_b")
-        item_b = self._mock_item("b", resources_forward=[resource_fw_b], resources_backward=[resource_bw_b])
-        resource_fw_c = _make_url_resource("db:///fw_c")
-        resource_bw_c = _make_url_resource("db:///bw_c")
-        item_c = self._mock_item("c", resources_forward=[resource_fw_c], resources_backward=[resource_bw_c])
-        item_instances = {"a": 2 * [item_a], "b": 4 * [item_b], "c": 2 * [item_c]}
-        items = {"a": {"type": "TestItem"}, "b": {"type": "TestItem"}, "c": {"type": "TestItem"}}
-        connections = [
-            c.to_dict() for c in (Connection("a", "right", "b", "left"), Connection("b", "bottom", "c", "top"))
-        ]
-        jumps = [
-            Jump("c", "right", "a", "right", self._LOOP_TWICE).to_dict(),
-            Jump("b", "top", "b", "top", self._LOOP_TWICE).to_dict(),
-        ]
-        self._run_engine(items, connections, item_instances, jumps=jumps)
-        expected = 2 * [[[], [self._default_backward_url_resource("db:///bw_b", "a", "b")]]]
-        self._assert_resource_args(item_a.execute.call_args_list, expected)
-        expected = 4 * [
-            [
-                [self._default_forward_url_resource("db:///fw_a", "a")],
-                [self._default_backward_url_resource("db:///bw_c", "b", "c")],
-            ]
-        ]
-        self._assert_resource_args(item_b.execute.call_args_list, expected)
-        expected = 2 * [[[self._default_forward_url_resource("db:///fw_b", "b")], []]]
-        self._assert_resource_args(item_c.execute.call_args_list, expected)
-
-    def _assert_resource_args(self, arg_packs, expected_packs):
-        self.assertEqual(len(arg_packs), len(expected_packs))
-        for pack, expected_pack in zip(arg_packs, expected_packs):
-            self.assertEqual(len(pack), len(expected_pack))
-            for args, expected in zip(pack[0], expected_pack):
-                self.assertEqual(len(args), len(expected))
-                for resource, expected_resource in zip(args, expected):
-                    self.assertEqual(resource.provider_name, expected_resource.provider_name)
-                    self.assertEqual(resource.label, expected_resource.label)
-                    self.assertEqual(clear_filter_configs(resource.url), expected_resource.url)
-                    for key, value in expected_resource.metadata.items():
-                        self.assertEqual(resource.metadata[key], value)
-
-
-class TestValidateSingleJump(unittest.TestCase):
-    def test_bug(self):
-        edges = {"a": ["b"], "b": ["c"], "c": "d"}
-        dag = make_dag(edges)
-        jumps = [Jump("b", "top", "a", "top"), Jump("d", "top", "c", "top")]
-        jump_to_check = jumps[0]
-        try:
-            validate_single_jump(jump_to_check, jumps, dag)
-        except EngineInitFailed:
-            self.fail("validate_single_jump shouldn't have raised")
-
-
-if __name__ == '__main__':
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Unit tests for `spine_engine` module.
+
+Inspired from tests for spinetoolbox.ExecutionInstance and spinetoolbox.ResourceMap,
+and intended to supersede them.
+
+"""
+import os.path
+import sys
+from tempfile import TemporaryDirectory
+import unittest
+from unittest.mock import MagicMock, NonCallableMagicMock, call, patch
+
+from spinedb_api import DatabaseMapping, import_scenarios, import_tools
+from spinedb_api.filters.scenario_filter import scenario_filter_config
+from spinedb_api.filters.tool_filter import tool_filter_config
+from spinedb_api.filters.execution_filter import execution_filter_config
+from spinedb_api.filters.tools import clear_filter_configs
+from spine_engine.exception import EngineInitFailed
+from spine_engine.spine_engine import validate_single_jump
+from spine_engine.utils.helpers import make_dag
+from spine_engine import ExecutionDirection, SpineEngine, SpineEngineState, ItemExecutionFinishState
+from spine_engine.project_item.connection import Jump, Connection
+from spine_engine.project_item.project_item_resource import ProjectItemResource
+
+
+def _make_url_resource(url):
+    return ProjectItemResource("name", "database", "label", url, filterable=True)
+
+
+class TestSpineEngine(unittest.TestCase):
+    _LOOP_TWICE = {
+        "type": "python-script",
+        "script": "\n".join(["import sys", "loop_counter = int(sys.argv[1])", "exit(0 if loop_counter < 2 else 1)"]),
+    }
+
+    @staticmethod
+    def _mock_item(
+        name, resources_forward=None, resources_backward=None, execute_outcome=ItemExecutionFinishState.SUCCESS
+    ):
+        """Returns a mock project item.
+
+        Args:
+            name (str)
+            resources_forward (list): Forward output_resources return value.
+            resources_backward (list): Backward output_resources return value.
+            execute_outcome (ItemExecutionFinishState): Execution return state.
+
+        Returns:
+            NonCallableMagicMock`
+        """
+        if resources_forward is None:
+            resources_forward = []
+        if resources_backward is None:
+            resources_backward = []
+        item = NonCallableMagicMock()
+        item.name = name
+        item.short_name = name.lower().replace(' ', '_')
+        item.execute.return_value = execute_outcome
+        item.exclude_execution = MagicMock()
+        for r in resources_forward + resources_backward:
+            r.provider_name = item.name
+        item.output_resources.side_effect = lambda direction: {
+            ExecutionDirection.FORWARD: resources_forward,
+            ExecutionDirection.BACKWARD: resources_backward,
+        }[direction]
+        return item
+
+    @staticmethod
+    def _default_forward_url_resource(url, predecessor_name):
+        resource = _make_url_resource(url)
+        resource.provider_name = predecessor_name
+        resource.metadata = {'filter_id': '', 'filter_stack': ()}
+        return resource
+
+    @staticmethod
+    def _default_backward_url_resource(url, item_name, successor_name, scenarios=None):
+        resource = _make_url_resource(url)
+        resource.provider_name = successor_name
+        if scenarios is None:
+            scenarios = list()
+        resource.metadata = {
+            "filter_stack": (
+                execution_filter_config(
+                    {"execution_item": item_name, "scenarios": scenarios, "timestamp": "timestamp"}
+                ),
+            )
+        }
+        return resource
+
+    def _run_engine(self, items, connections, item_instances, execution_permits=None, jumps=None):
+        if execution_permits is None:
+            execution_permits = {item_name: True for item_name in items}
+        with patch("spine_engine.spine_engine.create_timestamp") as mock_create_timestamp:
+            mock_create_timestamp.return_value = "timestamp"
+            engine = SpineEngine(
+                items=items,
+                connections=connections,
+                jumps=jumps,
+                execution_permits=execution_permits,
+                items_module_name="items_module",
+            )
+
+        def make_item(name, direction):
+            if direction == ExecutionDirection.FORWARD:
+                return item_instances[name].pop(0)
+            return item_instances[name][0]
+
+        engine.make_item = make_item
+        engine.run()
+        self.assertEqual(engine.state(), SpineEngineState.COMPLETED)
+
+    def setUp(self):
+        sys.path.insert(0, os.path.join(os.path.dirname(__file__), "mock_project_items"))
+
+    def tearDown(self):
+        sys.path.pop(0)
+
+    def test_single_item_execution(self):
+        """Test execution of a single item."""
+        url_a_fw = _make_url_resource("db:///url_a_fw")
+        url_a_bw = _make_url_resource("db:///url_b_fw")
+        mock_item_a = self._mock_item("item_a", resources_forward=[url_a_fw], resources_backward=[url_a_bw])
+        item_instances = {"item_a": [mock_item_a]}
+        items = {"item_a": {"type": "TestItem"}}
+        connections = []
+        self._run_engine(items, connections, item_instances)
+        item_a_execute_args = [[[], []]]
+        self._assert_resource_args(mock_item_a.execute.call_args_list, item_a_execute_args)
+        mock_item_a.exclude_execution.assert_not_called()
+        self.assertEqual(mock_item_a.filter_id, "")
+
+    def test_linear_execution(self):
+        """Test execution with items a-b-c in a line."""
+        url_a_fw = _make_url_resource("db:///url_a_fw")
+        url_b_fw = _make_url_resource("db:///url_b_fw")
+        url_c_fw = _make_url_resource("db:///url_c_fw")
+        url_a_bw = _make_url_resource("db:///url_a_bw")
+        url_b_bw = _make_url_resource("db:///url_b_bw")
+        url_c_bw = _make_url_resource("db:///url_c_bw")
+        mock_item_a = self._mock_item("item_a", resources_forward=[url_a_fw], resources_backward=[url_a_bw])
+        mock_item_b = self._mock_item("item_b", resources_forward=[url_b_fw], resources_backward=[url_b_bw])
+        mock_item_c = self._mock_item("item_c", resources_forward=[url_c_fw], resources_backward=[url_c_bw])
+        item_instances = {"item_a": [mock_item_a], "item_b": [mock_item_b], "item_c": [mock_item_c]}
+        items = {"item_a": {"type": "TestItem"}, "item_b": {"type": "TestItem"}, "item_c": {"type": "TestItem"}}
+        connections = [
+            {"from": ("item_a", "right"), "to": ("item_b", "left")},
+            {"from": ("item_b", "bottom"), "to": ("item_c", "left")},
+        ]
+        self._run_engine(items, connections, item_instances)
+        expected_bw_resource = self._default_backward_url_resource("db:///url_b_bw", "item_a", "item_b")
+        item_a_execute_args = [[[], [expected_bw_resource]]]
+        self._assert_resource_args(mock_item_a.execute.call_args_list, item_a_execute_args)
+        self.assertEqual(mock_item_a.filter_id, "")
+        mock_item_a.exclude_execution.assert_not_called()
+        expected_fw_resource = self._default_forward_url_resource("db:///url_a_fw", "item_a")
+        expected_bw_resource = self._default_backward_url_resource("db:///url_c_bw", "item_b", "item_c")
+        item_b_execute_args = [[[expected_fw_resource], [expected_bw_resource]]]
+        self._assert_resource_args(mock_item_b.execute.call_args_list, item_b_execute_args)
+        self.assertEqual(mock_item_b.filter_id, "")
+        mock_item_b.exclude_execution.assert_not_called()
+        expected_fw_resource = self._default_forward_url_resource("db:///url_b_fw", "item_b")
+        item_c_execute_args = [[[expected_fw_resource], []]]
+        self._assert_resource_args(mock_item_c.execute.call_args_list, item_c_execute_args)
+        mock_item_c.exclude_execution.assert_not_called()
+        self.assertEqual(mock_item_c.filter_id, "")
+
+    def test_fork_execution(self):
+        """Test execution that forks from item a to items b and c."""
+        url_a_fw = _make_url_resource("db:///url_a_fw")
+        url_b_fw = _make_url_resource("db:///url_b_fw")
+        url_c_fw = _make_url_resource("db:///url_c_fw")
+        url_a_bw = _make_url_resource("db:///url_a_bw")
+        url_b_bw = _make_url_resource("db:///url_b_bw")
+        url_c_bw = _make_url_resource("db:///url_c_bw")
+        mock_item_a = self._mock_item("item_a", resources_forward=[url_a_fw], resources_backward=[url_a_bw])
+        mock_item_b = self._mock_item("item_b", resources_forward=[url_b_fw], resources_backward=[url_b_bw])
+        mock_item_c = self._mock_item("item_c", resources_forward=[url_c_fw], resources_backward=[url_c_bw])
+        item_instances = {"item_a": [mock_item_a], "item_b": [mock_item_b], "item_c": [mock_item_c]}
+        items = {"item_a": {"type": "TestItem"}, "item_b": {"type": "TestItem"}, "item_c": {"type": "TestItem"}}
+        connections = [
+            {"from": ("item_a", "right"), "to": ("item_b", "left")},
+            {"from": ("item_a", "bottom"), "to": ("item_c", "left")},
+        ]
+        self._run_engine(items, connections, item_instances)
+        expected_bw_resource1 = self._default_backward_url_resource("db:///url_b_bw", "item_a", "item_b")
+        expected_bw_resource2 = self._default_backward_url_resource("db:///url_c_bw", "item_a", "item_c")
+        item_a_execute_args = [[[], [expected_bw_resource1, expected_bw_resource2]]]
+        self._assert_resource_args(mock_item_a.execute.call_args_list, item_a_execute_args)
+        self.assertEqual(mock_item_a.filter_id, "")
+        mock_item_a.exclude_execution.assert_not_called()
+        expected_fw_resource = self._default_forward_url_resource("db:///url_a_fw", "item_a")
+        item_b_execute_calls = [[[expected_fw_resource], []]]
+        self._assert_resource_args(mock_item_b.execute.call_args_list, item_b_execute_calls)
+        mock_item_b.exclude_execution.assert_not_called()
+        self.assertEqual(mock_item_b.filter_id, "")
+        item_c_execute_calls = [[[expected_fw_resource], []]]
+        self._assert_resource_args(mock_item_c.execute.call_args_list, item_c_execute_calls)
+        mock_item_c.exclude_execution.assert_not_called()
+        self.assertEqual(mock_item_c.filter_id, "")
+
+    def test_branch_merge_execution(self):
+        """Tests execution with items a and b as direct successors for c."""
+        url_a_fw = _make_url_resource("db:///url_a_fw")
+        url_b_fw = _make_url_resource("db:///url_b_fw")
+        url_c_fw = _make_url_resource("db:///url_c_fw")
+        url_a_bw = _make_url_resource("db:///url_a_bw")
+        url_b_bw = _make_url_resource("db:///url_b_bw")
+        url_c_bw = _make_url_resource("db:///url_c_bw")
+        mock_item_a = self._mock_item("item_a", resources_forward=[url_a_fw], resources_backward=[url_a_bw])
+        mock_item_b = self._mock_item("item_b", resources_forward=[url_b_fw], resources_backward=[url_b_bw])
+        mock_item_c = self._mock_item("item_c", resources_forward=[url_c_fw], resources_backward=[url_c_bw])
+        item_instances = {"item_a": [mock_item_a], "item_b": [mock_item_b], "item_c": [mock_item_c]}
+        items = {"item_a": {"type": "TestItem"}, "item_b": {"type": "TestItem"}, "item_c": {"type": "TestItem"}}
+        connections = [
+            {"from": ("item_a", "right"), "to": ("item_c", "left")},
+            {"from": ("item_b", "bottom"), "to": ("item_c", "left")},
+        ]
+        self._run_engine(items, connections, item_instances)
+        expected_bw_resource = self._default_backward_url_resource("db:///url_c_bw", "item_a", "item_c")
+        item_a_execute_args = [[[], [expected_bw_resource]]]
+        self._assert_resource_args(mock_item_a.execute.call_args_list, item_a_execute_args)
+        self.assertEqual(mock_item_a.filter_id, "")
+        expected_bw_resource = self._default_backward_url_resource("db:///url_c_bw", "item_b", "item_c")
+        item_b_execute_calls = [[[], [expected_bw_resource]]]
+        self._assert_resource_args(mock_item_b.execute.call_args_list, item_b_execute_calls)
+        self.assertEqual(mock_item_b.filter_id, "")
+        expected_fw_resource1 = self._default_forward_url_resource("db:///url_a_fw", "item_a")
+        expected_fw_resource2 = self._default_forward_url_resource("db:///url_b_fw", "item_b")
+        item_c_execute_calls = [[[expected_fw_resource1, expected_fw_resource2], []]]
+        self._assert_resource_args(mock_item_c.execute.call_args_list, item_c_execute_calls)
+        self.assertEqual(mock_item_c.filter_id, "")
+
+    def test_execution_permits(self):
+        """Tests that the middle item of an item triplet is not executed when its execution permit is False."""
+        url_a_fw = _make_url_resource("db:///url_a_fw")
+        url_b_fw = _make_url_resource("db:///url_b_fw")
+        url_c_fw = _make_url_resource("db:///url_c_fw")
+        url_a_bw = _make_url_resource("db:///url_a_bw")
+        url_b_bw = _make_url_resource("db:///url_b_bw")
+        url_c_bw = _make_url_resource("db:///url_c_bw")
+        mock_item_a = self._mock_item("item_a", resources_forward=[url_a_fw], resources_backward=[url_a_bw])
+        mock_item_b = self._mock_item("item_b", resources_forward=[url_b_fw], resources_backward=[url_b_bw])
+        mock_item_c = self._mock_item("item_c", resources_forward=[url_c_fw], resources_backward=[url_c_bw])
+        item_instances = {"item_a": [mock_item_a], "item_b": [mock_item_b], "item_c": [mock_item_c]}
+        items = {"item_a": {"type": "TestItem"}, "item_b": {"type": "TestItem"}, "item_c": {"type": "TestItem"}}
+        connections = [
+            {"from": ("item_a", "right"), "to": ("item_b", "left")},
+            {"from": ("item_b", "bottom"), "to": ("item_c", "left")},
+        ]
+        execution_permits = {"item_a": True, "item_b": False, "item_c": True}
+        self._run_engine(items, connections, item_instances, execution_permits=execution_permits)
+        expected_bw_resource = self._default_backward_url_resource("db:///url_b_bw", "item_a", "item_b")
+        item_a_execute_args = [[[], [expected_bw_resource]]]
+        self._assert_resource_args(mock_item_a.execute.call_args_list, item_a_execute_args)
+        mock_item_a.exclude_execution.assert_not_called()
+        mock_item_b.execute.assert_not_called()
+        expected_fw_resource = self._default_forward_url_resource("db:///url_a_fw", "item_a")
+        expected_bw_resource = self._default_backward_url_resource("db:///url_c_bw", "item_b", "item_c")
+        item_b_skip_execution_args = [[[expected_fw_resource], [expected_bw_resource]]]
+        self._assert_resource_args(mock_item_b.exclude_execution.call_args_list, item_b_skip_execution_args)
+        mock_item_b.output_resources.assert_called()
+        expected_fw_resource = self._default_forward_url_resource("db:///url_b_fw", "item_b")
+        item_c_execute_calls = [[[expected_fw_resource], []]]
+        self._assert_resource_args(mock_item_c.execute.call_args_list, item_c_execute_calls)
+        mock_item_c.exclude_execution.assert_not_called()
+
+    def test_filter_stacks(self):
+        """Tests filter stacks are properly applied."""
+        with TemporaryDirectory() as temp_dir:
+            url = "sqlite:///" + os.path.join(temp_dir, "db.sqlite")
+            db_map = DatabaseMapping(url, create=True)
+            import_scenarios(db_map, (("scen1", True), ("scen2", True)))
+            import_tools(db_map, ("toolA",))
+            db_map.commit_session("Add test data.")
+            db_map.connection.close()
+            url_a_fw = _make_url_resource(url)
+            url_b_fw1 = _make_url_resource("db:///url_b_fw")
+            url_b_fw2 = _make_url_resource("db:///url_b_fw")
+            url_c_bw = _make_url_resource("db:///url_c_bw")
+            mock_item_a = self._mock_item("item_a", resources_forward=[url_a_fw], resources_backward=[])
+            mock_item_b1 = self._mock_item("item_b", resources_forward=[url_b_fw1], resources_backward=[])
+            mock_item_b2 = self._mock_item("item_b", resources_forward=[url_b_fw2], resources_backward=[])
+            mock_item_c1 = self._mock_item("item_c", resources_forward=[], resources_backward=[url_c_bw])
+            mock_item_c2 = self._mock_item("item_c", resources_forward=[], resources_backward=[url_c_bw])
+            item_instances = {
+                "item_a": [mock_item_a],
+                "item_b": [mock_item_b1, mock_item_b2],
+                "item_c": [mock_item_c1, mock_item_c2],
+            }
+            items = {
+                "item_a": {"type": "TestItem"},
+                "item_b": {"type": "TestItem"},
+                "item_c": {"type": "TestItem"},
+            }
+            connections = [
+                {
+                    "from": ("item_a", "right"),
+                    "to": ("item_b", "left"),
+                    "disabled_filters": {url_a_fw.label: {"scenario_filter": [], "tool_filter": []}},
+                },
+                {"from": ("item_b", "bottom"), "to": ("item_c", "left")},
+            ]
+            self._run_engine(items, connections, item_instances)
+            item_a_execution_args = [[[], []]]
+            self._assert_resource_args(mock_item_a.execute.call_args_list, item_a_execution_args)
+            self.assertEqual(mock_item_a.filter_id, "")
+            # Check that item_b has been executed two times, with the right filters
+            expected_fw_resource1 = ProjectItemResource("item_a", "database", "label", url)
+            expected_filter_stack1 = (scenario_filter_config("scen1"), tool_filter_config("toolA"))
+            expected_fw_resource1.metadata = {"filter_stack": expected_filter_stack1, "filter_id": ""}
+            expected_bw_resource1 = self._default_backward_url_resource("db:///url_c_bw", "item_b", "item_c", ["scen1"])
+            item_b_execution_args = [[[expected_fw_resource1], [expected_bw_resource1]]]
+            self._assert_resource_args(mock_item_b1.execute.call_args_list, item_b_execution_args)
+            self.assertEqual(mock_item_b1.filter_id, "scen1, toolA - item_a")
+            expected_fw_resource2 = ProjectItemResource("item_a", "database", "label", url)
+            expected_filter_stack2 = (scenario_filter_config("scen2"), tool_filter_config("toolA"))
+            expected_fw_resource2.metadata = {"filter_stack": expected_filter_stack2, "filter_id": ""}
+            expected_bw_resource2 = self._default_backward_url_resource("db:///url_c_bw", "item_b", "item_c", ["scen2"])
+            item_b_execution_args = [[[expected_fw_resource2], [expected_bw_resource2]]]
+            self._assert_resource_args(mock_item_b2.execute.call_args_list, item_b_execution_args)
+            self.assertEqual(mock_item_b2.filter_id, "scen2, toolA - item_a")
+            # Check that item_c has been executed twice, with the right filters
+            expected_fw_resource1 = ProjectItemResource("item_b", "database", "label", "db:///url_b_fw")
+            expected_fw_resource1.metadata = {
+                "filter_stack": expected_filter_stack1,
+                "filter_id": "scen1, toolA - item_a",
+            }
+            item_c_execution_args = [[[expected_fw_resource1], []]]
+            self._assert_resource_args(mock_item_c1.execute.call_args_list, item_c_execution_args)
+            self.assertEqual(mock_item_c1.filter_id, "scen1, toolA - item_b")
+            expected_fw_resource2 = ProjectItemResource("item_b", "database", "label", "db:///url_b_fw")
+            expected_fw_resource2.metadata = {
+                "filter_stack": expected_filter_stack2,
+                "filter_id": "scen2, toolA - item_a",
+            }
+            item_c_execution_args = [[[expected_fw_resource2], []]]
+            self._assert_resource_args(mock_item_c2.execute.call_args_list, item_c_execution_args)
+            self.assertEqual(mock_item_c2.filter_id, "scen2, toolA - item_b")
+
+    def test_parallel_execution_ends_when_no_output_resources_are_generated(self):
+        with TemporaryDirectory() as temp_dir:
+            url = "sqlite:///" + os.path.join(temp_dir, "db.sqlite")
+            db_map = DatabaseMapping(url, create=True)
+            import_scenarios(db_map, (("scen1", True), ("scen2", True)))
+            import_tools(db_map, ("toolA",))
+            db_map.commit_session("Add test data.")
+            db_map.connection.close()
+            url_a_fw = _make_url_resource(url)
+            url_c_bw = _make_url_resource("db:///url_c_bw")
+            mock_item_a = self._mock_item("item_a", resources_forward=[url_a_fw], resources_backward=[])
+            mock_item_b1 = self._mock_item("item_b", resources_forward=[], resources_backward=[])
+            mock_item_b2 = self._mock_item("item_b", resources_forward=[], resources_backward=[])
+            mock_item_c = self._mock_item("item_c", resources_forward=[], resources_backward=[url_c_bw])
+            item_instances = {"item_a": [mock_item_a], "item_b": [mock_item_b1, mock_item_b2], "item_c": [mock_item_c]}
+            items = {
+                "item_a": {"type": "TestItem"},
+                "item_b": {"type": "TestItem"},
+                "item_c": {"type": "TestItem"},
+            }
+            connections = [
+                {
+                    "from": ("item_a", "right"),
+                    "to": ("item_b", "left"),
+                    "resource_filters": {url_a_fw.label: {"scenario_filter": [1, 2], "tool_filter": [1]}},
+                },
+                {"from": ("item_b", "bottom"), "to": ("item_c", "left")},
+            ]
+            self._run_engine(items, connections, item_instances)
+            item_a_execution_args = [[[], []]]
+            self._assert_resource_args(mock_item_a.execute.call_args_list, item_a_execution_args)
+            self.assertEqual(mock_item_a.filter_id, "")
+            # Check that item_b has been executed two times, with the right filters
+            expected_fw_resource1 = ProjectItemResource("item_a", "database", "label", url)
+            expected_filter_stack1 = (scenario_filter_config("scen1"), tool_filter_config("toolA"))
+            expected_fw_resource1.metadata = {"filter_stack": expected_filter_stack1, "filter_id": ""}
+            expected_bw_resource1 = self._default_backward_url_resource("db:///url_c_bw", "item_b", "item_c", ["scen1"])
+            item_b_execution_args = [[[expected_fw_resource1], [expected_bw_resource1]]]
+            self._assert_resource_args(mock_item_b1.execute.call_args_list, item_b_execution_args)
+            self.assertEqual(mock_item_b1.filter_id, "scen1, toolA - item_a")
+            expected_fw_resource2 = ProjectItemResource("item_a", "database", "label", url)
+            expected_filter_stack2 = (scenario_filter_config("scen2"), tool_filter_config("toolA"))
+            expected_fw_resource2.metadata = {"filter_stack": expected_filter_stack2, "filter_id": ""}
+            expected_bw_resource2 = self._default_backward_url_resource("db:///url_c_bw", "item_b", "item_c", ["scen2"])
+            item_b_execution_args = [[[expected_fw_resource2], [expected_bw_resource2]]]
+            self._assert_resource_args(mock_item_b2.execute.call_args_list, item_b_execution_args)
+            self.assertEqual(mock_item_b2.filter_id, "scen2, toolA - item_a")
+            # Check that item_c has been executed only once
+            self._assert_resource_args(mock_item_c.execute.call_args_list, [[[], []]])
+            self.assertEqual(mock_item_c.filter_id, "")
+
+    def test_filter_stacks_and_multiple_file_output_resources(self):
+        """Multiple file output resources should be combined correctly for a successor"""
+        with TemporaryDirectory() as temp_dir:
+            url = "sqlite:///" + os.path.join(temp_dir, "db.sqlite")
+            db_map = DatabaseMapping(url, create=True)
+            import_scenarios(db_map, (("scen1", True), ("scen2", True)))
+            import_tools(db_map, ("toolA",))
+            db_map.commit_session("Add test data.")
+            db_map.connection.close()
+            url_a_fw = _make_url_resource(url)
+            file_b_fw_11 = ProjectItemResource("item_b", "file", "label_1")
+            file_b_fw_12 = ProjectItemResource("item_b", "file", "label_1")
+            file_b_fw_21 = ProjectItemResource("item_b", "file", "label_2")
+            file_b_fw_22 = ProjectItemResource("item_b", "file", "label_2")
+            url_c_bw = _make_url_resource("db:///url_c_bw")
+            mock_item_a = self._mock_item("item_a", resources_forward=[url_a_fw], resources_backward=[])
+            mock_item_b1 = self._mock_item(
+                "item_b", resources_forward=[file_b_fw_11, file_b_fw_21], resources_backward=[]
+            )
+            mock_item_b2 = self._mock_item(
+                "item_b", resources_forward=[file_b_fw_12, file_b_fw_22], resources_backward=[]
+            )
+            mock_item_c1 = self._mock_item("item_c", resources_forward=[], resources_backward=[url_c_bw])
+            mock_item_c2 = self._mock_item("item_c", resources_forward=[], resources_backward=[url_c_bw])
+            item_instances = {
+                "item_a": [mock_item_a],
+                "item_b": [mock_item_b1, mock_item_b2],
+                "item_c": [mock_item_c1, mock_item_c2],
+            }
+            items = {
+                "item_a": {"type": "TestItem"},
+                "item_b": {"type": "TestItem"},
+                "item_c": {"type": "TestItem"},
+            }
+            connections = [
+                {
+                    "from": ("item_a", "right"),
+                    "to": ("item_b", "left"),
+                    "disabled_filters": {url_a_fw.label: {"scenario_filter": [], "tool_filter": []}},
+                },
+                {"from": ("item_b", "bottom"), "to": ("item_c", "left")},
+            ]
+            self._run_engine(items, connections, item_instances)
+            item_a_execution_args = [[[], []]]
+            self._assert_resource_args(mock_item_a.execute.call_args_list, item_a_execution_args)
+            self.assertEqual(mock_item_a.filter_id, "")
+            # Check that item_b has been executed two times, with the right filters
+            expected_fw_resource1 = ProjectItemResource("item_a", "database", "label", url)
+            expected_filter_stack1 = (scenario_filter_config("scen1"), tool_filter_config("toolA"))
+            expected_fw_resource1.metadata = {"filter_stack": expected_filter_stack1, "filter_id": ""}
+            expected_bw_resource1 = self._default_backward_url_resource("db:///url_c_bw", "item_b", "item_c", ["scen1"])
+            item_b_execution_args = [[[expected_fw_resource1], [expected_bw_resource1]]]
+            self._assert_resource_args(mock_item_b1.execute.call_args_list, item_b_execution_args)
+            self.assertEqual(mock_item_b1.filter_id, "scen1, toolA - item_a")
+            expected_fw_resource2 = ProjectItemResource("item_a", "database", "label", url)
+            expected_filter_stack2 = (scenario_filter_config("scen2"), tool_filter_config("toolA"))
+            expected_fw_resource2.metadata = {"filter_stack": expected_filter_stack2, "filter_id": ""}
+            expected_bw_resource2 = self._default_backward_url_resource("db:///url_c_bw", "item_b", "item_c", ["scen2"])
+            item_b_execution_args = [[[expected_fw_resource2], [expected_bw_resource2]]]
+            self._assert_resource_args(mock_item_b2.execute.call_args_list, item_b_execution_args)
+            self.assertEqual(mock_item_b2.filter_id, "scen2, toolA - item_a")
+            # Check that item_c has been executed twice, with the right filters
+            expected_fw_resource1 = ProjectItemResource("item_b", "file", "label_1")
+            expected_fw_resource1.metadata = {
+                "filter_stack": expected_filter_stack1,
+                "filter_id": "scen1, toolA - item_a",
+            }
+            expected_fw_resource2 = ProjectItemResource("item_b", "file", "label_2")
+            expected_fw_resource2.metadata = {
+                "filter_stack": expected_filter_stack1,
+                "filter_id": "scen1, toolA - item_a",
+            }
+            item_c_execution_args = [[[expected_fw_resource1, expected_fw_resource2], []]]
+            self._assert_resource_args(mock_item_c1.execute.call_args_list, item_c_execution_args)
+            self.assertEqual(mock_item_c1.filter_id, "scen1, toolA - item_a")
+            expected_fw_resource3 = ProjectItemResource("item_b", "file", "label_1")
+            expected_fw_resource3.metadata = {
+                "filter_stack": expected_filter_stack2,
+                "filter_id": "scen2, toolA - item_a",
+            }
+            expected_fw_resource4 = ProjectItemResource("item_b", "file", "label_2")
+            expected_fw_resource4.metadata = {
+                "filter_stack": expected_filter_stack2,
+                "filter_id": "scen2, toolA - item_a",
+            }
+            item_c_execution_args = [[[expected_fw_resource3, expected_fw_resource4], []]]
+            self._assert_resource_args(mock_item_c2.execute.call_args_list, item_c_execution_args)
+            self.assertEqual(mock_item_c2.filter_id, "scen2, toolA - item_a")
+
+    def test_merge_two_filtered_database_branches(self):
+        with TemporaryDirectory() as temp_dir:
+            urlA = "sqlite:///" + os.path.join(temp_dir, "dbA.sqlite")
+            db_map = DatabaseMapping(urlA, create=True)
+            import_scenarios(db_map, (("scenA1", True), ("scenA2", True)))
+            import_tools(db_map, ("toolA",))
+            db_map.commit_session("Add test data.")
+            db_map.connection.close()
+            urlB = "sqlite:///" + os.path.join(temp_dir, "dbB.sqlite")
+            db_map = DatabaseMapping(urlB, create=True)
+            import_scenarios(db_map, (("scenB1", True), ("scenB2", True)))
+            import_tools(db_map, ("toolB",))
+            db_map.commit_session("Add test data.")
+            db_map.connection.close()
+            url_a_fw = _make_url_resource(urlA)
+            url_b_fw = _make_url_resource(urlB)
+            mock_item_a = self._mock_item("item_a", resources_forward=[url_a_fw], resources_backward=[])
+            mock_item_b = self._mock_item("item_b", resources_forward=[url_b_fw], resources_backward=[])
+            mock_item_c1 = self._mock_item("item_c", resources_forward=[], resources_backward=[])
+            mock_item_c2 = self._mock_item("item_c", resources_forward=[], resources_backward=[])
+            mock_item_c3 = self._mock_item("item_c", resources_forward=[], resources_backward=[])
+            mock_item_c4 = self._mock_item("item_c", resources_forward=[], resources_backward=[])
+            item_instances = {
+                "item_a": [mock_item_a],
+                "item_b": [mock_item_b],
+                "item_c": [mock_item_c1, mock_item_c2, mock_item_c3, mock_item_c4],
+            }
+            items = {
+                "item_a": {"type": "TestItem"},
+                "item_b": {"type": "TestItem"},
+                "item_c": {"type": "TestItem"},
+            }
+            connections = [
+                {
+                    "from": ("item_a", "right"),
+                    "to": ("item_c", "left"),
+                    "disabled_filters": {url_a_fw.label: {"scenario_filter": [], "tool_filter": []}},
+                },
+                {
+                    "from": ("item_b", "right"),
+                    "to": ("item_c", "left"),
+                    "disabled_filters": {url_b_fw.label: {"scenario_filter": [], "tool_filter": []}},
+                },
+            ]
+            self._run_engine(items, connections, item_instances)
+            item_a_execution_args = [[[], []]]
+            self._assert_resource_args(mock_item_a.execute.call_args_list, item_a_execution_args)
+            item_b_execution_args = [[[], []]]
+            self._assert_resource_args(mock_item_b.execute.call_args_list, item_b_execution_args)
+            # Check that item_c has been executed four times, with all combinations of item_a and item_b filters
+            expected_fw_resource1 = ProjectItemResource("item_a", "database", "label", urlA)
+            expected_filter_stack1 = (scenario_filter_config("scenA1"), tool_filter_config("toolA"))
+            expected_fw_resource1.metadata = {"filter_stack": expected_filter_stack1, "filter_id": ""}
+            expected_fw_resource2 = ProjectItemResource("item_b", "database", "label", urlB)
+            expected_filter_stack2 = (scenario_filter_config("scenB1"), tool_filter_config("toolB"))
+            expected_fw_resource2.metadata = {"filter_stack": expected_filter_stack2, "filter_id": ""}
+            item_c_execution_args = [[[expected_fw_resource1, expected_fw_resource2], []]]
+            self._assert_resource_args(mock_item_c1.execute.call_args_list, item_c_execution_args)
+            expected_fw_resource3 = ProjectItemResource("item_b", "database", "label", urlB)
+            expected_filter_stack3 = (scenario_filter_config("scenB2"), tool_filter_config("toolB"))
+            expected_fw_resource3.metadata = {"filter_stack": expected_filter_stack3, "filter_id": ""}
+            item_c_execution_args = [[[expected_fw_resource1, expected_fw_resource3], []]]
+            self._assert_resource_args(mock_item_c2.execute.call_args_list, item_c_execution_args)
+            expected_fw_resource4 = ProjectItemResource("item_a", "database", "label", urlA)
+            expected_filter_stack4 = (scenario_filter_config("scenA2"), tool_filter_config("toolA"))
+            expected_fw_resource4.metadata = {"filter_stack": expected_filter_stack4, "filter_id": ""}
+            item_c_execution_args = [[[expected_fw_resource4, expected_fw_resource2], []]]
+            self._assert_resource_args(mock_item_c3.execute.call_args_list, item_c_execution_args)
+            item_c_execution_args = [[[expected_fw_resource4, expected_fw_resource3], []]]
+            self._assert_resource_args(mock_item_c4.execute.call_args_list, item_c_execution_args)
+
+    def test_merge_two_filtered_file_resource_branches(self):
+        with TemporaryDirectory() as temp_dir:
+            urlA = "sqlite:///" + os.path.join(temp_dir, "dbA.sqlite")
+            db_map = DatabaseMapping(urlA, create=True)
+            import_scenarios(db_map, (("scenA1", True), ("scenA2", True)))
+            import_tools(db_map, ("toolA",))
+            db_map.commit_session("Add test data.")
+            db_map.connection.close()
+            urlB = "sqlite:///" + os.path.join(temp_dir, "dbB.sqlite")
+            db_map = DatabaseMapping(urlB, create=True)
+            import_scenarios(db_map, (("scenB1", True), ("scenB2", True)))
+            import_tools(db_map, ("toolB",))
+            db_map.commit_session("Add test data.")
+            db_map.connection.close()
+            url_a_fw = _make_url_resource(urlA)
+            url_b_fw = _make_url_resource(urlB)
+            file_c_fw_11 = ProjectItemResource("item_c", "file", "label_1")
+            file_c_fw_12 = ProjectItemResource("item_c", "file", "label_1")
+            file_c_fw_21 = ProjectItemResource("item_c", "file", "label_2")
+            file_c_fw_22 = ProjectItemResource("item_c", "file", "label_2")
+            file_d_fw_11 = ProjectItemResource("item_d", "file", "label_3")
+            file_d_fw_12 = ProjectItemResource("item_d", "file", "label_3")
+            file_d_fw_21 = ProjectItemResource("item_d", "file", "label_4")
+            file_d_fw_22 = ProjectItemResource("item_d", "file", "label_4")
+            mock_item_a = self._mock_item("item_a", resources_forward=[url_a_fw], resources_backward=[])
+            mock_item_b = self._mock_item("item_b", resources_forward=[url_b_fw], resources_backward=[])
+            mock_item_c1 = self._mock_item(
+                "item_c", resources_forward=[file_c_fw_11, file_c_fw_21], resources_backward=[]
+            )
+            mock_item_c2 = self._mock_item(
+                "item_c", resources_forward=[file_c_fw_12, file_c_fw_22], resources_backward=[]
+            )
+            mock_item_d1 = self._mock_item(
+                "item_d", resources_forward=[file_d_fw_11, file_d_fw_21], resources_backward=[]
+            )
+            mock_item_d2 = self._mock_item(
+                "item_d", resources_forward=[file_d_fw_12, file_d_fw_22], resources_backward=[]
+            )
+            mock_item_e1 = self._mock_item("item_e", resources_forward=[], resources_backward=[])
+            mock_item_e2 = self._mock_item("item_e", resources_forward=[], resources_backward=[])
+            mock_item_e3 = self._mock_item("item_e", resources_forward=[], resources_backward=[])
+            mock_item_e4 = self._mock_item("item_e", resources_forward=[], resources_backward=[])
+            item_instances = {
+                "item_a": [mock_item_a],
+                "item_b": [mock_item_b],
+                "item_c": [mock_item_c1, mock_item_c2],
+                "item_d": [mock_item_d1, mock_item_d2],
+                "item_e": [mock_item_e1, mock_item_e2, mock_item_e3, mock_item_e4],
+            }
+            items = {
+                "item_a": {"type": "TestItem"},
+                "item_b": {"type": "TestItem"},
+                "item_c": {"type": "TestItem"},
+                "item_d": {"type": "TestItem"},
+                "item_e": {"type": "TestItem"},
+            }
+            connections = [
+                {
+                    "from": ("item_a", "right"),
+                    "to": ("item_c", "left"),
+                    "resource_filters": {url_a_fw.label: {"scenario_filter": [1, 2], "tool_filter": [1]}},
+                },
+                {
+                    "from": ("item_b", "right"),
+                    "to": ("item_d", "left"),
+                    "resource_filters": {url_b_fw.label: {"scenario_filter": [1, 2], "tool_filter": [1]}},
+                },
+                {"from": ("item_c", "right"), "to": ("item_e", "left")},
+                {"from": ("item_d", "bottom"), "to": ("item_e", "top")},
+            ]
+            self._run_engine(items, connections, item_instances)
+            item_a_execution_args = [[[], []]]
+            self._assert_resource_args(mock_item_a.execute.call_args_list, item_a_execution_args)
+            self.assertEqual(mock_item_a.filter_id, "")
+            item_b_execution_args = [[[], []]]
+            self._assert_resource_args(mock_item_b.execute.call_args_list, item_b_execution_args)
+            self.assertEqual(mock_item_b.filter_id, "")
+            # Check that item_c has been executed twice, with all combinations of item_a's filters
+            expected_fw_resource1 = ProjectItemResource("item_a", "database", "label", urlA)
+            expected_filter_stack1 = (scenario_filter_config("scenA1"), tool_filter_config("toolA"))
+            expected_fw_resource1.metadata = {"filter_stack": expected_filter_stack1, "filter_id": ""}
+            item_c_execution_args = [[[expected_fw_resource1], []]]
+            self._assert_resource_args(mock_item_c1.execute.call_args_list, item_c_execution_args)
+            self.assertEqual(mock_item_c1.filter_id, "scenA1, toolA - item_a")
+            expected_fw_resource2 = ProjectItemResource("item_a", "database", "label", urlA)
+            expected_filter_stack2 = (scenario_filter_config("scenA2"), tool_filter_config("toolA"))
+            expected_fw_resource2.metadata = {"filter_stack": expected_filter_stack2, "filter_id": ""}
+            item_c_execution_args = [[[expected_fw_resource2], []]]
+            self._assert_resource_args(mock_item_c2.execute.call_args_list, item_c_execution_args)
+            self.assertEqual(mock_item_c2.filter_id, "scenA2, toolA - item_a")
+            # Check that item_d has been executed twice, with all combinations of item_b's filters
+            expected_fw_resource3 = ProjectItemResource("item_b", "database", "label", urlB)
+            expected_filter_stack3 = (scenario_filter_config("scenB1"), tool_filter_config("toolB"))
+            expected_fw_resource3.metadata = {"filter_stack": expected_filter_stack3, "filter_id": ""}
+            item_d_execution_args = [[[expected_fw_resource3], []]]
+            self._assert_resource_args(mock_item_d1.execute.call_args_list, item_d_execution_args)
+            self.assertEqual(mock_item_d1.filter_id, "scenB1, toolB - item_b")
+            expected_fw_resource4 = ProjectItemResource("item_b", "database", "label", urlB)
+            expected_filter_stack4 = (scenario_filter_config("scenB2"), tool_filter_config("toolB"))
+            expected_fw_resource4.metadata = {"filter_stack": expected_filter_stack4, "filter_id": ""}
+            item_d_execution_args = [[[expected_fw_resource4], []]]
+            self._assert_resource_args(mock_item_d2.execute.call_args_list, item_d_execution_args)
+            self.assertEqual(mock_item_d2.filter_id, "scenB2, toolB - item_b")
+            # Check that item_e has been executed four times, with all combinations of item_c's and item_d's resources.
+            expected_fw_resource5 = ProjectItemResource("item_c", "file", "label_1")
+            expected_filter_stack5 = (scenario_filter_config("scenA1"), tool_filter_config("toolA"))
+            expected_fw_resource5.metadata = {
+                "filter_stack": expected_filter_stack5,
+                "filter_id": "scenA1, toolA - item_a",
+            }
+            expected_fw_resource6 = ProjectItemResource("item_c", "file", "label_2")
+            expected_fw_resource6.metadata = {
+                "filter_stack": expected_filter_stack5,
+                "filter_id": "scenA1, toolA - item_a",
+            }
+            expected_fw_resource7 = ProjectItemResource("item_d", "file", "label_3")
+            expected_filter_stack7 = (scenario_filter_config("scenB1"), tool_filter_config("toolB"))
+            expected_fw_resource7.metadata = {
+                "filter_stack": expected_filter_stack7,
+                "filter_id": "scenB1, toolB - item_b",
+            }
+            expected_fw_resource8 = ProjectItemResource("item_d", "file", "label_4")
+            expected_fw_resource8.metadata = {
+                "filter_stack": expected_filter_stack7,
+                "filter_id": "scenB1, toolB - item_b",
+            }
+            item_e_execution_args = [
+                [[expected_fw_resource5, expected_fw_resource6, expected_fw_resource7, expected_fw_resource8], []]
+            ]
+            self._assert_resource_args(mock_item_e1.execute.call_args_list, item_e_execution_args)
+            self.assertEqual(mock_item_e1.filter_id, "scenA1, toolA - item_a & scenB1, toolB - item_b")
+            expected_fw_resource9 = ProjectItemResource("item_d", "file", "label_3")
+            expected_filter_stack9 = (scenario_filter_config("scenB2"), tool_filter_config("toolB"))
+            expected_fw_resource9.metadata = {
+                "filter_stack": expected_filter_stack9,
+                "filter_id": "scenB2, toolB - item_b",
+            }
+            expected_fw_resource10 = ProjectItemResource("item_d", "file", "label_4")
+            expected_fw_resource10.metadata = {
+                "filter_stack": expected_filter_stack9,
+                "filter_id": "scenB2, toolB - item_b",
+            }
+            item_e_execution_args = [
+                [[expected_fw_resource5, expected_fw_resource6, expected_fw_resource9, expected_fw_resource10], []]
+            ]
+            self._assert_resource_args(mock_item_e2.execute.call_args_list, item_e_execution_args)
+            self.assertEqual(mock_item_e2.filter_id, "scenA1, toolA - item_a & scenB2, toolB - item_b")
+            expected_fw_resource11 = ProjectItemResource("item_c", "file", "label_1")
+            expected_filter_stack11 = (scenario_filter_config("scenA2"), tool_filter_config("toolA"))
+            expected_fw_resource11.metadata = {
+                "filter_stack": expected_filter_stack11,
+                "filter_id": "scenA2, toolA - item_a",
+            }
+            expected_fw_resource12 = ProjectItemResource("item_c", "file", "label_2")
+            expected_fw_resource12.metadata = {
+                "filter_stack": expected_filter_stack11,
+                "filter_id": "scenA2, toolA - item_a",
+            }
+            item_e_execution_args = [
+                [[expected_fw_resource11, expected_fw_resource12, expected_fw_resource7, expected_fw_resource8], []]
+            ]
+            self._assert_resource_args(mock_item_e3.execute.call_args_list, item_e_execution_args)
+            self.assertEqual(mock_item_e3.filter_id, "scenA2, toolA - item_a & scenB1, toolB - item_b")
+            item_e_execution_args = [
+                [[expected_fw_resource11, expected_fw_resource12, expected_fw_resource9, expected_fw_resource10], []]
+            ]
+            self._assert_resource_args(mock_item_e4.execute.call_args_list, item_e_execution_args)
+            self.assertEqual(mock_item_e4.filter_id, "scenA2, toolA - item_a & scenB2, toolB - item_b")
+
+    def test_self_jump_succeeds(self):
+        mock_item = self._mock_item("item", resources_forward=[], resources_backward=[])
+        items = {"item": {"type": "TestItem"}}
+        connections = []
+        jumps = [Jump("item", "bottom", "item", "top", self._LOOP_TWICE).to_dict()]
+        execution_permits = {"item": True}
+
+        engine = SpineEngine(
+            items=items,
+            connections=connections,
+            jumps=jumps,
+            execution_permits=execution_permits,
+            items_module_name="items_module",
+        )
+        engine.make_item = lambda name, direction: mock_item
+        engine.run()
+        lock_1 = mock_item.execute.call_args_list[0].args[-1]
+        lock_2 = mock_item.execute.call_args_list[1].args[-1]
+        self.assertEqual(mock_item.execute.call_args_list, [call([], [], lock_1), call([], [], lock_2)])
+        self.assertEqual(engine.state(), SpineEngineState.COMPLETED)
+
+    @unittest.skip("Hangs because something's not right in SpineDBServer")
+    def test_jump_resources_get_passed_correctly(self):
+        resource_fw_a = _make_url_resource("db:///fw_a")
+        resource_bw_a = _make_url_resource("db:///bw_a")
+        item_a = self._mock_item("a", resources_forward=[resource_fw_a], resources_backward=[resource_bw_a])
+        resource_fw_b = _make_url_resource("db:///fw_b")
+        resource_bw_b = _make_url_resource("db:///bw_b")
+        item_b = self._mock_item("b", resources_forward=[resource_fw_b], resources_backward=[resource_bw_b])
+        resource_fw_c = _make_url_resource("db:///fw_c")
+        resource_bw_c = _make_url_resource("db:///bw_c")
+        item_c = self._mock_item("c", resources_forward=[resource_fw_c], resources_backward=[resource_bw_c])
+        resource_fw_d = _make_url_resource("db:///fw_d")
+        resource_bw_d = _make_url_resource("db:///bw_d")
+        item_d = self._mock_item("d", resources_forward=[resource_fw_d], resources_backward=[resource_bw_d])
+        item_instances = {"a": [item_a], "b": [item_b, item_b], "c": [item_c, item_c], "d": [item_d]}
+        items = {
+            "a": {"type": "TestItem"},
+            "b": {"type": "TestItem"},
+            "c": {"type": "TestItem"},
+            "d": {"type": "TestItem"},
+        }
+        connections = [
+            c.to_dict()
+            for c in (
+                Connection("a", "right", "b", "left"),
+                Connection("b", "bottom", "c", "top"),
+                Connection("c", "left", "d", "right"),
+            )
+        ]
+        jumps = [Jump("c", "right", "b", "right", self._LOOP_TWICE).to_dict()]
+        self._run_engine(items, connections, item_instances, jumps=jumps)
+        self._assert_resource_args(
+            item_a.execute.call_args_list, [[[], [self._default_backward_url_resource("db:///bw_b", "a", "b")]]]
+        )
+        self._assert_resource_args(
+            item_b.execute.call_args_list,
+            2
+            * [
+                [
+                    [self._default_forward_url_resource("db:///fw_a", "a")],
+                    [self._default_backward_url_resource("db:///bw_c", "b", "c")],
+                ]
+            ],
+        )
+        self._assert_resource_args(
+            item_c.execute.call_args_list,
+            2
+            * [
+                [
+                    [self._default_forward_url_resource("db:///fw_b", "b")],
+                    [self._default_backward_url_resource("db:///bw_d", "c", "d")],
+                ]
+            ],
+        )
+        self._assert_resource_args(
+            item_d.execute.call_args_list, [[[self._default_forward_url_resource("db:///fw_c", "c")], []]]
+        )
+
+    @unittest.skip("Hangs because something's not right in SpineDBServer")
+    def test_nested_jump_with_inner_self_jump(self):
+        resource_fw_a = _make_url_resource("db:///fw_a")
+        resource_bw_a = _make_url_resource("db:///bw_a")
+        item_a = self._mock_item("a", resources_forward=[resource_fw_a], resources_backward=[resource_bw_a])
+        resource_fw_b = _make_url_resource("db:///fw_b")
+        resource_bw_b = _make_url_resource("db:///bw_b")
+        item_b = self._mock_item("b", resources_forward=[resource_fw_b], resources_backward=[resource_bw_b])
+        resource_fw_c = _make_url_resource("db:///fw_c")
+        resource_bw_c = _make_url_resource("db:///bw_c")
+        item_c = self._mock_item("c", resources_forward=[resource_fw_c], resources_backward=[resource_bw_c])
+        item_instances = {"a": 2 * [item_a], "b": 4 * [item_b], "c": 2 * [item_c]}
+        items = {"a": {"type": "TestItem"}, "b": {"type": "TestItem"}, "c": {"type": "TestItem"}}
+        connections = [
+            c.to_dict() for c in (Connection("a", "right", "b", "left"), Connection("b", "bottom", "c", "top"))
+        ]
+        jumps = [
+            Jump("c", "right", "a", "right", self._LOOP_TWICE).to_dict(),
+            Jump("b", "top", "b", "top", self._LOOP_TWICE).to_dict(),
+        ]
+        self._run_engine(items, connections, item_instances, jumps=jumps)
+        expected = 2 * [[[], [self._default_backward_url_resource("db:///bw_b", "a", "b")]]]
+        self._assert_resource_args(item_a.execute.call_args_list, expected)
+        expected = 4 * [
+            [
+                [self._default_forward_url_resource("db:///fw_a", "a")],
+                [self._default_backward_url_resource("db:///bw_c", "b", "c")],
+            ]
+        ]
+        self._assert_resource_args(item_b.execute.call_args_list, expected)
+        expected = 2 * [[[self._default_forward_url_resource("db:///fw_b", "b")], []]]
+        self._assert_resource_args(item_c.execute.call_args_list, expected)
+
+    def _assert_resource_args(self, arg_packs, expected_packs):
+        self.assertEqual(len(arg_packs), len(expected_packs))
+        for pack, expected_pack in zip(arg_packs, expected_packs):
+            self.assertEqual(len(pack), len(expected_pack))
+            for args, expected in zip(pack[0], expected_pack):
+                self.assertEqual(len(args), len(expected))
+                for resource, expected_resource in zip(args, expected):
+                    self.assertEqual(resource.provider_name, expected_resource.provider_name)
+                    self.assertEqual(resource.label, expected_resource.label)
+                    self.assertEqual(clear_filter_configs(resource.url), expected_resource.url)
+                    for key, value in expected_resource.metadata.items():
+                        self.assertEqual(resource.metadata[key], value)
+
+
+class TestValidateSingleJump(unittest.TestCase):
+    def test_bug(self):
+        edges = {"a": ["b"], "b": ["c"], "c": "d"}
+        dag = make_dag(edges)
+        jumps = [Jump("b", "top", "a", "top"), Jump("d", "top", "c", "top")]
+        jump_to_check = jumps[0]
+        try:
+            validate_single_jump(jump_to_check, jumps, dag)
+        except EngineInitFailed:
+            self.fail("validate_single_jump shouldn't have raised")
+
+
+if __name__ == '__main__':
+    unittest.main()
```

### Comparing `spine_engine-0.23.3/tests/utils/__init__.py` & `spine_engine-0.23.4/spine_engine/project_item/__init__.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,15 +1,15 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Init file for utils package. Intentionally empty.
-
-"""
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+This subpackage contains base classes for project items.
+
+"""
```

### Comparing `spine_engine-0.23.3/tests/utils/test_command_line_args.py` & `spine_engine-0.23.4/tests/utils/test_command_line_args.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,38 +1,38 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Unit tests for command line args module.
-
-"""
-
-import unittest
-from spine_engine.utils.command_line_arguments import split_cmdline_args
-
-
-class TestToolSpecification(unittest.TestCase):
-    def test_split_cmdline_args(self):
-        splitted = split_cmdline_args("")
-        self.assertFalse(bool(splitted))
-        splitted = split_cmdline_args("--version")
-        self.assertEqual(splitted, ["--version"])
-        splitted = split_cmdline_args("--input=data.dat -h 5")
-        self.assertEqual(splitted, ["--input=data.dat", "-h", "5"])
-        splitted = split_cmdline_args('--output="a long file name.txt"')
-        self.assertEqual(splitted, ["--output=a long file name.txt"])
-        splitted = split_cmdline_args("--file='file name with spaces.dat' -i 3")
-        self.assertEqual(splitted, ["--file=file name with spaces.dat", "-i", "3"])
-        splitted = split_cmdline_args("'quotation \"within\" a quotation'")
-        self.assertEqual(splitted, ['quotation "within" a quotation'])
-
-
-if __name__ == "__main__":
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Unit tests for command line args module.
+
+"""
+
+import unittest
+from spine_engine.utils.command_line_arguments import split_cmdline_args
+
+
+class TestToolSpecification(unittest.TestCase):
+    def test_split_cmdline_args(self):
+        splitted = split_cmdline_args("")
+        self.assertFalse(bool(splitted))
+        splitted = split_cmdline_args("--version")
+        self.assertEqual(splitted, ["--version"])
+        splitted = split_cmdline_args("--input=data.dat -h 5")
+        self.assertEqual(splitted, ["--input=data.dat", "-h", "5"])
+        splitted = split_cmdline_args('--output="a long file name.txt"')
+        self.assertEqual(splitted, ["--output=a long file name.txt"])
+        splitted = split_cmdline_args("--file='file name with spaces.dat' -i 3")
+        self.assertEqual(splitted, ["--file=file name with spaces.dat", "-i", "3"])
+        splitted = split_cmdline_args("'quotation \"within\" a quotation'")
+        self.assertEqual(splitted, ['quotation "within" a quotation'])
+
+
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `spine_engine-0.23.3/tests/utils/test_helpers.py` & `spine_engine-0.23.4/tests/utils/test_helpers.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,191 +1,191 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Unit tests for chunk module.
-
-"""
-import unittest
-
-from spine_engine.project_item.connection import Connection, FilterSettings
-from spinedb_api.filters.scenario_filter import SCENARIO_FILTER_TYPE
-from spinedb_api.helpers import remove_credentials_from_url
-from spine_engine.utils.helpers import make_dag, gather_leaf_data, get_file_size, required_items_for_execution
-
-
-class TestRequiredItemsForExecution(unittest.TestCase):
-    class NormalItem:
-        @staticmethod
-        def is_filter_terminus():
-            return False
-
-    class TerminusItem:
-        @staticmethod
-        def is_filter_terminus():
-            return True
-
-    _item_classes = {
-        "Normal": NormalItem,
-        "Terminus": TerminusItem,
-    }
-
-    def test_single_permitted_item(self):
-        items = {"item a": {"type": "Normal"}}
-        connections = []
-        permits = {"item a": True}
-        items = required_items_for_execution(items, connections, self._item_classes, permits)
-        self.assertEqual(items, {"item a"})
-
-    def test_two_connected_items_last_one_permitted(self):
-        items = {"item a": {"type": "Normal"}, "item b": {"type": "Normal"}}
-        connections = [Connection("item a", "right", "item b", "left")]
-        permits = {"item b": True}
-        items = required_items_for_execution(items, connections, self._item_classes, permits)
-        self.assertEqual(items, {"item b"})
-
-    def test_filtered_chain_last_item_permitted(self):
-        items = {"item a": {"type": "Normal"}, "item b": {"type": "Normal"}, "item c": {"type": "Normal"}}
-        filter_settings = FilterSettings({"resource@a": {SCENARIO_FILTER_TYPE: {"filter 1": True}}})
-        connections = [
-            Connection("item a", "right", "item b", "left", filter_settings=filter_settings),
-            Connection("item b", "right", "item c", "left"),
-        ]
-        permits = {"item c": True}
-        items = required_items_for_execution(items, connections, self._item_classes, permits)
-        self.assertEqual(items, {"item b", "item c"})
-
-    def test_filter_terminus_ends_filtered_fork(self):
-        items = {"item a": {"type": "Normal"}, "item b": {"type": "Terminus"}, "item c": {"type": "Normal"}}
-        filter_settings = FilterSettings({"resource@a": {SCENARIO_FILTER_TYPE: {"filter 1": True}}})
-        connections = [
-            Connection("item a", "right", "item b", "left", filter_settings=filter_settings),
-            Connection("item b", "right", "item c", "left"),
-        ]
-        permits = {"item c": True}
-        items = required_items_for_execution(items, connections, self._item_classes, permits)
-        self.assertEqual(items, {"item c"})
-
-    def test_filter_terminus_ends_filtered_fork_with_longer_chain(self):
-        items = {
-            "item a": {"type": "Normal"},
-            "item b": {"type": "Normal"},
-            "item c": {"type": "Terminus", "item d": {"type": "Normal"}},
-        }
-        filter_settings = FilterSettings({"resource@a": {SCENARIO_FILTER_TYPE: {"filter 1": True}}})
-        connections = [
-            Connection("item a", "right", "item b", "left", filter_settings=filter_settings),
-            Connection("item b", "right", "item c", "left"),
-            Connection("item c", "right", "item d", "left"),
-        ]
-        permits = {"item d": True}
-        items = required_items_for_execution(items, connections, self._item_classes, permits)
-        self.assertEqual(items, {"item d"})
-
-
-class TestMakeDAG(unittest.TestCase):
-    def test_single_node(self):
-        edges = {"a": None}
-        dag = make_dag(edges)
-        self.assertEqual(len(dag), 1)
-
-    def test_two_nodes(self):
-        edges = {"a": ["b"], "b": None}
-        dag = make_dag(edges)
-        self.assertEqual(set(dag.nodes), {"a", "b"})
-        self.assertEqual(list(dag.edges), [("a", "b")])
-
-    def test_branch(self):
-        edges = {"a": ["b", "c"], "b": None, "c": None}
-        dag = make_dag(edges)
-        self.assertEqual(set(dag.nodes), {"a", "b", "c"})
-        self.assertEqual(set(dag.edges), {("a", "b"), ("a", "c")})
-
-    def test_no_edges_one_node(self):
-        edges = {}
-        permitted_nodes = {"a": True}
-        dag = make_dag(edges, permitted_nodes)
-        self.assertEqual(len(dag), 1)
-
-    def test_no_edges_two_nodes(self):
-        # Note: This does not happen in practice because unconnected nodes will be executed on different Engines
-        edges = {}
-        permitted_nodes = {"a": True, "b": True}
-        dag = make_dag(edges, permitted_nodes)
-        self.assertEqual(len(dag), 2)
-
-
-class TestRemoveCredentialsFromUrl(unittest.TestCase):
-    def test_no_credentials_in_url(self):
-        no_credentials = r"sqlite:///C:\data\db.sqlite"
-        self.assertEqual(remove_credentials_from_url(no_credentials), no_credentials)
-
-    def test_credentials_in_database_url(self):
-        with_credentials = "mysql+pymysql://username:password@remote.fi/database"
-        self.assertEqual(remove_credentials_from_url(with_credentials), "mysql+pymysql://remote.fi/database")
-
-
-class TestGatherLeafData(unittest.TestCase):
-    def test_empty_inputs_give_empty_output(self):
-        self.assertEqual(gather_leaf_data({}, []), {})
-
-    def test_popping_non_existent_path_pop_nothing(self):
-        input_dict = {"a": 1}
-        popped = gather_leaf_data(input_dict, [("b",)], pop=True)
-        self.assertEqual(popped, {})
-        self.assertEqual(input_dict, {"a": 1})
-
-    def test_gathers_shallow_value(self):
-        input_dict = {"a": 1}
-        popped = gather_leaf_data(input_dict, [("a",)])
-        self.assertEqual(popped, {"a": 1})
-        self.assertEqual(input_dict, {"a": 1})
-
-    def test_pops_shallow_value(self):
-        input_dict = {"a": 1}
-        popped = gather_leaf_data(input_dict, [("a",)], pop=True)
-        self.assertEqual(popped, {"a": 1})
-        self.assertEqual(input_dict, {})
-
-    def test_gathers_leaf_of_deep_input_dict(self):
-        input_dict = {"a": {"b": 1, "c": 2}}
-        popped = gather_leaf_data(input_dict, [("a", "c")])
-        self.assertEqual(popped, {"a": {"c": 2}})
-        self.assertEqual(input_dict, {"a": {"b": 1, "c": 2}})
-
-    def test_pops_leaf_of_deep_input_dict(self):
-        input_dict = {"a": {"b": 1, "c": 2}}
-        popped = gather_leaf_data(input_dict, [("a", "c")], pop=True)
-        self.assertEqual(popped, {"a": {"c": 2}})
-        self.assertEqual(input_dict, {"a": {"b": 1}})
-
-
-class TestGetFileSize(unittest.TestCase):
-    def test_get_file_size(self):
-        s = 523
-        expected_output = "523 B"
-        output = get_file_size(s)
-        self.assertEqual(expected_output, output)
-        s = 2048
-        expected_output = "2.0 KB"
-        output = get_file_size(s)
-        self.assertEqual(expected_output, output)
-        s = 1024 * 1024 * 2
-        expected_output = "2.0 MB"
-        output = get_file_size(s)
-        self.assertEqual(expected_output, output)
-        s = 1024 * 1024 * 1024 * 2
-        expected_output = "2.0 GB"
-        output = get_file_size(s)
-        self.assertEqual(expected_output, output)
-
-
-if __name__ == '__main__':
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Unit tests for chunk module.
+
+"""
+import unittest
+
+from spine_engine.project_item.connection import Connection, FilterSettings
+from spinedb_api.filters.scenario_filter import SCENARIO_FILTER_TYPE
+from spinedb_api.helpers import remove_credentials_from_url
+from spine_engine.utils.helpers import make_dag, gather_leaf_data, get_file_size, required_items_for_execution
+
+
+class TestRequiredItemsForExecution(unittest.TestCase):
+    class NormalItem:
+        @staticmethod
+        def is_filter_terminus():
+            return False
+
+    class TerminusItem:
+        @staticmethod
+        def is_filter_terminus():
+            return True
+
+    _item_classes = {
+        "Normal": NormalItem,
+        "Terminus": TerminusItem,
+    }
+
+    def test_single_permitted_item(self):
+        items = {"item a": {"type": "Normal"}}
+        connections = []
+        permits = {"item a": True}
+        items = required_items_for_execution(items, connections, self._item_classes, permits)
+        self.assertEqual(items, {"item a"})
+
+    def test_two_connected_items_last_one_permitted(self):
+        items = {"item a": {"type": "Normal"}, "item b": {"type": "Normal"}}
+        connections = [Connection("item a", "right", "item b", "left")]
+        permits = {"item b": True}
+        items = required_items_for_execution(items, connections, self._item_classes, permits)
+        self.assertEqual(items, {"item b"})
+
+    def test_filtered_chain_last_item_permitted(self):
+        items = {"item a": {"type": "Normal"}, "item b": {"type": "Normal"}, "item c": {"type": "Normal"}}
+        filter_settings = FilterSettings({"resource@a": {SCENARIO_FILTER_TYPE: {"filter 1": True}}})
+        connections = [
+            Connection("item a", "right", "item b", "left", filter_settings=filter_settings),
+            Connection("item b", "right", "item c", "left"),
+        ]
+        permits = {"item c": True}
+        items = required_items_for_execution(items, connections, self._item_classes, permits)
+        self.assertEqual(items, {"item b", "item c"})
+
+    def test_filter_terminus_ends_filtered_fork(self):
+        items = {"item a": {"type": "Normal"}, "item b": {"type": "Terminus"}, "item c": {"type": "Normal"}}
+        filter_settings = FilterSettings({"resource@a": {SCENARIO_FILTER_TYPE: {"filter 1": True}}})
+        connections = [
+            Connection("item a", "right", "item b", "left", filter_settings=filter_settings),
+            Connection("item b", "right", "item c", "left"),
+        ]
+        permits = {"item c": True}
+        items = required_items_for_execution(items, connections, self._item_classes, permits)
+        self.assertEqual(items, {"item c"})
+
+    def test_filter_terminus_ends_filtered_fork_with_longer_chain(self):
+        items = {
+            "item a": {"type": "Normal"},
+            "item b": {"type": "Normal"},
+            "item c": {"type": "Terminus", "item d": {"type": "Normal"}},
+        }
+        filter_settings = FilterSettings({"resource@a": {SCENARIO_FILTER_TYPE: {"filter 1": True}}})
+        connections = [
+            Connection("item a", "right", "item b", "left", filter_settings=filter_settings),
+            Connection("item b", "right", "item c", "left"),
+            Connection("item c", "right", "item d", "left"),
+        ]
+        permits = {"item d": True}
+        items = required_items_for_execution(items, connections, self._item_classes, permits)
+        self.assertEqual(items, {"item d"})
+
+
+class TestMakeDAG(unittest.TestCase):
+    def test_single_node(self):
+        edges = {"a": None}
+        dag = make_dag(edges)
+        self.assertEqual(len(dag), 1)
+
+    def test_two_nodes(self):
+        edges = {"a": ["b"], "b": None}
+        dag = make_dag(edges)
+        self.assertEqual(set(dag.nodes), {"a", "b"})
+        self.assertEqual(list(dag.edges), [("a", "b")])
+
+    def test_branch(self):
+        edges = {"a": ["b", "c"], "b": None, "c": None}
+        dag = make_dag(edges)
+        self.assertEqual(set(dag.nodes), {"a", "b", "c"})
+        self.assertEqual(set(dag.edges), {("a", "b"), ("a", "c")})
+
+    def test_no_edges_one_node(self):
+        edges = {}
+        permitted_nodes = {"a": True}
+        dag = make_dag(edges, permitted_nodes)
+        self.assertEqual(len(dag), 1)
+
+    def test_no_edges_two_nodes(self):
+        # Note: This does not happen in practice because unconnected nodes will be executed on different Engines
+        edges = {}
+        permitted_nodes = {"a": True, "b": True}
+        dag = make_dag(edges, permitted_nodes)
+        self.assertEqual(len(dag), 2)
+
+
+class TestRemoveCredentialsFromUrl(unittest.TestCase):
+    def test_no_credentials_in_url(self):
+        no_credentials = r"sqlite:///C:\data\db.sqlite"
+        self.assertEqual(remove_credentials_from_url(no_credentials), no_credentials)
+
+    def test_credentials_in_database_url(self):
+        with_credentials = "mysql+pymysql://username:password@remote.fi/database"
+        self.assertEqual(remove_credentials_from_url(with_credentials), "mysql+pymysql://remote.fi/database")
+
+
+class TestGatherLeafData(unittest.TestCase):
+    def test_empty_inputs_give_empty_output(self):
+        self.assertEqual(gather_leaf_data({}, []), {})
+
+    def test_popping_non_existent_path_pop_nothing(self):
+        input_dict = {"a": 1}
+        popped = gather_leaf_data(input_dict, [("b",)], pop=True)
+        self.assertEqual(popped, {})
+        self.assertEqual(input_dict, {"a": 1})
+
+    def test_gathers_shallow_value(self):
+        input_dict = {"a": 1}
+        popped = gather_leaf_data(input_dict, [("a",)])
+        self.assertEqual(popped, {"a": 1})
+        self.assertEqual(input_dict, {"a": 1})
+
+    def test_pops_shallow_value(self):
+        input_dict = {"a": 1}
+        popped = gather_leaf_data(input_dict, [("a",)], pop=True)
+        self.assertEqual(popped, {"a": 1})
+        self.assertEqual(input_dict, {})
+
+    def test_gathers_leaf_of_deep_input_dict(self):
+        input_dict = {"a": {"b": 1, "c": 2}}
+        popped = gather_leaf_data(input_dict, [("a", "c")])
+        self.assertEqual(popped, {"a": {"c": 2}})
+        self.assertEqual(input_dict, {"a": {"b": 1, "c": 2}})
+
+    def test_pops_leaf_of_deep_input_dict(self):
+        input_dict = {"a": {"b": 1, "c": 2}}
+        popped = gather_leaf_data(input_dict, [("a", "c")], pop=True)
+        self.assertEqual(popped, {"a": {"c": 2}})
+        self.assertEqual(input_dict, {"a": {"b": 1}})
+
+
+class TestGetFileSize(unittest.TestCase):
+    def test_get_file_size(self):
+        s = 523
+        expected_output = "523 B"
+        output = get_file_size(s)
+        self.assertEqual(expected_output, output)
+        s = 2048
+        expected_output = "2.0 KB"
+        output = get_file_size(s)
+        self.assertEqual(expected_output, output)
+        s = 1024 * 1024 * 2
+        expected_output = "2.0 MB"
+        output = get_file_size(s)
+        self.assertEqual(expected_output, output)
+        s = 1024 * 1024 * 1024 * 2
+        expected_output = "2.0 GB"
+        output = get_file_size(s)
+        self.assertEqual(expected_output, output)
+
+
+if __name__ == '__main__':
+    unittest.main()
```

### Comparing `spine_engine-0.23.3/tests/utils/test_serialization.py` & `spine_engine-0.23.4/tests/utils/test_serialization.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,147 +1,147 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Unit tests for serialization module.
-
-"""
-import sys
-import unittest
-from pathlib import Path
-from tempfile import gettempdir, TemporaryDirectory, NamedTemporaryFile
-
-from spine_engine.utils.serialization import deserialize_path, path_in_dir, serialize_url, serialize_path
-
-
-class TestSerialization(unittest.TestCase):
-    def test_serialize_path_makes_relative_paths_from_paths_in_project_dir(self):
-        with TemporaryDirectory() as path:
-            project_dir = gettempdir()
-            serialized = serialize_path(path, project_dir)
-            expected_path = str(Path(path).relative_to(project_dir).as_posix())
-            self.assertEqual(serialized, {"type": "path", "relative": True, "path": expected_path})
-
-    def test_serialize_path_makes_absolute_paths_from_paths_not_in_project_dir(self):
-        with TemporaryDirectory() as project_dir:
-            with TemporaryDirectory() as path:
-                serialized = serialize_path(path, project_dir)
-                expected_path = str(Path(path).as_posix())
-                self.assertEqual(serialized, {"type": "path", "relative": False, "path": expected_path})
-
-    def test_serialize_url_makes_file_path_in_project_dir_relative(self):
-        with NamedTemporaryFile(mode="r") as temp_file:
-            url = "sqlite:///" + str(Path(temp_file.name).as_posix())
-            project_dir = gettempdir()
-            expected_path = str(Path(temp_file.name).relative_to(project_dir).as_posix())
-            serialized = serialize_url(url, project_dir)
-            self.assertEqual(
-                serialized, {"type": "file_url", "relative": True, "path": expected_path, "scheme": "sqlite"}
-            )
-
-    def test_serialize_url_keeps_file_path_not_in_project_dir_absolute(self):
-        with TemporaryDirectory() as project_dir:
-            with NamedTemporaryFile(mode="r") as temp_file:
-                expected_path = str(Path(temp_file.name).as_posix())
-                if sys.platform == "win32":
-                    url = "sqlite:///" + expected_path
-                else:
-                    url = "sqlite://" + expected_path
-                serialized = serialize_url(url, project_dir)
-                self.assertEqual(
-                    serialized, {"type": "file_url", "relative": False, "path": expected_path, "scheme": "sqlite"}
-                )
-
-    def test_serialize_url_with_non_file_urls(self):
-        project_dir = gettempdir()
-        url = "http://www.spine-model.org/"
-        serialized = serialize_url(url, project_dir)
-        self.assertEqual(serialized, {"type": "url", "relative": False, "path": url})
-
-    def test_serialize_relative_url_with_query(self):
-        with NamedTemporaryFile(mode="r") as temp_file:
-            url = "sqlite:///" + str(Path(temp_file.name).as_posix()) + "?filter=kol"
-            project_dir = gettempdir()
-            expected_path = str(Path(temp_file.name).relative_to(project_dir).as_posix())
-            serialized = serialize_url(url, project_dir)
-            self.assertEqual(
-                serialized,
-                {
-                    "type": "file_url",
-                    "relative": True,
-                    "path": expected_path,
-                    "scheme": "sqlite",
-                    "query": "filter=kol",
-                },
-            )
-
-    def test_deserialize_path_with_relative_path(self):
-        project_dir = gettempdir()
-        serialized = {"type": "path", "relative": True, "path": "subdir/file.fat"}
-        deserialized = deserialize_path(serialized, project_dir)
-        self.assertEqual(deserialized, str(Path(project_dir, "subdir", "file.fat")))
-
-    def test_deserialize_path_with_absolute_path(self):
-        with TemporaryDirectory() as project_dir:
-            serialized = {"type": "path", "relative": False, "path": str(Path(gettempdir(), "file.fat").as_posix())}
-            deserialized = deserialize_path(serialized, project_dir)
-            self.assertEqual(deserialized, str(Path(gettempdir(), "file.fat")))
-
-    def test_deserialize_path_with_relative_file_url(self):
-        project_dir = gettempdir()
-        serialized = {"type": "file_url", "relative": True, "path": "subdir/database.sqlite", "scheme": "sqlite"}
-        deserialized = deserialize_path(serialized, project_dir)
-        expected = "sqlite:///" + str(Path(project_dir, "subdir", "database.sqlite"))
-        self.assertEqual(deserialized, expected)
-
-    def test_deserialize_path_with_absolute_file_url(self):
-        with TemporaryDirectory() as project_dir:
-            path = str(Path(gettempdir(), "database.sqlite").as_posix())
-            serialized = {"type": "file_url", "relative": False, "path": path, "scheme": "sqlite"}
-            deserialized = deserialize_path(serialized, project_dir)
-            expected = "sqlite:///" + str(Path(gettempdir(), "database.sqlite"))
-            self.assertEqual(deserialized, expected)
-
-    def test_deserialize_path_with_non_file_url(self):
-        project_dir = gettempdir()
-        serialized = {"type": "url", "path": "http://www.spine-model.org/"}
-        deserialized = deserialize_path(serialized, project_dir)
-        self.assertEqual(deserialized, "http://www.spine-model.org/")
-
-    def test_deserialize_relative_url_with_query(self):
-        project_dir = gettempdir()
-        serialized = {
-            "type": "file_url",
-            "relative": True,
-            "path": "subdir/database.sqlite",
-            "scheme": "sqlite",
-            "query": "filter=kax",
-        }
-        deserialized = deserialize_path(serialized, project_dir)
-        expected = "sqlite:///" + str(Path(project_dir, "subdir", "database.sqlite")) + "?filter=kax"
-        self.assertEqual(deserialized, expected)
-
-
-class TestPathInDir(unittest.TestCase):
-    @unittest.skipIf(sys.platform != "win32", "This test uses Windows paths.")
-    def test_windows_paths_on_different_drives(self):
-        self.assertFalse(path_in_dir(r"Z:\path\to\file.xt", r"C:\path\\"))
-
-    @unittest.skipIf(sys.platform != "win32", "This test uses Windows paths.")
-    def test_windows_paths_on_same_drive_but_different_casing(self):
-        self.assertTrue(path_in_dir(r"c:\path\to\file.xt", r"C:\path\\"))
-
-    def test_unix_paths(self):
-        self.assertTrue(path_in_dir("/path/to/my/file.dat", "/path/to"))
-        self.assertFalse(path_in_dir("/path/to/my/file.dat", "/another/path"))
-
-
-if __name__ == '__main__':
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Unit tests for serialization module.
+
+"""
+import sys
+import unittest
+from pathlib import Path
+from tempfile import gettempdir, TemporaryDirectory, NamedTemporaryFile
+
+from spine_engine.utils.serialization import deserialize_path, path_in_dir, serialize_url, serialize_path
+
+
+class TestSerialization(unittest.TestCase):
+    def test_serialize_path_makes_relative_paths_from_paths_in_project_dir(self):
+        with TemporaryDirectory() as path:
+            project_dir = gettempdir()
+            serialized = serialize_path(path, project_dir)
+            expected_path = str(Path(path).relative_to(project_dir).as_posix())
+            self.assertEqual(serialized, {"type": "path", "relative": True, "path": expected_path})
+
+    def test_serialize_path_makes_absolute_paths_from_paths_not_in_project_dir(self):
+        with TemporaryDirectory() as project_dir:
+            with TemporaryDirectory() as path:
+                serialized = serialize_path(path, project_dir)
+                expected_path = str(Path(path).as_posix())
+                self.assertEqual(serialized, {"type": "path", "relative": False, "path": expected_path})
+
+    def test_serialize_url_makes_file_path_in_project_dir_relative(self):
+        with NamedTemporaryFile(mode="r") as temp_file:
+            url = "sqlite:///" + str(Path(temp_file.name).as_posix())
+            project_dir = gettempdir()
+            expected_path = str(Path(temp_file.name).relative_to(project_dir).as_posix())
+            serialized = serialize_url(url, project_dir)
+            self.assertEqual(
+                serialized, {"type": "file_url", "relative": True, "path": expected_path, "scheme": "sqlite"}
+            )
+
+    def test_serialize_url_keeps_file_path_not_in_project_dir_absolute(self):
+        with TemporaryDirectory() as project_dir:
+            with NamedTemporaryFile(mode="r") as temp_file:
+                expected_path = str(Path(temp_file.name).as_posix())
+                if sys.platform == "win32":
+                    url = "sqlite:///" + expected_path
+                else:
+                    url = "sqlite://" + expected_path
+                serialized = serialize_url(url, project_dir)
+                self.assertEqual(
+                    serialized, {"type": "file_url", "relative": False, "path": expected_path, "scheme": "sqlite"}
+                )
+
+    def test_serialize_url_with_non_file_urls(self):
+        project_dir = gettempdir()
+        url = "http://www.spine-model.org/"
+        serialized = serialize_url(url, project_dir)
+        self.assertEqual(serialized, {"type": "url", "relative": False, "path": url})
+
+    def test_serialize_relative_url_with_query(self):
+        with NamedTemporaryFile(mode="r") as temp_file:
+            url = "sqlite:///" + str(Path(temp_file.name).as_posix()) + "?filter=kol"
+            project_dir = gettempdir()
+            expected_path = str(Path(temp_file.name).relative_to(project_dir).as_posix())
+            serialized = serialize_url(url, project_dir)
+            self.assertEqual(
+                serialized,
+                {
+                    "type": "file_url",
+                    "relative": True,
+                    "path": expected_path,
+                    "scheme": "sqlite",
+                    "query": "filter=kol",
+                },
+            )
+
+    def test_deserialize_path_with_relative_path(self):
+        project_dir = gettempdir()
+        serialized = {"type": "path", "relative": True, "path": "subdir/file.fat"}
+        deserialized = deserialize_path(serialized, project_dir)
+        self.assertEqual(deserialized, str(Path(project_dir, "subdir", "file.fat")))
+
+    def test_deserialize_path_with_absolute_path(self):
+        with TemporaryDirectory() as project_dir:
+            serialized = {"type": "path", "relative": False, "path": str(Path(gettempdir(), "file.fat").as_posix())}
+            deserialized = deserialize_path(serialized, project_dir)
+            self.assertEqual(deserialized, str(Path(gettempdir(), "file.fat")))
+
+    def test_deserialize_path_with_relative_file_url(self):
+        project_dir = gettempdir()
+        serialized = {"type": "file_url", "relative": True, "path": "subdir/database.sqlite", "scheme": "sqlite"}
+        deserialized = deserialize_path(serialized, project_dir)
+        expected = "sqlite:///" + str(Path(project_dir, "subdir", "database.sqlite"))
+        self.assertEqual(deserialized, expected)
+
+    def test_deserialize_path_with_absolute_file_url(self):
+        with TemporaryDirectory() as project_dir:
+            path = str(Path(gettempdir(), "database.sqlite").as_posix())
+            serialized = {"type": "file_url", "relative": False, "path": path, "scheme": "sqlite"}
+            deserialized = deserialize_path(serialized, project_dir)
+            expected = "sqlite:///" + str(Path(gettempdir(), "database.sqlite"))
+            self.assertEqual(deserialized, expected)
+
+    def test_deserialize_path_with_non_file_url(self):
+        project_dir = gettempdir()
+        serialized = {"type": "url", "path": "http://www.spine-model.org/"}
+        deserialized = deserialize_path(serialized, project_dir)
+        self.assertEqual(deserialized, "http://www.spine-model.org/")
+
+    def test_deserialize_relative_url_with_query(self):
+        project_dir = gettempdir()
+        serialized = {
+            "type": "file_url",
+            "relative": True,
+            "path": "subdir/database.sqlite",
+            "scheme": "sqlite",
+            "query": "filter=kax",
+        }
+        deserialized = deserialize_path(serialized, project_dir)
+        expected = "sqlite:///" + str(Path(project_dir, "subdir", "database.sqlite")) + "?filter=kax"
+        self.assertEqual(deserialized, expected)
+
+
+class TestPathInDir(unittest.TestCase):
+    @unittest.skipIf(sys.platform != "win32", "This test uses Windows paths.")
+    def test_windows_paths_on_different_drives(self):
+        self.assertFalse(path_in_dir(r"Z:\path\to\file.xt", r"C:\path\\"))
+
+    @unittest.skipIf(sys.platform != "win32", "This test uses Windows paths.")
+    def test_windows_paths_on_same_drive_but_different_casing(self):
+        self.assertTrue(path_in_dir(r"c:\path\to\file.xt", r"C:\path\\"))
+
+    def test_unix_paths(self):
+        self.assertTrue(path_in_dir("/path/to/my/file.dat", "/path/to"))
+        self.assertFalse(path_in_dir("/path/to/my/file.dat", "/another/path"))
+
+
+if __name__ == '__main__':
+    unittest.main()
```

