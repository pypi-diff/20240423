# Comparing `tmp/dmlcloud-0.3.1-py3-none-any.whl.zip` & `tmp/dmlcloud-0.3.3-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,23 +1,23 @@
-Zip file size: 20544 bytes, number of entries: 21
--rw-r--r--  2.0 unx       36 b- defN 24-Mar-20 13:53 dmlcloud/__init__.py
--rw-r--r--  2.0 unx     3116 b- defN 24-Mar-20 13:53 dmlcloud/checkpoint.py
--rw-r--r--  2.0 unx    10041 b- defN 24-Mar-20 13:53 dmlcloud/metrics.py
--rw-r--r--  2.0 unx     9474 b- defN 24-Mar-20 13:53 dmlcloud/pipeline.py
--rw-r--r--  2.0 unx     8968 b- defN 24-Mar-20 13:53 dmlcloud/stage.py
--rw-r--r--  2.0 unx        0 b- defN 24-Mar-20 13:53 dmlcloud/util/__init__.py
--rw-r--r--  2.0 unx      947 b- defN 24-Mar-20 13:53 dmlcloud/util/argparse.py
--rw-r--r--  2.0 unx     3022 b- defN 24-Mar-20 13:53 dmlcloud/util/data.py
--rw-r--r--  2.0 unx     4325 b- defN 24-Mar-20 13:53 dmlcloud/util/distributed.py
--rw-r--r--  2.0 unx      419 b- defN 24-Mar-20 13:53 dmlcloud/util/git.py
--rw-r--r--  2.0 unx     5522 b- defN 24-Mar-20 13:53 dmlcloud/util/logging.py
--rw-r--r--  2.0 unx     2269 b- defN 24-Mar-20 13:53 dmlcloud/util/project.py
--rw-r--r--  2.0 unx      203 b- defN 24-Mar-20 13:53 dmlcloud/util/slurm.py
--rw-r--r--  2.0 unx      363 b- defN 24-Mar-20 13:53 dmlcloud/util/tcp.py
--rw-r--r--  2.0 unx      402 b- defN 24-Mar-20 13:53 dmlcloud/util/thirdparty.py
--rw-r--r--  2.0 unx      227 b- defN 24-Mar-20 13:53 dmlcloud/util/wandb.py
--rw-r--r--  2.0 unx     1505 b- defN 24-Mar-20 13:54 dmlcloud-0.3.1.dist-info/LICENSE
--rw-r--r--  2.0 unx     3570 b- defN 24-Mar-20 13:54 dmlcloud-0.3.1.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Mar-20 13:54 dmlcloud-0.3.1.dist-info/WHEEL
--rw-r--r--  2.0 unx        9 b- defN 24-Mar-20 13:54 dmlcloud-0.3.1.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     1648 b- defN 24-Mar-20 13:54 dmlcloud-0.3.1.dist-info/RECORD
-21 files, 56158 bytes uncompressed, 17886 bytes compressed:  68.2%
+Zip file size: 22216 bytes, number of entries: 21
+-rw-r--r--  2.0 unx       36 b- defN 24-Apr-23 12:37 dmlcloud/__init__.py
+-rw-r--r--  2.0 unx     3122 b- defN 24-Apr-23 12:37 dmlcloud/checkpoint.py
+-rw-r--r--  2.0 unx    10064 b- defN 24-Apr-23 12:37 dmlcloud/metrics.py
+-rw-r--r--  2.0 unx     9951 b- defN 24-Apr-23 12:37 dmlcloud/pipeline.py
+-rw-r--r--  2.0 unx    10042 b- defN 24-Apr-23 12:37 dmlcloud/stage.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 12:37 dmlcloud/util/__init__.py
+-rw-r--r--  2.0 unx      947 b- defN 24-Apr-23 12:37 dmlcloud/util/argparse.py
+-rw-r--r--  2.0 unx     7370 b- defN 24-Apr-23 12:37 dmlcloud/util/data.py
+-rw-r--r--  2.0 unx     4532 b- defN 24-Apr-23 12:37 dmlcloud/util/distributed.py
+-rw-r--r--  2.0 unx      419 b- defN 24-Apr-23 12:37 dmlcloud/util/git.py
+-rw-r--r--  2.0 unx     4784 b- defN 24-Apr-23 12:37 dmlcloud/util/logging.py
+-rw-r--r--  2.0 unx     2269 b- defN 24-Apr-23 12:37 dmlcloud/util/project.py
+-rw-r--r--  2.0 unx      203 b- defN 24-Apr-23 12:37 dmlcloud/util/slurm.py
+-rw-r--r--  2.0 unx      363 b- defN 24-Apr-23 12:37 dmlcloud/util/tcp.py
+-rw-r--r--  2.0 unx      638 b- defN 24-Apr-23 12:37 dmlcloud/util/thirdparty.py
+-rw-r--r--  2.0 unx      530 b- defN 24-Apr-23 12:37 dmlcloud/util/wandb.py
+-rw-r--r--  2.0 unx     1505 b- defN 24-Apr-23 12:37 dmlcloud-0.3.3.dist-info/LICENSE
+-rw-r--r--  2.0 unx     3803 b- defN 24-Apr-23 12:37 dmlcloud-0.3.3.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-23 12:37 dmlcloud-0.3.3.dist-info/WHEEL
+-rw-r--r--  2.0 unx        9 b- defN 24-Apr-23 12:37 dmlcloud-0.3.3.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     1649 b- defN 24-Apr-23 12:37 dmlcloud-0.3.3.dist-info/RECORD
+21 files, 62328 bytes uncompressed, 19558 bytes compressed:  68.6%
```

## zipnote {}

```diff
@@ -42,23 +42,23 @@
 
 Filename: dmlcloud/util/thirdparty.py
 Comment: 
 
 Filename: dmlcloud/util/wandb.py
 Comment: 
 
-Filename: dmlcloud-0.3.1.dist-info/LICENSE
+Filename: dmlcloud-0.3.3.dist-info/LICENSE
 Comment: 
 
-Filename: dmlcloud-0.3.1.dist-info/METADATA
+Filename: dmlcloud-0.3.3.dist-info/METADATA
 Comment: 
 
-Filename: dmlcloud-0.3.1.dist-info/WHEEL
+Filename: dmlcloud-0.3.3.dist-info/WHEEL
 Comment: 
 
-Filename: dmlcloud-0.3.1.dist-info/top_level.txt
+Filename: dmlcloud-0.3.3.dist-info/top_level.txt
 Comment: 
 
-Filename: dmlcloud-0.3.1.dist-info/RECORD
+Filename: dmlcloud-0.3.3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## dmlcloud/__init__.py

```diff
@@ -1,3 +1,3 @@
-__version__ = "0.3.1"
+__version__ = "0.3.3"
 
 __all__ = []
```

## dmlcloud/checkpoint.py

```diff
@@ -25,15 +25,15 @@
 
     if name is None:
         name = 'run'
 
     if creation_time is None:
         creation_time = datetime.datetime.now()
 
-    dt = datetime.datetime.now().strftime('%Y.%m.%d-%H:%M')
+    dt = datetime.datetime.now().strftime('%Y.%m.%d-%H.%M')
     name = sanitize_filename(name)
     return root / f'{name}-{dt}-{generate_id()}'
 
 
 def find_slurm_checkpoint(root: Path | str) -> Optional[Path]:
     root = Path(root)
 
@@ -46,15 +46,15 @@
             return child
 
     return None
 
 
 class CheckpointDir:
     def __init__(self, path: Path):
-        self.path = path.resolve()
+        self.path = Path(path).resolve()
         self.logger = logging.getLogger('dmlcloud')
 
     @property
     def config_file(self) -> Path:
         return self.path / 'config.yaml'
 
     @property
```

## dmlcloud/metrics.py

```diff
@@ -227,15 +227,15 @@
 
         self.histories[name] = [] + [None] * (self.epoch - 1)
         if reduction is not None:
             self.reducers[name] = MetricReducer(reduction=reduction, dim=dim, globally=globally)
 
     def track(self, name, value):
         if isinstance(value, torch.Tensor):
-            value = value.detach().cpu()
+            value = value.detach().to('cpu', non_blocking=True)
 
         if name not in self:
             raise ValueError(f'Metric {name} does not exist')
 
         if self.has_value(name):
             raise ValueError(f'History for {name} already has a value for epoch {self.epoch}')
```

## dmlcloud/pipeline.py

```diff
@@ -4,19 +4,19 @@
 
 import torch
 import torch.distributed as dist
 from omegaconf import OmegaConf
 from torch.nn.parallel import DistributedDataParallel
 from torch.utils.data import DataLoader, Dataset
 
-from dmlcloud.util.wandb import wandb_is_initialized, wandb_set_startup_timeout
+from dmlcloud.util.wandb import wandb, wandb_is_initialized, wandb_set_startup_timeout
 from .checkpoint import CheckpointDir, find_slurm_checkpoint, generate_checkpoint_path
 from .metrics import MetricTracker, Reduction
 from .stage import Stage
-from .util.distributed import local_rank
+from .util.distributed import is_root, local_rank, root_only
 from .util.logging import add_log_handlers, experiment_header, general_diagnostics, IORedirector
 
 
 class TrainingPipeline:
     def __init__(self, config: Optional[Union[OmegaConf, Dict]] = None, name: Optional[str] = None):
         if config is None:
             self.config = OmegaConf.create()
@@ -66,17 +66,17 @@
         if use_ddp:
             model = DistributedDataParallel(model, broadcast_buffers=False)
         model = model.to(self.device)
         self.models[name] = model
 
         if verbose:
             msg = f'Model "{name}":\n'
-            msg += f'  - Parameters: {sum(p.numel() for p in model.parameters())/1e6:.1f} kk\n'
-            msg += f'  - DDP: {use_ddp}\n'
-            msg += f'  - {model}'
+            msg += f'    - Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f} kk\n'
+            msg += f'    - DDP: {use_ddp}\n'
+            msg += f'    - {model}'
             self.logger.info(msg)
 
     def register_optimizer(self, name: str, optimizer, scheduler=None):
         if name in self.optimizers:
             raise ValueError(f'Optimizer with name {name} already exists')
         self.optimizers[name] = optimizer
         if scheduler is not None:
@@ -87,76 +87,80 @@
             raise ValueError(f'Dataset with name {name} already exists')
 
         self.datasets[name] = dataset
         if verbose:
             msg = f'Dataset "{name}":\n'
             try:
                 length = len(dataset)
-                msg += f'  - Batches (Total): ~{length * dist.get_world_size()}\n'
-                msg += f'  - Batches (/Worker): {length}\n'
+                msg += f'    - Batches (Total): ~{length * dist.get_world_size()}\n'
+                msg += f'    - Batches (/Worker): {length}\n'
             except TypeError:  # __len__ not implemented
-                msg += '  - Batches (Total): N/A\n'
-                msg += '  - Batches (/Worker): N/A\n'
+                msg += '    - Batches (Total): N/A\n'
+                msg += '    - Batches (/Worker): N/A\n'
             self.logger.info(msg)
 
     def append_stage(self, stage: Stage, max_epochs: Optional[int] = None, name: Optional[str] = None):
         if not isinstance(stage, Stage):
             raise ValueError('stage must be a Stage object')
 
         stage.pipeline = self
         stage.max_epochs = max_epochs
         stage.name = name
         self.stages.append(stage)
 
     def enable_checkpointing(
         self,
         root: str,
-        resume: bool = True,
+        resume: bool = False,
     ):
         if self.checkpointing_enabled:
             raise ValueError('Checkpointing already enabled')
 
         path = None
         if resume and CheckpointDir(root).is_valid:
             path = root
             self.resumed = True
         elif resume and find_slurm_checkpoint(root):
             path = find_slurm_checkpoint(root)
             self.resumed = True
-        if path is None:
-            path = generate_checkpoint_path(root=root, name=self.name, creation_time=self.start_time)
+
+        if path is None:  # no need for a barrier here, dir creation happens in _pre_run()
+            obj_list = [generate_checkpoint_path(root=root, name=self.name, creation_time=self.start_time)]
+            dist.broadcast_object_list(obj_list)
+            path = obj_list[0]
             self.resumed = False
+
         self.checkpoint_dir = CheckpointDir(path)
 
     def enable_wandb(
         self,
         project: str | None = None,
         entity: str | None = None,
         group: str | None = None,
         tags: List[str] | None = None,
         startup_timeout: int = 360,
         **kwargs,
     ):
         import wandb  # import now to avoid potential long import times later on
 
-        self.wandb = True
-
+        @root_only
         def initializer():
             wandb_set_startup_timeout(startup_timeout)
             wandb.init(
                 config=OmegaConf.to_container(self.config, resolve=True),
                 name=self.name,
                 entity=entity,
-                project=project,
+                project=project if project else self.name,
                 group=group,
                 tags=tags,
                 **kwargs,
             )
 
         self._wandb_initalizer = initializer
+        self.wandb = True
 
     def track_reduce(
         self,
         name: str,
         value: torch.Tensor,
         step: Optional[int] = None,
         reduction: Reduction = Reduction.MEAN,
@@ -211,35 +215,39 @@
             if local_rank() is None:
                 self.device = torch.device('cuda')
             else:
                 self.device = torch.device('cuda', local_rank())
         else:
             self.device = torch.device('cpu')
 
+        dist.barrier()  # important to prevent checkpoint dir creation before all processes searched for it
         if self.checkpointing_enabled:
             self._init_checkpointing()
 
         if self.wandb:
             self._wandb_initalizer()
 
+        dist.barrier()  # make sure everything is set up before starting the run
         self.start_time = datetime.now()
 
         add_log_handlers(self.logger)
         header = '\n' + experiment_header(self.name, self.checkpoint_dir, self.start_time)
         self.logger.info(header)
 
         if self.resumed:
             self._resume_run()
 
         diagnostics = general_diagnostics()
-        diagnostics += '\n* CONFIG:\n' + OmegaConf.to_yaml(self.config)
+        diagnostics += '\n* CONFIG:\n'
+        diagnostics += '\n'.join(f'    {line}' for line in OmegaConf.to_yaml(self.config, resolve=True).splitlines())
         self.logger.info(diagnostics)
 
         self.pre_run()
 
+    @root_only
     def _init_checkpointing(self):
         if not self.checkpoint_dir.is_valid:
             self.checkpoint_dir.create()
             self.checkpoint_dir.save_config(self.config)
         self.io_redirector = IORedirector(self.checkpoint_dir.log_file)
         self.io_redirector.install()
 
@@ -254,36 +262,31 @@
             self.logger.info(f'Outputs have been saved to {self.checkpoint_dir}')
         self.post_run()
 
     def _pre_epoch(self):
         pass
 
     def _post_epoch(self):
-        if self.wandb:
-            import wandb
-
+        if self.wandb and is_root():
             metrics = {name: self.tracker[name][-1] for name in self.tracker}
             wandb.log(metrics)
 
     def _cleanup(self, exc_type, exc_value, traceback):
         """
         Called by _RunGuard to ensure that the pipeline is properly cleaned up
         """
         if exc_type is KeyboardInterrupt:
             self.logger.info('------- Training interrupted by user -------')
         elif exc_type is not None:
             self.logger.error(
                 '------- Training failed with an exception -------', exc_info=(exc_type, exc_value, traceback)
             )
 
-        if self.wandb:
-            import wandb
-
-            if wandb_is_initialized():
-                wandb.finish(exit_code=0 if exc_type is None else 1)
+        if self.wandb and wandb_is_initialized():
+            wandb.finish(exit_code=0 if exc_type is None else 1)
 
         if self.io_redirector is not None:
             self.io_redirector.uninstall()
 
         return False
```

## dmlcloud/stage.py

```diff
@@ -1,12 +1,13 @@
 import sys
 from datetime import datetime
 from typing import Any, Dict, List, Optional, Union
 
 import torch
+import torch.distributed as dist
 from progress_table import ProgressTable
 
 from .metrics import MetricTracker, Reduction
 from .util.distributed import is_root, root_only
 
 
 class Stage:
@@ -123,21 +124,23 @@
         return columns
 
     def run(self):
         """
         Runs this stage. Either until max_epochs are reached, or until stop_stage() is called.
         """
         self._pre_stage()
+        dist.barrier()
         while self.max_epochs is None or self.current_epoch <= self.max_epochs:
             self._pre_epoch()
             self.run_epoch()
             self._post_epoch()
             if self._stop_requested:
                 break
         self._post_stage()
+        dist.barrier()  # this will time out if not all processes reach this point
 
     def _pre_stage(self):
         self.start_time = datetime.now()
         self.table = ProgressTable(file=sys.stdout)
         self._setup_table()
 
         if len(self.pipeline.stages) > 1:
@@ -218,68 +221,100 @@
 
 
 class TrainValStage(Stage):
     def __init__(self):
         super().__init__()
         self.is_train = True
 
+    def train_dataset(self):
+        train_ds = self.pipeline.datasets.get('train')
+        if train_ds is None:
+            raise ValueError(
+                'No "train" dataset found in pipeline. Use register_dataset("train", ...) to register a dataset.'
+            )
+        return train_ds
+
+    def val_dataset(self):
+        val_ds = self.pipeline.datasets.get('val')
+        if val_ds is None:
+            raise ValueError(
+                'No "val" dataset found in pipeline. Use register_dataset("val", ...) to register a dataset.'
+            )
+        return val_ds
+
+    def optimizers(self):
+        return self.pipeline.optimizers.values()
+
+    def loss_metric_name(self):
+        return 'loss'
+
+    def train_metric_prefix(self):
+        return 'train'
+
+    def val_metric_prefix(self):
+        return 'val'
+
+    def gradient_clip(self):
+        return 0.0
+
     def run_epoch(self):
         self.train_epoch()
         self.val_epoch()
 
     def step(self, batch) -> torch.Tensor:
         raise NotImplementedError()
 
     def train_step(self, batch):
         return self.step(batch)
 
     def val_step(self, batch):
         return self.step(batch)
 
+    def zero_grad(self):
+        for optimizer in self.optimizers():
+            optimizer.zero_grad()
+
+    def clip_gradients(self):
+        for optimizer in self.optimizers():
+            for group in optimizer.param_groups:
+                torch.nn.utils.clip_grad_norm_(group['params'], self.gradient_clip())
+
+    def optimize(self, loss):
+        loss.backward()
+
+        if self.gradient_clip():
+            self.clip_gradients()
+
+        for optimizer in self.optimizers():
+            optimizer.step()
+
     def train_epoch(self):
         self.is_train = True
-        self.metric_prefix = 'train'
-
-        train_ds = self.pipeline.datasets.get('train')
-        if train_ds is None:
-            raise ValueError(
-                'No "train" dataset found in pipeline. Use register_dataset("train", ...) to register a dataset.'
-            )
+        self.metric_prefix = self.train_metric_prefix()
 
+        train_ds = self.train_dataset()
         if hasattr(train_ds, 'sampler') and hasattr(train_ds.sampler, 'set_epoch'):
             train_ds.sampler.set_epoch(self.current_epoch)
 
         for batch in train_ds:
-            for optimizer in self.pipeline.optimizers.values():
-                optimizer.zero_grad()
-
+            self.zero_grad()
             loss = self.train_step(batch)
-            loss.backward()
-
-            for optimizer in self.pipeline.optimizers.values():
-                optimizer.step()
-
-            self.track_reduce('loss', loss)
+            self.optimize(loss)
+            self.track_reduce(self.loss_metric_name(), loss)
 
         for scheduler in self.pipeline.schedulers.values():
             scheduler.step()
 
     @torch.no_grad()
     def val_epoch(self):
         self.is_train = False
-        self.metric_prefix = 'val'
-
-        val_ds = self.pipeline.datasets.get('val')
-        if val_ds is None:
-            raise ValueError(
-                'No "val" dataset found in pipeline. Use register_dataset("val", ...) to register a dataset.'
-            )
+        self.metric_prefix = self.val_metric_prefix()
 
-        for batch in val_ds:
+        for batch in self.val_dataset():
             loss = self.val_step(batch)
             self.track_reduce('loss', loss)
 
     def table_columns(self):
         columns = super().table_columns()
-        columns.insert(1, {'name': '[Train] Loss', 'metric': 'train/loss'})
-        columns.insert(2, {'name': '[Val] Loss', 'metric': 'val/loss'})
+        columns.insert(1, {'name': '[Train] Loss', 'metric': f'{self.train_metric_prefix()}/{self.loss_metric_name()}'})
+        columns.insert(2, {'name': '[Val] Loss', 'metric': f'{self.val_metric_prefix()}/{self.loss_metric_name()}'})
         return columns
```

## dmlcloud/util/data.py

```diff
@@ -1,106 +1,232 @@
 from typing import Iterable
 
 import numpy as np
+import torch
 import torch.distributed as dist
 import xarray as xr
 from torch.utils.data import get_worker_info, IterableDataset
 
 
 def shard_indices(
-    n: int, rank: int, world_size: int, shuffle: bool = False, drop_remainder: bool = True, seed: int = 0
+    num_elements: int,
+    rank: int,
+    world_size: int,
+    shuffle: bool = False,
+    even_shards: bool = True,
+    seed: int = 0,
 ) -> list[int]:
-    indices = np.arange(n)
+    """
+    even_shards: If True, every worker receives the same number of shards, and the last shards are dropped.
+    """
+    indices = np.arange(num_elements)
 
     if shuffle:
         np.random.Generator(np.random.MT19937(seed)).shuffle(indices)
 
-    if drop_remainder:
-        indices = indices[: n - n % world_size]
+    if even_shards:
+        indices = indices[: num_elements - num_elements % world_size]
 
     return indices[rank::world_size].tolist()  # this also converts np.int64 to python's int
 
 
+def chunk_and_shard_indices(
+    num_elements: int,
+    chunk_size: int,
+    rank: int,
+    world_size: int,
+    chunk_overlap: int = 0,
+    even_shards: bool = True,
+    equal_chunks: bool = True,
+    shuffle: bool = False,
+    seed: int = 0,
+):
+    if equal_chunks:
+        num_chunks = num_elements // chunk_size
+    else:
+        num_chunks = (num_elements + chunk_size - 1) // chunk_size
+
+    chunk_indices = shard_indices(num_chunks, rank, world_size, shuffle=shuffle, even_shards=even_shards, seed=seed)
+    chunks = []
+    for chunk_idx in chunk_indices:
+        start = chunk_idx * chunk_size
+        end = start + chunk_size + chunk_overlap
+        chunks.append((start, end))
+    return chunks
+
+
 def sharded_xr_dataset(
     ds: xr.Dataset | xr.DataArray,
-    chunk_size: int,
     dim: str,
+    chunk_size: int,
+    chunk_overlap: int = 0,
+    even_shards: bool = True,
+    equal_chunks: bool = True,
     shuffle: bool = False,
     seed: int = 0,
     rank: int | None = None,
     world_size: int | None = None,
     process_group: dist.ProcessGroup | None = None,
-    load: bool = True,
+    load: bool = False,
+    load_kwargs: dict | None = None,
 ) -> Iterable[xr.Dataset | xr.DataArray]:
-    num_total_elements = len(ds[dim])
-    num_chunks = num_total_elements // chunk_size
-
     if rank is None:
         rank = dist.get_rank(process_group)
     if world_size is None:
         world_size = dist.get_world_size(process_group)
 
-    chunk_indices = shard_indices(num_chunks, rank, world_size, shuffle=shuffle, drop_remainder=True, seed=seed)
-
-    for chunk_idx in chunk_indices:
-        start = chunk_idx * chunk_size
-        end = start + chunk_size
+    num_elements = len(ds[dim])
+    chunks = chunk_and_shard_indices(
+        num_elements,
+        chunk_size,
+        rank,
+        world_size,
+        chunk_overlap=chunk_overlap,
+        even_shards=even_shards,
+        equal_chunks=equal_chunks,
+        shuffle=shuffle,
+        seed=seed,
+    )
+    for start, end in chunks:
         chunk = ds.isel({dim: slice(start, end)})
         if load:
-            chunk.load()
+            kwargs = load_kwargs or {}
+            chunk.load(**kwargs)
         yield chunk
 
 
 class ShardedXrDataset(IterableDataset):
     def __init__(
         self,
         ds: xr.Dataset | xr.DataArray,
-        chunk_size: int,
         dim: str,
+        chunk_size: int,
+        chunk_overlap: int = 0,
+        even_shards: bool = True,
+        equal_chunks: bool = True,
         shuffle: bool = False,
         seed: int = 0,
         rank: int | None = None,
         world_size: int | None = None,
         process_group: dist.ProcessGroup | None = None,
-        load: bool = True,
+        load: bool = False,
+        load_kwargs: dict | None = None,
     ):
         self.ds = ds
-        self.chunk_size = chunk_size
         self.dim = dim
+        self.chunk_size = chunk_size
+        self.chunk_overlap = chunk_overlap
+        self.even_shards = even_shards
+        self.equal_chunks = equal_chunks
         self.shuffle = shuffle
         self.seed = seed
         self.load = load
+        self.load_kwargs = load_kwargs
 
-        if rank is None:
-            self.rank = dist.get_rank(process_group)
-        else:
-            self.rank = rank
+        self.rank = rank if rank is not None else dist.get_rank(process_group)
+        self.world_size = world_size if world_size is not None else dist.get_world_size(process_group)
+        self._num_iters = 0
 
-        if world_size is None:
-            self.world_size = dist.get_world_size(process_group)
-        else:
-            self.world_size = world_size
-
-    def __len__(self):
-        num_total_elements = len(self.ds[self.dim])
-        num_chunks = num_total_elements // self.chunk_size
-        return num_chunks // self.world_size
+    def set_epoch(self, epoch: int):
+        self._num_iters = epoch
 
     def __iter__(self):
         worker_info = get_worker_info()
         if worker_info is None:
             rank = self.rank
             world_size = self.world_size
         else:
             rank = self.rank * worker_info.num_workers + worker_info.id
             world_size = self.world_size * worker_info.num_workers
 
         return sharded_xr_dataset(
             self.ds,
-            self.chunk_size,
             self.dim,
-            self.shuffle,
-            self.seed,
-            rank,
-            world_size,
-            self.load,
+            self.chunk_size,
+            chunk_overlap=self.chunk_overlap,
+            even_shards=self.even_shards,
+            equal_chunks=self.equal_chunks,
+            shuffle=self.shuffle,
+            seed=self.seed + self._num_iters,
+            rank=rank,
+            world_size=world_size,
+            load=self.load,
+            load_kwargs=self.load_kwargs,
         )
+
+
+def interleave_batches(
+    iterable: Iterable[torch.Tensor], num_batches: int, pin_memory: bool = False
+) -> Iterable[torch.Tensor]:
+    """
+    Interleaves batches from an iterable of batches.
+    Important: Returned batches must be used immediately or copied to avoid overwriting.
+    """
+    if num_batches < 1:
+        raise ValueError('num_batches must be greater than 0')
+
+    if num_batches == 1:
+        yield from iterable
+
+    batches = []
+    memory = None
+    batch_size = None
+    slice_size = None
+    for batch in iterable:
+        if memory is None:
+            batch_size = batch.shape[0]
+            slice_size = batch_size // num_batches
+            if batch_size % num_batches != 0:
+                raise ValueError(f'Batch dimension ({batch_size}) must be divisible by num_batches={num_batches}')
+            memory = torch.empty(
+                (num_batches, *batch.shape), dtype=batch.dtype, device=batch.device, pin_memory=pin_memory
+            )
+
+        batches.append(batch)
+
+        if len(batches) == num_batches:
+            for i in range(num_batches):
+                for j in range(num_batches):
+                    memory[i, j * slice_size : (j + 1) * slice_size] = batches[j][i * slice_size : (i + 1) * slice_size]
+            batches = []
+            for i in range(num_batches):
+                yield memory[i]
+
+
+def interleave_dict_batches(
+    iterable: Iterable[torch.Tensor], num_batches: int, pin_memory: bool = False
+) -> Iterable[torch.Tensor]:
+    """
+    Interleaves batches from an iterable of batches.
+    Important: Returned batches must be used immediately or copied to avoid overwriting.
+    """
+    if num_batches < 1:
+        raise ValueError('num_batches must be greater than 0')
+
+    if num_batches == 1:
+        yield from iterable
+
+    batches = []
+    memory = {}
+    slice_size = {}
+    for batch in iterable:
+        if not memory:
+            for k, tensor in batch.items():
+                batch_size = tensor.shape[0]
+                if batch_size % num_batches != 0:
+                    raise ValueError(f'Batch dimension ({batch_size}) must be divisible by num_batches={num_batches}')
+                slice_size[k] = batch_size // num_batches
+                memory[k] = torch.empty(
+                    (num_batches, *tensor.shape), dtype=tensor.dtype, device=tensor.device, pin_memory=pin_memory
+                )
+
+        batches.append(batch)
+
+        if len(batches) == num_batches:
+            for k in memory:
+                for i in range(num_batches):
+                    for j in range(num_batches):
+                        source = batches[j][k][i * slice_size[k] : (i + 1) * slice_size[k]]
+                        memory[k][i, j * slice_size[k] : (j + 1) * slice_size[k]] = source
+            batches = []
+            for i in range(num_batches):
+                yield {k: memory[k][i] for k in memory.keys()}
```

## dmlcloud/util/distributed.py

```diff
@@ -1,10 +1,11 @@
 import os
 from contextlib import contextmanager
 
+import torch
 import torch.distributed as dist
 
 from .tcp import find_free_port, get_local_ips
 
 
 def is_root():
     return dist.get_rank() == 0
@@ -75,22 +76,25 @@
     if barrier:
         dist.barrier()
     print(f'Worker {dist.get_rank()} ({dist.get_group_rank()}.{dist.get_process_group_ranks()}): {msg}', flush=flush)
     if barrier:
         dist.barrier()
 
 
-def init_process_group_dummy():
+def init_process_group_dummy(**kwargs):
     """
     Initializes the process group with a single process.
     Uses HashStore under the hood. Useful for applications that
     only run on a single gpu.
     """
+    backend = kwargs.get('backend', None)
+    if backend is None:
+        backend = 'cpu:gloo,cuda:nccl' if dist.is_nccl_available() and torch.cuda.is_available() else 'gloo'
     store = dist.HashStore()
-    dist.init_process_group(store=store, rank=0, world_size=1, backend='gloo')
+    dist.init_process_group(store=store, rank=0, world_size=1, backend=backend, **kwargs)
 
 
 def init_process_group_MPI(ip_idx=0, port=None, **kwargs):
     """
     This method setups up the distributed backend using MPI, even
     if torch was not built with MPI support. For this to work, you
     need to have mpi4py installed and the root rank must be reachable
```

## dmlcloud/util/logging.py

```diff
@@ -7,15 +7,15 @@
 
 import torch
 import torch.distributed as dist
 
 import dmlcloud
 from . import slurm
 from .git import git_hash
-from .thirdparty import try_get_version
+from .thirdparty import is_imported, ML_MODULES, try_get_version
 
 
 class IORedirector:
     """
     Context manager to redirect stdout and stderr to a file.
     Data is written to the file and the original streams.
     """
@@ -108,62 +108,48 @@
     msg += f'- Checkpoint Dir: {checkpoint_dir if checkpoint_dir else "N/A"}\n'
     msg += f'- Training on {dist.get_world_size()} GPUs\n'
     return msg
 
 
 def general_diagnostics() -> str:
     msg = '* GENERAL:\n'
-    msg += f'      - argv: {sys.argv}\n'
-    msg += f'      - cwd: {Path.cwd()}\n'
+    msg += f'    - argv: {sys.argv}\n'
+    msg += f'    - cwd: {Path.cwd()}\n'
 
-    msg += f'      - host (root): {os.environ.get("HOSTNAME")}\n'
-    msg += f'      - user: {os.environ.get("USER")}\n'
-    msg += f'      - git-hash: {git_hash()}\n'
-    msg += f'      - conda-env: {os.environ.get("CONDA_DEFAULT_ENV", "N/A")}\n'
-    msg += f'      - sys-prefix: {sys.prefix}\n'
-    msg += f'      - backend: {dist.get_backend()}\n'
-    msg += f'      - cuda: {torch.cuda.is_available()}\n'
+    msg += f'    - host (root): {os.environ.get("HOSTNAME")}\n'
+    msg += f'    - user: {os.environ.get("USER")}\n'
+    msg += f'    - git-hash: {git_hash()}\n'
+    msg += f'    - conda-env: {os.environ.get("CONDA_DEFAULT_ENV", "N/A")}\n'
+    msg += f'    - sys-prefix: {sys.prefix}\n'
+    msg += f'    - backend: {dist.get_backend()}\n'
+    msg += f'    - cuda: {torch.cuda.is_available()}\n'
 
     if torch.cuda.is_available():
         msg += '* GPUs (root):\n'
         nvsmi = subprocess.run(['nvidia-smi', '-L'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT).stdout.decode()
         for line in nvsmi.splitlines():
-            msg += f'      - {line}\n'
+            msg += f'    - {line}\n'
 
     msg += '* VERSIONS:\n'
-    msg += f'      - python: {sys.version}\n'
-    msg += f'      - dmlcloud: {dmlcloud.__version__}\n'
-    msg += f'      - cuda: {torch.version.cuda}\n'
+    msg += f'    - python: {sys.version}\n'
+    msg += f'    - dmlcloud: {dmlcloud.__version__}\n'
+    msg += f'    - cuda: {torch.version.cuda}\n'
     try:
         msg += '      - ' + Path('/proc/driver/nvidia/version').read_text().splitlines()[0] + '\n'
     except (FileNotFoundError, IndexError):
         pass
 
-    msg += f'      - torch: {torch.__version__}\n'
-    if try_get_version('torchvision'):
-        msg += f'      - torchvision: {try_get_version("torchvision")}\n'
-    if try_get_version('torchtext'):
-        msg += f'      - torchtext: {try_get_version("torchtext")}\n'
-    if try_get_version('torchaudio'):
-        msg += f'      - torchaudio: {try_get_version("torchaudio")}\n'
-    if try_get_version('einops'):
-        msg += f'      - einops: {try_get_version("einops")}\n'
-    if try_get_version('numpy'):
-        msg += f'      - numpy: {try_get_version("numpy")}\n'
-    if try_get_version('pandas'):
-        msg += f'      - pandas: {try_get_version("pandas")}\n'
-    if try_get_version('xarray'):
-        msg += f'      - xarray: {try_get_version("xarray")}\n'
-    if try_get_version('sklearn'):
-        msg += f'      - sklearn: {try_get_version("sklearn")}\n'
+    for module_name in ML_MODULES:
+        if is_imported(module_name):
+            msg += f'    - {module_name}: {try_get_version(module_name)}\n'
 
     if 'SLURM_JOB_ID' in os.environ:
         msg += '* SLURM:\n'
-        msg += f'      - SLURM_JOB_ID = {slurm.slurm_job_id()}\n'
-        msg += f'      - SLURM_STEP_ID = {slurm.slurm_step_id()}\n'
-        msg += f'      - SLURM_STEP_NODELIST = {os.environ.get("SLURM_STEP_NODELIST")}\n'
-        msg += f'      - SLURM_TASKS_PER_NODE = {os.environ.get("SLURM_TASKS_PER_NODE")}\n'
-        msg += f'      - SLURM_STEP_GPUS = {os.environ.get("SLURM_STEP_GPUS")}\n'
-        msg += f'      - SLURM_GPUS_ON_NODE = {os.environ.get("SLURM_GPUS_ON_NODE")}\n'
-        msg += f'      - SLURM_CPUS_PER_TASK = {os.environ.get("SLURM_CPUS_PER_TASK")}'
+        msg += f'    - SLURM_JOB_ID = {slurm.slurm_job_id()}\n'
+        msg += f'    - SLURM_STEP_ID = {slurm.slurm_step_id()}\n'
+        msg += f'    - SLURM_STEP_NODELIST = {os.environ.get("SLURM_STEP_NODELIST")}\n'
+        msg += f'    - SLURM_TASKS_PER_NODE = {os.environ.get("SLURM_TASKS_PER_NODE")}\n'
+        msg += f'    - SLURM_STEP_GPUS = {os.environ.get("SLURM_STEP_GPUS")}\n'
+        msg += f'    - SLURM_GPUS_ON_NODE = {os.environ.get("SLURM_GPUS_ON_NODE")}\n'
+        msg += f'    - SLURM_CPUS_PER_TASK = {os.environ.get("SLURM_CPUS_PER_TASK")}'
 
     return msg
```

## dmlcloud/util/thirdparty.py

```diff
@@ -1,12 +1,30 @@
 import importlib
+import sys
 from types import ModuleType
 from typing import Optional
 
 
+ML_MODULES = [
+    'torch',
+    'torchvision',
+    'torchtext',
+    'torchaudio',
+    'einops',
+    'numpy',
+    'pandas',
+    'xarray',
+    'sklearn',
+]
+
+
+def is_imported(name: str) -> bool:
+    return name in sys.modules
+
+
 def try_import(name: str) -> Optional[ModuleType]:
     try:
         return importlib.import_module(name)
     except ImportError:
         return None
```

## dmlcloud/util/wandb.py

```diff
@@ -1,12 +1,30 @@
 import os
+import sys
+
+
+class WandbModuleWrapper:
+    def __getattr__(self, name):
+        import wandb
+
+        return getattr(wandb, name)
+
+    def __setattr__(self, name, value):
+        import wandb
+
+        setattr(wandb, name, value)
+
+
+wandb = WandbModuleWrapper()
 
 
 def wandb_set_startup_timeout(seconds: int):
     assert isinstance(seconds, int)
     os.environ['WANDB__SERVICE_WAIT'] = f'{seconds}'
 
 
-def wandb_is_initialized():
-    import wandb
+def wandb_is_imported():
+    return 'wandb' in sys.modules
 
+
+def wandb_is_initialized():
     return wandb.run is not None
```

## Comparing `dmlcloud-0.3.1.dist-info/LICENSE` & `dmlcloud-0.3.3.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `dmlcloud-0.3.1.dist-info/METADATA` & `dmlcloud-0.3.3.dist-info/METADATA`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: dmlcloud
-Version: 0.3.1
+Version: 0.3.3
 Summary: Distributed torch training using horovod and slurm
 Author: Sebastian Hoffmann
 License: BSD 3-Clause License
         
         Copyright (c) 2023, Sebastian Hoffmann
         
         Redistribution and use in source and binary forms, with or without
@@ -47,15 +47,25 @@
 Requires-Dist: numpy
 Requires-Dist: xarray
 Requires-Dist: progress-table <1.0.0,>=0.1.20
 Requires-Dist: omegaconf
 
 # dmlcloud
 [![](https://img.shields.io/pypi/v/dmlcloud)](https://pypi.org/project/dmlcloud/)
-[![](https://img.shields.io/github/actions/workflow/status/sehoffmann/dmlcloud/run_tests.yml?logo=github)](https://github.com/sehoffmann/dmlcloud/actions/workflows/run_tests.yml)
+[![](https://img.shields.io/github/actions/workflow/status/sehoffmann/dmlcloud/run_tests.yml?label=tests&logo=github)](https://github.com/sehoffmann/dmlcloud/actions/workflows/run_tests.yml)
 [![](https://img.shields.io/github/actions/workflow/status/sehoffmann/dmlcloud/run_linting.yml?label=lint&logo=github)](https://github.com/sehoffmann/dmlcloud/actions/workflows/run_linting.yml)
 
-Flexibel, easy-to-use, opinionated
+*Flexibel, easy-to-use, opinionated*
 
-**dmlcloud** is a library for distributed training of deep learning models with torch. Its main aim is to do all these tiny little tedious things that everybody just copy pastes over and over again, while still giving you full control over the training loop and maximum flexibility.
+*dmlcloud* is a library for **distributed training** of deep learning models with *torch*. Unlike other similar frameworks, dmcloud adds as little additional complexity and abstraction as possible. It is tailored towards a carefully selected set of libraries and workflows.
 
-Unlike other similar frameworks, such as *lightning*, dmcloud tries to add as little additional complexity and abstraction as possible. Instead, it is tailored towards a careful selected set of libraries and workflows and sticks with them.
+## Installation
+```
+pip install dmlcloud
+```
+
+## Why dmlcloud?
+- Easy initialization of `torch.distributed` (supports *slurm* and *MPI*).
+- Simple, yet powerful, API. No unnecessary abstractions and complications.
+- Checkpointing and metric tracking (distributed)
+- Extensive logging and diagnostics out-of-the-box. Greatly improve reproducability and traceability.
+- A wealth of useful utility functions required for distributed training (e.g. for data set sharding)
```

## Comparing `dmlcloud-0.3.1.dist-info/RECORD` & `dmlcloud-0.3.3.dist-info/RECORD`

 * *Files 15% similar despite different names*

```diff
@@ -1,21 +1,21 @@
-dmlcloud/__init__.py,sha256=weHcBKNMbphE5NOT-XmTn5y7VdTE5FI6xwJ7ALEhlho,36
-dmlcloud/checkpoint.py,sha256=avStx_a0vHpEVlV7kfV9sc7I6yLqdUPk-OnkR3Lg_CY,3116
-dmlcloud/metrics.py,sha256=MEJpGKsQ_OZh4DYx1frp_QGNMr2seD0WzXDFTrp1PYs,10041
-dmlcloud/pipeline.py,sha256=xYytr2B_-qUJgvwh93BtF8VomJiYX5n0vklpEmevCPA,9474
-dmlcloud/stage.py,sha256=_ds6x2XD4tys9viwXtjA9OCDm_KiCwvI_MUA7kPGYV8,8968
+dmlcloud/__init__.py,sha256=3QwwB_-CILTMw6W2sugTNmDxivDn57VyO3r4K0m6vkw,36
+dmlcloud/checkpoint.py,sha256=5Iwyd8gQ4dYictdf_1be9LBA9MnXxB2b_VwadKH2DWw,3122
+dmlcloud/metrics.py,sha256=jdxZoRlj45aepFli5o7SkxXsbRBf2D32gycYHolUCrg,10064
+dmlcloud/pipeline.py,sha256=SGfo4D9KsxoG7PGcYt8XnanxYgCXNV2tTRtEm3BSKOg,9951
+dmlcloud/stage.py,sha256=TBtjnhKThHzeZ5J5BEK9mS6osMJZMF22NoGR_S3qCXc,10042
 dmlcloud/util/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 dmlcloud/util/argparse.py,sha256=4ViCh9QIlRxTK0okyf_5uo2ZW5IVkb5CMwoZzRM60dQ,947
-dmlcloud/util/data.py,sha256=lneNdD12c8M8xNrMRM_MBXcv_uMIkOdiARGEA16Qi2Y,3022
-dmlcloud/util/distributed.py,sha256=gVWNLqc4QeXPqke7qTJWifW496gF70S3fqs3auTVwqY,4325
+dmlcloud/util/data.py,sha256=A8SV3at8EGVl8LyKs179Qs8sZfVQdti5aOl75tzUyBE,7370
+dmlcloud/util/distributed.py,sha256=iYp2z1Z0G952zo6SmzevGYs7m4TzTY-2jhB2EnyE9zs,4532
 dmlcloud/util/git.py,sha256=noqKYwPDL1GMmeNoyNj3PmccgkO-p9z4VPS9Xfpe6hs,419
-dmlcloud/util/logging.py,sha256=o_76eORODZscGRe0QWX7wtASti8l7dthYOZtifKoFyc,5522
+dmlcloud/util/logging.py,sha256=BS_ZCcR-f4fL_OavHfh_GuFHaTMAO28mRiPpQdv8ZKc,4784
 dmlcloud/util/project.py,sha256=ioTDhWS5cFYVy1yJ_emVu-35xJuPOcpzlPym3mSyWmg,2269
 dmlcloud/util/slurm.py,sha256=wvXlLHP28HSVNFI1Rm6LG3nNEOMWZtdO-ASXfJ7L6VI,203
 dmlcloud/util/tcp.py,sha256=p4QCsTwOTpFHnfCwWVhbsoJnw8ocUudphXRbB44xSL4,363
-dmlcloud/util/thirdparty.py,sha256=vpwNYwq1XDH2QUAN1FiXtpH8lwKOuhJw_YqyDjbO1GI,402
-dmlcloud/util/wandb.py,sha256=LRZHI92au54dSr1FXiX-PdI-ZFPgvjKearF2uBEuPjk,227
-dmlcloud-0.3.1.dist-info/LICENSE,sha256=fV7TAUQuIbimvvzPTqsiDzhEv8hi7noPfkj9PK_PS8w,1505
-dmlcloud-0.3.1.dist-info/METADATA,sha256=ybEmCvpMx38o3FXuUrblFoAaa5T7aFAJLdNH8ANHPqM,3570
-dmlcloud-0.3.1.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-dmlcloud-0.3.1.dist-info/top_level.txt,sha256=NqA4DPAL8E5KduvXLklimZd36-pkmWF65V9RM5S69Zc,9
-dmlcloud-0.3.1.dist-info/RECORD,,
+dmlcloud/util/thirdparty.py,sha256=SNTEIusoAzoetXszCRwSYTHw9Z8f9qLm0IfSATWMVo4,638
+dmlcloud/util/wandb.py,sha256=3npD6Ud6BAbhwalnWzQPMUrVgWvn2S3re7sK1SfTJIs,530
+dmlcloud-0.3.3.dist-info/LICENSE,sha256=fV7TAUQuIbimvvzPTqsiDzhEv8hi7noPfkj9PK_PS8w,1505
+dmlcloud-0.3.3.dist-info/METADATA,sha256=S4ZCx4tzysoayHlQovJqT57rjmPhxXPXxkCJAE7wXxU,3803
+dmlcloud-0.3.3.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+dmlcloud-0.3.3.dist-info/top_level.txt,sha256=NqA4DPAL8E5KduvXLklimZd36-pkmWF65V9RM5S69Zc,9
+dmlcloud-0.3.3.dist-info/RECORD,,
```

