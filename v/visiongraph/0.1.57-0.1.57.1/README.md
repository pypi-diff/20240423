# Comparing `tmp/visiongraph-0.1.57-py3-none-any.whl.zip` & `tmp/visiongraph-0.1.57.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,291 +1,291 @@
-Zip file size: 263131 bytes, number of entries: 289
--rw-r--r--  2.0 unx     1758 b- defN 24-Apr-17 14:36 visiongraph/AsyncGraphNode.py
--rw-r--r--  2.0 unx     2806 b- defN 24-Apr-17 14:36 visiongraph/BaseGraph.py
--rw-r--r--  2.0 unx      678 b- defN 24-Apr-17 14:36 visiongraph/GraphNode.py
--rw-r--r--  2.0 unx      276 b- defN 24-Apr-17 14:36 visiongraph/Processable.py
--rw-r--r--  2.0 unx     2094 b- defN 24-Apr-17 14:36 visiongraph/VisionGraph.py
--rw-r--r--  2.0 unx     1898 b- defN 24-Apr-17 14:36 visiongraph/VisionGraphBuilder.py
--rw-r--r--  2.0 unx    25589 b- defN 24-Apr-17 14:36 visiongraph/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/cache/__init__.py
--rw-r--r--  2.0 unx      368 b- defN 24-Apr-17 14:36 visiongraph/data/Asset.py
--rw-r--r--  2.0 unx      376 b- defN 24-Apr-17 14:36 visiongraph/data/LocalAsset.py
--rw-r--r--  2.0 unx     1099 b- defN 24-Apr-17 14:36 visiongraph/data/RepositoryAsset.py
--rw-r--r--  2.0 unx      386 b- defN 24-Apr-17 14:36 visiongraph/data/__init__.py
--rw-r--r--  2.0 unx     2568 b- defN 24-Apr-17 14:36 visiongraph/data/labels/COCO.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/data/labels/__init__.py
--rw-r--r--  2.0 unx      172 b- defN 24-Apr-17 14:36 visiongraph/dsp/BaseFilterNumpy.py
--rw-r--r--  2.0 unx     2421 b- defN 24-Apr-17 14:36 visiongraph/dsp/LandmarkSmoothFilter.py
--rw-r--r--  2.0 unx     1487 b- defN 24-Apr-17 14:36 visiongraph/dsp/OneEuroFilter.py
--rw-r--r--  2.0 unx     1651 b- defN 24-Apr-17 14:36 visiongraph/dsp/OneEuroFilterNumba.py
--rw-r--r--  2.0 unx     2642 b- defN 24-Apr-17 14:36 visiongraph/dsp/OneEuroFilterNumpy.py
--rw-r--r--  2.0 unx     1037 b- defN 24-Apr-17 14:36 visiongraph/dsp/VectorNumpySmoothFilter.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/dsp/__init__.py
--rw-r--r--  2.0 unx      770 b- defN 24-Apr-17 14:36 visiongraph/estimator/BaseClassifier.py
--rw-r--r--  2.0 unx      397 b- defN 24-Apr-17 14:36 visiongraph/estimator/BaseEstimator.py
--rw-r--r--  2.0 unx     5398 b- defN 24-Apr-17 14:36 visiongraph/estimator/BaseVisionEngine.py
--rw-r--r--  2.0 unx     1100 b- defN 24-Apr-17 14:36 visiongraph/estimator/ChainEstimator.py
--rw-r--r--  2.0 unx      403 b- defN 24-Apr-17 14:36 visiongraph/estimator/ScoreThresholdEstimator.py
--rw-r--r--  2.0 unx      868 b- defN 24-Apr-17 14:36 visiongraph/estimator/VisionClassifier.py
--rw-r--r--  2.0 unx      410 b- defN 24-Apr-17 14:36 visiongraph/estimator/VisionEstimator.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/__init__.py
--rw-r--r--  2.0 unx     2448 b- defN 24-Apr-17 14:36 visiongraph/estimator/calculator/UndistortionCalculator.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/calculator/__init__.py
--rw-r--r--  2.0 unx     1231 b- defN 24-Apr-17 14:36 visiongraph/estimator/embedding/LandmarkEmbedder.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/embedding/__init__.py
--rw-r--r--  2.0 unx     3367 b- defN 24-Apr-17 14:36 visiongraph/estimator/embedding/knn/BaseKNNClassifier.py
--rw-r--r--  2.0 unx     2182 b- defN 24-Apr-17 14:36 visiongraph/estimator/embedding/knn/FaissKNNClassifier.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/embedding/knn/__init__.py
--rw-r--r--  2.0 unx     1664 b- defN 24-Apr-17 14:36 visiongraph/estimator/engine/InferenceEngineFactory.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/engine/__init__.py
--rw-r--r--  2.0 unx      529 b- defN 24-Apr-17 14:36 visiongraph/estimator/inpaint/BaseInpainter.py
--rw-r--r--  2.0 unx     2140 b- defN 24-Apr-17 14:36 visiongraph/estimator/inpaint/GMCNNInpainter.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/inpaint/__init__.py
--rw-r--r--  2.0 unx     4104 b- defN 24-Apr-17 14:36 visiongraph/estimator/onnx/ONNXVisionEngine.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/onnx/__init__.py
--rw-r--r--  2.0 unx     3298 b- defN 24-Apr-17 14:36 visiongraph/estimator/openvino/OpenVinoEngine.py
--rw-r--r--  2.0 unx     2516 b- defN 24-Apr-17 14:36 visiongraph/estimator/openvino/OpenVinoObjectDetector.py
--rw-r--r--  2.0 unx     2967 b- defN 24-Apr-17 14:36 visiongraph/estimator/openvino/OpenVinoPoseEstimator.py
--rw-r--r--  2.0 unx      641 b- defN 24-Apr-17 14:36 visiongraph/estimator/openvino/SyncInferencePipeline.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/openvino/__init__.py
--rw-r--r--  2.0 unx     1877 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/CenterNetDetector.py
--rw-r--r--  2.0 unx     3270 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/CrowdHumanDetector.py
--rw-r--r--  2.0 unx     1908 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/DETRDetector.py
--rw-r--r--  2.0 unx      544 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/InstanceSegmentationEstimator.py
--rw-r--r--  2.0 unx      523 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/LandmarkEstimator.py
--rw-r--r--  2.0 unx      524 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/ObjectDetector.py
--rw-r--r--  2.0 unx     1279 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/RoiEstimator.py
--rw-r--r--  2.0 unx     3251 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/SSDDetector.py
--rw-r--r--  2.0 unx     2082 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/SlidingWindowEstimator.py
--rw-r--r--  2.0 unx     1436 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/SpatialCascadeEstimator.py
--rw-r--r--  2.0 unx     3823 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/UltralyticsYOLODetector.py
--rw-r--r--  2.0 unx     3300 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/YOLODetector.py
--rw-r--r--  2.0 unx     2904 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/YOLOXE2EDetector.py
--rw-r--r--  2.0 unx     1635 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/YOLOv5Detector.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/__init__.py
--rw-r--r--  2.0 unx     3441 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/camera/ArUcoCameraPoseEstimator.py
--rw-r--r--  2.0 unx      850 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/camera/BoardCameraCalibrator.py
--rw-r--r--  2.0 unx     4276 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/camera/ChArUcoCalibrator.py
--rw-r--r--  2.0 unx     3145 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/camera/ChessboardCalibrator.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/camera/__init__.py
--rw-r--r--  2.0 unx     1127 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/AdasFaceDetector.py
--rw-r--r--  2.0 unx      512 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/FaceDetector.py
--rw-r--r--  2.0 unx     3805 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/OpenVinoFaceDetector.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/__init__.py
--rw-r--r--  2.0 unx     1760 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/emotion/AffectNetEmotionClassifier.py
--rw-r--r--  2.0 unx     2038 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/emotion/FERPlusEmotionClassifier.py
--rw-r--r--  2.0 unx      450 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/emotion/FaceEmotionEstimator.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/emotion/__init__.py
--rw-r--r--  2.0 unx     1595 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/eye/EyeOpenClosedEstimator.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/eye/__init__.py
--rw-r--r--  2.0 unx      609 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/landmark/FaceLandmarkEstimator.py
--rw-r--r--  2.0 unx     2646 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/landmark/IrisDistanceCalculator.py
--rw-r--r--  2.0 unx     2204 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/landmark/MediaPipeFaceDetector.py
--rw-r--r--  2.0 unx     2324 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/landmark/MediaPipeFaceMeshEstimator.py
--rw-r--r--  2.0 unx     1811 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/landmark/RegressionLandmarkEstimator.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/landmark/__init__.py
--rw-r--r--  2.0 unx     1280 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/pose/AdasHeadPoseEstimator.py
--rw-r--r--  2.0 unx      315 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/pose/HeadPoseEstimator.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/pose/__init__.py
--rw-r--r--  2.0 unx     4265 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/recognition/FaceRecognitionEstimator.py
--rw-r--r--  2.0 unx     2803 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/recognition/FaceReidentificationEstimator.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/recognition/__init__.py
--rw-r--r--  2.0 unx      517 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/hand/HandDetector.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/hand/__init__.py
--rw-r--r--  2.0 unx      608 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/hand/landmark/HandLandmarkEstimator.py
--rw-r--r--  2.0 unx     2778 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/hand/landmark/MediaPipeHandEstimator.py
--rw-r--r--  2.0 unx     2330 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/hand/landmark/OpenPoseHandEstimator.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/hand/landmark/__init__.py
--rw-r--r--  2.0 unx     2219 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/pose/AEPoseEstimator.py
--rw-r--r--  2.0 unx     5328 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/pose/EfficientPoseEstimator.py
--rw-r--r--  2.0 unx     7134 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/pose/KAPAOPoseEstimator.py
--rw-r--r--  2.0 unx     3869 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/pose/LiteHRNetEstimator.py
--rw-r--r--  2.0 unx     2834 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/pose/LitePoseEstimator.py
--rw-r--r--  2.0 unx     4302 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/pose/MediaPipeHolisticEstimator.py
--rw-r--r--  2.0 unx     3069 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/pose/MediaPipePoseEstimator.py
--rw-r--r--  2.0 unx     6641 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/pose/MobileHumanPoseEstimator.py
--rw-r--r--  2.0 unx     9862 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/pose/MobileNetV2PoseEstimator.py
--rw-r--r--  2.0 unx     4304 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/pose/MoveNetPoseEstimator.py
--rw-r--r--  2.0 unx     1876 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/pose/OpenPoseEstimator.py
--rw-r--r--  2.0 unx      518 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/pose/PoseEstimator.py
--rw-r--r--  2.0 unx     2347 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/pose/TopDownPoseEstimator.py
--rw-r--r--  2.0 unx     4451 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/pose/YOLOv8PoseEstimator.py
--rw-r--r--  2.0 unx     6637 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/pose/__init__.py
--rw-r--r--  2.0 unx     3165 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/segmentation/MODNetEstimator.py
--rw-r--r--  2.0 unx     8687 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/segmentation/MaskRCNNEstimator.py
--rw-r--r--  2.0 unx     1989 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/segmentation/MediaPipeSelfieSegmentation.py
--rw-r--r--  2.0 unx     3287 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/segmentation/YolactEstimator.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/segmentation/__init__.py
--rw-r--r--  2.0 unx     1627 b- defN 24-Apr-17 14:36 visiongraph/estimator/translation/DeblurGANv2.py
--rw-r--r--  2.0 unx      314 b- defN 24-Apr-17 14:36 visiongraph/estimator/translation/DepthEstimator.py
--rw-r--r--  2.0 unx     3887 b- defN 24-Apr-17 14:36 visiongraph/estimator/translation/MBLLENEstimator.py
--rw-r--r--  2.0 unx     2898 b- defN 24-Apr-17 14:36 visiongraph/estimator/translation/MidasDepthEstimator.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/translation/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/external/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/external/intel/__init__.py
--rw-r--r--  2.0 unx     4342 b- defN 24-Apr-17 14:36 visiongraph/external/intel/performance_metrics.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/external/intel/adapters/__init__.py
--rw-r--r--  2.0 unx     5682 b- defN 24-Apr-17 14:36 visiongraph/external/intel/adapters/inference_adapter.py
--rw-r--r--  2.0 unx     5151 b- defN 24-Apr-17 14:36 visiongraph/external/intel/adapters/model_adapter.py
--rw-r--r--  2.0 unx     8056 b- defN 24-Apr-17 14:36 visiongraph/external/intel/adapters/openvino_adapter.py
--rw-r--r--  2.0 unx     6585 b- defN 24-Apr-17 14:36 visiongraph/external/intel/adapters/ovms_adapter.py
--rw-r--r--  2.0 unx     2794 b- defN 24-Apr-17 14:36 visiongraph/external/intel/adapters/utils.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/external/intel/models/__init__.py
--rw-r--r--  2.0 unx     7442 b- defN 24-Apr-17 14:36 visiongraph/external/intel/models/centernet.py
--rw-r--r--  2.0 unx     5700 b- defN 24-Apr-17 14:36 visiongraph/external/intel/models/detection_model.py
--rw-r--r--  2.0 unx     2992 b- defN 24-Apr-17 14:36 visiongraph/external/intel/models/detr.py
--rw-r--r--  2.0 unx    14421 b- defN 24-Apr-17 14:36 visiongraph/external/intel/models/hpe_associative_embedding.py
--rw-r--r--  2.0 unx     7247 b- defN 24-Apr-17 14:36 visiongraph/external/intel/models/image_model.py
--rw-r--r--  2.0 unx    12558 b- defN 24-Apr-17 14:36 visiongraph/external/intel/models/model.py
--rw-r--r--  2.0 unx    17789 b- defN 24-Apr-17 14:36 visiongraph/external/intel/models/open_pose.py
--rw-r--r--  2.0 unx     5874 b- defN 24-Apr-17 14:36 visiongraph/external/intel/models/ssd.py
--rw-r--r--  2.0 unx     5805 b- defN 24-Apr-17 14:36 visiongraph/external/intel/models/types.py
--rw-r--r--  2.0 unx     7260 b- defN 24-Apr-17 14:36 visiongraph/external/intel/models/utils.py
--rw-r--r--  2.0 unx    21407 b- defN 24-Apr-17 14:36 visiongraph/external/intel/models/yolo.py
--rw-r--r--  2.0 unx      154 b- defN 24-Apr-17 14:36 visiongraph/external/intel/pipelines/__init__.py
--rw-r--r--  2.0 unx     5407 b- defN 24-Apr-17 14:36 visiongraph/external/intel/pipelines/async_pipeline.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/external/midas/__init__.py
--rw-r--r--  2.0 unx     7868 b- defN 24-Apr-17 14:36 visiongraph/external/midas/transforms.py
--rw-r--r--  2.0 unx      193 b- defN 24-Apr-17 14:36 visiongraph/external/motpy/__init__.py
--rw-r--r--  2.0 unx     1816 b- defN 24-Apr-17 14:36 visiongraph/external/motpy/core.py
--rw-r--r--  2.0 unx      333 b- defN 24-Apr-17 14:36 visiongraph/external/motpy/detector.py
--rw-r--r--  2.0 unx     1147 b- defN 24-Apr-17 14:36 visiongraph/external/motpy/metrics.py
--rw-r--r--  2.0 unx     5402 b- defN 24-Apr-17 14:36 visiongraph/external/motpy/model.py
--rw-r--r--  2.0 unx     3576 b- defN 24-Apr-17 14:36 visiongraph/external/motpy/testing.py
--rw-r--r--  2.0 unx     2647 b- defN 24-Apr-17 14:36 visiongraph/external/motpy/testing_viz.py
--rw-r--r--  2.0 unx    16512 b- defN 24-Apr-17 14:36 visiongraph/external/motpy/tracker.py
--rw-r--r--  2.0 unx      652 b- defN 24-Apr-17 14:36 visiongraph/external/motpy/utils.py
--rw-r--r--  2.0 unx     5312 b- defN 24-Apr-17 14:36 visiongraph/external/motrackers/Track.py
--rw-r--r--  2.0 unx     7486 b- defN 24-Apr-17 14:36 visiongraph/external/motrackers/Tracker.py
--rw-r--r--  2.0 unx      184 b- defN 24-Apr-17 14:36 visiongraph/external/motrackers/__init__.py
--rw-r--r--  2.0 unx       83 b- defN 24-Apr-17 14:36 visiongraph/external/motrackers/utils/__init__.py
--rw-r--r--  2.0 unx     8978 b- defN 24-Apr-17 14:36 visiongraph/external/motrackers/utils/misc.py
--rw-r--r--  2.0 unx    17144 b- defN 24-Apr-17 14:36 visiongraph/input/AzureKinectInput.py
--rw-r--r--  2.0 unx     3855 b- defN 24-Apr-17 14:36 visiongraph/input/BaseCamera.py
--rw-r--r--  2.0 unx     3338 b- defN 24-Apr-17 14:36 visiongraph/input/BaseDepthCamera.py
--rw-r--r--  2.0 unx     1309 b- defN 24-Apr-17 14:36 visiongraph/input/BaseDepthInput.py
--rw-r--r--  2.0 unx     4232 b- defN 24-Apr-17 14:36 visiongraph/input/BaseInput.py
--rw-r--r--  2.0 unx     1721 b- defN 24-Apr-17 14:36 visiongraph/input/CamGearInput.py
--rw-r--r--  2.0 unx     9093 b- defN 24-Apr-17 14:36 visiongraph/input/DepthAIBaseInput.py
--rw-r--r--  2.0 unx     1493 b- defN 24-Apr-17 14:36 visiongraph/input/ImageInput.py
--rw-r--r--  2.0 unx      295 b- defN 24-Apr-17 14:36 visiongraph/input/Oak1Input.py
--rw-r--r--  2.0 unx     7425 b- defN 24-Apr-17 14:36 visiongraph/input/OakDInput.py
--rw-r--r--  2.0 unx    19468 b- defN 24-Apr-17 14:36 visiongraph/input/RealSenseInput.py
--rw-r--r--  2.0 unx     5032 b- defN 24-Apr-17 14:36 visiongraph/input/VideoCaptureInput.py
--rw-r--r--  2.0 unx     4814 b- defN 24-Apr-17 14:36 visiongraph/input/ZEDInput.py
--rw-r--r--  2.0 unx     1744 b- defN 24-Apr-17 14:36 visiongraph/input/__init__.py
--rw-r--r--  2.0 unx     1887 b- defN 24-Apr-17 14:36 visiongraph/model/CameraIntrinsics.py
--rw-r--r--  2.0 unx      101 b- defN 24-Apr-17 14:36 visiongraph/model/CameraStreamType.py
--rw-r--r--  2.0 unx      549 b- defN 24-Apr-17 14:36 visiongraph/model/DepthBuffer.py
--rw-r--r--  2.0 unx      266 b- defN 24-Apr-17 14:36 visiongraph/model/VisionEngineModelLayer.py
--rw-r--r--  2.0 unx      753 b- defN 24-Apr-17 14:36 visiongraph/model/VisionEngineOutput.py
--rw-r--r--  2.0 unx      135 b- defN 24-Apr-17 14:36 visiongraph/model/_ImportStub.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/model/__init__.py
--rw-r--r--  2.0 unx     4229 b- defN 24-Apr-17 14:36 visiongraph/model/geometry/BoundingBox2D.py
--rw-r--r--  2.0 unx      815 b- defN 24-Apr-17 14:36 visiongraph/model/geometry/Size2D.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/model/geometry/__init__.py
--rw-r--r--  2.0 unx      585 b- defN 24-Apr-17 14:36 visiongraph/model/parameter/ArgumentConfigurable.py
--rw-r--r--  2.0 unx      186 b- defN 24-Apr-17 14:36 visiongraph/model/parameter/NamedParameter.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/model/parameter/__init__.py
--rw-r--r--  2.0 unx      399 b- defN 24-Apr-17 14:36 visiongraph/model/tracker/Trackable.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/model/tracker/__init__.py
--rw-r--r--  2.0 unx       78 b- defN 24-Apr-17 14:36 visiongraph/model/types/InputShapeOrder.py
--rw-r--r--  2.0 unx       99 b- defN 24-Apr-17 14:36 visiongraph/model/types/MediaPipePoseModelComplexity.py
--rw-r--r--  2.0 unx      440 b- defN 24-Apr-17 14:36 visiongraph/model/types/ModelPrecision.py
--rw-r--r--  2.0 unx      199 b- defN 24-Apr-17 14:36 visiongraph/model/types/RealSenseColorScheme.py
--rw-r--r--  2.0 unx      204 b- defN 24-Apr-17 14:36 visiongraph/model/types/RealSenseFilter.py
--rw-r--r--  2.0 unx     1180 b- defN 24-Apr-17 14:36 visiongraph/model/types/VideoCaptureBackend.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/model/types/__init__.py
--rw-r--r--  2.0 unx      905 b- defN 24-Apr-17 14:36 visiongraph/node/ApplyNode.py
--rw-r--r--  2.0 unx      753 b- defN 24-Apr-17 14:36 visiongraph/node/BreakpointNode.py
--rw-r--r--  2.0 unx      815 b- defN 24-Apr-17 14:36 visiongraph/node/CustomNode.py
--rw-r--r--  2.0 unx      903 b- defN 24-Apr-17 14:36 visiongraph/node/ExtractNode.py
--rw-r--r--  2.0 unx      525 b- defN 24-Apr-17 14:36 visiongraph/node/PassThroughNode.py
--rw-r--r--  2.0 unx      825 b- defN 24-Apr-17 14:36 visiongraph/node/SequenceNode.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/node/__init__.py
--rw-r--r--  2.0 unx     1676 b- defN 24-Apr-17 14:36 visiongraph/output/ImagePreview.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/output/__init__.py
--rw-r--r--  2.0 unx      965 b- defN 24-Apr-17 14:36 visiongraph/output/fbs/FrameBufferSharingServer.py
--rw-r--r--  2.0 unx     1427 b- defN 24-Apr-17 14:36 visiongraph/output/fbs/SpoutServer.py
--rw-r--r--  2.0 unx     1623 b- defN 24-Apr-17 14:36 visiongraph/output/fbs/SyphonServer.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/output/fbs/__init__.py
--rw-r--r--  2.0 unx      980 b- defN 24-Apr-17 14:36 visiongraph/recorder/AsyncFrameSetRecorder.py
--rw-r--r--  2.0 unx     1108 b- defN 24-Apr-17 14:36 visiongraph/recorder/BaseFrameRecorder.py
--rw-r--r--  2.0 unx     1143 b- defN 24-Apr-17 14:36 visiongraph/recorder/CV2VideoRecorder.py
--rw-r--r--  2.0 unx     1016 b- defN 24-Apr-17 14:36 visiongraph/recorder/FrameSetRecorder.py
--rw-r--r--  2.0 unx      905 b- defN 24-Apr-17 14:36 visiongraph/recorder/MoviePyVideoRecorder.py
--rw-r--r--  2.0 unx     1377 b- defN 24-Apr-17 14:36 visiongraph/recorder/VidGearVideoRecorder.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/recorder/__init__.py
--rw-r--r--  2.0 unx      564 b- defN 24-Apr-17 14:36 visiongraph/result/ArUcoCameraPose.py
--rw-r--r--  2.0 unx     1521 b- defN 24-Apr-17 14:36 visiongraph/result/ArUcoMarkerDetection.py
--rw-r--r--  2.0 unx      167 b- defN 24-Apr-17 14:36 visiongraph/result/BaseResult.py
--rw-r--r--  2.0 unx      525 b- defN 24-Apr-17 14:36 visiongraph/result/CameraPoseResult.py
--rw-r--r--  2.0 unx      378 b- defN 24-Apr-17 14:36 visiongraph/result/ClassificationResult.py
--rw-r--r--  2.0 unx     1709 b- defN 24-Apr-17 14:36 visiongraph/result/DepthMap.py
--rw-r--r--  2.0 unx      413 b- defN 24-Apr-17 14:36 visiongraph/result/EmbeddingResult.py
--rw-r--r--  2.0 unx      474 b- defN 24-Apr-17 14:36 visiongraph/result/HeadPoseResult.py
--rw-r--r--  2.0 unx      333 b- defN 24-Apr-17 14:36 visiongraph/result/ImageResult.py
--rw-r--r--  2.0 unx      534 b- defN 24-Apr-17 14:36 visiongraph/result/LandmarkEmbeddingResult.py
--rw-r--r--  2.0 unx     1165 b- defN 24-Apr-17 14:36 visiongraph/result/ResultAnnotator.py
--rw-r--r--  2.0 unx      483 b- defN 24-Apr-17 14:36 visiongraph/result/ResultDict.py
--rw-r--r--  2.0 unx      466 b- defN 24-Apr-17 14:36 visiongraph/result/ResultList.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/result/__init__.py
--rw-r--r--  2.0 unx      779 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/CrowdHumanResult.py
--rw-r--r--  2.0 unx     1354 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/InstanceSegmentationResult.py
--rw-r--r--  2.0 unx     3838 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/LandmarkDetectionResult.py
--rw-r--r--  2.0 unx     3475 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/ObjectDetectionResult.py
--rw-r--r--  2.0 unx      878 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/SpatialCascadeResult.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/__init__.py
--rw-r--r--  2.0 unx      924 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/face/BlazeFace.py
--rw-r--r--  2.0 unx     2194 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/face/BlazeFaceMesh.py
--rw-r--r--  2.0 unx      702 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/face/EmotionClassificationResult.py
--rw-r--r--  2.0 unx      694 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/face/EyeOpenClosedResult.py
--rw-r--r--  2.0 unx      409 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/face/FaceDetectionResult.py
--rw-r--r--  2.0 unx      881 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/face/FaceLandmarkResult.py
--rw-r--r--  2.0 unx     1390 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/face/IrisDistanceResult.py
--rw-r--r--  2.0 unx      868 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/face/RegressionFace.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/face/__init__.py
--rw-r--r--  2.0 unx     2648 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/hand/BlazeHand.py
--rw-r--r--  2.0 unx      409 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/hand/HandDetectionResult.py
--rw-r--r--  2.0 unx     2590 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/hand/HandLandmarkResult.py
--rw-r--r--  2.0 unx       88 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/hand/Handedness.py
--rw-r--r--  2.0 unx      106 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/hand/OpenPoseHand.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/hand/__init__.py
--rw-r--r--  2.0 unx     3435 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/pose/BlazePose.py
--rw-r--r--  2.0 unx     1266 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/pose/BlazePoseSegmentation.py
--rw-r--r--  2.0 unx     2826 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/pose/COCOOpenPose.py
--rw-r--r--  2.0 unx     2680 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/pose/COCOPose.py
--rw-r--r--  2.0 unx     2337 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/pose/EfficientPose.py
--rw-r--r--  2.0 unx     1726 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/pose/HolisticPose.py
--rw-r--r--  2.0 unx     2738 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/pose/MobileHumanPose.py
--rw-r--r--  2.0 unx     2963 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/pose/PoseLandmarkResult.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/pose/__init__.py
--rw-r--r--  2.0 unx      492 b- defN 24-Apr-17 14:36 visiongraph/tracker/BaseObjectDetectionTracker.py
--rw-r--r--  2.0 unx     1908 b- defN 24-Apr-17 14:36 visiongraph/tracker/CentroidTracker.py
--rw-r--r--  2.0 unx     4906 b- defN 24-Apr-17 14:36 visiongraph/tracker/FlateTracker.py
--rw-r--r--  2.0 unx     2705 b- defN 24-Apr-17 14:36 visiongraph/tracker/MotpyTracker.py
--rw-r--r--  2.0 unx     3304 b- defN 24-Apr-17 14:36 visiongraph/tracker/ObjectAssignmentSolver.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/tracker/__init__.py
--rw-r--r--  2.0 unx     3105 b- defN 24-Apr-17 14:36 visiongraph/util/ArgUtils.py
--rw-r--r--  2.0 unx     1009 b- defN 24-Apr-17 14:36 visiongraph/util/CodeUtils.py
--rw-r--r--  2.0 unx      222 b- defN 24-Apr-17 14:36 visiongraph/util/CollectionUtils.py
--rw-r--r--  2.0 unx      314 b- defN 24-Apr-17 14:36 visiongraph/util/CommonArgs.py
--rw-r--r--  2.0 unx     4659 b- defN 24-Apr-17 14:36 visiongraph/util/DrawingUtils.py
--rw-r--r--  2.0 unx     3926 b- defN 24-Apr-17 14:36 visiongraph/util/ImageUtils.py
--rw-r--r--  2.0 unx     2608 b- defN 24-Apr-17 14:36 visiongraph/util/LinalgUtils.py
--rw-r--r--  2.0 unx      539 b- defN 24-Apr-17 14:36 visiongraph/util/LoggingUtils.py
--rw-r--r--  2.0 unx     1752 b- defN 24-Apr-17 14:36 visiongraph/util/MathUtils.py
--rw-r--r--  2.0 unx      608 b- defN 24-Apr-17 14:36 visiongraph/util/MediaPipeUtils.py
--rw-r--r--  2.0 unx     2731 b- defN 24-Apr-17 14:36 visiongraph/util/NetworkUtils.py
--rw-r--r--  2.0 unx      195 b- defN 24-Apr-17 14:36 visiongraph/util/OSUtils.py
--rw-r--r--  2.0 unx      700 b- defN 24-Apr-17 14:36 visiongraph/util/OpenVinoUtils.py
--rw-r--r--  2.0 unx     5639 b- defN 24-Apr-17 14:36 visiongraph/util/PoseUtils.py
--rw-r--r--  2.0 unx     1608 b- defN 24-Apr-17 14:36 visiongraph/util/ResultUtils.py
--rw-r--r--  2.0 unx     2102 b- defN 24-Apr-17 14:36 visiongraph/util/TimeUtils.py
--rw-r--r--  2.0 unx     3643 b- defN 24-Apr-17 14:36 visiongraph/util/VectorUtils.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/util/__init__.py
--rw-r--r--  2.0 unx    11923 b- defN 24-Apr-17 14:36 visiongraph-0.1.57.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Apr-17 14:36 visiongraph-0.1.57.dist-info/WHEEL
--rw-r--r--  2.0 unx       20 b- defN 24-Apr-17 14:36 visiongraph-0.1.57.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       12 b- defN 24-Apr-17 14:36 visiongraph-0.1.57.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    29000 b- defN 24-Apr-17 14:36 visiongraph-0.1.57.dist-info/RECORD
-289 files, 714350 bytes uncompressed, 215551 bytes compressed:  69.8%
+Zip file size: 263259 bytes, number of entries: 289
+-rw-r--r--  2.0 unx     1758 b- defN 24-Apr-23 14:46 visiongraph/AsyncGraphNode.py
+-rw-r--r--  2.0 unx     2806 b- defN 24-Apr-23 14:46 visiongraph/BaseGraph.py
+-rw-r--r--  2.0 unx      678 b- defN 24-Apr-23 14:46 visiongraph/GraphNode.py
+-rw-r--r--  2.0 unx      276 b- defN 24-Apr-23 14:46 visiongraph/Processable.py
+-rw-r--r--  2.0 unx     2094 b- defN 24-Apr-23 14:46 visiongraph/VisionGraph.py
+-rw-r--r--  2.0 unx     1898 b- defN 24-Apr-23 14:46 visiongraph/VisionGraphBuilder.py
+-rw-r--r--  2.0 unx    25589 b- defN 24-Apr-23 14:46 visiongraph/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/cache/__init__.py
+-rw-r--r--  2.0 unx      368 b- defN 24-Apr-23 14:46 visiongraph/data/Asset.py
+-rw-r--r--  2.0 unx      376 b- defN 24-Apr-23 14:46 visiongraph/data/LocalAsset.py
+-rw-r--r--  2.0 unx     1099 b- defN 24-Apr-23 14:46 visiongraph/data/RepositoryAsset.py
+-rw-r--r--  2.0 unx      386 b- defN 24-Apr-23 14:46 visiongraph/data/__init__.py
+-rw-r--r--  2.0 unx     2568 b- defN 24-Apr-23 14:46 visiongraph/data/labels/COCO.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/data/labels/__init__.py
+-rw-r--r--  2.0 unx      172 b- defN 24-Apr-23 14:46 visiongraph/dsp/BaseFilterNumpy.py
+-rw-r--r--  2.0 unx     2421 b- defN 24-Apr-23 14:46 visiongraph/dsp/LandmarkSmoothFilter.py
+-rw-r--r--  2.0 unx     1487 b- defN 24-Apr-23 14:46 visiongraph/dsp/OneEuroFilter.py
+-rw-r--r--  2.0 unx     1651 b- defN 24-Apr-23 14:46 visiongraph/dsp/OneEuroFilterNumba.py
+-rw-r--r--  2.0 unx     2642 b- defN 24-Apr-23 14:46 visiongraph/dsp/OneEuroFilterNumpy.py
+-rw-r--r--  2.0 unx     1037 b- defN 24-Apr-23 14:46 visiongraph/dsp/VectorNumpySmoothFilter.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/dsp/__init__.py
+-rw-r--r--  2.0 unx      770 b- defN 24-Apr-23 14:46 visiongraph/estimator/BaseClassifier.py
+-rw-r--r--  2.0 unx      397 b- defN 24-Apr-23 14:46 visiongraph/estimator/BaseEstimator.py
+-rw-r--r--  2.0 unx     5398 b- defN 24-Apr-23 14:46 visiongraph/estimator/BaseVisionEngine.py
+-rw-r--r--  2.0 unx     1100 b- defN 24-Apr-23 14:46 visiongraph/estimator/ChainEstimator.py
+-rw-r--r--  2.0 unx      403 b- defN 24-Apr-23 14:46 visiongraph/estimator/ScoreThresholdEstimator.py
+-rw-r--r--  2.0 unx      868 b- defN 24-Apr-23 14:46 visiongraph/estimator/VisionClassifier.py
+-rw-r--r--  2.0 unx      410 b- defN 24-Apr-23 14:46 visiongraph/estimator/VisionEstimator.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/estimator/__init__.py
+-rw-r--r--  2.0 unx     2448 b- defN 24-Apr-23 14:46 visiongraph/estimator/calculator/UndistortionCalculator.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/estimator/calculator/__init__.py
+-rw-r--r--  2.0 unx     1231 b- defN 24-Apr-23 14:46 visiongraph/estimator/embedding/LandmarkEmbedder.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/estimator/embedding/__init__.py
+-rw-r--r--  2.0 unx     3367 b- defN 24-Apr-23 14:46 visiongraph/estimator/embedding/knn/BaseKNNClassifier.py
+-rw-r--r--  2.0 unx     2182 b- defN 24-Apr-23 14:46 visiongraph/estimator/embedding/knn/FaissKNNClassifier.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/estimator/embedding/knn/__init__.py
+-rw-r--r--  2.0 unx     1664 b- defN 24-Apr-23 14:46 visiongraph/estimator/engine/InferenceEngineFactory.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/estimator/engine/__init__.py
+-rw-r--r--  2.0 unx      529 b- defN 24-Apr-23 14:46 visiongraph/estimator/inpaint/BaseInpainter.py
+-rw-r--r--  2.0 unx     2140 b- defN 24-Apr-23 14:46 visiongraph/estimator/inpaint/GMCNNInpainter.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/estimator/inpaint/__init__.py
+-rw-r--r--  2.0 unx     4104 b- defN 24-Apr-23 14:46 visiongraph/estimator/onnx/ONNXVisionEngine.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/estimator/onnx/__init__.py
+-rw-r--r--  2.0 unx     3298 b- defN 24-Apr-23 14:46 visiongraph/estimator/openvino/OpenVinoEngine.py
+-rw-r--r--  2.0 unx     2516 b- defN 24-Apr-23 14:46 visiongraph/estimator/openvino/OpenVinoObjectDetector.py
+-rw-r--r--  2.0 unx     2967 b- defN 24-Apr-23 14:46 visiongraph/estimator/openvino/OpenVinoPoseEstimator.py
+-rw-r--r--  2.0 unx      641 b- defN 24-Apr-23 14:46 visiongraph/estimator/openvino/SyncInferencePipeline.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/estimator/openvino/__init__.py
+-rw-r--r--  2.0 unx     1877 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/CenterNetDetector.py
+-rw-r--r--  2.0 unx     3270 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/CrowdHumanDetector.py
+-rw-r--r--  2.0 unx     1908 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/DETRDetector.py
+-rw-r--r--  2.0 unx      544 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/InstanceSegmentationEstimator.py
+-rw-r--r--  2.0 unx      523 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/LandmarkEstimator.py
+-rw-r--r--  2.0 unx      524 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/ObjectDetector.py
+-rw-r--r--  2.0 unx     1279 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/RoiEstimator.py
+-rw-r--r--  2.0 unx     3251 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/SSDDetector.py
+-rw-r--r--  2.0 unx     2082 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/SlidingWindowEstimator.py
+-rw-r--r--  2.0 unx     1436 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/SpatialCascadeEstimator.py
+-rw-r--r--  2.0 unx     3823 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/UltralyticsYOLODetector.py
+-rw-r--r--  2.0 unx     3300 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/YOLODetector.py
+-rw-r--r--  2.0 unx     2904 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/YOLOXE2EDetector.py
+-rw-r--r--  2.0 unx     1635 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/YOLOv5Detector.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/__init__.py
+-rw-r--r--  2.0 unx     3441 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/camera/ArUcoCameraPoseEstimator.py
+-rw-r--r--  2.0 unx      850 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/camera/BoardCameraCalibrator.py
+-rw-r--r--  2.0 unx     4276 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/camera/ChArUcoCalibrator.py
+-rw-r--r--  2.0 unx     3145 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/camera/ChessboardCalibrator.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/camera/__init__.py
+-rw-r--r--  2.0 unx     1127 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/face/AdasFaceDetector.py
+-rw-r--r--  2.0 unx      512 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/face/FaceDetector.py
+-rw-r--r--  2.0 unx     3805 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/face/OpenVinoFaceDetector.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/face/__init__.py
+-rw-r--r--  2.0 unx     1760 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/face/emotion/AffectNetEmotionClassifier.py
+-rw-r--r--  2.0 unx     2038 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/face/emotion/FERPlusEmotionClassifier.py
+-rw-r--r--  2.0 unx      450 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/face/emotion/FaceEmotionEstimator.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/face/emotion/__init__.py
+-rw-r--r--  2.0 unx     1595 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/face/eye/EyeOpenClosedEstimator.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/face/eye/__init__.py
+-rw-r--r--  2.0 unx      609 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/face/landmark/FaceLandmarkEstimator.py
+-rw-r--r--  2.0 unx     2646 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/face/landmark/IrisDistanceCalculator.py
+-rw-r--r--  2.0 unx     2204 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/face/landmark/MediaPipeFaceDetector.py
+-rw-r--r--  2.0 unx     2324 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/face/landmark/MediaPipeFaceMeshEstimator.py
+-rw-r--r--  2.0 unx     1811 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/face/landmark/RegressionLandmarkEstimator.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/face/landmark/__init__.py
+-rw-r--r--  2.0 unx     1280 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/face/pose/AdasHeadPoseEstimator.py
+-rw-r--r--  2.0 unx      315 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/face/pose/HeadPoseEstimator.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/face/pose/__init__.py
+-rw-r--r--  2.0 unx     4265 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/face/recognition/FaceRecognitionEstimator.py
+-rw-r--r--  2.0 unx     2803 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/face/recognition/FaceReidentificationEstimator.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/face/recognition/__init__.py
+-rw-r--r--  2.0 unx      517 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/hand/HandDetector.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/hand/__init__.py
+-rw-r--r--  2.0 unx      608 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/hand/landmark/HandLandmarkEstimator.py
+-rw-r--r--  2.0 unx     2778 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/hand/landmark/MediaPipeHandEstimator.py
+-rw-r--r--  2.0 unx     2330 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/hand/landmark/OpenPoseHandEstimator.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/hand/landmark/__init__.py
+-rw-r--r--  2.0 unx     2219 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/pose/AEPoseEstimator.py
+-rw-r--r--  2.0 unx     5328 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/pose/EfficientPoseEstimator.py
+-rw-r--r--  2.0 unx     7134 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/pose/KAPAOPoseEstimator.py
+-rw-r--r--  2.0 unx     3869 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/pose/LiteHRNetEstimator.py
+-rw-r--r--  2.0 unx     2834 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/pose/LitePoseEstimator.py
+-rw-r--r--  2.0 unx     4302 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/pose/MediaPipeHolisticEstimator.py
+-rw-r--r--  2.0 unx     3069 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/pose/MediaPipePoseEstimator.py
+-rw-r--r--  2.0 unx     6641 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/pose/MobileHumanPoseEstimator.py
+-rw-r--r--  2.0 unx     9862 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/pose/MobileNetV2PoseEstimator.py
+-rw-r--r--  2.0 unx     4304 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/pose/MoveNetPoseEstimator.py
+-rw-r--r--  2.0 unx     1876 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/pose/OpenPoseEstimator.py
+-rw-r--r--  2.0 unx      518 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/pose/PoseEstimator.py
+-rw-r--r--  2.0 unx     2347 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/pose/TopDownPoseEstimator.py
+-rw-r--r--  2.0 unx     4451 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/pose/YOLOv8PoseEstimator.py
+-rw-r--r--  2.0 unx     6637 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/pose/__init__.py
+-rw-r--r--  2.0 unx     3165 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/segmentation/MODNetEstimator.py
+-rw-r--r--  2.0 unx     8687 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/segmentation/MaskRCNNEstimator.py
+-rw-r--r--  2.0 unx     1989 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/segmentation/MediaPipeSelfieSegmentation.py
+-rw-r--r--  2.0 unx     3287 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/segmentation/YolactEstimator.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/estimator/spatial/segmentation/__init__.py
+-rw-r--r--  2.0 unx     1627 b- defN 24-Apr-23 14:46 visiongraph/estimator/translation/DeblurGANv2.py
+-rw-r--r--  2.0 unx      314 b- defN 24-Apr-23 14:46 visiongraph/estimator/translation/DepthEstimator.py
+-rw-r--r--  2.0 unx     3887 b- defN 24-Apr-23 14:46 visiongraph/estimator/translation/MBLLENEstimator.py
+-rw-r--r--  2.0 unx     2898 b- defN 24-Apr-23 14:46 visiongraph/estimator/translation/MidasDepthEstimator.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/estimator/translation/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/external/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/external/intel/__init__.py
+-rw-r--r--  2.0 unx     4342 b- defN 24-Apr-23 14:46 visiongraph/external/intel/performance_metrics.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/external/intel/adapters/__init__.py
+-rw-r--r--  2.0 unx     5682 b- defN 24-Apr-23 14:46 visiongraph/external/intel/adapters/inference_adapter.py
+-rw-r--r--  2.0 unx     5151 b- defN 24-Apr-23 14:46 visiongraph/external/intel/adapters/model_adapter.py
+-rw-r--r--  2.0 unx     8056 b- defN 24-Apr-23 14:46 visiongraph/external/intel/adapters/openvino_adapter.py
+-rw-r--r--  2.0 unx     6585 b- defN 24-Apr-23 14:46 visiongraph/external/intel/adapters/ovms_adapter.py
+-rw-r--r--  2.0 unx     2794 b- defN 24-Apr-23 14:46 visiongraph/external/intel/adapters/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/external/intel/models/__init__.py
+-rw-r--r--  2.0 unx     7442 b- defN 24-Apr-23 14:46 visiongraph/external/intel/models/centernet.py
+-rw-r--r--  2.0 unx     5700 b- defN 24-Apr-23 14:46 visiongraph/external/intel/models/detection_model.py
+-rw-r--r--  2.0 unx     2992 b- defN 24-Apr-23 14:46 visiongraph/external/intel/models/detr.py
+-rw-r--r--  2.0 unx    14421 b- defN 24-Apr-23 14:46 visiongraph/external/intel/models/hpe_associative_embedding.py
+-rw-r--r--  2.0 unx     7247 b- defN 24-Apr-23 14:46 visiongraph/external/intel/models/image_model.py
+-rw-r--r--  2.0 unx    12558 b- defN 24-Apr-23 14:46 visiongraph/external/intel/models/model.py
+-rw-r--r--  2.0 unx    17789 b- defN 24-Apr-23 14:46 visiongraph/external/intel/models/open_pose.py
+-rw-r--r--  2.0 unx     5874 b- defN 24-Apr-23 14:46 visiongraph/external/intel/models/ssd.py
+-rw-r--r--  2.0 unx     5805 b- defN 24-Apr-23 14:46 visiongraph/external/intel/models/types.py
+-rw-r--r--  2.0 unx     7260 b- defN 24-Apr-23 14:46 visiongraph/external/intel/models/utils.py
+-rw-r--r--  2.0 unx    21407 b- defN 24-Apr-23 14:46 visiongraph/external/intel/models/yolo.py
+-rw-r--r--  2.0 unx      154 b- defN 24-Apr-23 14:46 visiongraph/external/intel/pipelines/__init__.py
+-rw-r--r--  2.0 unx     5407 b- defN 24-Apr-23 14:46 visiongraph/external/intel/pipelines/async_pipeline.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/external/midas/__init__.py
+-rw-r--r--  2.0 unx     7868 b- defN 24-Apr-23 14:46 visiongraph/external/midas/transforms.py
+-rw-r--r--  2.0 unx      193 b- defN 24-Apr-23 14:46 visiongraph/external/motpy/__init__.py
+-rw-r--r--  2.0 unx     1816 b- defN 24-Apr-23 14:46 visiongraph/external/motpy/core.py
+-rw-r--r--  2.0 unx      333 b- defN 24-Apr-23 14:46 visiongraph/external/motpy/detector.py
+-rw-r--r--  2.0 unx     1147 b- defN 24-Apr-23 14:46 visiongraph/external/motpy/metrics.py
+-rw-r--r--  2.0 unx     5402 b- defN 24-Apr-23 14:46 visiongraph/external/motpy/model.py
+-rw-r--r--  2.0 unx     3576 b- defN 24-Apr-23 14:46 visiongraph/external/motpy/testing.py
+-rw-r--r--  2.0 unx     2647 b- defN 24-Apr-23 14:46 visiongraph/external/motpy/testing_viz.py
+-rw-r--r--  2.0 unx    16512 b- defN 24-Apr-23 14:46 visiongraph/external/motpy/tracker.py
+-rw-r--r--  2.0 unx      652 b- defN 24-Apr-23 14:46 visiongraph/external/motpy/utils.py
+-rw-r--r--  2.0 unx     5312 b- defN 24-Apr-23 14:46 visiongraph/external/motrackers/Track.py
+-rw-r--r--  2.0 unx     7486 b- defN 24-Apr-23 14:46 visiongraph/external/motrackers/Tracker.py
+-rw-r--r--  2.0 unx      184 b- defN 24-Apr-23 14:46 visiongraph/external/motrackers/__init__.py
+-rw-r--r--  2.0 unx       83 b- defN 24-Apr-23 14:46 visiongraph/external/motrackers/utils/__init__.py
+-rw-r--r--  2.0 unx     8978 b- defN 24-Apr-23 14:46 visiongraph/external/motrackers/utils/misc.py
+-rw-r--r--  2.0 unx    17144 b- defN 24-Apr-23 14:46 visiongraph/input/AzureKinectInput.py
+-rw-r--r--  2.0 unx     3855 b- defN 24-Apr-23 14:46 visiongraph/input/BaseCamera.py
+-rw-r--r--  2.0 unx     3338 b- defN 24-Apr-23 14:46 visiongraph/input/BaseDepthCamera.py
+-rw-r--r--  2.0 unx     1309 b- defN 24-Apr-23 14:46 visiongraph/input/BaseDepthInput.py
+-rw-r--r--  2.0 unx     4232 b- defN 24-Apr-23 14:46 visiongraph/input/BaseInput.py
+-rw-r--r--  2.0 unx     1721 b- defN 24-Apr-23 14:46 visiongraph/input/CamGearInput.py
+-rw-r--r--  2.0 unx     9264 b- defN 24-Apr-23 14:46 visiongraph/input/DepthAIBaseInput.py
+-rw-r--r--  2.0 unx     1493 b- defN 24-Apr-23 14:46 visiongraph/input/ImageInput.py
+-rw-r--r--  2.0 unx      295 b- defN 24-Apr-23 14:46 visiongraph/input/Oak1Input.py
+-rw-r--r--  2.0 unx     8024 b- defN 24-Apr-23 14:46 visiongraph/input/OakDInput.py
+-rw-r--r--  2.0 unx    19468 b- defN 24-Apr-23 14:46 visiongraph/input/RealSenseInput.py
+-rw-r--r--  2.0 unx     5032 b- defN 24-Apr-23 14:46 visiongraph/input/VideoCaptureInput.py
+-rw-r--r--  2.0 unx     4814 b- defN 24-Apr-23 14:46 visiongraph/input/ZEDInput.py
+-rw-r--r--  2.0 unx     1744 b- defN 24-Apr-23 14:46 visiongraph/input/__init__.py
+-rw-r--r--  2.0 unx     1887 b- defN 24-Apr-23 14:46 visiongraph/model/CameraIntrinsics.py
+-rw-r--r--  2.0 unx      101 b- defN 24-Apr-23 14:46 visiongraph/model/CameraStreamType.py
+-rw-r--r--  2.0 unx      549 b- defN 24-Apr-23 14:46 visiongraph/model/DepthBuffer.py
+-rw-r--r--  2.0 unx      266 b- defN 24-Apr-23 14:46 visiongraph/model/VisionEngineModelLayer.py
+-rw-r--r--  2.0 unx      753 b- defN 24-Apr-23 14:46 visiongraph/model/VisionEngineOutput.py
+-rw-r--r--  2.0 unx      135 b- defN 24-Apr-23 14:46 visiongraph/model/_ImportStub.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/model/__init__.py
+-rw-r--r--  2.0 unx     4229 b- defN 24-Apr-23 14:46 visiongraph/model/geometry/BoundingBox2D.py
+-rw-r--r--  2.0 unx      815 b- defN 24-Apr-23 14:46 visiongraph/model/geometry/Size2D.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/model/geometry/__init__.py
+-rw-r--r--  2.0 unx      585 b- defN 24-Apr-23 14:46 visiongraph/model/parameter/ArgumentConfigurable.py
+-rw-r--r--  2.0 unx      186 b- defN 24-Apr-23 14:46 visiongraph/model/parameter/NamedParameter.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/model/parameter/__init__.py
+-rw-r--r--  2.0 unx      399 b- defN 24-Apr-23 14:46 visiongraph/model/tracker/Trackable.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/model/tracker/__init__.py
+-rw-r--r--  2.0 unx       78 b- defN 24-Apr-23 14:46 visiongraph/model/types/InputShapeOrder.py
+-rw-r--r--  2.0 unx       99 b- defN 24-Apr-23 14:46 visiongraph/model/types/MediaPipePoseModelComplexity.py
+-rw-r--r--  2.0 unx      440 b- defN 24-Apr-23 14:46 visiongraph/model/types/ModelPrecision.py
+-rw-r--r--  2.0 unx      199 b- defN 24-Apr-23 14:46 visiongraph/model/types/RealSenseColorScheme.py
+-rw-r--r--  2.0 unx      204 b- defN 24-Apr-23 14:46 visiongraph/model/types/RealSenseFilter.py
+-rw-r--r--  2.0 unx     1180 b- defN 24-Apr-23 14:46 visiongraph/model/types/VideoCaptureBackend.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/model/types/__init__.py
+-rw-r--r--  2.0 unx      905 b- defN 24-Apr-23 14:46 visiongraph/node/ApplyNode.py
+-rw-r--r--  2.0 unx      753 b- defN 24-Apr-23 14:46 visiongraph/node/BreakpointNode.py
+-rw-r--r--  2.0 unx      815 b- defN 24-Apr-23 14:46 visiongraph/node/CustomNode.py
+-rw-r--r--  2.0 unx      903 b- defN 24-Apr-23 14:46 visiongraph/node/ExtractNode.py
+-rw-r--r--  2.0 unx      525 b- defN 24-Apr-23 14:46 visiongraph/node/PassThroughNode.py
+-rw-r--r--  2.0 unx      825 b- defN 24-Apr-23 14:46 visiongraph/node/SequenceNode.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/node/__init__.py
+-rw-r--r--  2.0 unx     1676 b- defN 24-Apr-23 14:46 visiongraph/output/ImagePreview.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/output/__init__.py
+-rw-r--r--  2.0 unx      965 b- defN 24-Apr-23 14:46 visiongraph/output/fbs/FrameBufferSharingServer.py
+-rw-r--r--  2.0 unx     1427 b- defN 24-Apr-23 14:46 visiongraph/output/fbs/SpoutServer.py
+-rw-r--r--  2.0 unx     1623 b- defN 24-Apr-23 14:46 visiongraph/output/fbs/SyphonServer.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/output/fbs/__init__.py
+-rw-r--r--  2.0 unx      980 b- defN 24-Apr-23 14:46 visiongraph/recorder/AsyncFrameSetRecorder.py
+-rw-r--r--  2.0 unx     1108 b- defN 24-Apr-23 14:46 visiongraph/recorder/BaseFrameRecorder.py
+-rw-r--r--  2.0 unx     1143 b- defN 24-Apr-23 14:46 visiongraph/recorder/CV2VideoRecorder.py
+-rw-r--r--  2.0 unx     1016 b- defN 24-Apr-23 14:46 visiongraph/recorder/FrameSetRecorder.py
+-rw-r--r--  2.0 unx      905 b- defN 24-Apr-23 14:46 visiongraph/recorder/MoviePyVideoRecorder.py
+-rw-r--r--  2.0 unx     1377 b- defN 24-Apr-23 14:46 visiongraph/recorder/VidGearVideoRecorder.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/recorder/__init__.py
+-rw-r--r--  2.0 unx      564 b- defN 24-Apr-23 14:46 visiongraph/result/ArUcoCameraPose.py
+-rw-r--r--  2.0 unx     1521 b- defN 24-Apr-23 14:46 visiongraph/result/ArUcoMarkerDetection.py
+-rw-r--r--  2.0 unx      167 b- defN 24-Apr-23 14:46 visiongraph/result/BaseResult.py
+-rw-r--r--  2.0 unx      525 b- defN 24-Apr-23 14:46 visiongraph/result/CameraPoseResult.py
+-rw-r--r--  2.0 unx      378 b- defN 24-Apr-23 14:46 visiongraph/result/ClassificationResult.py
+-rw-r--r--  2.0 unx     1709 b- defN 24-Apr-23 14:46 visiongraph/result/DepthMap.py
+-rw-r--r--  2.0 unx      413 b- defN 24-Apr-23 14:46 visiongraph/result/EmbeddingResult.py
+-rw-r--r--  2.0 unx      474 b- defN 24-Apr-23 14:46 visiongraph/result/HeadPoseResult.py
+-rw-r--r--  2.0 unx      333 b- defN 24-Apr-23 14:46 visiongraph/result/ImageResult.py
+-rw-r--r--  2.0 unx      534 b- defN 24-Apr-23 14:46 visiongraph/result/LandmarkEmbeddingResult.py
+-rw-r--r--  2.0 unx     1165 b- defN 24-Apr-23 14:46 visiongraph/result/ResultAnnotator.py
+-rw-r--r--  2.0 unx      483 b- defN 24-Apr-23 14:46 visiongraph/result/ResultDict.py
+-rw-r--r--  2.0 unx      466 b- defN 24-Apr-23 14:46 visiongraph/result/ResultList.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/result/__init__.py
+-rw-r--r--  2.0 unx      779 b- defN 24-Apr-23 14:46 visiongraph/result/spatial/CrowdHumanResult.py
+-rw-r--r--  2.0 unx     1354 b- defN 24-Apr-23 14:46 visiongraph/result/spatial/InstanceSegmentationResult.py
+-rw-r--r--  2.0 unx     3838 b- defN 24-Apr-23 14:46 visiongraph/result/spatial/LandmarkDetectionResult.py
+-rw-r--r--  2.0 unx     3475 b- defN 24-Apr-23 14:46 visiongraph/result/spatial/ObjectDetectionResult.py
+-rw-r--r--  2.0 unx      878 b- defN 24-Apr-23 14:46 visiongraph/result/spatial/SpatialCascadeResult.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/result/spatial/__init__.py
+-rw-r--r--  2.0 unx      924 b- defN 24-Apr-23 14:46 visiongraph/result/spatial/face/BlazeFace.py
+-rw-r--r--  2.0 unx     2194 b- defN 24-Apr-23 14:46 visiongraph/result/spatial/face/BlazeFaceMesh.py
+-rw-r--r--  2.0 unx      702 b- defN 24-Apr-23 14:46 visiongraph/result/spatial/face/EmotionClassificationResult.py
+-rw-r--r--  2.0 unx      694 b- defN 24-Apr-23 14:46 visiongraph/result/spatial/face/EyeOpenClosedResult.py
+-rw-r--r--  2.0 unx      409 b- defN 24-Apr-23 14:46 visiongraph/result/spatial/face/FaceDetectionResult.py
+-rw-r--r--  2.0 unx      881 b- defN 24-Apr-23 14:46 visiongraph/result/spatial/face/FaceLandmarkResult.py
+-rw-r--r--  2.0 unx     1390 b- defN 24-Apr-23 14:46 visiongraph/result/spatial/face/IrisDistanceResult.py
+-rw-r--r--  2.0 unx      868 b- defN 24-Apr-23 14:46 visiongraph/result/spatial/face/RegressionFace.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/result/spatial/face/__init__.py
+-rw-r--r--  2.0 unx     2648 b- defN 24-Apr-23 14:46 visiongraph/result/spatial/hand/BlazeHand.py
+-rw-r--r--  2.0 unx      409 b- defN 24-Apr-23 14:46 visiongraph/result/spatial/hand/HandDetectionResult.py
+-rw-r--r--  2.0 unx     2590 b- defN 24-Apr-23 14:46 visiongraph/result/spatial/hand/HandLandmarkResult.py
+-rw-r--r--  2.0 unx       88 b- defN 24-Apr-23 14:46 visiongraph/result/spatial/hand/Handedness.py
+-rw-r--r--  2.0 unx      106 b- defN 24-Apr-23 14:46 visiongraph/result/spatial/hand/OpenPoseHand.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/result/spatial/hand/__init__.py
+-rw-r--r--  2.0 unx     3435 b- defN 24-Apr-23 14:46 visiongraph/result/spatial/pose/BlazePose.py
+-rw-r--r--  2.0 unx     1266 b- defN 24-Apr-23 14:46 visiongraph/result/spatial/pose/BlazePoseSegmentation.py
+-rw-r--r--  2.0 unx     2826 b- defN 24-Apr-23 14:46 visiongraph/result/spatial/pose/COCOOpenPose.py
+-rw-r--r--  2.0 unx     2680 b- defN 24-Apr-23 14:46 visiongraph/result/spatial/pose/COCOPose.py
+-rw-r--r--  2.0 unx     2337 b- defN 24-Apr-23 14:46 visiongraph/result/spatial/pose/EfficientPose.py
+-rw-r--r--  2.0 unx     1726 b- defN 24-Apr-23 14:46 visiongraph/result/spatial/pose/HolisticPose.py
+-rw-r--r--  2.0 unx     2738 b- defN 24-Apr-23 14:46 visiongraph/result/spatial/pose/MobileHumanPose.py
+-rw-r--r--  2.0 unx     2963 b- defN 24-Apr-23 14:46 visiongraph/result/spatial/pose/PoseLandmarkResult.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/result/spatial/pose/__init__.py
+-rw-r--r--  2.0 unx      492 b- defN 24-Apr-23 14:46 visiongraph/tracker/BaseObjectDetectionTracker.py
+-rw-r--r--  2.0 unx     1908 b- defN 24-Apr-23 14:46 visiongraph/tracker/CentroidTracker.py
+-rw-r--r--  2.0 unx     4906 b- defN 24-Apr-23 14:46 visiongraph/tracker/FlateTracker.py
+-rw-r--r--  2.0 unx     2705 b- defN 24-Apr-23 14:46 visiongraph/tracker/MotpyTracker.py
+-rw-r--r--  2.0 unx     3304 b- defN 24-Apr-23 14:46 visiongraph/tracker/ObjectAssignmentSolver.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/tracker/__init__.py
+-rw-r--r--  2.0 unx     3105 b- defN 24-Apr-23 14:46 visiongraph/util/ArgUtils.py
+-rw-r--r--  2.0 unx     1009 b- defN 24-Apr-23 14:46 visiongraph/util/CodeUtils.py
+-rw-r--r--  2.0 unx      222 b- defN 24-Apr-23 14:46 visiongraph/util/CollectionUtils.py
+-rw-r--r--  2.0 unx      314 b- defN 24-Apr-23 14:46 visiongraph/util/CommonArgs.py
+-rw-r--r--  2.0 unx     4659 b- defN 24-Apr-23 14:46 visiongraph/util/DrawingUtils.py
+-rw-r--r--  2.0 unx     3926 b- defN 24-Apr-23 14:46 visiongraph/util/ImageUtils.py
+-rw-r--r--  2.0 unx     2608 b- defN 24-Apr-23 14:46 visiongraph/util/LinalgUtils.py
+-rw-r--r--  2.0 unx      539 b- defN 24-Apr-23 14:46 visiongraph/util/LoggingUtils.py
+-rw-r--r--  2.0 unx     1752 b- defN 24-Apr-23 14:46 visiongraph/util/MathUtils.py
+-rw-r--r--  2.0 unx      608 b- defN 24-Apr-23 14:46 visiongraph/util/MediaPipeUtils.py
+-rw-r--r--  2.0 unx     2731 b- defN 24-Apr-23 14:46 visiongraph/util/NetworkUtils.py
+-rw-r--r--  2.0 unx      195 b- defN 24-Apr-23 14:46 visiongraph/util/OSUtils.py
+-rw-r--r--  2.0 unx      700 b- defN 24-Apr-23 14:46 visiongraph/util/OpenVinoUtils.py
+-rw-r--r--  2.0 unx     5639 b- defN 24-Apr-23 14:46 visiongraph/util/PoseUtils.py
+-rw-r--r--  2.0 unx     1608 b- defN 24-Apr-23 14:46 visiongraph/util/ResultUtils.py
+-rw-r--r--  2.0 unx     2102 b- defN 24-Apr-23 14:46 visiongraph/util/TimeUtils.py
+-rw-r--r--  2.0 unx     3643 b- defN 24-Apr-23 14:46 visiongraph/util/VectorUtils.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-23 14:46 visiongraph/util/__init__.py
+-rw-r--r--  2.0 unx    11925 b- defN 24-Apr-23 14:46 visiongraph-0.1.57.1.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-23 14:46 visiongraph-0.1.57.1.dist-info/WHEEL
+-rw-r--r--  2.0 unx       20 b- defN 24-Apr-23 14:46 visiongraph-0.1.57.1.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       12 b- defN 24-Apr-23 14:46 visiongraph-0.1.57.1.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    29010 b- defN 24-Apr-23 14:46 visiongraph-0.1.57.1.dist-info/RECORD
+289 files, 715132 bytes uncompressed, 215659 bytes compressed:  69.8%
```

## zipnote {}

```diff
@@ -846,23 +846,23 @@
 
 Filename: visiongraph/util/VectorUtils.py
 Comment: 
 
 Filename: visiongraph/util/__init__.py
 Comment: 
 
-Filename: visiongraph-0.1.57.dist-info/METADATA
+Filename: visiongraph-0.1.57.1.dist-info/METADATA
 Comment: 
 
-Filename: visiongraph-0.1.57.dist-info/WHEEL
+Filename: visiongraph-0.1.57.1.dist-info/WHEEL
 Comment: 
 
-Filename: visiongraph-0.1.57.dist-info/entry_points.txt
+Filename: visiongraph-0.1.57.1.dist-info/entry_points.txt
 Comment: 
 
-Filename: visiongraph-0.1.57.dist-info/top_level.txt
+Filename: visiongraph-0.1.57.1.dist-info/top_level.txt
 Comment: 
 
-Filename: visiongraph-0.1.57.dist-info/RECORD
+Filename: visiongraph-0.1.57.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## visiongraph/input/DepthAIBaseInput.py

```diff
@@ -22,14 +22,15 @@
         self.queue_max_size: int = 1
 
         self.color_sensor_resolution: dai.ColorCameraProperties.SensorResolution = dai.ColorCameraProperties.SensorResolution.THE_1080_P
 
         self.interleaved: bool = False
         self.color_isp_scale: Optional[Tuple[int, int]] = None
         self.color_board_socket: dai.CameraBoardSocket = dai.CameraBoardSocket.CAM_A
+        self.color_fps: Optional[float] = None
 
         self._focus_mode: dai.RawCameraControl.AutoFocusMode = dai.RawCameraControl.AutoFocusMode.AUTO
         self._manual_lens_pos: int = 0
 
         self._auto_exposure: bool = True
         self._exposure: timedelta = timedelta(microseconds=30)
         self._iso_sensitivity: int = 400
@@ -61,15 +62,15 @@
         self._last_rgb_frame: Optional[np.ndarray] = None
 
     def setup(self):
         self.pipeline = dai.Pipeline()
         self.pre_start_setup()
 
         # starts pipeline
-        self.device = dai.Device(self.pipeline)
+        self.device = dai.Device(self.pipeline).__enter__()
 
         self.rgb_control_queue = self.device.getInputQueue(self.rgb_control_in_name)
         self.rgb_queue = self.device.getOutputQueue(name=self.rgb_stream_name, maxSize=self.queue_max_size,
                                                     blocking=False)
         self.rgb_isp_queue = self.device.getOutputQueue(name=self.rgb_isp_stream_name, maxSize=self.queue_max_size,
                                                         blocking=False)
 
@@ -79,14 +80,17 @@
         self.height = rgb_isp_frame.getHeight()
 
     def pre_start_setup(self):
         self.color_camera = self.pipeline.create(dai.node.ColorCamera)
         self.color_camera.setBoardSocket(self.color_board_socket)
         self.color_camera.setResolution(self.color_sensor_resolution)
 
+        if self.color_fps is not None:
+            self.color_camera.setFps(self.color_fps)
+
         self.color_camera.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)
         self.color_camera.setInterleaved(self.interleaved)
 
         if self.color_isp_scale is not None:
             self.color_camera.setIspScale(self.color_isp_scale[0], self.color_isp_scale[1])
 
         self.color_x_out = self.pipeline.create(dai.node.XLinkOut)
@@ -113,15 +117,15 @@
         ts = int(frame.getTimestamp().total_seconds() * 1000)
         image = typing.cast(np.ndarray, frame.getCvFrame())
 
         self._last_rgb_frame = image
         self._last_ts = ts
 
     def release(self):
-        self.device.close()
+        self.device.__exit__(None, None, None)
 
     @property
     def gain(self) -> int:
         raise Exception("Gain is not supported.")
 
     @gain.setter
     def gain(self, value: int):
```

## visiongraph/input/OakDInput.py

```diff
@@ -2,14 +2,15 @@
 from argparse import Namespace
 from enum import Enum
 from typing import Optional
 
 import cv2
 import depthai as dai
 import numpy as np
+
 from visiongraph.input.BaseDepthCamera import BaseDepthCamera
 from visiongraph.input.DepthAIBaseInput import DepthAIBaseInput
 from visiongraph.model.CameraStreamType import CameraStreamType
 
 
 class OakDFrameAlignment(Enum):
     Disabled = 0
@@ -54,79 +55,89 @@
         self.depth_queue: Optional[dai.DataOutputQueue] = None
 
         # capture
         self._last_ir_frame: Optional[np.ndarray] = None
         self._last_depth_frame: Optional[np.ndarray] = None
 
     def pre_start_setup(self):
+        if self.use_depth_as_input:
+            self.enable_depth = True
+
         super().pre_start_setup()
 
         # setup ir camera's
-        self.ir_left_camera = self.pipeline.create(dai.node.MonoCamera)
-        self.ir_left_camera.setBoardSocket(dai.CameraBoardSocket.LEFT)
-        self.ir_left_camera.setResolution(self.ir_sensor_resolution)
-
-        self.ir_right_camera = self.pipeline.create(dai.node.MonoCamera)
-        self.ir_right_camera.setBoardSocket(dai.CameraBoardSocket.RIGHT)
-        self.ir_right_camera.setResolution(self.ir_sensor_resolution)
-
-        if self.select_ir_camera.LEFT:
-            self.active_ir_camera = self.ir_left_camera
-        else:
-            self.active_ir_camera = self.ir_right_camera
-
-        # link active ir camera
-        self.ir_x_out = self.pipeline.create(dai.node.XLinkOut)
-        self.ir_x_out.setStreamName(self.ir_stream_name)
-        self.active_ir_camera.out.link(self.ir_x_out.input)
+        if self.use_infrared or self.enable_depth:
+            self.ir_left_camera = self.pipeline.create(dai.node.MonoCamera)
+            self.ir_left_camera.setBoardSocket(dai.CameraBoardSocket.LEFT)
+            self.ir_left_camera.setResolution(self.ir_sensor_resolution)
+
+            self.ir_right_camera = self.pipeline.create(dai.node.MonoCamera)
+            self.ir_right_camera.setBoardSocket(dai.CameraBoardSocket.RIGHT)
+            self.ir_right_camera.setResolution(self.ir_sensor_resolution)
+
+            if self.select_ir_camera.LEFT:
+                self.active_ir_camera = self.ir_left_camera
+            else:
+                self.active_ir_camera = self.ir_right_camera
+
+            # link active ir camera
+            self.ir_x_out = self.pipeline.create(dai.node.XLinkOut)
+            self.ir_x_out.setStreamName(self.ir_stream_name)
+            self.active_ir_camera.out.link(self.ir_x_out.input)
 
         # set depth camera settings
-        self.depth_node = self.pipeline.create(dai.node.StereoDepth)
-        self.depth_node.setDefaultProfilePreset(self.depth_preset_mode)
-        self.depth_node.initialConfig.setMedianFilter(self.depth_median_filter)
-        self.depth_node.setLeftRightCheck(self.depth_left_right_check)
-        self.depth_node.setExtendedDisparity(self.depth_extended_disparity)
-        self.depth_node.setSubpixel(self.depth_subpixel)
-
-        # setup depth align
-        if self.frame_alignment == self.frame_alignment.Infrared:
-            self.depth_node.setDepthAlign(camera=self.active_ir_camera.getBoardSocket())
-        elif self.frame_alignment == self.frame_alignment.Color:
-            self.depth_node.setDepthAlign(camera=self.color_camera.getBoardSocket())
-
-        # link depth
-        self.ir_left_camera.out.link(self.depth_node.left)
-        self.ir_right_camera.out.link(self.depth_node.right)
-
-        self.depth_x_out = self.pipeline.create(dai.node.XLinkOut)
-        self.depth_x_out.setStreamName(self.depth_stream_name)
-        self.depth_node.depth.link(self.depth_x_out.input)
+        if self.enable_depth:
+            self.depth_node = self.pipeline.create(dai.node.StereoDepth)
+            self.depth_node.setDefaultProfilePreset(self.depth_preset_mode)
+            self.depth_node.initialConfig.setMedianFilter(self.depth_median_filter)
+            self.depth_node.setLeftRightCheck(self.depth_left_right_check)
+            self.depth_node.setExtendedDisparity(self.depth_extended_disparity)
+            self.depth_node.setSubpixel(self.depth_subpixel)
+
+            # setup depth align
+            if self.frame_alignment == self.frame_alignment.Infrared:
+                self.depth_node.setDepthAlign(camera=self.active_ir_camera.getBoardSocket())
+            elif self.frame_alignment == self.frame_alignment.Color:
+                self.depth_node.setDepthAlign(camera=self.color_camera.getBoardSocket())
+
+            # link depth
+            self.ir_left_camera.out.link(self.depth_node.left)
+            self.ir_right_camera.out.link(self.depth_node.right)
+
+            self.depth_x_out = self.pipeline.create(dai.node.XLinkOut)
+            self.depth_x_out.setStreamName(self.depth_stream_name)
+            self.depth_node.depth.link(self.depth_x_out.input)
 
     def setup(self):
         super().setup()
 
-        self.ir_queue = self.device.getOutputQueue(name=self.ir_stream_name, maxSize=self.queue_max_size,
+        self.ir_queue = self.device.getOutputQueue(name=self.ir_stream_name,
+                                                   maxSize=self.queue_max_size,
                                                    blocking=False)
-        self.depth_queue = self.device.getOutputQueue(name=self.depth_stream_name, maxSize=self.queue_max_size,
-                                                      blocking=False)
+
+        if self.enable_depth:
+            self.depth_queue = self.device.getOutputQueue(name=self.depth_stream_name,
+                                                          maxSize=self.queue_max_size,
+                                                          blocking=False)
 
         self.device.setIrLaserDotProjectorIntensity(self._ir_laser_dot_projector_intensity)
         self.device.setIrFloodLightIntensity(self._ir_flood_light_intensity)
 
     def read(self) -> (int, Optional[np.ndarray]):
         super().read()
 
-        ir_frame = typing.cast(dai.ImgFrame, self.ir_queue.get())
-        depth_frame = typing.cast(dai.ImgFrame, self.depth_queue.get())
-
-        ir_image = typing.cast(np.ndarray, ir_frame.getCvFrame())
-        depth_image = typing.cast(np.ndarray, depth_frame.getCvFrame())
-
-        self._last_ir_frame = ir_image
-        self._last_depth_frame = depth_image
+        if self.use_infrared:
+            ir_frame = typing.cast(dai.ImgFrame, self.ir_queue.get())
+            ir_image = typing.cast(np.ndarray, ir_frame.getCvFrame())
+            self._last_ir_frame = ir_image
+
+        if self.enable_depth:
+            depth_frame = typing.cast(dai.ImgFrame, self.depth_queue.get())
+            depth_image = typing.cast(np.ndarray, depth_frame.getCvFrame())
+            self._last_depth_frame = depth_image
 
         if self.use_depth_as_input:
             return self._post_process(self._last_ts, self.depth_map)
 
         if self.use_infrared:
             return self._post_process(self._last_ts, self._last_ir_frame)
 
@@ -155,25 +166,27 @@
 
     @property
     def ir_laser_dot_projector_intensity(self):
         return self._ir_laser_dot_projector_intensity
 
     @ir_laser_dot_projector_intensity.setter
     def ir_laser_dot_projector_intensity(self, value: int):
-        self.device.setIrLaserDotProjectorIntensity(value)
-        self._ir_laser_dot_projector_intensity = value
+        if self.device is not None:
+            self.device.setIrLaserDotProjectorIntensity(value)
+            self._ir_laser_dot_projector_intensity = value
 
     @property
     def ir_flood_light_intensity(self):
         return self._ir_laser_dot_projector_intensity
 
     @ir_flood_light_intensity.setter
     def ir_flood_light_intensity(self, value: int):
-        self.device.setIrFloodLightIntensity(value)
-        self._ir_flood_light_intensity = value
+        if self.device is not None:
+            self.device.setIrFloodLightIntensity(value)
+            self._ir_flood_light_intensity = value
 
     def get_raw_image(self, stream_type: CameraStreamType = CameraStreamType.Color) -> Optional[np.ndarray]:
         if stream_type == CameraStreamType.Depth:
             return self.depth_map
         elif stream_type == CameraStreamType.Infrared:
             return self._last_ir_frame
         elif stream_type == CameraStreamType.Color:
```

## Comparing `visiongraph-0.1.57.dist-info/METADATA` & `visiongraph-0.1.57.1.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: visiongraph
-Version: 0.1.57
+Version: 0.1.57.1
 Summary: Visiongraph is a high level computer vision framework.
 Home-page: https://github.com/cansik/visiongraph
 Author: Florian Bruggisser
 Author-email: github@broox.ch
 License: MIT License
 Platform: UNKNOWN
 Classifier: Development Status :: 3 - Alpha
```

## Comparing `visiongraph-0.1.57.dist-info/RECORD` & `visiongraph-0.1.57.1.dist-info/RECORD`

 * *Files 2% similar despite different names*

```diff
@@ -160,18 +160,18 @@
 visiongraph/external/motrackers/utils/misc.py,sha256=Q4QuYxw3nb6frDHRVREghVzRm9oWZFua3jDG_uBkTt8,8978
 visiongraph/input/AzureKinectInput.py,sha256=Y1I67MWMy_V_CRQ3Z6LGz9qdC8r1AsE3r23xmTKN3os,17144
 visiongraph/input/BaseCamera.py,sha256=OwRLEdEsIoQ6P8xKp67MA8o_ikbdIHzrFE_EZIPQzfQ,3855
 visiongraph/input/BaseDepthCamera.py,sha256=rfr5-DFOBL-80IjqhQmdpYr5xqKtN0nve02lZeOFnRg,3338
 visiongraph/input/BaseDepthInput.py,sha256=3nYxcLLLlPkT4oPewVQJMcLf8u3J5AABEW84qICY0mA,1309
 visiongraph/input/BaseInput.py,sha256=hgqyiHo5gz3yq91uzG-newww4DoNnbOnbnQK4LMpny0,4232
 visiongraph/input/CamGearInput.py,sha256=FVQROPlYVpVrk2CqVeg_bT83Z0UkN9GsCLj0lUg2SZs,1721
-visiongraph/input/DepthAIBaseInput.py,sha256=kZDL8GxvqRlWOqyhyTNagxNfGnsV2PWJrOl48OxSBuU,9093
+visiongraph/input/DepthAIBaseInput.py,sha256=5_VIKvEO-PoZfyL_LM19i5D3t3DPaWhBZBtuOKAf3W8,9264
 visiongraph/input/ImageInput.py,sha256=fX0sJcNcnd3vnu_2_23kpFPNr7goJNy7OIeERznTEjI,1493
 visiongraph/input/Oak1Input.py,sha256=r-39MqsuO-Rz2uzVjMzrX4o5eQlQd8PZpy7ofzWx5IA,295
-visiongraph/input/OakDInput.py,sha256=bCXdKad-H0eBrc7n5xFBbI2NLnxJ0w-ZtjQAij1Gjh4,7425
+visiongraph/input/OakDInput.py,sha256=l0nnbulvlpJeAZ1ncgli_MAutqv7a9Ik3ruAB5ynBMI,8024
 visiongraph/input/RealSenseInput.py,sha256=nTdtRhbPooVjUVuWJiKQJCcROjFIanAi13YVZKGJJvY,19468
 visiongraph/input/VideoCaptureInput.py,sha256=hupeQtiYPJyc610Hjpp8PleWQ1_Guvx0Xkvw0bOgmmE,5032
 visiongraph/input/ZEDInput.py,sha256=beNTRayN8OC3LtcrA4jyTBF3Fijax4eQsPuIKzdNJpI,4814
 visiongraph/input/__init__.py,sha256=SwVIv9t_VpDRy5NwzSH6stRdPsOhX1VCXXAscvg1Q-Y,1744
 visiongraph/model/CameraIntrinsics.py,sha256=8VwgAVjwMocZqoRsbpd2beYetI3_v1WpcM9jppirUJw,1887
 visiongraph/model/CameraStreamType.py,sha256=lYCzWM6blHEi_cwKo2U-xx8LqOIfbjvUJyAppRRTXMg,101
 visiongraph/model/DepthBuffer.py,sha256=YjvpgAIcdoXVB2QdVzZxplDlkW0lX1mxdgAPukc9E2Y,549
@@ -278,12 +278,12 @@
 visiongraph/util/OSUtils.py,sha256=B1wbK-iZMKCfVJf1j_0GGAror2VEDto1aikTBnhcEqw,195
 visiongraph/util/OpenVinoUtils.py,sha256=S5GPTf60VXCG9Sa5cnrvmxN1C6GDK9PqjjAqwqlLM5s,700
 visiongraph/util/PoseUtils.py,sha256=5G5YR5ywZ8k4mMGITQfXABCO3cA3O_gGUlDuIwXAsmI,5639
 visiongraph/util/ResultUtils.py,sha256=YZeBTTVI8qdT-cvQtdA31S5PTXXUTN5nZXkLs5ogUf0,1608
 visiongraph/util/TimeUtils.py,sha256=IVCLc5PtSYhmGB8hp6-oZMceva8Y6uU0K_1BDTZXGmE,2102
 visiongraph/util/VectorUtils.py,sha256=XA-DeZeruN7Sbzld7Knj-Ne4HOh2r7gK6k31nnm1EK8,3643
 visiongraph/util/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-visiongraph-0.1.57.dist-info/METADATA,sha256=xTlbVA1s_7I_si0q6IWICDCPqiqal7FGDzx2vIrLucI,11923
-visiongraph-0.1.57.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-visiongraph-0.1.57.dist-info/entry_points.txt,sha256=eqGnTHtEVMYwfXVk1Z9MhC8O2N8wuqAbr0lisLmrkxs,20
-visiongraph-0.1.57.dist-info/top_level.txt,sha256=rMp8bfRr_CcL2T8juTpUUszIVf1_BFmagl0lhq3L16o,12
-visiongraph-0.1.57.dist-info/RECORD,,
+visiongraph-0.1.57.1.dist-info/METADATA,sha256=p6jh06EhiOE6MZ6GbnahjwSKpNPn4rHEEEgwlGm6_pc,11925
+visiongraph-0.1.57.1.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+visiongraph-0.1.57.1.dist-info/entry_points.txt,sha256=eqGnTHtEVMYwfXVk1Z9MhC8O2N8wuqAbr0lisLmrkxs,20
+visiongraph-0.1.57.1.dist-info/top_level.txt,sha256=rMp8bfRr_CcL2T8juTpUUszIVf1_BFmagl0lhq3L16o,12
+visiongraph-0.1.57.1.dist-info/RECORD,,
```

