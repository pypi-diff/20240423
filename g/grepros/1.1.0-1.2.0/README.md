# Comparing `tmp/grepros-1.1.0-py2.py3-none-any.whl.zip` & `tmp/grepros-1.2.0-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,34 +1,34 @@
-Zip file size: 161060 bytes, number of entries: 32
--rw-rw-r--  2.0 unx      533 b- defN 24-Jan-03 18:00 grepros/__init__.py
--rw-rw-r--  2.0 unx      452 b- defN 24-Jan-03 17:27 grepros/__main__.py
--rw-rw-r--  2.0 unx    47295 b- defN 24-Jan-03 17:27 grepros/api.py
--rw-rw-r--  2.0 unx    44216 b- defN 24-Jan-03 17:27 grepros/common.py
--rw-rw-r--  2.0 unx    54752 b- defN 24-Jan-03 17:27 grepros/inputs.py
--rw-rw-r--  2.0 unx    19032 b- defN 24-Jan-03 17:27 grepros/library.py
--rw-rw-r--  2.0 unx    25352 b- defN 24-Jan-03 17:27 grepros/main.py
--rw-rw-r--  2.0 unx    47055 b- defN 24-Jan-03 17:27 grepros/outputs.py
--rw-rw-r--  2.0 unx    32248 b- defN 24-Jan-03 17:27 grepros/ros1.py
--rw-rw-r--  2.0 unx    44435 b- defN 24-Jan-03 17:27 grepros/ros2.py
--rw-rw-r--  2.0 unx    22214 b- defN 24-Jan-03 17:27 grepros/search.py
--rw-rw-r--  2.0 unx    11626 b- defN 24-Jan-03 17:27 grepros/plugins/__init__.py
--rw-rw-r--  2.0 unx    12480 b- defN 24-Jan-03 17:27 grepros/plugins/embag.py
--rw-rw-r--  2.0 unx    31550 b- defN 24-Jan-03 17:27 grepros/plugins/mcap.py
--rw-rw-r--  2.0 unx    25155 b- defN 24-Jan-03 17:27 grepros/plugins/parquet.py
--rw-rw-r--  2.0 unx    12099 b- defN 24-Jan-03 17:27 grepros/plugins/sql.py
--rw-rw-r--  2.0 unx      426 b- defN 24-Jan-03 17:27 grepros/plugins/auto/__init__.py
--rw-rw-r--  2.0 unx    12009 b- defN 24-Jan-03 17:27 grepros/plugins/auto/csv.py
--rw-rw-r--  2.0 unx    16875 b- defN 24-Jan-03 17:27 grepros/plugins/auto/dbbase.py
--rw-rw-r--  2.0 unx    11885 b- defN 24-Jan-03 17:27 grepros/plugins/auto/html.py
--rw-rw-r--  2.0 unx    42477 b- defN 24-Jan-03 17:27 grepros/plugins/auto/html.tpl
--rw-rw-r--  2.0 unx    10151 b- defN 24-Jan-03 17:27 grepros/plugins/auto/postgres.py
--rw-rw-r--  2.0 unx    32550 b- defN 24-Jan-03 17:27 grepros/plugins/auto/sqlbase.py
--rw-rw-r--  2.0 unx    11173 b- defN 24-Jan-03 17:27 grepros/plugins/auto/sqlite.py
--rw-rw-r--  2.0 unx        0 b- defN 24-Jan-03 17:27 grepros/vendor/__init__.py
--rw-rw-r--  2.0 unx     7750 b- defN 24-Jan-03 17:27 grepros/vendor/step.py
--rwxrwxr-x  2.0 unx      485 b- defN 24-Jan-03 18:00 grepros-1.1.0.data/scripts/grepros
--rw-rw-r--  2.0 unx     1523 b- defN 24-Jan-03 18:00 grepros-1.1.0.dist-info/LICENSE.md
--rw-rw-r--  2.0 unx      351 b- defN 24-Jan-03 18:00 grepros-1.1.0.dist-info/METADATA
--rw-rw-r--  2.0 unx      110 b- defN 24-Jan-03 18:00 grepros-1.1.0.dist-info/WHEEL
--rw-rw-r--  2.0 unx        8 b- defN 24-Jan-03 18:00 grepros-1.1.0.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     2568 b- defN 24-Jan-03 18:00 grepros-1.1.0.dist-info/RECORD
-32 files, 580835 bytes uncompressed, 157028 bytes compressed:  73.0%
+Zip file size: 182229 bytes, number of entries: 32
+-rw-rw-r--  2.0 unx      533 b- defN 24-Apr-23 17:22 grepros/__init__.py
+-rw-rw-r--  2.0 unx      452 b- defN 24-Apr-23 17:22 grepros/__main__.py
+-rw-rw-r--  2.0 unx    47674 b- defN 24-Apr-23 17:22 grepros/api.py
+-rw-rw-r--  2.0 unx    54073 b- defN 24-Apr-23 17:22 grepros/common.py
+-rw-rw-r--  2.0 unx    67446 b- defN 24-Apr-23 17:22 grepros/inputs.py
+-rw-rw-r--  2.0 unx    20353 b- defN 24-Apr-23 17:22 grepros/library.py
+-rw-rw-r--  2.0 unx    21689 b- defN 24-Apr-23 17:22 grepros/main.py
+-rw-rw-r--  2.0 unx    49897 b- defN 24-Apr-23 17:22 grepros/outputs.py
+-rw-rw-r--  2.0 unx    32696 b- defN 24-Apr-23 17:22 grepros/ros1.py
+-rw-rw-r--  2.0 unx    45297 b- defN 24-Apr-23 17:22 grepros/ros2.py
+-rw-rw-r--  2.0 unx    42847 b- defN 24-Apr-23 17:22 grepros/search.py
+-rw-rw-r--  2.0 unx    11626 b- defN 24-Apr-23 17:22 grepros/plugins/__init__.py
+-rw-rw-r--  2.0 unx    12480 b- defN 24-Apr-23 17:22 grepros/plugins/embag.py
+-rw-rw-r--  2.0 unx    31985 b- defN 24-Apr-23 17:22 grepros/plugins/mcap.py
+-rw-rw-r--  2.0 unx    25864 b- defN 24-Apr-23 17:22 grepros/plugins/parquet.py
+-rw-rw-r--  2.0 unx    12200 b- defN 24-Apr-23 17:22 grepros/plugins/sql.py
+-rw-rw-r--  2.0 unx      426 b- defN 24-Apr-23 17:22 grepros/plugins/auto/__init__.py
+-rw-rw-r--  2.0 unx    12221 b- defN 24-Apr-23 17:22 grepros/plugins/auto/csv.py
+-rw-rw-r--  2.0 unx    16945 b- defN 24-Apr-23 17:22 grepros/plugins/auto/dbbase.py
+-rw-rw-r--  2.0 unx    12231 b- defN 24-Apr-23 17:22 grepros/plugins/auto/html.py
+-rw-rw-r--  2.0 unx    43062 b- defN 24-Apr-23 17:22 grepros/plugins/auto/html.tpl
+-rw-rw-r--  2.0 unx    10151 b- defN 24-Apr-23 17:22 grepros/plugins/auto/postgres.py
+-rw-rw-r--  2.0 unx    32589 b- defN 24-Apr-23 17:22 grepros/plugins/auto/sqlbase.py
+-rw-rw-r--  2.0 unx    11326 b- defN 24-Apr-23 17:22 grepros/plugins/auto/sqlite.py
+-rw-rw-r--  2.0 unx        0 b- defN 24-Apr-23 17:22 grepros/vendor/__init__.py
+-rw-rw-r--  2.0 unx     7750 b- defN 24-Apr-23 17:22 grepros/vendor/step.py
+-rw-rw-r--  2.0 unx     1523 b- defN 24-Apr-23 17:31 grepros-1.2.0.dist-info/LICENSE.md
+-rw-rw-r--  2.0 unx    37074 b- defN 24-Apr-23 17:31 grepros-1.2.0.dist-info/METADATA
+-rw-rw-r--  2.0 unx      110 b- defN 24-Apr-23 17:31 grepros-1.2.0.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       46 b- defN 24-Apr-23 17:31 grepros-1.2.0.dist-info/entry_points.txt
+-rw-rw-r--  2.0 unx        8 b- defN 24-Apr-23 17:31 grepros-1.2.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     2575 b- defN 24-Apr-23 17:31 grepros-1.2.0.dist-info/RECORD
+32 files, 665149 bytes uncompressed, 178185 bytes compressed:  73.2%
```

## zipnote {}

```diff
@@ -72,26 +72,26 @@
 
 Filename: grepros/vendor/__init__.py
 Comment: 
 
 Filename: grepros/vendor/step.py
 Comment: 
 
-Filename: grepros-1.1.0.data/scripts/grepros
+Filename: grepros-1.2.0.dist-info/LICENSE.md
 Comment: 
 
-Filename: grepros-1.1.0.dist-info/LICENSE.md
+Filename: grepros-1.2.0.dist-info/METADATA
 Comment: 
 
-Filename: grepros-1.1.0.dist-info/METADATA
+Filename: grepros-1.2.0.dist-info/WHEEL
 Comment: 
 
-Filename: grepros-1.1.0.dist-info/WHEEL
+Filename: grepros-1.2.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: grepros-1.1.0.dist-info/top_level.txt
+Filename: grepros-1.2.0.dist-info/top_level.txt
 Comment: 
 
-Filename: grepros-1.1.0.dist-info/RECORD
+Filename: grepros-1.2.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## grepros/__init__.py

```diff
@@ -2,17 +2,17 @@
 """
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     31.10.2021
-@modified    03.01.2024
+@modified    23.04.2024
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros
 __title__        = "grepros"
-__version__      = "1.1.0"
-__version_info__ = (1, 1, 0)
-__version_date__ = "03.01.2024"
+__version__      = "1.2.0"
+__version_info__ = (1, 2, 0)
+__version_date__ = "23.04.2024"
 
 from . library import *
```

## grepros/api.py

```diff
@@ -4,15 +4,15 @@
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS1 bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     01.11.2021
-@modified    29.12.2023
+@modified    22.04.2024
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.api
 import abc
 import collections
 import datetime
 import decimal
@@ -511,15 +511,15 @@
     def make(cls, msg, topic=None, source=None, root=None, data=None):
         """
         Returns TypeMeta instance, registering message in cache if not present.
 
         Other parameters are only required for first registration.
 
         @param   topic   topic the message is in if root message
-        @param   source  message source like TopicSource or Bag,
+        @param   source  message source like LiveSource or Bag,
                          for looking up message type metadata
         @param   root    root message that msg is a nested value of, if any
         @param   data    message serialized binary, if any
         """
         msgid = id(msg)
         if msgid not in cls._CACHE:
             cls._CACHE[msgid] = TypeMeta(msg, topic, data)
@@ -540,15 +540,15 @@
         cls.sweep()
 
     @classmethod
     def sweep(cls):
         """Discards stale or surplus messages from cache."""
         if cls.POPULATION > 0 and len(cls._CACHE) > cls.POPULATION:
             count = len(cls._CACHE) - cls.POPULATION
-            for msgid, tm in sorted(x[::-1] for x in cls._TIMINGS.items())[:count]:
+            for msgid, tm in sorted(x[::-1] for x in list(cls._TIMINGS.items()))[:count]:
                 cls._CACHE.pop(msgid, None), cls._TIMINGS.pop(msgid, None)
                 for childid in cls._CHILDREN.pop(msgid, []):
                     cls._CACHE.pop(childid, None), cls._TIMINGS.pop(childid, None)
 
         now = time.time()
         if cls.LIFETIME <= 0 or cls._LASTSWEEP < now - cls.LIFETIME: return
 
@@ -586,22 +586,22 @@
     """
     Initializes ROS bindings, returns whether ROS environment set, prints or raises error if not.
 
     @param   live  whether environment must support launching a ROS node
     """
     global realapi, BAG_EXTENSIONS, SKIP_EXTENSIONS, ROS1, ROS2, ROS_VERSION, ROS_FAMILY, \
            ROS_COMMON_TYPES, ROS_TIME_TYPES, ROS_TIME_CLASSES, ROS_ALIAS_TYPES
-    if realapi:
+    if realapi and not live:
         return True
 
     success, version = False, os.getenv("ROS_VERSION")
     if "1" == version:
         from . import ros1
         realapi = ros1
-        success = realapi.validate()
+        success = realapi.validate(live)
         ROS1, ROS2, ROS_VERSION, ROS_FAMILY = True, False, 1, "rospy"
     elif "2" == version:
         from . import ros2
         realapi = ros2
         success = realapi.validate(live)
         ROS1, ROS2, ROS_VERSION, ROS_FAMILY = False, True, 2, "rclpy"
     elif not version:
@@ -623,15 +623,15 @@
 def calculate_definition_hash(typename, msgdef, extradefs=()):
     """
     Returns MD5 hash for message type definition.
 
     @param   extradefs  additional subtype definitions as ((typename, msgdef), )
     """
     # "type name (= constvalue)?" or "type name (defaultvalue)?" (ROS2 format)
-    FIELD_RGX = re.compile(r"^([a-z][^\s:]+)\s+([^\s=]+)(\s*=\s*([^\n]+))?(\s+([^\n]+))?", re.I)
+    FIELD_RGX = re.compile(r"^\s*([a-z][^\s:]+)\s+([^\s=]+)(\s*=\s*([^\n]+))?(\s+([^\n]+))?", re.I)
     STR_CONST_RGX = re.compile(r"^w?string\s+([^\s=#]+)\s*=")
     lines, pkg = [], typename.rsplit("/", 1)[0]
     subtypedefs = dict(extradefs, **parse_definition_subtypes(msgdef))
     extradefs = tuple(subtypedefs.items())
 
     # First pass: write constants
     for line in msgdef.splitlines():
@@ -753,17 +753,23 @@
 
 
 def get_message_type(msg_or_cls):
     """Returns ROS message type name, like "std_msgs/Header"."""
     return realapi.get_message_type(msg_or_cls)
 
 
-def get_message_value(msg, name, typename):
-    """Returns object attribute value, with numeric arrays converted to lists."""
-    return realapi.get_message_value(msg, name, typename)
+def get_message_value(msg, name, typename=None, default=Ellipsis):
+    """
+    Returns object attribute value, with numeric arrays converted to lists.
+
+    @param   name      message attribute name
+    @param   typename  value ROS type name, for identifying byte arrays
+    @param   default   value to return if attribute does not exist; raises exception otherwise
+    """
+    return realapi.get_message_value(msg, name, typename, default)
 
 
 def get_rostime(fallback=False):
     """
     Returns current ROS time, as rospy.Time or rclpy.time.Time.
 
     @param   fallback  use wall time if node not initialized
@@ -825,15 +831,15 @@
 
 def iter_message_fields(msg, messages_only=False, flat=False, scalars=(), include=(), exclude=(),
                         top=()):
     """
     Yields ((nested, path), value, typename) from ROS message.
 
     @param   messages_only  whether to yield only values that are ROS messages themselves
-                            or lists of ROS messages, else will yield scalar and list values
+                            or lists of ROS messages, else will yield scalar and scalar list values
     @param   flat           recurse into lists of nested messages, ignored if `messages_only`
     @param   scalars        sequence of ROS types to consider as scalars, like ("time", duration")
     @param   include        [((nested, path), re.Pattern())] to require in field path, if any
     @param   exclude        [((nested, path), re.Pattern())] to reject in field path, if any
     @param   top            internal recursion helper
     """
     fieldmap = realapi.get_message_fields(msg)
@@ -885,14 +891,15 @@
 
     @param   stamp   converted to ROS timestamp if int/float/str/duration/datetime/timedelta/decimal
     @param   bag     an open bag to use for relative start/end time
     """
     shift = 0
     if is_ros_time(stamp):
         if "duration" != get_ros_time_category(stamp): return stamp
+        stamp = realapi.to_sec(stamp)
         shift = bag.get_start_time() if stamp >= 0 else bag.get_end_time()
     elif isinstance(stamp, datetime.datetime):
         stamp = time.mktime(stamp.timetuple()) + stamp.microsecond / 1E6
     elif isinstance(stamp, datetime.timedelta):
         stamp = stamp.total_seconds()
         shift = bag.get_start_time() if stamp >= 0 else bag.get_end_time()
     elif isinstance(stamp, (six.binary_type, six.text_type)):
@@ -910,15 +917,15 @@
     if numeric string with sign prefix, or timedelta, or ROS duration.
 
     @param   stamp   converted to ROS timestamp if int/float/str/duration/datetime/timedelta/decimal
     """
     shift = 0
     if is_ros_time(stamp):
         if "duration" != get_ros_time_category(stamp): return stamp
-        shift = time.time()
+        stamp, shift = realapi.to_sec(stamp), time.time()
     elif isinstance(stamp, datetime.datetime):
         stamp = time.mktime(stamp.timetuple()) + stamp.microsecond / 1E6
     elif isinstance(stamp, datetime.timedelta):
         stamp, shift = stamp.total_seconds(), time.time()
     elif isinstance(stamp, (six.binary_type, six.text_type)):
         sign = stamp[0] in ("+", b"+") if six.text_type(stamp[0]) in "+-" else None
         stamp, shift = float(stamp), (0 if sign is None else time.time())
@@ -997,23 +1004,23 @@
     """
     for name, typename in realapi.get_message_fields(msg).items():
         if name not in dct:
             continue  # for
         v, msgv = dct[name], realapi.get_message_value(msg, name, typename)
 
         if realapi.is_ros_message(msgv):
-            v = v if is_ros_time(v) and is_ros_time(msgv) else dict_to_message(v, msgv)
+            v = v if is_ros_message(v) else dict_to_message(v, msgv)
         elif isinstance(msgv, (list, tuple)):
             scalarname = realapi.scalar(typename)
             if scalarname in ROS_BUILTIN_TYPES:
                 cls = ROS_BUILTIN_CTORS[scalarname]
                 v = [x if isinstance(x, cls) else cls(x) for x in v]
             else:
                 cls = realapi.get_message_class(scalarname)
-                v = [dict_to_message(x, cls()) for x in v]
+                v = [x if realapi.is_ros_message(x) else dict_to_message(x, cls()) for x in v]
         else:
             v = type(msgv)(v)
 
         setattr(msg, name, v)
     return msg
 
 
@@ -1027,15 +1034,15 @@
     @param   typename  ROS message type name, like "my_pkg/MyCls"
     @param   typedef   ROS message definition, like "Header header\nbool a\nMyCls2 b"
     @return            ordered {field name: type name},
                        like {"header": "std_msgs/Header", "a": "bool", "b": "my_pkg/MyCls2"}
     """
     result = collections.OrderedDict()  # {subtypename: subtypedef}
 
-    FIELD_RGX = re.compile(r"^([a-z][^\s:]+)\s+([^\s=]+)(\s*=\s*([^\n]+))?(\s+([^\n]+))?", re.I)
+    FIELD_RGX = re.compile(r"^\s*([a-z][^\s:]+)\s+([^\s=]+)(\s*=\s*([^\n]+))?(\s+([^\n]+))?", re.I)
     STR_CONST_RGX = re.compile(r"^w?string\s+([^\s=#]+)\s*=")
     pkg = typename.rsplit("/", 1)[0]
     for line in filter(bool, typedef.splitlines()):
         if set(line) == set("="):  # Subtype separator
             break  # for line
         if "#" in line and not STR_CONST_RGX.match(line): line = line[:line.index("#")]
         match = FIELD_RGX.match(line)
@@ -1077,15 +1084,15 @@
             curtype, curlines = m.group(4), []
         elif not m and curtype:  # Definition content
             curlines.append(line)
     if curtype:
         result[curtype] = "\n".join(curlines)
 
     # "type name (= constvalue)?" or "type name (defaultvalue)?" (ROS2 format)
-    FIELD_RGX = re.compile(r"^([a-z][^\s]+)\s+([^\s=]+)(\s*=\s*([^\n]+))?(\s+([^\n]+))?", re.I)
+    FIELD_RGX = re.compile(r"^\s*([a-z][^\s]+)\s+([^\s=]+)(\s*=\s*([^\n]+))?(\s+([^\n]+))?", re.I)
     # Concatenate nested subtype definitions to parent subtype definitions
     for subtype, subdef in list(result.items()):
         pkg, seen = subtype.rsplit("/", 1)[0], set()
         for line in subdef.splitlines():
             m = FIELD_RGX.match(line)
             if m and m.group(1):
                 scalartype, fulltype = realapi.scalar(m.group(1)), None
```

## grepros/common.py

```diff
@@ -4,15 +4,15 @@
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS1 bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     23.10.2021
-@modified    28.12.2023
+@modified    22.04.2024
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.common
 from __future__ import print_function
 import argparse
 import copy
 import datetime
@@ -272,14 +272,193 @@
         try: text, fmted = (text % kwargs if kwargs else text), fmted or bool(kwargs)
         except Exception: pass
         try: text = text.format(*args, **kwargs) if not fmted and (args or kwargs) else text
         except Exception: pass
         return text
 
 
+
+class ArgumentUtil(object):
+    """Namespace for program argument handling."""
+
+    UNSIGNED_INTS       = {"NTH_MESSAGE", "NTH_MATCH", "MAX_COUNT", "MAX_PER_TOPIC", "MAX_TOPICS",
+                           "BEFORE", "AFTER", "CONTEXT", "LINES_AROUND_MATCH", "MAX_FIELD_LINES",
+                           "MAX_MESSAGE_LINES", "WRAP_WIDTH", "QUEUE_SIZE_IN", "QUEUE_SIZE_OUT"}
+    UNSIGNED_FLOATS     = {"NTH_INTERVAL", "TIME_SCALE"}
+    SIGNED_INTS         = {"START_INDEX", "END_INDEX", "START_LINE", "END_LINE"}
+    STRINGS             = {"PUBLISH_PREFIX", "PUBLISH_SUFFIX", "PUBLISH_FIXNAME"}
+    STRING_COLLECTIONS  = {"TOPIC", "SKIP_TOPIC", "TYPE", "SKIP_TYPE", "SELECT_FIELD",
+                           "NO_SELECT_FIELD", "EMIT_FIELD", "NO_EMIT_FIELD", "MATCH_WRAPPER"}
+    NO_FLATTENS         = {"WRITE"}
+
+    UNSIGNED_WHEN       = {"START_INDEX": "LIVE", "END_INDEX": "LIVE"}
+    PRECASTS            = {"NTH_INTERVAL": lambda v: import_item("grepros.api").to_sec(v)}
+    DEDUPE_UNLESS       = {"PATTERN": "EXPRESSION"}
+
+
+    class HelpFormatter(argparse.RawTextHelpFormatter):
+        """RawTextHelpFormatter returning custom metavar for non-flattenable list arguments."""
+
+        def _format_action_invocation(self, action):
+            """Returns formatted invocation."""
+            # Avoids oververbose duplicate output like:
+            # --write TARGET [format=bag] [KEY=VALUE ...] [TARGET [format=bag] [KEY=VALUE ...] ...]
+            if action.dest in ArgumentUtil.NO_FLATTENS:
+                return " ".join(action.option_strings + [action.metavar])
+            return super(ArgumentUtil.HelpFormatter, self)._format_action_invocation(action)
+
+
+    @classmethod
+    def make_parser(cls, arguments, formatter=HelpFormatter):
+        """
+        Returns a configured ArgumentParser instance for program arguments.
+
+        @param  arguments  argparse options as {description, epilog, arguments: [], groups: []}
+        @param  formatter  help formatter class to use
+        """
+        kws = dict(description=arguments["description"], epilog=arguments["epilog"],
+                   formatter_class=formatter, add_help=False)
+        if sys.version_info >= (3, 5): kws.update(allow_abbrev=False)
+        argparser = argparse.ArgumentParser(**kws)
+        for arg in map(dict, arguments["arguments"]):
+            argparser.add_argument(*arg.pop("args"), **arg)
+        for group, groupargs in arguments.get("groups", {}).items():
+            grouper = argparser.add_argument_group(group)
+            for arg in map(dict, groupargs):
+                grouper.add_argument(*arg.pop("args"), **arg)
+        return argparser
+
+
+    @classmethod
+    def validate(cls, args, cli=False):
+        """
+        Converts and validates program argument namespace, prints and raises on error.
+
+        Returns new namespace with arguments in expected type and form.
+        """
+        args = cls.flatten(args)
+        cls.transform(args, cli)
+        if not cls.verify(args):
+            raise Exception("Invalid arguments")
+        return args
+
+
+    @classmethod
+    def flatten(cls, args):
+        """Returns new program argument namespace with list values flattened and deduplicated."""
+        args = structcopy(args)
+        for k, v in vars(args).items():
+            if not isinstance(v, list): continue  # for k, v
+            v2 = v
+            if k not in cls.NO_FLATTENS:
+                v2 = [x for xx in v2 for x in (xx if isinstance(xx, list) else [xx])]
+            if k not in cls.DEDUPE_UNLESS or not getattr(args, cls.DEDUPE_UNLESS[k], True):
+                v2 = [here.append(x) or x for here in ([],) for x in v2 if x not in here]
+            if v2 != v: setattr(args, k, v2)
+        return args
+
+
+    @classmethod
+    def transform(cls, args, cli=False):
+        """Sets command-line specific flag state to program argument namespace."""
+        if not cli: return
+
+        if args.CONTEXT:
+            args.BEFORE = args.AFTER = args.CONTEXT
+
+        # Show progress bar only if no console output
+        args.PROGRESS = args.PROGRESS and not args.CONSOLE
+
+        # Default to printing metadata for publish/write if no console output and no progress
+        args.VERBOSE = False if args.SKIP_VERBOSE else args.VERBOSE or \
+                       (False if args.PROGRESS else cli and not args.CONSOLE)
+
+        # Print filename prefix on each console message line if not single specific file
+        args.LINE_PREFIX = args.LINE_PREFIX and (args.RECURSE or len(args.FILE) != 1
+                                                 or args.PATH or any("*" in x for x in args.FILE))
+
+        for k, v in [("START_TIME", args.START_TIME), ("END_TIME", args.END_TIME)]:
+            if not isinstance(v, (six.binary_type, six.text_type)): continue  # for v, k
+            try: v = float(v)
+            except Exception: pass  # If numeric, retain string for source to read as relative time
+            try: v if isinstance(v, float) else setattr(args, k, parse_datetime(v))
+            except Exception: pass
+        
+
+    @classmethod
+    def verify(cls, args):
+        """
+        Validates arguments, prints errors, returns success.
+
+        @param   args  arguments object like argparse.Namespace
+        """
+        errors = []
+
+        # Validate --write .. key=value
+        for opts in getattr(args, "WRITE", []):  # List of lists, one for each --write
+            erropts = []
+            for opt in opts[1:]:
+                try: dict([opt.split("=", 1)])
+                except Exception: erropts.append(opt)
+            if erropts:
+                errors.append('Invalid KEY=VALUE in "--write %s": %s' %
+                              (" ".join(opts), " ".join(erropts)))
+
+        for n in ("START_TIME", "END_TIME"):
+            v = getattr(args, n, None)
+            if v is None: continue  # for v, n
+            try: v = float(v)
+            except Exception: pass
+            try: isinstance(v, (six.binary_type, six.text_type)) and parse_datetime(v)
+            except Exception: errors.append("Invalid ISO datetime for %s: %s" %
+                                            (n.lower().replace("_", " "), v))
+
+        errors.extend(cls.process_types(args))
+
+        for err in errors: ConsolePrinter.log(logging.ERROR, err)
+        return not errors
+
+
+    @classmethod
+    def process_types(cls, args):
+        """Converts and validates types in argument namespace, returns list of errors, if any."""
+        def cast(v, ctor):
+            try: return ctor(v), None
+            except Exception as e: return v, e
+
+        vals1, vals2, errors = vars(args), {}, {}
+
+        for k, f in cls.PRECASTS.items():
+            if vals1.get(k) is not None: vals1[k] = f(vals1[k])
+        for k, v in vals1.items():
+            if v is None: continue  # for k, v
+            err = None
+            if k in cls.UNSIGNED_INTS | cls.SIGNED_INTS: v, err = cast(v, int)
+            elif k in cls.UNSIGNED_FLOATS:               v, err = cast(v, float)
+            elif k in cls.STRINGS:                       v = str(v)
+            elif k in cls.STRING_COLLECTIONS:
+                v = [str(x) for x in (v if isinstance(v, (dict, list, set, tuple)) else [v])]
+            if not err and k in cls.UNSIGNED_INTS | cls.UNSIGNED_FLOATS and v < 0:
+                err = "Cannot be negative."
+            if not err and vals1.get(cls.UNSIGNED_WHEN.get(k)) and v < 0:
+                label = cls.UNSIGNED_WHEN[k].lower().replace("_", " ")
+                err = "Cannot be negative for %s." % label
+            (errors if err else vals2)[k] = err or v
+
+        error_texts = []
+        for k, err in errors.items():
+            text = "Invalid value for %s: %s" % (k.lower().replace("_", " "), getattr(args, k))
+            if isinstance(err, six.string_types): text += ". %s" % err
+            error_texts.append(text)
+        for k, v in vals2.items() if not errors else ():
+            setattr(args, k, v)
+        return error_texts
+
+
+
 class Decompressor(object):
     """Decompresses zstandard archives."""
 
     ## Supported archive extensions
     EXTENSIONS = (".zst", ".zstd")
 
     ## zstd file header magic start bytes
@@ -353,51 +532,64 @@
     '[---------/   36%            ] Progressing text..'.
     or for pulse mode
     '[    ----                    ] Progressing text..'.
     """
 
     def __init__(self, max=100, value=0, min=0, width=30, forechar="-",
                  backchar=" ", foreword="", afterword="", interval=1,
-                 pulse=False, aftertemplate=" {afterword}"):
+                 pulse=False, aftertemplate=" {afterword}", **afterargs):
         """
         Creates a new progress bar, without drawing it yet.
 
         @param   max            progress bar maximum value, 100%
         @param   value          progress bar initial value
         @param   min            progress bar minimum value, for 0%
         @param   width          progress bar width (in characters)
         @param   forechar       character used for filling the progress bar
         @param   backchar       character used for filling the background
         @param   foreword       text in front of progress bar
         @param   afterword      text after progress bar
         @param   interval       ticker thread interval, in seconds
         @param   pulse          ignore value-min-max, use constant pulse instead
-        @param   aftertemplate  afterword format() template, populated with vars(self)
+        @param   aftertemplate  afterword format() template, populated with vars(self) and afterargs
+        @param   afterargs      additional keywords for aftertemplate formatting
         """
         threading.Thread.__init__(self)
-        for k, v in locals().items(): setattr(self, k, v) if "self" != k else 0
-        afterword = aftertemplate.format(**vars(self))
-        self.daemon    = True   # Daemon threads do not keep application running
-        self.percent   = None   # Current progress ratio in per cent
-        self.value     = None   # Current progress bar value
-        self.pause     = False  # Whether drawing is currently paused
-        self.pulse_pos = 0      # Current pulse position
+        self.max           = max
+        self.value         = value
+        self.min           = min
+        self.width         = width
+        self.forechar      = forechar
+        self.backchar      = backchar
+        self.foreword      = foreword
+        self.afterword     = afterword
+        self.interval      = interval
+        self.pulse         = pulse
+        self.aftertemplate = aftertemplate
+        self.afterargs     = afterargs
+        self.daemon        = True   # Daemon threads do not keep application running
+        self.percent       = None   # Current progress ratio in per cent
+        self.value         = 0      # Current progress bar value
+        self.pause         = False  # Whether drawing is currently paused
+        self.pulse_pos     = 0      # Current pulse position
         self.bar = "%s[%s%s]%s" % (foreword,
                                    backchar if pulse else forechar,
                                    backchar * (width - 3),
-                                   afterword)
+                                   aftertemplate.format(**dict(vars(self), **self.afterargs)))
         self.printbar = self.bar   # Printable text, with padding to clear previous
         self.progresschar = itertools.cycle("-\\|/")
         self.is_running = False
 
 
     def update(self, value=None, draw=True, flush=False):
         """Updates the progress bar value, and refreshes by default; returns self."""
-        if value is not None: self.value = min(self.max, max(self.min, value))
-        afterword = self.aftertemplate.format(**vars(self))
+        if value is not None:
+            self.value = value if self.pulse else min(self.max, max(self.min, value))
+        args = dict(vars(self), **self.afterargs) if self.afterargs else vars(self)
+        afterword = self.aftertemplate.format(**args)
         w_full = self.width - 2
         if self.pulse:
             if self.pulse_pos is None:
                 bartext = "%s[%s]%s" % (self.foreword,
                                         self.forechar * (self.width - 2),
                                         afterword)
             else:
@@ -638,23 +830,23 @@
         if breakable:
             cur_line.append(text[:break_pos])
             reversed_chunks[-1] = text[break_pos:]
         elif not cur_line:
             cur_line.append(reversed_chunks.pop())
 
 
-
 def drop_zeros(v, replace=""):
-    """Drops or replaces trailing zeros and empty decimal separator, if any."""
-    return re.sub(r"\.?0+$", lambda x: len(x.group()) * replace, str(v))
+    """Drops trailing zeros and empty decimal separator, if any."""
+    repl = lambda m: ("." if m[1] or replace else "") + (m[1] or "") + len(m[2]) * replace
+    return re.sub(r"\.(\d*[1-9])?(0+)$", repl, str(v))
 
 
 def ellipsize(text, limit, ellipsis=".."):
     """Returns text ellipsized if beyond limit."""
-    if limit <= 0 or len(text) < limit:
+    if limit <= 0 or len(text) <= limit:
         return text
     return text[:max(0, limit - len(ellipsis))] + ellipsis
 
 
 def ensure_namespace(val, defaults=None, dashify=("WRITE_OPTIONS", ), **kwargs):
     """
     Returns a copy of value as `argparse.Namespace`, with all keys uppercase.
@@ -665,25 +857,25 @@
     @param  defaults  additional arguments to set to namespace if missing
     @param  dashify   names of dictionary arguments where to replace
                       the first underscore in string keys with a dash
     @param  kwargs    any and all argument overrides as keyword overrides
     """
     if val is None or isinstance(val, dict): val = argparse.Namespace(**val or {})
     else: val = structcopy(val)
-    for k, v in vars(val).items():
+    for k, v in list(vars(val).items()):
         if not k.isupper():
             delattr(val, k)
             setattr(val, k.upper(), v)
     for k, v in ((k.upper(), v) for k, v in (defaults.items() if defaults else ())):
         if not hasattr(val, k): setattr(val, k, structcopy(v))
     for k, v in ((k.upper(), v) for k, v in kwargs.items()): setattr(val, k, v)
     for k, v in ((k.upper(), v) for k, v in (defaults.items() if defaults else ())):
         if isinstance(v, (tuple, list)) and not isinstance(getattr(val, k), (tuple, list)):
             setattr(val, k, [getattr(val, k)])
-    for arg in (getattr(val, n, None) for n in dashify or ()):
+    for arg in (getattr(val, n.upper(), None) for n in dashify or ()):
         for k in (list(arg) if isinstance(arg, dict) else []):
             if isinstance(k, six.text_type) and "_" in k and 0 < k.index("_") < len(k) - 1:
                 arg[k.replace("_", "-", 1)] = arg.pop(k)
     return val
 
 
 def filter_dict(dct, keys=(), values=(), reverse=False):
@@ -708,48 +900,47 @@
         for v in (vv if is_array else [vv]):
             if  (k not in keys   and not any(p.match(k) for p in kpatterns)) \
             and (v not in values and not any(p.match(v) for p in vpatterns)):
                 result.setdefault(k, []).append(v) if is_array else result.update({k: v})
     return result
 
 
-def find_files(names=(), paths=(), extensions=(), skip_extensions=(), recurse=False):
+def find_files(names=(), paths=(), suffixes=(), skip_suffixes=(), recurse=False):
     """
     Yields filenames from current directory or given paths.
 
     Seeks only files with given extensions if names not given.
     Logs errors for names and paths not found.
 
-    @param   names            list of specific files to return (supports * wildcards)
-    @param   paths            list of paths to look under, if not using current directory
-    @param   extensions       list of extensions to select if not using names, as (".ext1", ..)
-    @param   skip_extensions  list of extensions to skip if not using names, as (".ext1", ..)
-    @param   recurse          whether to recurse into subdirectories
+    @param   names          list of specific files to return (supports * wildcards)
+    @param   paths          list of paths to look under, if not using current directory
+    @param   suffixes       list of suffixes to select if no wilcarded names, as (".ext1", ..)
+    @param   skip_suffixes  list of suffixes to skip if no wildcarded names, as (".ext1", ..)
+    @param   recurse        whether to recurse into subdirectories
     """
     namesfound, pathsfound = set(), set()
+    ok = lambda f: (not suffixes or any(map(f.endswith, suffixes))) \
+                   and not any(map(f.endswith, skip_suffixes))
     def iter_files(directory):
         """Yields matching filenames from path."""
         if os.path.isfile(directory):
             ConsolePrinter.log(logging.ERROR, "%s: Is a file", directory)
             return
-        for path in sorted(glob.glob(directory)):  # Expand * wildcards, if any
+        for root in sorted(glob.glob(directory)):  # Expand * wildcards, if any
             pathsfound.add(directory)
-            for n in names:
-                p = n if not paths or os.path.isabs(n) else os.path.join(path, n)
-                for f in (f for f in glob.glob(p) if "*" not in n
-                          or not any(map(f.endswith, skip_extensions))):
-                    if os.path.isdir(f):
-                        ConsolePrinter.log(logging.ERROR, "%s: Is a directory", f)
-                        continue  # for n
-                    namesfound.add(n)
-                    yield f
-            for root, _, files in os.walk(path) if not names else ():
-                for f in (os.path.join(root, f) for f in sorted(files)
-                          if (not extensions or any(map(f.endswith, extensions)))
-                          and not any(map(f.endswith, skip_extensions))):
+            for path, _, files in os.walk(root):
+                for n in names:
+                    p = n if not paths or os.path.isabs(n) else os.path.join(path, n)
+                    for f in (f for f in glob.glob(p) if "*" not in n or ok(f)):
+                        if os.path.isdir(f):
+                            ConsolePrinter.log(logging.ERROR, "%s: Is a directory", f)
+                            continue  # for f
+                        namesfound.add(n)
+                        yield f
+                for f in () if names else (os.path.join(root, f) for f in sorted(files) if ok(f)):
                     yield f
                 if not recurse:
                     break  # for root
 
     processed = set()
     for f in (f for p in paths or ["."] for f in iter_files(p)):
         if os.path.abspath(f) not in processed:
@@ -774,21 +965,22 @@
         f = "%d" % c if "sec" != n else drop_zeros(round(c, 9))
         if f != "0": items += [f + n]
     return " ".join(items or ["0sec"])
 
 
 def format_bytes(size, precision=2, inter=" ", strip=True):
     """Returns a formatted byte size (like 421.40 MB), trailing zeros optionally removed."""
-    result = "0 bytes"
-    if size:
-        UNITS = [("bytes", "byte")[1 == size]] + [x + "B" for x in "KMGTPEZY"]
+    result = "" if math.isinf(size) or math.isnan(size) else "0 bytes"
+    if size and result:
+        UNITS = ["bytes"] + [x + "B" for x in "KMGTPEZY"]
+        size, sign = abs(size), ("-" if size < 0 else "")
         exponent = min(int(math.log(size, 1024)), len(UNITS) - 1)
         result = "%.*f" % (precision, size / (1024. ** exponent))
-        result += "" if precision > 0 else "."  # Do not strip integer zeroes
-        result = (drop_zeros(result) if strip else result) + inter + UNITS[exponent]
+        if strip: result = drop_zeros(result)
+        result = sign + result + inter + (UNITS[exponent] if result != "1" or exponent else "byte")
     return result
 
 
 def format_stamp(stamp):
     """Returns ISO datetime from UNIX timestamp."""
     return datetime.datetime.fromtimestamp(stamp).isoformat(sep=" ")
 
@@ -800,28 +992,31 @@
     E.g. "my.thing" or "my.module.MyCls" or "my.module.MyCls.my_method"
     or "my.module.MyCls<0x1234abcd>" or "my.module.MyCls<0x1234abcd>.my_method".
     """
     namer = lambda x: getattr(x, "__qualname__", getattr(x, "__name__", ""))
     if inspect.ismodule(obj): return namer(obj)
     if inspect.isclass(obj):  return ".".join((obj.__module__, namer(obj)))
     if inspect.isroutine(obj):
-        parts, self = [], six.get_method_self(obj)
-        if self is not None:           parts.extend((get_name(self), obj.__name__))
-        elif hasattr(obj, "im_class"): parts.extend((get_name(obj.im_class), namer(obj)))  # Py2
-        else:                          parts.extend((obj.__module__, namer(obj)))          # Py3
+        parts = []
+        try: self = six.get_method_self(obj)
+        except Exception: self = None
+        if self is not None:             parts.extend((get_name(self), obj.__name__))
+        elif hasattr(obj, "im_class"):   parts.extend((get_name(obj.im_class), namer(obj)))  # Py2
+        elif hasattr(obj, "__module__"): parts.extend((obj.__module__, namer(obj)))
+        else:                            parts.append(namer(obj))
         return ".".join(parts)
     cls = type(obj)
     return "%s.%s<0x%x>" % (cls.__module__, namer(cls), id(obj))
 
 
 def has_arg(func, name):
     """Returns whether function supports taking specified argument by name."""
     spec = getattr(inspect, "getfullargspec", getattr(inspect, "getargspec", None))(func)  # Py3/Py2
     return name in spec.args or name in getattr(spec, "kwonlyargs", ()) or \
-           getattr(spec, "varkw", None) or getattr(spec, "keywords", None)
+           bool(getattr(spec, "varkw", None) or getattr(spec, "keywords", None))
 
 
 def import_item(name):
     """
     Returns imported module, or identifier from imported namespace; raises on error.
 
     @param   name  Python module name like "my.module"
@@ -888,20 +1083,21 @@
         if key not in cache:
             cache[key] = func(*args, **kwargs)
         return cache[key]
     return functools.update_wrapper(inner, func)
 
 
 def merge_dicts(d1, d2):
-    """Merges d2 into d1, recursively for nested dicts."""
+    """Merges d2 into d1, recursively for nested dicts, returns d1."""
     for k, v in d2.items():
         if k in d1 and isinstance(v, dict) and isinstance(d1[k], dict):
             merge_dicts(d1[k], v)
         else:
             d1[k] = v
+    return d1
 
 
 def merge_spans(spans, join_blanks=False):
     """
     Returns a sorted list of (start, end) spans with overlapping spans merged.
 
     @param   join_blanks  whether to merge consecutive zero-length spans,
@@ -929,15 +1125,15 @@
 
 def parse_datetime(text):
     """Returns datetime object from ISO datetime string (may be partial). Raises if invalid."""
     BASE = re.sub(r"\D", "", datetime.datetime.min.isoformat())  # "00010101000000"
     text = re.sub(r"\D", "", text)
     text += BASE[len(text):] if text else ""
     dt = datetime.datetime.strptime(text[:len(BASE)], "%Y%m%d%H%M%S")
-    return dt + datetime.timedelta(microseconds=int(text[len(BASE):] or "0"))
+    return dt + datetime.timedelta(microseconds=int(text[len(BASE):][:6] or "0"))
 
 
 def parse_number(value, suffixes=None):
     """
     Returns an integer parsed from text, raises on error.
 
     @param   value     text or binary string to parse, may contain abbrevations like "12K"
@@ -946,14 +1142,32 @@
     value, suffix = value.decode() if isinstance(value, six.binary_type) else value, None
     if suffixes:
         suffix = next((k for k, v in suffixes.items() if value.lower().endswith(k.lower())), None)
         value = value[:-len(suffix)] if suffix else value
     return int(float(value) * (suffixes[suffix] if suffix else 1))
 
 
+def path_to_regex(text, sep=".", wildcard="*", end=False, intify=False):
+    """
+    Returns re.Pattern for matching path strings with optional integer indexes.
+
+    @param   text      separated wildcarded path pattern like "foo*.bar"
+    @param   sep       path parts separator, optional
+    @param   wildcard  simple wildcard to convert to Python wildcard pattern, optional
+    @param   end       whether pattern should match until end (terminates with $)
+    @param   intify    whether path should match optional integer index between parts,
+                       like "foo.bar" as "foo(\.\d+)?\.bar"
+    """
+    pattern, split_wild = "", lambda x: x.split(wildcard) if wildcard else [x]
+    for i, part in enumerate(text.split(sep) if sep else [text]):
+        pattern += (r"(%s\d+)?" % re.escape(sep)) if i and intify else ""
+        pattern += (re.escape(sep) if i else "") + ".*".join(map(re.escape, split_wild(part)))
+    return re.compile(pattern + ("$" if end else ""), re.I)
+
+
 def plural(word, items=None, numbers=True, single="1", sep=",", pref="", suf=""):
     """
     Returns the word as 'count words', or '1 word' if count is 1,
     or 'words' if count omitted.
 
     @param   items      item collection or count,
                         or None to get just the plural of the word
@@ -961,20 +1175,20 @@
     @param   single     prefix to use for word if count is 1, e.g. "a"
     @param   sep        thousand-separator to use for count
     @param   pref       prefix to prepend to count, e.g. "~150"
     @param   suf        suffix to append to count, e.g. "150+"
     """
     count   = len(items) if hasattr(items, "__len__") else items or 0
     isupper = word[-1:].isupper()
-    suffix = "es" if word and word[-1:].lower() in "xyz" \
+    suffix = "es" if word and word[-1:].lower() in "sxyz" \
              and not word[-2:].lower().endswith("ay") \
              else "s" if word else ""
-    if isupper: suffix = suffix.upper()
     if count != 1 and "es" == suffix and "y" == word[-1:].lower():
         word = word[:-1] + ("I" if isupper else "i")
+    if isupper: suffix = suffix.upper()
     result = word + ("" if 1 == count else suffix)
     if numbers and items is not None:
         if 1 == count: fmtcount = single
         elif not count: fmtcount = "0"
         elif sep: fmtcount = "".join([
             x + (sep if i and not i % 3 else "") for i, x in enumerate(str(count)[::-1])
         ][::-1])
@@ -1054,24 +1268,25 @@
                 accum.append(parts.pop(0))
                 curpath = os.path.join(os.sep, accum[0] + os.sep, *accum[1:])  # Windows drive letter thing
                 if not os.path.exists(curpath):
                     os.mkdir(curpath)
                     paths_created.append(curpath)
         elif not present and "r" == mode:
             return False
+        op = " opening"
         with open(f, {"r": "rb", "w": "ab", "a": "ab+"}[mode]) as g:
             if mode in ("r", "a"):
                 op = " reading from"
                 result = isinstance(g.read(1), bytes)
             if result and mode in ("w", "a"):
                 op = " writing to"
                 result, _ = True, g.write(b"")
             return result
     except Exception as e:
-        ConsolePrinter.log(logging.ERROR, "Error%s %s: %s", f, e)
+        ConsolePrinter.log(logging.ERROR, "Error%s %s: %s", op, f, e)
         return False
     finally:
         if not present:
             try: os.remove(f)
             except Exception: pass
             for path in paths_created[::-1]:
                 try: os.rmdir(path)
@@ -1085,13 +1300,14 @@
     @param   end  whether pattern should match until end (adds $)
     """
     suff = "$" if end else ""
     return re.compile(".*".join(map(re.escape, text.split("*"))) + suff, re.I)
 
 
 __all__ = [
-    "PATH_TYPES", "ConsolePrinter", "Decompressor", "MatchMarkers", "ProgressBar", "TextWrapper",
-    "drop_zeros", "ellipsize", "ensure_namespace", "filter_dict", "find_files",
-    "format_bytes", "format_stamp", "format_timedelta", "get_name", "has_arg", "import_item",
-    "is_iterable", "is_stream", "makedirs", "memoize", "merge_dicts", "merge_spans",
-    "parse_datetime", "parse_number", "plural", "unique_path", "verify_io", "wildcard_to_regex",
+    "PATH_TYPES", "ArgumentUtil", "ConsolePrinter", "Decompressor", "LenIterable", "MatchMarkers",
+    "ProgressBar", "TextWrapper", "drop_zeros", "ellipsize", "ensure_namespace", "filter_dict",
+    "find_files", "format_bytes", "format_stamp", "format_timedelta", "get_name", "has_arg",
+    "import_item", "is_iterable", "is_stream", "makedirs", "memoize", "merge_dicts", "merge_spans",
+    "parse_datetime", "parse_number", "path_to_regex", "plural", "unique_path", "verify_io",
+    "wildcard_to_regex",
 ]
```

## grepros/inputs.py

```diff
@@ -4,70 +4,80 @@
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     23.10.2021
-@modified    30.12.2023
+@modified    20.04.2024
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.inputs
 from __future__ import print_function
 import collections
 import datetime
 import functools
 import itertools
 import os
 try: import queue  # Py3
 except ImportError: import Queue as queue  # Py2
 import re
 import threading
 import time
+import traceback
 
 import six
 
 from . import api
 from . import common
-from . common import ConsolePrinter, ensure_namespace, drop_zeros
+from . common import ArgumentUtil, ConsolePrinter, ensure_namespace, drop_zeros
 
 
 class Source(object):
     """Message producer base class."""
 
     ## Returned from read() as (topic name, ROS message, ROS timestamp object).
     class SourceMessage(api.Bag.BagMessage): pass
 
     ## Template for message metainfo line
     MESSAGE_META_TEMPLATE = "{topic} #{index} ({type}  {dt}  {stamp})"
 
     ## Constructor argument defaults
-    DEFAULT_ARGS = dict(START_TIME=None, END_TIME=None, UNIQUE=False, SELECT_FIELD=(),
-                        NOSELECT_FIELD=(), NTH_MESSAGE=1, NTH_INTERVAL=0)
+    DEFAULT_ARGS = dict(START_TIME=None, END_TIME=None, START_INDEX=None, END_INDEX=None,
+                        UNIQUE=False, SELECT_FIELD=(), NOSELECT_FIELD=(),
+                        NTH_MESSAGE=1, NTH_INTERVAL=0, PROGRESS=False)
 
     def __init__(self, args=None, **kwargs):
         """
         @param   args                   arguments as namespace or dictionary, case-insensitive
         @param   args.start_time        earliest timestamp of messages to read
         @param   args.end_time          latest timestamp of messages to read
         @param   args.unique            emit messages that are unique in topic
+        @param   args.start_index       message index within topic to start from
+        @param   args.end_index         message index within topic to stop at
         @param   args.select_field      message fields to use for uniqueness if not all
         @param   args.noselect_field    message fields to skip for uniqueness
-        @param   args.nth_message       read every Nth message in topic
-        @param   args.nth_interval      minimum time interval between messages in topic
+        @param   args.nth_message       read every Nth message in topic, starting from first
+        @param   args.nth_interval      minimum time interval between messages in topic,
+                                        as seconds or ROS duration
+        @param   args.progress          whether to print progress bar
         @param   kwargs                 any and all arguments as keyword overrides, case-insensitive
         """
         # {key: [(() if any field else ('nested', 'path') or re.Pattern, re.Pattern), ]}
         self._patterns = {}
         # {topic: ["pkg/MsgType", ]} searched in current source
         self._topics = collections.defaultdict(list)
         self._counts = collections.Counter()  # {(topic, typename, typehash): count processed}
         # {(topic, typename, typehash): (message hash over all fields used in matching)}
         self._hashes = collections.defaultdict(set)
-        self._processables = {}  # {(topic, typename, typehash): (index, stamp) of last processable}
+        self._processables  = {}  # {(topic, typename, typehash): (index, stamp) of last processable}
+        self._start_indexes = {}  # {(topic, typename, typehash): index to start producing from}
+        self._end_indexes   = {}  # {(topic, typename, typehash): index to stop producing at}
+        self._bar_args      = {}  # Progress bar options
+        self._status = None  # Processable/match status of last produced message
 
         self.args = ensure_namespace(args, Source.DEFAULT_ARGS, **kwargs)
         ## outputs.Sink instance bound to this source
         self.sink = None
         ## All topics in source, as {(topic, typenane, typehash): total message count or None}
         self.topics = {}
         ## ProgressBar instance, if any
@@ -94,26 +104,39 @@
     def read(self):
         """Yields messages from source, as (topic, msg, ROS time)."""
 
     def bind(self, sink):
         """Attaches sink to source"""
         self.sink = sink
 
+    def configure(self, args=None, **kwargs):
+        """
+        Updates source configuration.
+
+        @param   args    arguments as namespace or dictionary, case-insensitive
+        @param   kwargs  any and all arguments as keyword overrides, case-insensitive
+        """
+        self.args = ensure_namespace(args, vars(self.args), **kwargs)
+        self.valid = None
+
     def validate(self):
-        """Returns whether source prerequisites are met (like ROS environment for TopicSource)."""
-        if self.valid is None: self.valid = True
+        """Returns whether arguments are valid and source prerequisites are met."""
+        if self.valid is not None: return self.valid
+        try: self.args, self.valid = ArgumentUtil.validate(self.args), True
+        except Exception: self.valid = False
         return self.valid
 
     def close(self):
         """Shuts down input, closing any files or connections."""
         self.topics.clear()
         self._topics.clear()
         self._counts.clear()
         self._hashes.clear()
         self._processables.clear()
+        self._status = None
         if self.bar:
             self.bar.pulse_pos = None
             self.bar.update(flush=True).stop()
             self.bar = None
 
     def close_batch(self):
         """Shuts down input batch if any (like bagfile), else all input."""
@@ -152,48 +175,88 @@
         return api.get_message_definition(msg_or_type)
 
     def get_message_type_hash(self, msg_or_type):
         """Returns ROS message type MD5 hash."""
         return api.get_message_type_hash(msg_or_type)
 
     def is_processable(self, topic, msg, stamp, index=None):
-        """Returns whether message passes source filters."""
+        """Returns whether message passes source filters; registers status."""
         if self.args.START_TIME and stamp < self.args.START_TIME:
             return False
         if self.args.END_TIME and stamp > self.args.END_TIME:
             return False
-        if self.args.UNIQUE or self.args.NTH_MESSAGE > 1 or self.args.NTH_INTERVAL > 0:
+        if self.args.START_INDEX or self.args.END_INDEX \
+        or self.args.NTH_MESSAGE or self.args.UNIQUE:
             topickey = api.TypeMeta.make(msg, topic).topickey
+        if self.args.START_INDEX and index is not None:
+            if max(0, self._start_indexes.get(topickey, self.args.START_INDEX)) > index:
+                return False
+        if self.args.END_INDEX and index is not None:
+            if self._end_indexes.get(topickey, self.args.END_INDEX) < index:
+                return False
+        if self.args.NTH_MESSAGE > 1 or self.args.NTH_INTERVAL > 0:
             last_accepted = self._processables.get(topickey)
         if self.args.NTH_MESSAGE > 1 and last_accepted and index is not None:
-            if (index - 1) % self.args.NTH_MESSAGE:
+            shift = self.args.START_INDEX if (self.args.START_INDEX or 0) > 1 else 1
+            if (index - shift) % self.args.NTH_MESSAGE:
                 return False
         if self.args.NTH_INTERVAL > 0 and last_accepted and stamp is not None:
             if api.to_sec(stamp - last_accepted[1]) < self.args.NTH_INTERVAL:
                 return False
         if self.args.UNIQUE:
             include, exclude = self._patterns["select"], self._patterns["noselect"]
             msghash = api.make_message_hash(msg, include, exclude)
             if msghash in self._hashes[topickey]:
                 return False
             self._hashes[topickey].add(msghash)
+        self._status = True
         return True
 
     def notify(self, status):
         """Reports match status of last produced message."""
+        self._status = bool(status)
+        if self.bar and self._bar_args.get("source_value") is not None:
+            self.bar.update(self.bar.value + bool(status))
+
+    def configure_progress(self, **kwargs):
+        """Configures progress bar options, updates current bar if any."""
+        for k, v in kwargs.items():
+            if isinstance(self._bar_args.get(k), dict) and isinstance(v, dict):
+                self._bar_args[k].update(v)
+            else: self._bar_args[k] = v
+        if self.bar:
+            bar_attrs = set(k for k in vars(self.bar) if not k.startswith("_"))
+            for k, v in self._bar_args.items():
+                if k in bar_attrs: setattr(self.bar, k, v)
+                else: self.bar.afterargs[k] = v
+
+    def init_progress(self):
+        """Initializes progress bar, if any."""
+        if self.args.PROGRESS and not self.bar:
+            self.bar = common.ProgressBar(**self._bar_args)
+            self.bar.start() if self.bar.pulse else self.bar.update(value=0)
+
+    def update_progress(self, count, running=True):
+        """Updates progress bar, if any, with source processed count, pauses bar if not running."""
+        if self.bar:
+            if not running:
+                self.bar.pause, self.bar.pulse_pos = True, None
+            if self._bar_args.get("source_value") is not None:
+                self.bar.afterargs["source_value"] = count
+            else: self.bar.update(count)
 
     def thread_excepthook(self, text, exc):
         """Handles exception, used by background threads."""
         ConsolePrinter.error(text)
 
     def _parse_patterns(self):
         """Parses pattern arguments into re.Patterns."""
         selects, noselects = self.args.SELECT_FIELD, self.args.NOSELECT_FIELD
         for key, vals in [("select", selects), ("noselect", noselects)]:
-            self._patterns[key] = [(tuple(v.split(".")), common.wildcard_to_regex(v)) for v in vals]
+            self._patterns[key] = [(tuple(v.split(".")), common.path_to_regex(v)) for v in vals]
 
 
 class ConditionMixin(object):
     """
     Provides topic conditions evaluation.
 
     Evaluates a set of Python expressions, with a namespace of:
@@ -314,15 +377,14 @@
         # {(topic, typename, typehash): [.., last]}
         self._lastmsgs  = collections.defaultdict(collections.deque)
         # {topic: (max positive index + 1, max abs(negative index) or 1)}
         self._topic_limits = collections.defaultdict(lambda: [1, 1])
 
         ## {condition with <topic x> as get_topic("x"): compiled code object}
         self._conditions = collections.OrderedDict()
-        self._configure_conditions(ensure_namespace(args, ConditionMixin.DEFAULT_ARGS, **kwargs))
 
     def is_processable(self, topic, msg, stamp, index=None):
         """Returns whether message passes passes current state conditions, if any."""
         result = True
         if not self._conditions:
             return result
         for i, (expr, code) in enumerate(self._conditions.items()):
@@ -344,14 +406,33 @@
                 except Exception as e:
                     ConsolePrinter.error('Error evaluating condition "%s": %s', expr, e)
                     raise
                 if result: break  # for remaps
             if not result: break  # for i,
         return result
 
+    def validate(self):
+        """Returns whether conditions have valid syntax, sets options, prints errors."""
+        errors = []
+        for v in self.args.CONDITION:
+            v = self.TOPIC_RGX.sub("dummy", v)
+            try: compile(v, "", "eval")
+            except SyntaxError as e:
+                errors.append("'%s': %s at %schar %s" %
+                              (v, e.msg, "line %s " % e.lineno if e.lineno > 1 else "", e.offset))
+            except Exception as e:
+                errors.append("'%s': %s" % (v, e))
+        if errors:
+            ConsolePrinter.error("Invalid condition")
+            for err in errors:
+                ConsolePrinter.error("  %s" % err)
+        else:
+            self._configure_conditions(ensure_namespace(self.args, ConditionMixin.DEFAULT_ARGS))
+        return not errors
+
     def close_batch(self):
         """Clears cached messages."""
         self._firstmsgs.clear()
         self._lastmsgs.clear()
 
     def has_conditions(self):
         """Returns whether there are any conditions configured."""
@@ -402,14 +483,19 @@
         if topickey not in self._counts:
             return self.Empty()
         c, f, l = (d[topickey] for d in (self._counts, self._firstmsgs, self._lastmsgs))
         return self.Topic(c, f, l)
 
     def _configure_conditions(self, args):
         """Parses condition expressions and populates local structures."""
+        self._conditions.clear()
+        self._topic_limits.clear()
+        self._topic_states.clear()
+        self._wildcard_topics.clear()
+        del self._topics_per_condition[:]
         for v in args.CONDITION:
             topics = list(set(self.TOPIC_RGX.findall(v)))
             self._topic_states.update({t: True for t in topics})
             self._topics_per_condition.append(topics)
             for t in (t for t in topics if "*" in t):
                 self._wildcard_topics[t] = common.wildcard_to_regex(t, end=True)
             expr = self.TOPIC_RGX.sub(r'get_topic("\1")', v)
@@ -437,15 +523,15 @@
                     "File span {delta} ({start} - {end})"
 
     ## Constructor argument defaults
     DEFAULT_ARGS = dict(BAG=(), FILE=(), PATH=(), RECURSE=False, TOPIC=(), TYPE=(),
                         SKIP_TOPIC=(), SKIP_TYPE=(), START_TIME=None, END_TIME=None,
                         START_INDEX=None, END_INDEX=None, CONDITION=(), AFTER=0, ORDERBY=None,
                         DECOMPRESS=False, REINDEX=False, WRITE=(), PROGRESS=False,
-                        STOP_ON_ERROR=False)
+                        STOP_ON_ERROR=False, TIMESCALE=0, TIMESCALE_EMISSION=False, VERBOSE=False)
 
     def __init__(self, args=None, **kwargs):
         """
         @param   args                   arguments as namespace or dictionary, case-insensitive;
                                         or a single path as the ROS bagfile to read,
                                         or a stream to read from,
                                         or one or more {@link grepros.api.Bag Bag} instances
@@ -456,14 +542,19 @@
                                         or a stream to read from;
                                         or one or more {@link grepros.api.Bag Bag} instances
         @param   args.path              paths to scan if not current directory
         @param   args.recurse           recurse into subdirectories when looking for bagfiles
         @param   args.orderby           "topic" or "type" if any to group results by
         @param   args.decompress        decompress archived bags to file directory
         @param   args.reindex           make a copy of unindexed bags and reindex them (ROS1 only)
+        @param   args.timescale         emit messages on original timeline from first message
+                                        at given rate, 0 disables
+        @param   args.timescale_emission
+                                        timeline from first matched message not first in bag,
+                                        requires notify() for each message
         @param   args.write             outputs, to skip in input files
         @param   args.bag               one or more {@link grepros.api.Bag Bag} instances
         <!--sep-->
 
         General arguments:
         @param   args.topic             ROS topics to read if not all
         @param   args.type              ROS message types to read if not all
@@ -472,40 +563,41 @@
         @param   args.start_time        earliest timestamp of messages to read
         @param   args.end_time          latest timestamp of messages to read
         @param   args.start_index       message index within topic to start from
         @param   args.end_index         message index within topic to stop at
         @param   args.unique            emit messages that are unique in topic
         @param   args.select_field      message fields to use for uniqueness if not all
         @param   args.noselect_field    message fields to skip for uniqueness
-        @param   args.nth_message       read every Nth message in topic
-        @param   args.nth_interval      minimum time interval between messages in topic
+        @param   args.nth_message       read every Nth message in topic, starting from first
+        @param   args.nth_interval      minimum time interval between messages in topic,
+                                        as seconds or ROS duration
         @param   args.condition         Python expressions that must evaluate as true
                                         for message to be processable, see ConditionMixin
         @param   args.progress          whether to print progress bar
         @param   args.stop_on_error     stop execution on any error like unknown message type
+        @param   args.verbose           whether to print error stacktraces
         @param   kwargs                 any and all arguments as keyword overrides, case-insensitive
         """
         args0 = args
         is_bag = isinstance(args, api.Bag) or \
                  common.is_iterable(args) and all(isinstance(x, api.Bag) for x in args)
         args = {"FILE": str(args)} if isinstance(args, common.PATH_TYPES) else \
                {"FILE": args} if common.is_stream(args) else {} if is_bag else args
         args = ensure_namespace(args, BagSource.DEFAULT_ARGS, **kwargs)
         super(BagSource, self).__init__(args)
         ConditionMixin.__init__(self, args)
         self._args0     = common.structcopy(self.args)  # Original arguments
-        self._status    = None   # Match status of last produced message
-        self._sticky    = False  # Reading a single topic until all after-context emitted
         self._totals_ok = False  # Whether message count totals have been retrieved (ROS2 optimize)
         self._types_ok  = False  # Whether type definitions have been retrieved (ROS2 optimize)
         self._running   = False
         self._bag       = None   # Current bag object instance
         self._filename  = None   # Current bagfile path
         self._meta      = None   # Cached get_meta()
         self._bag0      = ([args0] if isinstance(args0, api.Bag) else args0) if is_bag else None
+        self._delaystamps = collections.defaultdict(int)  # Tracked timestamps for timeline emission
 
     def read(self):
         """Yields messages from ROS bagfiles, as (topic, msg, ROS time)."""
         if not self.validate(): raise Exception("invalid")
         self._running = True
 
         for _ in self._produce_bags():
@@ -518,31 +610,43 @@
             elif "type" == self.args.ORDERBY:  # Group output by sorted type names
                 typetopics = {}
                 for n, tt in self._topics.items():
                     for t in tt: typetopics.setdefault(t, []).append(n)
                 topicsets = [{n: [t] for n in nn} for t, nn in sorted(typetopics.items())]
 
             self._types_ok = False
-            self._init_progress()
+            self.init_progress()
             for topics in topicsets:
                 for topic, msg, stamp, index in self._produce(topics) if topics else ():
                     self.conditions_register_message(topic, msg)
                     if not self.is_conditions_topic(topic, pure=True) \
                     and (not self.preprocess or self.is_processable(topic, msg, stamp, index)):
                         yield self.SourceMessage(topic, msg, stamp)
                 if not self._running:
                     break  # for topics
             self._counts and self.sink and self.sink.flush()
             self.close_batch()
         self._running = False
 
+    def configure(self, args=None, **kwargs):
+        """
+        Updates source configuration.
+
+        @param   args    arguments as namespace or dictionary, case-insensitive
+        @param   kwargs  any and all arguments as keyword overrides, case-insensitive
+        """
+        super(BagSource, self).configure(args, **kwargs)
+        self._args0 = common.structcopy(self.args)
+
     def validate(self):
         """Returns whether ROS environment is set and arguments valid, prints error if not."""
         if self.valid is not None: return self.valid
-        self.valid = api.validate()
+        self.valid = Source.validate(self)
+        if not api.validate():
+            self.valid = False
         if not self._bag0 and self.args.FILE and os.path.isfile(self.args.FILE[0]) \
         and not common.verify_io(self.args.FILE[0], "r"):
             ConsolePrinter.error("File not readable.")
             self.valid = False
         if not self._bag0 and common.is_stream(self.args.FILE) \
         and not any(c.STREAMABLE for c in api.Bag.READER_CLASSES):
             ConsolePrinter.error("Bag format does not support reading streams.")
@@ -550,14 +654,19 @@
         if self._bag0 and not any(x.mode in ("r", "a") for x in self._bag0):
             ConsolePrinter.error("Bag not in read mode.")
             self.valid = False
         if self.args.ORDERBY and self.conditions_get_topics():
             ConsolePrinter.error("Cannot use topics in conditions and bag order by %s.",
                                  self.args.ORDERBY)
             self.valid = False
+        if self.args.TIMESCALE and self.args.TIMESCALE < 0:
+            ConsolePrinter.error("Invalid timescale factor: %r.", self.args.TIMESCALE)
+            self.valid = False
+        if not ConditionMixin.validate(self):
+            self.valid = False
         return self.valid
 
     def close(self):
         """Closes current bag, if any."""
         self._running = False
         if self._bag and not self._bag0: self._bag.close()
         ConditionMixin.close_batch(self)
@@ -567,14 +676,16 @@
         """Closes current bag, if any."""
         if self._bag0: self._running = False
         elif self._bag: self._bag.close()
         self._bag = None
         if self.bar:
             self.bar.update(flush=True)
             self.bar = None
+            if self._bar_args.get("source_value") is not None:
+                self._bar_args["source_value"] = 0
         ConditionMixin.close_batch(self)
 
     def format_meta(self):
         """Returns bagfile metainfo string."""
         return self.META_TEMPLATE.format(**self.get_meta())
 
     def format_message_meta(self, topic, msg, stamp, index=None):
@@ -623,77 +734,95 @@
     def get_message_type_hash(self, msg_or_type):
         """Returns ROS message type MD5 hash."""
         return self._bag.get_message_type_hash(msg_or_type) or \
                api.get_message_type_hash(msg_or_type)
 
     def notify(self, status):
         """Reports match status of last produced message."""
-        self._status = bool(status)
+        super(BagSource, self).notify(status)
         if status and not self._totals_ok:
             self._ensure_totals()
+        if status and self.args.TIMESCALE and self.args.TIMESCALE_EMISSION:
+            if "first" not in self._delaystamps:
+                self._delaystamps["first"] = self._delaystamps["current"]
+            else: self._delay_timeline()  # Delay until time met
 
     def is_processable(self, topic, msg, stamp, index=None):
-        """Returns whether message passes source filters."""
+        """Returns whether message passes source filters; registers status."""
+        self._status = False
         topickey = api.TypeMeta.make(msg, topic).topickey
-        if self.args.START_INDEX and index is not None:
+        if self.args.START_INDEX and index is not None and self.args.START_INDEX < 0 \
+        and topickey not in self._start_indexes:  # Populate topic in _start_indexes
             self._ensure_totals()
-            START = self.args.START_INDEX
-            MIN = max(0, START + (self.topics[topickey] if START < 0 else 0))
-            if MIN >= index:
-                return False
-        if self.args.END_INDEX and index is not None:
+            self._start_indexes[topickey] = max(0, self.args.START_INDEX + self.topics[topickey])
+        if self.args.END_INDEX and index is not None and self.args.END_INDEX < 0 \
+        and topickey not in self._end_indexes:  # Populate topic in _end_indexes
             self._ensure_totals()
-            END = self.args.END_INDEX
-            MAX = END + (self.topics[topickey] if END < 0 else 0)
-            if MAX < index:
-                return False
+            self._end_indexes[topickey] = (self.args.END_INDEX + self.topics[topickey])
+            if not self._end_indexes[topickey]: self._end_indexes[topickey] = -1
+
         if not super(BagSource, self).is_processable(topic, msg, stamp, index):
             return False
-        return ConditionMixin.is_processable(self, topic, msg, stamp, index)
+        if not ConditionMixin.is_processable(self, topic, msg, stamp, index):
+            return False
+        self._status = True
+        return True
+
+    def init_progress(self):
+        """Initializes progress bar, if any, for current bag."""
+        if self.args.PROGRESS and not self.bar:
+            self._ensure_totals()
+            self.configure_progress(**self._make_progress_args())
+            super(BagSource, self).init_progress()
 
     def _produce(self, topics, start_time=None):
         """
         Yields messages from current ROS bagfile, as (topic, msg, ROS time, index in topic).
 
         @param   topics  {topic: [typename, ]}
         """
         if not self._running or not self._bag: return
+        do_predelay = self.args.TIMESCALE and not self.args.TIMESCALE_EMISSION
+        if do_predelay: self._delaystamps["first"] = self._bag.get_start_time()
+        if self.args.TIMESCALE and "read" not in self._delaystamps:
+            self._delaystamps["read"] = getattr(time, "monotonic", time.time)()  # Py3 / Py2
         counts = collections.Counter()
+        endtime_indexes = {}  # {topickey: index at reaching END_TIME}
+        nametypes = {(n, t) for n, tt in topics.items() for t in tt}
         for topic, msg, stamp in self._bag.read_messages(list(topics), start_time):
             if not self._running or not self._bag:
-                break  # for topic, 
+                break  # for topic,
             typename = api.get_message_type(msg)
             if topics and typename not in topics[topic]:
-                continue  # for topic
+                continue  # for topic,
             if api.ROS2 and not self._types_ok:
                 self.topics, self._types_ok = self._bag.get_topic_info(counts=False), True
 
             topickey = api.TypeMeta.make(msg, topic, self).topickey
             counts[topickey] += 1; self._counts[topickey] += 1
             # Skip messages already processed during sticky
-            if not self._sticky and counts[topickey] != self._counts[topickey]:
-                continue  # for topic
+            if start_time is None and counts[topickey] != self._counts[topickey]:
+                continue  # for topic,
 
-            self._status = None
-            self.bar and self.bar.update(value=sum(self._counts.values()))
+            self._status, self._delaystamps["current"] = None, api.to_sec(stamp)
+            if do_predelay: self._delay_timeline()  # Delay emission until time
+            if self.bar: self.update_progress(sum(self._counts.values()))
             yield topic, msg, stamp, self._counts[topickey]
 
-            if self.args.NTH_MESSAGE > 1 or self.args.NTH_INTERVAL > 0:
+            if self._status:
                 self._processables[topickey] = (self._counts[topickey], stamp)
-            if self._status and self.args.AFTER and not self._sticky \
+            if self._status and not self.preprocess and self.args.AFTER and start_time is None \
             and not self.has_conditions() \
             and (len(self._topics) > 1 or len(next(iter(self._topics.values()))) > 1):
                 # Stick to one topic until trailing messages have been emitted
-                self._sticky = True
-                continue_from = stamp + api.make_duration(nsecs=1)
-                for entry in self._produce({topic: typename}, continue_from):
+                for entry in self._produce({topic: typename}, stamp + api.make_duration(nsecs=1)):
                     yield entry
-                self._sticky = False
-            if not self._running or not self._bag:
-                break  # for topic
+            if not self._running or not self._bag or (start_time is None
+            and self._is_at_end_threshold(topickey, stamp, nametypes, endtime_indexes)):
+                break  # for topic,
 
     def _produce_bags(self):
         """Yields Bag instances from configured arguments."""
         if self._bag0:
             for bag in self._bag0:
                 if self._configure(bag=bag):
                     yield self._bag
@@ -714,42 +843,88 @@
 
             if skip or not self._configure(filename):
                 continue  # for filename
 
             encountereds.add(self._bag.filename)
             yield self._bag
 
-    def _init_progress(self):
-        """Initializes progress bar, if any, for current bag."""
-        if self.args.PROGRESS and not self.bar:
-            self._ensure_totals()
-            self.bar = common.ProgressBar(aftertemplate=" {afterword} ({value:,d}/{max:,d})")
-            self.bar.afterword = os.path.basename(self._filename or "<stream>")
-            self.bar.max = sum(sum(c for (t, n, _), c in self.topics.items()
-                                   if c and t == t_ and n in nn)
-                               for t_, nn in self._topics.items())
-            self.bar.update(value=0)
+    def _make_progress_args(self):
+        """Returns dictionary with progress bar options"""
+        total = sum(sum(c for (t, n, _), c in self.topics.items() if c and t == t_ and n in nn)
+                    for t_, nn in self._topics.items())
+        result = dict(max=total, afterword=os.path.basename(self._filename or "<stream>"))
+
+        instr, outstr = "{value:,d}/{max:,d}", ""
+        if any([self.args.CONDITION, self.args.UNIQUE, self.args.NTH_INTERVAL,
+                self.args.START_TIME, self.args.END_TIME]) or self.args.NTH_MESSAGE > 1:
+            self._bar_args.setdefault("source_value", 0)  # Separate counts if not all messages
+        if self._bar_args.get("source_value") is not None \
+        or self._bar_args.get("match_max") is not None:
+            result.update(source_value=self._bar_args.get("source_value") or 0)
+            instr, outstr = "{source_value:,d}/{max:,d}", "matched {value:,d}"
+            if self._bar_args.get("match_max") is not None:
+                instr, outstr = "{source_value:,d}/{source_max:,d}", outstr + "/{match_max:,d}"
+                result.update(source_max=total, max=min(total, self._bar_args["match_max"]))
+        result.update(aftertemplate=" {afterword} (%s)" % "  ".join(filter(bool, (instr, outstr))))
+
+        return result
 
     def _ensure_totals(self):
         """Retrieves total message counts if not retrieved."""
         if not self._totals_ok:  # ROS2 bag probably
             has_ensure = common.has_arg(self._bag.get_topic_info, "ensure_types")
             kws = dict(ensure_types=False) if has_ensure else {}
             for (t, n, h), c in self._bag.get_topic_info(**kws).items():
                 self.topics[(t, n, h)] = c
             self._totals_ok = True
 
+    def _delay_timeline(self):
+        """Sleeps until message ought to be emitted in bag timeline."""
+        curstamp, readstamp, startstamp = map(self._delaystamps.get, ("current", "read", "first"))
+        delta = max(0, api.to_sec(curstamp) - startstamp) / (self.args.TIMESCALE or 1)
+        if delta: time.sleep(max(0, delta + readstamp - getattr(time, "monotonic", time.time)()))
+
+    def _is_at_end_threshold(self, topickey, stamp, nametypes, endtime_indexes):
+        """
+        Returns whether bag reading has reached END_INDEX or END_TIME in all given topics.
+
+        @param   topickey         (topic, typename, typehash) of current message
+        @param   stamp            ROS timestamp of current message
+        @param   nametypes        {(topic, typename)} to account for
+        @param   endtime_indexes  {topickey: index at reaching END_TIME}, gets modified
+        """
+        if self.args.END_INDEX:
+            max_index = self.args.END_INDEX + self.args.AFTER
+            if self._counts[topickey] >= max_index:  # Stop reading when reaching max in all topics
+                mycounts = {k: v for k, v in self._counts.items() if k[:2] in nametypes}
+                if nametypes == set(k[:2] for k in mycounts) \
+                and all(v >= self._end_indexes.get(k, max_index) for k, v in mycounts.items()):
+                    return True  # Early break if all topics at max index
+        if self.args.END_TIME and stamp > self.args.END_TIME:
+            self._ensure_totals()
+            if topickey not in endtime_indexes: endtime_indexes[topickey] = self._counts[topickey]
+            max_index = min(self.topics[topickey], endtime_indexes[topickey] + self.args.AFTER)
+            if self._counts[topickey] >= max_index: # One topic reached end: check all topics
+                myindexes = {k: v for k, v in endtime_indexes.items() if k[:2] in nametypes}
+                if nametypes == set(k[:2] for k in myindexes) \
+                and all(self._counts[k] >= min(self.topics[k], v + self.args.AFTER)
+                        for k, v in myindexes.items()):
+                    return True  # Early break if all topics at max time
+        return False
+
     def _configure(self, filename=None, bag=None):
         """Opens bag and populates bag-specific argument state, returns success."""
         self._meta      = None
         self._bag       = None
         self._filename  = None
-        self._sticky    = False
         self._totals_ok = False
+        self._delaystamps.clear()
         self._counts.clear()
+        self._start_indexes.clear()
+        self._end_indexes.clear()
         self._processables.clear()
         self._hashes.clear()
         self.topics.clear()
 
         if bag is not None and bag.mode not in ("r", "a"):
             ConsolePrinter.warn("Cannot read %s: bag in write mode.", bag)
             return False
@@ -766,14 +941,15 @@
             bag = api.Bag(filename, mode="r", reindex=self.args.REINDEX,
                           progress=self.args.PROGRESS) if bag is None else bag
             bag.stop_on_error = self.args.STOP_ON_ERROR
             bag.open()
         except Exception as e:
             ConsolePrinter.error("\nError opening %r: %s", filename or bag, e)
             if self.args.STOP_ON_ERROR: raise
+            if self.args.VERBOSE: traceback.print_exc()
             return False
 
         self._bag      = bag
         self._filename = bag.filename
 
         dct = fulldct = {}  # {topic: [typename, ]}
         kws = dict(ensure_types=False) if common.has_arg(bag.get_topic_info, "ensure_types") else {}
@@ -785,38 +961,39 @@
             self.conditions_set_topic_state(topic, True)
 
         dct = common.filter_dict(dct, self.args.TOPIC, self.args.TYPE)
         dct = common.filter_dict(dct, self.args.SKIP_TOPIC, self.args.SKIP_TYPE, reverse=True)
         for topic in self.conditions_get_topics():  # Add topics used in conditions
             matches = [t for p in [common.wildcard_to_regex(topic, end=True)] for t in fulldct
                        if t == topic or "*" in topic and p.match(t)]
-            for topic in matches:
-                self.conditions_set_topic_state(topic, topic not in dct)
-                dct.setdefault(topic, fulldct[topic])
+            for realtopic in matches:
+                self.conditions_set_topic_state(realtopic, realtopic not in dct)
+                dct.setdefault(realtopic, fulldct[realtopic])
         self._topics = dct
         self._meta   = self.get_meta()
 
         args = self.args = common.structcopy(self._args0)
         if args.START_TIME is not None:
             args.START_TIME = api.make_bag_time(args.START_TIME, bag)
         if args.END_TIME is not None:
             args.END_TIME = api.make_bag_time(args.END_TIME, bag)
         return True
 
 
-class TopicSource(Source, ConditionMixin):
+class LiveSource(Source, ConditionMixin):
     """Produces messages from live ROS topics."""
 
     ## Seconds between refreshing available topics from ROS master.
     MASTER_INTERVAL = 2
 
     ## Constructor argument defaults
     DEFAULT_ARGS = dict(TOPIC=(), TYPE=(), SKIP_TOPIC=(), SKIP_TYPE=(), START_TIME=None,
                         END_TIME=None, START_INDEX=None, END_INDEX=None, CONDITION=(),
-                        QUEUE_SIZE_IN=10, ROS_TIME_IN=False, PROGRESS=False, STOP_ON_ERROR=False)
+                        QUEUE_SIZE_IN=10, ROS_TIME_IN=False, PROGRESS=False, STOP_ON_ERROR=False,
+                        VERBOSE=False)
 
     def __init__(self, args=None, **kwargs):
         """
         @param   args                   arguments as namespace or dictionary, case-insensitive
         @param   args.topic             ROS topics to read if not all
         @param   args.type              ROS message types to read if not all
         @param   args.skip_topic        ROS topics to skip
@@ -824,94 +1001,110 @@
         @param   args.start_time        earliest timestamp of messages to read
         @param   args.end_time          latest timestamp of messages to read
         @param   args.start_index       message index within topic to start from
         @param   args.end_index         message index within topic to stop at
         @param   args.unique            emit messages that are unique in topic
         @param   args.select_field      message fields to use for uniqueness if not all
         @param   args.noselect_field    message fields to skip for uniqueness
-        @param   args.nth_message       read every Nth message in topic
-        @param   args.nth_interval      minimum time interval between messages in topic
+        @param   args.nth_message       read every Nth message in topic, starting from first
+        @param   args.nth_interval      minimum time interval between messages in topic,
+                                        as seconds or ROS duration
         @param   args.condition         Python expressions that must evaluate as true
                                         for message to be processable, see ConditionMixin
         @param   args.queue_size_in     subscriber queue size (default 10)
         @param   args.ros_time_in       stamp messages with ROS time instead of wall time
         @param   args.progress          whether to print progress bar
         @param   args.stop_on_error     stop execution on any error like unknown message type
+        @param   args.verbose           whether to print error stacktraces
         @param   kwargs                 any and all arguments as keyword overrides, case-insensitive
         """
-        args = ensure_namespace(args, TopicSource.DEFAULT_ARGS, **kwargs)
-        super(TopicSource, self).__init__(args)
+        args = ensure_namespace(args, LiveSource.DEFAULT_ARGS, **dict(kwargs, live=True))
+        super(LiveSource, self).__init__(args)
         ConditionMixin.__init__(self, args)
-        self._running = False  # Whether is in process of yielding messages from topics
-        self._queue   = None   # [(topic, msg, ROS time)]
-        self._subs    = {}     # {(topic, typename, typehash): ROS subscriber}
-
-        self._configure()
+        self._running    = False  # Whether is in process of yielding messages from topics
+        self._queue      = None   # [(topic, msg, ROS time)]
+        self._last_stamp = None   # ROS stamp of last message
+        self._subs       = {}     # {(topic, typename, typehash): ROS subscriber}
 
     def read(self):
         """Yields messages from subscribed ROS topics, as (topic, msg, ROS time)."""
         if not self._running:
             if not self.validate(): raise Exception("invalid")
             api.init_node()
             self._running = True
             self._queue = queue.Queue()
             self.refresh_topics()
             t = threading.Thread(target=self._run_refresh)
             t.daemon = True
             t.start()
+            if self.args.END_TIME:
+                self._last_stamp = None
+                t = threading.Thread(target=self._run_endtime_closer)
+                t.daemon = True
+                t.start()
 
         total = 0
-        self._init_progress()
+        self.init_progress()
         while self._running:
             topic, msg, stamp = self._queue.get()
             total += bool(topic)
-            self._update_progress(total, running=self._running and bool(topic))
-            if topic:
-                topickey = api.TypeMeta.make(msg, topic, self).topickey
-                self._counts[topickey] += 1
-                self.conditions_register_message(topic, msg)
-                if self.is_conditions_topic(topic, pure=True): continue  # while
-
-                if not self.preprocess \
-                or self.is_processable(topic, msg, stamp, self._counts[topickey]):
-                    yield self.SourceMessage(topic, msg, stamp)
-                if self.args.NTH_MESSAGE > 1 or self.args.NTH_INTERVAL > 0:
-                    self._processables[topickey] = (self._counts[topickey], stamp)
+            self.update_progress(total, running=self._running and bool(topic))
+            if not topic: continue  # while
+
+            topickey = api.TypeMeta.make(msg, topic, self).topickey
+            self._counts[topickey] += 1
+            self._last_stamp = stamp
+            self.conditions_register_message(topic, msg)
+            if self.is_conditions_topic(topic, pure=True): continue  # while
+
+            self._status = None
+            if not self.preprocess \
+            or self.is_processable(topic, msg, stamp, self._counts[topickey]):
+                yield self.SourceMessage(topic, msg, stamp)
+            if self._status and (self.args.NTH_MESSAGE > 1 or self.args.NTH_INTERVAL > 0):
+                self._processables[topickey] = (self._counts[topickey], stamp)
         self._queue = None
         self._running = False
 
     def bind(self, sink):
         """Attaches sink to source and blocks until connected to ROS live."""
         if not self.validate(): raise Exception("invalid")
-        super(TopicSource, self).bind(sink)
+        super(LiveSource, self).bind(sink)
         api.init_node()
 
     def validate(self):
-        """Returns whether ROS environment is set, prints error if not."""
-        if self.valid is None: self.valid = api.validate(live=True)
+        """Returns whether ROS environment is set and arguments valid, prints error if not."""
+        if self.valid is not None: return self.valid
+        self.valid = Source.validate(self)
+        if not api.validate(live=True):
+            self.valid = False
+        if not ConditionMixin.validate(self):
+            self.valid = False
+        if self.valid:
+            self._configure()
         return self.valid
 
     def close(self):
         """Shuts down subscribers and stops producing messages."""
         self._running = False
         for k in list(self._subs):
             self._subs.pop(k).unregister()
         self._queue and self._queue.put((None, None, None))  # Wake up iterator
         self._queue = None
         ConditionMixin.close_batch(self)
-        super(TopicSource, self).close()
+        super(LiveSource, self).close()
 
     def get_meta(self):
         """Returns source metainfo data dict."""
         ENV = {k: os.getenv(k) for k in ("ROS_MASTER_URI", "ROS_DOMAIN_ID") if os.getenv(k)}
-        return dict(ENV, tcount=len(self.topics))
+        return dict(ENV, tcount=len(self.topics), scount=len(self._subs))
 
     def get_message_meta(self, topic, msg, stamp, index=None):
         """Returns message metainfo data dict."""
-        result = super(TopicSource, self).get_message_meta(topic, msg, stamp, index)
+        result = super(LiveSource, self).get_message_meta(topic, msg, stamp, index)
         topickey = (topic, result["type"], result["hash"])
         if topickey in self._subs:
             result.update(qoses=self._subs[topickey].get_qoses())
         return result
 
     def get_message_class(self, typename, typehash=None):
         """Returns message type class, from active subscription if available."""
@@ -938,86 +1131,112 @@
         metadata = self.get_meta()
         result = "\nROS%s live" % api.ROS_VERSION
         if "ROS_MASTER_URI" in metadata:
             result += ", ROS master %s" % metadata["ROS_MASTER_URI"]
         if "ROS_DOMAIN_ID" in metadata:
             result += ", ROS domain ID %s" % metadata["ROS_DOMAIN_ID"]
         result += ", %s initially" % common.plural("topic", metadata["tcount"])
+        result += ", %s subscribed" % metadata["scount"]
         return result
 
     def is_processable(self, topic, msg, stamp, index=None):
-        """Returns whether message passes source filters."""
-        if self.args.START_INDEX and index is not None:
-            if max(0, self.args.START_INDEX) >= index:
-                return False
-        if self.args.END_INDEX and index is not None:
-            if 0 < self.args.END_INDEX < index:
-                return False
-        if not super(TopicSource, self).is_processable(topic, msg, stamp, index):
+        """Returns whether message passes source filters; registers status."""
+        self._status = False
+        if not super(LiveSource, self).is_processable(topic, msg, stamp, index):
             return False
-        return ConditionMixin.is_processable(self, topic, msg, stamp, index)
+        if not ConditionMixin.is_processable(self, topic, msg, stamp, index):
+            return False
+        self._status = True
+        return True
 
     def refresh_topics(self):
         """Refreshes topics and subscriptions from ROS live."""
         for topic, typename in api.get_topic_types():
+            topickey = (topic, typename, None)
+            self.topics[topickey] = None
             dct = common.filter_dict({topic: [typename]}, self.args.TOPIC, self.args.TYPE)
             if not common.filter_dict(dct, self.args.SKIP_TOPIC, self.args.SKIP_TYPE, reverse=True):
                 continue  # for topic, typename
             if api.ROS2 and api.get_message_class(typename) is None:
                 msg = "Error loading type %s in topic %s." % (typename, topic)
                 if self.args.STOP_ON_ERROR: raise Exception(msg)
                 ConsolePrinter.warn(msg, __once=True)
                 continue  # for topic, typename
-            topickey = (topic, typename, None)
-            if topickey in self.topics:
+            if topickey in self._subs:
                 continue  # for topic, typename
 
             handler = functools.partial(self._on_message, topic)
             try:
                 sub = api.create_subscriber(topic, typename, handler,
                                             queue_size=self.args.QUEUE_SIZE_IN)
             except Exception as e:
                 ConsolePrinter.warn("Error subscribing to topic %s: %%r" % topic,
                                     e, __once=True)
                 if self.args.STOP_ON_ERROR: raise
+                if self.args.VERBOSE: traceback.print_exc()
                 continue  # for topic, typename
             self._subs[topickey] = sub
-            self.topics[topickey] = None
 
-    def _init_progress(self):
+    def init_progress(self):
         """Initializes progress bar, if any."""
         if self.args.PROGRESS and not self.bar:
-            self.bar = common.ProgressBar(afterword="ROS%s live" % api.ROS_VERSION,
-                                          aftertemplate=" {afterword}", pulse=True)
-            self.bar.start()
+            self.configure_progress(**self._make_progress_args())
+            super(LiveSource, self).init_progress()
 
-    def _update_progress(self, count, running=True):
+    def update_progress(self, count, running=True):
         """Updates progress bar, if any."""
         if self.bar:
-            afterword = "ROS%s live, %s" % (api.ROS_VERSION, common.plural("message", count))
-            self.bar.afterword, self.bar.max = afterword, count
-            if not running:
-                self.bar.pause, self.bar.pulse_pos = True, None
-            self.bar.update(count)
+            if count in (1, 2):  # Change plurality
+                self.configure_progress(**self._make_progress_args(count))
+            super(LiveSource, self).update_progress(count, running)
 
     def _configure(self):
         """Adjusts start/end time filter values to current time."""
         if self.args.START_TIME is not None:
             self.args.START_TIME = api.make_live_time(self.args.START_TIME)
         if self.args.END_TIME is not None:
             self.args.END_TIME = api.make_live_time(self.args.END_TIME)
 
+    def _make_progress_args(self, count=None):
+        """Returns dictionary with progress bar options, for specific nessage index if any."""
+        result = dict(afterword = "ROS%s live" % api.ROS_VERSION, pulse=True)
+        if self._bar_args.get("match_max") is not None:
+            result.update(max=self._bar_args["match_max"], pulse=False)
+
+        instr, outstr = "{value:,d} message%s" % ("" if count == 1 else "s"), ""
+        if any([self.args.CONDITION, self.args.UNIQUE, self.args.NTH_INTERVAL,
+                self.args.START_TIME, self.args.END_TIME]) or self.args.NTH_MESSAGE > 1:
+            self._bar_args.setdefault("source_value", 0)  # Separate counts if not all messages
+        if self._bar_args.get("source_value") is not None:
+            instr = "{source_value:,d} message%s" % ("" if count == 1 else "s")
+            outstr = "matched {value:,d}"
+            if self._bar_args.get("match_max") is not None: outstr += "/{match_max:,d}"
+        elif self._bar_args.get("match_max") is not None:
+            instr = "{value:,d}/{max:,d}"
+        result.update(aftertemplate=" {afterword} (%s)" % "  ".join(filter(bool, (instr, outstr))))
+
+        return result
+
     def _run_refresh(self):
         """Periodically refreshes topics and subscriptions from ROS live."""
         time.sleep(self.MASTER_INTERVAL)
         while self._running:
             try: self.refresh_topics()
             except Exception as e: self.thread_excepthook("Error refreshing live topics: %r" % e, e)
             time.sleep(self.MASTER_INTERVAL)
 
+    def _run_endtime_closer(self):
+        """Periodically checks whether END_TIME has been reached, closes source when so."""
+        time.sleep(self.MASTER_INTERVAL)
+        while self._running and self.args.END_TIME:
+            if self._last_stamp and self._last_stamp > self.args.END_TIME:
+                time.sleep(self.MASTER_INTERVAL)  # Allow some more arrivals just in case
+                self.close()
+            else: time.sleep(self.MASTER_INTERVAL)
+
     def _on_message(self, topic, msg):
         """Subscription callback handler, queues message for yielding."""
         stamp = api.get_rostime() if self.args.ROS_TIME_IN else api.make_time(time.time())
         self._queue and self._queue.put((topic, msg, stamp))
 
 
 class AppSource(Source, ConditionMixin):
@@ -1040,38 +1259,38 @@
         @param   args.start_time       earliest timestamp of messages to read
         @param   args.end_time         latest timestamp of messages to read
         @param   args.start_index      message index within topic to start from
         @param   args.end_index        message index within topic to stop at
         @param   args.unique           emit messages that are unique in topic
         @param   args.select_field     message fields to use for uniqueness if not all
         @param   args.noselect_field   message fields to skip for uniqueness
-        @param   args.nth_message      read every Nth message in topic
-        @param   args.nth_interval     minimum time interval between messages in topic
+        @param   args.nth_message      read every Nth message in topic, starting from first
+        @param   args.nth_interval     minimum time interval between messages in topic,
+                                       as seconds or ROS duration
         @param   args.condition        Python expressions that must evaluate as true
                                        for message to be processable, see ConditionMixin
         @param   args.iterable         iterable yielding (topic, msg, stamp) or (topic, msg);
                                        yielding `None` signals end of content
         @param   kwargs                any and all arguments as keyword overrides, case-insensitive
         """
         if common.is_iterable(args) and not isinstance(args, dict):
             args = ensure_namespace(None, iterable=args)
         args = ensure_namespace(args, AppSource.DEFAULT_ARGS, **kwargs)
         super(AppSource, self).__init__(args)
         ConditionMixin.__init__(self, args)
         self._queue = queue.Queue()  # [(topic, msg, ROS time)]
         self._reading = False
 
-        self._configure()
-
     def read(self):
         """
         Yields messages from iterable or pushed data, as (topic, msg, ROS timestamp).
 
         Blocks until a message is available, or source is closed.
         """
+        if not self.validate(): raise Exception("invalid")
         def generate(iterable):
             for x in iterable: yield x
         feeder = generate(self.args.ITERABLE) if self.args.ITERABLE else None
         self._reading = True
         while self._reading:
             item = self._queue.get() if not feeder or self._queue.qsize() else next(feeder, None)
             if item is None: break  # while
@@ -1079,78 +1298,88 @@
             if len(item) > 2: topic, msg, stamp = item[:3]
             else: (topic, msg), stamp = item[:2], api.get_rostime(fallback=True)
             topickey = api.TypeMeta.make(msg, topic, self).topickey
             self._counts[topickey] += 1
             self.conditions_register_message(topic, msg)
             if self.is_conditions_topic(topic, pure=True): continue  # while
 
+            self._status = None
             if not self.preprocess \
             or self.is_processable(topic, msg, stamp, self._counts[topickey]):
                 yield self.SourceMessage(topic, msg, stamp)
-            if self.args.NTH_MESSAGE > 1 or self.args.NTH_INTERVAL > 0:
+            if self._status and (self.args.NTH_MESSAGE > 1 or self.args.NTH_INTERVAL > 0):
                 self._processables[topickey] = (self._counts[topickey], stamp)
         self._reading = False
 
     def close(self):
         """Closes current read() yielding, if any."""
         if self._reading:
             self._reading = False
             self._queue.put(None)
 
     def read_queue(self):
         """
         Returns (topic, msg, stamp) from push queue, or `None` if no queue
         or message in queue is condition topic only.
         """
+        if not self.validate(): raise Exception("invalid")
         item = None
         try: item = self._queue.get(block=False)
         except queue.Empty: pass
         if item is None: return None
 
         topic, msg, stamp = item
         topickey = api.TypeMeta.make(msg, topic, self).topickey
         self._counts[topickey] += 1
         self.conditions_register_message(topic, msg)
         return None if self.is_conditions_topic(topic, pure=True) else (topic, msg, stamp)
 
     def mark_queue(self, topic, msg, stamp):
         """Registers message produced from read_queue()."""
+        if not self.validate(): raise Exception("invalid")
         if self.args.NTH_MESSAGE > 1 or self.args.NTH_INTERVAL > 0:
             topickey = api.TypeMeta.make(msg, topic).topickey
             self._processables[topickey] = (self._counts[topickey], stamp)
 
     def push(self, topic, msg=None, stamp=None):
         """
         Pushes a message to be yielded from read().
 
         @param   topic  topic name, or `None` to signal end of content
         @param   msg    ROS message
         @param   stamp  message ROS timestamp, defaults to current wall time if `None`
         """
+        if not self.validate(): raise Exception("invalid")
         if topic is None: self._queue.put(None)
         else: self._queue.put((topic, msg, stamp or api.get_rostime(fallback=True)))
 
     def is_processable(self, topic, msg, stamp, index=None):
-        """Returns whether message passes source filters."""
+        """Returns whether message passes source filters; registers status."""
+        self._status = False
         dct = common.filter_dict({topic: [api.get_message_type(msg)]},
                                  self.args.TOPIC, self.args.TYPE)
         if not common.filter_dict(dct, self.args.SKIP_TOPIC, self.args.SKIP_TYPE, reverse=True):
             return False
-        if self.args.START_INDEX and index is not None:
-            if max(0, self.args.START_INDEX) >= index:
-                return False
-        if self.args.END_INDEX and index is not None:
-            if 0 < self.args.END_INDEX < index:
-                return False
         if not super(AppSource, self).is_processable(topic, msg, stamp, index):
             return False
-        return ConditionMixin.is_processable(self, topic, msg, stamp, index)
+        if not ConditionMixin.is_processable(self, topic, msg, stamp, index):
+            return False
+        self._status = True
+        return True
+
+    def validate(self):
+        """Returns whether configured arguments are valid, prints error if not."""
+        if self.valid is not None: return self.valid
+        self.valid = Source.validate(self)
+        if self.valid:
+            self._configure()
+        return self.valid
 
     def _configure(self):
         """Adjusts start/end time filter values to current time."""
         if self.args.START_TIME is not None:
             self.args.START_TIME = api.make_live_time(self.args.START_TIME)
         if self.args.END_TIME is not None:
             self.args.END_TIME = api.make_live_time(self.args.END_TIME)
 
 
-__all__ = ["AppSource", "BagSource", "ConditionMixin", "Source", "TopicSource"]
+__all__ = ["AppSource", "ConditionMixin", "BagSource", "LiveSource", "Source"]
```

## grepros/library.py

```diff
@@ -2,30 +2,30 @@
 """
 grepros library interface.
 
 Source classes:
 
 - {@link grepros.inputs.AppSource AppSource}: produces messages from iterable or pushed data
 - {@link grepros.inputs.BagSource BagSource}: produces messages from ROS bagfiles
-- {@link grepros.inputs.TopicSource TopicSource}: produces messages from live ROS topics
+- {@link grepros.inputs.LiveSource LiveSource}: produces messages from live ROS topics
 
 Sink classes:
 
 - {@link grepros.outputs.AppSink AppSink}: provides messages to callback function
 - {@link grepros.outputs.BagSink BagSink}: writes messages to bagfile
 - {@link grepros.outputs.ConsoleSink ConsoleSink}: prints messages to console
 - {@link grepros.plugins.auto.csv.CsvSink CsvSink}: writes messages to CSV files, each topic to a separate file
 - {@link grepros.plugins.auto.html.HtmlSink HtmlSink}: writes messages to an HTML file
+- {@link grepros.outputs.LiveSink LiveSink}: publishes messages to ROS topics
 - {@link grepros.plugins.mcap.McapSink McapSink}: writes messages to MCAP file
 - {@link grepros.outputs.MultiSink MultiSink}: combines any number of sinks
 - {@link grepros.plugins.parquet.ParquetSink ParquetSink}: writes messages to Apache Parquet files
 - {@link grepros.plugins.auto.postgres.PostgresSink PostgresSink}: writes messages to a Postgres database
 - {@link grepros.plugins.auto.sqlite.SqliteSink SqliteSink}: writes messages to an SQLite database
 - {@link grepros.plugins.sql.SqlSink SqlSink}: writes SQL schema file for message type tables and topic views
-- {@link grepros.outputs.TopicSink TopicSink}: publishes messages to ROS topics
 
 {@link grepros.api.BaseBag Bag}: generic bag interface.
 {@link grepros.search.Scanner Scanner}: ROS message grepper.
 
 Format-specific bag classes:
 
 - {@link grepros.ros1.ROS1Bag ROS1Bag}: ROS1 bag reader and writer in .bag format
@@ -39,33 +39,32 @@
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     09.12.2022
-@modified    28.12.2023
+@modified    22.03.2024
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.library
 from . plugins.auto.csv      import CsvSink
 from . plugins.auto.html     import HtmlSink
 from . plugins.auto.postgres import PostgresSink
 from . plugins.auto.sqlite   import SqliteSink
 from . plugins.mcap          import McapBag, McapSink
 from . plugins.parquet       import ParquetSink
 from . plugins.sql           import SqlSink
 
 from . api  import Bag
-from . inputs  import AppSource, BagSource, Source, TopicSource
-from . outputs import AppSink, BagSink, ConsoleSink, MultiSink, Sink, TopicSink
-from . search  import Scanner
+from . inputs  import AppSource, BagSource, LiveSource, Source
+from . outputs import AppSink, BagSink, ConsoleSink, LiveSink, MultiSink, Sink
+from . search  import BooleanResult, ExpressionTree, Scanner
 from . import api
 from . import common
-from . import main
 from . import plugins
 
 
 _inited = False
 
 
 def grep(args=None, **kwargs):
@@ -75,174 +74,186 @@
     Initializes grepros if not already initialized.
 
     Read from bagfiles: `grep(file="2022-10-*.bag", pattern="cpu")`.
 
     Read from live topics: `grep(live=True, pattern="cpu")`.
 
 
-    @param   args                     arguments as namespace or dictionary, case-insensitive;
-                                      or a single path as the ROS bagfile to read,
-                                      or one or more {@link grepros.api.Bag Bag} instances
-    @param   kwargs                   any and all arguments as keyword overrides, case-insensitive
+    @param   args                      arguments as namespace or dictionary, case-insensitive;
+                                       or a single path as the ROS bagfile to read,
+                                       or one or more {@link grepros.api.Bag Bag} instances,
+                                       or a {@link grepros.inputs.Source Source} instance
+    @param   kwargs                    any and all arguments as keyword overrides, case-insensitive
     <!--sep-->
 
     Bag source:
-    @param   args.file                names of ROS bagfiles to read if not all in directory
-    @param   args.path                paths to scan if not current directory
-    @param   args.recurse             recurse into subdirectories when looking for bagfiles
-    @param   args.decompress          decompress archived bags to file directory
-    @param   args.reindex             make a copy of unindexed bags and reindex them (ROS1 only)
-    @param   args.orderby             "topic" or "type" if any to group results by
-    @param   args.bag                 one or more {@link grepros.api.Bag Bag} instances
+    @param   args.file                 names of ROS bagfiles to read if not all in directory
+    @param   args.path                 paths to scan if not current directory
+    @param   args.recurse              recurse into subdirectories when looking for bagfiles
+    @param   args.decompress           decompress archived bags to file directory
+    @param   args.reindex              make a copy of unindexed bags and reindex them (ROS1 only)
+    @param   args.orderby              "topic" or "type" if any to group results by
+    @param   args.timescale            emit messages on original timeline from first message
+                                       at given rate, 0 disables
+    @param   args.timescale_emission   start timeline from first matched message not first in bag
+    @param   args.bag                  one or more {@link grepros.api.Bag Bag} instances
     <!--sep-->
 
     Live source:
-    @param   args.live                whether reading messages from live ROS topics
-    @param   args.queue_size_in       subscriber queue size (default 10)
-    @param   args.ros_time_in         stamp messages with ROS time instead of wall time
+    @param   args.live                 whether reading messages from live ROS topics
+    @param   args.queue_size_in        subscriber queue size (default 10)
+    @param   args.ros_time_in          stamp messages with ROS time instead of wall time
     <!--sep-->
 
     App source:
-    @param   args.app                 whether reading messages from iterable or pushed data;
-                                      may contain the iterable itself
-    @param   args.iterable            iterable yielding (topic, msg, stamp) or (topic, msg);
-                                      yielding `None` signals end of content
+    @param   args.app                  whether reading messages from iterable or pushed data;
+                                       may contain the iterable itself
+    @param   args.iterable             iterable yielding (topic, msg, stamp) or (topic, msg);
+                                       yielding `None` signals end of content
     Any source:
-    @param   args.topic               ROS topics to read if not all
-    @param   args.type                ROS message types to read if not all
-    @param   args.skip_topic          ROS topics to skip
-    @param   args.skip_type           ROS message types to skip
-    @param   args.start_time          earliest timestamp of messages to read
-    @param   args.end_time            latest timestamp of messages to read
-    @param   args.start_index         message index within topic to start from
-    @param   args.end_index           message index within topic to stop at
-
-    @param   args.nth_message         read every Nth message in topic
-    @param   args.nth_interval        minimum time interval between messages in topic
-
-    @param   args.select_field        message fields to use in matching if not all
-    @param   args.noselect_field      message fields to skip in matching
-
-    @param   args.unique              emit messages that are unique in topic
-                                      (select_field and noselect_field apply if specified)
-    @param   args.condition           Python expressions that must evaluate as true
-                                      for message to be processable, see ConditionMixin
+    @param   args.topic                ROS topics to read if not all
+    @param   args.type                 ROS message types to read if not all
+    @param   args.skip_topic           ROS topics to skip
+    @param   args.skip_type            ROS message types to skip
+    @param   args.start_time           earliest timestamp of messages to read
+    @param   args.end_time             latest timestamp of messages to read
+    @param   args.start_index          message index within topic to start from
+    @param   args.end_index            message index within topic to stop at
+
+    @param   args.nth_message          read every Nth message in topic, starting from first
+    @param   args.nth_interval         minimum time interval between messages in topic,
+                                       as seconds or ROS duration
+
+    @param   args.select_field         message fields to use in matching if not all
+    @param   args.noselect_field       message fields to skip in matching
+
+    @param   args.unique               emit messages that are unique in topic
+                                       (select_field and noselect_field apply if specified)
+    @param   args.condition            Python expressions that must evaluate as true
+                                       for message to be processable, see ConditionMixin
     <!--sep-->
 
     Search&zwj;:
-    @param   args.pattern             pattern(s) to find in message field values
-    @param   args.fixed_string        pattern contains ordinary strings, not regular expressions
-    @param   args.case                use case-sensitive matching in pattern
-    @param   args.invert              select messages not matching pattern
-
-    @param   args.nth_match           emit every Nth match in topic
-    @param   args.max_count           number of matched messages to emit (per file if bag input)
-    @param   args.max_per_topic       number of matched messages to emit from each topic
-    @param   args.max_topics          number of topics to print matches from
-
-    @param   args.before              number of messages of leading context to emit before match
-    @param   args.after               number of messages of trailing context to emit after match
-    @param   args.context             number of messages of leading and trailing context
-                                      to emit around match
-
-    @param   args.highlight           highlight matched values
-    @param   args.match_wrapper       string to wrap around matched values,
-                                      both sides if one value, start and end if more than one,
-                                      or no wrapping if zero values
+    @param   args.pattern              pattern(s) to find in message field values
+    @param   args.fixed_string         pattern contains ordinary strings, not regular expressions
+    @param   args.case                 use case-sensitive matching in pattern
+    @param   args.invert               select messages not matching pattern
+    @param   args.expression           pattern(s) are a logical expression
+                                       like 'this AND (this2 OR NOT "skip this")',
+                                       with elements as patterns to find in message fields
+
+    @param   args.nth_match            emit every Nth match in topic, starting from first
+    @param   args.max_count            number of matched messages to emit (per file if bag input)
+    @param   args.max_per_topic        number of matched messages to emit from each topic
+    @param   args.max_topics           number of topics to print matches from
+
+    @param   args.before               number of messages of leading context to emit before match
+    @param   args.after                number of messages of trailing context to emit after match
+    @param   args.context              number of messages of leading and trailing context
+                                       to emit around match
+
+    @param   args.highlight            highlight matched values
+    @param   args.match_wrapper        string to wrap around matched values,
+                                       both sides if one value, start and end if more than one,
+                                       or no wrapping if zero values
 
     @return  {@link grepros.Scanner.GrepMessage GrepMessage} namedtuples
              of (topic, message, timestamp, match, index)
     """
     DEFAULT_ARGS = dict(FILE=[], LIVE=False, APP=False, ITERABLE=None,
                         COLOR="never", HIGHLIGHT=False)
 
     args0 = args
     is_bag = isinstance(args, Bag) or \
              common.is_iterable(args) and all(isinstance(x, Bag) for x in args)
     args = {"FILE": str(args)} if isinstance(args, common.PATH_TYPES) else \
            {} if is_bag or isinstance(args, Source) else args
-    args = common.ensure_namespace(args, DEFAULT_ARGS, **kwargs)
-    main.validate_args(main.process_args(args))
+    args = common.ArgumentUtil.validate(common.ensure_namespace(args, DEFAULT_ARGS, **kwargs))
     if not _inited: init(args)
 
     if common.is_iterable(args.APP) and not common.is_iterable(args.ITERABLE):
         args.APP, args.ITERABLE = True, args.APP
     src = args0 if isinstance(args0, Source) else \
-          TopicSource(args) if args.LIVE else \
+          LiveSource(args) if args.LIVE else \
           AppSource(args) if args.APP else \
           BagSource(args0, **vars(args)) if is_bag else BagSource(args)
+    if args and isinstance(args0, Source): src.configure(**kwargs)
     src.validate()
 
     try:
         for x in Scanner(args).find(src): yield x
     finally:
         if not isinstance(args0, (Bag, Source)): src.close()
 
 
 def source(args=None, **kwargs):
     """
     Convenience for creating a {@link grepros.inputs.Source Source} instance from arguments.
 
     Initializes grepros if not already initialized.
 
-    @param   args                  arguments as namespace or dictionary, case-insensitive;
-                                   or a single path as the ROS bagfile to read
-    @param   kwargs                any and all arguments as keyword overrides, case-insensitive
-    @param   args.file             one or more names of ROS bagfiles to read from
-    @param   args.live             read messages from live ROS topics instead
-    @param   args.app              read messages from iterable or pushed data instead;
-                                   may contain the iterable itself
+    @param   args                      arguments as namespace or dictionary, case-insensitive;
+                                       or a single path as the ROS bagfile to read
+    @param   kwargs                    any and all arguments as keyword overrides, case-insensitive
+    @param   args.file                 one or more names of ROS bagfiles to read from
+    @param   args.live                 read messages from live ROS topics instead
+    @param   args.app                  read messages from iterable or pushed data instead;
+                                       may contain the iterable itself
     <!--sep-->
 
     Bag source:
-    @param   args.file             names of ROS bagfiles to read if not all in directory
-    @param   args.path             paths to scan if not current directory
-    @param   args.recurse          recurse into subdirectories when looking for bagfiles
-    @param   args.orderby          "topic" or "type" if any to group results by
-    @param   args.decompress       decompress archived bags to file directory
-    @param   args.reindex          make a copy of unindexed bags and reindex them (ROS1 only)
-    @param   args.progress         whether to print progress bar
+    @param   args.file                 names of ROS bagfiles to read if not all in directory
+    @param   args.path                 paths to scan if not current directory
+    @param   args.recurse              recurse into subdirectories when looking for bagfiles
+    @param   args.orderby              "topic" or "type" if any to group results by
+    @param   args.decompress           decompress archived bags to file directory
+    @param   args.reindex              make a copy of unindexed bags and reindex them (ROS1 only)
+    @param   args.timescale            emit messages on original timeline from first message
+                                       at given rate, 0 disables
+    @param   args.timescale_emission   start timeline from first matched message not first in bag
+    @param   args.progress             whether to print progress bar
     <!--sep-->
 
     Live source:
-    @param   args.queue_size_in    subscriber queue size (default 10)
-    @param   args.ros_time_in      stamp messages with ROS time instead of wall time
-    @param   args.progress         whether to print progress bar
+    @param   args.queue_size_in        subscriber queue size (default 10)
+    @param   args.ros_time_in          stamp messages with ROS time instead of wall time
+    @param   args.progress             whether to print progress bar
     <!--sep-->
 
     App source:
-    @param   args.iterable         iterable yielding (topic, msg, stamp) or (topic, msg);
-                                   yielding `None` signals end of content
+    @param   args.iterable             iterable yielding (topic, msg, stamp) or (topic, msg);
+                                       yielding `None` signals end of content
     <!--sep-->
 
     Any source:
-    @param   args.topic            ROS topics to read if not all
-    @param   args.type             ROS message types to read if not all
-    @param   args.skip_topic       ROS topics to skip
-    @param   args.skip_type        ROS message types to skip
-    @param   args.start_time       earliest timestamp of messages to read
-    @param   args.end_time         latest timestamp of messages to read
-    @param   args.start_index      message index within topic to start from
-    @param   args.end_index        message index within topic to stop at
-    @param   args.unique           emit messages that are unique in topic
-    @param   args.select_field     message fields to use for uniqueness if not all
-    @param   args.noselect_field   message fields to skip for uniqueness
-    @param   args.nth_message      read every Nth message in topic
-    @param   args.nth_interval     minimum time interval between messages in topic
-    @param   args.condition        Python expressions that must evaluate as true
-                                   for message to be processable, see ConditionMixin
+    @param   args.topic                ROS topics to read if not all
+    @param   args.type                 ROS message types to read if not all
+    @param   args.skip_topic           ROS topics to skip
+    @param   args.skip_type            ROS message types to skip
+    @param   args.start_time           earliest timestamp of messages to read
+    @param   args.end_time             latest timestamp of messages to read
+    @param   args.start_index          message index within topic to start from
+    @param   args.end_index            message index within topic to stop at
+    @param   args.unique               emit messages that are unique in topic
+    @param   args.select_field         message fields to use for uniqueness if not all
+    @param   args.noselect_field       message fields to skip for uniqueness
+    @param   args.nth_message          read every Nth message in topic, starting from first
+    @param   args.nth_interval         minimum time interval between messages in topic,
+                                       as seconds or ROS duration
+    @param   args.condition            Python expressions that must evaluate as true
+                                       for message to be processable, see ConditionMixin
     """
     DEFAULT_ARGS = dict(FILE=[], LIVE=False, APP=False, ITERABLE=None)
     args = {"FILE": str(args)} if isinstance(args, common.PATH_TYPES) else args
     args = common.ensure_namespace(args, DEFAULT_ARGS, **kwargs)
     if not _inited: init(args)
 
     if common.is_iterable(args.APP) and not common.is_iterable(args.ITERABLE):
         args.APP, args.ITERABLE = True, args.APP
-    result = (TopicSource if args.LIVE else AppSource if args.APP else BagSource)(args)
+    result = (LiveSource if args.LIVE else AppSource if args.APP else BagSource)(args)
     result.validate()
     return result
 
 
 def sink(args=None, **kwargs):
     """
     Convenience for creating a {@link grepros.outputs.Sink Sink} instance from arguments,
@@ -361,12 +372,12 @@
     # Switch message metadata cache to constrain on total number instead of time
     api.TypeMeta.LIFETIME, api.TypeMeta.POPULATION = 0, 100
     _inited = True
 
 
 
 __all__ = [
-    "AppSink", "AppSource", "Bag", "BagSink", "BagSource", "ConsoleSink", "CsvSink", "HtmlSink",
-    "McapBag", "McapSink", "MultiSink", "ParquetSink", "PostgresSink", "Scanner", "Sink", "Source",
-    "SqliteSink", "SqlSink", "TopicSink", "TopicSource",
-    "grep", "init", "sink", "source",
+    "AppSink", "AppSource", "Bag", "BagSink", "BagSource", "BooleanResult", "ConsoleSink",
+    "CsvSink", "ExpressionTree", "HtmlSink", "LiveSink", "LiveSource", "McapBag", "McapSink",
+    "MultiSink", "ParquetSink", "PostgresSink", "Scanner", "Sink", "Source", "SqliteSink",
+    "SqlSink", "grep", "init", "sink", "source",
 ]
```

### html2text {}

```diff
@@ -1,156 +1,166 @@
 # -*- coding: utf-8 -*- """ grepros library interface. Source classes: - {@link
 grepros.inputs.AppSource AppSource}: produces messages from iterable or pushed
 data - {@link grepros.inputs.BagSource BagSource}: produces messages from ROS
-bagfiles - {@link grepros.inputs.TopicSource TopicSource}: produces messages
-from live ROS topics Sink classes: - {@link grepros.outputs.AppSink AppSink}:
+bagfiles - {@link grepros.inputs.LiveSource LiveSource}: produces messages from
+live ROS topics Sink classes: - {@link grepros.outputs.AppSink AppSink}:
 provides messages to callback function - {@link grepros.outputs.BagSink
 BagSink}: writes messages to bagfile - {@link grepros.outputs.ConsoleSink
 ConsoleSink}: prints messages to console - {@link
 grepros.plugins.auto.csv.CsvSink CsvSink}: writes messages to CSV files, each
 topic to a separate file - {@link grepros.plugins.auto.html.HtmlSink HtmlSink}:
-writes messages to an HTML file - {@link grepros.plugins.mcap.McapSink
+writes messages to an HTML file - {@link grepros.outputs.LiveSink LiveSink}:
+publishes messages to ROS topics - {@link grepros.plugins.mcap.McapSink
 McapSink}: writes messages to MCAP file - {@link grepros.outputs.MultiSink
 MultiSink}: combines any number of sinks - {@link
 grepros.plugins.parquet.ParquetSink ParquetSink}: writes messages to Apache
 Parquet files - {@link grepros.plugins.auto.postgres.PostgresSink
 PostgresSink}: writes messages to a Postgres database - {@link
 grepros.plugins.auto.sqlite.SqliteSink SqliteSink}: writes messages to an
 SQLite database - {@link grepros.plugins.sql.SqlSink SqlSink}: writes SQL
-schema file for message type tables and topic views - {@link
-grepros.outputs.TopicSink TopicSink}: publishes messages to ROS topics {@link
-grepros.api.BaseBag Bag}: generic bag interface. {@link grepros.search.Scanner
-Scanner}: ROS message grepper. Format-specific bag classes: - {@link
-grepros.ros1.ROS1Bag ROS1Bag}: ROS1 bag reader and writer in .bag format -
-{@link grepros.ros2.ROS2Bag ROS2Bag}: ROS2 bag reader and writer in .db3 SQLite
-format - {@link grepros.plugins.embag.EmbagReader EmbagReader}: ROS1 bag reader
-using the _e_m_b_a_g library - {@link grepros.plugins.mcap.McapBag McapBag}: ROS1/
-ROS2 bag reader and writer in MCAP format Output sink `write_options` arguments
-can be given with underscores instead of dashes, e.g. `"rollover_size"` instead
-of `"rollover-size"`. ---------------------------------------------------------
---------------------- This file is part of grepros - grep for ROS bag files and
+schema file for message type tables and topic views {@link grepros.api.BaseBag
+Bag}: generic bag interface. {@link grepros.search.Scanner Scanner}: ROS
+message grepper. Format-specific bag classes: - {@link grepros.ros1.ROS1Bag
+ROS1Bag}: ROS1 bag reader and writer in .bag format - {@link
+grepros.ros2.ROS2Bag ROS2Bag}: ROS2 bag reader and writer in .db3 SQLite format
+- {@link grepros.plugins.embag.EmbagReader EmbagReader}: ROS1 bag reader using
+the _e_m_b_a_g library - {@link grepros.plugins.mcap.McapBag McapBag}: ROS1/ROS2 bag
+reader and writer in MCAP format Output sink `write_options` arguments can be
+given with underscores instead of dashes, e.g. `"rollover_size"` instead of
+`"rollover-size"`. ------------------------------------------------------------
+------------------ This file is part of grepros - grep for ROS bag files and
 live topics. Released under the BSD License. @author Erki Suurjaak @created
-09.12.2022 @modified 28.12.2023 -----------------------------------------------
+09.12.2022 @modified 22.03.2024 -----------------------------------------------
 ------------------------------- """ ## @namespace grepros.library from .
 plugins.auto.csv import CsvSink from . plugins.auto.html import HtmlSink from .
 plugins.auto.postgres import PostgresSink from . plugins.auto.sqlite import
 SqliteSink from . plugins.mcap import McapBag, McapSink from . plugins.parquet
 import ParquetSink from . plugins.sql import SqlSink from . api import Bag from
-. inputs import AppSource, BagSource, Source, TopicSource from . outputs import
-AppSink, BagSink, ConsoleSink, MultiSink, Sink, TopicSink from . search import
-Scanner from . import api from . import common from . import main from . import
-plugins _inited = False def grep(args=None, **kwargs): """ Yields matching
-messages from specified source. Initializes grepros if not already initialized.
-Read from bagfiles: `grep(file="2022-10-*.bag", pattern="cpu")`. Read from live
-topics: `grep(live=True, pattern="cpu")`. @param args arguments as namespace or
-dictionary, case-insensitive; or a single path as the ROS bagfile to read, or
-one or more {@link grepros.api.Bag Bag} instances @param kwargs any and all
+. inputs import AppSource, BagSource, LiveSource, Source from . outputs import
+AppSink, BagSink, ConsoleSink, LiveSink, MultiSink, Sink from . search import
+BooleanResult, ExpressionTree, Scanner from . import api from . import common
+from . import plugins _inited = False def grep(args=None, **kwargs): """ Yields
+matching messages from specified source. Initializes grepros if not already
+initialized. Read from bagfiles: `grep(file="2022-10-*.bag", pattern="cpu")`.
+Read from live topics: `grep(live=True, pattern="cpu")`. @param args arguments
+as namespace or dictionary, case-insensitive; or a single path as the ROS
+bagfile to read, or one or more {@link grepros.api.Bag Bag} instances, or a
+{@link grepros.inputs.Source Source} instance @param kwargs any and all
 arguments as keyword overrides, case-insensitive Bag source: @param args.file
 names of ROS bagfiles to read if not all in directory @param args.path paths to
 scan if not current directory @param args.recurse recurse into subdirectories
 when looking for bagfiles @param args.decompress decompress archived bags to
 file directory @param args.reindex make a copy of unindexed bags and reindex
 them (ROS1 only) @param args.orderby "topic" or "type" if any to group results
-by @param args.bag one or more {@link grepros.api.Bag Bag} instances Live
-source: @param args.live whether reading messages from live ROS topics @param
-args.queue_size_in subscriber queue size (default 10) @param args.ros_time_in
-stamp messages with ROS time instead of wall time App source: @param args.app
-whether reading messages from iterable or pushed data; may contain the iterable
-itself @param args.iterable iterable yielding (topic, msg, stamp) or (topic,
-msg); yielding `None` signals end of content Any source: @param args.topic ROS
-topics to read if not all @param args.type ROS message types to read if not all
-@param args.skip_topic ROS topics to skip @param args.skip_type ROS message
-types to skip @param args.start_time earliest timestamp of messages to read
-@param args.end_time latest timestamp of messages to read @param
-args.start_index message index within topic to start from @param args.end_index
-message index within topic to stop at @param args.nth_message read every Nth
-message in topic @param args.nth_interval minimum time interval between
-messages in topic @param args.select_field message fields to use in matching if
-not all @param args.noselect_field message fields to skip in matching @param
-args.unique emit messages that are unique in topic (select_field and
-noselect_field apply if specified) @param args.condition Python expressions
-that must evaluate as true for message to be processable, see ConditionMixin
-Search‍: @param args.pattern pattern(s) to find in message field values @param
-args.fixed_string pattern contains ordinary strings, not regular expressions
-@param args.case use case-sensitive matching in pattern @param args.invert
-select messages not matching pattern @param args.nth_match emit every Nth match
-in topic @param args.max_count number of matched messages to emit (per file if
-bag input) @param args.max_per_topic number of matched messages to emit from
-each topic @param args.max_topics number of topics to print matches from @param
-args.before number of messages of leading context to emit before match @param
-args.after number of messages of trailing context to emit after match @param
-args.context number of messages of leading and trailing context to emit around
-match @param args.highlight highlight matched values @param args.match_wrapper
-string to wrap around matched values, both sides if one value, start and end if
-more than one, or no wrapping if zero values @return {@link
-grepros.Scanner.GrepMessage GrepMessage} namedtuples of (topic, message,
-timestamp, match, index) """ DEFAULT_ARGS = dict(FILE=[], LIVE=False,
-APP=False, ITERABLE=None, COLOR="never", HIGHLIGHT=False) args0 = args is_bag =
-isinstance(args, Bag) or \ common.is_iterable(args) and all(isinstance(x, Bag)
-for x in args) args = {"FILE": str(args)} if isinstance(args,
-common.PATH_TYPES) else \ {} if is_bag or isinstance(args, Source) else args
-args = common.ensure_namespace(args, DEFAULT_ARGS, **kwargs) main.validate_args
-(main.process_args(args)) if not _inited: init(args) if common.is_iterable
-(args.APP) and not common.is_iterable(args.ITERABLE): args.APP, args.ITERABLE =
-True, args.APP src = args0 if isinstance(args0, Source) else \ TopicSource
-(args) if args.LIVE else \ AppSource(args) if args.APP else \ BagSource(args0,
-**vars(args)) if is_bag else BagSource(args) src.validate() try: for x in
-Scanner(args).find(src): yield x finally: if not isinstance(args0, (Bag,
-Source)): src.close() def source(args=None, **kwargs): """ Convenience for
-creating a {@link grepros.inputs.Source Source} instance from arguments.
-Initializes grepros if not already initialized. @param args arguments as
-namespace or dictionary, case-insensitive; or a single path as the ROS bagfile
-to read @param kwargs any and all arguments as keyword overrides, case-
-insensitive @param args.file one or more names of ROS bagfiles to read from
-@param args.live read messages from live ROS topics instead @param args.app
-read messages from iterable or pushed data instead; may contain the iterable
-itself Bag source: @param args.file names of ROS bagfiles to read if not all in
-directory @param args.path paths to scan if not current directory @param
-args.recurse recurse into subdirectories when looking for bagfiles @param
-args.orderby "topic" or "type" if any to group results by @param
+by @param args.timescale emit messages on original timeline from first message
+at given rate, 0 disables @param args.timescale_emission start timeline from
+first matched message not first in bag @param args.bag one or more {@link
+grepros.api.Bag Bag} instances Live source: @param args.live whether reading
+messages from live ROS topics @param args.queue_size_in subscriber queue size
+(default 10) @param args.ros_time_in stamp messages with ROS time instead of
+wall time App source: @param args.app whether reading messages from iterable or
+pushed data; may contain the iterable itself @param args.iterable iterable
+yielding (topic, msg, stamp) or (topic, msg); yielding `None` signals end of
+content Any source: @param args.topic ROS topics to read if not all @param
+args.type ROS message types to read if not all @param args.skip_topic ROS
+topics to skip @param args.skip_type ROS message types to skip @param
+args.start_time earliest timestamp of messages to read @param args.end_time
+latest timestamp of messages to read @param args.start_index message index
+within topic to start from @param args.end_index message index within topic to
+stop at @param args.nth_message read every Nth message in topic, starting from
+first @param args.nth_interval minimum time interval between messages in topic,
+as seconds or ROS duration @param args.select_field message fields to use in
+matching if not all @param args.noselect_field message fields to skip in
+matching @param args.unique emit messages that are unique in topic
+(select_field and noselect_field apply if specified) @param args.condition
+Python expressions that must evaluate as true for message to be processable,
+see ConditionMixin Search‍: @param args.pattern pattern(s) to find in message
+field values @param args.fixed_string pattern contains ordinary strings, not
+regular expressions @param args.case use case-sensitive matching in pattern
+@param args.invert select messages not matching pattern @param args.expression
+pattern(s) are a logical expression like 'this AND (this2 OR NOT "skip this")',
+with elements as patterns to find in message fields @param args.nth_match emit
+every Nth match in topic, starting from first @param args.max_count number of
+matched messages to emit (per file if bag input) @param args.max_per_topic
+number of matched messages to emit from each topic @param args.max_topics
+number of topics to print matches from @param args.before number of messages of
+leading context to emit before match @param args.after number of messages of
+trailing context to emit after match @param args.context number of messages of
+leading and trailing context to emit around match @param args.highlight
+highlight matched values @param args.match_wrapper string to wrap around
+matched values, both sides if one value, start and end if more than one, or no
+wrapping if zero values @return {@link grepros.Scanner.GrepMessage GrepMessage}
+namedtuples of (topic, message, timestamp, match, index) """ DEFAULT_ARGS =
+dict(FILE=[], LIVE=False, APP=False, ITERABLE=None, COLOR="never",
+HIGHLIGHT=False) args0 = args is_bag = isinstance(args, Bag) or \
+common.is_iterable(args) and all(isinstance(x, Bag) for x in args) args =
+{"FILE": str(args)} if isinstance(args, common.PATH_TYPES) else \ {} if is_bag
+or isinstance(args, Source) else args args = common.ArgumentUtil.validate
+(common.ensure_namespace(args, DEFAULT_ARGS, **kwargs)) if not _inited: init
+(args) if common.is_iterable(args.APP) and not common.is_iterable
+(args.ITERABLE): args.APP, args.ITERABLE = True, args.APP src = args0 if
+isinstance(args0, Source) else \ LiveSource(args) if args.LIVE else \ AppSource
+(args) if args.APP else \ BagSource(args0, **vars(args)) if is_bag else
+BagSource(args) if args and isinstance(args0, Source): src.configure(**kwargs)
+src.validate() try: for x in Scanner(args).find(src): yield x finally: if not
+isinstance(args0, (Bag, Source)): src.close() def source(args=None, **kwargs):
+""" Convenience for creating a {@link grepros.inputs.Source Source} instance
+from arguments. Initializes grepros if not already initialized. @param args
+arguments as namespace or dictionary, case-insensitive; or a single path as the
+ROS bagfile to read @param kwargs any and all arguments as keyword overrides,
+case-insensitive @param args.file one or more names of ROS bagfiles to read
+from @param args.live read messages from live ROS topics instead @param
+args.app read messages from iterable or pushed data instead; may contain the
+iterable itself Bag source: @param args.file names of ROS bagfiles to read if
+not all in directory @param args.path paths to scan if not current directory
+@param args.recurse recurse into subdirectories when looking for bagfiles
+@param args.orderby "topic" or "type" if any to group results by @param
 args.decompress decompress archived bags to file directory @param args.reindex
-make a copy of unindexed bags and reindex them (ROS1 only) @param args.progress
-whether to print progress bar Live source: @param args.queue_size_in subscriber
-queue size (default 10) @param args.ros_time_in stamp messages with ROS time
-instead of wall time @param args.progress whether to print progress bar App
-source: @param args.iterable iterable yielding (topic, msg, stamp) or (topic,
-msg); yielding `None` signals end of content Any source: @param args.topic ROS
-topics to read if not all @param args.type ROS message types to read if not all
-@param args.skip_topic ROS topics to skip @param args.skip_type ROS message
-types to skip @param args.start_time earliest timestamp of messages to read
-@param args.end_time latest timestamp of messages to read @param
-args.start_index message index within topic to start from @param args.end_index
-message index within topic to stop at @param args.unique emit messages that are
-unique in topic @param args.select_field message fields to use for uniqueness
-if not all @param args.noselect_field message fields to skip for uniqueness
-@param args.nth_message read every Nth message in topic @param
-args.nth_interval minimum time interval between messages in topic @param
-args.condition Python expressions that must evaluate as true for message to be
-processable, see ConditionMixin """ DEFAULT_ARGS = dict(FILE=[], LIVE=False,
-APP=False, ITERABLE=None) args = {"FILE": str(args)} if isinstance(args,
-common.PATH_TYPES) else args args = common.ensure_namespace(args, DEFAULT_ARGS,
-**kwargs) if not _inited: init(args) if common.is_iterable(args.APP) and not
-common.is_iterable(args.ITERABLE): args.APP, args.ITERABLE = True, args.APP
-result = (TopicSource if args.LIVE else AppSource if args.APP else BagSource)
-(args) result.validate() return result def sink(args=None, **kwargs): """
-Convenience for creating a {@link grepros.outputs.Sink Sink} instance from
-arguments, {@link grepros.outputs.MultiSink MultiSink} if several outputs.
-Initializes grepros if not already initialized. @param args arguments as
-namespace or dictionary, case-insensitive; or a single item as sink target like
-bag filename @param kwargs any and all arguments as keyword overrides, case-
-insensitive @param args.app provide messages to given callback function @param
-args.console print matches to console @param args.publish publish matches to
-live topics @param args.write file or other target like Postgres database to
-write, as "target", or ["target", dict(format="format", ..)] or [[..target1..],
-[..target2..], ..] @param args.write_options format-specific options like
-{"overwrite": whether to overwrite existing file (default false)} Console sink:
-@param args.line_prefix print source prefix like bag filename on each message
-line @param args.max_field_lines maximum number of lines to print per field
-@param args.start_line message line number to start output from @param
-args.end_line message line number to stop output at @param
+make a copy of unindexed bags and reindex them (ROS1 only) @param
+args.timescale emit messages on original timeline from first message at given
+rate, 0 disables @param args.timescale_emission start timeline from first
+matched message not first in bag @param args.progress whether to print progress
+bar Live source: @param args.queue_size_in subscriber queue size (default 10)
+@param args.ros_time_in stamp messages with ROS time instead of wall time
+@param args.progress whether to print progress bar App source: @param
+args.iterable iterable yielding (topic, msg, stamp) or (topic, msg); yielding
+`None` signals end of content Any source: @param args.topic ROS topics to read
+if not all @param args.type ROS message types to read if not all @param
+args.skip_topic ROS topics to skip @param args.skip_type ROS message types to
+skip @param args.start_time earliest timestamp of messages to read @param
+args.end_time latest timestamp of messages to read @param args.start_index
+message index within topic to start from @param args.end_index message index
+within topic to stop at @param args.unique emit messages that are unique in
+topic @param args.select_field message fields to use for uniqueness if not all
+@param args.noselect_field message fields to skip for uniqueness @param
+args.nth_message read every Nth message in topic, starting from first @param
+args.nth_interval minimum time interval between messages in topic, as seconds
+or ROS duration @param args.condition Python expressions that must evaluate as
+true for message to be processable, see ConditionMixin """ DEFAULT_ARGS = dict
+(FILE=[], LIVE=False, APP=False, ITERABLE=None) args = {"FILE": str(args)} if
+isinstance(args, common.PATH_TYPES) else args args = common.ensure_namespace
+(args, DEFAULT_ARGS, **kwargs) if not _inited: init(args) if common.is_iterable
+(args.APP) and not common.is_iterable(args.ITERABLE): args.APP, args.ITERABLE =
+True, args.APP result = (LiveSource if args.LIVE else AppSource if args.APP
+else BagSource)(args) result.validate() return result def sink(args=None,
+**kwargs): """ Convenience for creating a {@link grepros.outputs.Sink Sink}
+instance from arguments, {@link grepros.outputs.MultiSink MultiSink} if several
+outputs. Initializes grepros if not already initialized. @param args arguments
+as namespace or dictionary, case-insensitive; or a single item as sink target
+like bag filename @param kwargs any and all arguments as keyword overrides,
+case-insensitive @param args.app provide messages to given callback function
+@param args.console print matches to console @param args.publish publish
+matches to live topics @param args.write file or other target like Postgres
+database to write, as "target", or ["target", dict(format="format", ..)] or [
+[..target1..], [..target2..], ..] @param args.write_options format-specific
+options like {"overwrite": whether to overwrite existing file (default false)}
+Console sink: @param args.line_prefix print source prefix like bag filename on
+each message line @param args.max_field_lines maximum number of lines to print
+per field @param args.start_line message line number to start output from
+@param args.end_line message line number to stop output at @param
 args.max_message_lines maximum number of lines to output per message @param
 args.lines_around_match number of message lines around matched fields to output
 @param args.matched_fields_only output only the fields where match was found
 @param args.wrap_width character width to wrap message YAML output at @param
 args.match_wrapper string to wrap around matched values, both sides if one
 value, start and end if more than one, or no wrapping if zero values Console /
 HTML sink: @param args.color False or "never" for not using colors in
@@ -193,11 +203,12 @@
 apimode=True) api.validate() plugins.init(args) for x in (plugins.mcap,
 plugins.parquet, plugins.sql): try: plugins.configure(PLUGIN=x) except
 Exception: pass Bag.READER_CLASSES.add(McapBag) # Ensure MCAP files at least
 get recognized, Bag.WRITER_CLASSES.add(McapBag) # even if loading them will
 fail when dependencies missing if args.PLUGIN: plugins.configure(args) # Switch
 message metadata cache to constrain on total number instead of time
 api.TypeMeta.LIFETIME, api.TypeMeta.POPULATION = 0, 100 _inited = True __all__
-= [ "AppSink", "AppSource", "Bag", "BagSink", "BagSource", "ConsoleSink",
-"CsvSink", "HtmlSink", "McapBag", "McapSink", "MultiSink", "ParquetSink",
-"PostgresSink", "Scanner", "Sink", "Source", "SqliteSink", "SqlSink",
-"TopicSink", "TopicSource", "grep", "init", "sink", "source", ]
+= [ "AppSink", "AppSource", "Bag", "BagSink", "BagSource", "BooleanResult",
+"ConsoleSink", "CsvSink", "ExpressionTree", "HtmlSink", "LiveSink",
+"LiveSource", "McapBag", "McapSink", "MultiSink", "ParquetSink",
+"PostgresSink", "Scanner", "Sink", "Source", "SqliteSink", "SqlSink", "grep",
+"init", "sink", "source", ]
```

## grepros/main.py

```diff
@@ -4,31 +4,28 @@
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     23.10.2021
-@modified    27.12.2023
+@modified    24.03.2024
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.main
 import argparse
 import atexit
-import collections
-import logging
 import os
 import random
-import re
+import signal
 import sys
-
-import six
+import traceback
 
 from . import __title__, __version__, __version_date__, api, inputs, outputs, search
-from . common import ConsolePrinter, MatchMarkers, parse_datetime
+from . common import ArgumentUtil, ConsolePrinter, MatchMarkers
 from . import plugins
 
 
 
 ## Configuration for argparse, as {description, epilog, args: [..], groups: {name: [..]}}
 ARGUMENTS = {
     "description": "Searches through messages in ROS bag files or live topics.",
@@ -94,15 +91,21 @@
 
         dict(args=["-I", "--no-ignore-case"],
              dest="CASE", action="store_true",
              help="use case-sensitive matching in PATTERNs"),
 
         dict(args=["-v", "--invert-match"],
              dest="INVERT", action="store_true",
-             help="select messages not matching PATTERN"),
+             help="select messages not matching PATTERNs"),
+
+        dict(args=["-e", "--expression"],
+             dest="EXPRESSION", action="store_true",
+             help="PATTERNs are a logical expression\n"
+                  "like 'this AND (this2 OR NOT \"skip this\")',\n"
+                  "with elements as patterns to find in message fields"),
 
         dict(args=["--version"],
              dest="VERSION", action="version",
              version="%s: grep for ROS bag files and live topics, v%s (%s)" %
                      (__title__, __version__, __version_date__),
              help="display version information and exit"),
 
@@ -188,23 +191,23 @@
         dict(args=["-n1", "--end-index"],
              dest="END_INDEX", metavar="INDEX", type=int,
              help="message index within topic to stop at\n"
                   "(1-based if positive, counts back from bag total if negative)"),
 
         dict(args=["--every-nth-message"],
              dest="NTH_MESSAGE", metavar="NUM", type=int, default=1,
-             help="read every Nth message within topic"),
+             help="read every Nth message within topic, starting from first"),
 
         dict(args=["--every-nth-interval"],
-             dest="NTH_INTERVAL", metavar="SECONDS", type=int, default=0,
+             dest="NTH_INTERVAL", metavar="SECONDS", type=float, default=0,
              help="read messages at least N seconds apart within topic"),
 
         dict(args=["--every-nth-match"],
              dest="NTH_MATCH", metavar="NUM", type=int, default=1,
-             help="emit every Nth match in topic"),
+             help="emit every Nth match in topic, starting from first"),
 
         dict(args=["-sf", "--select-field"],
              dest="SELECT_FIELD", metavar="FIELD", nargs="+", default=[], action="append",
              help="message fields to use in matching if not all\n"
                   "(supports nested.paths and * wildcards)"),
 
         dict(args=["-ns", "--no-select-field"],
@@ -312,15 +315,15 @@
              help="do not print matches to console"),
 
         dict(args=["--progress"], dest="PROGRESS", action="store_true",
              help="show progress bar when not printing matches to console"),
 
         dict(args=["--verbose"], dest="VERBOSE", action="store_true",
              help="print status messages during console output\n"
-                  "for publishing and writing"),
+                  "for publishing and writing, and error stacktraces"),
 
         dict(args=["--no-verbose"], dest="SKIP_VERBOSE", action="store_true",
              help="do not print status messages during console output\n"
                   "for publishing and writing"),
 
     ], "Bag input control": [
 
@@ -346,14 +349,23 @@
              dest="DECOMPRESS", action="store_true",
              help="decompress archived bagfiles with recognized extensions (.zst .zstd)"),
 
         dict(args=["--reindex-if-unindexed"],
              dest="REINDEX", action="store_true",
              help="reindex unindexed bagfiles (ROS1 only), makes backup copies"),
 
+        dict(args=["--time-scale"],
+             dest="TIMESCALE", metavar="FACTOR", nargs="?", type=float, const=1, default=0,
+             help="emit messages on original bag timeline from first matched message,\n"
+                  "optionally with a speedup or slowdown factor"),
+
+        dict(args=["--time-scale-emission"],
+             dest="TIMESCALE_EMISSION", nargs="?", type=int, const=True, default=True,
+             help=argparse.SUPPRESS),  # Timeline from first matched message vs first in bag
+
     ], "Live topic control": [
 
         dict(args=["--publish-prefix"],
              dest="PUBLISH_PREFIX", metavar="PREFIX", default="",
              help="prefix to prepend to input topic name on publishing match"),
 
         dict(args=["--publish-suffix"],
@@ -381,211 +393,110 @@
     ]},
 }
 
 ## List of command-line arguments the program was invoked with
 CLI_ARGS = None
 
 
-class HelpFormatter(argparse.RawTextHelpFormatter):
-    """RawTextHelpFormatter returning custom metavar for WRITE."""
-
-    def _format_action_invocation(self, action):
-        """Returns formatted invocation."""
-        if "WRITE" == action.dest:
-            return " ".join(action.option_strings + [action.metavar])
-        return super(HelpFormatter, self)._format_action_invocation(action)
-
-
-def make_parser():
-    """Returns a configured ArgumentParser instance."""
-    kws = dict(description=ARGUMENTS["description"], epilog=ARGUMENTS["epilog"],
-               formatter_class=HelpFormatter, add_help=False)
-    if sys.version_info >= (3, 5): kws.update(allow_abbrev=False)
-    argparser = argparse.ArgumentParser(**kws)
-    for arg in map(dict, ARGUMENTS["arguments"]):
-        argparser.add_argument(*arg.pop("args"), **arg)
-    for group, groupargs in ARGUMENTS.get("groups", {}).items():
-        grouper = argparser.add_argument_group(group)
-        for arg in map(dict, groupargs):
-            grouper.add_argument(*arg.pop("args"), **arg)
-    return argparser
-
-
-def process_args(args):
-    """
-    Converts or combines arguments where necessary, returns full args.
-
-    @param   args  arguments object like argparse.Namespace
-    """
-    for arg in sum(ARGUMENTS.get("groups", {}).values(), ARGUMENTS["arguments"][:]):
-        name = arg.get("dest") or arg["args"][0]
-        if "version" != arg.get("action") and argparse.SUPPRESS != arg.get("default") \
-        and "HELP" != name and not hasattr(args, name):
-            value = False if arg.get("store_true") else True if arg.get("store_false") else None
-            setattr(args, name, arg.get("default", value))
-
-    if args.CONTEXT:
-        args.BEFORE = args.AFTER = args.CONTEXT
-
-    # Default to printing metadata for publish/write if no console output
-    args.VERBOSE = False if args.SKIP_VERBOSE else \
-                   (args.VERBOSE or not args.CONSOLE and bool(CLI_ARGS))
-
-    # Show progress bar only if no console output
-    args.PROGRESS = args.PROGRESS and not args.CONSOLE
-
-    # Print filename prefix on each console message line if not single specific file
-    args.LINE_PREFIX = args.LINE_PREFIX and (args.RECURSE or len(args.FILE) != 1
-                                             or args.PATH or any("*" in x for x in args.FILE))
-
-    for k, v in vars(args).items():  # Flatten lists of lists and drop duplicates
-        if k != "WRITE" and isinstance(v, list):
-            here = set()
-            setattr(args, k, [x for xx in v for x in (xx if isinstance(xx, list) else [xx])
-                              if not (x in here or here.add(x))])
-
-    for n, v in [("START_TIME", args.START_TIME), ("END_TIME", args.END_TIME)]:
-        if not isinstance(v, (six.binary_type, six.text_type)): continue  # for v, n
-        try: v = float(v)
-        except Exception: pass  # If numeric, leave as string for source to process as relative time
-        try: not isinstance(v, float) and setattr(args, n, parse_datetime(v))
-        except Exception: pass
-
-    return  args
-
-
-def validate_args(args):
-    """
-    Validates arguments, prints errors, returns success.
-
-    @param   args  arguments object like argparse.Namespace
-    """
-    errors = collections.defaultdict(list)  # {category: [error, ]}
-
-    # Validate --write .. key=value
-    for opts in args.WRITE:  # List of lists, one for each --write
-        erropts = []
-        for opt in opts[1:]:
-            try: dict([opt.split("=", 1)])
-            except Exception: erropts.append(opt)
-        if erropts:
-            errors[""].append('Invalid KEY=VALUE in "--write %s": %s' %
-                              (" ".join(opts), " ".join(erropts)))
-
-    for n, v in [("START_TIME", args.START_TIME), ("END_TIME", args.END_TIME)]:
-        if v is None: continue  # for v, n
-        try: v = float(v)
-        except Exception: pass
-        try: isinstance(v, (six.binary_type, six.text_type)) and parse_datetime(v)
-        except Exception: errors[""].append("Invalid ISO datetime for %s: %s" %
-                                            (n.lower().replace("_", " "), v))
-
-    for v in args.PATTERN if not args.FIXED_STRING else ():
-        split = v.find("=", 1, -1)  # May be "PATTERN" or "attribute=PATTERN"
-        v = v[split + 1:] if split > 0 else v
-        try: re.compile(re.escape(v) if args.FIXED_STRING else v)
-        except Exception as e:
-            errors["Invalid regular expression"].append("'%s': %s" % (v, e))
-
-    for v in args.CONDITION:
-        v = inputs.ConditionMixin.TOPIC_RGX.sub("dummy", v)
-        try: compile(v, "", "eval")
-        except SyntaxError as e:
-            errors["Invalid condition"].append("'%s': %s at %schar %s" %
-                (v, e.msg, "line %s " % e.lineno if e.lineno > 1 else "", e.offset))
-        except Exception as e:
-            errors["Invalid condition"].append("'%s': %s" % (v, e))
-
-    for err in errors.get("", []):
-        ConsolePrinter.log(logging.ERROR, err)
-    for category in filter(bool, errors):
-        ConsolePrinter.log(logging.ERROR, category)
-        for err in errors[category]:
-            ConsolePrinter.log(logging.ERROR, "  %s" % err)
-    return not errors
-
-
 def flush_stdout():
     """Writes a linefeed to sdtout if nothing has been printed to it so far."""
     if not ConsolePrinter.PRINTS.get(sys.stdout) and not sys.stdout.isatty():
         try: print()  # Piping cursed output to `more` remains paging if nothing is printed
         except (Exception, KeyboardInterrupt): pass
 
 
-def preload_plugins():
+def preload_plugins(cli_args):
     """Imports and initializes plugins from auto-load folder and from arguments."""
     plugins.add_write_format("bag", outputs.BagSink, "bag", [
         ("overwrite=true|false",   "overwrite existing file\nin bag output\n"
                                    "instead of appending to if bag or database\n"
                                    "or appending unique counter to file name\n"
                                    "(default false)")
 
     ] + outputs.RolloverSinkMixin.get_write_options("bag"))
-    args = make_parser().parse_known_args(CLI_ARGS)[0] if "--plugin" in CLI_ARGS else None
-    try: plugins.init(process_args(args) if args else None)
+    args = None
+    if "--plugin" in cli_args:
+        args, _ = ArgumentUtil.make_parser(ARGUMENTS).parse_known_args(cli_args)
+        args = ArgumentUtil.flatten(args)
+    try: plugins.init(args)
     except ImportWarning: sys.exit(1)
 
 
+def make_thread_excepthook(args, exitcode_dict):
+    """Returns thread exception handler: function(text, exc) prints error, stops application."""
+    def thread_excepthook(text, exc):
+        """Prints error, sets exitcode flag, shuts down ROS node if any, interrupts main thread."""
+        ConsolePrinter.error(text)
+        if args.VERBOSE: traceback.print_exc()
+        exitcode_dict["value"] = 1
+        api.shutdown_node()
+        os.kill(os.getpid(), signal.SIGINT)
+    return thread_excepthook
+
+
 def run():
     """Parses command-line arguments and runs search."""
     global CLI_ARGS
     CLI_ARGS = sys.argv[1:]
     MatchMarkers.populate("%08x" % random.randint(1, 1E9))
-    preload_plugins()
-    argparser = make_parser()
+    preload_plugins(CLI_ARGS)
+    argparser = ArgumentUtil.make_parser(ARGUMENTS)
     if not CLI_ARGS:
         argparser.print_usage()
         return
 
     atexit.register(flush_stdout)
     args = argparser.parse_args(CLI_ARGS)
     if args.HELP:
         argparser.print_help()
         return
 
     BREAK_EXS = (KeyboardInterrupt, )
     try: BREAK_EXS += (BrokenPipeError, )  # Py3
     except NameError: pass  # Py2
 
+    exitcode = {"value": 0}
     source, sink = None, None
     try:
         ConsolePrinter.configure({"always": True, "never": False}.get(args.COLOR))
-        if not validate_args(process_args(args)):
-            sys.exit(1)
+        api.validate()
+        args = ArgumentUtil.validate(args, cli=True)
 
         source = plugins.load("source", args) or \
-                 (inputs.TopicSource if args.LIVE else inputs.BagSource)(args)
+                 (inputs.LiveSource if args.LIVE else inputs.BagSource)(args)
         if not source.validate():
             sys.exit(1)
         sink = outputs.MultiSink(args)
         sink.sinks.extend(filter(bool, plugins.load("sink", args, collect=True)))
         if not sink.validate():
             sys.exit(1)
 
-        thread_excepthook = lambda t, e: (ConsolePrinter.error(t), sys.exit(1))
-        source.thread_excepthook = sink.thread_excepthook = thread_excepthook
+        source.thread_excepthook = sink.thread_excepthook = make_thread_excepthook(args, exitcode)
         grepper = plugins.load("scan", args) or search.Scanner(args)
         grepper.work(source, sink)
     except BREAK_EXS:
-        try: source and source.close()
-        except (Exception, KeyboardInterrupt): pass
         try: sink and sink.close()
         except (Exception, KeyboardInterrupt): pass
+        try: source and source.close()
+        except (Exception, KeyboardInterrupt): pass
         # Redirect remaining output to devnull to avoid another BrokenPipeError
         try: os.dup2(os.open(os.devnull, os.O_WRONLY), sys.stdout.fileno())
         except (Exception, KeyboardInterrupt): pass
-        sys.exit()
+        sys.exit(exitcode["value"])
+    except Exception as e:
+        ConsolePrinter.error(e)
+        if args.VERBOSE: traceback.print_exc()
     finally:
         sink and sink.close()
         source and source.close()
-        api.shutdown_node()
+        try: api.shutdown_node()
+        except BREAK_EXS: pass
 
 
 __all__ = [
-    "ARGUMENTS", "CLI_ARGS", "HelpFormatter",
-    "make_parser", "process_args", "validate_args", "flush_stdout", "preload_plugins", "run",
+    "ARGUMENTS", "CLI_ARGS", "flush_stdout", "make_thread_excepthook", "preload_plugins", "run",
 ]
 
 
 
 if "__main__" == __name__:
     run()
```

## grepros/outputs.py

```diff
@@ -4,15 +4,15 @@
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     23.10.2021
-@modified    28.12.2023
+@modified    21.04.2024
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.outputs
 from __future__ import print_function
 import atexit
 import collections
 import datetime
@@ -21,15 +21,15 @@
 import sys
 
 import six
 import yaml
 
 from . import api
 from . import common
-from . common import ConsolePrinter, MatchMarkers
+from . common import ArgumentUtil, ConsolePrinter, MatchMarkers
 from . inputs import Source
 
 
 class Sink(object):
     """Output base class."""
 
     ## Auto-detection file extensions for subclasses, as (".ext", )
@@ -81,17 +81,29 @@
         topickey = api.TypeMeta.make(msg, topic).topickey
         self._counts[topickey] = self._counts.get(topickey, 0) + 1
 
     def bind(self, source):
         """Attaches source to sink."""
         self.source = source
 
+    def configure(self, args=None, **kwargs):
+        """
+        Updates sink configuration.
+
+        @param   args    arguments as namespace or dictionary, case-insensitive
+        @param   kwargs  any and all arguments as keyword overrides, case-insensitive
+        """
+        self.args = common.ensure_namespace(args, vars(self.args), **kwargs)
+        self.valid = None
+
     def validate(self):
-        """Returns whether sink prerequisites are met (like ROS environment set if TopicSink)."""
-        if self.valid is None: self.valid = True
+        """Returns whether sink prerequisites are met (like ROS environment set if LiveSink)."""
+        if self.valid is not None: return self.valid
+        try: self.args, self.valid = ArgumentUtil.validate(self.args), True
+        except Exception: self.valid = False
         return self.valid
 
     def close(self):
         """Shuts down output, closing any files or connections."""
         self._batch_meta.clear()
         self._counts.clear()
 
@@ -152,19 +164,27 @@
         """
         self._prefix       = ""    # Put before each message line (filename if grepping 1+ files)
         self._wrapper      = None  # TextWrapper instance
         self._patterns     = {}    # {key: [(() if any field else ('path', ), re.Pattern), ]}
         self._format_repls = {}    # {text to replace if highlight: replacement text}
         self._styles = collections.defaultdict(str)  # {label: ANSI code string}
 
-        self._configure(common.ensure_namespace(args, TextSinkMixin.DEFAULT_ARGS, **kwargs))
+
+    def validate(self):
+        """Returns whether arguments are valid, emits error if not, else populates options."""
+        args = common.ensure_namespace(self.args, TextSinkMixin.DEFAULT_ARGS)
+        try: args = ArgumentUtil.validate(args)
+        except Exception: return False
+        self._configure(args)
+        return True
 
 
     def format_message(self, msg, highlight=False):
         """Returns message as formatted string, optionally highlighted for matches if configured."""
+        if self.args.MAX_MESSAGE_LINES == 0: return ""
         text = self.message_to_yaml(msg).rstrip("\n")
 
         highlight = highlight and self.args.HIGHLIGHT
         if self._prefix or self.args.START_LINE or self.args.END_LINE \
         or self.args.MAX_MESSAGE_LINES or (self.args.LINES_AROUND_MATCH and highlight):
             lines = text.splitlines()
 
@@ -253,18 +273,19 @@
             return yaml.safe_dump(truncate(val), default_style='"', width=sys.maxsize).rstrip("\n")
         if isinstance(val, (list, tuple)):
             if not val:
                 return "[]"
             if api.scalar(typename) in api.ROS_STRING_TYPES:
                 yaml_str = yaml.safe_dump(truncate(val)).rstrip('\n')
                 return "\n" + "\n".join(indent + line for line in yaml_str.splitlines())
-            vals = [x for v in truncate(val) for x in [self.message_to_yaml(v, top, typename)] if x]
+            vals = [x for i, v in enumerate(truncate(val))
+                    for x in [self.message_to_yaml(v, top + (i, ), typename)] if x]
             if api.scalar(typename) in api.ROS_NUMERIC_TYPES:
                 return "[%s]" % ", ".join(unquote(str(v)) for v in vals)
-            return ("\n" + "\n".join(indent + "- " + v for v in vals)) if vals else ""
+            return ("\n" + "\n".join(indent + "-   " + v for v in vals)) if vals else ""
         if api.is_ros_message(val):
             MATCHED_ONLY = self.args.MATCHED_FIELDS_ONLY and not self.args.LINES_AROUND_MATCH
             vals, fieldmap = [], api.get_message_fields(val)
             prints, noprints = self._patterns["print"], self._patterns["noprint"]
             fieldmap = api.filter_fields(fieldmap, top, include=prints, exclude=noprints)
             for k, t in fieldmap.items():
                 v = self.message_to_yaml(api.get_message_value(val, k, t), top + (k, ), t)
@@ -285,17 +306,20 @@
             return ("\n" if indent and vals else "") + "\n".join(vals)
 
         return str(val)
 
 
     def _configure(self, args):
         """Initializes output settings."""
+        self._patterns.clear()
+        self._styles.clear()
+        self._styles.default_factory = str
         prints, noprints = args.EMIT_FIELD, args.NOEMIT_FIELD
         for key, vals in [("print", prints), ("noprint", noprints)]:
-            self._patterns[key] = [(tuple(v.split(".")), common.wildcard_to_regex(v)) for v in vals]
+            self._patterns[key] = [(tuple(v.split(".")), common.path_to_regex(v)) for v in vals]
 
         if args.COLOR not in ("never", False):
             self._styles.update({"hl0":  ConsolePrinter.STYLE_HIGHLIGHT if self.args.HIGHLIGHT
                                          else "",
                                  "ll0":  ConsolePrinter.STYLE_LOWLIGHT,
                                  "pfx0": ConsolePrinter.STYLE_SPECIAL,  # Content line prefix start
                                  "sep0": ConsolePrinter.STYLE_SPECIAL2})
@@ -320,15 +344,15 @@
 
 
 
 class RolloverSinkMixin(object):
     """Provides output file rollover by size, duration, or message count."""
 
     ## Constructor argument defaults
-    DEFAULT_ARGS = dict(VERBOSE=False, WRITE=None, WRITE_OPTIONS={})
+    DEFAULT_ARGS = dict(WRITE=None, WRITE_OPTIONS={}, VERBOSE=False)
 
     ## Command-line help templates for rollover options, as [(name, text with %s label placeholder)]
     OPTIONS_TEMPLATES = [
         ("rollover-size=NUM",           "size limit for individual files\nin {label} output\n"
                                         "as bytes (supports abbreviations like 1K or 2M or 3G)"),
         ("rollover-count=NUM",          "message limit for individual files\nin {label} output\n"
                                         "(supports abbreviations like 1K or 2M or 3G)"),
@@ -357,27 +381,30 @@
                                        "rollover-duration": time span limit for individual output files,
                                                             as ROS duration or convertible seconds,
                                        "rollover-template": output filename template, supporting
                                                             strftime format codes like "%H-%M-%S"
                                                             and "%(index)s" as output file index,
                                        "overwrite": whether to overwrite existing file
                                                     (default false)}
+        @param   args.verbose         whether to emit debug information
         @param   kwargs               any and all arguments as keyword overrides, case-insensitive
         """
         self._rollover_limits = {}  # {?"size": int, ?"count": int, ?"duration": ROS duration}
         self._rollover_template = None
         self._rollover_files = collections.OrderedDict()  # {path: {"counts", "start", "size"}}
 
         ## Current output file path
         self.filename = None
 
 
     def validate(self):
         """Returns whether write options are valid, emits error if not, else populates options."""
         ok = True
+        self._rollover_limits.clear()
+        self._rollover_template = None
         for k in ("size", "count", "duration"):
             value = value0 = self.args.WRITE_OPTIONS.get("rollover-%s" % k)
             if value is None: continue  # for k
             SUFFIXES = dict(zip("smhd", [1, 60, 3600, 24*3600])) if "duration" == k else \
                        dict(zip("KMGT", [2**10, 2**20, 2**30, 2**40])) if "size" == k else \
                        dict(zip("KMGT", [10**3, 10**6, 10**9, 10**12]))
             try:
@@ -397,15 +424,16 @@
             value = re.sub(r"(^|[^%])%\(index\)", r"\1%%(index)", value)
             try: datetime.datetime.now().strftime(value)
             except Exception:
                 ConsolePrinter.error("Invalid rollover template option: %r. "
                                      "Value must contain valid strftime codes.", value)
                 ok = False
             else:
-                self._rollover_template = value
+                if ok:
+                    self._rollover_template = value
                 if ok and not self._rollover_limits:
                     ConsolePrinter.warn("Ignoring rollover template option: "
                                         "no rollover limits given.")
         return ok
 
 
     def ensure_rollover(self, topic, msg, stamp):
@@ -473,16 +501,16 @@
                     try: props["size"] = os.path.getsize(path)
                     except Exception as e:
                         ConsolePrinter.warn("Error getting size of %s: %s", path, e)
             sizesum = sum(x["size"] for x in self._rollover_files.values() if x["size"] is not None)
             result += self.FILE_META_TEMPLATE.format(
                 name=common.plural("file", self._rollover_files),
                 size=common.format_bytes(sizesum)
-            ) + ":"
-            for path, props in self._rollover_files.items():
+            ) + (":" if self.args.VERBOSE else ".")
+            for path, props in self._rollover_files.items() if self.args.VERBOSE else ():
                 sizestr = SIZE_ERROR if props["size"] is None else common.format_bytes(props["size"])
                 result += self.MULTI_META_TEMPLATE.format(name=path, size=sizestr,
                     mcount=common.plural("message", sum(props["counts"].values())),
                     tcount=common.plural("topic", props["counts"])
                 )
         return result
 
@@ -538,35 +566,33 @@
         @param   args.wrap_width            character width to wrap message YAML output at
         @param   args.match_wrapper         string to wrap around matched values,
                                             both sides if one value, start and end if more than one,
                                             or no wrapping if zero values
         @param   kwargs                     any and all arguments as keyword overrides, case-insensitive
         """
         args = common.ensure_namespace(args, ConsoleSink.DEFAULT_ARGS, **kwargs)
-        if args.WRAP_WIDTH is None:
-            args = common.structcopy(args)
-            args.WRAP_WIDTH = ConsolePrinter.WIDTH
-
         super(ConsoleSink, self).__init__(args)
         TextSinkMixin.__init__(self, args)
 
 
     def emit_meta(self):
         """Prints source metainfo like bag header, if not already printed."""
+        if not self.validate(): raise Exception("invalid")
         batch = self.args.META and self.source.get_batch()
         if self.args.META and batch not in self._batch_meta:
             meta = self._batch_meta[batch] = self.source.format_meta()
             kws = dict(self._styles, sep=self.SEP)
             meta = "\n".join(x and self.META_LINE_TEMPLATE.format(**dict(kws, line=x))
                              for x in meta.splitlines())
             meta and ConsolePrinter.print(meta)
 
 
     def emit(self, topic, msg, stamp=None, match=None, index=None):
         """Prints separator line and message text."""
+        if not self.validate(): raise Exception("invalid")
         self._prefix = ""
         stamp, index = self._ensure_stamp_index(topic, msg, stamp, index)
         if self.args.LINE_PREFIX and self.source.get_batch():
             sep = self.MATCH_PREFIX_SEP if match else self.CONTEXT_PREFIX_SEP
             kws = dict(self._styles, sep=sep, batch=self.source.get_batch())
             self._prefix = self.PREFIX_TEMPLATE.format(**kws)
         kws = dict(self._styles, sep=self.SEP)
@@ -583,14 +609,23 @@
 
 
     def is_highlighting(self):
         """Returns True if sink is configured to highlight matched values."""
         return bool(self.args.HIGHLIGHT)
 
 
+    def validate(self):
+        """Returns whether arguments environment set, populates options, emits error if not."""
+        if self.valid is not None: return self.valid
+        if self.args.WRAP_WIDTH is None:
+            self.args.WRAP_WIDTH = ConsolePrinter.WIDTH
+        self.valid = Sink.validate(self) and TextSinkMixin.validate(self)
+        return self.valid
+
+
 
 class BagSink(Sink, RolloverSinkMixin):
     """Writes messages to bagfile."""
 
     ## Constructor argument defaults
     DEFAULT_ARGS = dict(META=False, WRITE_OPTIONS={}, VERBOSE=False)
 
@@ -632,25 +667,27 @@
     def emit(self, topic, msg, stamp=None, match=None, index=None):
         """Writes message to output bagfile."""
         if not self.validate(): raise Exception("invalid")
         stamp, index = self._ensure_stamp_index(topic, msg, stamp, index)
         if self._is_pathed: RolloverSinkMixin.ensure_rollover(self, topic, msg, stamp)
         self._ensure_open()
         topickey = api.TypeMeta.make(msg, topic).topickey
-        if topickey not in self._counts and self.args.VERBOSE:
+        if self.args.VERBOSE and topickey not in self._counts:
             ConsolePrinter.debug("Adding topic %s in bag output.", topic)
 
         qoses = self.source.get_message_meta(topic, msg, stamp).get("qoses")
         self._bag.write(topic, msg, stamp, qoses=qoses)
         super(BagSink, self).emit(topic, msg, stamp, match, index)
 
     def validate(self):
         """Returns whether write options are valid and ROS environment set, emits error if not."""
         if self.valid is not None: return self.valid
-        result = RolloverSinkMixin.validate(self)
+        result = Sink.validate(self)
+        if not RolloverSinkMixin.validate(self):
+            result = False
         if self.args.WRITE_OPTIONS.get("overwrite") not in (None, True, False, "true", "false"):
             ConsolePrinter.error("Invalid overwrite option for bag: %r. "
                                  "Choose one of {true, false}.",
                                  self.args.WRITE_OPTIONS["overwrite"])
             result = False
         if not self._bag \
         and not common.verify_io(self.make_filename() if self._is_pathed else self.args.WRITE, "w"):
@@ -721,15 +758,15 @@
     @classmethod
     def autodetect(cls, target):
         """Returns true if target is recognizable as a ROS bag."""
         ext = os.path.splitext(target or "")[-1].lower()
         return ext in api.BAG_EXTENSIONS
 
 
-class TopicSink(Sink):
+class LiveSink(Sink):
     """Publishes messages to ROS topics."""
 
     ## Constructor argument defaults
     DEFAULT_ARGS = dict(LIVE=False, META=False, QUEUE_SIZE_OUT=10, PUBLISH_PREFIX="",
                         PUBLISH_SUFFIX="", PUBLISH_FIXNAME="", VERBOSE=False)
 
     def __init__(self, args=None, **kwargs):
@@ -741,16 +778,16 @@
         @param   args.publish_suffix    output topic suffix, appended to output topic
         @param   args.publish_fixname   single output topic name to publish to,
                                         overrides prefix and suffix if given
         @param   args.meta              whether to emit metainfo
         @param   args.verbose           whether to emit debug information
         @param   kwargs                 any and all arguments as keyword overrides, case-insensitive
         """
-        args = common.ensure_namespace(args, TopicSink.DEFAULT_ARGS, **kwargs)
-        super(TopicSink, self).__init__(args)
+        args = common.ensure_namespace(args, LiveSink.DEFAULT_ARGS, **kwargs)
+        super(LiveSink, self).__init__(args)
         self._pubs = {}  # {(intopic, typename, typehash): ROS publisher}
         self._close_printed = False
 
     def emit(self, topic, msg, stamp=None, match=None, index=None):
         """Publishes message to output topic."""
         if not self.validate(): raise Exception("invalid")
         api.init_node()
@@ -766,32 +803,34 @@
             if self.args.PUBLISH_FIXNAME:
                 pub = next((v for (_, c), v in self._pubs.items() if c == cls), None)
             pub = pub or api.create_publisher(topic2, cls, queue_size=self.args.QUEUE_SIZE_OUT)
             self._pubs[topickey] = pub
 
         self._pubs[topickey].publish(msg)
         self._close_printed = False
-        super(TopicSink, self).emit(topic, msg, stamp, match, index)
+        super(LiveSink, self).emit(topic, msg, stamp, match, index)
 
     def bind(self, source):
         """Attaches source to sink and blocks until connected to ROS."""
         if not self.validate(): raise Exception("invalid")
-        super(TopicSink, self).bind(source)
+        super(LiveSink, self).bind(source)
         api.init_node()
 
     def validate(self):
         """
         Returns whether ROS environment is set for publishing,
         and output topic configuration is valid, emits error if not.
         """
         if self.valid is not None: return self.valid
-        result = api.validate(live=True)
+        result = Sink.validate(self)
+        if not api.validate(live=True):
+            result = False
         config_ok = True
         if self.args.LIVE and not any((self.args.PUBLISH_PREFIX, self.args.PUBLISH_SUFFIX,
-                                        self.args.PUBLISH_FIXNAME)):
+                                       self.args.PUBLISH_FIXNAME)):
             ConsolePrinter.error("Need topic prefix or suffix or fixname "
                                  "when republishing messages from live ROS topics.")
             config_ok = False
         self.valid = result and config_ok
         return self.valid
 
     def close(self):
@@ -802,15 +841,15 @@
                                  common.plural("message", sum(self._counts.values())),
                                  common.plural("topic", self._pubs))
         for k in list(self._pubs):
             try: self._pubs.pop(k).unregister()
             except Exception as e:
                 if self.args.VERBOSE:
                     ConsolePrinter.warn("Error closing publisher on topic %r: %s", k[0], e)
-        super(TopicSink, self).close()
+        super(LiveSink, self).close()
 
 
 class AppSink(Sink):
     """Provides messages to callback function."""
 
     ## Constructor argument defaults
     DEFAULT_ARGS = dict(EMIT=None, METAEMIT=None, HIGHLIGHT=False)
@@ -826,36 +865,48 @@
         """
         if callable(args): args = common.ensure_namespace(None, emit=args)
         args = common.ensure_namespace(args, AppSink.DEFAULT_ARGS, **kwargs)
         super(AppSink, self).__init__(args)
 
     def emit_meta(self):
         """Invokes registered metaemit callback, if any, and not already invoked."""
+        if not self.validate(): raise Exception("invalid")
         if not self.source: return
         batch = self.source.get_batch() if self.args.METAEMIT else None
         if self.args.METAEMIT and batch not in self._batch_meta:
             meta = self._batch_meta[batch] = self.source.get_meta()
             self.args.METAEMIT(meta)
 
     def emit(self, topic, msg, stamp=None, match=None, index=None):
         """Registers message and invokes registered emit callback, if any."""
+        if not self.validate(): raise Exception("invalid")
         stamp, index = self._ensure_stamp_index(topic, msg, stamp, index)
         super(AppSink, self).emit(topic, msg, stamp, match, index)
         if self.args.EMIT: self.args.EMIT(topic, msg, stamp, match, index)
 
     def is_highlighting(self):
         """Returns whether emitted matches are highlighted."""
         return self.args.HIGHLIGHT
 
+    def validate(self):
+        """Returns whether callbacks are valid, emits error if not."""
+        if self.valid is not None: return self.valid
+        self.valid = True
+        for key in ("EMIT", "METAEMIT"):
+            if getattr(self.args, key) and not callable(getattr(self.args, key)):
+                ConsolePrinter.error("Invalid callback for %s: %r", key, getattr(self.args, key))
+                self.valid = False
+        return self.valid
+
 
 class MultiSink(Sink):
     """Combines any number of sinks."""
 
     ## Autobinding between argument flags and sink classes
-    FLAG_CLASSES = {"PUBLISH": TopicSink, "CONSOLE": ConsoleSink, "APP": AppSink}
+    FLAG_CLASSES = {"PUBLISH": LiveSink, "CONSOLE": ConsoleSink, "APP": AppSink}
 
     ## Autobinding between `--write TARGET format=FORMAT` and sink classes
     FORMAT_CLASSES = {"bag": BagSink}
 
     def __init__(self, args=None, sinks=(), **kwargs):
         """
         Accepts more arguments, given to the real sinks constructed.
@@ -891,33 +942,46 @@
                 continue  # for dumpopts
             clsargs = common.structcopy(args)
             clsargs.WRITE, clsargs.WRITE_OPTIONS = target, kwargs
             self.sinks += [cls(clsargs)]
 
     def emit_meta(self):
         """Outputs source metainfo in one sink, if not already emitted."""
+        if not self.validate(): raise Exception("invalid")
         sink = next((s for s in self.sinks if isinstance(s, ConsoleSink)), None)
         # Emit meta in one sink only, prefer console
         sink = sink or self.sinks[0] if self.sinks else None
         sink and sink.emit_meta()
 
     def emit(self, topic, msg, stamp=None, match=None, index=None):
         """Outputs ROS message to all sinks."""
+        if not self.validate(): raise Exception("invalid")
         stamp, index = self._ensure_stamp_index(topic, msg, stamp, index)
         for sink in self.sinks:
             sink.emit(topic, msg, stamp, match, index)
         super(MultiSink, self).emit(topic, msg, stamp, match, index)
 
     def bind(self, source):
         """Attaches source to all sinks, sets thread_excepthook on all sinks."""
         super(MultiSink, self).bind(source)
         for sink in self.sinks:
             sink.bind(source)
             sink.thread_excepthook = self.thread_excepthook
 
+    def configure(self, args=None, **kwargs):
+        """
+        Updates sinks configuration.
+
+        @param   args    arguments as namespace or dictionary, case-insensitive
+        @param   kwargs  any and all arguments as keyword overrides, case-insensitive
+        """
+        args = common.ensure_namespace(args, **kwargs)
+        hasattr(args, "WRITE") and delattr(args, "WRITE")  # Special arg for MultiSink
+        for sink in self.sinks: sink.configure(args, **kwargs)
+
     def validate(self):
         """Returns whether prerequisites are met for all sinks."""
         if not self.sinks:
             ConsolePrinter.error("No output configured.")
         return bool(self.sinks) and all([sink.validate() for sink in self.sinks]) and self.valid
 
     def close(self):
@@ -932,10 +996,10 @@
 
     def is_highlighting(self):
         """Returns whether any sink requires highlighted matches."""
         return any(s.is_highlighting() for s in self.sinks)
 
 
 __all__ = [
-    "AppSink", "BagSink", "ConsoleSink", "MultiSink", "RolloverSinkMixin", "Sink", "TextSinkMixin",
-    "TopicSink"
+    "AppSink", "BagSink", "ConsoleSink", "LiveSink", "MultiSink", "RolloverSinkMixin", "Sink",
+    "TextSinkMixin",
 ]
```

## grepros/ros1.py

```diff
@@ -4,15 +4,15 @@
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     01.11.2021
-@modified    27.12.2023
+@modified    19.04.2024
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.ros1
 import collections
 import datetime
 import decimal
 import inspect
@@ -546,15 +546,17 @@
         if msg._connection_header["type"] != typename:
             return
         typekey = (typename, msg._connection_header["md5sum"])
         if typekey not in TYPECLASSES:
             typedef = msg._connection_header["message_definition"]
             for name, cls in generate_message_classes(typename, typedef).items():
                 TYPECLASSES.setdefault((name, cls._md5sum), cls)
-        handler(TYPECLASSES[typekey]().deserialize(msg._buff))
+        if isinstance(msg, rospy.AnyMsg):  # /clock can yield already deserialized messages
+            msg = TYPECLASSES[typekey]().deserialize(msg._buff)
+        handler(msg)
 
     sub = rospy.Subscriber(topic, rospy.AnyMsg, myhandler, queue_size=queue_size)
     sub.get_message_class      = lambda: next(c for (n, h), c in TYPECLASSES.items()
                                               if n == typename)
     sub.get_message_definition = lambda: next(get_message_definition(c)
                                               for (n, h), c in TYPECLASSES.items() if n == typename)
     sub.get_message_type_hash  = lambda: next(h for n, h in TYPECLASSES if n == typename)
@@ -637,18 +639,25 @@
     """Returns ROS1 message type name, like "std_msgs/Header"."""
     if is_ros_time(msg_or_cls):
         cls = msg_or_cls if inspect.isclass(msg_or_cls) else type(msg_or_cls)
         return "duration" if "duration" in cls.__name__.lower() else "time"
     return msg_or_cls._type
 
 
-def get_message_value(msg, name, typename):
-    """Returns object attribute value, with numeric arrays converted to lists."""
+def get_message_value(msg, name, typename=None, default=Ellipsis):
+    """
+    Returns object attribute value, with numeric arrays converted to lists.
+
+    @param   name      message attribute name
+    @param   typename  value ROS type name, for identifying byte arrays
+    @param   default   value to return if attribute does not exist; raises exception otherwise
+    """
+    if default is not Ellipsis and not hasattr(msg, name): return default
     v = getattr(msg, name)
-    listifiable = typename.startswith(("uint8[", "char[")) and isinstance(v, bytes)
+    listifiable = typename and typename.startswith(("uint8[", "char[")) and isinstance(v, bytes)
     if six.PY2 and listifiable:  # Ignore already highlighted values from Scanner
         listifiable = v[:1] != "[" or v[-1:] != "]" or common.MatchMarkers.START not in v
         return list(bytearray(v)) if listifiable else v
     return list(v) if listifiable or isinstance(v, tuple) else v
 
 
 def get_rostime(fallback=False):
```

## grepros/ros2.py

```diff
@@ -4,15 +4,15 @@
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     02.11.2021
-@modified    27.12.2023
+@modified    21.04.2024
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.ros2
 import array
 import collections
 import datetime
 import decimal
@@ -113,14 +113,17 @@
 
 CREATE INDEX IF NOT EXISTS timestamp_idx ON messages (timestamp ASC);
 
 PRAGMA journal_mode=WAL;
 PRAGMA synchronous=NORMAL;
     """
 
+    ## SQLite file header magic start bytes
+    SQLITE_MAGIC = b"SQLite format 3\x00"
+
 
     def __init__(self, filename, mode="a", *_, **__):
         """
         @param   filename  bag file path to open
         @param   mode      file will be overwritten if "w"
         """
         if not isinstance(filename, PATH_TYPES):
@@ -509,14 +512,26 @@
                 self._counts[(topic, typename, typehash)] = count
 
 
     def _has_table(self, name):
         """Returns whether specified table exists in database."""
         sql = "SELECT 1 FROM sqlite_master WHERE type = ? AND name = ?"
         return bool(self._db.execute(sql, ("table", name)).fetchone())
+
+
+    @classmethod
+    def autodetect(cls, f):
+        """Returns whether file is recognizable as SQLite format."""
+        if os.path.isfile(f) and os.path.getsize(f):
+            with open(f, "rb") as f:
+                result = (f.read(len(cls.SQLITE_MAGIC)) == cls.SQLITE_MAGIC)
+        else:
+            ext = os.path.splitext(f or "")[-1].lower()
+            result = ext in BAG_EXTENSIONS
+        return result
 Bag = ROS2Bag
 
 
 
 def init_node(name):
     """Initializes a ROS2 node if not already initialized."""
     global node, context, executor
@@ -690,15 +705,15 @@
 @memoize
 def _get_message_definition(typename):
     """Returns ROS2 message type definition full text, or None on error (internal caching method)."""
     try:
         texts, pkg = collections.OrderedDict(), typename.rsplit("/", 1)[0]
         try:
             typepath = rosidl_runtime_py.get_interface_path(make_full_typename(typename) + ".msg")
-            with open(typepath) as f:
+            with open(typepath, encoding="utf-8") as f:
                 texts[typename] = f.read()
         except Exception:  # .msg file unavailable: parse IDL
             texts[typename] = get_message_definition_idl(typename)
         for line in texts[typename].splitlines():
             if not line or not line[0].isalpha():
                 continue  # for line
             linetype = scalar(canonical(re.sub(r"^([a-zA-Z][^\s]+)(.+)", r"\1", line)))
@@ -767,15 +782,15 @@
 
     def get_comments(obj):
         """Returns all comments for annotatable object, as [text, ]."""
         return [v.get("text", "") for v in obj.get_annotation_values("verbatim")
                 if "comment" == v.get("language")]
 
     typepath = rosidl_runtime_py.get_interface_path(make_full_typename(typename) + ".idl")
-    with open(typepath) as f:
+    with open(typepath, encoding="utf-8") as f:
         idlcontent = rosidl_parser.parser.parse_idl_string(f.read())
     msgidl = idlcontent.get_elements_of_type(rosidl_parser.definition.Message)[0]
     package = msgidl.structure.namespaced_type.namespaces[0]
     DUMMY = rosidl_parser.definition.EMPTY_STRUCTURE_REQUIRED_MEMBER_NAME
 
     lines = []
     # Add general comments
@@ -814,16 +829,23 @@
 
 def get_message_type(msg_or_cls):
     """Returns ROS2 message type name, like "std_msgs/Header"."""
     cls = msg_or_cls if inspect.isclass(msg_or_cls) else type(msg_or_cls)
     return canonical("%s/%s" % (cls.__module__.split(".")[0], cls.__name__))
 
 
-def get_message_value(msg, name, typename):
-    """Returns object attribute value, with numeric arrays converted to lists."""
+def get_message_value(msg, name, typename=None, default=Ellipsis):
+    """
+    Returns object attribute value, with numeric arrays converted to lists.
+
+    @param   name      message attribute name
+    @param   typename  value ROS type name, for identifying byte arrays
+    @param   default   value to return if attribute does not exist; raises exception otherwise
+    """
+    if default is not Ellipsis and not hasattr(msg, name): return default
     v, scalartype = getattr(msg, name), scalar(typename)
     if isinstance(v, (bytes, array.array)): v = list(v)
     elif numpy and isinstance(v, (numpy.generic, numpy.ndarray)):
         v = v.tolist()  # Returns value as Python type, either scalar or list
     if v and isinstance(v, (list, tuple)) and scalartype in ("byte", "uint8"):
         if isinstance(v[0], bytes):
             v = list(map(ord, v))  # In ROS2, a byte array like [0, 1] is [b"\0", b"\1"]
```

## grepros/search.py

```diff
@@ -4,20 +4,23 @@
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     28.09.2021
-@modified    23.12.2023
+@modified    24.03.2024
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.search
+from argparse import Namespace
 import copy
 import collections
+import functools
+import logging
 import re
 
 import six
 
 from . import api
 from . import common
 from . import inputs
@@ -35,36 +38,39 @@
     ## Namedtuple of (topic name, ROS message, ROS time object, message if matched, index in topic).
     GrepMessage = collections.namedtuple("BagMessage", "topic message timestamp match index")
 
     ## Match patterns for global any-match
     ANY_MATCHES = [((), re.compile("(.*)", re.DOTALL)), (), re.compile("(.?)", re.DOTALL)]
 
     ## Constructor argument defaults
-    DEFAULT_ARGS = dict(PATTERN=(), CASE=False, FIXED_STRING=False, INVERT=False, HIGHLIGHT=False,
-                        NTH_MATCH=1, BEFORE=0, AFTER=0, CONTEXT=0, MAX_COUNT=0,
+    DEFAULT_ARGS = dict(PATTERN=(), CASE=False, FIXED_STRING=False, INVERT=False, EXPRESSION=False,
+                        HIGHLIGHT=False, NTH_MATCH=1, BEFORE=0, AFTER=0, CONTEXT=0, MAX_COUNT=0,
                         MAX_PER_TOPIC=0, MAX_TOPICS=0, SELECT_FIELD=(), NOSELECT_FIELD=(),
                         MATCH_WRAPPER="**")
 
 
     def __init__(self, args=None, **kwargs):
         """
         @param   args                     arguments as namespace or dictionary, case-insensitive
         @param   args.pattern             pattern(s) to find in message field values
         @param   args.fixed_string        pattern contains ordinary strings, not regular expressions
         @param   args.case                use case-sensitive matching in pattern
         @param   args.invert              select messages not matching pattern
+        @param   args.expression          pattern(s) are a logical expression
+                                          like 'this AND (this2 OR NOT "skip this")',
+                                          with elements as patterns to find in message fields
         @param   args.highlight           highlight matched values
         @param   args.before              number of messages of leading context to emit before match
         @param   args.after               number of messages of trailing context to emit after match
         @param   args.context             number of messages of leading and trailing context to emit
                                           around match, overrides args.before and args.after
         @param   args.max_count           number of matched messages to emit (per file if bag input)
         @param   args.max_per_topic       number of matched messages to emit from each topic
         @param   args.max_topics          number of topics to emit matches from
-        @param   args.nth_match           emit every Nth match in topic
+        @param   args.nth_match           emit every Nth match in topic, starting from first
         @param   args.select_field        message fields to use in matching if not all
         @param   args.noselect_field      message fields to skip in matching
         @param   args.match_wrapper       string to wrap around matched values in find() and match(),
                                           both sides if one value, start and end if more than one,
                                           or no wrapping if zero values (default "**")
         @param   kwargs                   any and all arguments as keyword overrides, case-insensitive
         <!--sep-->
@@ -76,62 +82,73 @@
         @param   args.skip_topic          ROS topics to skip
         @param   args.skip_type           ROS message types to skip
         @param   args.start_time          earliest timestamp of messages to read
         @param   args.end_time            latest timestamp of messages to read
         @param   args.start_index         message index within topic to start from
         @param   args.end_index           message index within topic to stop at
         @param   args.unique              emit messages that are unique in topic
-        @param   args.nth_message         read every Nth message in topic
-        @param   args.nth_interval        minimum time interval between messages in topic
+        @param   args.nth_message         read every Nth message in topic, starting from first
+        @param   args.nth_interval        minimum time interval between messages in topic,
+                                          as seconds or ROS duration
         @param   args.condition           Python expressions that must evaluate as true
                                           for message to be processable, see ConditionMixin
         @param   args.progress            whether to print progress bar
         @param   args.stop_on_error       stop execution on any error like unknown message type
         """
         # {key: [(() if any field else ('nested', 'path') or re.Pattern, re.Pattern), ]}
         self._patterns = {}
+        self._expressor = ExpressionTree()
+        self._expression = None # Nested [op, val] like ["NOT", ["VAL", "skip this"]]
         # {(topic, typename, typehash): {message ID: message}}
         self._messages = collections.defaultdict(collections.OrderedDict)
         # {(topic, typename, typehash): {message ID: ROS time}}
         self._stamps   = collections.defaultdict(collections.OrderedDict)
         # {(topic, typename, typehash): {None: processed, True: matched, False: emitted as context}}
         self._counts   = collections.defaultdict(collections.Counter)
         # {(topic, typename, typehash): {message ID: True if matched else False if emitted else None}}
         self._statuses = collections.defaultdict(collections.OrderedDict)
         # Patterns to check in message plaintext and skip full matching if not found
-        self._brute_prechecks = []
+        self._brute_prechecks = []     # [re.Pattern to match against message fulltext for early skip]
         self._idcounter       = 0      # Counter for unique message IDs
-        self._highlight       = None   # Highlight matched values in message fields
-        self._passthrough     = False  # Emit messages without pattern-matching and highlighting
+        self._settings = {             # Various cached settings
+            "highlight":       None,   # Highlight matched values in message fields
+            "passthrough":     False,  # Emit messages without pattern-matching and highlighting
+            "pure_anymatch":   False,  # Simple match for any content
+            "wraps":           [],     # Match wrapper start-end strings
+        }
 
         ## Source instance
         self.source = None
         ## Sink instance
         self.sink   = None
+        ## Result of validate()
+        self.valid = None
 
-        self.args = common.ensure_namespace(args, Scanner.DEFAULT_ARGS, **kwargs)
+        self.args0 = common.ensure_namespace(args, **kwargs)
+        self.args = common.ArgumentUtil.validate(common.ensure_namespace(args, Scanner.DEFAULT_ARGS, **kwargs))
         if self.args.CONTEXT: self.args.BEFORE = self.args.AFTER = self.args.CONTEXT
-        self._parse_patterns()
 
 
     def find(self, source, highlight=None):
         """
         Yields matched and context messages from source.
 
         @param   source     inputs.Source or api.Bag instance
         @param   highlight  whether to highlight matched values in message fields,
                             defaults to flag from constructor
         @return             GrepMessage namedtuples of
                             (topic, message, timestamp, match, index in topic),
                             where match is matched optionally highlighted message
                             or `None` if yielding a context message
         """
+        if not self.validate(reset=True):
+            return
         if isinstance(source, api.Bag):
             source = inputs.BagSource(source, **vars(self.args))
-        self._prepare(source, highlight=highlight)
+        self._prepare(source, highlight=highlight, progress=True)
         for topic, msg, stamp, matched, index in self._generate():
             yield self.GrepMessage(topic, msg, stamp, matched, index)
 
 
     def match(self, topic, msg, stamp, highlight=None):
         """
         Returns matched message if message matches search filters.
@@ -140,17 +157,18 @@
         @param   msg        ROS message
         @param   stamp      message ROS timestamp
         @param   highlight  whether to highlight matched values in message fields,
                             defaults to flag from constructor
         @return             original or highlighted message on match else `None`
         """
         result = None
-        if not isinstance(self.source, inputs.AppSource):
-            self._prepare(inputs.AppSource(self.args), highlight=highlight)
-        if self._highlight != bool(highlight): self._configure_flags(highlight=highlight)
+        if not self.validate(reset=True):
+            return result
+        if isinstance(self.source, inputs.AppSource): self._configure_settings(highlight=highlight)
+        else: self._prepare(inputs.AppSource(self.args), highlight=highlight)
 
         self.source.push(topic, msg, stamp)
         item = self.source.read_queue()
         if item is not None:
             msgid = self._idcounter = self._idcounter + 1
             topickey = api.TypeMeta.make(msg, topic).topickey
             self._register_message(topickey, msgid, msg, stamp)
@@ -173,25 +191,53 @@
         """
         Greps messages yielded from source and emits matched content to sink.
 
         @param   source  inputs.Source or api.Bag instance
         @param   sink    outputs.Sink instance
         @return          count matched
         """
+        if not self.validate(reset=True):
+            return
         if isinstance(source, api.Bag):
             source = inputs.BagSource(source, **vars(self.args))
-        self._prepare(source, sink, highlight=self.args.HIGHLIGHT)
+        self._prepare(source, sink, highlight=self.args.HIGHLIGHT, progress=True)
         total_matched = 0
         for topic, msg, stamp, matched, index in self._generate():
             sink.emit_meta()
             sink.emit(topic, msg, stamp, matched, index)
             total_matched += bool(matched)
         return total_matched
 
 
+    def validate(self, reset=False):
+        """Returns whether conditions have valid syntax, prints errors."""
+        if self.valid is not None and not reset: return self.valid
+
+        errors = collections.defaultdict(list)  # {category: [error, ]}
+        if not self.args.FIXED_STRING and not self.args.EXPRESSION:
+            for v in self.args.PATTERN:  # Pre-check patterns before parsing for full error state
+                split = v.find("=", 1, -1)  # May be "PATTERN" or "attribute=PATTERN"
+                v = v[split + 1:] if split > 0 else v
+                try: re.compile(re.escape(v) if self.args.FIXED_STRING else v)
+                except Exception as e:
+                    errors["Invalid regular expression"].append("'%s': %s" % (v, e))
+        try: self._parse_patterns()
+        except Exception as e: errors[""].append(str(e))
+
+        for err in errors.get("", []):
+            common.ConsolePrinter.log(logging.ERROR, err)
+        for category in filter(bool, errors):
+            common.ConsolePrinter.log(logging.ERROR, category)
+            for err in errors[category]:
+                common.ConsolePrinter.log(logging.ERROR, "  %s" % err)
+
+        self.valid = not errors
+        return self.valid
+
+
     def __enter__(self):
         """Context manager entry, does nothing, returns self."""
         return self
 
 
     def __exit__(self, exc_type, exc_value, traceback):
         """Context manager exit, does nothing."""
@@ -274,75 +320,105 @@
     def _clear_data(self):
         """Clears local structures."""
         for d in (self._counts, self._messages, self._stamps, self._statuses):
             d.clear()
         api.TypeMeta.clear()
 
 
-    def _prepare(self, source, sink=None, highlight=None):
+    def _prepare(self, source, sink=None, highlight=None, progress=False):
         """Clears local structures, binds and registers source and sink, if any."""
         self._clear_data()
         self.source, self.sink = source, sink
         source.bind(sink), sink and sink.bind(source)
         source.preprocess = False
-        self._configure_flags(highlight=highlight)
+        self._configure_settings(highlight=highlight, progress=progress)
 
 
     def _prune_data(self, topickey):
         """Drops history older than context window."""
         WINDOW = max(self.args.BEFORE, self.args.AFTER) + 1
         for dct in (self._messages, self._stamps, self._statuses):
             while len(dct[topickey]) > WINDOW:
                 msgid = next(iter(dct[topickey]))
                 value = dct[topickey].pop(msgid)
                 dct is self._messages and api.TypeMeta.discard(value)
 
 
     def _parse_patterns(self):
-        """Parses pattern arguments into re.Patterns."""
+        """Parses pattern arguments into re.Patterns. Raises on invalid pattern."""
         NOBRUTE_SIGILS = r"\A", r"\Z", "?("  # Regex specials ruling out brute precheck
         BRUTE, FLAGS = not self.args.INVERT, re.DOTALL | (0 if self.args.CASE else re.I)
         self._patterns.clear()
+        self._expression = None
         del self._brute_prechecks[:]
         contents = []
-        for v in self.args.PATTERN:
+
+        def make_pattern(v):
+            """Returns (path Pattern or (), value Pattern)."""
             split = v.find("=", 1, -1)
             v, path = (v[split + 1:], v[:split]) if split > 0 else (v, ())
             # Special case if '' or "": add pattern for matching empty string
             v = "|^$" if v in ("''", '""') else (re.escape(v) if self.args.FIXED_STRING else v)
             path = re.compile(r"(^|\.)%s($|\.)" % ".*".join(map(re.escape, path.split("*")))) \
                    if path else ()
-            contents.append((path, re.compile("(%s)" % v, FLAGS)))
+            try: return (path, re.compile("(%s)" % v, FLAGS))
+            except Exception as e:
+                raise ValueError("Invalid regular expression\n  '%s': %s" % (v, e))
+
+        if self.args.EXPRESSION and self.args.PATTERN:
+            self._expression = self._expressor.parse(" ".join(self.args.PATTERN), make_pattern)
+        for v in self.args.PATTERN if not self._expression else ():
+            contents.append(make_pattern(v))
             if BRUTE and (self.args.FIXED_STRING or not any(x in v for x in NOBRUTE_SIGILS)):
+                if self.args.FIXED_STRING: v = re.escape(v)
                 self._brute_prechecks.append(re.compile(v, re.I | re.M))
         if not self.args.PATTERN:  # Add match-all pattern
             contents.append(self.ANY_MATCHES[0])
         self._patterns["content"] = contents
 
         selects, noselects = self.args.SELECT_FIELD, self.args.NOSELECT_FIELD
         for key, vals in [("select", selects), ("noselect", noselects)]:
-            self._patterns[key] = [(tuple(v.split(".")), common.wildcard_to_regex(v)) for v in vals]
+            self._patterns[key] = [(tuple(v.split(".")), common.path_to_regex(v)) for v in vals]
 
 
     def _register_message(self, topickey, msgid, msg, stamp):
         """Registers message with local structures."""
         self._counts[topickey][None] += 1
         self._messages[topickey][msgid] = msg
         self._stamps  [topickey][msgid] = stamp
         self._statuses[topickey][msgid] = None
 
 
-    def _configure_flags(self, highlight=None):
-        """Sets highlight and passthrough flags from current settings."""
-        self._highlight = bool(highlight if highlight is not None else
-                               False if self.sink and not self.sink.is_highlighting() else
-                               self.args.HIGHLIGHT)
-        self._passthrough = not self._highlight and not self._patterns["select"] \
-                            and not self._patterns["noselect"] and not self.args.INVERT \
-                            and set(self._patterns["content"]) <= set(self.ANY_MATCHES)
+    def _configure_settings(self, highlight=None, progress=False):
+        """Caches settings for message matching."""
+        highlight = bool(highlight if highlight is not None else self.args.HIGHLIGHT
+                         if not self.sink or self.sink.is_highlighting() else False)
+        pure_anymatch = not self.args.INVERT and not self._patterns["select"] \
+                        and set(self._patterns["content"]) <= set(self.ANY_MATCHES)
+        no_matching = pure_anymatch and not self._expression and not self._patterns["noselect"]
+        passthrough = no_matching and not highlight  # No message processing at all
+        wraps = [] if not highlight else self.args.MATCH_WRAPPER if not self.sink else \
+                (common.MatchMarkers.START, common.MatchMarkers.END)
+        wraps = wraps if isinstance(wraps, (list, tuple)) else [] if wraps is None else [wraps]
+        wraps = ((wraps or [""]) * 2)[:2]
+        if wraps:  # Track pattern contribution to wrapping
+            ops = {ExpressionTree.AND: BooleanResult.and_, ExpressionTree.NOT: BooleanResult.not_,
+                   ExpressionTree.OR: functools.partial(BooleanResult.or_, eager=True)}
+            self._expressor.configure(operators=ops, void=BooleanResult(None))
+        else: self._expressor.configure(operators=ExpressionTree.OPERATORS,
+                                        void=ExpressionTree.VOID)  # Ensure defaults
+        self._settings.update(highlight=highlight, passthrough=passthrough,
+                              pure_anymatch=pure_anymatch, wraps=wraps)
+        self.source.configure(self.args0)
+        self.sink and self.sink.configure(self.args0)
+        if progress and (not no_matching or self.args.MAX_COUNT):
+            bar_opts = dict()
+            if self.args.MAX_COUNT: bar_opts.update(match_max=self.args.MAX_COUNT)
+            if not no_matching: bar_opts.update(source_value=0)  # Count source and match separately
+            self.source.configure_progress(**bar_opts)
 
 
     def _is_max_done(self):
         """Returns whether max match count has been reached (and message after-context emitted)."""
         result, is_maxed = False, False
         if self.args.MAX_COUNT:
             is_maxed = sum(vv[True] for vv in self._counts.values()) >= self.args.MAX_COUNT
@@ -370,76 +446,361 @@
         """
         Returns transformed message if all patterns find a match in message, else None.
 
         Matching field values are converted to strings and surrounded by markers.
         Returns original message if any-match and sink does not require highlighting.
         """
 
-        def wrap_matches(v, top, is_collection=False):
-            """Returns string with matching parts wrapped in marker tags; updates `matched`."""
-            spans = []
-            # Omit collection brackets from match unless empty: allow matching "[]"
-            v1 = v2 = v[1:-1] if is_collection and v != "[]" else v
-            topstr = ".".join(top)
-            for i, (path, p) in enumerate(self._patterns["content"]):
+        def process_value(v, parent, top, patterns):
+            """
+            Populates `field_matches` and `pattern_spans` for patterns matching given string value.
+            Populates `field_values`. Returns set of pattern indexes that found a match.
+            """
+            indexes, spans, topstr = set(), [], ".".join(map(str, top))
+            v2 = str(list(v) if isinstance(v, LISTIFIABLES) else v)
+            if v and isinstance(v, (list, tuple)): v2 = v2[1:-1]  # Omit collection braces leave []
+            for i, (path, p) in enumerate(patterns):
                 if path and not path.search(topstr): continue  # for
-                matches = [next(p.finditer(v1), None)] if self.args.INVERT else list(p.finditer(v1))
+                matches = [next(p.finditer(v2), None)] if PLAIN_INVERT else list(p.finditer(v2))
                 # Join consecutive zero-length matches, extend remaining zero-lengths to end of value
                 matchspans = common.merge_spans([x.span() for x in matches if x], join_blanks=True)
-                matchspans = [(a, b if a != b else len(v1)) for a, b in matchspans]
+                matchspans = [(a, b if a != b else len(v2)) for a, b in matchspans]
                 if matchspans:
-                    matched[i] = True
-                    spans.extend(matchspans)
-            if any(WRAPS):
-                spans = common.merge_spans(spans) if not self.args.INVERT else \
-                        [] if spans else [(0, len(v1))] if v1 or not is_collection else []
-                for a, b in reversed(spans):  # Work from last to first, indices stay the same
-                    v2 = v2[:a] + WRAPS[0] + v2[a:b] + WRAPS[1] + v2[b:]
-            return "[%s]" % v2 if is_collection and v != "[]" else v2
-
-        def process_message(obj, top=()):
-            """Recursively converts field values to pattern-matched strings; updates `matched`."""
-            LISTIFIABLES = (bytes, tuple) if six.PY3 else (tuple, )
+                    indexes.add(i), spans.extend(matchspans)
+                    pattern_spans.setdefault(id(patterns[i]), {})[top] = matchspans
+            field_values.setdefault(top, (parent, v, v2))
+            if PLAIN_INVERT: spans = [(0, len(v2))] if v2 and not spans else []
+            if spans: field_matches.setdefault(top, []).extend(spans)
+            return indexes
+
+        def populate_matches(obj, patterns, top=(), parent=None):
+            """
+            Recursively populates `field_matches`  and `pattern_spans` for fields matching patterns.
+            Populates `field_values`. Returns set of pattern indexes that found a match.
+            """
+            indexes = set()
             selects, noselects = self._patterns["select"], self._patterns["noselect"]
-            fieldmap = fieldmap0 = api.get_message_fields(obj)  # Returns obj if not ROS message
+            fieldmap = api.get_message_fields(obj)  # Returns obj if not ROS message
             if fieldmap != obj:
                 fieldmap = api.filter_fields(fieldmap, top, include=selects, exclude=noselects)
             for k, t in fieldmap.items() if fieldmap != obj else ():
                 v, path = api.get_message_value(obj, k, t), top + (k, )
-                is_collection = isinstance(v, (list, tuple))
-                if api.is_ros_message(v):
-                    process_message(v, path)
-                elif v and is_collection and api.scalar(t) not in api.ROS_NUMERIC_TYPES:
-                    api.set_message_value(obj, k, [process_message(x, path) for x in v])
-                else:
-                    v1 = str(list(v) if isinstance(v, LISTIFIABLES) else v)
-                    v2 = wrap_matches(v1, path, is_collection)
-                    if len(v1) != len(v2):
-                        api.set_message_value(obj, k, v2)
+                if api.is_ros_message(v):  # Nested message
+                    indexes |= populate_matches(v, patterns, path, obj)
+                elif v and isinstance(v, (list, tuple)) \
+                and api.scalar(t) not in api.ROS_NUMERIC_TYPES:
+                    for i, x in enumerate(v):  # List of strings or nested messages
+                        indexes |= populate_matches(x, patterns, path + (i, ), v)
+                else:  # Scalar value, empty list, or list of numbers
+                    indexes |= process_value(v, obj, path, patterns)
             if not api.is_ros_message(obj):
-                v1 = str(list(obj) if isinstance(obj, LISTIFIABLES) else obj)
-                v2 = wrap_matches(v1, top)
-                obj = v2 if len(v1) != len(v2) else obj
-            if not top and not matched and not selects and not fieldmap0 and not self.args.INVERT \
-            and set(self._patterns["content"]) <= set(self.ANY_MATCHES):  # Ensure Empty any-match
-                matched.update({i: True for i, _ in enumerate(self._patterns["content"])})
-            return obj
+                indexes |= process_value(obj, parent, top, patterns)
+            return indexes
+
+        def wrap_matches(values, matches):
+            """Replaces result-message field values with matched parts wrapped in marker tags."""
+            for path, spans in matches.items() if any(WRAPS) else ():
+                parent, v1, v2 = values[path]
+                for a, b in reversed(common.merge_spans(spans)):  # Backwards for stable indexes
+                    v2 = v2[:a] + WRAPS[0] + v2[a:b] + WRAPS[1] + v2[b:]
+                if v1 and isinstance(v1, (list, tuple)): v2 = "[%s]" % v2  # Readd collection braces
+                if isinstance(parent, list) and isinstance(path[-1], int): parent[path[-1]] = v2
+                else: api.set_message_value(parent, path[-1], v2)
+
+        def process_message(obj, patterns):
+            """Returns whether message matches patterns, wraps matches in marker tags if so."""
+            indexes = populate_matches(obj, patterns)
+            is_match = not indexes if self.args.INVERT else len(indexes) == len(patterns)
+            if not indexes and self._settings["pure_anymatch"] and not api.get_message_fields(obj):
+                is_match = True  # Ensure any-match for messages with no fields
+            if is_match and WRAPS: wrap_matches(field_values, field_matches)
+            return is_match
 
-        if self._passthrough: return msg
+
+        if self._settings["passthrough"]: return msg
 
         if self._brute_prechecks:
             text  = "\n".join("%r" % (v, ) for _, v, _ in api.iter_message_fields(msg, flat=True))
             if not all(any(p.finditer(text)) for p in self._brute_prechecks):
                 return None  # Skip detailed matching if patterns not present at all
 
-        WRAPS = [] if not self._highlight else self.args.MATCH_WRAPPER if not self.sink else \
-                (common.MatchMarkers.START, common.MatchMarkers.END)
-        WRAPS = WRAPS if isinstance(WRAPS, (list, tuple)) else [] if WRAPS is None else [WRAPS]
-        WRAPS = ((WRAPS or [""]) * 2)[:2]
+        WRAPS         = self._settings["wraps"]
+        LISTIFIABLES  = (bytes, tuple) if six.PY3 else (tuple, )
+        PLAIN_INVERT  = self.args.INVERT and not self._expression
+        field_values  = {}  # {field path: (parent, original value, stringified value)}
+        field_matches = {}  # {field path: [(span), ]}
+        pattern_spans = {}  # {id(pattern tuple): {field path: (span)}}
+
+        result, is_match = copy.deepcopy(msg) if WRAPS else msg, False
+        if self._expression:
+            evaler = lambda x: bool(populate_matches(result, [x]))
+            terminal = evaler if not WRAPS else lambda x: BooleanResult(x, evaler)
+            eager = [ExpressionTree.OR] if WRAPS else ()
+            evalresult = self._expressor.evaluate(self._expression, terminal, eager)
+            is_match = not evalresult if self.args.INVERT else evalresult
+            if is_match and WRAPS:
+                actives = [pattern_spans[id(v)] for v in evalresult]
+                matches = {k: sum((v.get(k, []) for v in actives), [])
+                           for k in set(sum((list(v) for v in actives), []))} or \
+                          {k: [(0, len(v))] for k, (_, _, v) in field_values.items()}  # Wrap all
+                wrap_matches(field_values, matches)
+        else:
+            is_match = process_message(result, self._patterns["content"])
+        return result if is_match else None
+
+
+
+class ExpressionTree(object):
+    """
+    Parses and evaluates operator expressions like "a AND (b OR NOT c)".
+
+    Operands can be quoted strings, `\` can be used to escape quotes within the string.
+    Operators are case-insensitive.
+    """
 
-        result, matched = copy.deepcopy(msg), {}  # {pattern index: True}
-        process_message(result)
-        yes = not matched if self.args.INVERT else len(matched) == len(self._patterns["content"])
-        return (result if self._highlight else msg) if yes else None
+    QUOTES, ESCAPE, LBRACE, RBRACE, WHITESPACE = "'\"", "\\", "(", ")", " \n\r\t"
+    SEPARATORS = WHITESPACE + LBRACE + RBRACE
+    AND, OR, NOT, VAL = "AND", "OR", "NOT", "VAL"
+
+    CASED     = False      # Whether operators are case-sensitive
+    IMPLICIT  = AND        # Implicit operator inserted between operands lacking one
+    UNARIES   = (NOT, )    # Unary operators, expecting operand after operator
+    BINARIES  = (AND, OR)  # Binary operators, expecting operands before and after operator
+    OPERATORS = {AND: (lambda a, b: a and b), OR: (lambda a, b: a or b), NOT: lambda a: not a}
+    RANKS     = {VAL: 1, NOT: 2, AND: 3, OR: 4}
+
+    SHORTCIRCUITS    = {AND: False, OR: True}  # Values for binary operators to short-circuit on
+    FORMAT_TEMPLATES = {AND: "%s and %s", OR: "%s or %s", NOT: "not %s"}
+    VOID = None  # Placeholder for operands skipped as short-circuited
+
+
+    def __init__(self, **props):
+        """
+        @param   props  class property overrides, case-insensitive, e.g. `cased=False`
+        """
+        self._state = None  # Temporary state namespace dictionary during parse
+        self.configure(**props)
+
+
+    def configure(self, **props):
+        """
+        Overrides instance configuration.
+
+        @param   props  class property overrides, case-insensitive, e.g. `cased=False`
+        """
+        for k, v in props.items():
+            K, V = k.upper(), getattr(self, k.upper())
+            accept = (type(V), type(None)) if "IMPLICIT" == K else () if "VOID" == K else type(V)
+            if accept and not isinstance(v, accept) and set(map(type, (v, V))) - set([list, tuple]):
+                raise ValueError("Invalid value for %s=%s: expected %s" % (k, v, type(V).__name__))
+            setattr(self, K, v)
+
+
+    def evaluate(self, tree, terminal=None, eager=()):
+        """
+        Returns result of evaluating expression tree.
+
+        @param   tree      expression tree structure as given by parse()
+        @param   terminal  callback(value) to evaluate value nodes with, if not using value directly
+        @param   eager     operators where to evaluate both operands in full, despite short-circuit
+        """
+        stack = [(tree, [], [], None)] if tree else []
+        while stack:  # [(node, evaled vals, parent evaled vals, parent op)]
+            ((op, val), nvals, pvals, parentop), done = stack.pop(), set()
+            if nvals: done.add(pvals.append(self.OPERATORS[op](*nvals)))  # Backtracking: fill parent
+            elif pvals and parentop in self.SHORTCIRCUITS and not (eager and parentop in eager):
+                ctor = type(self.SHORTCIRCUITS[parentop])  # Skip if first sibling short-circuits op
+                if ctor(pvals[0]) == self.SHORTCIRCUITS[parentop]: done.add(pvals.append(self.VOID))
+            if done: continue  # while
+            if op not in self.OPERATORS: pvals.append(val if terminal is None else terminal(val))
+            else: stack.extend([((op, val), nvals, pvals, parentop)] +
+                               [(v, [], nvals, op) for v in val[::-1]])  # Queue in processing order
+        return pvals.pop() if tree else None
+
+
+    def format(self, tree, terminal=None):
+        """
+        Returns expression tree formatted as string.
+
+        @param   tree      expression tree structure as given by parse()
+        @param   terminal  callback(value) to format value nodes with, if not using value directly
+        """
+        BRACED = "%s".join(self.FORMAT_TEMPLATES.get(x, x) for x in [self.LBRACE, self.RBRACE])
+        TPL = lambda op: ("%s {0} %s" if op in self.BINARIES else "{0} %s").format(op)
+        WRP = lambda op, parentop: BRACED if self.RANKS[op] > self.RANKS[parentop] else "%s"
+        FMT = lambda vv, op, nodes: tuple(WRP(nop, op) % v for (nop, _), v in zip(nodes, vv))
+        stack = [(tree, [], [])] if tree else [] # [(node, formatted vals, parent formatted vals)]
+        while stack:  # Add to parent if all processed or terminal node, else queue for processing
+            (op, val), nvals, pvals = stack.pop()
+            if nvals: pvals.append((self.FORMAT_TEMPLATES.get(op) or TPL(op)) % FMT(nvals, op, val))
+            elif op not in self.OPERATORS: pvals.append(val if terminal is None else terminal(val))
+            else: stack.extend([((op, val), nvals, pvals)] + [(v, [], nvals) for v in val[::-1]])
+        return pvals.pop() if tree else ""
+
+
+    def parse(self, text, terminal=None):
+        """
+        Returns an operator expression like "a AND (b OR NOT c)" parsed into a binary tree.
+
+        Binary tree like ["AND", [["VAL", "a"], ["OR", [["VAL", "b"], ["NOT", [["VAL", "c"]]]]]]].
+        Raises on invalid expression.
+
+        @param   terminal  callback(text) returning node value for operands, if not using plain text
+        """
+        root, node, buf, quote, escape, i = [], [], "", "", "", -1
+        self._state = locals()
+        h = self._make_helpers(self._state, text, terminal)
+
+        for i, char in enumerate(text + " "):  # Append space to simplify termination
+            # First pass: handle quotes, or explicit/implicit word ends and operators
+            if quote:
+                if escape:
+                    if   char == self.ESCAPE: char = ""  # Double escape: retain single
+                    elif char in self.QUOTES: buf = buf[:-1]  # Drop escape char from before quote
+                elif char == quote:  # End quote
+                    (node, root), buf, quote, char = h.add_node(self.VAL, buf, i), "", "", ""
+                escape = char if self.ESCAPE == char else ""
+            elif char in self.QUOTES:
+                h.validate(i, "quotes", buf=buf)
+                if node and h.finished(node): node, root = h.add_implicit(node, i)
+                quote, char = char, ""  # Start quoted string, consume quotemark
+            elif char in self.SEPARATORS:
+                op = h.parse_op(buf)
+                if op:  # Explicit operator
+                    h.validate(i, "op", op=op)
+                    if op in self.UNARIES and node and h.finished(node):
+                        node, root = h.add_implicit(node, i)
+                    val = h.make_val(op, None if op in self.UNARIES else node)
+                    node, root = h.add_node(op, val, i)
+                else:  # Consume accumulated buffer if any, handle implicit operators
+                    if (buf or char == self.LBRACE) and node and h.finished(node):
+                        node, root = h.add_implicit(node, i)
+                    if buf: node, root = h.add_node(self.VAL, buf, i)  # Completed operand
+                buf = ""
+            # Second pass: accumulate text buffer, or enter/exit bracket groups
+            if quote or char not in self.SEPARATORS: buf += char
+            elif char == self.LBRACE: _, (node, root) = h.stack_push((node, root, i)), ([], [])
+            elif char == self.RBRACE: _, (node, root) = h.validate(i, "rbrace"), h.stack_pop()
+            self._state.update(locals())
+        h.validate(i)
+        return root
+
+
+    def _make_helpers(self, state, text, terminal=None):
+        """Returns namespace object with parsing helper functions."""
+        ERRLABEL, OP_MAXLEN = "Invalid expression: ", max(map(len, self.OPERATORS))
+        OPERATORS = {x if self.CASED else x.upper(): x for x in self.OPERATORS}
+        stack, parents = [], {}
+
+        finished  = lambda n:     not (isinstance(n[1], list) and n[1] and n[1][-1] is None)
+        outranks  = lambda a, b:  self.RANKS[a] > self.RANKS[b]    # whether operator a ranks over b
+        postbrace = lambda:       state.get("stacki") is not None  # whether brackets just ended
+        mark      = lambda i:     "\n%s\n%s^" % (text, " " * i)    # expression text marked at pos
+        oper      = lambda n:     n[0]                             # node type
+        parse_op  = lambda b:     OPERATORS.get(b if self.CASED else b.upper()) \
+                                  if len(b) <= OP_MAXLEN else None
+        make_node = lambda o, v:  [o, terminal(v) if terminal and self.VAL == o else v]
+        make_val  = lambda o, *a: list(a) + [None] * (1 + (o in self.BINARIES) - len(a))
+        add_child = lambda a, b:  (a[1].__setitem__(-1, b), parents.update({id(b): a}))
+        get_child = lambda n, i:  n[1][i] if n[0] in self.OPERATORS else None
+
+        def missing(op, first=False):  # Return error text for missing operand in operator
+            label = ("1st " if first else "2nd ") if op in self.BINARIES else ""
+            return ERRLABEL + "missing %selement for %s-operator" % (label, op)
+
+        def add_node(op, val, i):  # Add new node to tree, return (node to use as last, root)
+            node0, root0 = _, newroot = state["node"], state["root"]
+            if op in self.BINARIES:  # Attach last child or root to new if needed
+                if not postbrace() and finished(node0) and not outranks(op, oper(node0)):
+                    val = make_val(op, get_child(node0, -1), None)  # Last child into new
+                elif not outranks(oper(root0), op):
+                    val = make_val(op, root0, None)  # Root into new
+            newnode = make_node(op, val)
+
+            if node0 and not postbrace() and (not finished(node0)  # Last is unfinished
+            or oper(node0) in self.BINARIES and not outranks(op, oper(node0))):  # op <= last binop
+                add_child(node0, newnode)  # Attach new node to last
+            elif not root0 or (root0 is node0 if postbrace() else not outranks(oper(root0), op)):
+                newroot = newnode  # Replace root if new outranks, or expression so far was braced
+            latest = node0 if node0 and op == self.VAL else newnode
+            while oper(latest) in self.UNARIES and finished(latest) and id(latest) in parents:
+                latest = parents[id(latest)]  # Walk up filled unary nodes until binop/root
+            state.update(node=latest, root=newroot, nodei=i, stacki=None)
+            return latest, newroot
+
+        def add_implicit(node, i):  # Add implicit operator, return (node to use as last, root)
+            if not self.IMPLICIT: raise ValueError(ERRLABEL + "missing operator" + mark(i))
+            return add_node(self.IMPLICIT, make_val(self.IMPLICIT, node), i)
+
+        def stack_pop():  # Unstack previous and add current, return (node to use as last, root)
+            (node, root, stacki), nodex, rootx = stack.pop(), state["node"], state["root"]
+            if node: node, stacki, _ = root, None, add_child(node, rootx)  # Nest into last
+            elif not root: node, root = nodex, rootx  # Replace empty root with nested
+            state.update(node=node, root=root, stacki=stacki)
+            return node, root
+
+        def validate(i, ctx=None, **kws):  # Raise ValueError if parse state invalid
+            if "quotes" == ctx:
+                if kws["buf"]: raise ValueError(ERRLABEL + "invalid syntax" + mark(i))
+            elif "op" == ctx:
+                op, node = kws["op"], state["node"]
+                if op in self.BINARIES and (not node or not finished(node)):
+                    raise ValueError(missing(oper(node) if node else op, first=not node) + mark(i))
+            elif "rbrace" == ctx:
+                node, nodei = (state.get(k) for k in ("node", "nodei"))
+                if not stack: raise ValueError(ERRLABEL + "bracket end has no start" + mark(i))
+                if not node: raise ValueError(ERRLABEL + "empty bracket" + mark(i))
+                if not finished(node): raise ValueError(missing(oper(node)) + mark(nodei))
+            else:  # All parsing done, tree in final state
+                quote, node, nodei = (state.get(k) for k in ("quote", "node", "nodei"))
+                if quote: raise ValueError(ERRLABEL + "unfinished quote" + mark(i - 1))
+                if stack: raise ValueError(ERRLABEL + "unterminated bracket" + mark(stack[-1][-1]))
+                if node and not finished(node): raise ValueError(missing(oper(node)) + mark(nodei))
+
+        return Namespace(add_child=add_child, add_implicit=add_implicit, add_node=add_node,
+                         make_val=make_val, finished=finished, parse_op=parse_op,
+                         stack_push=stack.append, stack_pop=stack_pop, validate=validate)
+
+
+class BooleanResult(object):
+    """Accumulative result of boolean expression evaluation, tracking value contribution."""
+
+    def __init__(self, value=Ellipsis, terminal=None, **__props):
+        self._result  = Ellipsis  # Final accumulated result of expression
+        self._values  = []  # All accumulated operands
+        self._actives = []  # For each operand, True/False/None: active/invertedly active/inactive
+        for k, v in __props.items(): setattr(self, "_" + k, v)
+        if value is not Ellipsis: self.set(value, terminal)
+
+    def set(self, value, terminal=None):
+        """Sets value to instance, using terminal callback for evaluation if given."""
+        self._result = bool(terminal(value) if terminal else value)
+        self._values, self._actives = [value], [True if self._result else None]
+
+    def __iter__(self):
+        """Yields active values: contributing to true result positively."""
+        for v in (v for v, a in zip(self._values, self._actives) if a): yield v
+
+    def __bool__(self):      return self._result  # Py3
+    def __nonzero__(self):   return self._result  # Py2´
+    def __eq__(self, other): return (bool(self) if isinstance(other, bool) else self) is other
+
+    @classmethod
+    def and_(cls, a, b):
+        """Returns new BooleanResult as a and b."""
+        actives = [on if y else None for x, y in ((a, b), (b, a)) for on in x._actives]
+        return cls(result=bool(a and b), values=a._values + b._values, actives=actives)
+
+    @classmethod
+    def or_(cls, a, b, eager=False):
+        """Returns new BooleanResult as a or b."""
+        actives = a._actives + [x if eager or not a else None for x in b._actives]
+        return cls(result=bool(a or b), values=a._values + b._values, actives=actives)
+
+    @classmethod
+    def not_(cls, a):
+        """Returns new BooleanResult as not a."""
+        actives = [None if x is None else not x for x in a._actives]
+        return cls(result=not a, values=a._values, actives=actives)
 
 
-__all__ = ["Scanner"]
+__all__ = ["BooleanResult", "ExpressionTree", "Scanner"]
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## grepros/plugins/mcap.py

```diff
@@ -4,15 +4,15 @@
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     14.10.2022
-@modified    28.12.2023
+@modified    21.04.2024
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.plugins.mcap
 from __future__ import absolute_import
 import atexit
 import copy
 import io
@@ -294,15 +294,17 @@
         if self._opened and "w" == self._mode:
             raise io.UnsupportedOperation("Cannot reopen bag for writing.")
 
         if self._file is None: self._file = open(self._filename, "%sb" % self._mode)
         self._reader  = mcap.reader.make_reader(self._file) if "r" == self._mode else None
         self._decoder = mcap_ros.decoder.Decoder()          if "r" == self._mode else None
         self._writer  = mcap_ros.writer.Writer(self._file)  if "w" == self._mode else None
-        if "r" == self._mode: self._populate_meta()
+        try: "r" == self._mode and self._populate_meta()
+        except Exception as e:
+            raise Exception("Error reading MCAP metadata: %r" % e)
         self._opened = True
 
 
     def close(self):
         """Closes the bag file."""
         if self._file is not None:
             if self._writer: self._writer.finish()
@@ -471,40 +473,45 @@
 
     def _populate_meta(self):
         """Populates bag metainfo."""
         summary = self._reader.get_summary()
         self._start_time = summary.statistics.message_start_time / 1E9
         self._end_time   = summary.statistics.message_end_time   / 1E9
 
+        def make_hash(typename, msgdef, extradefs):
+            """Returns MD5 hash calculated for type definition, or None on error."""
+            try: return api.calculate_definition_hash(typename, msgdef, extradefs)
+            except Exception as e:
+                ConsolePrinter.warn("Error calculating message type hash for %r: %r", typename, e)
+                return None
+
         defhashes = {}  # Cached {type definition full text: type hash}
         for cid, channel in summary.channels.items():
             schema = summary.schemas[channel.schema_id]
             topic, typename = channel.topic, api.canonical(schema.name)
 
             typedef = schema.data.decode("utf-8")  # Full definition including subtype definitions
             subtypedefs, nesting = api.parse_definition_subtypes(typedef, nesting=True)
             typehash = channel.metadata.get("md5sum") or \
-                       api.calculate_definition_hash(typename, typedef,
-                                                        tuple(subtypedefs.items()))
+                       make_hash(typename, typedef, tuple(subtypedefs.items()))
             topickey, typekey = (topic, typename, typehash), (typename, typehash)
 
             qoses = None
             if channel.metadata.get("offered_qos_profiles"):
                 try: qoses = yaml.safe_load(channel.metadata["offered_qos_profiles"])
                 except Exception as e:
                     ConsolePrinter.warn("Error parsing topic QoS profiles from %r: %s.",
                                         channel.metadata["offered_qos_profiles"], e)
 
             self._topics.setdefault(topickey, 0)
             self._topics[topickey] += summary.statistics.channel_message_counts[cid]
             self._typedefs[typekey] = typedef
             defhashes[typedef] = typehash
             for t, d in subtypedefs.items():  # Populate subtype definitions and hashes
-                if d in defhashes: h = defhashes[d]
-                else: h = api.calculate_definition_hash(t, d, tuple(subtypedefs.items()))
+                h = defhashes[d] if d in defhashes else make_hash(t, d, tuple(subtypedefs.items()))
                 self._typedefs.setdefault((t, h), d)
                 self._type_subtypes.setdefault(typekey, {})[t] = h
                 defhashes[d] = h
             for t, subtypes in nesting.items():  # Populate all nested type references
                 h = self._type_subtypes[typekey][t]
                 for t2 in subtypes:
                     h2 = self._type_subtypes[typekey][t2]
@@ -513,15 +520,15 @@
             if qoses: self._qoses[topickey] = qoses
             self._schemas[typekey] = schema
             self._schematypes[schema.id] = typekey
 
 
     @classmethod
     def autodetect(cls, f):
-        """Returns whether file is readable as MCAP format."""
+        """Returns whether file is recognizable as MCAP format."""
         if common.is_stream(f):
             pos, _ = f.tell(), f.seek(0)
             result, _ = (f.read(len(cls.MCAP_MAGIC)) == cls.MCAP_MAGIC), f.seek(pos)
         elif os.path.isfile(f) and os.path.getsize(f):
             with open(f, "rb") as f:
                 result = (f.read(len(cls.MCAP_MAGIC)) == cls.MCAP_MAGIC)
         else:
@@ -591,15 +598,15 @@
         args = common.ensure_namespace(args, McapSink.DEFAULT_ARGS, **kwargs)
         super(McapSink, self).__init__(args)
         RolloverSinkMixin.__init__(self, args)
 
         self._file          = None  # Open file() object
         self._writer        = None  # mcap_ros.writer.Writer object
         self._schemas       = {}    # {(typename, typehash): mcap.records.Schema}
-        self._overwrite     = (args.WRITE_OPTIONS.get("overwrite") in (True, "true"))
+        self._overwrite     = None
         self._close_printed = False
 
         atexit.register(self.close)
 
 
     def validate(self):
         """
@@ -617,14 +624,16 @@
             ConsolePrinter.error("mcap not available: cannot work with MCAP files.")
         if not mcap_ros_ok:
             ConsolePrinter.error("mcap_ros%s not available: cannot work with MCAP files.",
                                  api.ROS_VERSION or "")
         if not common.verify_io(self.args.WRITE, "w"):
             ok = False
         self.valid = ok and mcap_ok and mcap_ros_ok
+        if self.valid:
+            self._overwrite = (self.args.WRITE_OPTIONS.get("overwrite") in (True, "true"))
         return self.valid
 
 
     def emit(self, topic, msg, stamp=None, match=None, index=None):
         """Writes out message to MCAP file."""
         if not self.validate(): raise Exception("invalid")
         stamp, index = self._ensure_stamp_index(topic, msg, stamp, index)
```

## grepros/plugins/parquet.py

```diff
@@ -4,23 +4,24 @@
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     14.12.2021
-@modified    28.12.2023
+@modified    21.04.2024
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.plugins.parquet
 import itertools
 import json
 import os
 import re
 import uuid
+import warnings
 
 try: import pandas
 except Exception: pandas = None
 try: import pyarrow
 except Exception: pyarrow = None
 try: import pyarrow.parquet
 except Exception: pass
@@ -83,16 +84,15 @@
                               ("_parent_type", "string"),
                               ("_parent_id",   "string"), ]
 
     ## Custom arguments for pyarrow.parquet.ParquetWriter
     WRITER_ARGS = {"version": "2.6"}
 
     ## Constructor argument defaults
-    DEFAULT_ARGS = dict(EMIT_FIELD=(), META=False, NOEMIT_FIELD=(), WRITE_OPTIONS={},
-                        VERBOSE=False)
+    DEFAULT_ARGS = dict(EMIT_FIELD=(), META=False, NOEMIT_FIELD=(), WRITE_OPTIONS={}, VERBOSE=False)
 
 
     def __init__(self, args=None, **kwargs):
         """
         @param   args                 arguments as namespace or dictionary, case-insensitive;
                                       or a single path as the base name of Parquet files to write
         @param   args.emit_field      message fields to emit in output if not all
@@ -119,15 +119,15 @@
         @param   kwargs               any and all arguments as keyword overrides, case-insensitive
         """
         args = {"WRITE": str(args)} if isinstance(args, common.PATH_TYPES) else args
         args = common.ensure_namespace(args, ParquetSink.DEFAULT_ARGS, **kwargs)
         super(ParquetSink, self).__init__(args)
 
         self._filebase       = args.WRITE
-        self._overwrite      = (args.WRITE_OPTIONS.get("overwrite") in (True, "true"))
+        self._overwrite      = None
         self._filenames      = {}  # {(typename, typehash): Parquet file path}
         self._caches         = {}  # {(typename, typehash): [{data}, ]}
         self._schemas        = {}  # {(typename, typehash): pyarrow.Schema}
         self._writers        = {}  # {(typename, typehash): pyarrow.parquet.ParquetWriter}
         self._extra_basecols = []  # [(name, rostype)]
         self._extra_basevals = []  # [(name, value)]
         self._patterns       = {}  # {key: [(() if any field else ('path', ), re.Pattern), ]}
@@ -139,15 +139,16 @@
 
     def validate(self):
         """
         Returns whether required libraries are available (pandas and pyarrow) and overwrite is valid
         and file base is writable.
         """
         if self.valid is not None: return self.valid
-        ok, pandas_ok, pyarrow_ok = self._configure(), bool(pandas), bool(pyarrow)
+        ok = all([Sink.validate(self), self._configure()])
+        pandas_ok, pyarrow_ok = bool(pandas), bool(pyarrow)
         if self.args.WRITE_OPTIONS.get("overwrite") not in (None, True, False, "true", "false"):
             ConsolePrinter.error("Invalid overwrite option for Parquet: %r. "
                                  "Choose one of {true, false}.",
                                  self.args.WRITE_OPTIONS["overwrite"])
             ok = False
         if self.args.WRITE_OPTIONS.get("nesting") not in (None, "", "array", "all"):
             ConsolePrinter.error("Invalid nesting option for Parquet: %r. "
@@ -157,14 +158,16 @@
         if not pandas_ok:
             ConsolePrinter.error("pandas not available: cannot write Parquet files.")
         if not pyarrow_ok:
             ConsolePrinter.error("PyArrow not available: cannot write Parquet files.")
         if not common.verify_io(self.args.WRITE, "w"):
             ok = False
         self.valid = ok and pandas_ok and pyarrow_ok
+        if self.valid:
+            self._overwrite = (self.args.WRITE_OPTIONS.get("overwrite") in (True, "true"))
         return self.valid
 
 
     def emit(self, topic, msg, stamp=None, match=None, index=None):
         """Writes message to a Parquet file."""
         if not self.validate(): raise Exception("invalid")
         stamp, index = self._ensure_stamp_index(topic, msg, stamp, index)
@@ -183,20 +186,21 @@
         finally:
             if not self._close_printed and self._counts:
                 self._close_printed = True
                 sizes = {n: None for n in self._filenames.values()}
                 for n in self._filenames.values():
                     try: sizes[n] = os.path.getsize(n)
                     except Exception as e: ConsolePrinter.warn("Error getting size of %s: %s", n, e)
-                ConsolePrinter.debug("Wrote %s in %s to %s (%s):",
+                ConsolePrinter.debug("Wrote %s in %s to %s (%s)%s",
                                      common.plural("message", sum(self._counts.values())),
                                      common.plural("topic", self._counts),
                                      common.plural("Parquet file", sizes),
-                                     common.format_bytes(sum(filter(bool, sizes.values()))))
-                for (t, h), name in self._filenames.items():
+                                     common.format_bytes(sum(filter(bool, sizes.values()))),
+                                     ":" if self.args.VERBOSE else ".")
+                for (t, h), name in self._filenames.items() if self.args.VERBOSE else ():
                     count = sum(c for (_, t_, h_), c in self._counts.items() if (t, h) == (t_, h_))
                     ConsolePrinter.debug("- %s (%s, %s)", name,
                                          "error getting size" if sizes[name] is None else
                                          common.format_bytes(sizes[name]),
                                          common.plural("message", count))
             self._caches.clear()
             self._schemas.clear()
@@ -204,15 +208,15 @@
 
 
     def _process_type(self, topic, msg, rootmsg=None):
         """Prepares Parquet schema and writer if not existing."""
         rootmsg = rootmsg or msg
         with api.TypeMeta.make(msg, root=rootmsg) as m:
             typename, typehash, typekey = (m.typename, m.typehash, m.typekey)
-        if topic and (topic, typename, typehash) not in self._counts and self.args.VERBOSE:
+        if self.args.VERBOSE and topic and (topic, typename, typehash) not in self._counts:
             ConsolePrinter.debug("Adding topic %s in Parquet output.", topic)
         if typekey in self._writers: return
 
         basedir, basename = os.path.split(self._filebase)
         pathname = os.path.join(basedir, re.sub(r"\W", "__", "%s__%s" % (typename, typehash)))
         filename = os.path.join(pathname, basename)
         if not self._overwrite:
@@ -345,20 +349,27 @@
 
 
     def _write_table(self, typekey):
         """Writes out cached messages for type."""
         dicts = self._caches[typekey][:]
         del self._caches[typekey][:]
         mapping = {k: [d[k] for d in dicts] for k in dicts[0]}
-        table = pyarrow.Table.from_pydict(mapping, self._schemas[typekey])
+        with warnings.catch_warnings():  # PyArrow can raise UserWarning about pandas version
+            warnings.simplefilter("ignore", UserWarning)
+            table = pyarrow.Table.from_pydict(mapping, self._schemas[typekey])
         self._writers[typekey].write_table(table)
 
 
     def _configure(self):
         """Parses args.WRITE_OPTIONS, returns success."""
+        self.COMMON_TYPES = type(self).COMMON_TYPES.copy()
+        self.WRITER_ARGS = type(self).WRITER_ARGS.copy()
+        del self._extra_basecols[:]
+        del self._extra_basevals[:]
+        self._patterns.clear()
         ok = self._configure_ids()
 
         def process_column(name, rostype, value):  # Parse "column-name=rostype:value"
             v, myok = value, True
             if "string" not in rostype:
                 v = json.loads(v)
             if not name:
@@ -374,15 +385,15 @@
 
         def process_type(rostype, arrowtype):  # Eval pyarrow datatype from value like "float64()"
             if arrowtype not in self.ARROW_TYPES.values():
                 arrowtype = eval(compile(arrowtype, "", "eval"), {"__builtins__": self.ARROW_TYPES})
             self.COMMON_TYPES[rostype] = arrowtype
 
         for key, vals in [("print", self.args.EMIT_FIELD), ("noprint", self.args.NOEMIT_FIELD)]:
-            self._patterns[key] = [(tuple(v.split(".")), common.wildcard_to_regex(v)) for v in vals]
+            self._patterns[key] = [(tuple(v.split(".")), common.path_to_regex(v)) for v in vals]
 
         # Populate ROS type aliases like "byte" and "char"
         for rostype in list(self.COMMON_TYPES):
             alias = api.get_type_alias(rostype)
             if alias:
                 self.COMMON_TYPES[alias] = self.COMMON_TYPES[rostype]
             if alias and rostype + "[]" in self.COMMON_TYPES:
@@ -417,14 +428,16 @@
                     ConsolePrinter.error("Invalid %s option in %s=%s: %s", category, k, v, e)
         return ok
 
 
     def _configure_ids(self):
         """Configures ID generator from args.WRITE_OPTIONS, returns success."""
         ok = True
+        self.MESSAGE_TYPE_BASECOLS = type(self).MESSAGE_TYPE_BASECOLS[:]
+        self.MESSAGE_TYPE_NESTCOLS = type(self).MESSAGE_TYPE_NESTCOLS[:]
 
         k, v = "idgenerator", self.args.WRITE_OPTIONS.get("idgenerator")
         if k in self.args.WRITE_OPTIONS:
             val, ex, ns = v, None, dict(self.ARROW_TYPES, itertools=itertools, uuid=uuid)
             for root in v.split(".", 1)[:1]:
                 try: ns[root] = common.import_item(root)  # Provide root module
                 except Exception: pass
```

## grepros/plugins/sql.py

```diff
@@ -4,15 +4,15 @@
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     20.12.2021
-@modified    28.12.2023
+@modified    23.03.2024
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.plugins.sql
 import atexit
 import collections
 import datetime
 import os
@@ -74,21 +74,21 @@
         SqlMixin.__init__(self, args)
 
         self._filename      = None   # Unique output filename
         self._file          = None   # Open file() object
         self._batch         = None   # Current source batch
         self._nested_types  = {}     # {(typename, typehash): "CREATE TABLE .."}
         self._batch_metas   = []     # [source batch metainfo string, ]
-        self._overwrite     = (args.WRITE_OPTIONS.get("overwrite") in (True, "true"))
+        self._overwrite     = None
         self._close_printed = False
 
         # Whether to create tables for nested message types,
         # "array" if to do this only for arrays of nested types, or
         # "all" for any nested type, including those fully flattened into parent fields.
-        self._nesting = args.WRITE_OPTIONS.get("nesting")
+        self._nesting = None
 
         atexit.register(self.close)
 
 
     def validate(self):
         """
         Returns whether "dialect" and "nesting" and "overwrite" parameters contain supported values
@@ -105,14 +105,17 @@
             ConsolePrinter.error("Invalid overwrite option for SQL: %r. "
                                  "Choose one of {true, false}.",
                                  self.args.WRITE_OPTIONS["overwrite"])
             ok = False
         if not common.verify_io(self.args.WRITE, "w"):
             ok = False
         self.valid = sqlconfig_ok and ok
+        if self.valid:
+            self._overwrite = (self.args.WRITE_OPTIONS.get("overwrite") in (True, "true"))
+            self._nesting = self.args.WRITE_OPTIONS.get("nesting")
         return self.valid
 
 
     def emit(self, topic, msg, stamp=None, match=None, index=None):
         """Writes out message type CREATE TABLE statements to SQL schema file."""
         if not self.validate(): raise Exception("invalid")
         batch = self.source.get_batch()
```

## grepros/plugins/auto/csv.py

```diff
@@ -4,15 +4,15 @@
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     03.12.2021
-@modified    28.12.2023
+@modified    21.04.2024
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.plugins.auto.csv
 from __future__ import absolute_import
 import atexit
 import csv
 import itertools
@@ -54,43 +54,45 @@
         args = common.ensure_namespace(args, CsvSink.DEFAULT_ARGS, **kwargs)
         super(CsvSink, self).__init__(args)
         self._filebase      = args.WRITE  # Filename base, will be made unique
         self._files         = {}          # {(topic, typename, typehash): file()}
         self._writers       = {}          # {(topic, typename, typehash): CsvWriter}
         self._patterns      = {}          # {key: [(() if any field else ('path', ), re.Pattern), ]}
         self._lasttopickey  = None        # Last (topic, typename, typehash) emitted
-        self._overwrite     = (args.WRITE_OPTIONS.get("overwrite") in (True, "true"))
+        self._overwrite     = None
         self._close_printed = False
 
         for key, vals in [("print", args.EMIT_FIELD), ("noprint", args.NOEMIT_FIELD)]:
-            self._patterns[key] = [(tuple(v.split(".")), common.wildcard_to_regex(v)) for v in vals]
+            self._patterns[key] = [(tuple(v.split(".")), common.path_to_regex(v)) for v in vals]
         atexit.register(self.close)
 
     def emit(self, topic, msg, stamp=None, match=None, index=None):
         """Writes message to output file."""
         if not self.validate(): raise Exception("invalid")
         stamp, index = self._ensure_stamp_index(topic, msg, stamp, index)
         data = (v for _, v in self._iter_fields(msg))
         metadata = [api.to_sec(stamp), api.to_datetime(stamp), api.get_message_type(msg)]
         self._make_writer(topic, msg).writerow(itertools.chain(metadata, data))
         self._close_printed = False
         super(CsvSink, self).emit(topic, msg, stamp, match, index)
 
     def validate(self):
-        """Returns whether overwrite option is valid and file base is writable."""
+        """Returns whether arguments and overwrite option are valid, and file base is writable."""
         if self.valid is not None: return self.valid
-        result = True
+        result = Sink.validate(self)
         if self.args.WRITE_OPTIONS.get("overwrite") not in (None, True, False, "true", "false"):
             ConsolePrinter.error("Invalid overwrite option for CSV: %r. "
                                  "Choose one of {true, false}.",
                                  self.args.WRITE_OPTIONS["overwrite"])
             result = False
         if not common.verify_io(self.args.WRITE, "w"):
             result = False
         self.valid = result
+        if self.valid:
+            self._overwrite = self.args.WRITE_OPTIONS.get("overwrite") in (True, "true")
         return self.valid
 
     def close(self):
         """Closes output file(s), if any."""
         try:
             names = {k: f.name for k, f in self._files.items()}
             for k in names:
@@ -100,19 +102,20 @@
         finally:
             if not self._close_printed and self._counts:
                 self._close_printed = True
                 sizes = {k: None for k in names.values()}
                 for k, n in names.items():
                     try: sizes[k] = os.path.getsize(n)
                     except Exception as e: ConsolePrinter.warn("Error getting size of %s: %s", n, e)
-                ConsolePrinter.debug("Wrote %s in %s to CSV (%s):",
+                ConsolePrinter.debug("Wrote %s in %s to CSV (%s)%s",
                                      plural("message", sum(self._counts.values())),
                                      plural("topic", self._counts),
-                                     common.format_bytes(sum(filter(bool, sizes.values()))))
-                for topickey, name in names.items():
+                                     common.format_bytes(sum(filter(bool, sizes.values()))),
+                                     ":" if self.args.VERBOSE else ".")
+                for topickey, name in names.items() if self.args.VERBOSE else ():
                     ConsolePrinter.debug("- %s (%s, %s)", name,
                                          "error getting size" if sizes[topickey] is None else
                                          common.format_bytes(sizes[topickey]),
                                          plural("message", self._counts[topickey]))
             super(CsvSink, self).close()
 
     def _make_writer(self, topic, msg):
@@ -130,17 +133,17 @@
             name = self._files[topickey].name if topickey in self._files else None
             action = "Creating"  # Or "Overwriting"
             if not name:
                 base, ext = os.path.splitext(self._filebase)
                 name = "%s.%s%s" % (base, topic.lstrip("/").replace("/", "__"), ext)
                 if self._overwrite:
                     if os.path.isfile(name) and os.path.getsize(name): action = "Overwriting"
-                    open(name, "w").close()
+                    open(name, "wb").close()
                 else: name = common.unique_path(name)
-            flags = {"mode": "ab"} if six.PY2 else {"mode": "a", "newline": ""}
+            flags = {"mode": "ab"} if six.PY2 else {"mode": "a", "newline": "", "encoding": "utf-8"}
             f = open(name, **flags)
             w = CsvWriter(f)
             if topickey not in self._files:
                 if self.args.VERBOSE:
                     ConsolePrinter.debug("%s %s.", action, name)
                 header = (topic + "/" + ".".join(map(str, p)) for p, _ in self._iter_fields(msg))
                 metaheader = ["__time", "__datetime", "__type"]
```

## grepros/plugins/auto/dbbase.py

```diff
@@ -4,15 +4,15 @@
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     11.12.2021
-@modified    27.12.2023
+@modified    24.03.2024
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.plugins.auto.dbbase
 import atexit
 import collections
 
 from ... import api
@@ -90,15 +90,15 @@
         self._dialect       = self.ENGINE.lower()  # Override SqlMixin._dialect
         self._close_printed = False
 
         # Whether to create tables and rows for nested message types,
         # "array" if to do this only for arrays of nested types, or
         # "all" for any nested type, including those fully flattened into parent fields.
         # In parent, nested arrays are inserted as foreign keys instead of formatted values.
-        self._nesting = args.WRITE_OPTIONS.get("nesting")
+        self._nesting = None
 
         self._checkeds      = {}  # {topickey/typekey: whether existence checks are done}
         self._sql_queue     = {}  # {SQL: [(args), ]}
         self._nested_counts = {}  # {(typename, typehash): count}
 
         atexit.register(self.close)
 
@@ -117,14 +117,16 @@
                 ConsolePrinter.error("Invalid commit-interval option for %s: %r.",
                                      self.ENGINE, self.args.WRITE_OPTIONS["commit-interval"])
         if self.args.WRITE_OPTIONS.get("nesting") not in (None, False, "", "array", "all"):
             ConsolePrinter.error("Invalid nesting option for %s: %r. "
                                  "Choose one of {array,all}.",
                                  self.ENGINE, self.args.WRITE_OPTIONS["nesting"])
             ok = False
+        if ok and sqlconfig_ok:
+            self._nesting = self.args.WRITE_OPTIONS.get("nesting")
         return ok and sqlconfig_ok
 
 
     def emit(self, topic, msg, stamp=None, match=None, index=None):
         """Writes message to database."""
         if not self.validate(): raise Exception("invalid")
         if not self.db:
```

## grepros/plugins/auto/html.py

```diff
@@ -4,29 +4,29 @@
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     03.12.2021
-@modified    28.12.2023
+@modified    21.04.2024
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.plugins.auto.html
 import atexit
 import os
 try: import queue  # Py3
 except ImportError: import Queue as queue  # Py2
 import re
 import threading
 
 from ... import api
 from ... import common
 from ... import main
-from ... common import ConsolePrinter, MatchMarkers, plural
+from ... common import ConsolePrinter, MatchMarkers
 from ... outputs import RolloverSinkMixin, Sink, TextSinkMixin
 from ... vendor import step
 
 
 class HtmlSink(Sink, RolloverSinkMixin, TextSinkMixin):
     """Writes messages to an HTML file."""
 
@@ -90,28 +90,21 @@
         args.COLOR = bool(args.HIGHLIGHT)
 
         super(HtmlSink, self).__init__(args)
         RolloverSinkMixin.__init__(self, args)
         TextSinkMixin.__init__(self, args)
         self._queue     = queue.Queue()
         self._writer    = None        # threading.Thread running _stream()
-        self._overwrite = (args.WRITE_OPTIONS.get("overwrite") in (True, "true"))
-        self._template_path = args.WRITE_OPTIONS.get("template") or self.TEMPLATE_PATH
+        self._overwrite = None
+        self._template_path = None
         self._close_printed = False
+        self._tag_repls = {}
+        self._tag_rgx = None
 
-        WRAPS = ((args.MATCH_WRAPPER or [""]) * 2)[:2]
-        START = ('<span class="match">' + step.escape_html(WRAPS[0])) if args.HIGHLIGHT else ""
-        END = (step.escape_html(WRAPS[1]) + '</span>') if args.HIGHLIGHT else ""
-        self._tag_repls = {MatchMarkers.START: START,
-                           MatchMarkers.END:   END,
-                           ConsolePrinter.STYLE_LOWLIGHT: '<span class="lowlight">',
-                           ConsolePrinter.STYLE_RESET:    '</span>'}
-        self._tag_rgx = re.compile("(%s)" % "|".join(map(re.escape, self._tag_repls)))
-
-        self._format_repls.clear()
+        self._format_repls.clear()  # TextSinkMixin member
         atexit.register(self.close)
 
     def emit(self, topic, msg, stamp=None, match=None, index=None):
         """Writes message to output file."""
         if not self.validate(): raise Exception("invalid")
         stamp, index = self._ensure_stamp_index(topic, msg, stamp, index)
         RolloverSinkMixin.ensure_rollover(self, topic, msg, stamp)
@@ -124,26 +117,41 @@
 
     def validate(self):
         """
         Returns whether write options are valid and ROS environment is set and file is writable,
         emits error if not.
         """
         if self.valid is not None: return self.valid
-        result = RolloverSinkMixin.validate(self)
+        result = all([Sink.validate(self), TextSinkMixin.validate(self)])
+        if not RolloverSinkMixin.validate(self):
+            result = False
         if self.args.WRITE_OPTIONS.get("template") and not os.path.isfile(self._template_path):
             result = False
             ConsolePrinter.error("Template does not exist: %s.", self._template_path)
         if self.args.WRITE_OPTIONS.get("overwrite") not in (None, True, False, "true", "false"):
             ConsolePrinter.error("Invalid overwrite option for HTML: %r. "
                                  "Choose one of {true, false}.",
                                  self.args.WRITE_OPTIONS["overwrite"])
             result = False
         if not common.verify_io(self.args.WRITE, "w"):
             result = False
         self.valid = api.validate() and result
+        if self.valid:
+            self._overwrite = (self.args.WRITE_OPTIONS.get("overwrite") in (True, "true"))
+            self._template_path = self.args.WRITE_OPTIONS.get("template") or self.TEMPLATE_PATH
+
+            WRAPS, START = ((self.args.MATCH_WRAPPER or [""]) * 2)[:2], ""
+            if self.args.HIGHLIGHT: START = ('<span class="match">' + step.escape_html(WRAPS[0]))
+            END = (step.escape_html(WRAPS[1]) + '</span>') if self.args.HIGHLIGHT else ""
+            self._tag_repls = {MatchMarkers.START: START,
+                               MatchMarkers.END:   END,
+                               ConsolePrinter.STYLE_LOWLIGHT: '<span class="lowlight">',
+                               ConsolePrinter.STYLE_RESET:    '</span>'}
+            self._tag_rgx = re.compile("(%s)" % "|".join(map(re.escape, self._tag_repls)))
+
         return self.valid
 
     def close(self):
         """Closes output file, if any, emits metainfo."""
         try: self.close_output()
         finally:
             if not self._close_printed and self._counts:
@@ -175,15 +183,15 @@
 
     def _stream(self):
         """Writer-loop, streams HTML template to file."""
         if not self._writer:
             return
 
         try:
-            with open(self._template_path, "r") as f: tpl = f.read()
+            with open(self._template_path, encoding="utf-8") as f: tpl = f.read()
             template = step.Template(tpl, escape=True, strip=False, postprocess=convert_lf)
             ns = dict(source=self.source, sink=self, messages=self._produce(),
                       args=None, timeline=not self.args.ORDERBY)
             if main.CLI_ARGS: ns.update(args=main.CLI_ARGS)
             self.filename = self.filename or RolloverSinkMixin.make_filename(self)
             if self.args.VERBOSE:
                 sz = os.path.isfile(self.filename) and os.path.getsize(self.filename)
```

## grepros/plugins/auto/html.tpl

```diff
@@ -10,15 +10,15 @@
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     06.11.2021
-@modified    31.08.2023
+@modified    04.03.2023
 ------------------------------------------------------------------------------
 """
 import datetime, os, re
 from grepros import __title__, __version__, api
 
 dt = datetime.datetime.now().strftime("%Y-%m-%d %H:%M")
 sourcemeta = source.get_meta()
@@ -35,14 +35,18 @@
       color:                  white;
       font-family:            monospace;
       position:               relative;
     }
     #header #meta {
       white-space:            pre-wrap;
     }
+    #header #meta #result_count:empty::after {
+      content:                "Loading..";
+      color:                  gray;
+    }
     #footer {
       color:                  gray;
       bottom:                 0;
       position:               absolute;
     }
     #content {
       padding-bottom:         20px;
@@ -453,14 +457,22 @@
       var TOPICKEYS = self.TOPICKEYS = [];  // [[topic, typename, typehash], ]
       var SCHEMAS   = self.SCHEMAS   = {};  // {[typename, typehash]: schema}
       var FIRSTMSGS = self.FIRSTMSGS = {};  // {[topic, typename, typehash]: {id, dt}}
       var LASTMSGS  = self.LASTMSGS  = {};  // {[topic, typename, typehash]: {id, dt}}
       var MSGCOUNTS = self.MSGCOUNTS = {};  // {[topic, typename, typehash]: count}
 
 
+      /** Adds total message count to metainfo content. */
+      self.init = function() {
+        var count = Object.keys(MSGCOUNTS).reduce(function(sum, k) { return sum + MSGCOUNTS[k]; }, 0);
+        var countstr = count.toString().replace(/\B(?=(\d{3})+(?!\d))/g, ",");  // Thousand-separators
+        document.getElementById("result_count").innerHTML = countstr;
+      };
+
+
       /** Registers entry in a topic. */
       self.registerTopic = function(topic, type, hash, schema, id, secs_nsecs) {
         var topickey = [topic, type, hash];
         TOPICKEYS.push(topickey);
         SCHEMAS[[type, hash]] = schema;
         self.registerMessage(null, id, secs_nsecs, topic, type, hash);
       };
@@ -956,15 +968,16 @@
         scroll_timer = scroll_timer || window.setTimeout(self.highlight , 100);
       };
 
     };
 
 
     window.addEventListener("DOMContentLoaded", function() {
-      TOC.init()
+      Messages.init();
+      TOC.init();
 %if timeline:
       Timeline.init();
 %endif
     });
 
     if (document.location.hash) document.location.hash = "";
 
@@ -976,14 +989,15 @@
 
 <div id="header">
 %if source.format_meta().strip() or isdef("args") and args:
   <div id="meta">{{ source.format_meta().strip() }}
     %if isdef("args") and args:
 Command: {{ __title__ }} {{ " ".join(args) }}
     %endif
+Messages in result: <span id="result_count"></span>
 %endif
   </div>
   <div id="topics">
     <span title="Toggle contents" onclick="return toggleClass('toc', 'collapsed', document.getElementById('toggle_topics'))">
       Contents: <span id="toggle_topics" class="toggle collapsed"></span>
     </span>
     <table id="toc" class="collapsed">
```

## grepros/plugins/auto/sqlbase.py

```diff
@@ -4,25 +4,25 @@
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     03.01.2022
-@modified    28.06.2023
+@modified    21.04.2024
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.plugins.auto.sqlbase
 import json
 import re
 
 import yaml
 
 from ... import api
-from ... common import ConsolePrinter, ellipsize, ensure_namespace, import_item, merge_dicts
+from ... common import ConsolePrinter, ellipsize, import_item, merge_dicts
 
 
 
 class SqlMixin(object):
     """
     Base class for producing SQL for topics and messages.
 
@@ -30,51 +30,54 @@
     from a YAML/JSON file.
     """
 
     ## Default SQL dialect used if dialect not specified
     DEFAULT_DIALECT = "sqlite"
 
     ## Constructor argument defaults
-    DEFAULT_ARGS = dict(META=False, WRITE_OPTIONS={}, MATCH_WRAPPER=None, VERBOSE=False)
+    DEFAULT_ARGS = dict(WRITE_OPTIONS={})
 
 
     def __init__(self, args=None, **kwargs):
         """
         @param   args                 arguments as namespace or dictionary, case-insensitive
         @param   args.write_options   ```
                                       {"dialect": SQL dialect if not default,
                                        "nesting": true|false to created nested type tables}
                                       ```
         @param   kwargs               any and all arguments as keyword overrides, case-insensitive
         """
-        self._args      = ensure_namespace(args, SqlMixin.DEFAULT_ARGS, **kwargs)
         self._topics    = {}  # {(topic, typename, typehash): {name, table_name, view_name, sql, ..}}
         self._types     = {}  # {(typename, typehash): {type, table_name, sql, ..}}
         self._schema    = {}  # {(typename, typehash): {cols}}
         self._sql_cache = {}  # {table: "INSERT INTO table VALUES (%s, ..)"}
-        self._dialect   = args.WRITE_OPTIONS.get("dialect", self.DEFAULT_DIALECT)
-        self._nesting   = args.WRITE_OPTIONS.get("nesting")
+        self._dialect   = None
+        self._nesting   = None
 
 
     def validate(self):
         """
         Returns whether arguments are valid.
 
         Verifies that "dialect-file" is valid and "dialect" contains supported value, if any.
         """
+        write_options = getattr(self.args, "WRITE_OPTIONS", {})
+        self._dialect = write_options.get("dialect", (self._dialect or self.DEFAULT_DIALECT))
+        self._nesting = write_options.get("nesting")
         return all([self._validate_dialect_file(), self._validate_dialect()])
 
 
     def _validate_dialect_file(self):
         """Returns whether "dialect-file" is valid in args.WRITE_OPTIONS."""
         ok = True
-        if self._args.WRITE_OPTIONS.get("dialect-file"):
-            filename = self._args.WRITE_OPTIONS["dialect-file"]
+        write_options = getattr(self.args, "WRITE_OPTIONS", {})
+        if write_options.get("dialect-file"):
+            filename = write_options["dialect-file"]
             try:
-                with open(filename) as f:
+                with open(filename, encoding="utf-8") as f:
                     dialects = yaml.safe_load(f.read())
                 if any(not isinstance(v, dict) for v in dialects.values()):
                     raise Exception("Each dialect must be a dictionary.")
                 for opts in dialects.values():
                     for k, v in list(opts.get("adapters", {}).items()):
                         try: opts["adapters"][k] = import_item(v)
                         except ImportError:
@@ -97,20 +100,20 @@
 
         return ok
 
 
     def _validate_dialect(self):
         """Returns whether "dialect" is valid in args.WRITE_OPTIONS."""
         ok = True
-        if "dialect" in self._args.WRITE_OPTIONS \
-        and self._args.WRITE_OPTIONS["dialect"] not in tuple(filter(bool, self.DIALECTS)):
+        write_options = getattr(self.args, "WRITE_OPTIONS", {})
+        if "dialect" in write_options \
+        and write_options["dialect"] not in tuple(filter(bool, self.DIALECTS)):
             ok = False
-            ConsolePrinter.error("Unknown dialect for SQL: %r. "
-                                 "Choose one of {%s}.",
-                                 self._args.WRITE_OPTIONS["dialect"],
+            ConsolePrinter.error("Unknown dialect for SQL: %r. Choose one of {%s}.",
+                                 write_options["dialect"],
                                  "|".join(sorted(filter(bool, self.DIALECTS))))
         return ok
 
 
     def close(self):
         """Clears data structures."""
         self._topics.clear()
```

## grepros/plugins/auto/sqlite.py

```diff
@@ -4,15 +4,15 @@
 
 ------------------------------------------------------------------------------
 This file is part of grepros - grep for ROS bag files and live topics.
 Released under the BSD License.
 
 @author      Erki Suurjaak
 @created     03.12.2021
-@modified    27.12.2023
+@modified    21.04.2024
 ------------------------------------------------------------------------------
 """
 ## @namespace grepros.plugins.auto.sqlite
 import collections
 import json
 import os
 import sqlite3
@@ -84,16 +84,16 @@
         @param   args.meta            whether to emit metainfo
         @param   args.verbose         whether to emit debug information
         @param   kwargs               any and all arguments as keyword overrides, case-insensitive
         """
         super(SqliteSink, self).__init__(args, **kwargs)
         RolloverSinkMixin.__init__(self, args)
 
-        self._do_yaml     = (self.args.WRITE_OPTIONS.get("message-yaml") != "false")
-        self._overwrite   = (self.args.WRITE_OPTIONS.get("overwrite") in (True, "true"))
+        self._do_yaml     = None
+        self._overwrite   = None
         self._id_counters = {}  # {table next: max ID}
 
 
     def validate(self):
         """
         Returns whether "commit-interval" and "nesting" in args.write_options have valid value, if any,
         and file is writable; parses "message-yaml" and "overwrite" from args.write_options.
@@ -109,19 +109,23 @@
             ConsolePrinter.error("Invalid overwrite option for %s: %r. "
                                  "Choose one of {true, false}.",
                                  self.ENGINE, self.args.WRITE_OPTIONS["overwrite"])
             ok = False
         if not verify_io(self.args.WRITE, "w"):
             ok = False
         self.valid = ok
+        if self.valid:
+            self._do_yaml   = (self.args.WRITE_OPTIONS.get("message-yaml") != "false")
+            self._overwrite = (self.args.WRITE_OPTIONS.get("overwrite") in (True, "true"))
         return self.valid
 
 
     def emit(self, topic, msg, stamp=None, match=None, index=None):
         """Writes message to database."""
+        if not self.validate(): raise Exception("invalid")
         stamp, index = self._ensure_stamp_index(topic, msg, stamp, index)
         RolloverSinkMixin.ensure_rollover(self, topic, msg, stamp)
         super(SqliteSink, self).emit(topic, msg, stamp, match, index)
 
 
     @property
     def size(self):
@@ -172,15 +176,15 @@
         self._ensure_execute(self._get_dialect_option("insert_message"), margs)
         super(SqliteSink, self)._process_message(topic, msg, stamp)
 
 
     def _connect(self):
         """Returns new database connection."""
         makedirs(os.path.dirname(self.filename))
-        if self._overwrite: open(self.filename, "w").close()
+        if self._overwrite: open(self.filename, "wb").close()
         db = sqlite3.connect(self.filename, check_same_thread=False,
                              detect_types=sqlite3.PARSE_DECLTYPES)
         if not self.COMMIT_INTERVAL: db.isolation_level = None
         db.row_factory = lambda cursor, row: dict(sqlite3.Row(cursor, row))
         return db
```

## Comparing `grepros-1.1.0.dist-info/LICENSE.md` & `grepros-1.2.0.dist-info/LICENSE.md`

 * *Files identical despite different names*

## Comparing `grepros-1.1.0.dist-info/RECORD` & `grepros-1.2.0.dist-info/RECORD`

 * *Files 15% similar despite different names*

```diff
@@ -1,32 +1,32 @@
-grepros/__init__.py,sha256=8V3RPFeyGaudg5yxYFdw-MoptiM7pgHpii31RJ12q4I,533
+grepros/__init__.py,sha256=6CoIJBHutOwlHbLpn5s5OoSTbmJF4ZaSPMfEAH_TmNc,533
 grepros/__main__.py,sha256=mr_Xu92us0Ya3d5IedjZPN6hkOd8dedbOB_sJOicwnI,452
-grepros/api.py,sha256=MRJQupDm13ksSrrPqjydVrBMOGkz23Zb73n3NS8mIPk,47295
-grepros/common.py,sha256=1Y-36pZyMjLAGN9vzHYlzxe_bxlN9ay2aNXbVcLDJdI,44216
-grepros/inputs.py,sha256=aya_K4B8QtcZpJIV-hd_8rH94TK698JP4l-7LyyPVvU,54752
-grepros/library.py,sha256=UXtagVR64r1XYLjzR65awpDgRnhw4gu10QxGEShgOAw,19032
-grepros/main.py,sha256=HsFK2UpedKAfDRSb0Fdl2aM0biI5x8gd-N-xPBn0uoA,25352
-grepros/outputs.py,sha256=ntVybUUM276_jh8vuDgPWe8u5mAoqEq_wWrdRSEbOW0,47055
-grepros/ros1.py,sha256=mMtsBhUep5r90AjrJZd6puFfNEGXMDxodxzzFaLdWXM,32248
-grepros/ros2.py,sha256=JHn312r0r_ywDqIfzX0_7b6BOPLwzHSd4_HmAYqwTfE,44435
-grepros/search.py,sha256=FMJco4cJuaEmSO14ZEsa_Zs2l5QzbBZkzkanGslmzbE,22214
+grepros/api.py,sha256=9xDfVnf_oD2RKl_AOorih4kqwYth5ZOV4x4gfap6y4o,47674
+grepros/common.py,sha256=mMurdKzZ7GO7GN2Xf6Wbn71KMbGopDfRSguIzy07aVY,54073
+grepros/inputs.py,sha256=dH2ZO6qL9F0oV4pULh9PazNuOK6WjGIhDwKSvv6C1eA,67446
+grepros/library.py,sha256=dsCopU1FaaXbPRwkyq_tAAWqTw9qRePBE5rvB4idmWI,20353
+grepros/main.py,sha256=vRj-Xp3xDpzx7KpRp-vqKqhRPgqpyxWwMd3B_lhnc7M,21689
+grepros/outputs.py,sha256=niBndRVJXMM660KZuF_H4Ad_CO9Gamev3b3B48lOuwU,49897
+grepros/ros1.py,sha256=uwQ18rgrGdiDIGaERLdplNdkY5OfIYq2xXTTTND8Rq4,32696
+grepros/ros2.py,sha256=Ve5vILg4WgOLQft3waIyqGCXcPOyclzk07w0CE6fKHM,45297
+grepros/search.py,sha256=aLyf9qiPvE2ltPAgfnom4nQ5VR32XI-Jf9n9SSHsQSc,42847
 grepros/plugins/__init__.py,sha256=gx2cEJNVcz4nkb1ZYGIXPJ06eRUEZYWJYzbL6sXV0fA,11626
 grepros/plugins/embag.py,sha256=K2aFVQxyIpJdUgyIbJ0-zCoDCvzWyHwm4sd0avscCR0,12480
-grepros/plugins/mcap.py,sha256=JQxS7c8TSlmUk7jbbaxlmvDXgpsQFD3IUKm8U9f0QDA,31550
-grepros/plugins/parquet.py,sha256=Bk5NiE2OwGBkUpdPYf0l5_sDAqRJ35vFeI6L2-Qhksg,25155
-grepros/plugins/sql.py,sha256=8sXNgzcXxcMNawE2ZxzVjt7bKGQb1pAkC2iVRBB_Nzk,12099
+grepros/plugins/mcap.py,sha256=oHnpjUA5jkoPRQqZSETJScpUHezMYTXaAhZYV-xBWnA,31985
+grepros/plugins/parquet.py,sha256=iLaYH3C9DpTJ4bU9D9a_5STi3rfBYOPwS4YYiPbEKBY,25864
+grepros/plugins/sql.py,sha256=E_K6UU6W4u6iNO1Vymbvu2W7FQdRAJG4-LWb8hb_aNM,12200
 grepros/plugins/auto/__init__.py,sha256=LQiEql_xKJooVHXQnGL9MgFaEyw_tWCttLC9d3-SxbA,426
-grepros/plugins/auto/csv.py,sha256=7yj3rZUlFbgv4FSnmQbpu90-wQ8m5eMHj4w2GU0pyrA,12009
-grepros/plugins/auto/dbbase.py,sha256=rGHaKultuYkrlhqzr5dQA8jE1ROoVzBqyFwmAT_To28,16875
-grepros/plugins/auto/html.py,sha256=zP81cHipeXEisk60s3e4JzOzlb5O7dG03QxN9nbDWwY,11885
-grepros/plugins/auto/html.tpl,sha256=nW5eFmWsR5KebGieKHDKw0CN3fsCmhYVdDnsT-3Jz-A,42477
+grepros/plugins/auto/csv.py,sha256=Kfwnl3mxXFMaEe76j_oxRER7Tn3nZ7FJautP9poJhFE,12221
+grepros/plugins/auto/dbbase.py,sha256=MuWw6VoIG50c5yXFBNNDuoZaxTKe7yIWM7DhxCuSP9E,16945
+grepros/plugins/auto/html.py,sha256=QNraKm8SdbnotsfGRoKDnjb2FmbixtWctGARuiiSdoo,12231
+grepros/plugins/auto/html.tpl,sha256=czZV1tBlAoINI01iRtYxRDS3WuYlLo-RoNURI3pUoTM,43062
 grepros/plugins/auto/postgres.py,sha256=ZDSHrGVc5cjQuGBXSZKqJpAXH8DMexZo5pStOQndrvs,10151
-grepros/plugins/auto/sqlbase.py,sha256=pQR137yjL8eRlZ8BzFi2S-r6QDL2D82gbzUdeix5lI4,32550
-grepros/plugins/auto/sqlite.py,sha256=5hjOdw7_Q76cu8kWnvVDCw3_OUd8KEyUVpT9D_gwh7c,11173
+grepros/plugins/auto/sqlbase.py,sha256=QUPXRl63hiutZV9l8RUg6i190Fs96ZJErcYT56BJ3qA,32589
+grepros/plugins/auto/sqlite.py,sha256=HCAscH7T3Ytalk-Cj2FggxtvfLmS-HPrcUTgc5UUIC8,11326
 grepros/vendor/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 grepros/vendor/step.py,sha256=shM1t0Lj2YhIyO_8e7yX1iE3u_lvAlqKddx9Gnmx_PQ,7750
-grepros-1.1.0.data/scripts/grepros,sha256=q45osRF80_0faCXKj47IJFLjIb21L7-z2_Fp90Ru6fs,485
-grepros-1.1.0.dist-info/LICENSE.md,sha256=qlDZaCV5FQE44nTvlsLWm9av1VhgIlKbu3h4TK6wknI,1523
-grepros-1.1.0.dist-info/METADATA,sha256=yPO-6BXAOAXWfSB1_alCyzKPWCDTCcOlkslCTn9bycc,351
-grepros-1.1.0.dist-info/WHEEL,sha256=iYlv5fX357PQyRT2o6tw1bN-YcKFFHKqB_LwHO5wP-g,110
-grepros-1.1.0.dist-info/top_level.txt,sha256=DfFuozU8--EUKxcjRvbaWrUwm9uWPK6vsRPJUyRQ8yU,8
-grepros-1.1.0.dist-info/RECORD,,
+grepros-1.2.0.dist-info/LICENSE.md,sha256=qlDZaCV5FQE44nTvlsLWm9av1VhgIlKbu3h4TK6wknI,1523
+grepros-1.2.0.dist-info/METADATA,sha256=O2-7ArNFy6xGGOMkd1J3nKNq55AfECPSFMXpvqdAPDU,37074
+grepros-1.2.0.dist-info/WHEEL,sha256=iYlv5fX357PQyRT2o6tw1bN-YcKFFHKqB_LwHO5wP-g,110
+grepros-1.2.0.dist-info/entry_points.txt,sha256=5emWEbHZTH6c0sAAHZtwCLs_lut1qsd0VNcG7-9GNEE,46
+grepros-1.2.0.dist-info/top_level.txt,sha256=DfFuozU8--EUKxcjRvbaWrUwm9uWPK6vsRPJUyRQ8yU,8
+grepros-1.2.0.dist-info/RECORD,,
```

