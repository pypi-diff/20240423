# Comparing `tmp/ml_metadata-1.8.0-cp39-cp39-win_amd64.whl.zip` & `tmp/ml_metadata-1.9.0-cp39-cp39-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,27 +1,27 @@
-Zip file size: 1611074 bytes, number of entries: 25
--rw-rw-rw-  2.0 fat     1178 b- defN 22-May-11 00:00 ml_metadata/__init__.py
--rw-rw-rw-  2.0 fat     6261 b- defN 22-May-11 00:00 ml_metadata/errors.py
--rw-rw-rw-  2.0 fat      707 b- defN 22-May-11 00:00 ml_metadata/version.py
--rw-rw-rw-  2.0 fat      901 b- defN 22-May-11 00:00 ml_metadata/metadata_store/__init__.py
--rw-rw-rw-  2.0 fat    54021 b- defN 22-May-11 00:01 ml_metadata/metadata_store/metadata_store.py
--rw-rw-rw-  2.0 fat    84242 b- defN 22-May-11 00:01 ml_metadata/metadata_store/metadata_store_test.py
--rw-rw-rw-  2.0 fat     5859 b- defN 22-May-11 00:00 ml_metadata/metadata_store/mlmd_types.py
--rw-rw-rw-  2.0 fat     2824 b- defN 22-May-11 00:00 ml_metadata/metadata_store/mlmd_types_test.py
--rw-rw-rw-  2.0 fat    57300 b- defN 22-May-11 00:00 ml_metadata/metadata_store/types.py
--rw-rw-rw-  2.0 fat    21658 b- defN 22-May-11 00:00 ml_metadata/metadata_store/types_test.py
--rw-rw-rw-  2.0 fat      589 b- defN 22-May-11 00:00 ml_metadata/metadata_store/pywrap/__init__.py
--rw-rw-rw-  2.0 fat  4041728 b- defN 22-May-11 00:03 ml_metadata/metadata_store/pywrap/metadata_store_extension.pyd
--rw-rw-rw-  2.0 fat     4086 b- defN 22-May-11 00:00 ml_metadata/proto/__init__.py
--rw-rw-rw-  2.0 fat   157216 b- defN 22-May-11 00:03 ml_metadata/proto/metadata_store_pb2.py
--rw-rw-rw-  2.0 fat   233313 b- defN 22-May-11 00:03 ml_metadata/proto/metadata_store_service_pb2.py
--rw-rw-rw-  2.0 fat    90353 b- defN 22-May-11 00:03 ml_metadata/proto/metadata_store_service_pb2_grpc.py
--rw-rw-rw-  2.0 fat      589 b- defN 22-May-11 00:00 ml_metadata/simple_types/__init__.py
--rw-rw-rw-  2.0 fat      589 b- defN 22-May-11 00:00 ml_metadata/simple_types/proto/__init__.py
--rw-rw-rw-  2.0 fat     3826 b- defN 22-May-11 00:03 ml_metadata/simple_types/proto/simple_types_pb2.py
--rw-rw-rw-  2.0 fat    11541 b- defN 22-May-11 00:03 ml_metadata-1.8.0.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     4943 b- defN 22-May-11 00:03 ml_metadata-1.8.0.dist-info/METADATA
--rw-rw-rw-  2.0 fat      100 b- defN 22-May-11 00:03 ml_metadata-1.8.0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        1 b- defN 22-May-11 00:03 ml_metadata-1.8.0.dist-info/namespace_packages.txt
--rw-rw-rw-  2.0 fat       12 b- defN 22-May-11 00:03 ml_metadata-1.8.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     2361 b- defN 22-May-11 00:03 ml_metadata-1.8.0.dist-info/RECORD
-25 files, 4786198 bytes uncompressed, 1607176 bytes compressed:  66.4%
+Zip file size: 1618570 bytes, number of entries: 25
+-rw-rw-rw-  2.0 fat     1178 b- defN 22-Jun-21 20:45 ml_metadata/__init__.py
+-rw-rw-rw-  2.0 fat     6261 b- defN 22-Jun-21 20:45 ml_metadata/errors.py
+-rw-rw-rw-  2.0 fat      707 b- defN 22-Jun-21 20:45 ml_metadata/version.py
+-rw-rw-rw-  2.0 fat      901 b- defN 22-Jun-21 20:45 ml_metadata/metadata_store/__init__.py
+-rw-rw-rw-  2.0 fat    57338 b- defN 22-Jun-21 20:46 ml_metadata/metadata_store/metadata_store.py
+-rw-rw-rw-  2.0 fat    92457 b- defN 22-Jun-21 20:46 ml_metadata/metadata_store/metadata_store_test.py
+-rw-rw-rw-  2.0 fat     5859 b- defN 22-Jun-21 20:45 ml_metadata/metadata_store/mlmd_types.py
+-rw-rw-rw-  2.0 fat     2824 b- defN 22-Jun-21 20:45 ml_metadata/metadata_store/mlmd_types_test.py
+-rw-rw-rw-  2.0 fat    57300 b- defN 22-Jun-21 20:45 ml_metadata/metadata_store/types.py
+-rw-rw-rw-  2.0 fat    21658 b- defN 22-Jun-21 20:45 ml_metadata/metadata_store/types_test.py
+-rw-rw-rw-  2.0 fat      589 b- defN 22-Jun-21 20:45 ml_metadata/metadata_store/pywrap/__init__.py
+-rw-rw-rw-  2.0 fat  4058112 b- defN 22-Jun-21 20:48 ml_metadata/metadata_store/pywrap/metadata_store_extension.pyd
+-rw-rw-rw-  2.0 fat     4086 b- defN 22-Jun-21 20:45 ml_metadata/proto/__init__.py
+-rw-rw-rw-  2.0 fat   157216 b- defN 22-Jun-21 20:48 ml_metadata/proto/metadata_store_pb2.py
+-rw-rw-rw-  2.0 fat   233313 b- defN 22-Jun-21 20:48 ml_metadata/proto/metadata_store_service_pb2.py
+-rw-rw-rw-  2.0 fat    90353 b- defN 22-Jun-21 20:48 ml_metadata/proto/metadata_store_service_pb2_grpc.py
+-rw-rw-rw-  2.0 fat      589 b- defN 22-Jun-21 20:45 ml_metadata/simple_types/__init__.py
+-rw-rw-rw-  2.0 fat      589 b- defN 22-Jun-21 20:45 ml_metadata/simple_types/proto/__init__.py
+-rw-rw-rw-  2.0 fat     3826 b- defN 22-Jun-21 20:48 ml_metadata/simple_types/proto/simple_types_pb2.py
+-rw-rw-rw-  2.0 fat    11541 b- defN 22-Jun-21 20:48 ml_metadata-1.9.0.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat     4958 b- defN 22-Jun-21 20:48 ml_metadata-1.9.0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      100 b- defN 22-Jun-21 20:48 ml_metadata-1.9.0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        1 b- defN 22-Jun-21 20:48 ml_metadata-1.9.0.dist-info/namespace_packages.txt
+-rw-rw-rw-  2.0 fat       12 b- defN 22-Jun-21 20:48 ml_metadata-1.9.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     2361 b- defN 22-Jun-21 20:49 ml_metadata-1.9.0.dist-info/RECORD
+25 files, 4814129 bytes uncompressed, 1614672 bytes compressed:  66.5%
```

## zipnote {}

```diff
@@ -51,26 +51,26 @@
 
 Filename: ml_metadata/simple_types/proto/__init__.py
 Comment: 
 
 Filename: ml_metadata/simple_types/proto/simple_types_pb2.py
 Comment: 
 
-Filename: ml_metadata-1.8.0.dist-info/LICENSE
+Filename: ml_metadata-1.9.0.dist-info/LICENSE
 Comment: 
 
-Filename: ml_metadata-1.8.0.dist-info/METADATA
+Filename: ml_metadata-1.9.0.dist-info/METADATA
 Comment: 
 
-Filename: ml_metadata-1.8.0.dist-info/WHEEL
+Filename: ml_metadata-1.9.0.dist-info/WHEEL
 Comment: 
 
-Filename: ml_metadata-1.8.0.dist-info/namespace_packages.txt
+Filename: ml_metadata-1.9.0.dist-info/namespace_packages.txt
 Comment: 
 
-Filename: ml_metadata-1.8.0.dist-info/top_level.txt
+Filename: ml_metadata-1.9.0.dist-info/top_level.txt
 Comment: 
 
-Filename: ml_metadata-1.8.0.dist-info/RECORD
+Filename: ml_metadata-1.9.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## ml_metadata/version.py

```diff
@@ -11,8 +11,8 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Contains the version string of ML Metadata."""
 
 # Note that setup.py uses this version.
-__version__ = '1.8.0'
+__version__ = '1.9.0'
```

## ml_metadata/metadata_store/metadata_store.py

```diff
@@ -589,14 +589,82 @@
             artifact=pair[0], event=pair[1] if len(pair) == 2 else None)
     response = metadata_store_service_pb2.PutExecutionResponse()
     self._call('PutExecution', request, response)
     artifact_ids = [x for x in response.artifact_ids]
     context_ids = [x for x in response.context_ids]
     return response.execution_id, artifact_ids, context_ids
 
+  def put_lineage_subgraph(
+      self,
+      executions: Sequence[proto.Execution],
+      artifacts: Sequence[proto.Artifact],
+      contexts: Sequence[proto.Context],
+      event_edges: Sequence[Tuple[Optional[int], Optional[int], proto.Event]],
+      reuse_context_if_already_exist: bool = False
+  ) -> Tuple[List[int], List[int], List[int]]:
+    """Inserts a collection of executions, artifacts, contexts, and events.
+
+    This method atomically inserts or updates all specified executions,
+    artifacts, and events and adds attributions and associations to related
+    contexts.
+
+    Args:
+      executions: List of executions to be created or updated.
+      artifacts: List of artifacts to be created or updated.
+      contexts: List of contexts to be created or reused. Contexts will be
+        associated with the inserted executions and attributed to the inserted
+        artifacts.
+      event_edges: List of event edges in the subgraph to be inserted. Event
+        edges are defined as an optional execution_index, an optional
+        artifact_index, and a required event. Event edges must have an
+        execution_index and/or an event.execution_id. Execution_index
+        corresponds to an execution in the executions list at the specified
+        index. If both execution_index and event.execution_id are provided, the
+        execution ids of the execution and the event must match. The same rules
+        apply to artifact_index and event.artifact_id.
+      reuse_context_if_already_exist: When there's a race to publish executions
+        with a new context (no id) with the same context.name, by default there
+        will be one writer that succeeds and the rest of the writers will fail
+        with AlreadyExists errors. If set to True, failed writers will reuse the
+        stored context.
+
+    Returns:
+      The lists of execution ids, artifact ids, and context ids index aligned
+      to the input executions, artifacts, and contexts.
+
+    Raises:
+      errors.InvalidArgumentError: If the id of the input nodes do not align
+        with the store. Please refer to InvalidArgument errors in other put
+        methods.
+      errors.AlreadyExistsError: If the new nodes to be created already exist.
+        Please refer to AlreadyExists errors in other put methods.
+      errors.OutOfRangeError: If event_edge indices do not correspond to
+        existing indices in the input lists of executions and artifacts.
+    """
+    request = metadata_store_service_pb2.PutLineageSubgraphRequest(
+        executions=executions,
+        artifacts=artifacts,
+        contexts=contexts,
+        options=metadata_store_service_pb2.PutLineageSubgraphRequest.Options(
+            reuse_context_if_already_exist=reuse_context_if_already_exist))
+
+    # Add event edges to the request
+    for execution_index, artifact_index, event in event_edges:
+      request.event_edges.add(
+          execution_index=execution_index,
+          artifact_index=artifact_index,
+          event=event)
+
+    response = metadata_store_service_pb2.PutLineageSubgraphResponse()
+    self._call('PutLineageSubgraph', request, response)
+    execution_ids = list(response.execution_ids)
+    artifact_ids = list(response.artifact_ids)
+    context_ids = list(response.context_ids)
+    return execution_ids, artifact_ids, context_ids
+
   def get_artifacts_by_type(
       self,
       type_name: Text,
       type_version: Optional[Text] = None) -> List[proto.Artifact]:
     """Gets all the artifacts of a given type.
 
     Args:
```

## ml_metadata/metadata_store/metadata_store_test.py

```diff
@@ -1618,14 +1618,176 @@
     self.assertEqual(context_id, context_id_2)
 
     [context_result] = store.get_contexts_by_id([context_id])
     self.assertEqual(context_result.name, context_2.name)
     self.assertEqual(context_result.properties["bar"].string_value, "Goodbye")
     self.assertEqual(context_result.properties["foo"].int_value, 12)
 
+  def test_put_lineage_subgraph(self):
+    store = _get_metadata_store()
+    execution_type = _create_example_execution_type(self._get_test_type_name())
+    execution_type_id = store.put_execution_type(execution_type)
+    artifact_type = _create_example_artifact_type(self._get_test_type_name())
+    artifact_type_id = store.put_artifact_type(artifact_type)
+    context_type = _create_example_context_type(self._get_test_type_name())
+    context_type_id = store.put_context_type(context_type)
+
+    existing_context = metadata_store_pb2.Context(
+        type_id=context_type_id, name="existing_context")
+    [existing_context_id] = store.put_contexts([existing_context])
+    new_context = metadata_store_pb2.Context(
+        type_id=context_type_id, name="new_context")
+    request_contexts = [existing_context, new_context]
+
+    existing_execution = metadata_store_pb2.Execution(
+        type_id=execution_type_id, name="existing_execution")
+    [existing_execution_id] = store.put_executions([existing_execution])
+    existing_execution.id = existing_execution_id
+    new_execution = metadata_store_pb2.Execution(
+        type_id=execution_type_id, name="new_execution")
+    request_executions = [existing_execution, new_execution]
+
+    input_artifact = metadata_store_pb2.Artifact(
+        type_id=artifact_type_id, uri="testuri")
+    [input_artifact_id] = store.put_artifacts([input_artifact])
+    input_artifact.id = input_artifact_id
+    output_artifact = metadata_store_pb2.Artifact(
+        type_id=artifact_type_id, uri="output_artifact")
+    request_artifacts = [input_artifact, output_artifact]
+
+    input_event_for_existing_execution = metadata_store_pb2.Event(
+        type=metadata_store_pb2.Event.INPUT,
+        execution_id=existing_execution_id,
+        artifact_id=input_artifact_id)
+    input_event_for_new_execution = metadata_store_pb2.Event(
+        type=metadata_store_pb2.Event.INPUT, artifact_id=input_artifact_id)
+    output_event_for_existing_execution = metadata_store_pb2.Event(
+        type=metadata_store_pb2.Event.OUTPUT,
+        execution_id=existing_execution_id)
+    output_event_for_new_execution = metadata_store_pb2.Event(
+        type=metadata_store_pb2.Event.OUTPUT)
+    request_event_edges = [(0, 0, input_event_for_existing_execution),
+                           (None, 1, output_event_for_existing_execution),
+                           (1, None, input_event_for_new_execution),
+                           (1, 1, output_event_for_new_execution)]
+
+    # Request should fail since existing_context already inserted
+    with self.assertRaises(errors.AlreadyExistsError):
+      store.put_lineage_subgraph(request_executions, request_artifacts,
+                                 request_contexts, request_event_edges)
+
+    # Request should succeed with `reuse_context_if_already_exist` set
+    execution_ids, artifact_ids, context_ids = store.put_lineage_subgraph(
+        request_executions,
+        request_artifacts,
+        request_contexts,
+        request_event_edges,
+        reuse_context_if_already_exist=True)
+    for execution, execution_id in zip(request_executions, execution_ids):
+      execution.id = execution_id
+    for artifact, artifact_id in zip(request_artifacts, artifact_ids):
+      artifact.id = artifact_id
+    for context, context_id in zip(request_contexts, context_ids):
+      context.id = context_id
+
+    # Verify inserted items
+    self.assertLen(execution_ids, 2)
+    self.assertEqual(execution_ids[0], existing_execution_id)
+    self.assertLen(artifact_ids, 2)
+    self.assertEqual(artifact_ids[0], input_artifact_id)
+    self.assertLen(context_ids, 2)
+    self.assertEqual(context_ids[0], existing_context_id)
+
+    get_contexts_results = store.get_contexts_by_type(
+        type_name=context_type.name)
+    self.assertLen(get_contexts_results, 2)
+    get_contexts_results = {
+        context.id: context for context in get_contexts_results
+    }
+    self.assertIn(existing_context.id, get_contexts_results)
+    self.assertEqual(get_contexts_results[existing_context.id].name,
+                     existing_context.name)
+    self.assertEqual(get_contexts_results[existing_context.id].type_id,
+                     existing_context.type_id)
+    self.assertIn(new_context.id, get_contexts_results)
+    self.assertEqual(get_contexts_results[new_context.id].name,
+                     new_context.name)
+    self.assertEqual(get_contexts_results[new_context.id].type_id,
+                     new_context.type_id)
+
+    get_artifacts_by_existing_context_result = store.get_artifacts_by_context(
+        existing_context.id)
+    get_artifacts_by_new_context_result = store.get_artifacts_by_context(
+        new_context.id)
+    self.assertEqual(get_artifacts_by_existing_context_result,
+                     get_artifacts_by_new_context_result)
+    self.assertLen(get_artifacts_by_new_context_result, 2)
+    get_artifacts_result = {
+        artifact.id: artifact
+        for artifact in get_artifacts_by_new_context_result
+    }
+    self.assertIn(artifact_ids[0], get_artifacts_result)
+    self.assertEqual(get_artifacts_result[artifact_ids[0]].type_id,
+                     input_artifact.type_id)
+    self.assertEqual(get_artifacts_result[artifact_ids[0]].uri,
+                     input_artifact.uri)
+    self.assertIn(artifact_ids[1], get_artifacts_result)
+    self.assertEqual(get_artifacts_result[artifact_ids[1]].type_id,
+                     output_artifact.type_id)
+    self.assertEqual(get_artifacts_result[artifact_ids[1]].uri,
+                     output_artifact.uri)
+
+    get_executions_by_existing_context_result = store.get_executions_by_context(
+        existing_context.id)
+    get_executions_by_new_context_result = store.get_executions_by_context(
+        new_context.id)
+    self.assertEqual(get_executions_by_existing_context_result,
+                     get_executions_by_new_context_result)
+    self.assertLen(get_executions_by_new_context_result, 2)
+    get_executions_result = {
+        execution.id: execution
+        for execution in get_executions_by_new_context_result
+    }
+    self.assertIn(execution_ids[0], get_executions_result)
+    self.assertEqual(get_executions_result[execution_ids[0]].type_id,
+                     existing_execution.type_id)
+    self.assertEqual(get_executions_result[execution_ids[0]].name,
+                     existing_execution.name)
+    self.assertIn(execution_ids[1], get_executions_result)
+    self.assertEqual(get_executions_result[execution_ids[1]].type_id,
+                     new_execution.type_id)
+    self.assertEqual(get_executions_result[execution_ids[1]].name,
+                     new_execution.name)
+
+    get_events_result = store.get_events_by_execution_ids(execution_ids)
+    self.assertLen(get_events_result, 4)
+    get_events_result = {(event.execution_id, event.artifact_id): event
+                         for event in get_events_result}
+    input_event_for_existing_execution_key = (existing_execution.id,
+                                              input_artifact.id)
+    self.assertIn(input_event_for_existing_execution_key, get_events_result)
+    self.assertEqual(
+        get_events_result[input_event_for_existing_execution_key].type,
+        input_event_for_existing_execution.type)
+    output_event_for_existing_execution_key = (new_execution.id,
+                                               output_artifact.id)
+    self.assertIn(output_event_for_existing_execution_key, get_events_result)
+    self.assertEqual(
+        get_events_result[output_event_for_existing_execution_key].type,
+        output_event_for_existing_execution.type)
+    input_event_for_new_execution_key = (existing_execution.id,
+                                         input_artifact.id)
+    self.assertIn(input_event_for_new_execution_key, get_events_result)
+    self.assertEqual(get_events_result[input_event_for_new_execution_key].type,
+                     input_event_for_new_execution.type)
+    output_event_for_new_execution_key = (new_execution.id, output_artifact.id)
+    self.assertIn(output_event_for_new_execution_key, get_events_result)
+    self.assertEqual(get_events_result[output_event_for_new_execution_key].type,
+                     output_event_for_new_execution.type)
+
   def test_put_and_use_attributions_and_associations(self):
     store = _get_metadata_store()
     context_type = _create_example_context_type(self._get_test_type_name())
     context_type_id = store.put_context_type(context_type)
     want_context = metadata_store_pb2.Context()
     want_context.type_id = context_type_id
     want_context.name = self._get_test_type_name()
```

## Comparing `ml_metadata-1.8.0.dist-info/LICENSE` & `ml_metadata-1.9.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `ml_metadata-1.8.0.dist-info/METADATA` & `ml_metadata-1.9.0.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: ml-metadata
-Version: 1.8.0
+Version: 1.9.0
 Summary: A library for maintaining metadata for artifacts.
 Home-page: https://github.com/google/ml-metadata
 Download-URL: https://github.com/google/ml-metadata/tags
 Author: Google LLC
 Author-email: tensorflow-extended-dev@googlegroups.com
 License: Apache 2.0
 Keywords: machine learning metadata tfx
@@ -73,15 +73,15 @@
 ### Nightly Packages
 
 ML Metadata (MLMD) also hosts nightly packages at
 https://pypi-nightly.tensorflow.org on Google Cloud. To install the latest
 nightly package, please use the following command:
 
 ```bash
-pip install -i https://pypi-nightly.tensorflow.org/simple ml-metadata
+pip install --extra-index-url https://pypi-nightly.tensorflow.org/simple ml-metadata
 ```
 
 ## Installing with Docker
 
 This is the recommended way to build ML Metadata under Linux, and is
 continuously tested at Google.
```

## Comparing `ml_metadata-1.8.0.dist-info/RECORD` & `ml_metadata-1.9.0.dist-info/RECORD`

 * *Files 9% similar despite different names*

```diff
@@ -1,25 +1,25 @@
 ml_metadata/__init__.py,sha256=q7ymIxFXLtaEiLMu0jhhYDbQjux3yCSI77tNsl_85fk,1178
 ml_metadata/errors.py,sha256=YEPLBPbSOCS5zMAgv6y2A4e-49TVQbpqqtk3gI2yKXQ,6261
-ml_metadata/version.py,sha256=ZzcBFlSqY0IeTFhQjlAuUC4jgAeqtMxjJ4KdmJoUJ2Y,707
+ml_metadata/version.py,sha256=GmBIClZG945QcEauxk6KP1qFFGdH5u_MWbAHfMEV9vI,707
 ml_metadata/metadata_store/__init__.py,sha256=F_wx1q_ZdwEl0y5JaunuzKAzt2Gy297YmtVyyF48f00,901
-ml_metadata/metadata_store/metadata_store.py,sha256=pGc2UAR5qzdG3UGIhWgfXiwofdxhY66LrZ5Z6kX_mL4,54021
-ml_metadata/metadata_store/metadata_store_test.py,sha256=a7emoKiRkZT9OlXB5MDpNN0WmS8OcgHm2TVwHGRuMZk,84242
+ml_metadata/metadata_store/metadata_store.py,sha256=CRfpN3r026vBViaAQ0fowkjUtVmgf8xNK5gd6WXED2g,57338
+ml_metadata/metadata_store/metadata_store_test.py,sha256=O4ahKy2H1_g-l_OKyGY3tbsKZr7hCqXY0QS1iRaUMLU,92457
 ml_metadata/metadata_store/mlmd_types.py,sha256=E4EctyywoZ_nSEHaeg4ED0GIZ4Y1S57GDKC582LamVU,5859
 ml_metadata/metadata_store/mlmd_types_test.py,sha256=87C5HEaX-DkOo1vHkJjmhJe4AOvoOPQAdKPW5cTKndE,2824
 ml_metadata/metadata_store/types.py,sha256=mtKK0AXPSlTPWPBO_iaS6waDiMnJbolEUvI0EDm0dHA,57300
 ml_metadata/metadata_store/types_test.py,sha256=gEl_7yJvS2ojoymrVu8AsEzKl5x5EDbWKlVsXc3kS3c,21658
 ml_metadata/metadata_store/pywrap/__init__.py,sha256=b9zuvHT5wT_41JMXg1LnseTSkJUBbffTo0zu0aRpYok,589
-ml_metadata/metadata_store/pywrap/metadata_store_extension.pyd,sha256=Pc7ZagCCmwwkTe1rDTFwFJMIdhZhz397-3tLKzRdI6A,4041728
+ml_metadata/metadata_store/pywrap/metadata_store_extension.pyd,sha256=JZtDviiL0DH80-QW7fzZLuJurXH6FiQJYv2TayLqgDw,4058112
 ml_metadata/proto/__init__.py,sha256=vRFOeAcpgY1ES_zLPXJARUEOh83OkzzoXq6OHp7YCj0,4086
 ml_metadata/proto/metadata_store_pb2.py,sha256=inUy51Z2HtZQ7Jw6HXYpcje42Ss3T_gHhmGbdBm11HA,157216
 ml_metadata/proto/metadata_store_service_pb2.py,sha256=jn4LKozjaMWJ8CnQ3s5R0k4IIeEfYlZ7J_hyumWGLEk,233313
 ml_metadata/proto/metadata_store_service_pb2_grpc.py,sha256=jBkU758oITpdp93CiCSYHAVzyjWS2D7BAUeZGuJe23Y,90353
 ml_metadata/simple_types/__init__.py,sha256=b9zuvHT5wT_41JMXg1LnseTSkJUBbffTo0zu0aRpYok,589
 ml_metadata/simple_types/proto/__init__.py,sha256=b9zuvHT5wT_41JMXg1LnseTSkJUBbffTo0zu0aRpYok,589
 ml_metadata/simple_types/proto/simple_types_pb2.py,sha256=OXy2eVOfk2ah3NVu-slC6l7zY_F2IZzJNbEFBO-gIpg,3826
-ml_metadata-1.8.0.dist-info/LICENSE,sha256=WN-rvitqASjnV5PHXzT0lx8nEuC2OkZs16GdI5ZB_z4,11541
-ml_metadata-1.8.0.dist-info/METADATA,sha256=3SGxUCJgPUdLgeYmVIBnq80J5_W80nwnQCXNufFgOP0,4943
-ml_metadata-1.8.0.dist-info/WHEEL,sha256=fVcVlLzi8CGi_Ul8vjMdn8gER25dn5GBg9E6k9z41-Y,100
-ml_metadata-1.8.0.dist-info/namespace_packages.txt,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
-ml_metadata-1.8.0.dist-info/top_level.txt,sha256=qNb5tF2_mHF_d51VKxwVDUDWhOsYpwyCan96IGjksFg,12
-ml_metadata-1.8.0.dist-info/RECORD,,
+ml_metadata-1.9.0.dist-info/LICENSE,sha256=WN-rvitqASjnV5PHXzT0lx8nEuC2OkZs16GdI5ZB_z4,11541
+ml_metadata-1.9.0.dist-info/METADATA,sha256=jIwCbSMyDORLO1DuWWv1f_kXzEiDHMgOr3wTl9w_ZdQ,4958
+ml_metadata-1.9.0.dist-info/WHEEL,sha256=fVcVlLzi8CGi_Ul8vjMdn8gER25dn5GBg9E6k9z41-Y,100
+ml_metadata-1.9.0.dist-info/namespace_packages.txt,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
+ml_metadata-1.9.0.dist-info/top_level.txt,sha256=qNb5tF2_mHF_d51VKxwVDUDWhOsYpwyCan96IGjksFg,12
+ml_metadata-1.9.0.dist-info/RECORD,,
```

