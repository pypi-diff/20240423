# Comparing `tmp/river-0.8.0.tar.gz` & `tmp/river-0.9.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "river-0.8.0.tar", last modified: Mon Aug 30 12:12:45 2021, max compression
+gzip compressed data, was "river-0.9.0.tar", last modified: Tue Nov 30 18:46:04 2021, max compression
```

## Comparing `river-0.8.0.tar` & `river-0.9.0.tar`

### file list

```diff
@@ -1,435 +1,561 @@
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.574225 river-0.8.0/
--rw-r--r--   0 runner    (1001) docker     (121)     1528 2021-08-30 12:03:09.000000 river-0.8.0/LICENSE
--rw-r--r--   0 runner    (1001) docker     (121)      155 2021-08-30 12:03:09.000000 river-0.8.0/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (121)    12686 2021-08-30 12:12:45.578225 river-0.8.0/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (121)    10113 2021-08-30 12:03:09.000000 river-0.8.0/README.md
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.530222 river-0.8.0/river/
--rw-r--r--   0 runner    (1001) docker     (121)     1682 2021-08-30 12:03:09.000000 river-0.8.0/river/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)       77 2021-08-30 12:03:09.000000 river-0.8.0/river/__version__.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.534222 river-0.8.0/river/anomaly/
--rw-r--r--   0 runner    (1001) docker     (121)      447 2021-08-30 12:03:09.000000 river-0.8.0/river/anomaly/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     8328 2021-08-30 12:03:09.000000 river-0.8.0/river/anomaly/hst.py
--rw-r--r--   0 runner    (1001) docker     (121)      817 2021-08-30 12:03:09.000000 river-0.8.0/river/anomaly/test_hst.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.534222 river-0.8.0/river/base/
--rw-r--r--   0 runner    (1001) docker     (121)     1317 2021-08-30 12:03:09.000000 river-0.8.0/river/base/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)      887 2021-08-30 12:03:09.000000 river-0.8.0/river/base/anomaly.py
--rw-r--r--   0 runner    (1001) docker     (121)     9511 2021-08-30 12:03:09.000000 river-0.8.0/river/base/base.py
--rw-r--r--   0 runner    (1001) docker     (121)     3954 2021-08-30 12:03:09.000000 river-0.8.0/river/base/classifier.py
--rw-r--r--   0 runner    (1001) docker     (121)      917 2021-08-30 12:03:09.000000 river-0.8.0/river/base/clusterer.py
--rw-r--r--   0 runner    (1001) docker     (121)     1374 2021-08-30 12:03:09.000000 river-0.8.0/river/base/drift_detector.py
--rw-r--r--   0 runner    (1001) docker     (121)      349 2021-08-30 12:03:09.000000 river-0.8.0/river/base/ensemble.py
--rw-r--r--   0 runner    (1001) docker     (121)     2627 2021-08-30 12:03:09.000000 river-0.8.0/river/base/estimator.py
--rw-r--r--   0 runner    (1001) docker     (121)       60 2021-08-30 12:03:09.000000 river-0.8.0/river/base/multi_output.py
--rw-r--r--   0 runner    (1001) docker     (121)     1839 2021-08-30 12:03:09.000000 river-0.8.0/river/base/regressor.py
--rw-r--r--   0 runner    (1001) docker     (121)       60 2021-08-30 12:03:09.000000 river-0.8.0/river/base/tags.py
--rw-r--r--   0 runner    (1001) docker     (121)     1556 2021-08-30 12:03:09.000000 river-0.8.0/river/base/test_base.py
--rw-r--r--   0 runner    (1001) docker     (121)     2604 2021-08-30 12:03:09.000000 river-0.8.0/river/base/transformer.py
--rw-r--r--   0 runner    (1001) docker     (121)      229 2021-08-30 12:03:09.000000 river-0.8.0/river/base/typing.py
--rw-r--r--   0 runner    (1001) docker     (121)      649 2021-08-30 12:03:09.000000 river-0.8.0/river/base/wrapper.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.534222 river-0.8.0/river/cluster/
--rw-r--r--   0 runner    (1001) docker     (121)      271 2021-08-30 12:03:09.000000 river-0.8.0/river/cluster/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    14921 2021-08-30 12:03:09.000000 river-0.8.0/river/cluster/clustream.py
--rw-r--r--   0 runner    (1001) docker     (121)    16288 2021-08-30 12:03:09.000000 river-0.8.0/river/cluster/dbstream.py
--rw-r--r--   0 runner    (1001) docker     (121)    17191 2021-08-30 12:03:09.000000 river-0.8.0/river/cluster/denstream.py
--rw-r--r--   0 runner    (1001) docker     (121)     4113 2021-08-30 12:03:09.000000 river-0.8.0/river/cluster/k_means.py
--rw-r--r--   0 runner    (1001) docker     (121)     4215 2021-08-30 12:03:09.000000 river-0.8.0/river/cluster/streamkmeans.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.534222 river-0.8.0/river/compat/
--rw-r--r--   0 runner    (1001) docker     (121)     1182 2021-08-30 12:03:09.000000 river-0.8.0/river/compat/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     2644 2021-08-30 12:03:09.000000 river-0.8.0/river/compat/pytorch.py
--rw-r--r--   0 runner    (1001) docker     (121)    18648 2021-08-30 12:03:09.000000 river-0.8.0/river/compat/river_to_sklearn.py
--rw-r--r--   0 runner    (1001) docker     (121)     5108 2021-08-30 12:03:09.000000 river-0.8.0/river/compat/sklearn_to_river.py
--rw-r--r--   0 runner    (1001) docker     (121)     1125 2021-08-30 12:03:09.000000 river-0.8.0/river/compat/test_sklearn.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.538222 river-0.8.0/river/compose/
--rw-r--r--   0 runner    (1001) docker     (121)      586 2021-08-30 12:03:09.000000 river-0.8.0/river/compose/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     3424 2021-08-30 12:03:09.000000 river-0.8.0/river/compose/func.py
--rw-r--r--   0 runner    (1001) docker     (121)     1420 2021-08-30 12:03:09.000000 river-0.8.0/river/compose/grouper.py
--rw-r--r--   0 runner    (1001) docker     (121)    21964 2021-08-30 12:03:09.000000 river-0.8.0/river/compose/pipeline.py
--rw-r--r--   0 runner    (1001) docker     (121)      695 2021-08-30 12:03:09.000000 river-0.8.0/river/compose/rename.py
--rw-r--r--   0 runner    (1001) docker     (121)     4878 2021-08-30 12:03:09.000000 river-0.8.0/river/compose/select.py
--rw-r--r--   0 runner    (1001) docker     (121)     5078 2021-08-30 12:03:09.000000 river-0.8.0/river/compose/test_.py
--rw-r--r--   0 runner    (1001) docker     (121)     7400 2021-08-30 12:03:09.000000 river-0.8.0/river/compose/union.py
--rw-r--r--   0 runner    (1001) docker     (121)     2494 2021-08-30 12:03:09.000000 river-0.8.0/river/compose/viz.py
--rw-r--r--   0 runner    (1001) docker     (121)      651 2021-08-30 12:03:09.000000 river-0.8.0/river/conftest.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.538222 river-0.8.0/river/datasets/
--rw-r--r--   0 runner    (1001) docker     (121)     1398 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     2182 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/airline-passengers.csv
--rw-r--r--   0 runner    (1001) docker     (121)     1023 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/airline_passengers.py
--rw-r--r--   0 runner    (1001) docker     (121)    49917 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/banana.zip
--rw-r--r--   0 runner    (1001) docker     (121)      620 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/bananas.py
--rw-r--r--   0 runner    (1001) docker     (121)     8044 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/base.py
--rw-r--r--   0 runner    (1001) docker     (121)     1138 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/bikes.py
--rw-r--r--   0 runner    (1001) docker     (121)     6331 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/chick-weights.csv
--rw-r--r--   0 runner    (1001) docker     (121)      840 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/chick_weights.py
--rw-r--r--   0 runner    (1001) docker     (121)     3695 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/credit_card.py
--rw-r--r--   0 runner    (1001) docker     (121)     1595 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/elec2.py
--rw-r--r--   0 runner    (1001) docker     (121)     1918 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/higgs.py
--rw-r--r--   0 runner    (1001) docker     (121)      950 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/http.py
--rw-r--r--   0 runner    (1001) docker     (121)     2653 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/insects.py
--rw-r--r--   0 runner    (1001) docker     (121)     1459 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/malicious_url.py
--rw-r--r--   0 runner    (1001) docker     (121)     1288 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/movielens100k.py
--rw-r--r--   0 runner    (1001) docker     (121)     5398 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/music.py
--rw-r--r--   0 runner    (1001) docker     (121)     3991 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/phishing.csv.gz
--rw-r--r--   0 runner    (1001) docker     (121)     1090 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/phishing.py
--rw-r--r--   0 runner    (1001) docker     (121)     1170 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/restaurants.py
--rw-r--r--   0 runner    (1001) docker     (121)   122552 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/segment.csv.zip
--rw-r--r--   0 runner    (1001) docker     (121)     1460 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/segment.py
--rw-r--r--   0 runner    (1001) docker     (121)     1115 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/sms_spam.py
--rw-r--r--   0 runner    (1001) docker     (121)      973 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/smtp.py
--rw-r--r--   0 runner    (1001) docker     (121)     3117 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/solar-flare.csv.zip
--rw-r--r--   0 runner    (1001) docker     (121)     1173 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/solar_flare.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.542222 river-0.8.0/river/datasets/synth/
--rw-r--r--   0 runner    (1001) docker     (121)     1131 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/synth/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    12621 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/synth/agrawal.py
--rw-r--r--   0 runner    (1001) docker     (121)     4975 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/synth/anomaly_sine.py
--rw-r--r--   0 runner    (1001) docker     (121)     6154 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/synth/concept_drift_stream.py
--rw-r--r--   0 runner    (1001) docker     (121)    13326 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/synth/friedman.py
--rw-r--r--   0 runner    (1001) docker     (121)     4875 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/synth/hyper_plane.py
--rw-r--r--   0 runner    (1001) docker     (121)     8124 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/synth/led.py
--rw-r--r--   0 runner    (1001) docker     (121)     3086 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/synth/logical.py
--rw-r--r--   0 runner    (1001) docker     (121)     5632 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/synth/mixed.py
--rw-r--r--   0 runner    (1001) docker     (121)     3877 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/synth/mv.py
--rw-r--r--   0 runner    (1001) docker     (121)     2005 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/synth/planes_2d.py
--rw-r--r--   0 runner    (1001) docker     (121)    10104 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/synth/random_rbf.py
--rw-r--r--   0 runner    (1001) docker     (121)    10662 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/synth/random_tree.py
--rw-r--r--   0 runner    (1001) docker     (121)     2411 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/synth/sea.py
--rw-r--r--   0 runner    (1001) docker     (121)     6556 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/synth/sine.py
--rw-r--r--   0 runner    (1001) docker     (121)     5715 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/synth/stagger.py
--rw-r--r--   0 runner    (1001) docker     (121)     4841 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/synth/waveform.py
--rw-r--r--   0 runner    (1001) docker     (121)     1164 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/taxis.py
--rw-r--r--   0 runner    (1001) docker     (121)     3064 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/test_datasets.py
--rw-r--r--   0 runner    (1001) docker     (121)     1185 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/trec07.py
--rw-r--r--   0 runner    (1001) docker     (121)    10957 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/trump_approval.csv.gz
--rw-r--r--   0 runner    (1001) docker     (121)     1238 2021-08-30 12:03:09.000000 river-0.8.0/river/datasets/trump_approval.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.542222 river-0.8.0/river/drift/
--rw-r--r--   0 runner    (1001) docker     (121)      575 2021-08-30 12:03:09.000000 river-0.8.0/river/drift/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     3987 2021-08-30 12:03:09.000000 river-0.8.0/river/drift/adwin.py
--rw-r--r--   0 runner    (1001) docker     (121)   575731 2021-08-30 12:05:47.000000 river-0.8.0/river/drift/adwin_c.c
--rw-r--r--   0 runner    (1001) docker     (121)    12558 2021-08-30 12:03:09.000000 river-0.8.0/river/drift/adwin_c.pyx
--rw-r--r--   0 runner    (1001) docker     (121)     5658 2021-08-30 12:03:09.000000 river-0.8.0/river/drift/ddm.py
--rw-r--r--   0 runner    (1001) docker     (121)     5878 2021-08-30 12:03:09.000000 river-0.8.0/river/drift/eddm.py
--rw-r--r--   0 runner    (1001) docker     (121)     6902 2021-08-30 12:03:09.000000 river-0.8.0/river/drift/hddm_a.py
--rw-r--r--   0 runner    (1001) docker     (121)     9531 2021-08-30 12:03:09.000000 river-0.8.0/river/drift/hddm_w.py
--rw-r--r--   0 runner    (1001) docker     (121)     4868 2021-08-30 12:03:09.000000 river-0.8.0/river/drift/kswin.py
--rw-r--r--   0 runner    (1001) docker     (121)     3043 2021-08-30 12:03:09.000000 river-0.8.0/river/drift/page_hinkley.py
--rw-r--r--   0 runner    (1001) docker     (121)     3469 2021-08-30 12:03:09.000000 river-0.8.0/river/drift/test_drift_detectors.py
--rw-r--r--   0 runner    (1001) docker     (121)     4294 2021-08-30 12:03:09.000000 river-0.8.0/river/dummy.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.542222 river-0.8.0/river/ensemble/
--rw-r--r--   0 runner    (1001) docker     (121)      846 2021-08-30 12:03:09.000000 river-0.8.0/river/ensemble/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    40793 2021-08-30 12:03:09.000000 river-0.8.0/river/ensemble/adaptive_random_forest.py
--rw-r--r--   0 runner    (1001) docker     (121)    14528 2021-08-30 12:03:09.000000 river-0.8.0/river/ensemble/bagging.py
--rw-r--r--   0 runner    (1001) docker     (121)     4311 2021-08-30 12:03:09.000000 river-0.8.0/river/ensemble/boosting.py
--rw-r--r--   0 runner    (1001) docker     (121)    37611 2021-08-30 12:03:09.000000 river-0.8.0/river/ensemble/streaming_random_patches.py
--rw-r--r--   0 runner    (1001) docker     (121)     2545 2021-08-30 12:03:09.000000 river-0.8.0/river/ensemble/voting.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.546223 river-0.8.0/river/evaluate/
--rw-r--r--   0 runner    (1001) docker     (121)      856 2021-08-30 12:03:09.000000 river-0.8.0/river/evaluate/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     8907 2021-08-30 12:03:09.000000 river-0.8.0/river/evaluate/progressive_validation.py
--rw-r--r--   0 runner    (1001) docker     (121)     2292 2021-08-30 12:03:09.000000 river-0.8.0/river/evaluate/tracks.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.546223 river-0.8.0/river/expert/
--rw-r--r--   0 runner    (1001) docker     (121)     1363 2021-08-30 12:03:09.000000 river-0.8.0/river/expert/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    19723 2021-08-30 12:03:09.000000 river-0.8.0/river/expert/bandit.py
--rw-r--r--   0 runner    (1001) docker     (121)     4346 2021-08-30 12:03:09.000000 river-0.8.0/river/expert/ewa.py
--rw-r--r--   0 runner    (1001) docker     (121)      235 2021-08-30 12:03:09.000000 river-0.8.0/river/expert/exceptions.py
--rw-r--r--   0 runner    (1001) docker     (121)    15307 2021-08-30 12:03:09.000000 river-0.8.0/river/expert/sh.py
--rw-r--r--   0 runner    (1001) docker     (121)     2853 2021-08-30 12:03:09.000000 river-0.8.0/river/expert/stacking.py
--rw-r--r--   0 runner    (1001) docker     (121)      500 2021-08-30 12:03:09.000000 river-0.8.0/river/expert/test_sh.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.546223 river-0.8.0/river/facto/
--rw-r--r--   0 runner    (1001) docker     (121)      394 2021-08-30 12:03:09.000000 river-0.8.0/river/facto/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     4106 2021-08-30 12:03:09.000000 river-0.8.0/river/facto/base.py
--rw-r--r--   0 runner    (1001) docker     (121)    13864 2021-08-30 12:03:09.000000 river-0.8.0/river/facto/ffm.py
--rw-r--r--   0 runner    (1001) docker     (121)    12898 2021-08-30 12:03:09.000000 river-0.8.0/river/facto/fm.py
--rw-r--r--   0 runner    (1001) docker     (121)    14943 2021-08-30 12:03:09.000000 river-0.8.0/river/facto/fwfm.py
--rw-r--r--   0 runner    (1001) docker     (121)    14353 2021-08-30 12:03:09.000000 river-0.8.0/river/facto/hofm.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.546223 river-0.8.0/river/feature_extraction/
--rw-r--r--   0 runner    (1001) docker     (121)      634 2021-08-30 12:03:09.000000 river-0.8.0/river/feature_extraction/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     9635 2021-08-30 12:03:09.000000 river-0.8.0/river/feature_extraction/agg.py
--rw-r--r--   0 runner    (1001) docker     (121)     2462 2021-08-30 12:03:09.000000 river-0.8.0/river/feature_extraction/kernel_approx.py
--rw-r--r--   0 runner    (1001) docker     (121)     3690 2021-08-30 12:03:09.000000 river-0.8.0/river/feature_extraction/poly.py
--rw-r--r--   0 runner    (1001) docker     (121)    13981 2021-08-30 12:03:09.000000 river-0.8.0/river/feature_extraction/vectorize.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.546223 river-0.8.0/river/feature_selection/
--rw-r--r--   0 runner    (1001) docker     (121)      202 2021-08-30 12:03:09.000000 river-0.8.0/river/feature_selection/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     2193 2021-08-30 12:03:09.000000 river-0.8.0/river/feature_selection/k_best.py
--rw-r--r--   0 runner    (1001) docker     (121)     2102 2021-08-30 12:03:09.000000 river-0.8.0/river/feature_selection/random.py
--rw-r--r--   0 runner    (1001) docker     (121)     1531 2021-08-30 12:03:09.000000 river-0.8.0/river/feature_selection/variance.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.546223 river-0.8.0/river/imblearn/
--rw-r--r--   0 runner    (1001) docker     (121)      316 2021-08-30 12:03:09.000000 river-0.8.0/river/imblearn/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     7050 2021-08-30 12:03:09.000000 river-0.8.0/river/imblearn/hard_sampling.py
--rw-r--r--   0 runner    (1001) docker     (121)     7798 2021-08-30 12:03:09.000000 river-0.8.0/river/imblearn/random.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.546223 river-0.8.0/river/linear_model/
--rw-r--r--   0 runner    (1001) docker     (121)      370 2021-08-30 12:03:09.000000 river-0.8.0/river/linear_model/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     2087 2021-08-30 12:03:09.000000 river-0.8.0/river/linear_model/alma.py
--rw-r--r--   0 runner    (1001) docker     (121)    14063 2021-08-30 12:03:09.000000 river-0.8.0/river/linear_model/glm.py
--rw-r--r--   0 runner    (1001) docker     (121)     5436 2021-08-30 12:03:09.000000 river-0.8.0/river/linear_model/pa.py
--rw-r--r--   0 runner    (1001) docker     (121)     3461 2021-08-30 12:03:09.000000 river-0.8.0/river/linear_model/softmax.py
--rw-r--r--   0 runner    (1001) docker     (121)     7233 2021-08-30 12:03:09.000000 river-0.8.0/river/linear_model/test_glm.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.546223 river-0.8.0/river/meta/
--rw-r--r--   0 runner    (1001) docker     (121)      285 2021-08-30 12:03:09.000000 river-0.8.0/river/meta/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     1445 2021-08-30 12:03:09.000000 river-0.8.0/river/meta/pred_clipper.py
--rw-r--r--   0 runner    (1001) docker     (121)     4728 2021-08-30 12:03:09.000000 river-0.8.0/river/meta/target_transform.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.554223 river-0.8.0/river/metrics/
--rw-r--r--   0 runner    (1001) docker     (121)     4282 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    34504 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/_performance_evaluator.py
--rw-r--r--   0 runner    (1001) docker     (121)      929 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/accuracy.py
--rw-r--r--   0 runner    (1001) docker     (121)     1657 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/balanced_accuracy.py
--rw-r--r--   0 runner    (1001) docker     (121)    12360 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/base.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.554223 river-0.8.0/river/metrics/cluster/
--rw-r--r--   0 runner    (1001) docker     (121)     1133 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/cluster/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     2353 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/cluster/base.py
--rw-r--r--   0 runner    (1001) docker     (121)     4395 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/cluster/bic.py
--rw-r--r--   0 runner    (1001) docker     (121)     3364 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/cluster/daviesbouldin.py
--rw-r--r--   0 runner    (1001) docker     (121)     7624 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/cluster/generalized_dunn.py
--rw-r--r--   0 runner    (1001) docker     (121)     4313 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/cluster/i_index.py
--rw-r--r--   0 runner    (1001) docker     (121)     4439 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/cluster/ps.py
--rw-r--r--   0 runner    (1001) docker     (121)     3391 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/cluster/r2.py
--rw-r--r--   0 runner    (1001) docker     (121)     3558 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/cluster/rmsstd.py
--rw-r--r--   0 runner    (1001) docker     (121)     4665 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/cluster/sd_validation.py
--rw-r--r--   0 runner    (1001) docker     (121)     1553 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/cluster/separation.py
--rw-r--r--   0 runner    (1001) docker     (121)     3675 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/cluster/silhouette.py
--rw-r--r--   0 runner    (1001) docker     (121)     3325 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/cluster/ssb.py
--rw-r--r--   0 runner    (1001) docker     (121)     7311 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/cluster/ssq_based.py
--rw-r--r--   0 runner    (1001) docker     (121)     7099 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/cluster/ssw.py
--rw-r--r--   0 runner    (1001) docker     (121)     2974 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/cluster/xiebeni.py
--rw-r--r--   0 runner    (1001) docker     (121)   929212 2021-08-30 12:05:48.000000 river-0.8.0/river/metrics/confusion.c
--rw-r--r--   0 runner    (1001) docker     (121)     2651 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/confusion.pxd
--rw-r--r--   0 runner    (1001) docker     (121)    13400 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/confusion.pyx
--rw-r--r--   0 runner    (1001) docker     (121)     1045 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/cross_entropy.py
--rw-r--r--   0 runner    (1001) docker     (121)     1359 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/exact_match.py
--rw-r--r--   0 runner    (1001) docker     (121)   385287 2021-08-30 12:05:48.000000 river-0.8.0/river/metrics/expected_mutual_info.c
--rw-r--r--   0 runner    (1001) docker     (121)     5324 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/expected_mutual_info.pyx
--rw-r--r--   0 runner    (1001) docker     (121)    14769 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/fbeta.py
--rw-r--r--   0 runner    (1001) docker     (121)     2841 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/fowlkes_mallows.py
--rw-r--r--   0 runner    (1001) docker     (121)     1877 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/geometric_mean.py
--rw-r--r--   0 runner    (1001) docker     (121)     2584 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/hamming.py
--rw-r--r--   0 runner    (1001) docker     (121)     3778 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/jaccard.py
--rw-r--r--   0 runner    (1001) docker     (121)     5173 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/kappa.py
--rw-r--r--   0 runner    (1001) docker     (121)      932 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/log_loss.py
--rw-r--r--   0 runner    (1001) docker     (121)      518 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/mae.py
--rw-r--r--   0 runner    (1001) docker     (121)     3199 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/matthews_corrcoef.py
--rw-r--r--   0 runner    (1001) docker     (121)     1270 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/mcc.py
--rw-r--r--   0 runner    (1001) docker     (121)     1478 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/mse.py
--rw-r--r--   0 runner    (1001) docker     (121)     1036 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/multioutput.py
--rw-r--r--   0 runner    (1001) docker     (121)    12028 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/mutual_info.py
--rw-r--r--   0 runner    (1001) docker     (121)     3175 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/pair_confusion.py
--rw-r--r--   0 runner    (1001) docker     (121)     5884 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/precision.py
--rw-r--r--   0 runner    (1001) docker     (121)     2959 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/prevalence_threshold.py
--rw-r--r--   0 runner    (1001) docker     (121)     1917 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/purity.py
--rw-r--r--   0 runner    (1001) docker     (121)     5948 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/q0.py
--rw-r--r--   0 runner    (1001) docker     (121)     2348 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/r2.py
--rw-r--r--   0 runner    (1001) docker     (121)     5481 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/rand.py
--rw-r--r--   0 runner    (1001) docker     (121)     5410 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/recall.py
--rw-r--r--   0 runner    (1001) docker     (121)     4672 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/report.py
--rw-r--r--   0 runner    (1001) docker     (121)     3085 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/roc_auc.py
--rw-r--r--   0 runner    (1001) docker     (121)     2323 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/rolling.py
--rw-r--r--   0 runner    (1001) docker     (121)      764 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/smape.py
--rw-r--r--   0 runner    (1001) docker     (121)    15777 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/test_.py
--rw-r--r--   0 runner    (1001) docker     (121)     2156 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/time_rolling.py
--rw-r--r--   0 runner    (1001) docker     (121)     3690 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/variation_info.py
--rw-r--r--   0 runner    (1001) docker     (121)     8726 2021-08-30 12:03:09.000000 river-0.8.0/river/metrics/vbeta.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.554223 river-0.8.0/river/multiclass/
--rw-r--r--   0 runner    (1001) docker     (121)      226 2021-08-30 12:03:09.000000 river-0.8.0/river/multiclass/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     4590 2021-08-30 12:03:09.000000 river-0.8.0/river/multiclass/occ.py
--rw-r--r--   0 runner    (1001) docker     (121)     2784 2021-08-30 12:03:09.000000 river-0.8.0/river/multiclass/ovo.py
--rw-r--r--   0 runner    (1001) docker     (121)     4117 2021-08-30 12:03:09.000000 river-0.8.0/river/multiclass/ovr.py
--rw-r--r--   0 runner    (1001) docker     (121)     1221 2021-08-30 12:03:09.000000 river-0.8.0/river/multiclass/test_ovr.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.554223 river-0.8.0/river/multioutput/
--rw-r--r--   0 runner    (1001) docker     (121)      285 2021-08-30 12:03:09.000000 river-0.8.0/river/multioutput/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    13283 2021-08-30 12:03:09.000000 river-0.8.0/river/multioutput/chain.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.558224 river-0.8.0/river/naive_bayes/
--rw-r--r--   0 runner    (1001) docker     (121)      248 2021-08-30 12:03:09.000000 river-0.8.0/river/naive_bayes/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     2495 2021-08-30 12:03:09.000000 river-0.8.0/river/naive_bayes/base.py
--rw-r--r--   0 runner    (1001) docker     (121)    11219 2021-08-30 12:03:09.000000 river-0.8.0/river/naive_bayes/bernoulli.py
--rw-r--r--   0 runner    (1001) docker     (121)     8719 2021-08-30 12:03:09.000000 river-0.8.0/river/naive_bayes/complement.py
--rw-r--r--   0 runner    (1001) docker     (121)     2154 2021-08-30 12:03:09.000000 river-0.8.0/river/naive_bayes/gaussian.py
--rw-r--r--   0 runner    (1001) docker     (121)    10130 2021-08-30 12:03:09.000000 river-0.8.0/river/naive_bayes/multinomial.py
--rw-r--r--   0 runner    (1001) docker     (121)     1592 2021-08-30 12:03:09.000000 river-0.8.0/river/naive_bayes/test_multinomial.py
--rw-r--r--   0 runner    (1001) docker     (121)     5923 2021-08-30 12:03:09.000000 river-0.8.0/river/naive_bayes/test_naive_bayes.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.558224 river-0.8.0/river/neighbors/
--rw-r--r--   0 runner    (1001) docker     (121)      406 2021-08-30 12:03:09.000000 river-0.8.0/river/neighbors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     7193 2021-08-30 12:03:09.000000 river-0.8.0/river/neighbors/base_neighbors.py
--rw-r--r--   0 runner    (1001) docker     (121)     3537 2021-08-30 12:03:09.000000 river-0.8.0/river/neighbors/knn_adwin.py
--rw-r--r--   0 runner    (1001) docker     (121)     5239 2021-08-30 12:03:09.000000 river-0.8.0/river/neighbors/knn_classifier.py
--rw-r--r--   0 runner    (1001) docker     (121)     5403 2021-08-30 12:03:09.000000 river-0.8.0/river/neighbors/knn_regressor.py
--rw-r--r--   0 runner    (1001) docker     (121)    32699 2021-08-30 12:03:09.000000 river-0.8.0/river/neighbors/sam_knn.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.530222 river-0.8.0/river/neighbors/src/
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.558224 river-0.8.0/river/neighbors/src/libNearestNeighbor/
--rw-r--r--   0 runner    (1001) docker     (121)    11714 2021-08-30 12:03:09.000000 river-0.8.0/river/neighbors/src/libNearestNeighbor/nearestNeighbor.cpp
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.558224 river-0.8.0/river/neural_net/
--rw-r--r--   0 runner    (1001) docker     (121)      122 2021-08-30 12:03:09.000000 river-0.8.0/river/neural_net/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     1512 2021-08-30 12:03:09.000000 river-0.8.0/river/neural_net/activations.py
--rw-r--r--   0 runner    (1001) docker     (121)     9711 2021-08-30 12:03:09.000000 river-0.8.0/river/neural_net/mlp.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.558224 river-0.8.0/river/optim/
--rw-r--r--   0 runner    (1001) docker     (121)      804 2021-08-30 12:03:09.000000 river-0.8.0/river/optim/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     2770 2021-08-30 12:03:09.000000 river-0.8.0/river/optim/ada_bound.py
--rw-r--r--   0 runner    (1001) docker     (121)     1762 2021-08-30 12:03:09.000000 river-0.8.0/river/optim/ada_delta.py
--rw-r--r--   0 runner    (1001) docker     (121)     1415 2021-08-30 12:03:09.000000 river-0.8.0/river/optim/ada_grad.py
--rw-r--r--   0 runner    (1001) docker     (121)     1919 2021-08-30 12:03:09.000000 river-0.8.0/river/optim/ada_max.py
--rw-r--r--   0 runner    (1001) docker     (121)     1802 2021-08-30 12:03:09.000000 river-0.8.0/river/optim/adam.py
--rw-r--r--   0 runner    (1001) docker     (121)     2263 2021-08-30 12:03:09.000000 river-0.8.0/river/optim/ams_grad.py
--rw-r--r--   0 runner    (1001) docker     (121)     2590 2021-08-30 12:03:09.000000 river-0.8.0/river/optim/average.py
--rw-r--r--   0 runner    (1001) docker     (121)     1693 2021-08-30 12:03:09.000000 river-0.8.0/river/optim/base.py
--rw-r--r--   0 runner    (1001) docker     (121)     2287 2021-08-30 12:03:09.000000 river-0.8.0/river/optim/ftrl.py
--rw-r--r--   0 runner    (1001) docker     (121)     2428 2021-08-30 12:03:09.000000 river-0.8.0/river/optim/initializers.py
--rw-r--r--   0 runner    (1001) docker     (121)    14324 2021-08-30 12:03:09.000000 river-0.8.0/river/optim/losses.py
--rw-r--r--   0 runner    (1001) docker     (121)     1059 2021-08-30 12:03:09.000000 river-0.8.0/river/optim/momentum.py
--rw-r--r--   0 runner    (1001) docker     (121)     1911 2021-08-30 12:03:09.000000 river-0.8.0/river/optim/nadam.py
--rw-r--r--   0 runner    (1001) docker     (121)     1311 2021-08-30 12:03:09.000000 river-0.8.0/river/optim/nesterov.py
--rw-r--r--   0 runner    (1001) docker     (121)     1187 2021-08-30 12:03:09.000000 river-0.8.0/river/optim/newton.py
--rw-r--r--   0 runner    (1001) docker     (121)     1328 2021-08-30 12:03:09.000000 river-0.8.0/river/optim/rms_prop.py
--rw-r--r--   0 runner    (1001) docker     (121)     2143 2021-08-30 12:03:09.000000 river-0.8.0/river/optim/schedulers.py
--rw-r--r--   0 runner    (1001) docker     (121)     1436 2021-08-30 12:03:09.000000 river-0.8.0/river/optim/sgd.py
--rw-r--r--   0 runner    (1001) docker     (121)      732 2021-08-30 12:03:09.000000 river-0.8.0/river/optim/test_losses.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.562224 river-0.8.0/river/preprocessing/
--rw-r--r--   0 runner    (1001) docker     (121)      932 2021-08-30 12:03:09.000000 river-0.8.0/river/preprocessing/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     1893 2021-08-30 12:03:09.000000 river-0.8.0/river/preprocessing/feature_hasher.py
--rw-r--r--   0 runner    (1001) docker     (121)     7333 2021-08-30 12:03:09.000000 river-0.8.0/river/preprocessing/impute.py
--rw-r--r--   0 runner    (1001) docker     (121)    14023 2021-08-30 12:03:09.000000 river-0.8.0/river/preprocessing/lda.py
--rw-r--r--   0 runner    (1001) docker     (121)     3155 2021-08-30 12:03:09.000000 river-0.8.0/river/preprocessing/one_hot.py
--rw-r--r--   0 runner    (1001) docker     (121)    15250 2021-08-30 12:03:09.000000 river-0.8.0/river/preprocessing/scale.py
--rw-r--r--   0 runner    (1001) docker     (121)     7877 2021-08-30 12:03:09.000000 river-0.8.0/river/preprocessing/test_lda.py
--rw-r--r--   0 runner    (1001) docker     (121)     1907 2021-08-30 12:03:09.000000 river-0.8.0/river/preprocessing/test_scale.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.562224 river-0.8.0/river/proba/
--rw-r--r--   0 runner    (1001) docker     (121)      140 2021-08-30 12:03:09.000000 river-0.8.0/river/proba/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)      922 2021-08-30 12:03:09.000000 river-0.8.0/river/proba/base.py
--rw-r--r--   0 runner    (1001) docker     (121)     1410 2021-08-30 12:03:09.000000 river-0.8.0/river/proba/gaussian.py
--rw-r--r--   0 runner    (1001) docker     (121)     1337 2021-08-30 12:03:09.000000 river-0.8.0/river/proba/multinomial.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.562224 river-0.8.0/river/reco/
--rw-r--r--   0 runner    (1001) docker     (121)      326 2021-08-30 12:03:09.000000 river-0.8.0/river/reco/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     1048 2021-08-30 12:03:09.000000 river-0.8.0/river/reco/base.py
--rw-r--r--   0 runner    (1001) docker     (121)     4502 2021-08-30 12:03:09.000000 river-0.8.0/river/reco/baseline.py
--rw-r--r--   0 runner    (1001) docker     (121)     8007 2021-08-30 12:03:09.000000 river-0.8.0/river/reco/biased_mf.py
--rw-r--r--   0 runner    (1001) docker     (121)     5043 2021-08-30 12:03:09.000000 river-0.8.0/river/reco/funk_mf.py
--rw-r--r--   0 runner    (1001) docker     (121)     2069 2021-08-30 12:03:09.000000 river-0.8.0/river/reco/normal.py
--rw-r--r--   0 runner    (1001) docker     (121)      656 2021-08-30 12:03:09.000000 river-0.8.0/river/reco/surprise.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.562224 river-0.8.0/river/rules/
--rw-r--r--   0 runner    (1001) docker     (121)       92 2021-08-30 12:03:09.000000 river-0.8.0/river/rules/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    18694 2021-08-30 12:03:09.000000 river-0.8.0/river/rules/amrules.py
--rw-r--r--   0 runner    (1001) docker     (121)     9105 2021-08-30 12:03:09.000000 river-0.8.0/river/rules/base.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.566224 river-0.8.0/river/stats/
--rw-r--r--   0 runner    (1001) docker     (121)     1471 2021-08-30 12:03:09.000000 river-0.8.0/river/stats/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     1571 2021-08-30 12:03:09.000000 river-0.8.0/river/stats/auto_corr.py
--rw-r--r--   0 runner    (1001) docker     (121)     1486 2021-08-30 12:03:09.000000 river-0.8.0/river/stats/base.py
--rw-r--r--   0 runner    (1001) docker     (121)      329 2021-08-30 12:03:09.000000 river-0.8.0/river/stats/count.py
--rw-r--r--   0 runner    (1001) docker     (121)     5618 2021-08-30 12:03:09.000000 river-0.8.0/river/stats/cov.py
--rw-r--r--   0 runner    (1001) docker     (121)     2469 2021-08-30 12:03:09.000000 river-0.8.0/river/stats/entropy.py
--rw-r--r--   0 runner    (1001) docker     (121)     1305 2021-08-30 12:03:09.000000 river-0.8.0/river/stats/ewmean.py
--rw-r--r--   0 runner    (1001) docker     (121)     1603 2021-08-30 12:03:09.000000 river-0.8.0/river/stats/ewvar.py
--rw-r--r--   0 runner    (1001) docker     (121)     2931 2021-08-30 12:03:09.000000 river-0.8.0/river/stats/iqr.py
--rw-r--r--   0 runner    (1001) docker     (121)     2434 2021-08-30 12:03:09.000000 river-0.8.0/river/stats/kurtosis.py
--rw-r--r--   0 runner    (1001) docker     (121)     2320 2021-08-30 12:03:09.000000 river-0.8.0/river/stats/link.py
--rw-r--r--   0 runner    (1001) docker     (121)     2637 2021-08-30 12:03:09.000000 river-0.8.0/river/stats/maximum.py
--rw-r--r--   0 runner    (1001) docker     (121)     4220 2021-08-30 12:03:09.000000 river-0.8.0/river/stats/mean.py
--rw-r--r--   0 runner    (1001) docker     (121)     1079 2021-08-30 12:03:09.000000 river-0.8.0/river/stats/minimum.py
--rw-r--r--   0 runner    (1001) docker     (121)     2841 2021-08-30 12:03:09.000000 river-0.8.0/river/stats/mode.py
--rw-r--r--   0 runner    (1001) docker     (121)     1793 2021-08-30 12:03:09.000000 river-0.8.0/river/stats/moments.py
--rw-r--r--   0 runner    (1001) docker     (121)     3115 2021-08-30 12:03:09.000000 river-0.8.0/river/stats/n_unique.py
--rw-r--r--   0 runner    (1001) docker     (121)     2755 2021-08-30 12:03:09.000000 river-0.8.0/river/stats/pearson.py
--rw-r--r--   0 runner    (1001) docker     (121)     1812 2021-08-30 12:03:09.000000 river-0.8.0/river/stats/ptp.py
--rw-r--r--   0 runner    (1001) docker     (121)     6824 2021-08-30 12:03:09.000000 river-0.8.0/river/stats/quantile.py
--rw-r--r--   0 runner    (1001) docker     (121)     1662 2021-08-30 12:03:09.000000 river-0.8.0/river/stats/sem.py
--rw-r--r--   0 runner    (1001) docker     (121)     3380 2021-08-30 12:03:09.000000 river-0.8.0/river/stats/shift.py
--rw-r--r--   0 runner    (1001) docker     (121)     2399 2021-08-30 12:03:09.000000 river-0.8.0/river/stats/skew.py
--rw-r--r--   0 runner    (1001) docker     (121)     1445 2021-08-30 12:03:09.000000 river-0.8.0/river/stats/summing.py
--rw-r--r--   0 runner    (1001) docker     (121)     4819 2021-08-30 12:03:09.000000 river-0.8.0/river/stats/test_.py
--rw-r--r--   0 runner    (1001) docker     (121)     3425 2021-08-30 12:03:09.000000 river-0.8.0/river/stats/test_mean_var_cov.py
--rw-r--r--   0 runner    (1001) docker     (121)     4859 2021-08-30 12:03:09.000000 river-0.8.0/river/stats/var.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.566224 river-0.8.0/river/stream/
--rw-r--r--   0 runner    (1001) docker     (121)      874 2021-08-30 12:03:09.000000 river-0.8.0/river/stream/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     4604 2021-08-30 12:03:09.000000 river-0.8.0/river/stream/cache.py
--rw-r--r--   0 runner    (1001) docker     (121)     1627 2021-08-30 12:03:09.000000 river-0.8.0/river/stream/iter_arff.py
--rw-r--r--   0 runner    (1001) docker     (121)     2323 2021-08-30 12:03:09.000000 river-0.8.0/river/stream/iter_array.py
--rw-r--r--   0 runner    (1001) docker     (121)     6321 2021-08-30 12:03:09.000000 river-0.8.0/river/stream/iter_csv.py
--rw-r--r--   0 runner    (1001) docker     (121)     2067 2021-08-30 12:03:09.000000 river-0.8.0/river/stream/iter_libsvm.py
--rw-r--r--   0 runner    (1001) docker     (121)     1217 2021-08-30 12:03:09.000000 river-0.8.0/river/stream/iter_pandas.py
--rw-r--r--   0 runner    (1001) docker     (121)     1611 2021-08-30 12:03:09.000000 river-0.8.0/river/stream/iter_sklearn.py
--rw-r--r--   0 runner    (1001) docker     (121)     3214 2021-08-30 12:03:09.000000 river-0.8.0/river/stream/iter_sql.py
--rw-r--r--   0 runner    (1001) docker     (121)     1443 2021-08-30 12:03:09.000000 river-0.8.0/river/stream/iter_vaex.py
--rw-r--r--   0 runner    (1001) docker     (121)     4693 2021-08-30 12:03:09.000000 river-0.8.0/river/stream/pokedb.zip
--rw-r--r--   0 runner    (1001) docker     (121)     6419 2021-08-30 12:03:09.000000 river-0.8.0/river/stream/qa.py
--rw-r--r--   0 runner    (1001) docker     (121)     2248 2021-08-30 12:03:09.000000 river-0.8.0/river/stream/shuffling.py
--rw-r--r--   0 runner    (1001) docker     (121)     1163 2021-08-30 12:03:09.000000 river-0.8.0/river/stream/test_iter_csv.py
--rw-r--r--   0 runner    (1001) docker     (121)     3294 2021-08-30 12:03:09.000000 river-0.8.0/river/stream/test_sql.py
--rw-r--r--   0 runner    (1001) docker     (121)      825 2021-08-30 12:03:09.000000 river-0.8.0/river/stream/utils.py
--rw-r--r--   0 runner    (1001) docker     (121)     4247 2021-08-30 12:03:09.000000 river-0.8.0/river/test_.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.566224 river-0.8.0/river/time_series/
--rw-r--r--   0 runner    (1001) docker     (121)      166 2021-08-30 12:03:09.000000 river-0.8.0/river/time_series/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)      900 2021-08-30 12:03:09.000000 river-0.8.0/river/time_series/base.py
--rw-r--r--   0 runner    (1001) docker     (121)     2278 2021-08-30 12:03:09.000000 river-0.8.0/river/time_series/detrender.py
--rw-r--r--   0 runner    (1001) docker     (121)        0 2021-08-30 12:03:09.000000 river-0.8.0/river/time_series/expo_smoothing.py
--rw-r--r--   0 runner    (1001) docker     (121)    12048 2021-08-30 12:03:09.000000 river-0.8.0/river/time_series/snarimax.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.570224 river-0.8.0/river/tree/
--rwxr-xr-x   0 runner    (1001) docker     (121)     3190 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     5625 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/base.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    22210 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/extremely_fast_decision_tree.py
--rw-r--r--   0 runner    (1001) docker     (121)     9857 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/hoeffding_adaptive_tree_classifier.py
--rw-r--r--   0 runner    (1001) docker     (121)    12280 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/hoeffding_adaptive_tree_regressor.py
--rw-r--r--   0 runner    (1001) docker     (121)    17781 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/hoeffding_tree.py
--rwxr-xr-x   0 runner    (1001) docker     (121)    16496 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/hoeffding_tree_classifier.py
--rw-r--r--   0 runner    (1001) docker     (121)    16542 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/hoeffding_tree_regressor.py
--rw-r--r--   0 runner    (1001) docker     (121)    10429 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/isoup_tree_regressor.py
--rw-r--r--   0 runner    (1001) docker     (121)     6981 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/label_combination_hoeffding_tree.py
--rw-r--r--   0 runner    (1001) docker     (121)     1357 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/losses.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.570224 river-0.8.0/river/tree/nodes/
--rw-r--r--   0 runner    (1001) docker     (121)      112 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/nodes/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     4758 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/nodes/arf_htc_nodes.py
--rw-r--r--   0 runner    (1001) docker     (121)     3673 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/nodes/arf_htr_nodes.py
--rw-r--r--   0 runner    (1001) docker     (121)     5905 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/nodes/branch.py
--rw-r--r--   0 runner    (1001) docker     (121)     9800 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/nodes/efdtc_nodes.py
--rw-r--r--   0 runner    (1001) docker     (121)    13997 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/nodes/hatc_nodes.py
--rw-r--r--   0 runner    (1001) docker     (121)    13805 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/nodes/hatr_nodes.py
--rw-r--r--   0 runner    (1001) docker     (121)     6312 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/nodes/htc_nodes.py
--rw-r--r--   0 runner    (1001) docker     (121)     6477 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/nodes/htr_nodes.py
--rw-r--r--   0 runner    (1001) docker     (121)     6731 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/nodes/isouptr_nodes.py
--rw-r--r--   0 runner    (1001) docker     (121)     5238 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/nodes/leaf.py
--rw-r--r--   0 runner    (1001) docker     (121)     8571 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/nodes/sgt_nodes.py
--rw-r--r--   0 runner    (1001) docker     (121)      471 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.570224 river-0.8.0/river/tree/split_criterion/
--rw-r--r--   0 runner    (1001) docker     (121)      681 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/split_criterion/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     1194 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/split_criterion/base.py
--rw-r--r--   0 runner    (1001) docker     (121)     1006 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/split_criterion/gini_split_criterion.py
--rw-r--r--   0 runner    (1001) docker     (121)     2623 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/split_criterion/hellinger_distance_criterion.py
--rw-r--r--   0 runner    (1001) docker     (121)     2792 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/split_criterion/info_gain_split_criterion.py
--rw-r--r--   0 runner    (1001) docker     (121)     1157 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/split_criterion/intra_cluster_variance_reduction_split_criterion.py
--rw-r--r--   0 runner    (1001) docker     (121)     1566 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/split_criterion/variance_ratio_split_criterion.py
--rw-r--r--   0 runner    (1001) docker     (121)     1565 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/split_criterion/variance_reduction_split_criterion.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.574225 river-0.8.0/river/tree/splitter/
--rw-r--r--   0 runner    (1001) docker     (121)     1533 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/splitter/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     3408 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/splitter/base.py
--rw-r--r--   0 runner    (1001) docker     (121)    11107 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/splitter/ebst_splitter.py
--rw-r--r--   0 runner    (1001) docker     (121)     6998 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/splitter/exhaustive_splitter.py
--rw-r--r--   0 runner    (1001) docker     (121)     3862 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/splitter/gaussian_splitter.py
--rw-r--r--   0 runner    (1001) docker     (121)     3450 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/splitter/histogram_splitter.py
--rw-r--r--   0 runner    (1001) docker     (121)     3841 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/splitter/nominal_splitter_classif.py
--rw-r--r--   0 runner    (1001) docker     (121)     3363 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/splitter/nominal_splitter_reg.py
--rw-r--r--   0 runner    (1001) docker     (121)     8064 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/splitter/qo_splitter.py
--rw-r--r--   0 runner    (1001) docker     (121)     5830 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/splitter/sgt_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (121)     1022 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/splitter/tebst_splitter.py
--rw-r--r--   0 runner    (1001) docker     (121)    15414 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/stochastic_gradient_tree.py
--rw-r--r--   0 runner    (1001) docker     (121)     2786 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/test_base.py
--rw-r--r--   0 runner    (1001) docker     (121)     1794 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/test_splitter.py
--rw-r--r--   0 runner    (1001) docker     (121)     4576 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/test_trees.py
--rw-r--r--   0 runner    (1001) docker     (121)     6992 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/utils.py
--rw-r--r--   0 runner    (1001) docker     (121)     1987 2021-08-30 12:03:09.000000 river-0.8.0/river/tree/viz.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.574225 river-0.8.0/river/utils/
--rw-r--r--   0 runner    (1001) docker     (121)      654 2021-08-30 12:03:09.000000 river-0.8.0/river/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     1469 2021-08-30 12:03:09.000000 river-0.8.0/river/utils/data_conversion.py
--rw-r--r--   0 runner    (1001) docker     (121)     9970 2021-08-30 12:03:09.000000 river-0.8.0/river/utils/estimator_checks.py
--rw-r--r--   0 runner    (1001) docker     (121)     8203 2021-08-30 12:03:09.000000 river-0.8.0/river/utils/histogram.py
--rw-r--r--   0 runner    (1001) docker     (121)     1672 2021-08-30 12:03:09.000000 river-0.8.0/river/utils/inspect.py
--rw-r--r--   0 runner    (1001) docker     (121)     6064 2021-08-30 12:03:09.000000 river-0.8.0/river/utils/math.py
--rw-r--r--   0 runner    (1001) docker     (121)     4756 2021-08-30 12:03:09.000000 river-0.8.0/river/utils/param_grid.py
--rw-r--r--   0 runner    (1001) docker     (121)     2041 2021-08-30 12:03:09.000000 river-0.8.0/river/utils/pretty.py
--rw-r--r--   0 runner    (1001) docker     (121)     2096 2021-08-30 12:03:09.000000 river-0.8.0/river/utils/sdft.py
--rw-r--r--   0 runner    (1001) docker     (121)     8889 2021-08-30 12:03:09.000000 river-0.8.0/river/utils/skmultiflow_utils.py
--rw-r--r--   0 runner    (1001) docker     (121)     6232 2021-08-30 12:03:09.000000 river-0.8.0/river/utils/skyline.py
--rw-r--r--   0 runner    (1001) docker     (121)     2078 2021-08-30 12:03:09.000000 river-0.8.0/river/utils/test_param_grid.py
--rw-r--r--   0 runner    (1001) docker     (121)     3724 2021-08-30 12:03:09.000000 river-0.8.0/river/utils/test_vectordict.py
--rw-r--r--   0 runner    (1001) docker     (121)  1003950 2021-08-30 12:03:20.000000 river-0.8.0/river/utils/vectordict.c
--rw-r--r--   0 runner    (1001) docker     (121)    18763 2021-08-30 12:03:09.000000 river-0.8.0/river/utils/vectordict.pyx
--rw-r--r--   0 runner    (1001) docker     (121)     2393 2021-08-30 12:03:09.000000 river-0.8.0/river/utils/window.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2021-08-30 12:12:45.534222 river-0.8.0/river.egg-info/
--rw-r--r--   0 runner    (1001) docker     (121)    12686 2021-08-30 12:12:45.000000 river-0.8.0/river.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (121)    10992 2021-08-30 12:12:45.000000 river-0.8.0/river.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (121)        1 2021-08-30 12:12:45.000000 river-0.8.0/river.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (121)      671 2021-08-30 12:12:45.000000 river-0.8.0/river.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (121)        6 2021-08-30 12:12:45.000000 river-0.8.0/river.egg-info/top_level.txt
--rw-r--r--   0 runner    (1001) docker     (121)      117 2021-08-30 12:12:45.578225 river-0.8.0/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (121)     3964 2021-08-30 12:03:09.000000 river-0.8.0/setup.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.722368 river-0.9.0/
+-rw-rw-rw-   0        0        0       70 2021-11-30 18:29:09.000000 river-0.9.0/.coveragerc
+-rw-rw-rw-   0        0        0       82 2021-11-30 18:29:09.000000 river-0.9.0/.gitattributes
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.164971 river-0.9.0/.github/
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.167952 river-0.9.0/.github/ISSUE_TEMPLATE/
+-rw-rw-rw-   0        0        0     1274 2021-11-30 18:29:09.000000 river-0.9.0/.github/ISSUE_TEMPLATE/bug_report.md
+-rw-rw-rw-   0        0        0      288 2021-11-30 18:29:09.000000 river-0.9.0/.github/ISSUE_TEMPLATE/discussion.md
+-rw-rw-rw-   0        0        0      999 2021-11-30 18:29:09.000000 river-0.9.0/.github/ISSUE_TEMPLATE/performance_issue.md
+-rw-rw-rw-   0        0        0      319 2021-11-30 18:29:09.000000 river-0.9.0/.github/pull_request_template.md
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.179986 river-0.9.0/.github/workflows/
+-rw-rw-rw-   0        0        0      968 2021-11-30 18:29:09.000000 river-0.9.0/.github/workflows/branch-docs.yml
+-rw-rw-rw-   0        0        0     1276 2021-11-30 18:29:09.000000 river-0.9.0/.github/workflows/branch-tests.yml
+-rw-rw-rw-   0        0        0      894 2021-11-30 18:29:09.000000 river-0.9.0/.github/workflows/code-quality.yml
+-rw-rw-rw-   0        0        0     1432 2021-11-30 18:29:09.000000 river-0.9.0/.github/workflows/dev-docs.yml
+-rw-rw-rw-   0        0        0     1042 2021-11-30 18:29:09.000000 river-0.9.0/.github/workflows/pypi.yml
+-rw-rw-rw-   0        0        0     1433 2021-11-30 18:29:09.000000 river-0.9.0/.github/workflows/release-docs.yml
+-rw-rw-rw-   0        0        0     1410 2021-11-30 18:29:09.000000 river-0.9.0/.github/workflows/unit-tests.yml
+-rw-rw-rw-   0        0        0     1385 2021-11-30 18:29:09.000000 river-0.9.0/.gitignore
+-rw-rw-rw-   0        0        0      234 2021-11-30 18:29:09.000000 river-0.9.0/.isort.cfg
+-rw-rw-rw-   0        0        0      597 2021-11-30 18:29:09.000000 river-0.9.0/.pre-commit-config.yaml
+-rw-rw-rw-   0        0        0     3430 2021-11-30 18:29:09.000000 river-0.9.0/CODE_OF_CONDUCT.md
+-rw-rw-rw-   0        0        0     6018 2021-11-30 18:29:09.000000 river-0.9.0/CONTRIBUTING.md
+-rw-rw-rw-   0        0        0     1557 2021-11-30 18:29:09.000000 river-0.9.0/LICENSE
+-rw-rw-rw-   0        0        0      161 2021-11-30 18:29:09.000000 river-0.9.0/MANIFEST.in
+-rw-rw-rw-   0        0        0      700 2021-11-30 18:29:09.000000 river-0.9.0/Makefile
+-rw-rw-rw-   0        0        0    12934 2021-11-30 18:46:04.724366 river-0.9.0/PKG-INFO
+-rw-rw-rw-   0        0        0    10314 2021-11-30 18:29:09.000000 river-0.9.0/README.md
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.194952 river-0.9.0/benchmarks/
+-rw-rw-rw-   0        0        0   443761 2021-11-30 18:29:09.000000 river-0.9.0/benchmarks/Batch versus online.ipynb
+-rw-rw-rw-   0        0        0    24636 2021-11-30 18:29:09.000000 river-0.9.0/benchmarks/Factorization machines.ipynb
+-rw-rw-rw-   0        0        0      412 2021-11-30 18:29:09.000000 river-0.9.0/benchmarks/README.md
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.196950 river-0.9.0/benchmarks/logistic_regression/
+-rw-rw-rw-   0        0        0     9078 2021-11-30 18:29:09.000000 river-0.9.0/benchmarks/logistic_regression/README.md
+-rw-rw-rw-   0        0        0     5281 2021-11-30 18:29:09.000000 river-0.9.0/benchmarks/logistic_regression/run.py
+-rw-rw-rw-   0        0        0      108 2021-11-30 18:29:09.000000 river-0.9.0/benchmarks/requirements.txt
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.199988 river-0.9.0/docs/
+-rw-rw-rw-   0        0        0      128 2021-11-30 18:29:09.000000 river-0.9.0/docs/.pages
+-rw-rw-rw-   0        0        0       13 2021-11-30 18:29:09.000000 river-0.9.0/docs/CNAME
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.199988 river-0.9.0/docs/css/
+-rw-rw-rw-   0        0        0      106 2021-11-30 18:29:09.000000 river-0.9.0/docs/css/version-select.css
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.257985 river-0.9.0/docs/examples/
+-rw-rw-rw-   0        0        0    22412 2021-11-30 18:29:09.000000 river-0.9.0/docs/examples/batch-to-online.ipynb
+-rw-rw-rw-   0        0        0    34301 2021-11-30 18:29:09.000000 river-0.9.0/docs/examples/bike-sharing-forecasting.ipynb
+-rw-rw-rw-   0        0        0   327097 2021-11-30 18:29:09.000000 river-0.9.0/docs/examples/building-a-simple-nowcasting-model.ipynb
+-rw-rw-rw-   0        0        0    50163 2021-11-30 18:29:09.000000 river-0.9.0/docs/examples/concept-drift-detection.ipynb
+-rw-rw-rw-   0        0        0    25625 2021-11-30 18:29:09.000000 river-0.9.0/docs/examples/debugging-a-pipeline.ipynb
+-rw-rw-rw-   0        0        0    22716 2021-11-30 18:29:09.000000 river-0.9.0/docs/examples/imbalanced-learning.ipynb
+-rw-rw-rw-   0        0        0    19526 2021-11-30 18:29:09.000000 river-0.9.0/docs/examples/matrix-factorization-for-recommender-systems-part-1.ipynb
+-rw-rw-rw-   0        0        0    23048 2021-11-30 18:29:09.000000 river-0.9.0/docs/examples/matrix-factorization-for-recommender-systems-part-2.ipynb
+-rw-rw-rw-   0        0        0     1445 2021-11-30 18:29:09.000000 river-0.9.0/docs/examples/matrix-factorization-for-recommender-systems-part-3.ipynb
+-rw-rw-rw-   0        0        0    75327 2021-11-30 18:29:09.000000 river-0.9.0/docs/examples/quantile-regression-uncertainty.ipynb
+-rw-rw-rw-   0        0        0  1304291 2021-11-30 18:29:09.000000 river-0.9.0/docs/examples/sentence_classification.ipynb
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.267962 river-0.9.0/docs/examples/sentence_classification_files/
+-rw-rw-rw-   0        0        0     3276 2021-11-30 18:29:09.000000 river-0.9.0/docs/examples/sentence_classification_files/sentence_classification_14_0.svg
+-rw-rw-rw-   0        0        0     3890 2021-11-30 18:29:09.000000 river-0.9.0/docs/examples/sentence_classification_files/sentence_classification_19_0.svg
+-rw-rw-rw-   0        0        0     3923 2021-11-30 18:29:09.000000 river-0.9.0/docs/examples/sentence_classification_files/sentence_classification_31_0.svg
+-rw-rw-rw-   0        0        0   246099 2021-11-30 18:29:09.000000 river-0.9.0/docs/examples/the-art-of-using-pipelines.ipynb
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.268986 river-0.9.0/docs/faq/
+-rw-rw-rw-   0        0        0     3830 2021-11-30 18:29:09.000000 river-0.9.0/docs/faq/faq.md
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.272952 river-0.9.0/docs/getting-started/
+-rw-rw-rw-   0        0        0       68 2021-11-30 18:29:09.000000 river-0.9.0/docs/getting-started/.pages
+-rw-rw-rw-   0        0        0    12971 2021-11-30 18:29:09.000000 river-0.9.0/docs/getting-started/getting-started.ipynb
+-rw-rw-rw-   0        0        0      516 2021-11-30 18:29:09.000000 river-0.9.0/docs/getting-started/installation.md
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.295951 river-0.9.0/docs/img/
+-rw-rw-rw-   0        0        0     4740 2021-11-30 18:29:09.000000 river-0.9.0/docs/img/dtree_draw.svg
+-rw-rw-rw-   0        0        0   119177 2021-11-30 18:29:09.000000 river-0.9.0/docs/img/favicon.ico
+-rw-rw-rw-   0        0        0    25206 2021-11-30 18:29:09.000000 river-0.9.0/docs/img/histogram_docstring.svg
+-rw-rw-rw-   0        0        0   311139 2021-11-30 18:29:09.000000 river-0.9.0/docs/img/illustration.png
+-rw-rw-rw-   0        0        0    68673 2021-11-30 18:29:09.000000 river-0.9.0/docs/img/logo.svg
+-rw-rw-rw-   0        0        0     7402 2021-11-30 18:29:09.000000 river-0.9.0/docs/img/pipeline_docstring.svg
+-rw-rw-rw-   0        0        0     6542 2021-11-30 18:29:09.000000 river-0.9.0/docs/img/river_square.svg
+-rw-rw-rw-   0        0        0    30446 2021-11-30 18:29:09.000000 river-0.9.0/docs/img/skyline_docstring.svg
+-rw-rw-rw-   0        0        0       45 2021-11-30 18:29:09.000000 river-0.9.0/docs/index.md
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.297985 river-0.9.0/docs/javascripts/
+-rw-rw-rw-   0        0        0      253 2021-11-30 18:29:09.000000 river-0.9.0/docs/javascripts/config.js
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.298985 river-0.9.0/docs/js/
+-rw-rw-rw-   0        0        0     1694 2021-11-30 18:29:09.000000 river-0.9.0/docs/js/version-select.js
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.299638 river-0.9.0/docs/overrides/
+-rw-rw-rw-   0        0        0    10964 2021-11-30 18:29:09.000000 river-0.9.0/docs/overrides/home.html
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.300659 river-0.9.0/docs/overrides/partials/
+-rw-rw-rw-   0        0        0      816 2021-11-30 18:29:09.000000 river-0.9.0/docs/overrides/partials/footer.html
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.321651 river-0.9.0/docs/releases/
+-rw-rw-rw-   0        0        0       13 2021-11-30 18:29:09.000000 river-0.9.0/docs/releases/.pages
+-rw-rw-rw-   0        0        0      859 2021-11-30 18:29:09.000000 river-0.9.0/docs/releases/0.0.2.md
+-rw-rw-rw-   0        0        0     2381 2021-11-30 18:29:09.000000 river-0.9.0/docs/releases/0.0.3.md
+-rw-rw-rw-   0        0        0     2430 2021-11-30 18:29:09.000000 river-0.9.0/docs/releases/0.1.0.md
+-rw-rw-rw-   0        0        0     2028 2021-11-30 18:29:09.000000 river-0.9.0/docs/releases/0.2.0.md
+-rw-rw-rw-   0        0        0     1133 2021-11-30 18:29:09.000000 river-0.9.0/docs/releases/0.3.0.md
+-rw-rw-rw-   0        0        0     2359 2021-11-30 18:29:09.000000 river-0.9.0/docs/releases/0.4.1.md
+-rw-rw-rw-   0        0        0      439 2021-11-30 18:29:09.000000 river-0.9.0/docs/releases/0.4.3.md
+-rw-rw-rw-   0        0        0     1198 2021-11-30 18:29:09.000000 river-0.9.0/docs/releases/0.4.4.md
+-rw-rw-rw-   0        0        0     3979 2021-11-30 18:29:09.000000 river-0.9.0/docs/releases/0.5.0.md
+-rw-rw-rw-   0        0        0      952 2021-11-30 18:29:09.000000 river-0.9.0/docs/releases/0.5.1.md
+-rw-rw-rw-   0        0        0     2761 2021-11-30 18:29:09.000000 river-0.9.0/docs/releases/0.6.0.md
+-rw-rw-rw-   0        0        0      158 2021-11-30 18:29:09.000000 river-0.9.0/docs/releases/0.6.1.md
+-rw-rw-rw-   0        0        0       62 2021-11-30 18:29:09.000000 river-0.9.0/docs/releases/0.7.0.md
+-rw-rw-rw-   0        0        0      195 2021-11-30 18:29:09.000000 river-0.9.0/docs/releases/0.7.1.md
+-rw-rw-rw-   0        0        0      124 2021-11-30 18:29:09.000000 river-0.9.0/docs/releases/0.7.2.md
+-rw-rw-rw-   0        0        0      769 2021-11-30 18:29:09.000000 river-0.9.0/docs/releases/0.8.0.md
+-rw-rw-rw-   0        0        0     3525 2021-11-30 18:29:09.000000 river-0.9.0/docs/releases/0.9.0.md
+-rw-rw-rw-   0        0        0       14 2021-11-30 18:29:09.000000 river-0.9.0/docs/releases/unreleased.md
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.322652 river-0.9.0/docs/stylesheets/
+-rw-rw-rw-   0        0        0      587 2021-11-30 18:29:09.000000 river-0.9.0/docs/stylesheets/extra.css
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.355534 river-0.9.0/docs/user-guide/
+-rw-rw-rw-   0        0        0      209 2021-11-30 18:29:09.000000 river-0.9.0/docs/user-guide/.pages
+-rw-rw-rw-   0        0        0      604 2021-11-30 18:29:09.000000 river-0.9.0/docs/user-guide/feature-extraction.ipynb
+-rw-rw-rw-   0        0        0      607 2021-11-30 18:29:09.000000 river-0.9.0/docs/user-guide/hyperparameter-tuning.ipynb
+-rw-rw-rw-   0        0        0     9292 2021-11-30 18:29:09.000000 river-0.9.0/docs/user-guide/mini-batching.ipynb
+-rw-rw-rw-   0        0        0      602 2021-11-30 18:29:09.000000 river-0.9.0/docs/user-guide/model-evaluation.ipynb
+-rw-rw-rw-   0        0        0  1013907 2021-11-30 18:29:09.000000 river-0.9.0/docs/user-guide/on-hoeffding-trees.ipynb
+-rw-rw-rw-   0        0        0    24953 2021-11-30 18:29:09.000000 river-0.9.0/docs/user-guide/pipelines.ipynb
+-rw-rw-rw-   0        0        0    11847 2021-11-30 18:29:09.000000 river-0.9.0/docs/user-guide/reading-data.ipynb
+-rw-rw-rw-   0        0        0     1384 2021-11-30 18:29:09.000000 river-0.9.0/mkdocs.yml
+-rw-rw-rw-   0        0        0      609 2021-11-30 18:29:09.000000 river-0.9.0/mypy.ini
+-rw-rw-rw-   0        0        0      496 2021-11-30 18:29:09.000000 river-0.9.0/pytest.ini
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.359533 river-0.9.0/river/
+-rw-rw-rw-   0        0        0     1758 2021-11-30 18:29:09.000000 river-0.9.0/river/__init__.py
+-rw-rw-rw-   0        0        0       80 2021-11-30 18:29:09.000000 river-0.9.0/river/__version__.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.369534 river-0.9.0/river/anomaly/
+-rw-rw-rw-   0        0        0      774 2021-11-30 18:29:09.000000 river-0.9.0/river/anomaly/__init__.py
+-rw-rw-rw-   0        0        0      925 2021-11-30 18:29:09.000000 river-0.9.0/river/anomaly/base.py
+-rw-rw-rw-   0        0        0     8629 2021-11-30 18:29:09.000000 river-0.9.0/river/anomaly/hst.py
+-rw-rw-rw-   0        0        0     2932 2021-11-30 18:29:09.000000 river-0.9.0/river/anomaly/svm.py
+-rw-rw-rw-   0        0        0      847 2021-11-30 18:29:09.000000 river-0.9.0/river/anomaly/test_hst.py
+-rw-rw-rw-   0        0        0     1032 2021-11-30 18:29:09.000000 river-0.9.0/river/anomaly/test_svm.py
+-rw-rw-rw-   0        0        0     4336 2021-11-30 18:29:09.000000 river-0.9.0/river/anomaly/threshold.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.381533 river-0.9.0/river/base/
+-rw-rw-rw-   0        0        0     1320 2021-11-30 18:29:09.000000 river-0.9.0/river/base/__init__.py
+-rw-rw-rw-   0        0        0    12293 2021-11-30 18:29:09.000000 river-0.9.0/river/base/base.py
+-rw-rw-rw-   0        0        0     4100 2021-11-30 18:29:09.000000 river-0.9.0/river/base/classifier.py
+-rw-rw-rw-   0        0        0      962 2021-11-30 18:29:09.000000 river-0.9.0/river/base/clusterer.py
+-rw-rw-rw-   0        0        0     1396 2021-11-30 18:29:09.000000 river-0.9.0/river/base/drift_detector.py
+-rw-rw-rw-   0        0        0     1458 2021-11-30 18:29:09.000000 river-0.9.0/river/base/ensemble.py
+-rw-rw-rw-   0        0        0     2710 2021-11-30 18:29:09.000000 river-0.9.0/river/base/estimator.py
+-rw-rw-rw-   0        0        0       62 2021-11-30 18:29:09.000000 river-0.9.0/river/base/multi_output.py
+-rw-rw-rw-   0        0        0     1924 2021-11-30 18:29:09.000000 river-0.9.0/river/base/regressor.py
+-rw-rw-rw-   0        0        0       62 2021-11-30 18:29:09.000000 river-0.9.0/river/base/tags.py
+-rw-rw-rw-   0        0        0     1608 2021-11-30 18:29:09.000000 river-0.9.0/river/base/test_base.py
+-rw-rw-rw-   0        0        0     2726 2021-11-30 18:29:09.000000 river-0.9.0/river/base/transformer.py
+-rw-rw-rw-   0        0        0      296 2021-11-30 18:29:09.000000 river-0.9.0/river/base/typing.py
+-rw-rw-rw-   0        0        0      711 2021-11-30 18:29:09.000000 river-0.9.0/river/base/wrapper.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.396534 river-0.9.0/river/cluster/
+-rw-rw-rw-   0        0        0      279 2021-11-30 18:29:09.000000 river-0.9.0/river/cluster/__init__.py
+-rw-rw-rw-   0        0        0    15488 2021-11-30 18:29:09.000000 river-0.9.0/river/cluster/clustream.py
+-rw-rw-rw-   0        0        0    16707 2021-11-30 18:29:09.000000 river-0.9.0/river/cluster/dbstream.py
+-rw-rw-rw-   0        0        0    17829 2021-11-30 18:29:09.000000 river-0.9.0/river/cluster/denstream.py
+-rw-rw-rw-   0        0        0     4322 2021-11-30 18:29:09.000000 river-0.9.0/river/cluster/k_means.py
+-rw-rw-rw-   0        0        0     4337 2021-11-30 18:29:09.000000 river-0.9.0/river/cluster/streamkmeans.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.402371 river-0.9.0/river/compat/
+-rw-rw-rw-   0        0        0     1279 2021-11-30 18:29:09.000000 river-0.9.0/river/compat/__init__.py
+-rw-rw-rw-   0        0        0    10722 2021-11-30 18:29:09.000000 river-0.9.0/river/compat/pytorch.py
+-rw-rw-rw-   0        0        0    19277 2021-11-30 18:29:09.000000 river-0.9.0/river/compat/river_to_sklearn.py
+-rw-rw-rw-   0        0        0     5292 2021-11-30 18:29:09.000000 river-0.9.0/river/compat/sklearn_to_river.py
+-rw-rw-rw-   0        0        0     1156 2021-11-30 18:29:09.000000 river-0.9.0/river/compat/test_sklearn.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.411367 river-0.9.0/river/compose/
+-rw-rw-rw-   0        0        0      766 2021-11-30 18:29:09.000000 river-0.9.0/river/compose/__init__.py
+-rw-rw-rw-   0        0        0     3524 2021-11-30 18:29:09.000000 river-0.9.0/river/compose/func.py
+-rw-rw-rw-   0        0        0     1470 2021-11-30 18:29:09.000000 river-0.9.0/river/compose/grouper.py
+-rw-rw-rw-   0        0        0    26059 2021-11-30 18:29:09.000000 river-0.9.0/river/compose/pipeline.py
+-rw-rw-rw-   0        0        0     2376 2021-11-30 18:29:09.000000 river-0.9.0/river/compose/product.py
+-rw-rw-rw-   0        0        0      728 2021-11-30 18:29:09.000000 river-0.9.0/river/compose/rename.py
+-rw-rw-rw-   0        0        0     4802 2021-11-30 18:29:09.000000 river-0.9.0/river/compose/select.py
+-rw-rw-rw-   0        0        0     1724 2021-11-30 18:29:09.000000 river-0.9.0/river/compose/target_transform.py
+-rw-rw-rw-   0        0        0     3246 2021-11-30 18:29:09.000000 river-0.9.0/river/compose/test_.py
+-rw-rw-rw-   0        0        0     1441 2021-11-30 18:29:09.000000 river-0.9.0/river/compose/test_product.py
+-rw-rw-rw-   0        0        0     7635 2021-11-30 18:29:09.000000 river-0.9.0/river/compose/union.py
+-rw-rw-rw-   0        0        0     3549 2021-11-30 18:29:09.000000 river-0.9.0/river/compose/viz.py
+-rw-rw-rw-   0        0        0      679 2021-11-30 18:29:09.000000 river-0.9.0/river/conftest.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.440367 river-0.9.0/river/datasets/
+-rw-rw-rw-   0        0        0     1503 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/__init__.py
+-rw-rw-rw-   0        0        0     2182 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/airline-passengers.csv
+-rw-rw-rw-   0        0        0     1056 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/airline_passengers.py
+-rw-rw-rw-   0        0        0    49917 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/banana.zip
+-rw-rw-rw-   0        0        0      644 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/bananas.py
+-rw-rw-rw-   0        0        0     8314 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/base.py
+-rw-rw-rw-   0        0        0     1181 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/bikes.py
+-rw-rw-rw-   0        0        0     6910 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/chick-weights.csv
+-rw-rw-rw-   0        0        0      869 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/chick_weights.py
+-rw-rw-rw-   0        0        0     3750 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/credit_card.py
+-rw-rw-rw-   0        0        0     1646 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/elec2.py
+-rw-rw-rw-   0        0        0     1992 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/higgs.py
+-rw-rw-rw-   0        0        0      987 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/http.py
+-rw-rw-rw-   0        0        0     2764 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/insects.py
+-rw-rw-rw-   0        0        0     2049 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/keystroke.py
+-rw-rw-rw-   0        0        0     1508 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/malicious_url.py
+-rw-rw-rw-   0        0        0     1332 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/movielens100k.py
+-rw-rw-rw-   0        0        0     5518 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/music.py
+-rw-rw-rw-   0        0        0     3991 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/phishing.csv.gz
+-rw-rw-rw-   0        0        0     1132 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/phishing.py
+-rw-rw-rw-   0        0        0     1213 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/restaurants.py
+-rw-rw-rw-   0        0        0   122552 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/segment.csv.zip
+-rw-rw-rw-   0        0        0     1511 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/segment.py
+-rw-rw-rw-   0        0        0     1146 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/sms_spam.py
+-rw-rw-rw-   0        0        0     1013 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/smtp.py
+-rw-rw-rw-   0        0        0     3117 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/solar-flare.csv.zip
+-rw-rw-rw-   0        0        0     1217 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/solar_flare.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.454365 river-0.9.0/river/datasets/synth/
+-rw-rw-rw-   0        0        0     1176 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/synth/__init__.py
+-rw-rw-rw-   0        0        0    12960 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/synth/agrawal.py
+-rw-rw-rw-   0        0        0     5121 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/synth/anomaly_sine.py
+-rw-rw-rw-   0        0        0     6338 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/synth/concept_drift_stream.py
+-rw-rw-rw-   0        0        0    13685 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/synth/friedman.py
+-rw-rw-rw-   0        0        0     5017 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/synth/hyper_plane.py
+-rw-rw-rw-   0        0        0     8349 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/synth/led.py
+-rw-rw-rw-   0        0        0     3190 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/synth/logical.py
+-rw-rw-rw-   0        0        0     5784 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/synth/mixed.py
+-rw-rw-rw-   0        0        0     4010 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/synth/mv.py
+-rw-rw-rw-   0        0        0     2078 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/synth/planes_2d.py
+-rw-rw-rw-   0        0        0    10399 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/synth/random_rbf.py
+-rw-rw-rw-   0        0        0    10940 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/synth/random_tree.py
+-rw-rw-rw-   0        0        0     2492 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/synth/sea.py
+-rw-rw-rw-   0        0        0     6735 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/synth/sine.py
+-rw-rw-rw-   0        0        0     5873 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/synth/stagger.py
+-rw-rw-rw-   0        0        0     4958 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/synth/waveform.py
+-rw-rw-rw-   0        0        0     1207 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/taxis.py
+-rw-rw-rw-   0        0        0     3193 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/test_datasets.py
+-rw-rw-rw-   0        0        0     1226 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/trec07.py
+-rw-rw-rw-   0        0        0    10957 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/trump_approval.csv.gz
+-rw-rw-rw-   0        0        0     1280 2021-11-30 18:29:09.000000 river-0.9.0/river/datasets/trump_approval.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.464367 river-0.9.0/river/drift/
+-rw-rw-rw-   0        0        0      593 2021-11-30 18:29:09.000000 river-0.9.0/river/drift/__init__.py
+-rw-rw-rw-   0        0        0     4109 2021-11-30 18:29:09.000000 river-0.9.0/river/drift/adwin.py
+-rw-rw-rw-   0        0        0   572790 2021-11-30 18:45:44.000000 river-0.9.0/river/drift/adwin_c.c
+-rw-rw-rw-   0        0        0    12919 2021-11-30 18:29:09.000000 river-0.9.0/river/drift/adwin_c.pyx
+-rw-rw-rw-   0        0        0     5824 2021-11-30 18:29:09.000000 river-0.9.0/river/drift/ddm.py
+-rw-rw-rw-   0        0        0     6000 2021-11-30 18:29:09.000000 river-0.9.0/river/drift/eddm.py
+-rw-rw-rw-   0        0        0     7099 2021-11-30 18:29:09.000000 river-0.9.0/river/drift/hddm_a.py
+-rw-rw-rw-   0        0        0     9772 2021-11-30 18:29:09.000000 river-0.9.0/river/drift/hddm_w.py
+-rw-rw-rw-   0        0        0     5005 2021-11-30 18:29:09.000000 river-0.9.0/river/drift/kswin.py
+-rw-rw-rw-   0        0        0     3127 2021-11-30 18:29:09.000000 river-0.9.0/river/drift/page_hinkley.py
+-rw-rw-rw-   0        0        0     3600 2021-11-30 18:29:09.000000 river-0.9.0/river/drift/test_drift_detectors.py
+-rw-rw-rw-   0        0        0     4473 2021-11-30 18:29:09.000000 river-0.9.0/river/dummy.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.470367 river-0.9.0/river/ensemble/
+-rw-rw-rw-   0        0        0     1117 2021-11-30 18:29:09.000000 river-0.9.0/river/ensemble/__init__.py
+-rw-rw-rw-   0        0        0    41884 2021-11-30 18:29:09.000000 river-0.9.0/river/ensemble/adaptive_random_forest.py
+-rw-rw-rw-   0        0        0    14600 2021-11-30 18:29:09.000000 river-0.9.0/river/ensemble/bagging.py
+-rw-rw-rw-   0        0        0     4186 2021-11-30 18:29:09.000000 river-0.9.0/river/ensemble/boosting.py
+-rw-rw-rw-   0        0        0     4214 2021-11-30 18:29:09.000000 river-0.9.0/river/ensemble/ewa.py
+-rw-rw-rw-   0        0        0     3126 2021-11-30 18:29:09.000000 river-0.9.0/river/ensemble/stacking.py
+-rw-rw-rw-   0        0        0    38681 2021-11-30 18:29:09.000000 river-0.9.0/river/ensemble/streaming_random_patches.py
+-rw-rw-rw-   0        0        0     2570 2021-11-30 18:29:09.000000 river-0.9.0/river/ensemble/voting.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.473367 river-0.9.0/river/evaluate/
+-rw-rw-rw-   0        0        0      874 2021-11-30 18:29:09.000000 river-0.9.0/river/evaluate/__init__.py
+-rw-rw-rw-   0        0        0     9150 2021-11-30 18:29:09.000000 river-0.9.0/river/evaluate/progressive_validation.py
+-rw-rw-rw-   0        0        0     2369 2021-11-30 18:29:09.000000 river-0.9.0/river/evaluate/tracks.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.477367 river-0.9.0/river/facto/
+-rw-rw-rw-   0        0        0      410 2021-11-30 18:29:09.000000 river-0.9.0/river/facto/__init__.py
+-rw-rw-rw-   0        0        0     4241 2021-11-30 18:29:09.000000 river-0.9.0/river/facto/base.py
+-rw-rw-rw-   0        0        0    14245 2021-11-30 18:29:09.000000 river-0.9.0/river/facto/ffm.py
+-rw-rw-rw-   0        0        0    15833 2021-11-30 18:29:09.000000 river-0.9.0/river/facto/fm.py
+-rw-rw-rw-   0        0        0    15351 2021-11-30 18:29:09.000000 river-0.9.0/river/facto/fwfm.py
+-rw-rw-rw-   0        0        0    14756 2021-11-30 18:29:09.000000 river-0.9.0/river/facto/hofm.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.483368 river-0.9.0/river/feature_extraction/
+-rw-rw-rw-   0        0        0      730 2021-11-30 18:29:09.000000 river-0.9.0/river/feature_extraction/__init__.py
+-rw-rw-rw-   0        0        0     9624 2021-11-30 18:29:10.000000 river-0.9.0/river/feature_extraction/agg.py
+-rw-rw-rw-   0        0        0     2550 2021-11-30 18:29:10.000000 river-0.9.0/river/feature_extraction/kernel_approx.py
+-rw-rw-rw-   0        0        0     9324 2021-11-30 18:29:10.000000 river-0.9.0/river/feature_extraction/lag.py
+-rw-rw-rw-   0        0        0     3809 2021-11-30 18:29:10.000000 river-0.9.0/river/feature_extraction/poly.py
+-rw-rw-rw-   0        0        0    14407 2021-11-30 18:29:10.000000 river-0.9.0/river/feature_extraction/vectorize.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.486366 river-0.9.0/river/feature_selection/
+-rw-rw-rw-   0        0        0      208 2021-11-30 18:29:10.000000 river-0.9.0/river/feature_selection/__init__.py
+-rw-rw-rw-   0        0        0     2284 2021-11-30 18:29:10.000000 river-0.9.0/river/feature_selection/k_best.py
+-rw-rw-rw-   0        0        0     2172 2021-11-30 18:29:10.000000 river-0.9.0/river/feature_selection/random.py
+-rw-rw-rw-   0        0        0     1594 2021-11-30 18:29:10.000000 river-0.9.0/river/feature_selection/variance.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.488366 river-0.9.0/river/imblearn/
+-rw-rw-rw-   0        0        0      327 2021-11-30 18:29:10.000000 river-0.9.0/river/imblearn/__init__.py
+-rw-rw-rw-   0        0        0     7282 2021-11-30 18:29:10.000000 river-0.9.0/river/imblearn/hard_sampling.py
+-rw-rw-rw-   0        0        0     8055 2021-11-30 18:29:10.000000 river-0.9.0/river/imblearn/random.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.495366 river-0.9.0/river/linear_model/
+-rw-rw-rw-   0        0        0      434 2021-11-30 18:29:10.000000 river-0.9.0/river/linear_model/__init__.py
+-rw-rw-rw-   0        0        0     2176 2021-11-30 18:29:10.000000 river-0.9.0/river/linear_model/alma.py
+-rw-rw-rw-   0        0        0     4308 2021-11-30 18:29:10.000000 river-0.9.0/river/linear_model/glm.py
+-rw-rw-rw-   0        0        0     5618 2021-11-30 18:29:10.000000 river-0.9.0/river/linear_model/lin_reg.py
+-rw-rw-rw-   0        0        0     3224 2021-11-30 18:29:10.000000 river-0.9.0/river/linear_model/log_reg.py
+-rw-rw-rw-   0        0        0     5623 2021-11-30 18:29:10.000000 river-0.9.0/river/linear_model/pa.py
+-rw-rw-rw-   0        0        0     1655 2021-11-30 18:29:10.000000 river-0.9.0/river/linear_model/perceptron.py
+-rw-rw-rw-   0        0        0     3569 2021-11-30 18:29:10.000000 river-0.9.0/river/linear_model/softmax.py
+-rw-rw-rw-   0        0        0    10676 2021-11-30 18:29:10.000000 river-0.9.0/river/linear_model/test_glm.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.532367 river-0.9.0/river/metrics/
+-rw-rw-rw-   0        0        0     4444 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/__init__.py
+-rw-rw-rw-   0        0        0    35610 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/_performance_evaluator.py
+-rw-rw-rw-   0        0        0      968 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/accuracy.py
+-rw-rw-rw-   0        0        0     1717 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/balanced_accuracy.py
+-rw-rw-rw-   0        0        0    12990 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/base.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.546367 river-0.9.0/river/metrics/cluster/
+-rw-rw-rw-   0        0        0     1181 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/cluster/__init__.py
+-rw-rw-rw-   0        0        0     2438 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/cluster/base.py
+-rw-rw-rw-   0        0        0     4549 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/cluster/bic.py
+-rw-rw-rw-   0        0        0     3471 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/cluster/daviesbouldin.py
+-rw-rw-rw-   0        0        0     7885 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/cluster/generalized_dunn.py
+-rw-rw-rw-   0        0        0     4452 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/cluster/i_index.py
+-rw-rw-rw-   0        0        0     4598 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/cluster/ps.py
+-rw-rw-rw-   0        0        0     3503 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/cluster/r2.py
+-rw-rw-rw-   0        0        0     3698 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/cluster/rmsstd.py
+-rw-rw-rw-   0        0        0     4819 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/cluster/sd_validation.py
+-rw-rw-rw-   0        0        0     1616 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/cluster/separation.py
+-rw-rw-rw-   0        0        0     3790 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/cluster/silhouette.py
+-rw-rw-rw-   0        0        0     3446 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/cluster/ssb.py
+-rw-rw-rw-   0        0        0     7582 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/cluster/ssq_based.py
+-rw-rw-rw-   0        0        0     7384 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/cluster/ssw.py
+-rw-rw-rw-   0        0        0     3077 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/cluster/xiebeni.py
+-rw-rw-rw-   0        0        0   926503 2021-11-30 18:45:44.000000 river-0.9.0/river/metrics/confusion.c
+-rw-rw-rw-   0        0        0     2698 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/confusion.pxd
+-rw-rw-rw-   0        0        0    13806 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/confusion.pyx
+-rw-rw-rw-   0        0        0     1093 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/cross_entropy.py
+-rw-rw-rw-   0        0        0     1417 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/exact_match.py
+-rw-rw-rw-   0        0        0   382346 2021-11-30 18:45:45.000000 river-0.9.0/river/metrics/expected_mutual_info.c
+-rw-rw-rw-   0        0        0     5451 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/expected_mutual_info.pyx
+-rw-rw-rw-   0        0        0    15357 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/fbeta.py
+-rw-rw-rw-   0        0        0     2939 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/fowlkes_mallows.py
+-rw-rw-rw-   0        0        0     1939 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/geometric_mean.py
+-rw-rw-rw-   0        0        0     2699 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/hamming.py
+-rw-rw-rw-   0        0        0     3906 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/jaccard.py
+-rw-rw-rw-   0        0        0     5356 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/kappa.py
+-rw-rw-rw-   0        0        0      978 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/log_loss.py
+-rw-rw-rw-   0        0        0      550 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/mae.py
+-rw-rw-rw-   0        0        0     3302 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/matthews_corrcoef.py
+-rw-rw-rw-   0        0        0     1324 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/mcc.py
+-rw-rw-rw-   0        0        0     1561 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/mse.py
+-rw-rw-rw-   0        0        0     1073 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/multioutput.py
+-rw-rw-rw-   0        0        0    12396 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/mutual_info.py
+-rw-rw-rw-   0        0        0     3245 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/pair_confusion.py
+-rw-rw-rw-   0        0        0     6122 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/precision.py
+-rw-rw-rw-   0        0        0     3054 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/prevalence_threshold.py
+-rw-rw-rw-   0        0        0     1987 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/purity.py
+-rw-rw-rw-   0        0        0     6145 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/q0.py
+-rw-rw-rw-   0        0        0     2429 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/r2.py
+-rw-rw-rw-   0        0        0     5656 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/rand.py
+-rw-rw-rw-   0        0        0     5628 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/recall.py
+-rw-rw-rw-   0        0        0     4811 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/report.py
+-rw-rw-rw-   0        0        0     3182 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/roc_auc.py
+-rw-rw-rw-   0        0        0     2396 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/rolling.py
+-rw-rw-rw-   0        0        0      797 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/smape.py
+-rw-rw-rw-   0        0        0    16294 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/test_.py
+-rw-rw-rw-   0        0        0     2238 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/time_rolling.py
+-rw-rw-rw-   0        0        0     3800 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/variation_info.py
+-rw-rw-rw-   0        0        0     9015 2021-11-30 18:29:10.000000 river-0.9.0/river/metrics/vbeta.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.553366 river-0.9.0/river/model_selection/
+-rw-rw-rw-   0        0        0     1247 2021-11-30 18:29:10.000000 river-0.9.0/river/model_selection/__init__.py
+-rw-rw-rw-   0        0        0     3459 2021-11-30 18:29:10.000000 river-0.9.0/river/model_selection/bandit.py
+-rw-rw-rw-   0        0        0     2051 2021-11-30 18:29:10.000000 river-0.9.0/river/model_selection/base.py
+-rw-rw-rw-   0        0        0     4310 2021-11-30 18:29:10.000000 river-0.9.0/river/model_selection/epsilon_greedy.py
+-rw-rw-rw-   0        0        0     2303 2021-11-30 18:29:10.000000 river-0.9.0/river/model_selection/greedy.py
+-rw-rw-rw-   0        0        0    15371 2021-11-30 18:29:10.000000 river-0.9.0/river/model_selection/sh.py
+-rw-rw-rw-   0        0        0      832 2021-11-30 18:29:10.000000 river-0.9.0/river/model_selection/test_bandit.py
+-rw-rw-rw-   0        0        0     4148 2021-11-30 18:29:10.000000 river-0.9.0/river/model_selection/ucb.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.558367 river-0.9.0/river/multiclass/
+-rw-rw-rw-   0        0        0      232 2021-11-30 18:29:10.000000 river-0.9.0/river/multiclass/__init__.py
+-rw-rw-rw-   0        0        0     4719 2021-11-30 18:29:10.000000 river-0.9.0/river/multiclass/occ.py
+-rw-rw-rw-   0        0        0     2875 2021-11-30 18:29:10.000000 river-0.9.0/river/multiclass/ovo.py
+-rw-rw-rw-   0        0        0     4245 2021-11-30 18:29:10.000000 river-0.9.0/river/multiclass/ovr.py
+-rw-rw-rw-   0        0        0     1272 2021-11-30 18:29:10.000000 river-0.9.0/river/multiclass/test_ovr.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.559367 river-0.9.0/river/multioutput/
+-rw-rw-rw-   0        0        0      299 2021-11-30 18:29:10.000000 river-0.9.0/river/multioutput/__init__.py
+-rw-rw-rw-   0        0        0    13729 2021-11-30 18:29:10.000000 river-0.9.0/river/multioutput/chain.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.565366 river-0.9.0/river/naive_bayes/
+-rw-rw-rw-   0        0        0      255 2021-11-30 18:29:10.000000 river-0.9.0/river/naive_bayes/__init__.py
+-rw-rw-rw-   0        0        0     2588 2021-11-30 18:29:10.000000 river-0.9.0/river/naive_bayes/base.py
+-rw-rw-rw-   0        0        0     8750 2021-11-30 18:29:10.000000 river-0.9.0/river/naive_bayes/bernoulli.py
+-rw-rw-rw-   0        0        0     8794 2021-11-30 18:29:10.000000 river-0.9.0/river/naive_bayes/complement.py
+-rw-rw-rw-   0        0        0     2233 2021-11-30 18:29:10.000000 river-0.9.0/river/naive_bayes/gaussian.py
+-rw-rw-rw-   0        0        0     8629 2021-11-30 18:29:10.000000 river-0.9.0/river/naive_bayes/multinomial.py
+-rw-rw-rw-   0        0        0     8901 2021-11-30 18:29:10.000000 river-0.9.0/river/naive_bayes/test_naive_bayes.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.571377 river-0.9.0/river/neighbors/
+-rw-rw-rw-   0        0        0      418 2021-11-30 18:29:10.000000 river-0.9.0/river/neighbors/__init__.py
+-rw-rw-rw-   0        0        0     7426 2021-11-30 18:29:10.000000 river-0.9.0/river/neighbors/base_neighbors.py
+-rw-rw-rw-   0        0        0     3639 2021-11-30 18:29:10.000000 river-0.9.0/river/neighbors/knn_adwin.py
+-rw-rw-rw-   0        0        0     5415 2021-11-30 18:29:10.000000 river-0.9.0/river/neighbors/knn_classifier.py
+-rw-rw-rw-   0        0        0     5581 2021-11-30 18:29:10.000000 river-0.9.0/river/neighbors/knn_regressor.py
+-rw-rw-rw-   0        0        0    33508 2021-11-30 18:29:10.000000 river-0.9.0/river/neighbors/sam_knn.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.141950 river-0.9.0/river/neighbors/src/
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.572367 river-0.9.0/river/neighbors/src/libNearestNeighbor/
+-rw-rw-rw-   0        0        0    12103 2021-11-30 18:29:10.000000 river-0.9.0/river/neighbors/src/libNearestNeighbor/nearestNeighbor.cpp
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.574366 river-0.9.0/river/neural_net/
+-rw-rw-rw-   0        0        0      127 2021-11-30 18:29:10.000000 river-0.9.0/river/neural_net/__init__.py
+-rw-rw-rw-   0        0        0     1577 2021-11-30 18:29:10.000000 river-0.9.0/river/neural_net/activations.py
+-rw-rw-rw-   0        0        0    10169 2021-11-30 18:29:10.000000 river-0.9.0/river/neural_net/mlp.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.592366 river-0.9.0/river/optim/
+-rw-rw-rw-   0        0        0      841 2021-11-30 18:29:10.000000 river-0.9.0/river/optim/__init__.py
+-rw-rw-rw-   0        0        0     2546 2021-11-30 18:29:10.000000 river-0.9.0/river/optim/ada_bound.py
+-rw-rw-rw-   0        0        0     1846 2021-11-30 18:29:10.000000 river-0.9.0/river/optim/ada_delta.py
+-rw-rw-rw-   0        0        0     1483 2021-11-30 18:29:10.000000 river-0.9.0/river/optim/ada_grad.py
+-rw-rw-rw-   0        0        0     2000 2021-11-30 18:29:10.000000 river-0.9.0/river/optim/ada_max.py
+-rw-rw-rw-   0        0        0     2680 2021-11-30 18:29:10.000000 river-0.9.0/river/optim/adam.py
+-rw-rw-rw-   0        0        0     2361 2021-11-30 18:29:10.000000 river-0.9.0/river/optim/ams_grad.py
+-rw-rw-rw-   0        0        0     2771 2021-11-30 18:29:10.000000 river-0.9.0/river/optim/average.py
+-rw-rw-rw-   0        0        0     2216 2021-11-30 18:29:10.000000 river-0.9.0/river/optim/base.py
+-rw-rw-rw-   0        0        0     2379 2021-11-30 18:29:10.000000 river-0.9.0/river/optim/ftrl.py
+-rw-rw-rw-   0        0        0     2428 2021-11-30 18:29:10.000000 river-0.9.0/river/optim/initializers.py
+-rw-rw-rw-   0        0        0    16254 2021-11-30 18:29:10.000000 river-0.9.0/river/optim/losses.py
+-rw-rw-rw-   0        0        0     1119 2021-11-30 18:29:10.000000 river-0.9.0/river/optim/momentum.py
+-rw-rw-rw-   0        0        0     1995 2021-11-30 18:29:10.000000 river-0.9.0/river/optim/nadam.py
+-rw-rw-rw-   0        0        0     1382 2021-11-30 18:29:10.000000 river-0.9.0/river/optim/nesterov.py
+-rw-rw-rw-   0        0        0     1242 2021-11-30 18:29:10.000000 river-0.9.0/river/optim/newton.py
+-rw-rw-rw-   0        0        0     1880 2021-11-30 18:29:10.000000 river-0.9.0/river/optim/rms_prop.py
+-rw-rw-rw-   0        0        0     2235 2021-11-30 18:29:10.000000 river-0.9.0/river/optim/schedulers.py
+-rw-rw-rw-   0        0        0     1290 2021-11-30 18:29:10.000000 river-0.9.0/river/optim/sgd.py
+-rw-rw-rw-   0        0        0     2831 2021-11-30 18:29:10.000000 river-0.9.0/river/optim/test_.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.600366 river-0.9.0/river/preprocessing/
+-rw-rw-rw-   0        0        0     1083 2021-11-30 18:29:10.000000 river-0.9.0/river/preprocessing/__init__.py
+-rw-rw-rw-   0        0        0     1964 2021-11-30 18:29:10.000000 river-0.9.0/river/preprocessing/feature_hasher.py
+-rw-rw-rw-   0        0        0     7620 2021-11-30 18:29:10.000000 river-0.9.0/river/preprocessing/impute.py
+-rw-rw-rw-   0        0        0    14440 2021-11-30 18:29:10.000000 river-0.9.0/river/preprocessing/lda.py
+-rw-rw-rw-   0        0        0     4288 2021-11-30 18:29:10.000000 river-0.9.0/river/preprocessing/one_hot.py
+-rw-rw-rw-   0        0        0     1715 2021-11-30 18:29:10.000000 river-0.9.0/river/preprocessing/pred_clipper.py
+-rw-rw-rw-   0        0        0    17221 2021-11-30 18:29:10.000000 river-0.9.0/river/preprocessing/scale.py
+-rw-rw-rw-   0        0        0     8158 2021-11-30 18:29:10.000000 river-0.9.0/river/preprocessing/test_lda.py
+-rw-rw-rw-   0        0        0     1966 2021-11-30 18:29:10.000000 river-0.9.0/river/preprocessing/test_scale.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.603367 river-0.9.0/river/proba/
+-rw-rw-rw-   0        0        0      145 2021-11-30 18:29:10.000000 river-0.9.0/river/proba/__init__.py
+-rw-rw-rw-   0        0        0      986 2021-11-30 18:29:10.000000 river-0.9.0/river/proba/base.py
+-rw-rw-rw-   0        0        0     1480 2021-11-30 18:29:10.000000 river-0.9.0/river/proba/gaussian.py
+-rw-rw-rw-   0        0        0     1402 2021-11-30 18:29:10.000000 river-0.9.0/river/proba/multinomial.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.609367 river-0.9.0/river/reco/
+-rw-rw-rw-   0        0        0      340 2021-11-30 18:29:10.000000 river-0.9.0/river/reco/__init__.py
+-rw-rw-rw-   0        0        0     1098 2021-11-30 18:29:10.000000 river-0.9.0/river/reco/base.py
+-rw-rw-rw-   0        0        0     4639 2021-11-30 18:29:10.000000 river-0.9.0/river/reco/baseline.py
+-rw-rw-rw-   0        0        0     8226 2021-11-30 18:29:10.000000 river-0.9.0/river/reco/biased_mf.py
+-rw-rw-rw-   0        0        0     5196 2021-11-30 18:29:10.000000 river-0.9.0/river/reco/funk_mf.py
+-rw-rw-rw-   0        0        0     2143 2021-11-30 18:29:10.000000 river-0.9.0/river/reco/normal.py
+-rw-rw-rw-   0        0        0      681 2021-11-30 18:29:10.000000 river-0.9.0/river/reco/surprise.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.611367 river-0.9.0/river/rules/
+-rw-rw-rw-   0        0        0       97 2021-11-30 18:29:10.000000 river-0.9.0/river/rules/__init__.py
+-rw-rw-rw-   0        0        0    19265 2021-11-30 18:29:10.000000 river-0.9.0/river/rules/amrules.py
+-rw-rw-rw-   0        0        0     9404 2021-11-30 18:29:10.000000 river-0.9.0/river/rules/base.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.634368 river-0.9.0/river/stats/
+-rw-rw-rw-   0        0        0     1571 2021-11-30 18:29:10.000000 river-0.9.0/river/stats/__init__.py
+-rw-rw-rw-   0        0        0     1642 2021-11-30 18:29:10.000000 river-0.9.0/river/stats/auto_corr.py
+-rw-rw-rw-   0        0        0     1760 2021-11-30 18:29:10.000000 river-0.9.0/river/stats/base.py
+-rw-rw-rw-   0        0        0      351 2021-11-30 18:29:10.000000 river-0.9.0/river/stats/count.py
+-rw-rw-rw-   0        0        0     5910 2021-11-30 18:29:10.000000 river-0.9.0/river/stats/cov.py
+-rw-rw-rw-   0        0        0     2568 2021-11-30 18:29:10.000000 river-0.9.0/river/stats/entropy.py
+-rw-rw-rw-   0        0        0     1361 2021-11-30 18:29:10.000000 river-0.9.0/river/stats/ewmean.py
+-rw-rw-rw-   0        0        0     1664 2021-11-30 18:29:10.000000 river-0.9.0/river/stats/ewvar.py
+-rw-rw-rw-   0        0        0     3355 2021-11-30 18:29:10.000000 river-0.9.0/river/stats/iqr.py
+-rw-rw-rw-   0        0        0     2535 2021-11-30 18:29:10.000000 river-0.9.0/river/stats/kurtosis.py
+-rw-rw-rw-   0        0        0     2404 2021-11-30 18:29:10.000000 river-0.9.0/river/stats/link.py
+-rw-rw-rw-   0        0        0     1374 2021-11-30 18:29:10.000000 river-0.9.0/river/stats/mad.py
+-rw-rw-rw-   0        0        0     2942 2021-11-30 18:29:10.000000 river-0.9.0/river/stats/maximum.py
+-rw-rw-rw-   0        0        0     4479 2021-11-30 18:29:10.000000 river-0.9.0/river/stats/mean.py
+-rw-rw-rw-   0        0        0     1218 2021-11-30 18:29:10.000000 river-0.9.0/river/stats/minimum.py
+-rw-rw-rw-   0        0        0     3006 2021-11-30 18:29:10.000000 river-0.9.0/river/stats/mode.py
+-rw-rw-rw-   0        0        0     1861 2021-11-30 18:29:10.000000 river-0.9.0/river/stats/moments.py
+-rw-rw-rw-   0        0        0     3222 2021-11-30 18:29:10.000000 river-0.9.0/river/stats/n_unique.py
+-rw-rw-rw-   0        0        0     2882 2021-11-30 18:29:10.000000 river-0.9.0/river/stats/pearson.py
+-rw-rw-rw-   0        0        0     2072 2021-11-30 18:29:10.000000 river-0.9.0/river/stats/ptp.py
+-rw-rw-rw-   0        0        0     7140 2021-11-30 18:29:10.000000 river-0.9.0/river/stats/quantile.py
+-rw-rw-rw-   0        0        0     1904 2021-11-30 18:29:10.000000 river-0.9.0/river/stats/sem.py
+-rw-rw-rw-   0        0        0     3487 2021-11-30 18:29:10.000000 river-0.9.0/river/stats/shift.py
+-rw-rw-rw-   0        0        0     2503 2021-11-30 18:29:10.000000 river-0.9.0/river/stats/skew.py
+-rw-rw-rw-   0        0        0     1534 2021-11-30 18:29:10.000000 river-0.9.0/river/stats/summing.py
+-rw-rw-rw-   0        0        0     5944 2021-11-30 18:29:10.000000 river-0.9.0/river/stats/test_.py
+-rw-rw-rw-   0        0        0     3795 2021-11-30 18:29:10.000000 river-0.9.0/river/stats/test_parallel.py
+-rw-rw-rw-   0        0        0     4665 2021-11-30 18:29:10.000000 river-0.9.0/river/stats/var.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.647366 river-0.9.0/river/stream/
+-rw-rw-rw-   0        0        0      924 2021-11-30 18:29:10.000000 river-0.9.0/river/stream/__init__.py
+-rw-rw-rw-   0        0        0     4765 2021-11-30 18:29:10.000000 river-0.9.0/river/stream/cache.py
+-rw-rw-rw-   0        0        0     1681 2021-11-30 18:29:10.000000 river-0.9.0/river/stream/iter_arff.py
+-rw-rw-rw-   0        0        0     2401 2021-11-30 18:29:10.000000 river-0.9.0/river/stream/iter_array.py
+-rw-rw-rw-   0        0        0     6502 2021-11-30 18:29:10.000000 river-0.9.0/river/stream/iter_csv.py
+-rw-rw-rw-   0        0        0     2136 2021-11-30 18:29:10.000000 river-0.9.0/river/stream/iter_libsvm.py
+-rw-rw-rw-   0        0        0     1267 2021-11-30 18:29:10.000000 river-0.9.0/river/stream/iter_pandas.py
+-rw-rw-rw-   0        0        0     1710 2021-11-30 18:29:10.000000 river-0.9.0/river/stream/iter_sklearn.py
+-rw-rw-rw-   0        0        0     3307 2021-11-30 18:29:10.000000 river-0.9.0/river/stream/iter_sql.py
+-rw-rw-rw-   0        0        0     1494 2021-11-30 18:29:10.000000 river-0.9.0/river/stream/iter_vaex.py
+-rw-rw-rw-   0        0        0     4693 2021-11-30 18:29:10.000000 river-0.9.0/river/stream/pokedb.zip
+-rw-rw-rw-   0        0        0     6604 2021-11-30 18:29:10.000000 river-0.9.0/river/stream/qa.py
+-rw-rw-rw-   0        0        0     2327 2021-11-30 18:29:10.000000 river-0.9.0/river/stream/shuffling.py
+-rw-rw-rw-   0        0        0     1209 2021-11-30 18:29:10.000000 river-0.9.0/river/stream/test_iter_csv.py
+-rw-rw-rw-   0        0        0     3411 2021-11-30 18:29:10.000000 river-0.9.0/river/stream/test_sql.py
+-rw-rw-rw-   0        0        0      855 2021-11-30 18:29:10.000000 river-0.9.0/river/stream/utils.py
+-rw-rw-rw-   0        0        0     4127 2021-11-30 18:29:10.000000 river-0.9.0/river/test_estimators.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.654367 river-0.9.0/river/time_series/
+-rw-rw-rw-   0        0        0      232 2021-11-30 18:29:10.000000 river-0.9.0/river/time_series/__init__.py
+-rw-rw-rw-   0        0        0      936 2021-11-30 18:29:10.000000 river-0.9.0/river/time_series/base.py
+-rw-rw-rw-   0        0        0     4117 2021-11-30 18:29:10.000000 river-0.9.0/river/time_series/evaluate.py
+-rw-rw-rw-   0        0        0     6544 2021-11-30 18:29:10.000000 river-0.9.0/river/time_series/holt_winters.py
+-rw-rw-rw-   0        0        0     2017 2021-11-30 18:29:10.000000 river-0.9.0/river/time_series/metric.py
+-rw-rw-rw-   0        0        0    11923 2021-11-30 18:29:10.000000 river-0.9.0/river/time_series/snarimax.py
+-rw-rw-rw-   0        0        0     1131 2021-11-30 18:29:10.000000 river-0.9.0/river/time_series/test_evaluate.py
+-rw-rw-rw-   0        0        0    15529 2021-11-30 18:29:10.000000 river-0.9.0/river/time_series/test_holt_winters.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.670381 river-0.9.0/river/tree/
+-rw-rw-rw-   0        0        0     3264 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/__init__.py
+-rw-rw-rw-   0        0        0     5732 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/base.py
+-rw-rw-rw-   0        0        0    22792 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/extremely_fast_decision_tree.py
+-rw-rw-rw-   0        0        0    10127 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/hoeffding_adaptive_tree_classifier.py
+-rw-rw-rw-   0        0        0    12597 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/hoeffding_adaptive_tree_regressor.py
+-rw-rw-rw-   0        0        0    18326 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/hoeffding_tree.py
+-rw-rw-rw-   0        0        0    16915 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/hoeffding_tree_classifier.py
+-rw-rw-rw-   0        0        0    16967 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/hoeffding_tree_regressor.py
+-rw-rw-rw-   0        0        0    10701 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/isoup_tree_regressor.py
+-rw-rw-rw-   0        0        0     7184 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/label_combination_hoeffding_tree.py
+-rw-rw-rw-   0        0        0     1409 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/losses.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.680367 river-0.9.0/river/tree/nodes/
+-rw-rw-rw-   0        0        0      116 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/nodes/__init__.py
+-rw-rw-rw-   0        0        0     4894 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/nodes/arf_htc_nodes.py
+-rw-rw-rw-   0        0        0     3776 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/nodes/arf_htr_nodes.py
+-rw-rw-rw-   0        0        0     6115 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/nodes/branch.py
+-rw-rw-rw-   0        0        0    10113 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/nodes/efdtc_nodes.py
+-rw-rw-rw-   0        0        0    14388 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/nodes/hatc_nodes.py
+-rw-rw-rw-   0        0        0    14181 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/nodes/hatr_nodes.py
+-rw-rw-rw-   0        0        0     6528 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/nodes/htc_nodes.py
+-rw-rw-rw-   0        0        0     6668 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/nodes/htr_nodes.py
+-rw-rw-rw-   0        0        0     6913 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/nodes/isouptr_nodes.py
+-rw-rw-rw-   0        0        0     5428 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/nodes/leaf.py
+-rw-rw-rw-   0        0        0     8817 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/nodes/sgt_nodes.py
+-rw-rw-rw-   0        0        0      489 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/setup.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.689367 river-0.9.0/river/tree/split_criterion/
+-rw-rw-rw-   0        0        0      698 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/split_criterion/__init__.py
+-rw-rw-rw-   0        0        0     1240 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/split_criterion/base.py
+-rw-rw-rw-   0        0        0     1037 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/split_criterion/gini_split_criterion.py
+-rw-rw-rw-   0        0        0     2703 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/split_criterion/hellinger_distance_criterion.py
+-rw-rw-rw-   0        0        0     2878 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/split_criterion/info_gain_split_criterion.py
+-rw-rw-rw-   0        0        0     1189 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/split_criterion/intra_cluster_variance_reduction_split_criterion.py
+-rw-rw-rw-   0        0        0     1620 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/split_criterion/variance_ratio_split_criterion.py
+-rw-rw-rw-   0        0        0     1611 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/split_criterion/variance_reduction_split_criterion.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.699366 river-0.9.0/river/tree/splitter/
+-rw-rw-rw-   0        0        0     1570 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/splitter/__init__.py
+-rw-rw-rw-   0        0        0     3525 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/splitter/base.py
+-rw-rw-rw-   0        0        0    11402 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/splitter/ebst_splitter.py
+-rw-rw-rw-   0        0        0     7188 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/splitter/exhaustive_splitter.py
+-rw-rw-rw-   0        0        0     3963 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/splitter/gaussian_splitter.py
+-rw-rw-rw-   0        0        0     3564 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/splitter/histogram_splitter.py
+-rw-rw-rw-   0        0        0     3946 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/splitter/nominal_splitter_classif.py
+-rw-rw-rw-   0        0        0     3456 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/splitter/nominal_splitter_reg.py
+-rw-rw-rw-   0        0        0     8292 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/splitter/qo_splitter.py
+-rw-rw-rw-   0        0        0     6011 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/splitter/sgt_quantizer.py
+-rw-rw-rw-   0        0        0     1054 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/splitter/tebst_splitter.py
+-rw-rw-rw-   0        0        0    15888 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/stochastic_gradient_tree.py
+-rw-rw-rw-   0        0        0     2897 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/test_base.py
+-rw-rw-rw-   0        0        0     1861 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/test_splitter.py
+-rw-rw-rw-   0        0        0     4757 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/test_trees.py
+-rw-rw-rw-   0        0        0     7176 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/utils.py
+-rw-rw-rw-   0        0        0     2115 2021-11-30 18:29:10.000000 river-0.9.0/river/tree/viz.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.721367 river-0.9.0/river/utils/
+-rw-rw-rw-   0        0        0      883 2021-11-30 18:29:10.000000 river-0.9.0/river/utils/__init__.py
+-rw-rw-rw-   0        0        0      189 2021-11-30 18:29:10.000000 river-0.9.0/river/utils/context_managers.py
+-rw-rw-rw-   0        0        0     1523 2021-11-30 18:29:10.000000 river-0.9.0/river/utils/data_conversion.py
+-rw-rw-rw-   0        0        0    11325 2021-11-30 18:29:10.000000 river-0.9.0/river/utils/estimator_checks.py
+-rw-rw-rw-   0        0        0     8540 2021-11-30 18:29:10.000000 river-0.9.0/river/utils/histogram.py
+-rw-rw-rw-   0        0        0     1704 2021-11-30 18:29:10.000000 river-0.9.0/river/utils/inspect.py
+-rw-rw-rw-   0        0        0     6411 2021-11-30 18:29:10.000000 river-0.9.0/river/utils/math.py
+-rw-rw-rw-   0        0        0     4933 2021-11-30 18:29:10.000000 river-0.9.0/river/utils/param_grid.py
+-rw-rw-rw-   0        0        0     2120 2021-11-30 18:29:10.000000 river-0.9.0/river/utils/pretty.py
+-rw-rw-rw-   0        0        0      496 2021-11-30 18:29:10.000000 river-0.9.0/river/utils/random.py
+-rw-rw-rw-   0        0        0     2165 2021-11-30 18:29:10.000000 river-0.9.0/river/utils/sdft.py
+-rw-rw-rw-   0        0        0     9236 2021-11-30 18:29:10.000000 river-0.9.0/river/utils/skmultiflow_utils.py
+-rw-rw-rw-   0        0        0     6435 2021-11-30 18:29:10.000000 river-0.9.0/river/utils/skyline.py
+-rw-rw-rw-   0        0        0     2147 2021-11-30 18:29:10.000000 river-0.9.0/river/utils/test_param_grid.py
+-rw-rw-rw-   0        0        0     3849 2021-11-30 18:29:10.000000 river-0.9.0/river/utils/test_vectordict.py
+-rw-rw-rw-   0        0        0  1003912 2021-11-30 18:29:51.000000 river-0.9.0/river/utils/vectordict.c
+-rw-rw-rw-   0        0        0    19286 2021-11-30 18:29:10.000000 river-0.9.0/river/utils/vectordict.pyx
+-rw-rw-rw-   0        0        0     2510 2021-11-30 18:29:10.000000 river-0.9.0/river/utils/window.py
+drwxrwxrwx   0        0        0        0 2021-11-30 18:46:04.364534 river-0.9.0/river.egg-info/
+-rw-rw-rw-   0        0        0    12934 2021-11-30 18:46:02.000000 river-0.9.0/river.egg-info/PKG-INFO
+-rw-rw-rw-   0        0        0    14402 2021-11-30 18:46:03.000000 river-0.9.0/river.egg-info/SOURCES.txt
+-rw-rw-rw-   0        0        0        1 2021-11-30 18:46:02.000000 river-0.9.0/river.egg-info/dependency_links.txt
+-rw-rw-rw-   0        0        0      697 2021-11-30 18:46:02.000000 river-0.9.0/river.egg-info/requires.txt
+-rw-rw-rw-   0        0        0        6 2021-11-30 18:46:02.000000 river-0.9.0/river.egg-info/top_level.txt
+-rw-rw-rw-   0        0        0      126 2021-11-30 18:46:04.725367 river-0.9.0/setup.cfg
+-rw-rw-rw-   0        0        0     4166 2021-11-30 18:29:10.000000 river-0.9.0/setup.py
```

### Comparing `river-0.8.0/LICENSE` & `river-0.9.0/LICENSE`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,29 +1,29 @@
-BSD 3-Clause License
-
-Copyright (c) 2020, the river developers
-All rights reserved.
-
-Redistribution and use in source and binary forms, with or without
-modification, are permitted provided that the following conditions are met:
-
-1. Redistributions of source code must retain the above copyright notice, this
-   list of conditions and the following disclaimer.
-
-2. Redistributions in binary form must reproduce the above copyright notice,
-   this list of conditions and the following disclaimer in the documentation
-   and/or other materials provided with the distribution.
-
-3. Neither the name of the copyright holder nor the names of its
-   contributors may be used to endorse or promote products derived from
-   this software without specific prior written permission.
-
-THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
-AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
-IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
-DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
-FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
-DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
-SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
-CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
-OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
-OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+BSD 3-Clause License
+
+Copyright (c) 2020, the river developers
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are met:
+
+1. Redistributions of source code must retain the above copyright notice, this
+   list of conditions and the following disclaimer.
+
+2. Redistributions in binary form must reproduce the above copyright notice,
+   this list of conditions and the following disclaimer in the documentation
+   and/or other materials provided with the distribution.
+
+3. Neither the name of the copyright holder nor the names of its
+   contributors may be used to endorse or promote products derived from
+   this software without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
```

### Comparing `river-0.8.0/PKG-INFO` & `river-0.9.0/PKG-INFO`

 * *Files 12% similar despite different names*

```diff
@@ -1,244 +1,245 @@
-Metadata-Version: 2.1
-Name: river
-Version: 0.8.0
-Summary: Online machine learning in Python
-Home-page: https://github.com/online-ml/river
-Author: Max Halford
-Author-email: maxhalford25@gmail.com
-License: BSD-3
-Description: 
-        </br>
-        
-        <p align="center">
-          <img height="80px" src="docs/img/logo.svg" alt="river_logo">
-        </p>
-        
-        </br>
-        
-        <p align="center">
-          <!-- Tests -->
-          <a href="https://github.com/online-ml/river/actions?query=workflow%3Atests+branch%3Amaster">
-            <img src="https://github.com/online-ml/river/workflows/tests/badge.svg?branch=master" alt="tests">
-          </a>
-          <!-- Code coverage -->
-          <a href="https://codecov.io/gh/online-ml/river">
-            <img src="https://codecov.io/gh/online-ml/river/branch/master/graph/badge.svg?token=luK6eFoMa9"/>
-          </a>
-          <!-- Documentation -->
-          <a href="https://riverml.xyz">
-            <img src="https://img.shields.io/website?label=docs&style=flat-square&url=https%3A%2F%2Friverml.xyz%2F" alt="documentation">
-          </a>
-          <!-- Roadmap -->
-          <a href="https://www.notion.so/d1e86fcdf21e4deda16eedab2b3361fb?v=503f44740b8b44a99a961aa96e9e46e1">
-            <img src="https://img.shields.io/website?label=roadmap&style=flat-square&url=https://www.notion.so/d1e86fcdf21e4deda16eedab2b3361fb?v=503f44740b8b44a99a961aa96e9e46e1" alt="roadmap">
-          </a>
-          <!-- PyPI -->
-          <a href="https://pypi.org/project/river">
-            <img src="https://img.shields.io/pypi/v/river.svg?label=release&color=blue&style=flat-square" alt="pypi">
-          </a>
-          <!-- PePy -->
-          <a href="https://pepy.tech/project/river">
-            <img src="https://static.pepy.tech/badge/river?style=flat-square" alt="pepy">
-          </a>
-          <!-- License -->
-          <a href="https://opensource.org/licenses/BSD-3-Clause">
-            <img src="https://img.shields.io/badge/License-BSD%203--Clause-blue.svg?style=flat-square" alt="bsd_3_license">
-          </a>
-        </p>
-        
-        </br>
-        
-        <p align="center">
-          River is a Python library for <a href="https://www.wikiwand.com/en/Online_machine_learning">online machine learning</a>. It is the result of a merger between <a href="https://github.com/MaxHalford/creme">creme</a> and <a href="https://github.com/scikit-multiflow/scikit-multiflow">scikit-multiflow</a>. River's ambition is to be the go-to library for doing machine learning on streaming data.
-        </p>
-        
-        ## ⚡️ Quickstart
-        
-        As a quick example, we'll train a logistic regression to classify the [website phishing dataset](http://archive.ics.uci.edu/ml/datasets/Website+Phishing). Here's a look at the first observation in the dataset.
-        
-        ```python
-        >>> from pprint import pprint
-        >>> from river import datasets
-        
-        >>> dataset = datasets.Phishing()
-        
-        >>> for x, y in dataset:
-        ...     pprint(x)
-        ...     print(y)
-        ...     break
-        {'age_of_domain': 1,
-         'anchor_from_other_domain': 0.0,
-         'empty_server_form_handler': 0.0,
-         'https': 0.0,
-         'ip_in_url': 1,
-         'is_popular': 0.5,
-         'long_url': 1.0,
-         'popup_window': 0.0,
-         'request_from_other_domain': 0.0}
-        True
-        
-        ```
-        
-        Now let's run the model on the dataset in a streaming fashion. We sequentially interleave predictions and model updates. Meanwhile, we update a performance metric to see how well the model is doing.
-        
-        ```python
-        >>> from river import compose
-        >>> from river import linear_model
-        >>> from river import metrics
-        >>> from river import preprocessing
-        
-        >>> model = compose.Pipeline(
-        ...     preprocessing.StandardScaler(),
-        ...     linear_model.LogisticRegression()
-        ... )
-        
-        >>> metric = metrics.Accuracy()
-        
-        >>> for x, y in dataset:
-        ...     y_pred = model.predict_one(x)      # make a prediction
-        ...     metric = metric.update(y, y_pred)  # update the metric
-        ...     model = model.learn_one(x, y)      # make the model learn
-        
-        >>> metric
-        Accuracy: 89.20%
-        
-        ```
-        
-        ## 🛠 Installation
-        
-        River is intended to work with **Python 3.6 or above**. Installation can be done with `pip`:
-        
-        ```sh
-        pip install river
-        ```
-        
-        There are [wheels available](https://pypi.org/project/river/#files) for Linux, MacOS, and Windows, which means that you most probably won't have to build River from source.
-        
-        You can install the latest development version from GitHub as so:
-        
-        ```sh
-        pip install git+https://github.com/online-ml/river --upgrade
-        ```
-        
-        Or, through SSH:
-        
-        ```sh
-        pip install git+ssh://git@github.com/online-ml/river.git --upgrade
-        ```
-        
-        ## 🧠 Philosophy
-        
-        Machine learning is often done in a batch setting, whereby a model is fitted to a dataset in one go. This results in a static model which has to be retrained in order to learn from new data. In many cases, this isn't elegant nor efficient, and usually incurs [a fair amount of technical debt](https://research.google/pubs/pub43146/). Indeed, if you're using a batch model, then you need to think about maintaining a training set, monitoring real-time performance, model retraining, etc.
-        
-        With River, we encourage a different approach, which is to continuously learn a stream of data. This means that the model process one observation at a time, and can therefore be updated on the fly. This allows to learn from massive datasets that don't fit in main memory. Online machine learning also integrates nicely in cases where new data is constantly arriving. It shines in many use cases, such as time series forecasting, spam filtering, recommender systems, CTR prediction, and IoT applications. If you're bored with retraining models and want to instead build dynamic models, then online machine learning (and therefore River!) might be what you're looking for.
-        
-        Here are some benefits of using River (and online machine learning in general):
-        
-        - **Incremental**: models can update themselves in real-time.
-        - **Adaptive**: models can adapt to [concept drift](https://www.wikiwand.com/en/Concept_drift).
-        - **Production-ready**: working with data streams makes it simple to replicate production scenarios during model development.
-        - **Efficient**: models don't have to be retrained and require little compute power, which [lowers their carbon footprint](https://arxiv.org/abs/1907.10597)
-        - **Fast**: when the goal is to learn and predict with a single instance at a time, then River is an order of magnitude faster than PyTorch, Tensorflow, and scikit-learn.
-        
-        ## 🔥 Features
-        
-        - Linear models with a wide array of optimizers
-        - Nearest neighbors, decision trees, naïve Bayes
-        - [Progressive model validation](https://hunch.net/~jl/projects/prediction_bounds/progressive_validation/coltfinal.pdf)
-        - Model pipelines as a first-class citizen
-        - Anomaly detection
-        - Recommender systems
-        - Time series forecasting
-        - Imbalanced learning
-        - Clustering
-        - Feature extraction and selection
-        - Online statistics and metrics
-        - Built-in datasets
-        - And [much more](https://riverml.xyz/latest/api/overview/)
-        
-        ## 🔗 Useful links
-        
-        - [Documentation](https://riverml.xyz)
-        - [Benchmarks](https://github.com/online-ml/river/tree/master/benchmarks)
-        - [Issue tracker](https://github.com/online-ml/river/issues)
-        - [Package releases](https://pypi.org/project/river/#history)
-        
-        ## 👁️ Media
-        
-        - PyData Amsterdam 2019 presentation ([slides](https://maxhalford.github.io/slides/creme-pydata), [video](https://www.youtube.com/watch?v=P3M6dt7bY9U&list=PLGVZCDnMOq0q7_6SdrC2wRtdkojGBTAht&index=11))
-        - [Toulouse Data Science Meetup presentation](https://maxhalford.github.io/slides/creme-tds)
-        - [Machine learning for streaming data with creme](https://towardsdatascience.com/machine-learning-for-streaming-data-with-creme-dacf5fb469df)
-        - [Hong Kong Data Science Meetup presentation](https://maxhalford.github.io/slides/hkml2020.pdf)
-        
-        ## 👍 Contributing
-        
-        Feel free to contribute in any way you like, we're always open to new ideas and approaches.
-        
-        There are three ways for users to get involved:
-        
-        - [Issue tracker](https://github.com/online-ml/river/issues): this place is meant to report bugs, request for minor features, or small improvements. Issues should be short-lived and solved as fast as possible.
-        - [Discussions](https://github.com/online-ml/river/discussions): you can ask for new features, submit your questions and get help, propose new ideas, or even show the community what you are achieving with River! If you have a new technique or want to port a new functionality to River, this is the place to discuss.
-        - [Roadmap](https://www.notion.so/d1e86fcdf21e4deda16eedab2b3361fb?v=503f44740b8b44a99a961aa96e9e46e1): you can check what we are doing, what are the next planned milestones for River, and look for cool ideas that still need someone to make them become a reality!
-        
-        Please check out the [contribution guidelines](https://github.com/online-ml/river/blob/master/CONTRIBUTING.md) if you want to bring modifications to the code base. You can view the list of people who have contributed [here](https://github.com/online-ml/river/graphs/contributors).
-        
-        ## ❤️ They've used us
-        
-        These are companies that we know have been using River, be it in production or for prototyping.
-        
-        <p align="center">
-          <img width="70%" src="https://docs.google.com/drawings/d/e/2PACX-1vQbCUQkTU74dBf411r4nDl4udmqOEbLqzRtokUC-N7JDJUA7BGTfnMGmiMNqbcSuOaWAmazp1rFGwDC/pub?w=1194&h=567" alt="companies">
-        </p>
-        
-        Feel welcome to get in touch if you want us to add your company logo!
-        
-        ## 🤝 Affiliations
-        
-        **Sponsors**
-        
-        <p align="center">
-          <img width="55%" src="https://docs.google.com/drawings/d/e/2PACX-1vSagEhWAjDsb0c24En_fhWAf9DJZbyh5YjU7lK0sNowD2m9uv9TuFm-U77k6ObqTyN2mP05Avf6TCJc/pub?w=2073&h=1127" alt="sponsors">
-        </p>
-        
-        **Collaborating institutions and groups**
-        
-        <p align="center">
-          <img width="55%" src="https://docs.google.com/drawings/d/e/2PACX-1vQB0C8YgnkCt_3C3cp-Csaw8NLZUwishdbJFB3iSbBPUD0AxEVS9AlF-Rs5PJq8UVRzRtFwZIOucuXj/pub?w=1442&h=489" alt="collaborations">
-        </p>
-        
-        ## 💬 Citation
-        
-        If `river` has been useful for your research and you would like to cite it in an scientific publication, please refer to this [paper](https://arxiv.org/abs/2012.04740):
-        
-        ```bibtex
-        @misc{2020river,
-              title={River: machine learning for streaming data in Python},
-              author={Jacob Montiel and Max Halford and Saulo Martiello Mastelini
-                      and Geoffrey Bolmier and Raphael Sourty and Robin Vaysse
-                      and Adil Zouitine and Heitor Murilo Gomes and Jesse Read
-                      and Talel Abdessalem and Albert Bifet},
-              year={2020},
-              eprint={2012.04740},
-              archivePrefix={arXiv},
-              primaryClass={cs.LG}
-        }
-        ```
-        
-        ## 📝 License
-        
-        River is free and open-source software licensed under the [3-clause BSD license](https://github.com/online-ml/river/blob/master/LICENSE).
-        
-Platform: UNKNOWN
-Classifier: License :: OSI Approved :: BSD License
-Classifier: Programming Language :: Python
-Classifier: Programming Language :: Python :: 3
-Classifier: Programming Language :: Python :: 3.6
-Classifier: Programming Language :: Python :: 3.7
-Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: Implementation :: CPython
-Classifier: Programming Language :: Python :: Implementation :: PyPy
-Requires-Python: >=3.6.0
-Description-Content-Type: text/markdown
-Provides-Extra: dev
-Provides-Extra: compat
-Provides-Extra: docs
+Metadata-Version: 2.1
+Name: river
+Version: 0.9.0
+Summary: Online machine learning in Python
+Home-page: https://github.com/online-ml/river
+Author: Max Halford
+Author-email: maxhalford25@gmail.com
+License: BSD-3
+Description: 
+        </br>
+        
+        <p align="center">
+          <img height="80px" src="docs/img/logo.svg" alt="river_logo">
+        </p>
+        
+        </br>
+        
+        <p align="center">
+          <!-- Tests -->
+          <a href="https://github.com/online-ml/river/actions/workflows/unit-tests.yml">
+            <img src="https://github.com/online-ml/river/actions/workflows/unit-tests.yml/badge.svg" alt="tests">
+          </a>
+          <!-- Code coverage -->
+          <a href="https://codecov.io/gh/online-ml/river">
+            <img src="https://codecov.io/gh/online-ml/river/branch/main/graph/badge.svg?token=luK6eFoMa9"/>
+          </a>
+          <!-- Documentation -->
+          <a href="https://riverml.xyz">
+            <img src="https://img.shields.io/website?label=docs&style=flat-square&url=https%3A%2F%2Friverml.xyz%2F" alt="documentation">
+          </a>
+          <!-- Roadmap -->
+          <a href="https://www.notion.so/d1e86fcdf21e4deda16eedab2b3361fb?v=503f44740b8b44a99a961aa96e9e46e1">
+            <img src="https://img.shields.io/website?label=roadmap&style=flat-square&url=https://www.notion.so/d1e86fcdf21e4deda16eedab2b3361fb?v=503f44740b8b44a99a961aa96e9e46e1" alt="roadmap">
+          </a>
+          <!-- PyPI -->
+          <a href="https://pypi.org/project/river">
+            <img src="https://img.shields.io/pypi/v/river.svg?label=release&color=blue&style=flat-square" alt="pypi">
+          </a>
+          <!-- PePy -->
+          <a href="https://pepy.tech/project/river">
+            <img src="https://static.pepy.tech/badge/river?style=flat-square" alt="pepy">
+          </a>
+          <!-- License -->
+          <a href="https://opensource.org/licenses/BSD-3-Clause">
+            <img src="https://img.shields.io/badge/License-BSD%203--Clause-blue.svg?style=flat-square" alt="bsd_3_license">
+          </a>
+        </p>
+        
+        </br>
+        
+        <p align="center">
+          River is a Python library for <a href="https://www.wikiwand.com/en/Online_machine_learning">online machine learning</a>. It is the result of a merger between <a href="https://github.com/MaxHalford/creme">creme</a> and <a href="https://github.com/scikit-multiflow/scikit-multiflow">scikit-multiflow</a>. River's ambition is to be the go-to library for doing machine learning on streaming data.
+        </p>
+        
+        ## ⚡️ Quickstart
+        
+        As a quick example, we'll train a logistic regression to classify the [website phishing dataset](http://archive.ics.uci.edu/ml/datasets/Website+Phishing). Here's a look at the first observation in the dataset.
+        
+        ```python
+        >>> from pprint import pprint
+        >>> from river import datasets
+        
+        >>> dataset = datasets.Phishing()
+        
+        >>> for x, y in dataset:
+        ...     pprint(x)
+        ...     print(y)
+        ...     break
+        {'age_of_domain': 1,
+         'anchor_from_other_domain': 0.0,
+         'empty_server_form_handler': 0.0,
+         'https': 0.0,
+         'ip_in_url': 1,
+         'is_popular': 0.5,
+         'long_url': 1.0,
+         'popup_window': 0.0,
+         'request_from_other_domain': 0.0}
+        True
+        
+        ```
+        
+        Now let's run the model on the dataset in a streaming fashion. We sequentially interleave predictions and model updates. Meanwhile, we update a performance metric to see how well the model is doing.
+        
+        ```python
+        >>> from river import compose
+        >>> from river import linear_model
+        >>> from river import metrics
+        >>> from river import preprocessing
+        
+        >>> model = compose.Pipeline(
+        ...     preprocessing.StandardScaler(),
+        ...     linear_model.LogisticRegression()
+        ... )
+        
+        >>> metric = metrics.Accuracy()
+        
+        >>> for x, y in dataset:
+        ...     y_pred = model.predict_one(x)      # make a prediction
+        ...     metric = metric.update(y, y_pred)  # update the metric
+        ...     model = model.learn_one(x, y)      # make the model learn
+        
+        >>> metric
+        Accuracy: 89.20%
+        
+        ```
+        
+        ## 🛠 Installation
+        
+        River is intended to work with **Python 3.6 or above**. Installation can be done with `pip`:
+        
+        ```sh
+        pip install river
+        ```
+        
+        There are [wheels available](https://pypi.org/project/river/#files) for Linux, MacOS, and Windows, which means that you most probably won't have to build River from source.
+        
+        You can install the latest development version from GitHub as so:
+        
+        ```sh
+        pip install git+https://github.com/online-ml/river --upgrade
+        ```
+        
+        Or, through SSH:
+        
+        ```sh
+        pip install git+ssh://git@github.com/online-ml/river.git --upgrade
+        ```
+        
+        ## 🧠 Philosophy
+        
+        Machine learning is often done in a batch setting, whereby a model is fitted to a dataset in one go. This results in a static model which has to be retrained in order to learn from new data. In many cases, this isn't elegant nor efficient, and usually incurs [a fair amount of technical debt](https://research.google/pubs/pub43146/). Indeed, if you're using a batch model, then you need to think about maintaining a training set, monitoring real-time performance, model retraining, etc.
+        
+        With River, we encourage a different approach, which is to continuously learn a stream of data. This means that the model process one observation at a time, and can therefore be updated on the fly. This allows to learn from massive datasets that don't fit in main memory. Online machine learning also integrates nicely in cases where new data is constantly arriving. It shines in many use cases, such as time series forecasting, spam filtering, recommender systems, CTR prediction, and IoT applications. If you're bored with retraining models and want to instead build dynamic models, then online machine learning (and therefore River!) might be what you're looking for.
+        
+        Here are some benefits of using River (and online machine learning in general):
+        
+        - **Incremental**: models can update themselves in real-time.
+        - **Adaptive**: models can adapt to [concept drift](https://www.wikiwand.com/en/Concept_drift).
+        - **Production-ready**: working with data streams makes it simple to replicate production scenarios during model development.
+        - **Efficient**: models don't have to be retrained and require little compute power, which [lowers their carbon footprint](https://arxiv.org/abs/1907.10597)
+        - **Fast**: when the goal is to learn and predict with a single instance at a time, then River is an order of magnitude faster than PyTorch, Tensorflow, and scikit-learn.
+        
+        ## 🔥 Features
+        
+        - Linear models with a wide array of optimizers
+        - Nearest neighbors, decision trees, naïve Bayes
+        - [Progressive model validation](https://hunch.net/~jl/projects/prediction_bounds/progressive_validation/coltfinal.pdf)
+        - Model pipelines as a first-class citizen
+        - Anomaly detection
+        - Recommender systems
+        - Time series forecasting
+        - Imbalanced learning
+        - Clustering
+        - Feature extraction and selection
+        - Online statistics and metrics
+        - Built-in datasets
+        - And [much more](https://riverml.xyz/latest/api/overview/)
+        
+        ## 🔗 Useful links
+        
+        - [Documentation](https://riverml.xyz)
+        - [Benchmarks](https://github.com/online-ml/river/tree/main/benchmarks)
+        - [Issue tracker](https://github.com/online-ml/river/issues)
+        - [Package releases](https://pypi.org/project/river/#history)
+        
+        ## 👁️ Media
+        
+        - PyData Amsterdam 2019 presentation ([slides](https://maxhalford.github.io/slides/creme-pydata), [video](https://www.youtube.com/watch?v=P3M6dt7bY9U&list=PLGVZCDnMOq0q7_6SdrC2wRtdkojGBTAht&index=11))
+        - [Toulouse Data Science Meetup presentation](https://maxhalford.github.io/slides/creme-tds)
+        - [Machine learning for streaming data with creme](https://towardsdatascience.com/machine-learning-for-streaming-data-with-creme-dacf5fb469df)
+        - [Hong Kong Data Science Meetup presentation](https://maxhalford.github.io/slides/hkml2020.pdf)
+        
+        ## 👍 Contributing
+        
+        Feel free to contribute in any way you like, we're always open to new ideas and approaches.
+        
+        There are three ways for users to get involved:
+        
+        - [Issue tracker](https://github.com/online-ml/river/issues): this place is meant to report bugs, request for minor features, or small improvements. Issues should be short-lived and solved as fast as possible.
+        - [Discussions](https://github.com/online-ml/river/discussions): you can ask for new features, submit your questions and get help, propose new ideas, or even show the community what you are achieving with River! If you have a new technique or want to port a new functionality to River, this is the place to discuss.
+        - [Roadmap](https://www.notion.so/d1e86fcdf21e4deda16eedab2b3361fb?v=503f44740b8b44a99a961aa96e9e46e1): you can check what we are doing, what are the next planned milestones for River, and look for cool ideas that still need someone to make them become a reality!
+        
+        Please check out the [contribution guidelines](https://github.com/online-ml/river/blob/main/CONTRIBUTING.md) if you want to bring modifications to the code base. You can view the list of people who have contributed [here](https://github.com/online-ml/river/graphs/contributors).
+        
+        ## ❤️ They've used us
+        
+        These are companies that we know have been using River, be it in production or for prototyping.
+        
+        <p align="center">
+          <img width="70%" src="https://docs.google.com/drawings/d/e/2PACX-1vQbCUQkTU74dBf411r4nDl4udmqOEbLqzRtokUC-N7JDJUA7BGTfnMGmiMNqbcSuOaWAmazp1rFGwDC/pub?w=1194&h=567" alt="companies">
+        </p>
+        
+        Feel welcome to get in touch if you want us to add your company logo!
+        
+        ## 🤝 Affiliations
+        
+        **Sponsors**
+        
+        <p align="center">
+          <img width="55%" src="https://docs.google.com/drawings/d/e/2PACX-1vSagEhWAjDsb0c24En_fhWAf9DJZbyh5YjU7lK0sNowD2m9uv9TuFm-U77k6ObqTyN2mP05Avf6TCJc/pub?w=2073&h=1127" alt="sponsors">
+        </p>
+        
+        **Collaborating institutions and groups**
+        
+        <p align="center">
+          <img width="55%" src="https://docs.google.com/drawings/d/e/2PACX-1vQB0C8YgnkCt_3C3cp-Csaw8NLZUwishdbJFB3iSbBPUD0AxEVS9AlF-Rs5PJq8UVRzRtFwZIOucuXj/pub?w=1442&h=489" alt="collaborations">
+        </p>
+        
+        ## 💬 Citation
+        
+        If `river` has been useful for your research and you would like to cite it in an scientific publication, please refer to this [paper](https://arxiv.org/abs/2012.04740):
+        
+        ```bibtex
+        @misc{2020river,
+              title={River: machine learning for streaming data in Python},
+              author={Jacob Montiel and Max Halford and Saulo Martiello Mastelini
+                      and Geoffrey Bolmier and Raphael Sourty and Robin Vaysse
+                      and Adil Zouitine and Heitor Murilo Gomes and Jesse Read
+                      and Talel Abdessalem and Albert Bifet},
+              year={2020},
+              eprint={2012.04740},
+              archivePrefix={arXiv},
+              primaryClass={cs.LG}
+        }
+        ```
+        
+        ## 📝 License
+        
+        River is free and open-source software licensed under the [3-clause BSD license](https://github.com/online-ml/river/blob/main/LICENSE).
+        
+Platform: UNKNOWN
+Classifier: License :: OSI Approved :: BSD License
+Classifier: Programming Language :: Python
+Classifier: Programming Language :: Python :: 3
+Classifier: Programming Language :: Python :: 3.6
+Classifier: Programming Language :: Python :: 3.7
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: Implementation :: CPython
+Classifier: Programming Language :: Python :: Implementation :: PyPy
+Requires-Python: >=3.7.0
+Description-Content-Type: text/markdown
+Provides-Extra: dev
+Provides-Extra: compat
+Provides-Extra: docs
+Provides-Extra: extra
```

#### html2text {}

```diff
@@ -1,12 +1,12 @@
-Metadata-Version: 2.1 Name: river Version: 0.8.0 Summary: Online machine
+Metadata-Version: 2.1 Name: river Version: 0.9.0 Summary: Online machine
 learning in Python Home-page: https://github.com/online-ml/river Author: Max
 Halford Author-email: maxhalford25@gmail.com License: BSD-3 Description:
                                  [river_logo]
-      _[_t_e_s_t_s_]_[_h_t_t_p_s_:_/_/_c_o_d_e_c_o_v_._i_o_/_g_h_/_o_n_l_i_n_e_-_m_l_/_r_i_v_e_r_/_b_r_a_n_c_h_/_m_a_s_t_e_r_/_g_r_a_p_h_/
+       _[_t_e_s_t_s_]_[_h_t_t_p_s_:_/_/_c_o_d_e_c_o_v_._i_o_/_g_h_/_o_n_l_i_n_e_-_m_l_/_r_i_v_e_r_/_b_r_a_n_c_h_/_m_a_i_n_/_g_r_a_p_h_/
 _b_a_d_g_e_._s_v_g_?_t_o_k_e_n_=_l_u_K_6_e_F_o_M_a_9_]_[_d_o_c_u_m_e_n_t_a_t_i_o_n_]_[_r_o_a_d_m_a_p_]_[_p_y_p_i_]_[_p_e_p_y_]_[_b_s_d___3___l_i_c_e_n_s_e_]
  River is a Python library for _o_n_l_i_n_e_ _m_a_c_h_i_n_e_ _l_e_a_r_n_i_n_g. It is the result of a
 merger between _c_r_e_m_e and _s_c_i_k_i_t_-_m_u_l_t_i_f_l_o_w. River's ambition is to be the go-to
              library for doing machine learning on streaming data.
 ## â¡ï¸ Quickstart As a quick example, we'll train a logistic regression to
 classify the [website phishing dataset](http://archive.ics.uci.edu/ml/datasets/
 Website+Phishing). Here's a look at the first observation in the dataset.
@@ -60,17 +60,17 @@
 trees, naÃ¯ve Bayes - [Progressive model validation](https://hunch.net/~jl/
 projects/prediction_bounds/progressive_validation/coltfinal.pdf) - Model
 pipelines as a first-class citizen - Anomaly detection - Recommender systems -
 Time series forecasting - Imbalanced learning - Clustering - Feature extraction
 and selection - Online statistics and metrics - Built-in datasets - And [much
 more](https://riverml.xyz/latest/api/overview/) ## ð Useful links -
 [Documentation](https://riverml.xyz) - [Benchmarks](https://github.com/online-
-ml/river/tree/master/benchmarks) - [Issue tracker](https://github.com/online-
-ml/river/issues) - [Package releases](https://pypi.org/project/river/#history)
-## ðï¸ Media - PyData Amsterdam 2019 presentation ([slides](https://
+ml/river/tree/main/benchmarks) - [Issue tracker](https://github.com/online-ml/
+river/issues) - [Package releases](https://pypi.org/project/river/#history) ##
+ðï¸ Media - PyData Amsterdam 2019 presentation ([slides](https://
 maxhalford.github.io/slides/creme-pydata), [video](https://www.youtube.com/
 watch?v=P3M6dt7bY9U&list=PLGVZCDnMOq0q7_6SdrC2wRtdkojGBTAht&index=11)) -
 [Toulouse Data Science Meetup presentation](https://maxhalford.github.io/
 slides/creme-tds) - [Machine learning for streaming data with creme](https://
 towardsdatascience.com/machine-learning-for-streaming-data-with-creme-
 dacf5fb469df) - [Hong Kong Data Science Meetup presentation](https://
 maxhalford.github.io/slides/hkml2020.pdf) ## ð Contributing Feel free to
@@ -83,16 +83,16 @@
 propose new ideas, or even show the community what you are achieving with
 River! If you have a new technique or want to port a new functionality to
 River, this is the place to discuss. - [Roadmap](https://www.notion.so/
 d1e86fcdf21e4deda16eedab2b3361fb?v=503f44740b8b44a99a961aa96e9e46e1): you can
 check what we are doing, what are the next planned milestones for River, and
 look for cool ideas that still need someone to make them become a reality!
 Please check out the [contribution guidelines](https://github.com/online-ml/
-river/blob/master/CONTRIBUTING.md) if you want to bring modifications to the
-code base. You can view the list of people who have contributed [here](https://
+river/blob/main/CONTRIBUTING.md) if you want to bring modifications to the code
+base. You can view the list of people who have contributed [here](https://
 github.com/online-ml/river/graphs/contributors). ## â¤ï¸ They've used us
 These are companies that we know have been using River, be it in production or
 for prototyping.
                                   [companies]
 Feel welcome to get in touch if you want us to add your company logo! ## ð¤
 Affiliations **Sponsors**
                                   [sponsors]
@@ -103,16 +103,16 @@
 (https://arxiv.org/abs/2012.04740): ```bibtex @misc{2020river, title={River:
 machine learning for streaming data in Python}, author={Jacob Montiel and Max
 Halford and Saulo Martiello Mastelini and Geoffrey Bolmier and Raphael Sourty
 and Robin Vaysse and Adil Zouitine and Heitor Murilo Gomes and Jesse Read and
 Talel Abdessalem and Albert Bifet}, year={2020}, eprint={2012.04740},
 archivePrefix={arXiv}, primaryClass={cs.LG} } ``` ## ð License River is free
 and open-source software licensed under the [3-clause BSD license](https://
-github.com/online-ml/river/blob/master/LICENSE). Platform: UNKNOWN Classifier:
+github.com/online-ml/river/blob/main/LICENSE). Platform: UNKNOWN Classifier:
 License :: OSI Approved :: BSD License Classifier: Programming Language ::
 Python Classifier: Programming Language :: Python :: 3 Classifier: Programming
 Language :: Python :: 3.6 Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8 Classifier: Programming
 Language :: Python :: Implementation :: CPython Classifier: Programming
-Language :: Python :: Implementation :: PyPy Requires-Python: >=3.6.0
+Language :: Python :: Implementation :: PyPy Requires-Python: >=3.7.0
 Description-Content-Type: text/markdown Provides-Extra: dev Provides-Extra:
-compat Provides-Extra: docs
+compat Provides-Extra: docs Provides-Extra: extra
```

### Comparing `river-0.8.0/README.md` & `river-0.9.0/README.md`

 * *Files 9% similar despite different names*

```diff
@@ -1,220 +1,220 @@
-</br>
-
-<p align="center">
-  <img height="80px" src="docs/img/logo.svg" alt="river_logo">
-</p>
-
-</br>
-
-<p align="center">
-  <!-- Tests -->
-  <a href="https://github.com/online-ml/river/actions?query=workflow%3Atests+branch%3Amaster">
-    <img src="https://github.com/online-ml/river/workflows/tests/badge.svg?branch=master" alt="tests">
-  </a>
-  <!-- Code coverage -->
-  <a href="https://codecov.io/gh/online-ml/river">
-    <img src="https://codecov.io/gh/online-ml/river/branch/master/graph/badge.svg?token=luK6eFoMa9"/>
-  </a>
-  <!-- Documentation -->
-  <a href="https://riverml.xyz">
-    <img src="https://img.shields.io/website?label=docs&style=flat-square&url=https%3A%2F%2Friverml.xyz%2F" alt="documentation">
-  </a>
-  <!-- Roadmap -->
-  <a href="https://www.notion.so/d1e86fcdf21e4deda16eedab2b3361fb?v=503f44740b8b44a99a961aa96e9e46e1">
-    <img src="https://img.shields.io/website?label=roadmap&style=flat-square&url=https://www.notion.so/d1e86fcdf21e4deda16eedab2b3361fb?v=503f44740b8b44a99a961aa96e9e46e1" alt="roadmap">
-  </a>
-  <!-- PyPI -->
-  <a href="https://pypi.org/project/river">
-    <img src="https://img.shields.io/pypi/v/river.svg?label=release&color=blue&style=flat-square" alt="pypi">
-  </a>
-  <!-- PePy -->
-  <a href="https://pepy.tech/project/river">
-    <img src="https://static.pepy.tech/badge/river?style=flat-square" alt="pepy">
-  </a>
-  <!-- License -->
-  <a href="https://opensource.org/licenses/BSD-3-Clause">
-    <img src="https://img.shields.io/badge/License-BSD%203--Clause-blue.svg?style=flat-square" alt="bsd_3_license">
-  </a>
-</p>
-
-</br>
-
-<p align="center">
-  River is a Python library for <a href="https://www.wikiwand.com/en/Online_machine_learning">online machine learning</a>. It is the result of a merger between <a href="https://github.com/MaxHalford/creme">creme</a> and <a href="https://github.com/scikit-multiflow/scikit-multiflow">scikit-multiflow</a>. River's ambition is to be the go-to library for doing machine learning on streaming data.
-</p>
-
-## ⚡️ Quickstart
-
-As a quick example, we'll train a logistic regression to classify the [website phishing dataset](http://archive.ics.uci.edu/ml/datasets/Website+Phishing). Here's a look at the first observation in the dataset.
-
-```python
->>> from pprint import pprint
->>> from river import datasets
-
->>> dataset = datasets.Phishing()
-
->>> for x, y in dataset:
-...     pprint(x)
-...     print(y)
-...     break
-{'age_of_domain': 1,
- 'anchor_from_other_domain': 0.0,
- 'empty_server_form_handler': 0.0,
- 'https': 0.0,
- 'ip_in_url': 1,
- 'is_popular': 0.5,
- 'long_url': 1.0,
- 'popup_window': 0.0,
- 'request_from_other_domain': 0.0}
-True
-
-```
-
-Now let's run the model on the dataset in a streaming fashion. We sequentially interleave predictions and model updates. Meanwhile, we update a performance metric to see how well the model is doing.
-
-```python
->>> from river import compose
->>> from river import linear_model
->>> from river import metrics
->>> from river import preprocessing
-
->>> model = compose.Pipeline(
-...     preprocessing.StandardScaler(),
-...     linear_model.LogisticRegression()
-... )
-
->>> metric = metrics.Accuracy()
-
->>> for x, y in dataset:
-...     y_pred = model.predict_one(x)      # make a prediction
-...     metric = metric.update(y, y_pred)  # update the metric
-...     model = model.learn_one(x, y)      # make the model learn
-
->>> metric
-Accuracy: 89.20%
-
-```
-
-## 🛠 Installation
-
-River is intended to work with **Python 3.6 or above**. Installation can be done with `pip`:
-
-```sh
-pip install river
-```
-
-There are [wheels available](https://pypi.org/project/river/#files) for Linux, MacOS, and Windows, which means that you most probably won't have to build River from source.
-
-You can install the latest development version from GitHub as so:
-
-```sh
-pip install git+https://github.com/online-ml/river --upgrade
-```
-
-Or, through SSH:
-
-```sh
-pip install git+ssh://git@github.com/online-ml/river.git --upgrade
-```
-
-## 🧠 Philosophy
-
-Machine learning is often done in a batch setting, whereby a model is fitted to a dataset in one go. This results in a static model which has to be retrained in order to learn from new data. In many cases, this isn't elegant nor efficient, and usually incurs [a fair amount of technical debt](https://research.google/pubs/pub43146/). Indeed, if you're using a batch model, then you need to think about maintaining a training set, monitoring real-time performance, model retraining, etc.
-
-With River, we encourage a different approach, which is to continuously learn a stream of data. This means that the model process one observation at a time, and can therefore be updated on the fly. This allows to learn from massive datasets that don't fit in main memory. Online machine learning also integrates nicely in cases where new data is constantly arriving. It shines in many use cases, such as time series forecasting, spam filtering, recommender systems, CTR prediction, and IoT applications. If you're bored with retraining models and want to instead build dynamic models, then online machine learning (and therefore River!) might be what you're looking for.
-
-Here are some benefits of using River (and online machine learning in general):
-
-- **Incremental**: models can update themselves in real-time.
-- **Adaptive**: models can adapt to [concept drift](https://www.wikiwand.com/en/Concept_drift).
-- **Production-ready**: working with data streams makes it simple to replicate production scenarios during model development.
-- **Efficient**: models don't have to be retrained and require little compute power, which [lowers their carbon footprint](https://arxiv.org/abs/1907.10597)
-- **Fast**: when the goal is to learn and predict with a single instance at a time, then River is an order of magnitude faster than PyTorch, Tensorflow, and scikit-learn.
-
-## 🔥 Features
-
-- Linear models with a wide array of optimizers
-- Nearest neighbors, decision trees, naïve Bayes
-- [Progressive model validation](https://hunch.net/~jl/projects/prediction_bounds/progressive_validation/coltfinal.pdf)
-- Model pipelines as a first-class citizen
-- Anomaly detection
-- Recommender systems
-- Time series forecasting
-- Imbalanced learning
-- Clustering
-- Feature extraction and selection
-- Online statistics and metrics
-- Built-in datasets
-- And [much more](https://riverml.xyz/latest/api/overview/)
-
-## 🔗 Useful links
-
-- [Documentation](https://riverml.xyz)
-- [Benchmarks](https://github.com/online-ml/river/tree/master/benchmarks)
-- [Issue tracker](https://github.com/online-ml/river/issues)
-- [Package releases](https://pypi.org/project/river/#history)
-
-## 👁️ Media
-
-- PyData Amsterdam 2019 presentation ([slides](https://maxhalford.github.io/slides/creme-pydata), [video](https://www.youtube.com/watch?v=P3M6dt7bY9U&list=PLGVZCDnMOq0q7_6SdrC2wRtdkojGBTAht&index=11))
-- [Toulouse Data Science Meetup presentation](https://maxhalford.github.io/slides/creme-tds)
-- [Machine learning for streaming data with creme](https://towardsdatascience.com/machine-learning-for-streaming-data-with-creme-dacf5fb469df)
-- [Hong Kong Data Science Meetup presentation](https://maxhalford.github.io/slides/hkml2020.pdf)
-
-## 👍 Contributing
-
-Feel free to contribute in any way you like, we're always open to new ideas and approaches.
-
-There are three ways for users to get involved:
-
-- [Issue tracker](https://github.com/online-ml/river/issues): this place is meant to report bugs, request for minor features, or small improvements. Issues should be short-lived and solved as fast as possible.
-- [Discussions](https://github.com/online-ml/river/discussions): you can ask for new features, submit your questions and get help, propose new ideas, or even show the community what you are achieving with River! If you have a new technique or want to port a new functionality to River, this is the place to discuss.
-- [Roadmap](https://www.notion.so/d1e86fcdf21e4deda16eedab2b3361fb?v=503f44740b8b44a99a961aa96e9e46e1): you can check what we are doing, what are the next planned milestones for River, and look for cool ideas that still need someone to make them become a reality!
-
-Please check out the [contribution guidelines](https://github.com/online-ml/river/blob/master/CONTRIBUTING.md) if you want to bring modifications to the code base. You can view the list of people who have contributed [here](https://github.com/online-ml/river/graphs/contributors).
-
-## ❤️ They've used us
-
-These are companies that we know have been using River, be it in production or for prototyping.
-
-<p align="center">
-  <img width="70%" src="https://docs.google.com/drawings/d/e/2PACX-1vQbCUQkTU74dBf411r4nDl4udmqOEbLqzRtokUC-N7JDJUA7BGTfnMGmiMNqbcSuOaWAmazp1rFGwDC/pub?w=1194&h=567" alt="companies">
-</p>
-
-Feel welcome to get in touch if you want us to add your company logo!
-
-## 🤝 Affiliations
-
-**Sponsors**
-
-<p align="center">
-  <img width="55%" src="https://docs.google.com/drawings/d/e/2PACX-1vSagEhWAjDsb0c24En_fhWAf9DJZbyh5YjU7lK0sNowD2m9uv9TuFm-U77k6ObqTyN2mP05Avf6TCJc/pub?w=2073&h=1127" alt="sponsors">
-</p>
-
-**Collaborating institutions and groups**
-
-<p align="center">
-  <img width="55%" src="https://docs.google.com/drawings/d/e/2PACX-1vQB0C8YgnkCt_3C3cp-Csaw8NLZUwishdbJFB3iSbBPUD0AxEVS9AlF-Rs5PJq8UVRzRtFwZIOucuXj/pub?w=1442&h=489" alt="collaborations">
-</p>
-
-## 💬 Citation
-
-If `river` has been useful for your research and you would like to cite it in an scientific publication, please refer to this [paper](https://arxiv.org/abs/2012.04740):
-
-```bibtex
-@misc{2020river,
-      title={River: machine learning for streaming data in Python},
-      author={Jacob Montiel and Max Halford and Saulo Martiello Mastelini
-              and Geoffrey Bolmier and Raphael Sourty and Robin Vaysse
-              and Adil Zouitine and Heitor Murilo Gomes and Jesse Read
-              and Talel Abdessalem and Albert Bifet},
-      year={2020},
-      eprint={2012.04740},
-      archivePrefix={arXiv},
-      primaryClass={cs.LG}
-}
-```
-
-## 📝 License
-
-River is free and open-source software licensed under the [3-clause BSD license](https://github.com/online-ml/river/blob/master/LICENSE).
+</br>
+
+<p align="center">
+  <img height="80px" src="docs/img/logo.svg" alt="river_logo">
+</p>
+
+</br>
+
+<p align="center">
+  <!-- Tests -->
+  <a href="https://github.com/online-ml/river/actions/workflows/unit-tests.yml">
+    <img src="https://github.com/online-ml/river/actions/workflows/unit-tests.yml/badge.svg" alt="tests">
+  </a>
+  <!-- Code coverage -->
+  <a href="https://codecov.io/gh/online-ml/river">
+    <img src="https://codecov.io/gh/online-ml/river/branch/main/graph/badge.svg?token=luK6eFoMa9"/>
+  </a>
+  <!-- Documentation -->
+  <a href="https://riverml.xyz">
+    <img src="https://img.shields.io/website?label=docs&style=flat-square&url=https%3A%2F%2Friverml.xyz%2F" alt="documentation">
+  </a>
+  <!-- Roadmap -->
+  <a href="https://www.notion.so/d1e86fcdf21e4deda16eedab2b3361fb?v=503f44740b8b44a99a961aa96e9e46e1">
+    <img src="https://img.shields.io/website?label=roadmap&style=flat-square&url=https://www.notion.so/d1e86fcdf21e4deda16eedab2b3361fb?v=503f44740b8b44a99a961aa96e9e46e1" alt="roadmap">
+  </a>
+  <!-- PyPI -->
+  <a href="https://pypi.org/project/river">
+    <img src="https://img.shields.io/pypi/v/river.svg?label=release&color=blue&style=flat-square" alt="pypi">
+  </a>
+  <!-- PePy -->
+  <a href="https://pepy.tech/project/river">
+    <img src="https://static.pepy.tech/badge/river?style=flat-square" alt="pepy">
+  </a>
+  <!-- License -->
+  <a href="https://opensource.org/licenses/BSD-3-Clause">
+    <img src="https://img.shields.io/badge/License-BSD%203--Clause-blue.svg?style=flat-square" alt="bsd_3_license">
+  </a>
+</p>
+
+</br>
+
+<p align="center">
+  River is a Python library for <a href="https://www.wikiwand.com/en/Online_machine_learning">online machine learning</a>. It is the result of a merger between <a href="https://github.com/MaxHalford/creme">creme</a> and <a href="https://github.com/scikit-multiflow/scikit-multiflow">scikit-multiflow</a>. River's ambition is to be the go-to library for doing machine learning on streaming data.
+</p>
+
+## ⚡️ Quickstart
+
+As a quick example, we'll train a logistic regression to classify the [website phishing dataset](http://archive.ics.uci.edu/ml/datasets/Website+Phishing). Here's a look at the first observation in the dataset.
+
+```python
+>>> from pprint import pprint
+>>> from river import datasets
+
+>>> dataset = datasets.Phishing()
+
+>>> for x, y in dataset:
+...     pprint(x)
+...     print(y)
+...     break
+{'age_of_domain': 1,
+ 'anchor_from_other_domain': 0.0,
+ 'empty_server_form_handler': 0.0,
+ 'https': 0.0,
+ 'ip_in_url': 1,
+ 'is_popular': 0.5,
+ 'long_url': 1.0,
+ 'popup_window': 0.0,
+ 'request_from_other_domain': 0.0}
+True
+
+```
+
+Now let's run the model on the dataset in a streaming fashion. We sequentially interleave predictions and model updates. Meanwhile, we update a performance metric to see how well the model is doing.
+
+```python
+>>> from river import compose
+>>> from river import linear_model
+>>> from river import metrics
+>>> from river import preprocessing
+
+>>> model = compose.Pipeline(
+...     preprocessing.StandardScaler(),
+...     linear_model.LogisticRegression()
+... )
+
+>>> metric = metrics.Accuracy()
+
+>>> for x, y in dataset:
+...     y_pred = model.predict_one(x)      # make a prediction
+...     metric = metric.update(y, y_pred)  # update the metric
+...     model = model.learn_one(x, y)      # make the model learn
+
+>>> metric
+Accuracy: 89.20%
+
+```
+
+## 🛠 Installation
+
+River is intended to work with **Python 3.6 or above**. Installation can be done with `pip`:
+
+```sh
+pip install river
+```
+
+There are [wheels available](https://pypi.org/project/river/#files) for Linux, MacOS, and Windows, which means that you most probably won't have to build River from source.
+
+You can install the latest development version from GitHub as so:
+
+```sh
+pip install git+https://github.com/online-ml/river --upgrade
+```
+
+Or, through SSH:
+
+```sh
+pip install git+ssh://git@github.com/online-ml/river.git --upgrade
+```
+
+## 🧠 Philosophy
+
+Machine learning is often done in a batch setting, whereby a model is fitted to a dataset in one go. This results in a static model which has to be retrained in order to learn from new data. In many cases, this isn't elegant nor efficient, and usually incurs [a fair amount of technical debt](https://research.google/pubs/pub43146/). Indeed, if you're using a batch model, then you need to think about maintaining a training set, monitoring real-time performance, model retraining, etc.
+
+With River, we encourage a different approach, which is to continuously learn a stream of data. This means that the model process one observation at a time, and can therefore be updated on the fly. This allows to learn from massive datasets that don't fit in main memory. Online machine learning also integrates nicely in cases where new data is constantly arriving. It shines in many use cases, such as time series forecasting, spam filtering, recommender systems, CTR prediction, and IoT applications. If you're bored with retraining models and want to instead build dynamic models, then online machine learning (and therefore River!) might be what you're looking for.
+
+Here are some benefits of using River (and online machine learning in general):
+
+- **Incremental**: models can update themselves in real-time.
+- **Adaptive**: models can adapt to [concept drift](https://www.wikiwand.com/en/Concept_drift).
+- **Production-ready**: working with data streams makes it simple to replicate production scenarios during model development.
+- **Efficient**: models don't have to be retrained and require little compute power, which [lowers their carbon footprint](https://arxiv.org/abs/1907.10597)
+- **Fast**: when the goal is to learn and predict with a single instance at a time, then River is an order of magnitude faster than PyTorch, Tensorflow, and scikit-learn.
+
+## 🔥 Features
+
+- Linear models with a wide array of optimizers
+- Nearest neighbors, decision trees, naïve Bayes
+- [Progressive model validation](https://hunch.net/~jl/projects/prediction_bounds/progressive_validation/coltfinal.pdf)
+- Model pipelines as a first-class citizen
+- Anomaly detection
+- Recommender systems
+- Time series forecasting
+- Imbalanced learning
+- Clustering
+- Feature extraction and selection
+- Online statistics and metrics
+- Built-in datasets
+- And [much more](https://riverml.xyz/latest/api/overview/)
+
+## 🔗 Useful links
+
+- [Documentation](https://riverml.xyz)
+- [Benchmarks](https://github.com/online-ml/river/tree/main/benchmarks)
+- [Issue tracker](https://github.com/online-ml/river/issues)
+- [Package releases](https://pypi.org/project/river/#history)
+
+## 👁️ Media
+
+- PyData Amsterdam 2019 presentation ([slides](https://maxhalford.github.io/slides/creme-pydata), [video](https://www.youtube.com/watch?v=P3M6dt7bY9U&list=PLGVZCDnMOq0q7_6SdrC2wRtdkojGBTAht&index=11))
+- [Toulouse Data Science Meetup presentation](https://maxhalford.github.io/slides/creme-tds)
+- [Machine learning for streaming data with creme](https://towardsdatascience.com/machine-learning-for-streaming-data-with-creme-dacf5fb469df)
+- [Hong Kong Data Science Meetup presentation](https://maxhalford.github.io/slides/hkml2020.pdf)
+
+## 👍 Contributing
+
+Feel free to contribute in any way you like, we're always open to new ideas and approaches.
+
+There are three ways for users to get involved:
+
+- [Issue tracker](https://github.com/online-ml/river/issues): this place is meant to report bugs, request for minor features, or small improvements. Issues should be short-lived and solved as fast as possible.
+- [Discussions](https://github.com/online-ml/river/discussions): you can ask for new features, submit your questions and get help, propose new ideas, or even show the community what you are achieving with River! If you have a new technique or want to port a new functionality to River, this is the place to discuss.
+- [Roadmap](https://www.notion.so/d1e86fcdf21e4deda16eedab2b3361fb?v=503f44740b8b44a99a961aa96e9e46e1): you can check what we are doing, what are the next planned milestones for River, and look for cool ideas that still need someone to make them become a reality!
+
+Please check out the [contribution guidelines](https://github.com/online-ml/river/blob/main/CONTRIBUTING.md) if you want to bring modifications to the code base. You can view the list of people who have contributed [here](https://github.com/online-ml/river/graphs/contributors).
+
+## ❤️ They've used us
+
+These are companies that we know have been using River, be it in production or for prototyping.
+
+<p align="center">
+  <img width="70%" src="https://docs.google.com/drawings/d/e/2PACX-1vQbCUQkTU74dBf411r4nDl4udmqOEbLqzRtokUC-N7JDJUA7BGTfnMGmiMNqbcSuOaWAmazp1rFGwDC/pub?w=1194&h=567" alt="companies">
+</p>
+
+Feel welcome to get in touch if you want us to add your company logo!
+
+## 🤝 Affiliations
+
+**Sponsors**
+
+<p align="center">
+  <img width="55%" src="https://docs.google.com/drawings/d/e/2PACX-1vSagEhWAjDsb0c24En_fhWAf9DJZbyh5YjU7lK0sNowD2m9uv9TuFm-U77k6ObqTyN2mP05Avf6TCJc/pub?w=2073&h=1127" alt="sponsors">
+</p>
+
+**Collaborating institutions and groups**
+
+<p align="center">
+  <img width="55%" src="https://docs.google.com/drawings/d/e/2PACX-1vQB0C8YgnkCt_3C3cp-Csaw8NLZUwishdbJFB3iSbBPUD0AxEVS9AlF-Rs5PJq8UVRzRtFwZIOucuXj/pub?w=1442&h=489" alt="collaborations">
+</p>
+
+## 💬 Citation
+
+If `river` has been useful for your research and you would like to cite it in an scientific publication, please refer to this [paper](https://arxiv.org/abs/2012.04740):
+
+```bibtex
+@misc{2020river,
+      title={River: machine learning for streaming data in Python},
+      author={Jacob Montiel and Max Halford and Saulo Martiello Mastelini
+              and Geoffrey Bolmier and Raphael Sourty and Robin Vaysse
+              and Adil Zouitine and Heitor Murilo Gomes and Jesse Read
+              and Talel Abdessalem and Albert Bifet},
+      year={2020},
+      eprint={2012.04740},
+      archivePrefix={arXiv},
+      primaryClass={cs.LG}
+}
+```
+
+## 📝 License
+
+River is free and open-source software licensed under the [3-clause BSD license](https://github.com/online-ml/river/blob/main/LICENSE).
```

#### html2text {}

```diff
@@ -1,9 +1,9 @@
                                  [river_logo]
-      _[_t_e_s_t_s_]_[_h_t_t_p_s_:_/_/_c_o_d_e_c_o_v_._i_o_/_g_h_/_o_n_l_i_n_e_-_m_l_/_r_i_v_e_r_/_b_r_a_n_c_h_/_m_a_s_t_e_r_/_g_r_a_p_h_/
+       _[_t_e_s_t_s_]_[_h_t_t_p_s_:_/_/_c_o_d_e_c_o_v_._i_o_/_g_h_/_o_n_l_i_n_e_-_m_l_/_r_i_v_e_r_/_b_r_a_n_c_h_/_m_a_i_n_/_g_r_a_p_h_/
 _b_a_d_g_e_._s_v_g_?_t_o_k_e_n_=_l_u_K_6_e_F_o_M_a_9_]_[_d_o_c_u_m_e_n_t_a_t_i_o_n_]_[_r_o_a_d_m_a_p_]_[_p_y_p_i_]_[_p_e_p_y_]_[_b_s_d___3___l_i_c_e_n_s_e_]
  River is a Python library for _o_n_l_i_n_e_ _m_a_c_h_i_n_e_ _l_e_a_r_n_i_n_g. It is the result of a
 merger between _c_r_e_m_e and _s_c_i_k_i_t_-_m_u_l_t_i_f_l_o_w. River's ambition is to be the go-to
              library for doing machine learning on streaming data.
 ## â¡ï¸ Quickstart As a quick example, we'll train a logistic regression to
 classify the [website phishing dataset](http://archive.ics.uci.edu/ml/datasets/
 Website+Phishing). Here's a look at the first observation in the dataset.
@@ -57,17 +57,17 @@
 trees, naÃ¯ve Bayes - [Progressive model validation](https://hunch.net/~jl/
 projects/prediction_bounds/progressive_validation/coltfinal.pdf) - Model
 pipelines as a first-class citizen - Anomaly detection - Recommender systems -
 Time series forecasting - Imbalanced learning - Clustering - Feature extraction
 and selection - Online statistics and metrics - Built-in datasets - And [much
 more](https://riverml.xyz/latest/api/overview/) ## ð Useful links -
 [Documentation](https://riverml.xyz) - [Benchmarks](https://github.com/online-
-ml/river/tree/master/benchmarks) - [Issue tracker](https://github.com/online-
-ml/river/issues) - [Package releases](https://pypi.org/project/river/#history)
-## ðï¸ Media - PyData Amsterdam 2019 presentation ([slides](https://
+ml/river/tree/main/benchmarks) - [Issue tracker](https://github.com/online-ml/
+river/issues) - [Package releases](https://pypi.org/project/river/#history) ##
+ðï¸ Media - PyData Amsterdam 2019 presentation ([slides](https://
 maxhalford.github.io/slides/creme-pydata), [video](https://www.youtube.com/
 watch?v=P3M6dt7bY9U&list=PLGVZCDnMOq0q7_6SdrC2wRtdkojGBTAht&index=11)) -
 [Toulouse Data Science Meetup presentation](https://maxhalford.github.io/
 slides/creme-tds) - [Machine learning for streaming data with creme](https://
 towardsdatascience.com/machine-learning-for-streaming-data-with-creme-
 dacf5fb469df) - [Hong Kong Data Science Meetup presentation](https://
 maxhalford.github.io/slides/hkml2020.pdf) ## ð Contributing Feel free to
@@ -80,16 +80,16 @@
 propose new ideas, or even show the community what you are achieving with
 River! If you have a new technique or want to port a new functionality to
 River, this is the place to discuss. - [Roadmap](https://www.notion.so/
 d1e86fcdf21e4deda16eedab2b3361fb?v=503f44740b8b44a99a961aa96e9e46e1): you can
 check what we are doing, what are the next planned milestones for River, and
 look for cool ideas that still need someone to make them become a reality!
 Please check out the [contribution guidelines](https://github.com/online-ml/
-river/blob/master/CONTRIBUTING.md) if you want to bring modifications to the
-code base. You can view the list of people who have contributed [here](https://
+river/blob/main/CONTRIBUTING.md) if you want to bring modifications to the code
+base. You can view the list of people who have contributed [here](https://
 github.com/online-ml/river/graphs/contributors). ## â¤ï¸ They've used us
 These are companies that we know have been using River, be it in production or
 for prototyping.
                                   [companies]
 Feel welcome to get in touch if you want us to add your company logo! ## ð¤
 Affiliations **Sponsors**
                                   [sponsors]
@@ -100,8 +100,8 @@
 (https://arxiv.org/abs/2012.04740): ```bibtex @misc{2020river, title={River:
 machine learning for streaming data in Python}, author={Jacob Montiel and Max
 Halford and Saulo Martiello Mastelini and Geoffrey Bolmier and Raphael Sourty
 and Robin Vaysse and Adil Zouitine and Heitor Murilo Gomes and Jesse Read and
 Talel Abdessalem and Albert Bifet}, year={2020}, eprint={2012.04740},
 archivePrefix={arXiv}, primaryClass={cs.LG} } ``` ## ð License River is free
 and open-source software licensed under the [3-clause BSD license](https://
-github.com/online-ml/river/blob/master/LICENSE).
+github.com/online-ml/river/blob/main/LICENSE).
```

### Comparing `river-0.8.0/river/base/__init__.py` & `river-0.9.0/river/base/__init__.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,44 +1,43 @@
-"""Base interfaces.
-
-Every estimator in `river` is a class, and as such inherits from at least one base interface.
-These are used to categorize, organize, and standardize the many estimators that `river`
-contains.
-
-This module contains mixin classes, which are all suffixed by `Mixin`. Their purpose is to
-provide additional functionality to an estimator, and thus need to be used in conjunction with a
-non-mixin base class.
-
-This module also contains utilities for type hinting and tagging estimators.
-
-"""
-from . import tags, typing
-from .anomaly import AnomalyDetector
-from .base import Base
-from .classifier import Classifier, MiniBatchClassifier
-from .clusterer import Clusterer
-from .drift_detector import DriftDetector
-from .ensemble import EnsembleMixin
-from .estimator import Estimator
-from .multi_output import MultiOutputMixin
-from .regressor import MiniBatchRegressor, Regressor
-from .transformer import SupervisedTransformer, Transformer
-from .wrapper import WrapperMixin
-
-__all__ = [
-    "AnomalyDetector",
-    "Base",
-    "Classifier",
-    "Clusterer",
-    "DriftDetector",
-    "EnsembleMixin",
-    "Estimator",
-    "MiniBatchClassifier",
-    "MiniBatchRegressor",
-    "MultiOutputMixin",
-    "Regressor",
-    "SupervisedTransformer",
-    "tags",
-    "Transformer",
-    "typing",
-    "WrapperMixin",
-]
+"""Base interfaces.
+
+Every estimator in `river` is a class, and as such inherits from at least one base interface.
+These are used to categorize, organize, and standardize the many estimators that `river`
+contains.
+
+This module contains mixin classes, which are all suffixed by `Mixin`. Their purpose is to
+provide additional functionality to an estimator, and thus need to be used in conjunction with a
+non-mixin base class.
+
+This module also contains utilities for type hinting and tagging estimators.
+
+"""
+from . import tags, typing
+from .base import Base
+from .classifier import Classifier, MiniBatchClassifier
+from .clusterer import Clusterer
+from .drift_detector import DriftDetector
+from .ensemble import Ensemble, WrapperEnsemble
+from .estimator import Estimator
+from .multi_output import MultiOutputMixin
+from .regressor import MiniBatchRegressor, Regressor
+from .transformer import SupervisedTransformer, Transformer
+from .wrapper import Wrapper
+
+__all__ = [
+    "Base",
+    "Classifier",
+    "Clusterer",
+    "DriftDetector",
+    "Ensemble",
+    "Estimator",
+    "MiniBatchClassifier",
+    "MiniBatchRegressor",
+    "MultiOutputMixin",
+    "Regressor",
+    "SupervisedTransformer",
+    "tags",
+    "Transformer",
+    "typing",
+    "WrapperEnsemble",
+    "Wrapper",
+]
```

### Comparing `river-0.8.0/river/base/anomaly.py` & `river-0.9.0/river/anomaly/base.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,44 +1,44 @@
-import abc
-
-from . import estimator
-
-
-class AnomalyDetector(estimator.Estimator):
-    """An anomaly detector."""
-
-    @property
-    def _supervised(self):
-        return False
-
-    @abc.abstractmethod
-    def learn_one(self, x: dict) -> "AnomalyDetector":
-        """Update the model.
-
-        Parameters
-        ----------
-        x
-            A dictionary of features.
-
-        Returns
-        -------
-        self
-
-        """
-
-    @abc.abstractmethod
-    def score_one(self, x: dict) -> float:
-        """Return an outlier score.
-
-        A high score is indicative of an anomaly. A low score corresponds a normal observation.
-
-        Parameters
-        ----------
-        x
-            A dictionary of features.
-
-        Returns
-        -------
-        An anomaly score. A high score is indicative of an anomaly. A low score corresponds a
-        normal observation.
-
-        """
+import abc
+
+from river import base
+
+
+class AnomalyDetector(base.Estimator):
+    """An anomaly detector."""
+
+    @property
+    def _supervised(self):
+        return False
+
+    @abc.abstractmethod
+    def learn_one(self, x: dict) -> "AnomalyDetector":
+        """Update the model.
+
+        Parameters
+        ----------
+        x
+            A dictionary of features.
+
+        Returns
+        -------
+        self
+
+        """
+
+    @abc.abstractmethod
+    def score_one(self, x: dict) -> float:
+        """Return an outlier score.
+
+        A high score is indicative of an anomaly. A low score corresponds a normal observation.
+
+        Parameters
+        ----------
+        x
+            A dictionary of features.
+
+        Returns
+        -------
+        An anomaly score. A high score is indicative of an anomaly. A low score corresponds a
+        normal observation.
+
+        """
```

### Comparing `river-0.8.0/river/base/classifier.py` & `river-0.9.0/river/base/classifier.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,146 +1,146 @@
-import abc
-import typing
-
-import pandas as pd
-
-from river import base
-
-from . import estimator
-
-
-class Classifier(estimator.Estimator):
-    """A classifier."""
-
-    @abc.abstractmethod
-    def learn_one(self, x: dict, y: base.typing.ClfTarget, **kwargs) -> "Classifier":
-        """Update the model with a set of features `x` and a label `y`.
-
-        Parameters
-        ----------
-        x
-            A dictionary of features.
-        y
-            A label.
-        kwargs
-            Some models might allow/require providing extra parameters, such as sample weights.
-
-        Returns
-        -------
-        self
-
-        """
-
-    def predict_proba_one(self, x: dict) -> typing.Dict[base.typing.ClfTarget, float]:
-        """Predict the probability of each label for a dictionary of features `x`.
-
-        Parameters
-        ----------
-        x
-            A dictionary of features.
-
-        Returns
-        -------
-        A dictionary that associates a probability which each label.
-
-        """
-
-        # Some classifiers don't have the ability to output probabilities, and instead only
-        # predict labels directly. Therefore, we cannot impose predict_proba_one as an abstract
-        # method that each classifier has to implement. Instead, we raise an exception to indicate
-        # that a classifier does not support predict_proba_one.
-        raise NotImplementedError
-
-    def predict_one(self, x: dict) -> base.typing.ClfTarget:
-        """Predict the label of a set of features `x`.
-
-        Parameters
-        ----------
-        x
-            A dictionary of features.
-
-        Returns
-        -------
-        The predicted label.
-
-        """
-
-        # The following code acts as a default for each classifier, and may be overridden on an
-        # individual basis.
-        y_pred = self.predict_proba_one(x)
-        if y_pred:
-            return max(y_pred, key=y_pred.get)
-        return None
-
-    @property
-    def _multiclass(self):
-        return False
-
-    @property
-    def _supervised(self):
-        return True
-
-
-class MiniBatchClassifier(Classifier):
-    """A classifier that can can operate on mini-batches."""
-
-    @abc.abstractmethod
-    def learn_many(
-        self, X: pd.DataFrame, y: pd.Series, **kwargs
-    ) -> "MiniBatchClassifier":
-        """Update the model with a mini-batch of features `X` and boolean targets `y`.
-
-        Parameters
-        ----------
-        X
-            A dataframe of features.
-        y
-            A series of boolean target values.
-        kwargs
-            Some models might allow/require providing extra parameters, such as sample weights.
-
-        Returns
-        -------
-        self
-
-        """
-
-    def predict_proba_many(self, X: pd.DataFrame) -> pd.DataFrame:
-        """Predict the outcome probabilities for each given sample.
-
-        Parameters
-        ----------
-        X
-            A dataframe of features.
-
-        Returns
-        -------
-        A dataframe with probabilities of `True` and `False` for each sample.
-
-        """
-
-        # Some classifiers don't have the ability to output probabilities, and instead only
-        # predict labels directly. Therefore, we cannot impose predict_proba_many as an abstract
-        # method that each classifier has to implement. Instead, we raise an exception to indicate
-        # that a classifier does not support predict_proba_many.
-        raise NotImplementedError
-
-    def predict_many(self, X: pd.DataFrame) -> pd.Series:
-        """Predict the outcome for each given sample.
-
-        Parameters
-        ---------
-        X
-            A dataframe of features.
-
-        Returns
-        -------
-        The predicted labels.
-
-        """
-
-        # The following code acts as a default for each classifier, and may be overridden on an
-        # individual basis.
-        y_pred = self.predict_proba_many(X)
-        if y_pred.empty:
-            return y_pred
-        return y_pred.idxmax(axis="columns")
+import abc
+import typing
+
+import pandas as pd
+
+from river import base
+
+from . import estimator
+
+
+class Classifier(estimator.Estimator):
+    """A classifier."""
+
+    @abc.abstractmethod
+    def learn_one(self, x: dict, y: base.typing.ClfTarget, **kwargs) -> "Classifier":
+        """Update the model with a set of features `x` and a label `y`.
+
+        Parameters
+        ----------
+        x
+            A dictionary of features.
+        y
+            A label.
+        kwargs
+            Some models might allow/require providing extra parameters, such as sample weights.
+
+        Returns
+        -------
+        self
+
+        """
+
+    def predict_proba_one(self, x: dict) -> typing.Dict[base.typing.ClfTarget, float]:
+        """Predict the probability of each label for a dictionary of features `x`.
+
+        Parameters
+        ----------
+        x
+            A dictionary of features.
+
+        Returns
+        -------
+        A dictionary that associates a probability which each label.
+
+        """
+
+        # Some classifiers don't have the ability to output probabilities, and instead only
+        # predict labels directly. Therefore, we cannot impose predict_proba_one as an abstract
+        # method that each classifier has to implement. Instead, we raise an exception to indicate
+        # that a classifier does not support predict_proba_one.
+        raise NotImplementedError
+
+    def predict_one(self, x: dict) -> base.typing.ClfTarget:
+        """Predict the label of a set of features `x`.
+
+        Parameters
+        ----------
+        x
+            A dictionary of features.
+
+        Returns
+        -------
+        The predicted label.
+
+        """
+
+        # The following code acts as a default for each classifier, and may be overridden on an
+        # individual basis.
+        y_pred = self.predict_proba_one(x)
+        if y_pred:
+            return max(y_pred, key=y_pred.get)
+        return None
+
+    @property
+    def _multiclass(self):
+        return False
+
+    @property
+    def _supervised(self):
+        return True
+
+
+class MiniBatchClassifier(Classifier):
+    """A classifier that can can operate on mini-batches."""
+
+    @abc.abstractmethod
+    def learn_many(
+        self, X: pd.DataFrame, y: pd.Series, **kwargs
+    ) -> "MiniBatchClassifier":
+        """Update the model with a mini-batch of features `X` and boolean targets `y`.
+
+        Parameters
+        ----------
+        X
+            A dataframe of features.
+        y
+            A series of boolean target values.
+        kwargs
+            Some models might allow/require providing extra parameters, such as sample weights.
+
+        Returns
+        -------
+        self
+
+        """
+
+    def predict_proba_many(self, X: pd.DataFrame) -> pd.DataFrame:
+        """Predict the outcome probabilities for each given sample.
+
+        Parameters
+        ----------
+        X
+            A dataframe of features.
+
+        Returns
+        -------
+        A dataframe with probabilities of `True` and `False` for each sample.
+
+        """
+
+        # Some classifiers don't have the ability to output probabilities, and instead only
+        # predict labels directly. Therefore, we cannot impose predict_proba_many as an abstract
+        # method that each classifier has to implement. Instead, we raise an exception to indicate
+        # that a classifier does not support predict_proba_many.
+        raise NotImplementedError
+
+    def predict_many(self, X: pd.DataFrame) -> pd.Series:
+        """Predict the outcome for each given sample.
+
+        Parameters
+        ---------
+        X
+            A dataframe of features.
+
+        Returns
+        -------
+        The predicted labels.
+
+        """
+
+        # The following code acts as a default for each classifier, and may be overridden on an
+        # individual basis.
+        y_pred = self.predict_proba_many(X)
+        if y_pred.empty:
+            return y_pred
+        return y_pred.idxmax(axis="columns")
```

### Comparing `river-0.8.0/river/base/clusterer.py` & `river-0.9.0/river/base/clusterer.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,45 +1,45 @@
-import abc
-
-from . import estimator
-
-
-class Clusterer(estimator.Estimator):
-    """A clustering model."""
-
-    @property
-    def _supervised(self):
-        return False
-
-    @abc.abstractmethod
-    def learn_one(self, x: dict, sample_weight: int) -> "Clusterer":
-        """Update the model with a set of features `x`.
-
-        Parameters
-        ----------
-        x
-            A dictionary of features.
-
-        sample_weight
-            Instance weight. If not provided, uniform weights are assumed.
-            Applicability varies depending on the algorithm.
-
-        Returns
-        -------
-        self
-
-        """
-
-    @abc.abstractmethod
-    def predict_one(self, x: dict) -> int:
-        """Predicts the cluster number for a set of features `x`.
-
-        Parameters
-        ----------
-        x
-            A dictionary of features.
-
-        Returns
-        -------
-        A cluster number.
-
-        """
+import abc
+
+from . import estimator
+
+
+class Clusterer(estimator.Estimator):
+    """A clustering model."""
+
+    @property
+    def _supervised(self):
+        return False
+
+    @abc.abstractmethod
+    def learn_one(self, x: dict, sample_weight: int) -> "Clusterer":
+        """Update the model with a set of features `x`.
+
+        Parameters
+        ----------
+        x
+            A dictionary of features.
+
+        sample_weight
+            Instance weight. If not provided, uniform weights are assumed.
+            Applicability varies depending on the algorithm.
+
+        Returns
+        -------
+        self
+
+        """
+
+    @abc.abstractmethod
+    def predict_one(self, x: dict) -> int:
+        """Predicts the cluster number for a set of features `x`.
+
+        Parameters
+        ----------
+        x
+            A dictionary of features.
+
+        Returns
+        -------
+        A cluster number.
+
+        """
```

### Comparing `river-0.8.0/river/base/drift_detector.py` & `river-0.9.0/river/base/drift_detector.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,57 +1,56 @@
-import abc
-import numbers
-import typing
-
-from . import base
-
-
-class DriftDetector(base.Base):
-    """A drift detector."""
-
-    def __init__(self):
-        super().__init__()
-        self._in_concept_change = False
-        self._in_warning_zone = False
-
-    def reset(self):
-        """Reset the change detector."""
-        self._in_concept_change = False
-        self._in_warning_zone = False
-
-    @property
-    def change_detected(self) -> bool:
-        """Concept drift alarm.
-
-        True if concept drift is detected.
-
-        """
-        return self._in_concept_change
-
-    @property
-    def warning_detected(self) -> bool:
-        """Warning zone alarm.
-
-        Indicates if the drift detector is in the warning zone. Applicability depends on each drift
-        detector implementation. True if the change detector is in the warning zone.
-
-        """
-        return self._in_warning_zone
-
-    @abc.abstractmethod
-    def update(self, value: numbers.Number) -> typing.Tuple[bool, bool]:
-        """Update the change detector with a single data point.
-
-        Parameters
-        ----------
-        value
-            Input value.
-
-        Returns
-        -------
-        A tuple (drift, warning) where its elements indicate if a drift or a warning is detected.
-
-        """
-        raise NotImplementedError
-
-    def __iadd__(self, other):
-        self.update(other)
+import abc
+import numbers
+import typing
+
+from . import base
+
+
+class DriftDetector(base.Base):
+    """A drift detector."""
+
+    def __init__(self):
+        super().__init__()
+        self._in_concept_change = False
+        self._in_warning_zone = False
+
+    def reset(self):
+        """Reset the change detector."""
+        self._in_concept_change = False
+        self._in_warning_zone = False
+
+    @property
+    def change_detected(self) -> bool:
+        """Concept drift alarm.
+
+        True if concept drift is detected.
+
+        """
+        return self._in_concept_change
+
+    @property
+    def warning_detected(self) -> bool:
+        """Warning zone alarm.
+
+        Indicates if the drift detector is in the warning zone. Applicability depends on each drift
+        detector implementation. True if the change detector is in the warning zone.
+
+        """
+        return self._in_warning_zone
+
+    @abc.abstractmethod
+    def update(self, value: numbers.Number) -> typing.Tuple[bool, bool]:
+        """Update the change detector with a single data point.
+
+        Parameters
+        ----------
+        value
+            Input value.
+
+        Returns
+        -------
+        A tuple (drift, warning) where its elements indicate if a drift or a warning is detected.
+
+        """
+
+    def __iadd__(self, other):
+        self.update(other)
```

### Comparing `river-0.8.0/river/base/estimator.py` & `river-0.9.0/river/base/estimator.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,84 +1,84 @@
-import abc
-import typing
-
-from . import base
-
-
-class Estimator(base.Base, abc.ABC):
-    """An estimator."""
-
-    @property
-    def _supervised(self):
-        """Indicates whether or not the estimator is supervised or not.
-
-        This is useful internally for determining if an estimator expects to be provided with a `y`
-        value in it's `learn_one` method. For instance we use this in a pipeline to know whether or
-        not we should pass `y` to an estimator or not.
-
-        """
-        return True
-
-    def __or__(self, other):
-        """Merge with another Transformer into a Pipeline."""
-        from .. import compose
-
-        if isinstance(other, compose.Pipeline):
-            return other.__ror__(self)
-        return compose.Pipeline(self, other)
-
-    def __ror__(self, other):
-        """Merge with another Transformer into a Pipeline."""
-        from .. import compose
-
-        if isinstance(other, compose.Pipeline):
-            return other.__or__(self)
-        return compose.Pipeline(other, self)
-
-    @property
-    def _tags(self) -> typing.Dict[str, bool]:
-        """Return the estimator's tags.
-
-        Tags can be used to specify what kind of inputs an estimator is able to process. For
-        instance, some estimators can handle text, whilst others don't. Inheriting from
-        `base.Estimator` will imply a set of default tags which can be overridden by implementing
-        the `_more_tags` property.
-
-        TODO: this could be a cachedproperty.
-
-        """
-
-        try:
-            tags = self._more_tags()
-        except AttributeError:
-            tags = set()
-
-        for parent in self.__class__.__mro__:
-            try:
-                tags |= parent._more_tags(self)
-            except AttributeError:
-                pass
-
-        return tags
-
-    @classmethod
-    def _unit_test_params(self):
-        """Indicates which parameters to use during unit testing.
-
-        Most estimators have a default value for each of their parameters. However, in some cases,
-        no default value is specified. This class method allows to circumvent this issue when the
-        model has to be instantiated during unit testing.
-
-        This can also be used to override default parameters that are computationally expensive,
-        such as the number of base models in an ensemble.
-
-        """
-        return {}
-
-    def _unit_test_skips(self):
-        """Indicates which checks to skip during unit testing.
-
-        Most estimators pass the full test suite. However, in some cases, some estimators might not
-        be able to pass certain checks.
-
-        """
-        return set()
+import abc
+import typing
+
+from . import base
+
+
+class Estimator(base.Base, abc.ABC):
+    """An estimator."""
+
+    @property
+    def _supervised(self):
+        """Indicates whether or not the estimator is supervised or not.
+
+        This is useful internally for determining if an estimator expects to be provided with a `y`
+        value in it's `learn_one` method. For instance we use this in a pipeline to know whether or
+        not we should pass `y` to an estimator or not.
+
+        """
+        return True
+
+    def __or__(self, other):
+        """Merge with another Transformer into a Pipeline."""
+        from .. import compose
+
+        if isinstance(other, compose.Pipeline):
+            return other.__ror__(self)
+        return compose.Pipeline(self, other)
+
+    def __ror__(self, other):
+        """Merge with another Transformer into a Pipeline."""
+        from .. import compose
+
+        if isinstance(other, compose.Pipeline):
+            return other.__or__(self)
+        return compose.Pipeline(other, self)
+
+    @property
+    def _tags(self) -> typing.Dict[str, bool]:
+        """Return the estimator's tags.
+
+        Tags can be used to specify what kind of inputs an estimator is able to process. For
+        instance, some estimators can handle text, whilst others don't. Inheriting from
+        `base.Estimator` will imply a set of default tags which can be overridden by implementing
+        the `_more_tags` property.
+
+        TODO: this could be a cachedproperty.
+
+        """
+
+        try:
+            tags = self._more_tags()
+        except AttributeError:
+            tags = set()
+
+        for parent in self.__class__.__mro__:
+            try:
+                tags |= parent._more_tags(self)
+            except AttributeError:
+                pass
+
+        return tags
+
+    @classmethod
+    def _unit_test_params(self):
+        """Indicates which parameters to use during unit testing.
+
+        Most estimators have a default value for each of their parameters. However, in some cases,
+        no default value is specified. This class method allows to circumvent this issue when the
+        model has to be instantiated during unit testing.
+
+        This can also be used to override default parameters that are computationally expensive,
+        such as the number of base models in an ensemble.
+
+        """
+        yield {}
+
+    def _unit_test_skips(self):
+        """Indicates which checks to skip during unit testing.
+
+        Most estimators pass the full test suite. However, in some cases, some estimators might not
+        be able to pass certain checks.
+
+        """
+        return set()
```

### Comparing `river-0.8.0/river/base/regressor.py` & `river-0.9.0/river/base/regressor.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,85 +1,85 @@
-import abc
-
-import pandas as pd
-
-from river import base
-
-from . import estimator
-
-
-class Regressor(estimator.Estimator):
-    """A regressor."""
-
-    @abc.abstractmethod
-    def learn_one(self, x: dict, y: base.typing.RegTarget, **kwargs) -> "Regressor":
-        """Fits to a set of features `x` and a real-valued target `y`.
-
-        Parameters
-        ----------
-        x
-            A dictionary of features.
-        y
-            A numeric target.
-        kwargs
-            Some models might allow/require providing extra parameters, such as sample weights.
-
-        Returns
-        -------
-        self
-
-        """
-
-    @abc.abstractmethod
-    def predict_one(self, x: dict) -> base.typing.RegTarget:
-        """Predicts the target value of a set of features `x`.
-
-        Parameters
-        ----------
-        x
-            A dictionary of features.
-
-        Returns
-        -------
-        The prediction.
-
-        """
-
-
-class MiniBatchRegressor(Regressor):
-    """A regressor that can operate on mini-batches."""
-
-    @abc.abstractmethod
-    def learn_many(
-        self, X: pd.DataFrame, y: pd.Series, **kwargs
-    ) -> "MiniBatchRegressor":
-        """Update the model with a mini-batch of features `X` and boolean targets `y`.
-
-        Parameters
-        ----------
-        X
-            A dataframe of features.
-        y
-            A series of numbers.
-        kwargs
-            Some models might allow/require providing extra parameters, such as sample weights.
-
-        Returns
-        -------
-        self
-
-        """
-
-    @abc.abstractmethod
-    def predict_many(self, X: pd.DataFrame) -> pd.Series:
-        """Predict the outcome for each given sample.
-
-        Parameters
-        ---------
-        X
-            A dataframe of features.
-
-        Returns
-        -------
-        The predicted outcomes.
-
-        """
+import abc
+
+import pandas as pd
+
+from river import base
+
+from . import estimator
+
+
+class Regressor(estimator.Estimator):
+    """A regressor."""
+
+    @abc.abstractmethod
+    def learn_one(self, x: dict, y: base.typing.RegTarget, **kwargs) -> "Regressor":
+        """Fits to a set of features `x` and a real-valued target `y`.
+
+        Parameters
+        ----------
+        x
+            A dictionary of features.
+        y
+            A numeric target.
+        kwargs
+            Some models might allow/require providing extra parameters, such as sample weights.
+
+        Returns
+        -------
+        self
+
+        """
+
+    @abc.abstractmethod
+    def predict_one(self, x: dict) -> base.typing.RegTarget:
+        """Predicts the target value of a set of features `x`.
+
+        Parameters
+        ----------
+        x
+            A dictionary of features.
+
+        Returns
+        -------
+        The prediction.
+
+        """
+
+
+class MiniBatchRegressor(Regressor):
+    """A regressor that can operate on mini-batches."""
+
+    @abc.abstractmethod
+    def learn_many(
+        self, X: pd.DataFrame, y: pd.Series, **kwargs
+    ) -> "MiniBatchRegressor":
+        """Update the model with a mini-batch of features `X` and boolean targets `y`.
+
+        Parameters
+        ----------
+        X
+            A dataframe of features.
+        y
+            A series of numbers.
+        kwargs
+            Some models might allow/require providing extra parameters, such as sample weights.
+
+        Returns
+        -------
+        self
+
+        """
+
+    @abc.abstractmethod
+    def predict_many(self, X: pd.DataFrame) -> pd.Series:
+        """Predict the outcome for each given sample.
+
+        Parameters
+        ---------
+        X
+            A dataframe of features.
+
+        Returns
+        -------
+        The predicted outcomes.
+
+        """
```

### Comparing `river-0.8.0/river/base/test_base.py` & `river-0.9.0/river/base/test_base.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,52 +1,52 @@
-from river import datasets, linear_model, optim, preprocessing
-
-
-def test_set_params():
-
-    obj = linear_model.LinearRegression(l2=42)
-    obj.learn_one({"x": 3}, 6)
-
-    new = obj._set_params({"l2": 21})
-    assert new.l2 == 21
-    assert obj.l2 == 42
-    assert new.weights == {}
-    assert new.weights != obj.weights
-
-
-def test_set_params_pipeline():
-
-    obj = preprocessing.StandardScaler() | linear_model.LinearRegression(l2=42)
-    obj.learn_one({"x": 3}, 6)
-
-    params = {"LinearRegression": {"l2": 21}}
-    new = obj._set_params(params)
-    assert new["LinearRegression"].l2 == 21
-    assert obj["LinearRegression"].l2 == 42
-    assert new["LinearRegression"].weights == {}
-    assert new["LinearRegression"].weights != obj["LinearRegression"].weights
-
-
-def test_clone_idempotent():
-
-    model = preprocessing.StandardScaler() | linear_model.LogisticRegression(
-        optimizer=optim.Adam(), l2=0.1
-    )
-
-    trace = []
-    for x, y in datasets.Phishing():
-        trace.append(model.predict_proba_one(x))
-        model.learn_one(x, y)
-
-    clone = model.clone()
-    for i, (x, y) in enumerate(datasets.Phishing()):
-        assert clone.predict_proba_one(x) == trace[i]
-        clone.learn_one(x, y)
-
-
-def test_memory_usage():
-
-    model = preprocessing.StandardScaler() | linear_model.LogisticRegression()
-
-    # We can't test the exact value because it depends on the platform and the Python version
-    # TODO: we could create a table of expected values for each platform and Python version
-    assert isinstance(model._memory_usage, str)
+from river import datasets, linear_model, optim, preprocessing
+
+
+def test_set_params():
+
+    obj = linear_model.LinearRegression(l2=42)
+    obj.learn_one({"x": 3}, 6)
+
+    new = obj._set_params({"l2": 21})
+    assert new.l2 == 21
+    assert obj.l2 == 42
+    assert new.weights == {}
+    assert new.weights != obj.weights
+
+
+def test_set_params_pipeline():
+
+    obj = preprocessing.StandardScaler() | linear_model.LinearRegression(l2=42)
+    obj.learn_one({"x": 3}, 6)
+
+    params = {"LinearRegression": {"l2": 21}}
+    new = obj._set_params(params)
+    assert new["LinearRegression"].l2 == 21
+    assert obj["LinearRegression"].l2 == 42
+    assert new["LinearRegression"].weights == {}
+    assert new["LinearRegression"].weights != obj["LinearRegression"].weights
+
+
+def test_clone_idempotent():
+
+    model = preprocessing.StandardScaler() | linear_model.LogisticRegression(
+        optimizer=optim.Adam(), l2=0.1
+    )
+
+    trace = []
+    for x, y in datasets.Phishing():
+        trace.append(model.predict_proba_one(x))
+        model.learn_one(x, y)
+
+    clone = model.clone()
+    for i, (x, y) in enumerate(datasets.Phishing()):
+        assert clone.predict_proba_one(x) == trace[i]
+        clone.learn_one(x, y)
+
+
+def test_memory_usage():
+
+    model = preprocessing.StandardScaler() | linear_model.LogisticRegression()
+
+    # We can't test the exact value because it depends on the platform and the Python version
+    # TODO: we could create a table of expected values for each platform and Python version
+    assert isinstance(model._memory_usage, str)
```

### Comparing `river-0.8.0/river/cluster/clustream.py` & `river-0.9.0/river/cluster/clustream.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,435 +1,438 @@
-import math
-from abc import ABCMeta
-
-from river import base, cluster, utils
-
-EPSILON = 0.00005
-MIN_VARIANCE = 1e-50
-
-
-class CluStream(base.Clusterer):
-    """CluStream
-
-    The CluStream algorithm [^1] maintains statistical information about the
-    data using micro-clusters. These micro-clusters are temporal extensions of
-    cluster feature vectors. The micro-clusters are stored at snapshots in time
-    following a pyramidal pattern. This pattern allows to recall summary
-    statistics from different time horizons.
-
-    Training with a new point `p` is performed in two main tasks:
-
-    * Determinate closest micro-cluster to `p`
-
-    * Check whether `p` fits (memory) into the closest micro-cluster:
-
-        - if `p` fits, put into micro-cluster
-
-        - if `p` does not fit, free some space to insert a new micro-cluster.
-          This is done in two ways, delete an old micro-cluster or merge the
-          two micro-clusters closest to each other.
-
-    Parameters
-    ----------
-    seed
-       Random seed used for generating initial centroid positions.
-
-    time_window
-        If the current time is `T` and the time window is `h`, we only consider
-        the data that arrived within the period `(T-h,T)`.
-
-    max_micro_clusters
-        The maximum number of micro-clusters to use.
-
-    micro_cluster_r_factor
-        Multiplier for the micro-cluster radius.
-        When deciding to add a new data point to a micro-cluster, the maximum
-        boundary is defined as a factor of the `micro_cluster_r_factor` of
-        the RMS deviation of the data points in the micro-cluster from the
-        centroid.
-
-    n_macro_clusters
-        The number of clusters (k) for the k-means algorithm.
-
-    kwargs
-        Other parameters passed to the incremental kmeans at `cluster.KMeans`.
-
-    Attributes
-    ----------
-    centers : dict
-        Central positions of each cluster.
-
-    References
-    ----------
-    [^1]: Aggarwal, C.C., Philip, S.Y., Han, J. and Wang, J., 2003,
-          A framework for clustering evolving data streams.
-          In Proceedings 2003 VLDB conference (pp. 81-92). Morgan Kaufmann.
-
-    Examples
-    --------
-
-    In the following example, `max_micro_clusters` and `time_window` are set
-    relatively low due to the limited number of training points.
-    Moreover, all points are learnt before any predictions are made.
-    The `halflife` is set at 0.4, to show that you can pass `cluster.KMeans`
-    parameters via keyword arguments.
-
-    >>> from river import cluster
-    >>> from river import stream
-
-    >>> X = [
-    ...     [1, 2],
-    ...     [1, 4],
-    ...     [1, 0],
-    ...     [4, 2],
-    ...     [4, 4],
-    ...     [4, 0]
-    ... ]
-
-    >>> clustream = cluster.CluStream(time_window=1,
-    ...                               max_micro_clusters=3,
-    ...                               n_macro_clusters=2,
-    ...                               seed=0,
-    ...                               halflife=0.4)
-
-    >>> for i, (x, _) in enumerate(stream.iter_array(X)):
-    ...     clustream = clustream.learn_one(x)
-
-    >>> clustream.predict_one({0: 1, 1: 1})
-    1
-
-    >>> clustream.predict_one({0: 4, 1: 3})
-    0
-
-    """
-
-    def __init__(
-        self,
-        seed: int = None,
-        time_window: int = 1000,
-        max_micro_clusters: int = 100,
-        micro_cluster_r_factor: int = 2,
-        n_macro_clusters: int = 5,
-        **kwargs
-    ):
-        super().__init__()
-        self.time_window = time_window
-        self.time_stamp = -1
-        self.micro_clusters = {n: None for n in range(max_micro_clusters)}
-        self.initialized = False
-        self.buffer = {}
-        self.max_micro_clusters = max_micro_clusters
-        self.centers = {}
-        self.micro_cluster_r_factor = micro_cluster_r_factor
-        self.max_micro_clusters = max_micro_clusters
-        self.n_macro_clusters = n_macro_clusters
-        self._train_weight_seen_by_model = 0.0
-        self.seed = seed
-        self.kwargs = kwargs
-
-    def _initialize(self, x, sample_weight):
-
-        # Create a micro-cluster with the new point
-        if len(self.buffer) < self.max_micro_clusters:
-            self.buffer[len(self.buffer)] = CluStreamMicroCluster(
-                x=x,
-                sample_weight=sample_weight,
-                timestamp=self.time_stamp,
-                micro_cluster_r_factor=self.micro_cluster_r_factor,
-                max_micro_clusters=self.max_micro_clusters,
-            )
-        else:
-            # The buffer is full. Use the micro-clusters centers to create the
-            # micro-clusters set.
-            for i in range(self.max_micro_clusters):
-                self.micro_clusters[i] = CluStreamMicroCluster(
-                    x=self.buffer[i].center,
-                    sample_weight=1.0,
-                    timestamp=self.time_stamp,
-                    micro_cluster_r_factor=self.micro_cluster_r_factor,
-                    max_micro_clusters=self.max_micro_clusters,
-                )
-            self.buffer.clear()
-            self.initialized = True
-
-    def _maintain_micro_clusters(self, x, sample_weight):
-        # Calculate the threshold to delete old micro-clusters
-        threshold = self.time_stamp - self.time_window
-
-        # Delete old micro-clusters if its relevance stamp is smaller than the threshold
-        for i, micro_cluster_a in self.micro_clusters.items():
-            if micro_cluster_a.relevance_stamp < threshold:
-                self.micro_clusters[i] = CluStreamMicroCluster(
-                    x=x,
-                    sample_weight=sample_weight,
-                    timestamp=self.time_stamp,
-                    micro_cluster_r_factor=self.micro_cluster_r_factor,
-                    max_micro_clusters=self.max_micro_clusters,
-                )
-                return self
-
-        # Merge the two closest micro-clusters
-        closest_a = 0
-        closest_b = 0
-        min_distance = math.inf
-        for i, micro_cluster_a in self.micro_clusters.items():
-            for j, micro_cluster_b in self.micro_clusters.items():
-                dist = self._distance(micro_cluster_a.center, micro_cluster_b.center)
-                if dist < min_distance and j > i:
-                    min_distance = dist
-                    closest_a = i
-                    closest_b = j
-        self.micro_clusters[closest_a].add(self.micro_clusters[closest_b])
-        self.micro_clusters[closest_b] = CluStreamMicroCluster(
-            x=x,
-            sample_weight=sample_weight,
-            timestamp=self.time_stamp,
-            micro_cluster_r_factor=self.micro_cluster_r_factor,
-            max_micro_clusters=self.max_micro_clusters,
-        )
-
-    def _get_micro_clustering_result(self):
-        if not self.initialized:
-            return {}
-        res = {
-            i: CluStreamMicroCluster(
-                micro_cluster=micro_cluster,
-                micro_cluster_r_factor=self.micro_cluster_r_factor,
-                max_micro_clusters=self.max_micro_clusters,
-            )
-            for i, micro_cluster in self.micro_clusters.items()
-        }
-        return res
-
-    def _get_closest_micro_cluster(self, x, micro_clusters):
-        min_distance = math.inf
-        closest_micro_cluster_idx = -1
-        for i, micro_cluster in micro_clusters.items():
-            distance = self._distance(micro_cluster.center, x)
-            if distance < min_distance:
-                min_distance = distance
-                closest_micro_cluster_idx = i
-        return closest_micro_cluster_idx, min_distance
-
-    @staticmethod
-    def _distance(point_a, point_b):
-        return math.sqrt(utils.math.minkowski_distance(point_a, point_b, 2))
-
-    def learn_one(self, x, sample_weight=None):
-
-        if sample_weight == 0:
-            return
-        elif sample_weight is None:
-            sample_weight = 1.0
-
-        self._train_weight_seen_by_model += sample_weight
-
-        # merge _learn_one into learn_one
-        self.time_stamp += 1
-
-        if not self.initialized:
-            self._initialize(x=x, sample_weight=sample_weight)
-            return self
-
-        # determine the closest micro-cluster with respect to the new point instance
-        closest_micro_cluster = None
-        min_distance = math.inf
-        for micro_cluster in self.micro_clusters.values():
-            distance = self._distance(x, micro_cluster.center)
-            if distance < min_distance:
-                closest_micro_cluster = micro_cluster
-                min_distance = distance
-
-        # check whether the new instance fits into the closest micro-cluster
-        if closest_micro_cluster.weight == 1:
-            radius = math.inf
-            center = closest_micro_cluster.center
-            for micro_cluster in self.micro_clusters.values():
-                if micro_cluster == closest_micro_cluster:
-                    continue
-                distance = self._distance(micro_cluster.center, center)
-                radius = min(distance, radius)
-        else:
-            radius = closest_micro_cluster.radius
-
-        if min_distance < radius:
-            closest_micro_cluster.insert(x, sample_weight, self.time_stamp)
-            return self
-
-        # If the new point does not fit in the micro-cluster, micro-clusters
-        # whose relevance stamps are less than the threshold are deleted.
-        # Otherwise, closest micro-clusters are merged with each other.
-        self._maintain_micro_clusters(x=x, sample_weight=sample_weight)
-
-        return self
-
-    def predict_one(self, x):
-
-        micro_cluster_centers = {
-            i: self._get_micro_clustering_result()[i].center
-            for i in range(len(self._get_micro_clustering_result()))
-        }
-
-        kmeans = cluster.KMeans(
-            n_clusters=self.n_macro_clusters, seed=self.seed, **self.kwargs
-        )
-        for center in micro_cluster_centers.values():
-            kmeans = kmeans.learn_one(center)
-
-        self.centers = kmeans.centers
-
-        index, _ = self._get_closest_micro_cluster(
-            x, self._get_micro_clustering_result()
-        )
-        try:
-            return kmeans.predict_one(micro_cluster_centers[index])
-        except KeyError:
-            return 0
-
-
-class CluStreamMicroCluster(metaclass=ABCMeta):
-    """ Micro-cluster class """
-
-    def __init__(
-        self,
-        x: dict = None,
-        sample_weight: float = None,
-        micro_cluster=None,
-        timestamp: int = None,
-        micro_cluster_r_factor: int = None,
-        max_micro_clusters: int = None,
-    ):
-
-        self.micro_cluster_r_factor = micro_cluster_r_factor
-        self.max_micro_clusters = max_micro_clusters
-
-        if x is not None and sample_weight is not None:
-            # Initialize with sample x
-            self.n_samples = 1
-            self.linear_sum = {}
-            self.squared_sum = {}
-            for key in x.keys():
-                self.linear_sum[key] = x[key] * sample_weight
-                self.squared_sum[key] = x[key] * x[key] * sample_weight
-            self.linear_sum_timestamp = timestamp * sample_weight
-            self.squared_sum_timestamp = timestamp * timestamp * sample_weight
-        elif micro_cluster is not None:
-            # Initialize with micro-cluster
-            self.n_samples = micro_cluster.n_samples
-            self.linear_sum = micro_cluster.linear_sum.copy()
-            self.squared_sum = micro_cluster.squared_sum.copy()
-            self.linear_sum_timestamp = micro_cluster.linear_sum_timestamp
-            self.squared_sum_timestamp = micro_cluster.squared_sum_timestamp
-
-    @property
-    def center(self):
-        return {
-            i: linear_sum_i / self.n_samples
-            for i, linear_sum_i in self.linear_sum.items()
-        }
-
-    def is_empty(self):
-        return self.n_samples == 0
-
-    @property
-    def radius(self):
-        if self.n_samples == 1:
-            return 0
-        return self._deviation * self.micro_cluster_r_factor
-
-    @property
-    def _deviation(self):
-        variance = self._variance_vector
-        sum_of_deviation = 0
-        for i in range(len(variance)):
-            d = math.sqrt(variance[i])
-            sum_of_deviation += d
-        return sum_of_deviation / len(variance)
-
-    @property
-    def _variance_vector(self):
-        res = {}
-        for key in self.linear_sum.keys():
-            ls = self.linear_sum[key]
-            ss = self.squared_sum[key]
-            ls_div_n = ls / self.weight
-            ls_div_n_squared = ls_div_n * ls_div_n
-            ss_div_n = ss / self.weight
-            res[key] = ss_div_n - ls_div_n_squared
-
-            if res[key] <= 0.0:
-                if res[key] > -EPSILON:
-                    res[key] = MIN_VARIANCE
-        return res
-
-    @property
-    def weight(self):
-        return self.n_samples
-
-    def insert(self, x, sample_weight, timestamp):
-        self.n_samples += 1
-        self.linear_sum_timestamp += timestamp * sample_weight
-        self.squared_sum_timestamp += timestamp * sample_weight
-        for i in range(len(x)):
-            self.linear_sum[i] += x[i] * sample_weight
-            self.squared_sum[i] += x[i] * x[i] * sample_weight
-
-    @property
-    def relevance_stamp(self):
-        if self.n_samples < 2 * self.max_micro_clusters:
-            return self._mu_time
-        return self._mu_time + self._sigma_time * self._quantile(
-            float(self.max_micro_clusters) / (2 * self.n_samples)
-        )
-
-    @property
-    def _mu_time(self):
-        return self.linear_sum_timestamp / self.n_samples
-
-    @property
-    def _sigma_time(self):
-        return math.sqrt(
-            self.squared_sum_timestamp / self.n_samples
-            - (self.linear_sum_timestamp / self.n_samples)
-            * (self.linear_sum_timestamp / self.n_samples)
-        )
-
-    def _quantile(self, z):
-        assert 0 <= z <= 1
-        return math.sqrt(2) * self.inverse_error(2 * z - 1)
-
-    @staticmethod
-    def inverse_error(x):
-        z = math.sqrt(math.pi) * x
-        res = x / 2
-        z2 = z * z
-
-        zprod = z2 * z
-        res += (1.0 / 24) * zprod
-
-        zprod *= z2  # z5
-        res += (7.0 / 960) * zprod
-
-        zprod *= z2  # z ^ 7
-        res += (127 * zprod) / 80640
-
-        zprod *= z2  # z ^ 9
-        res += (4369 * zprod) / 11612160
-
-        zprod *= z2  # z ^ 11
-        res += (34807 * zprod) / 364953600
-
-        zprod *= z2  # z ^ 13
-        res += (20036983 * zprod) / 797058662400
-
-        return res
-
-    def add(self, micro_cluster):
-        self.n_samples += micro_cluster.n_samples
-        self.linear_sum_timestamp += micro_cluster.linear_sum_timestamp
-        self.squared_sum_timestamp += micro_cluster.squared_sum_timestamp
-        utils.skmultiflow_utils.add_dict_values(
-            self.linear_sum, micro_cluster.linear_sum, inplace=True
-        )
-        utils.skmultiflow_utils.add_dict_values(
-            self.squared_sum, micro_cluster.squared_sum, inplace=True
-        )
+import math
+from abc import ABCMeta
+from collections import defaultdict
+
+from river import base, cluster, utils
+
+EPSILON = 0.00005
+MIN_VARIANCE = 1e-50
+
+
+class CluStream(base.Clusterer):
+    """CluStream
+
+    The CluStream algorithm [^1] maintains statistical information about the
+    data using micro-clusters. These micro-clusters are temporal extensions of
+    cluster feature vectors. The micro-clusters are stored at snapshots in time
+    following a pyramidal pattern. This pattern allows to recall summary
+    statistics from different time horizons.
+
+    Training with a new point `p` is performed in two main tasks:
+
+    * Determinate closest micro-cluster to `p`
+
+    * Check whether `p` fits (memory) into the closest micro-cluster:
+
+        - if `p` fits, put into micro-cluster
+
+        - if `p` does not fit, free some space to insert a new micro-cluster.
+          This is done in two ways, delete an old micro-cluster or merge the
+          two micro-clusters closest to each other.
+
+    Parameters
+    ----------
+    seed
+       Random seed used for generating initial centroid positions.
+
+    time_window
+        If the current time is `T` and the time window is `h`, we only consider
+        the data that arrived within the period `(T-h,T)`.
+
+    max_micro_clusters
+        The maximum number of micro-clusters to use.
+
+    micro_cluster_r_factor
+        Multiplier for the micro-cluster radius.
+        When deciding to add a new data point to a micro-cluster, the maximum
+        boundary is defined as a factor of the `micro_cluster_r_factor` of
+        the RMS deviation of the data points in the micro-cluster from the
+        centroid.
+
+    n_macro_clusters
+        The number of clusters (k) for the k-means algorithm.
+
+    kwargs
+        Other parameters passed to the incremental kmeans at `cluster.KMeans`.
+
+    Attributes
+    ----------
+    centers : dict
+        Central positions of each cluster.
+
+    References
+    ----------
+    [^1]: Aggarwal, C.C., Philip, S.Y., Han, J. and Wang, J., 2003,
+          A framework for clustering evolving data streams.
+          In Proceedings 2003 VLDB conference (pp. 81-92). Morgan Kaufmann.
+
+    Examples
+    --------
+
+    In the following example, `max_micro_clusters` and `time_window` are set
+    relatively low due to the limited number of training points.
+    Moreover, all points are learnt before any predictions are made.
+    The `halflife` is set at 0.4, to show that you can pass `cluster.KMeans`
+    parameters via keyword arguments.
+
+    >>> from river import cluster
+    >>> from river import stream
+
+    >>> X = [
+    ...     [1, 2],
+    ...     [1, 4],
+    ...     [1, 0],
+    ...     [4, 2],
+    ...     [4, 4],
+    ...     [4, 0]
+    ... ]
+
+    >>> clustream = cluster.CluStream(time_window=1,
+    ...                               max_micro_clusters=3,
+    ...                               n_macro_clusters=2,
+    ...                               seed=0,
+    ...                               halflife=0.4)
+
+    >>> for i, (x, _) in enumerate(stream.iter_array(X)):
+    ...     clustream = clustream.learn_one(x)
+
+    >>> clustream.predict_one({0: 1, 1: 1})
+    1
+
+    >>> clustream.predict_one({0: 4, 1: 3})
+    0
+
+    """
+
+    def __init__(
+        self,
+        seed: int = None,
+        time_window: int = 1000,
+        max_micro_clusters: int = 100,
+        micro_cluster_r_factor: int = 2,
+        n_macro_clusters: int = 5,
+        **kwargs
+    ):
+        super().__init__()
+        self.time_window = time_window
+        self.time_stamp = -1
+        self.micro_clusters = {n: None for n in range(max_micro_clusters)}
+        self.initialized = False
+        self.buffer = {}
+        self.centers = {}
+        self.micro_cluster_r_factor = micro_cluster_r_factor
+        self.max_micro_clusters = max_micro_clusters
+        self.n_macro_clusters = n_macro_clusters
+        self._train_weight_seen_by_model = 0.0
+        self.seed = seed
+        self.kwargs = kwargs
+
+    def _initialize(self, x, sample_weight):
+
+        # Create a micro-cluster with the new point
+        if len(self.buffer) < self.max_micro_clusters:
+            self.buffer[len(self.buffer)] = CluStreamMicroCluster(
+                x=x,
+                sample_weight=sample_weight,
+                timestamp=self.time_stamp,
+                micro_cluster_r_factor=self.micro_cluster_r_factor,
+                max_micro_clusters=self.max_micro_clusters,
+            )
+        else:
+            # The buffer is full. Use the micro-clusters centers to create the
+            # micro-clusters set.
+            for i in range(self.max_micro_clusters):
+                self.micro_clusters[i] = CluStreamMicroCluster(
+                    x=self.buffer[i].center,
+                    sample_weight=1.0,
+                    timestamp=self.time_stamp,
+                    micro_cluster_r_factor=self.micro_cluster_r_factor,
+                    max_micro_clusters=self.max_micro_clusters,
+                )
+            self.buffer.clear()
+            self.initialized = True
+
+    def _maintain_micro_clusters(self, x, sample_weight):
+        # Calculate the threshold to delete old micro-clusters
+        threshold = self.time_stamp - self.time_window
+
+        # Delete old micro-clusters if its relevance stamp is smaller than the threshold
+        for i, micro_cluster_a in self.micro_clusters.items():
+            if micro_cluster_a.relevance_stamp < threshold:
+                self.micro_clusters[i] = CluStreamMicroCluster(
+                    x=x,
+                    sample_weight=sample_weight,
+                    timestamp=self.time_stamp,
+                    micro_cluster_r_factor=self.micro_cluster_r_factor,
+                    max_micro_clusters=self.max_micro_clusters,
+                )
+                return self
+
+        # Merge the two closest micro-clusters
+        closest_a = 0
+        closest_b = 0
+        min_distance = math.inf
+        for i, micro_cluster_a in self.micro_clusters.items():
+            for j, micro_cluster_b in self.micro_clusters.items():
+                dist = self._distance(micro_cluster_a.center, micro_cluster_b.center)
+                if dist < min_distance and j > i:
+                    min_distance = dist
+                    closest_a = i
+                    closest_b = j
+        self.micro_clusters[closest_a].add(self.micro_clusters[closest_b])
+        self.micro_clusters[closest_b] = CluStreamMicroCluster(
+            x=x,
+            sample_weight=sample_weight,
+            timestamp=self.time_stamp,
+            micro_cluster_r_factor=self.micro_cluster_r_factor,
+            max_micro_clusters=self.max_micro_clusters,
+        )
+
+    def _get_micro_clustering_result(self):
+        if not self.initialized:
+            return {}
+        res = {
+            i: CluStreamMicroCluster(
+                micro_cluster=micro_cluster,
+                micro_cluster_r_factor=self.micro_cluster_r_factor,
+                max_micro_clusters=self.max_micro_clusters,
+            )
+            for i, micro_cluster in self.micro_clusters.items()
+        }
+        return res
+
+    def _get_closest_micro_cluster(self, x, micro_clusters):
+        min_distance = math.inf
+        closest_micro_cluster_idx = -1
+        for i, micro_cluster in micro_clusters.items():
+            distance = self._distance(micro_cluster.center, x)
+            if distance < min_distance:
+                min_distance = distance
+                closest_micro_cluster_idx = i
+        return closest_micro_cluster_idx, min_distance
+
+    @staticmethod
+    def _distance(point_a, point_b):
+        return math.sqrt(utils.math.minkowski_distance(point_a, point_b, 2))
+
+    def learn_one(self, x, sample_weight=None):
+
+        if sample_weight == 0:
+            return
+        elif sample_weight is None:
+            sample_weight = 1.0
+
+        self._train_weight_seen_by_model += sample_weight
+
+        # merge _learn_one into learn_one
+        self.time_stamp += 1
+
+        if not self.initialized:
+            self._initialize(x=x, sample_weight=sample_weight)
+            return self
+
+        # determine the closest micro-cluster with respect to the new point instance
+        closest_micro_cluster = None
+        min_distance = math.inf
+        for micro_cluster in self.micro_clusters.values():
+            distance = self._distance(x, micro_cluster.center)
+            if distance < min_distance:
+                closest_micro_cluster = micro_cluster
+                min_distance = distance
+
+        # check whether the new instance fits into the closest micro-cluster
+        if closest_micro_cluster.weight == 1:
+            radius = math.inf
+            center = closest_micro_cluster.center
+            for micro_cluster in self.micro_clusters.values():
+                if micro_cluster == closest_micro_cluster:
+                    continue
+                distance = self._distance(micro_cluster.center, center)
+                radius = min(distance, radius)
+        else:
+            radius = closest_micro_cluster.radius
+
+        if min_distance < radius:
+            closest_micro_cluster.insert(x, sample_weight, self.time_stamp)
+            return self
+
+        # If the new point does not fit in the micro-cluster, micro-clusters
+        # whose relevance stamps are less than the threshold are deleted.
+        # Otherwise, closest micro-clusters are merged with each other.
+        self._maintain_micro_clusters(x=x, sample_weight=sample_weight)
+
+        return self
+
+    def predict_one(self, x):
+
+        micro_cluster_centers = {
+            i: result.center
+            for i, result in self._get_micro_clustering_result().items()
+        }
+
+        kmeans = cluster.KMeans(
+            n_clusters=self.n_macro_clusters, seed=self.seed, **self.kwargs
+        )
+        for center in micro_cluster_centers.values():
+            kmeans = kmeans.learn_one(center)
+
+        self.centers = kmeans.centers
+
+        index, _ = self._get_closest_micro_cluster(
+            x, self._get_micro_clustering_result()
+        )
+        try:
+            return kmeans.predict_one(micro_cluster_centers[index])
+        except KeyError:
+            return 0
+
+
+class CluStreamMicroCluster(metaclass=ABCMeta):
+    """ Micro-cluster class """
+
+    def __init__(
+        self,
+        x: dict = None,
+        sample_weight: float = None,
+        micro_cluster=None,
+        timestamp: int = None,
+        micro_cluster_r_factor: int = None,
+        max_micro_clusters: int = None,
+    ):
+
+        self.micro_cluster_r_factor = micro_cluster_r_factor
+        self.max_micro_clusters = max_micro_clusters
+
+        if x is not None and sample_weight is not None:
+            # Initialize with sample x
+            self.n_samples = 1
+            self.linear_sum = defaultdict(float)
+            self.squared_sum = defaultdict(float)
+            for key in x.keys():
+                self.linear_sum[key] = x[key] * sample_weight
+                self.squared_sum[key] = x[key] * x[key] * sample_weight
+            self.linear_sum_timestamp = timestamp * sample_weight
+            self.squared_sum_timestamp = (
+                timestamp * sample_weight * timestamp * sample_weight
+            )
+        elif micro_cluster is not None:
+            # Initialize with micro-cluster
+            self.n_samples = micro_cluster.n_samples
+            self.linear_sum = micro_cluster.linear_sum.copy()
+            self.squared_sum = micro_cluster.squared_sum.copy()
+            self.linear_sum_timestamp = micro_cluster.linear_sum_timestamp
+            self.squared_sum_timestamp = micro_cluster.squared_sum_timestamp
+
+    @property
+    def center(self):
+        return {
+            i: linear_sum_i / self.n_samples
+            for i, linear_sum_i in self.linear_sum.items()
+        }
+
+    def is_empty(self):
+        return self.n_samples == 0
+
+    @property
+    def radius(self):
+        if self.n_samples == 1:
+            return 0
+        return self._deviation * self.micro_cluster_r_factor
+
+    @property
+    def _deviation(self):
+        variance = self._variance_vector
+        sum_of_deviation = 0
+        for var in variance.values():
+            sum_of_deviation += math.sqrt(var)
+        return sum_of_deviation / len(variance) if len(variance) > 0 else math.inf
+
+    @property
+    def _variance_vector(self):
+        res = {}
+        for key in self.linear_sum.keys():
+            ls = self.linear_sum[key]
+            ss = self.squared_sum[key]
+            ls_div_n = ls / self.weight
+            ls_div_n_squared = ls_div_n * ls_div_n
+            ss_div_n = ss / self.weight
+            res[key] = ss_div_n - ls_div_n_squared
+
+            if res[key] <= 0.0:
+                if res[key] > -EPSILON:
+                    res[key] = MIN_VARIANCE
+        return res
+
+    @property
+    def weight(self):
+        return self.n_samples
+
+    def insert(self, x, sample_weight, timestamp):
+        self.n_samples += 1
+        self.linear_sum_timestamp += timestamp * sample_weight
+        self.squared_sum_timestamp += (
+            timestamp * sample_weight * timestamp * sample_weight
+        )
+        for x_idx, x_val in x.items():
+            self.linear_sum[x_idx] += x_val * sample_weight
+            self.squared_sum[x_idx] += x_val * sample_weight * x_val * sample_weight
+
+    @property
+    def relevance_stamp(self):
+        if self.n_samples < 2 * self.max_micro_clusters:
+            return self._mu_time
+        return self._mu_time + self._sigma_time * self._quantile(
+            float(self.max_micro_clusters) / (2 * self.n_samples)
+        )
+
+    @property
+    def _mu_time(self):
+        return self.linear_sum_timestamp / self.n_samples
+
+    @property
+    def _sigma_time(self):
+        return math.sqrt(
+            self.squared_sum_timestamp / self.n_samples
+            - (self.linear_sum_timestamp / self.n_samples)
+            * (self.linear_sum_timestamp / self.n_samples)
+        )
+
+    def _quantile(self, z):
+        assert 0 <= z <= 1
+        return math.sqrt(2) * self.inverse_error(2 * z - 1)
+
+    @staticmethod
+    def inverse_error(x):
+        z = math.sqrt(math.pi) * x
+        res = x / 2
+        z2 = z * z
+
+        zprod = z2 * z
+        res += (1.0 / 24) * zprod
+
+        zprod *= z2  # z5
+        res += (7.0 / 960) * zprod
+
+        zprod *= z2  # z ^ 7
+        res += (127 * zprod) / 80640
+
+        zprod *= z2  # z ^ 9
+        res += (4369 * zprod) / 11612160
+
+        zprod *= z2  # z ^ 11
+        res += (34807 * zprod) / 364953600
+
+        zprod *= z2  # z ^ 13
+        res += (20036983 * zprod) / 797058662400
+
+        return res
+
+    def add(self, micro_cluster):
+        self.n_samples += micro_cluster.n_samples
+        self.linear_sum_timestamp += micro_cluster.linear_sum_timestamp
+        self.squared_sum_timestamp += micro_cluster.squared_sum_timestamp
+        utils.skmultiflow_utils.add_dict_values(
+            self.linear_sum, micro_cluster.linear_sum, inplace=True
+        )
+        utils.skmultiflow_utils.add_dict_values(
+            self.squared_sum, micro_cluster.squared_sum, inplace=True
+        )
```

### Comparing `river-0.8.0/river/cluster/dbstream.py` & `river-0.9.0/river/cluster/denstream.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,419 +1,461 @@
-import collections
-import math
-from abc import ABCMeta
-
-from river import base, utils
-
-
-class DBSTREAM(base.Clusterer):
-    r"""DBSTREAM
-
-    DBSTREAM [^1] is a clustering algorithm for evolving data streams.
-    It is the first micro-cluster-based online clustering component that
-    explicitely captures the density between micro-clusters via a shared
-    density graph. The density information in the graph is then exploited
-    for reclustering based on actual density between adjacent micro clusters.
-
-    The algorithm is divided into two parts:
-
-    **Online micro-cluster maintenance (learning)**
-
-    For a new point `p`:
-
-    * Find all micro clusters for which `p` falls within the fixed radius
-    (clustering threshold). If no neighbor is found, a new micro cluster
-    with a weight of 1 is created for `p`.
-
-    * If no neighbor is found, a new micro cluster with a weight of 1 is
-    created for `p`. If one or more neighbors of `p` are found, we update
-    the micro clusters by applying the appropriate fading, increasing
-    their weight and then we try to move them closer to `p` using the
-    Gaussian neighborhood function.
-
-    * Next, the shared density graph is updated. To prevent collapsing
-    micro clusters, we will restrict the movement for micro clusters in case
-    they come closer than $r$ (clustering threshold) to each other. Finishing
-    this process, the time stamp is also increased by 1.
-
-    * Finally, the cleanup will be processed. It is executed every `t_gap`
-    time steps, removing weak micro clusters and weak entries in the
-    shared density graph to recover memory and improve the clustering algorithm's
-    processing speed.
-
-    **Offline generation of macro clusters (clustering)**
-
-    The offline generation of macro clusters is generated through the two following steps:
-
-    * The connectivity graph `C` is constructed using shared density entries
-    between strong micro clusters. The edges in this connectivity graph with
-    a connectivity value greater than the intersection threshold ($\alpha$)
-    are used to find connected components representing the final cluster.
-
-    * After the connectivity graph is generated, a variant of the DBSCAN algorithm
-    proposed by Ester et al. is applied to form all macro clusters
-    from $\alpha$-connected micro clusters.
-
-    Parameters
-    ----------
-    clustering_threshold
-        DBStream represents each micro cluster by a leader (a data point defining the
-        micro cluster's center) and the density in an area of a user-specified radius
-        $r$ (`clustering_threshold`) around the center.
-    fading_factor
-        Parameter that controls the importance of historical data to current cluster.
-        Note that `fading_factor` has to be different from `0`.
-    cleanup_interval
-        The time interval between two consecutive time points when the cleanup process is
-         conducted.
-    minimum_weight
-        The minimum weight for a cluster to be not "noisy".
-    intersection_factor
-        The intersection factor related to the area of the overlap of the micro clusters
-        relative to the area cover by micro clusters. This parameter is used to determine
-        whether a micro cluster or a shared density is weak.
-
-
-    Attributes
-    ----------
-    n_clusters
-        Number of clusters generated by the algorithm.
-    clusters
-        A set of final clusters of type `DBStreamMicroCluster`. However, these are either
-        micro clusters, or macro clusters that are generated by merging all $\alpha$-connected
-        micro clusters. This set is generated through the offline phase of the algorithm.
-    centers
-        Final clusters' centers.
-    micro_clusters
-        Micro clusters generated by the algorithm. Instead of updating directly the new instance points
-        into a nearest micro cluster, through each iteration, the weight and center will be modified
-        so that the clusters are closer to the new points, using the Gaussian neighborhood function.
-
-    References
-    ----------
-    [^1]: Michael Hahsler and Matthew Bolanos (2016, pp 1449-1461). Clsutering Data Streams Based on
-          Shared Density between Micro-Clusters, IEEE Transactions on Knowledge and Data Engineering 28(6) .
-          In Proceedings of the Sixth SIAM International Conference on Data Mining,
-          April 20–22, 2006, Bethesda, MD, USA.
-    [^2]: Ester et al (1996). A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases
-          with Noise. In KDD-96 Proceedings, AAAI.
-
-    Examples
-    ----------
-
-    >>> from river import cluster
-    >>> from river import stream
-
-    >>> X = [
-    ...     [1, 0.5], [1, 0.625], [1, 0.75], [1, 1.125], [1, 1.5], [1, 1.75],
-    ...     [4, 1.5], [4, 2.25], [4, 2.5], [4, 3], [4, 3.25], [4, 3.5]
-    ... ]
-
-    >>> dbstream = cluster.DBSTREAM(clustering_threshold = 1.5,
-    ...                             fading_factor = 0.05,
-    ...                             cleanup_interval = 4,
-    ...                             intersection_factor = 0.5,
-    ...                             minimum_weight = 1)
-
-    >>> for x, _ in stream.iter_array(X):
-    ...     dbstream = dbstream.learn_one(x)
-
-    >>> dbstream.predict_one({0: 1, 1: 2})
-    0
-
-    >>> dbstream.predict_one({0: 5, 1: 2})
-    1
-
-    >>> dbstream.n_clusters
-    2
-    """
-
-    def __init__(
-        self,
-        clustering_threshold: float = 1.0,
-        fading_factor: float = 0.01,
-        cleanup_interval: float = 2,
-        intersection_factor: float = 0.3,
-        minimum_weight: float = 1.0,
-    ):
-        super().__init__()
-        self.time_stamp = 0
-
-        self.clustering_threshold = clustering_threshold
-        self.fading_factor = fading_factor
-        self.cleanup_interval = cleanup_interval
-        self.minimum_weight = minimum_weight
-        self.intersection_factor = intersection_factor
-
-        self.n_clusters = 0
-        self.clusters = {}
-        self.centers = {}
-        self.micro_clusters = {}
-
-        self.s = {}
-        self.s_t = {}
-
-    @staticmethod
-    def _distance(point_a, point_b):
-        return math.sqrt(utils.math.minkowski_distance(point_a, point_b, 2))
-
-    def _find_fixed_radius_nn(self, x):
-        fixed_radius_nn = {}
-        for i in self.micro_clusters.keys():
-            if (
-                self._distance(self.micro_clusters[i].center, x)
-                < self.clustering_threshold
-            ):
-                fixed_radius_nn[i] = self.micro_clusters[i]
-        return fixed_radius_nn
-
-    def _gaussian_neighborhood(self, point_a, point_b):
-        distance = self._distance(point_a, point_b)
-        sigma = self.clustering_threshold / 3
-        gaussian_neighborhood = math.exp(-(distance * distance) / (2 * (sigma * sigma)))
-        return gaussian_neighborhood
-
-    def _update(self, x):
-        # Algorithm 1 of Michael Hahsler and Matthew Bolanos
-
-        neighbor_clusters = self._find_fixed_radius_nn(x)
-
-        if len(neighbor_clusters) < 1:
-            # create new micro cluster
-            self.micro_clusters[len(self.micro_clusters)] = DBSTREAMMicroCluster(
-                x=x, last_update=self.time_stamp, weight=1
-            )
-        else:
-            # update existing micro clusters
-            current_centers = {}
-            for i in neighbor_clusters.keys():
-                current_centers[i] = self.micro_clusters[i].center
-                self.micro_clusters[i].weight = (
-                    self.micro_clusters[i].weight
-                    * 2
-                    ** (
-                        -self.fading_factor
-                        * (self.time_stamp - self.micro_clusters[i].last_update)
-                    )
-                    + 1
-                )
-                self.micro_clusters[i].center = {
-                    j: self.micro_clusters[i].center[j]
-                    + self._gaussian_neighborhood(x, self.micro_clusters[i].center)
-                    * (x[j] - self.micro_clusters[i].center[j])
-                    for j in self.micro_clusters[i].center.keys()
-                }
-                self.micro_clusters[i].last_update = self.time_stamp
-
-                # update shared density
-                for j in neighbor_clusters.keys():
-                    if j > i:
-                        try:
-                            self.s[i][j] = (
-                                self.s[i][j]
-                                * 2
-                                ** (
-                                    -self.fading_factor
-                                    * (self.time_stamp - self.s_t[i][j])
-                                )
-                                + 1
-                            )
-                            self.s_t[i][j] = self.time_stamp
-                        except KeyError:
-                            try:
-                                self.s[i][j] = 0
-                                self.s_t[i][j] = 0
-                            except KeyError:
-                                self.s[i] = {j: 0}
-                                self.s_t[i] = {j: 0}
-
-            # prevent collapsing clusters
-            for i in neighbor_clusters.keys():
-                for j in neighbor_clusters.keys():
-                    if j > i:
-                        if (
-                            self._distance(
-                                self.micro_clusters[i].center,
-                                self.micro_clusters[j].center,
-                            )
-                            < self.clustering_threshold
-                        ):
-                            # revert centers of mc_i and mc_j to previous positions
-                            self.micro_clusters[i].center = current_centers[i]
-                            self.micro_clusters[j].center = current_centers[j]
-
-        self.time_stamp += 1
-
-    def _cleanup(self):
-        # Algorithm 2 of Michael Hahsler and Matthew Bolanos: Cleanup process to remove
-        # inactive clusters and shared density entries from memory
-
-        weight_weak = 2 ** (-self.fading_factor * self.cleanup_interval)
-
-        for i, micro_cluster_i in self.micro_clusters.items():
-            if (
-                micro_cluster_i.weight
-                * (
-                    2
-                    ** (
-                        self.fading_factor
-                        * (self.time_stamp - micro_cluster_i.last_update)
-                    )
-                )
-                < weight_weak
-            ):
-                self.micro_clusters.pop(i)
-
-        for i in self.s.keys():
-            for j in self.s[i].keys():
-                if (
-                    self.s[i][j]
-                    * (2 ** (self.fading_factor * (self.time_stamp - self.s_t[i][j])))
-                    < self.intersection_factor * weight_weak
-                ):
-                    self.s[i][j] = 0
-                    self.s_t[i][j] = 0
-
-    def _generate_weighted_adjacency_matrix(self):
-        # Algorithm 3 of Michael Hahsler and Matthew Bolanos: Reclustering using
-        # shared density graph
-
-        weighted_adjacency_matrix = {}
-        for i in list(self.s.keys()):
-            for j in list(self.s[i].keys()):
-                if (
-                    self.micro_clusters[i].weight >= self.minimum_weight
-                    and self.micro_clusters[j].weight >= self.minimum_weight
-                ):
-                    value = self.s[i][j] / (
-                        (self.micro_clusters[i].weight + self.micro_clusters[j].weight)
-                        / 2
-                    )
-                    if value > self.intersection_factor:
-                        try:
-                            weighted_adjacency_matrix[i][j] = value
-                        except KeyError:
-                            weighted_adjacency_matrix[i] = {j: value}
-
-        return weighted_adjacency_matrix
-
-    def _generate_labels(self, weighted_adjacency_list):
-
-        # This function handles the weighted adjacency list created above and
-        # generate a cluster label for all micro clusters, using a variant of
-        # the DBSCAN algorithm proposed by Ester et al. for alpha-connected micro clusters
-
-        # initiate labels of micro clusters to None
-        labels = {i: None for i in self.micro_clusters.keys()}
-
-        # cluster counter; in this algorithm, cluster labels starts with 0
-        count = -1
-
-        for index in labels.keys():
-            if labels[index] is not None:
-                continue
-            count += 1
-            labels[index] = count
-            # if it is not in list of alpha-connected micro-clusters, label and continue
-            if index not in weighted_adjacency_list.keys():
-                continue
-            seed_set = collections.deque(weighted_adjacency_list[index].keys())
-            while seed_set:
-                # check previously processed points
-                if labels[seed_set[0]] is not None:
-                    seed_set.popleft()
-                    continue
-                # proceed DBSCAN when seed set is not blank
-                if seed_set:
-                    labels[seed_set[0]] = count
-                    # find neighbors
-                    if seed_set[0] in weighted_adjacency_list.keys():
-                        neighbor_neighbors = collections.deque(
-                            weighted_adjacency_list[seed_set[0]].keys()
-                        )
-                        # add new neighbors to seed set
-                        for neighbor_neighbor in neighbor_neighbors:
-                            if labels[neighbor_neighbor] is not None:
-                                seed_set.append(neighbor_neighbor)
-
-        return labels
-
-    def _generate_clusters_from_labels(self, cluster_labels):
-        # initiate the set for final clusters
-        clusters = {}
-
-        # generate set of clusters with the same label with the structure {j: micro_cluster_index}
-        for i in range(max(cluster_labels.values()) + 1):
-            j = 0
-            mcs_with_label_i = {}
-            for index, label in cluster_labels.items():
-                if label == i:
-                    mcs_with_label_i[j] = self.micro_clusters[index]
-                    j += 1
-
-            # generate a final macro-cluster from clusters with the same label using the
-            # merge function of DBStreamMicroCluster
-            macro_cluster = mcs_with_label_i[0]
-            for m in range(1, len(mcs_with_label_i)):
-                macro_cluster.merge(mcs_with_label_i[m])
-
-            clusters[i] = macro_cluster
-
-        n_clusters = len(clusters)
-
-        return n_clusters, clusters
-
-    def _recluster(self):
-        # Algorithm 3 of Michael Hahsler and Matthew Bolanos: Reclustering
-        # using shared density graph
-
-        weighted_adjacency_list = self._generate_weighted_adjacency_matrix()
-
-        labels = self._generate_labels(weighted_adjacency_list)
-
-        self.n_clusters, self.clusters = self._generate_clusters_from_labels(labels)
-
-        self.centers = {i: self.clusters[i].center for i in self.clusters.keys()}
-
-    def learn_one(self, x, sample_weight=None):
-
-        self._update(x)
-
-        if self.time_stamp % self.cleanup_interval == 0:
-            self._cleanup()
-
-        return self
-
-    def predict_one(self, x, sample_weight=None):
-
-        self._recluster()
-
-        min_distance = math.inf
-
-        # default result of all clustering results, regardless of whether there already
-        # exists macro-clusters
-        closest_cluster_index = 0
-
-        for i, center_i in self.centers.items():
-            distance = self._distance(center_i, x)
-            if distance < min_distance:
-                min_distance = distance
-                closest_cluster_index = i
-        return closest_cluster_index
-
-
-class DBSTREAMMicroCluster(metaclass=ABCMeta):
-    """ DBStream Micro-cluster class """
-
-    def __init__(self, x=None, last_update=None, weight=None):
-
-        self.center = x
-        self.last_update = last_update
-        self.weight = weight
-
-    def merge(self, cluster):
-        self.center = {
-            i: (self.center[i] * self.weight + cluster.center[i] * cluster.weight)
-            / (self.weight + cluster.weight)
-            for i in self.center.keys()
-        }
-        self.weight += cluster.weight
+import copy
+import math
+from abc import ABCMeta
+from collections import defaultdict, deque
+
+from river import base, utils
+
+
+class DenStream(base.Clusterer):
+    r"""DenStream
+
+    DenStream [^1] is a clustering algorithm for evolving data streams.
+    DenStream can discover clusters with arbitrary shape and is robust against
+    noise (outliers).
+
+    "Dense" micro-clusters (named core-micro-clusters) summarise the clusters
+    of arbitrary shape. A pruning strategy based on the concepts of potential
+    and outlier micro-clusters guarantees the precision of the weights of the
+    micro-clusters with limited memory.
+
+    The algorithm is divided into two parts:
+
+    **Online micro-cluster maintenance (learning)**
+
+    For a new point `p`:
+
+    * Try to merge `p` into either the nearest `p-micro-cluster` (potential),
+    `o-micro-cluster` (outlier), or create a new `o-micro-cluster` and insert it
+    into the outlier buffer.
+
+    * For each `T_p` iterations, consider the weights of all potential and
+    outlier micro-clusters. If their weights are smaller than a certain
+    threshold (different for each type of micro-clusters), the micro-cluster is
+    deleted.
+
+    **Offline generation of clusters on-demand (clustering)**
+
+    A variant of the DBSCAN algorithm [^2] is used, such that all
+    density-connected p-micro-clusters determine the final clusters.
+
+    Parameters
+    ----------
+    decaying_factor
+        Parameter that controls the importance of historical data to current cluster.
+        Note that `decaying_factor` has to be different from `0`.
+
+    beta
+        Parameter to determine the threshold of outlier relative to core micro-clusters.
+        Valid values are `0 < \beta <= 1`.
+
+    mu
+        Parameter to determine the threshold of outliers relative to core micro-cluster.
+        Valid values are `\mu > 0`.
+
+    epsilon
+        Defines the epsilon neighborhood
+
+    n_samples_init
+        Number of points to to initiqalize the online process
+
+    stream_speed
+        Number of points arrived in unit time
+
+    Attributes
+    ----------
+    n_clusters
+        Number of clusters generated by the algorithm.
+
+    clusters
+        A set of final clusters of type `MicroCluster`, which means that these cluster include all
+        the required information, including number of points, creation time, weight, (weighted)
+        linear sum, (weighted) square sum, center and radius.
+
+    p_micro_clusters
+        The potential core-icro-clusters that are generated by the algorithm. When a generate
+        cluster request arrives, these p-micro-clusters will go through a variant of the DBSCAN
+        algorithm to determine the final clusters.
+
+    o_micro_clusters
+        The outlier micro-clusters.
+
+    References
+    ----------
+    [^1]: Feng et al (2006, pp 328-339). Density-Based Clustering over an Evolving Data Stream with
+          Noise. In Proceedings of the Sixth SIAM International Conference on Data Mining,
+          April 20–22, 2006, Bethesda, MD, USA.
+    [^2]: Ester et al (1996). A Density-Based Algorithm for Discovering Clusters in Large Spatial
+          Databases with Noise. In KDD-96 Proceedings, AAAI.
+
+    Examples
+    ----------
+
+    The following example uses the default parameters of the algorithm to test its functionality.
+    The set of evolving points `X` are designed so that clusters are easily identifiable.
+
+    >>> from river import cluster
+    >>> from river import stream
+
+    >>> X = [
+    ...     [-1, -0.5], [-1, -0.625], [-1, -0.75], [-1, -1], [-1, -1.125],
+    ...     [-1, -1.25], [-1.5, -0.5], [-1.5, -0.625], [-1.5, -0.75], [-1.5, -1],
+    ...     [-1.5, -1.125], [-1.5, -1.25], [1, 1.5], [1, 1.75], [1, 2],
+    ...     [4, 1.25], [4, 1.5], [4, 2.25], [4, 2.5], [4, 3],
+    ...     [4, 3.25], [4, 3.5], [4, 3.75], [4, 4],
+    ... ]
+
+    >>> denstream = cluster.DenStream(decaying_factor = 0.01,
+    ...                               beta = 1.01,
+    ...                               mu = 1.0005,
+    ...                               epsilon = 0.5,
+    ...                               n_samples_init=10)
+
+    >>> for x, _ in stream.iter_array(X):
+    ...     denstream = denstream.learn_one(x)
+
+    >>> denstream.predict_one({0: -1, 1: -2})
+    1
+
+    >>> denstream.predict_one({0:5, 1:4})
+    0
+
+    >>> denstream.predict_one({0:1, 1:1})
+    2
+
+    >>> denstream.n_clusters
+    3
+
+    """
+
+    class BufferItem:
+        def __init__(self, x, timestamp, covered):
+            self.x = x
+            self.timestamp = (timestamp,)
+            self.covered = covered
+
+    def __init__(
+        self,
+        decaying_factor: float = 0.25,
+        beta: float = 5,
+        mu: float = 0.5,
+        epsilon: float = 0.02,
+        n_samples_init: int = 1000,
+        stream_speed: int = 100,
+    ):
+        super().__init__()
+        self.timestamp = -1
+        self.initialized = False
+        self.decaying_factor = decaying_factor
+        self.beta = beta
+        self.mu = mu
+        self.epsilon = epsilon
+        self.n_samples_init = n_samples_init
+        self.stream_speed = stream_speed
+
+        # number of clusters generated by applying the variant of DBSCAN algorithm
+        # on p-micro-cluster centers and their centers
+        self.n_clusters = 0
+        self.clusters = {}
+        self.p_micro_clusters = {}
+        self.o_micro_clusters = {}
+
+        self._time_period = math.ceil(
+            (1 / self.decaying_factor)
+            * math.log((self.mu * self.beta) / (self.mu * self.beta - 1))
+        )
+        self._init_buffer = deque()
+        self._n_samples_seen = 0
+
+    @property
+    def centers(self):
+        return {
+            k: cluster.calc_center(self.timestamp)
+            for k, cluster in self.clusters.items()
+        }
+
+    @staticmethod
+    def _distance(point_a, point_b):
+        return math.sqrt(utils.math.minkowski_distance(point_a, point_b, 2))
+
+    def _get_closest_cluster_key(self, point, clusters):
+        min_distance = math.inf
+        key = -1
+        for k, cluster in clusters.items():
+            center = cluster.calc_center(self.timestamp)
+            distance = self._distance(center, point)
+            if distance < min_distance:
+                min_distance = distance
+                key = k
+        return key
+
+    def _merge(self, point):
+        # initiate merged status
+        merged_status = False
+
+        if len(self.p_micro_clusters) != 0:
+            # try to merge p into its nearest p-micro-cluster c_p
+            closest_pmc_key = self._get_closest_cluster_key(
+                point, self.p_micro_clusters
+            )
+            updated_pmc = copy.copy(self.p_micro_clusters[closest_pmc_key])
+            updated_pmc.insert(point, self.timestamp)
+            if updated_pmc.calc_radius(self.timestamp) <= self.epsilon:
+                # keep updated p-micro-cluster
+                self.p_micro_clusters[closest_pmc_key] = updated_pmc
+                merged_status = True
+
+        if not merged_status and len(self.o_micro_clusters) != 0:
+            closest_omc_key = self._get_closest_cluster_key(
+                point, self.o_micro_clusters
+            )
+            updated_omc = copy.copy(self.o_micro_clusters[closest_omc_key])
+            updated_omc.insert(point, self.timestamp)
+            if updated_omc.calc_radius(self.timestamp) <= self.epsilon:
+                # keep updated o-micro-cluster
+                if updated_omc.calc_weight(self.timestamp) > self.mu * self.beta:
+                    # it has grown into a p-micro-cluster
+                    del self.o_micro_clusters[closest_omc_key]
+                    self.p_micro_clusters[len(self.p_micro_clusters)] = updated_omc
+                else:
+                    self.o_micro_clusters[closest_omc_key] = updated_omc
+            else:
+                # create a new o-micro-cluster by p and add it to o_micro_clusters
+                mc_from_p = DenStreamMicroCluster(
+                    x=point,
+                    timestamp=self.timestamp,
+                    decaying_factor=self.decaying_factor,
+                )
+                self.o_micro_clusters[len(self.o_micro_clusters)] = mc_from_p
+
+    def _is_directly_density_reachable(self, c_p, c_q):
+        if (
+            c_p.calc_weight(self.timestamp) > self.mu
+            and c_q.calc_weight(self.timestamp) > self.mu
+        ):
+            # check distance of two clusters and compare with 2*epsilon
+            c_p_center = c_p.calc_center(self.timestamp)
+            c_q_center = c_q.calc_center(self.timestamp)
+            distance = self._distance(c_p_center, c_q_center)
+            if distance < 2 * self.epsilon and distance <= c_p.calc_radius(
+                self.timestamp
+            ) + c_q.calc_radius(self.timestamp):
+                return True
+        return False
+
+    def _query_neighbor(self, cluster):
+        neighbors = deque()
+        # scan all clusters within self.p_micro_clusters
+        for pmc in self.p_micro_clusters.values():
+            # check density reachable and that the cluster itself does not appear in neighbors
+            if cluster != pmc and self._is_directly_density_reachable(cluster, pmc):
+                neighbors.append(pmc)
+        return neighbors
+
+    @staticmethod
+    def _generate_clusters_for_labels(cluster_labels):
+        # initiate the dictionary for final clusters
+        clusters = {}
+
+        # group clusters per label
+        mcs_per_label = defaultdict(deque)
+        for mc, label in cluster_labels.items():
+            mcs_per_label[label].append(mc)
+
+        # generate set of clusters with the same label
+        for label, micro_clusters in mcs_per_label.items():
+            # merge clusters with the same label into a big cluster
+            cluster = copy.copy(micro_clusters[0])
+            for mc in range(1, len(micro_clusters)):
+                cluster.merge(micro_clusters[mc])
+
+            clusters[label] = cluster
+
+        return len(clusters), clusters
+
+    def _expand_cluster(self, mc, neighborhood):
+        for idx in neighborhood:
+            item = self._init_buffer[idx]
+            if not item.covered:
+                item.covered = True
+                mc.insert(item.x, self.timestamp)
+
+    def _get_neighborhood_ids(self, item):
+        neighborhood_ids = deque()
+        for idx, other in enumerate(self._init_buffer):
+            if not other.covered:
+                if self._distance(item.x, other.x) < self.epsilon:
+                    neighborhood_ids.append(idx)
+        return neighborhood_ids
+
+    def _initial_dbscan(self):
+        for item in self._init_buffer:
+            if not item.covered:
+                item.covered = True
+                neighborhood = self._get_neighborhood_ids(item)
+                if len(neighborhood) > self.mu:
+                    mc = DenStreamMicroCluster(
+                        x=item.x,
+                        timestamp=self.timestamp,
+                        decaying_factor=self.decaying_factor,
+                    )
+                    self._expand_cluster(mc, neighborhood)
+                    self.p_micro_clusters.update({len(self.p_micro_clusters): mc})
+                else:
+                    item.covered = False
+
+    def learn_one(self, x, sample_weight=None):
+        self._n_samples_seen += 1
+        # control the stream speed
+        if self._n_samples_seen % self.stream_speed == 0:
+            self.timestamp += 1
+
+        # Initialization
+        if not self.initialized:
+            self._init_buffer.append(self.BufferItem(x, self.timestamp, False))
+            if len(self._init_buffer) == self.n_samples_init:
+                self._initial_dbscan()
+                self.initialized = True
+                del self._init_buffer
+            return self
+
+        # Merge
+        self._merge(x)
+
+        # Periodic cluster removal
+        if self.timestamp > 0 and self.timestamp % self._time_period == 0:
+            for i, p_micro_cluster_i in list(self.p_micro_clusters.items()):
+                if p_micro_cluster_i.calc_weight(self.timestamp) < self.mu * self.beta:
+                    # c_p became an outlier and should be deleted
+                    del self.p_micro_clusters[i]
+            for j, o_micro_cluster_j in list(self.o_micro_clusters.items()):
+                # calculate xi
+                xi = (
+                    2
+                    ** (
+                        -self.decaying_factor
+                        * (
+                            self.timestamp
+                            - o_micro_cluster_j.creation_time
+                            + self._time_period
+                        )
+                    )
+                    - 1
+                ) / (2 ** (-self.decaying_factor * self._time_period) - 1)
+                if o_micro_cluster_j.calc_weight(self.timestamp) < xi:
+                    # c_o might not grow into a p-micro-cluster, we can safely delete it
+                    self.o_micro_clusters.pop(j)
+        return self
+
+    def predict_one(self, x, sample_weight=None):
+
+        # This function handles the case when a clustering request arrives.
+        # implementation of the DBSCAN algorithm proposed by Ester et al.
+        if not self.initialized:
+            # The model is not ready
+            return 0
+
+        # cluster counter; in this algorithm cluster labels start with 0
+        c = -1
+        # initiate labels of p-micro-clusters to None
+        labels = {pmc: None for pmc in self.p_micro_clusters.values()}
+
+        for pmc in self.p_micro_clusters.values():
+            # previously processed in inner loop
+            if labels[pmc] is not None:
+                continue
+            # next cluster label
+            c += 1
+            labels[pmc] = c
+            # neighbors to expand
+            seed_queue = self._query_neighbor(pmc)
+            # process every point in seed set
+            while seed_queue:
+                # check previously proceeded points
+                if labels[seed_queue[0]] is not None:
+                    seed_queue.popleft()
+                    continue
+                if seed_queue:
+                    labels[seed_queue[0]] = c
+                    # find neighbors of neighbors
+                    neighbor_neighbors = self._query_neighbor(seed_queue[0])
+                    # add new neighbors to seed set
+                    for neighbor_neighbor in neighbor_neighbors:
+                        if labels[neighbor_neighbor] is not None:
+                            seed_queue.append(neighbor_neighbor)
+
+        self.n_clusters, self.clusters = self._generate_clusters_for_labels(labels)
+
+        return self._get_closest_cluster_key(x, self.clusters)
+
+
+class DenStreamMicroCluster(metaclass=ABCMeta):
+    """ DenStream Micro-cluster class """
+
+    def __init__(self, x, timestamp, decaying_factor):
+
+        self.x = x
+        self.last_edit_time = timestamp
+        self.creation_time = timestamp
+        self.decaying_factor = decaying_factor
+
+        self.N = 1
+        self.linear_sum = x
+        self.squared_sum = {i: (x_val * x_val) for i, x_val in x.items()}
+
+    def calc_norm_cf1_cf2(self, fading_factor):
+        # |CF1| and |CF2| in the paper
+        sum_of_squares_cf1 = 0
+        sum_of_squares_cf2 = 0
+        for key in self.linear_sum.keys():
+            val_ls = self.linear_sum[key]
+            val_ss = self.squared_sum[key]
+            sum_of_squares_cf1 += fading_factor * val_ls * fading_factor * val_ls
+            sum_of_squares_cf2 += fading_factor * val_ss * fading_factor * val_ss
+        # return |CF1| and |CF2|
+        return math.sqrt(sum_of_squares_cf1), math.sqrt(sum_of_squares_cf2)
+
+    def calc_weight(self, timestamp):
+        return self._weight(self.fading_function(timestamp - self.last_edit_time))
+
+    def _weight(self, fading_factor):
+        return self.N * fading_factor
+
+    def calc_center(self, timestamp):
+        ff = self.fading_function(timestamp - self.last_edit_time)
+        weight = self._weight(ff)
+        center = {key: (ff * val) / weight for key, val in self.linear_sum.items()}
+        return center
+
+    def calc_radius(self, timestamp):
+        ff = self.fading_function(timestamp - self.last_edit_time)
+        weight = self._weight(ff)
+        norm_cf1, norm_cf2 = self.calc_norm_cf1_cf2(ff)
+        diff = (norm_cf2 / weight) - ((norm_cf1 / weight) ** 2)
+        radius = math.sqrt(diff) if diff > 0 else 0
+        return radius
+
+    def insert(self, x, timestamp):
+        self.N += 1
+        self.last_edit_time = timestamp
+        for key, val in x.items():
+            try:
+                self.linear_sum[key] += val
+                self.squared_sum[key] += val * val
+            except KeyError:
+                self.linear_sum[key] = val
+                self.squared_sum[key] = val * val
+
+    def merge(self, cluster):
+        self.N += cluster.N
+        for key in cluster.linear_sum.keys():
+            try:
+                self.linear_sum[key] += cluster.linear_sum[key]
+                self.squared_sum[key] += cluster.squared_sum[key]
+            except KeyError:
+                self.linear_sum[key] = cluster.linear_sum[key]
+                self.squared_sum[key] = cluster.squared_sum[key]
+        if self.last_edit_time < cluster.creation_time:
+            self.last_edit_time = cluster.creation_time
+
+    def fading_function(self, time):
+        return 2 ** (-self.decaying_factor * time)
```

### Comparing `river-0.8.0/river/cluster/k_means.py` & `river-0.9.0/river/cluster/k_means.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,123 +1,127 @@
-import collections
-import functools
-import random
-
-from river import base, utils
-
-__all__ = ["KMeans"]
-
-
-class KMeans(base.Clusterer):
-    """Incremental k-means.
-
-    The most common way to implement batch k-means is to use Lloyd's algorithm, which consists in
-    assigning all the data points to a set of cluster centers and then moving the centers
-    accordingly. This requires multiple passes over the data and thus isn't applicable in a
-    streaming setting.
-
-    In this implementation we start by finding the cluster that is closest to the current
-    observation. We then move the cluster's central position towards the new observation. The
-    `halflife` parameter determines by how much to move the cluster toward the new observation.
-    You will get better results if you scale your data appropriately.
-
-    Parameters
-    ----------
-    n_clusters
-        Maximum number of clusters to assign.
-    halflife
-        Amount by which to move the cluster centers, a reasonable value if between 0 and 1.
-    mu
-        Mean of the normal distribution used to instantiate cluster positions.
-    sigma
-        Standard deviation of the normal distribution used to instantiate cluster positions.
-    p
-        Power parameter for the Minkowski metric. When `p=1`, this corresponds to the Manhattan
-        distance, while `p=2` corresponds to the Euclidean distance.
-    seed
-        Random seed used for generating initial centroid positions.
-
-    Attributes
-    ----------
-    centers : dict
-        Central positions of each cluster.
-
-    Examples
-    --------
-
-    In the following example the cluster assignments are exactly the same as when using
-    `sklearn`'s batch implementation. However changing the `halflife` parameter will
-    produce different outputs.
-
-    >>> from river import cluster
-    >>> from river import stream
-
-    >>> X = [
-    ...     [1, 2],
-    ...     [1, 4],
-    ...     [1, 0],
-    ...     [4, 2],
-    ...     [4, 4],
-    ...     [4, 0]
-    ... ]
-
-    >>> k_means = cluster.KMeans(n_clusters=2, halflife=0.4, sigma=3, seed=0)
-
-    >>> for i, (x, _) in enumerate(stream.iter_array(X)):
-    ...     k_means = k_means.learn_one(x)
-    ...     print(f'{X[i]} is assigned to cluster {k_means.predict_one(x)}')
-    [1, 2] is assigned to cluster 1
-    [1, 4] is assigned to cluster 1
-    [1, 0] is assigned to cluster 0
-    [4, 2] is assigned to cluster 0
-    [4, 4] is assigned to cluster 0
-    [4, 0] is assigned to cluster 0
-
-    >>> k_means.predict_one({0: 0, 1: 0})
-    1
-
-    >>> k_means.predict_one({0: 4, 1: 4})
-    0
-
-    References
-    ----------
-    [^1]: [Sequential k-Means Clustering](http://www.cs.princeton.edu/courses/archive/fall08/cos436/Duda/C/sk_means.htm)
-    [^2]: [Sculley, D., 2010, April. Web-scale k-means clustering. In Proceedings of the 19th international conference on World wide web (pp. 1177-1178](https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf)
-
-    """
-
-    def __init__(
-        self, n_clusters=5, halflife=0.5, mu=0, sigma=1, p=2, seed: int = None
-    ):
-        self.n_clusters = n_clusters
-        self.halflife = halflife
-        self.mu = mu
-        self.sigma = sigma
-        self.p = p
-        self.seed = seed
-        self._rng = random.Random(seed)
-        rand_gauss = functools.partial(self._rng.gauss, self.mu, self.sigma)
-        self.centers = {
-            i: collections.defaultdict(rand_gauss) for i in range(n_clusters)
-        }  # type: ignore
-
-    def learn_predict_one(self, x):
-        """Equivalent to `k_means.learn_one(x).predict_one(x)`, but faster."""
-
-        # Find the cluster with the closest center
-        closest = self.predict_one(x)
-
-        # Move the cluster's center
-        for i, xi in x.items():
-            self.centers[closest][i] += self.halflife * (xi - self.centers[closest][i])
-
-        return closest
-
-    def learn_one(self, x):
-        self.learn_predict_one(x)
-        return self
-
-    def predict_one(self, x):
-        def get_distance(c):
-            return utils.math.minkowski_distance(a=self.centers[c], b=x, p=self.p)
-
-        return min(self.centers, key=get_distance)
+import collections
+import functools
+import random
+
+from river import base, utils
+
+__all__ = ["KMeans"]
+
+
+class KMeans(base.Clusterer):
+    """Incremental k-means.
+
+    The most common way to implement batch k-means is to use Lloyd's algorithm, which consists in
+    assigning all the data points to a set of cluster centers and then moving the centers
+    accordingly. This requires multiple passes over the data and thus isn't applicable in a
+    streaming setting.
+
+    In this implementation we start by finding the cluster that is closest to the current
+    observation. We then move the cluster's central position towards the new observation. The
+    `halflife` parameter determines by how much to move the cluster toward the new observation.
+    You will get better results if you scale your data appropriately.
+
+    Parameters
+    ----------
+    n_clusters
+        Maximum number of clusters to assign.
+    halflife
+        Amount by which to move the cluster centers, a reasonable value if between 0 and 1.
+    mu
+        Mean of the normal distribution used to instantiate cluster positions.
+    sigma
+        Standard deviation of the normal distribution used to instantiate cluster positions.
+    p
+        Power parameter for the Minkowski metric. When `p=1`, this corresponds to the Manhattan
+        distance, while `p=2` corresponds to the Euclidean distance.
+    seed
+        Random seed used for generating initial centroid positions.
+
+    Attributes
+    ----------
+    centers : dict
+        Central positions of each cluster.
+
+    Examples
+    --------
+
+    In the following example the cluster assignments are exactly the same as when using
+    `sklearn`'s batch implementation. However changing the `halflife` parameter will
+    produce different outputs.
+
+    >>> from river import cluster
+    >>> from river import stream
+
+    >>> X = [
+    ...     [1, 2],
+    ...     [1, 4],
+    ...     [1, 0],
+    ...     [4, 2],
+    ...     [4, 4],
+    ...     [4, 0]
+    ... ]
+
+    >>> k_means = cluster.KMeans(n_clusters=2, halflife=0.4, sigma=3, seed=0)
+
+    >>> for i, (x, _) in enumerate(stream.iter_array(X)):
+    ...     k_means = k_means.learn_one(x)
+    ...     print(f'{X[i]} is assigned to cluster {k_means.predict_one(x)}')
+    [1, 2] is assigned to cluster 1
+    [1, 4] is assigned to cluster 1
+    [1, 0] is assigned to cluster 0
+    [4, 2] is assigned to cluster 0
+    [4, 4] is assigned to cluster 0
+    [4, 0] is assigned to cluster 0
+
+    >>> k_means.predict_one({0: 0, 1: 0})
+    1
+
+    >>> k_means.predict_one({0: 4, 1: 4})
+    0
+
+    References
+    ----------
+    [^1]: [Sequential k-Means Clustering](http://www.cs.princeton.edu/courses/archive/fall08/cos436/Duda/C/sk_means.htm)
+    [^2]: [Sculley, D., 2010, April. Web-scale k-means clustering. In Proceedings of the 19th international conference on World wide web (pp. 1177-1178](https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf)
+
+    """
+
+    def __init__(
+        self, n_clusters=5, halflife=0.5, mu=0, sigma=1, p=2, seed: int = None
+    ):
+        self.n_clusters = n_clusters
+        self.halflife = halflife
+        self.mu = mu
+        self.sigma = sigma
+        self.p = p
+        self.seed = seed
+        self._rng = random.Random(seed)
+        rand_gauss = functools.partial(self._rng.gauss, self.mu, self.sigma)
+        self.centers = {
+            i: collections.defaultdict(rand_gauss) for i in range(n_clusters)
+        }  # type: ignore
+
+    def learn_predict_one(self, x):
+        """Equivalent to `k_means.learn_one(x).predict_one(x)`, but faster."""
+
+        # Find the cluster with the closest center
+        closest = self.predict_one(x)
+
+        # Move the cluster's center
+        for i, xi in x.items():
+            self.centers[closest][i] += self.halflife * (xi - self.centers[closest][i])
+
+        return closest
+
+    def learn_one(self, x):
+        self.learn_predict_one(x)
+        return self
+
+    def predict_one(self, x):
+        def get_distance(c):
+            return utils.math.minkowski_distance(a=self.centers[c], b=x, p=self.p)
+
+        return min(self.centers, key=get_distance)
+
+    @classmethod
+    def _unit_test_params(cls):
+        yield {"n_clusters": 5}
```

### Comparing `river-0.8.0/river/cluster/streamkmeans.py` & `river-0.9.0/river/cluster/streamkmeans.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,122 +1,122 @@
-from river import base, cluster, utils
-
-
-class STREAMKMeans(base.Clusterer):
-    r"""STREAMKMeans
-
-    STREAMKMeans is an alternative version of the original algorithm STREAMLSEARCH proposed by
-    O'Callaghan et al. [^1] by replacing the `k-Medians` using `LSEARCH` by the classical
-    `KMeans` algorithm.
-
-    However, instead of using the traditional `KMeans` that requires a total reclustering after
-    each time the temporary chunk of data points is full, the implementation of this algorithm
-    in `River` uses the increamental `KMeans`. This allows the algorithm to update `KMeans`
-    without the need of re-initialization, saving a substantial amount of computing resources.
-
-    The algorithm is constructed as follows. To begin, the algorithm will be initialized
-    with an incremental `KMeans` algorithm with the same number of centers as required.
-    For a new point `p`:
-
-    * If the size of chunk is less than the maximum size allowed, add the new point to
-    the temporary chunk.
-
-    * When the size of chunk reaches the maximum value size allowed
-
-        - A new incremental `KMeans` algorithm will be initiated. This algorithm will run
-        through all points in the temporary chunk. The centers of this new algorithm will
-        be passed through the originally initialized `KMeans` to update the centers of the
-        algorithm
-
-        - All points will be deleted from the temporary chunk to continue adding new points later.
-
-    * When a prediction request arrives, the centers of the algorithm will be exactly the same
-    as the centers of the original `KMeans` at the time of retrieval.
-
-    Parameters
-    ----------
-    chunk_size
-        Maximum size allowed for the temporary data chunk.
-
-    n_clusters
-        Number of clusters generated by the algorithm.
-
-    kwargs
-        Other parameters passed to the incremental kmeans at `cluster.KMeans`.
-
-    Attributes
-    ----------
-    centers
-        Cluster centers generated from running the incremental `KMeans` algorithm
-        through centers of each chunk.
-
-    References
-    ----------
-    [^1]: O'Callaghan et al. (2002). Streaming-data algorithms for high-quality clustering.
-          In Proceedings 18th International Conference on Data Engineering, Feb 26 - March 1,
-          San Jose, CA, USA. DOI: 10.1109/ICDE.2002.994785.
-
-    Examples
-    ----------
-
-    >>> from river import cluster
-    >>> from river import stream
-
-    >>> X = [
-    ...     [1, 0.5], [1, 0.625], [1, 0.75], [1, 1.125], [1, 1.5], [1, 1.75],
-    ...     [4, 1.5], [4, 2.25], [4, 2.5], [4, 3], [4, 3.25], [4, 3.5]
-    ... ]
-
-    >>> streamkmeans = cluster.STREAMKMeans(chunk_size=3, n_clusters=2, halflife=0.5, sigma=1.5, seed=0)
-
-    >>> for x, _ in stream.iter_array(X):
-    ...     streamkmeans = streamkmeans.learn_one(x)
-
-    >>> streamkmeans.predict_one({0:1, 1:0})
-    1
-
-    >>> streamkmeans.predict_one({0:5, 1:2})
-    0
-
-    """
-
-    def __init__(self, chunk_size=10, n_clusters=2, **kwargs):
-
-        super().__init__()
-        self.time_stamp = 0
-        self.n_clusters = n_clusters
-        self.chunk_size = chunk_size
-        self.kwargs = kwargs
-
-        self._kmeans = cluster.KMeans(n_clusters=self.n_clusters, **self.kwargs)
-        self._temp_chunk = {}
-        self.centers = {}
-
-    def learn_one(self, x, sample_weight=None):
-
-        self.time_stamp += 1
-
-        index = self.time_stamp % self.chunk_size
-
-        if index == 0:
-            self._temp_chunk[self.chunk_size - 1] = x
-        elif index == 1:
-            self._temp_chunk = {0: x}
-        else:
-            self._temp_chunk[index - 1] = x
-
-        if index == 0:
-            kmeans_i = cluster.KMeans(n_clusters=self.n_clusters, **self.kwargs)
-            for point_j in self._temp_chunk.values():
-                kmeans_i = kmeans_i.learn_one(point_j)
-            for center_j in kmeans_i.centers.values():
-                self._kmeans = self._kmeans.learn_one(center_j)
-
-        self.centers = self._kmeans.centers
-
-        return self
-
-    def predict_one(self, x, sample_weight=None):
-        def get_distance(c):
-            return utils.math.minkowski_distance(self.centers[c], x, 2)
-
-        return min(self.centers, key=get_distance)
+from river import base, cluster, utils
+
+
+class STREAMKMeans(base.Clusterer):
+    r"""STREAMKMeans
+
+    STREAMKMeans is an alternative version of the original algorithm STREAMLSEARCH proposed by
+    O'Callaghan et al. [^1] by replacing the `k-Medians` using `LSEARCH` by the classical
+    `KMeans` algorithm.
+
+    However, instead of using the traditional `KMeans` that requires a total reclustering after
+    each time the temporary chunk of data points is full, the implementation of this algorithm
+    in `River` uses the increamental `KMeans`. This allows the algorithm to update `KMeans`
+    without the need of re-initialization, saving a substantial amount of computing resources.
+
+    The algorithm is constructed as follows. To begin, the algorithm will be initialized
+    with an incremental `KMeans` algorithm with the same number of centers as required.
+    For a new point `p`:
+
+    * If the size of chunk is less than the maximum size allowed, add the new point to
+    the temporary chunk.
+
+    * When the size of chunk reaches the maximum value size allowed
+
+        - A new incremental `KMeans` algorithm will be initiated. This algorithm will run
+        through all points in the temporary chunk. The centers of this new algorithm will
+        be passed through the originally initialized `KMeans` to update the centers of the
+        algorithm
+
+        - All points will be deleted from the temporary chunk to continue adding new points later.
+
+    * When a prediction request arrives, the centers of the algorithm will be exactly the same
+    as the centers of the original `KMeans` at the time of retrieval.
+
+    Parameters
+    ----------
+    chunk_size
+        Maximum size allowed for the temporary data chunk.
+
+    n_clusters
+        Number of clusters generated by the algorithm.
+
+    kwargs
+        Other parameters passed to the incremental kmeans at `cluster.KMeans`.
+
+    Attributes
+    ----------
+    centers
+        Cluster centers generated from running the incremental `KMeans` algorithm
+        through centers of each chunk.
+
+    References
+    ----------
+    [^1]: O'Callaghan et al. (2002). Streaming-data algorithms for high-quality clustering.
+          In Proceedings 18th International Conference on Data Engineering, Feb 26 - March 1,
+          San Jose, CA, USA. DOI: 10.1109/ICDE.2002.994785.
+
+    Examples
+    ----------
+
+    >>> from river import cluster
+    >>> from river import stream
+
+    >>> X = [
+    ...     [1, 0.5], [1, 0.625], [1, 0.75], [1, 1.125], [1, 1.5], [1, 1.75],
+    ...     [4, 1.5], [4, 2.25], [4, 2.5], [4, 3], [4, 3.25], [4, 3.5]
+    ... ]
+
+    >>> streamkmeans = cluster.STREAMKMeans(chunk_size=3, n_clusters=2, halflife=0.5, sigma=1.5, seed=0)
+
+    >>> for x, _ in stream.iter_array(X):
+    ...     streamkmeans = streamkmeans.learn_one(x)
+
+    >>> streamkmeans.predict_one({0:1, 1:0})
+    1
+
+    >>> streamkmeans.predict_one({0:5, 1:2})
+    0
+
+    """
+
+    def __init__(self, chunk_size=10, n_clusters=2, **kwargs):
+
+        super().__init__()
+        self.time_stamp = 0
+        self.n_clusters = n_clusters
+        self.chunk_size = chunk_size
+        self.kwargs = kwargs
+
+        self._kmeans = cluster.KMeans(n_clusters=self.n_clusters, **self.kwargs)
+        self._temp_chunk = {}
+        self.centers = {}
+
+    def learn_one(self, x, sample_weight=None):
+
+        self.time_stamp += 1
+
+        index = self.time_stamp % self.chunk_size
+
+        if index == 0:
+            self._temp_chunk[self.chunk_size - 1] = x
+        elif index == 1:
+            self._temp_chunk = {0: x}
+        else:
+            self._temp_chunk[index - 1] = x
+
+        if index == 0:
+            kmeans_i = cluster.KMeans(n_clusters=self.n_clusters, **self.kwargs)
+            for point_j in self._temp_chunk.values():
+                kmeans_i = kmeans_i.learn_one(point_j)
+            for center_j in kmeans_i.centers.values():
+                self._kmeans = self._kmeans.learn_one(center_j)
+
+        self.centers = self._kmeans.centers
+
+        return self
+
+    def predict_one(self, x, sample_weight=None):
+        def get_distance(c):
+            return utils.math.minkowski_distance(self.centers[c], x, 2)
+
+        return min(self.centers, key=get_distance)
```

### Comparing `river-0.8.0/river/compat/river_to_sklearn.py` & `river-0.9.0/river/compat/river_to_sklearn.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,634 +1,634 @@
-import copy
-import typing
-
-import numpy as np
-
-try:
-    import pandas as pd
-
-    PANDAS_INSTALLED = True
-except ImportError:
-    PANDAS_INSTALLED = False
-from sklearn import base as sklearn_base
-from sklearn import pipeline, preprocessing, utils
-
-from river import base, compose, stream
-
-__all__ = [
-    "convert_river_to_sklearn",
-    "River2SKLRegressor",
-    "River2SKLClassifier",
-    "River2SKLClusterer",
-    "River2SKLTransformer",
-]
-
-
-# Define a streaming method for each kind of batch input
-STREAM_METHODS: typing.Dict[typing.Type, typing.Callable] = {
-    np.ndarray: stream.iter_array
-}
-
-if PANDAS_INSTALLED:
-    STREAM_METHODS[pd.DataFrame] = stream.iter_pandas
-
-# Params passed to sklearn.utils.check_X_y and sklearn.utils.check_array
-SKLEARN_INPUT_X_PARAMS = {
-    "accept_sparse": False,
-    "accept_large_sparse": True,
-    "dtype": "numeric",
-    "order": None,
-    "copy": False,
-    "force_all_finite": True,
-    "ensure_2d": True,
-    "allow_nd": False,
-    "ensure_min_samples": 1,
-    "ensure_min_features": 1,
-}
-
-# Params passed to sklearn.utils.check_X_y in addition to SKLEARN_INPUT_X_PARAMS
-SKLEARN_INPUT_Y_PARAMS = {"multi_output": False, "y_numeric": False}
-
-
-def convert_river_to_sklearn(estimator: base.Estimator):
-    """Wraps a river estimator to make it compatible with scikit-learn.
-
-    Parameters
-    ----------
-    estimator
-
-    """
-
-    if isinstance(estimator, compose.Pipeline):
-        return pipeline.Pipeline(
-            [
-                (name, convert_river_to_sklearn(step))
-                for name, step in estimator.steps.items()
-            ]
-        )
-
-    wrappers = [
-        (base.Classifier, River2SKLClassifier),
-        (base.Clusterer, River2SKLClusterer),
-        (base.Regressor, River2SKLRegressor),
-        (base.Transformer, River2SKLTransformer),
-    ]
-
-    for base_type, wrapper in wrappers:
-        if isinstance(estimator, base_type):
-            obj = wrapper(estimator)
-            obj.instance_ = copy.deepcopy(estimator)
-            return obj
-
-    raise ValueError("Couldn't find an appropriate wrapper")
-
-
-class River2SKLBase(sklearn_base.BaseEstimator, base.WrapperMixin):
-    """This class is just here for house-keeping."""
-
-    @property
-    def _wrapped_model(self):
-        return self.river_estimator
-
-    _required_parameters = ["river_estimator"]
-
-
-class River2SKLRegressor(River2SKLBase, sklearn_base.RegressorMixin):
-    """Compatibility layer from River to scikit-learn for regression.
-
-    Parameters
-    ----------
-    river_estimator
-
-    """
-
-    def __init__(self, river_estimator: base.Regressor):
-
-        # Check the estimator is a Regressor
-        if not isinstance(river_estimator, base.Regressor):
-            raise ValueError("river_estimator is not a Regressor")
-
-        self.river_estimator = river_estimator
-
-    def _partial_fit(self, X, y):
-
-        # Check the inputs
-        X, y = utils.check_X_y(X, y, **SKLEARN_INPUT_X_PARAMS, **SKLEARN_INPUT_Y_PARAMS)
-
-        # Store the number of features so that future inputs can be checked
-        if hasattr(self, "n_features_in_") and X.shape[1] != self.n_features_in_:
-            raise ValueError(
-                f"Expected {self.n_features_in_} features, got {X.shape[1]}"
-            )
-        self.n_features_in_ = X.shape[1]
-
-        # scikit-learn's convention is that fit shouldn't mutate the input parameters; we have to
-        # deep copy the provided estimator in order to respect this convention
-        if not hasattr(self, "instance_"):
-            self.instance_ = copy.deepcopy(self.river_estimator)
-
-        # Call learn_one for each observation
-        for x, yi in STREAM_METHODS[type(X)](X, y):
-            self.instance_.learn_one(x, yi)
-
-        return self
-
-    def fit(self, X, y):
-        """Fits to an entire dataset contained in memory.
-
-        Parameters
-        ----------
-        X
-            array-like of shape (n_samples, n_features).
-        y
-            array-like of shape n_samples.
-
-        Returns
-        -------
-        self
-
-        """
-
-        # Reset the state if already fitted
-        for attr in ("instance_", "n_features_in_"):
-            self.__dict__.pop(attr, None)
-
-        # Fit with one pass of the dataset
-        return self._partial_fit(X, y)
-
-    def partial_fit(self, X, y):
-        """Fits incrementally on a portion of a dataset.
-
-        Parameters
-        ----------
-        X
-            array-like of shape (n_samples, n_features).
-        y
-            array-like of shape n_samples.
-
-        Returns
-        -------
-        self
-
-        """
-
-        # Fit with one pass of the dataset portion
-        return self._partial_fit(X, y)
-
-    def predict(self, X) -> np.ndarray:
-        """Predicts the target of an entire dataset contained in memory.
-
-        Parameters
-        ----------
-        X
-            array-like of shape (n_samples, n_features).
-
-        Returns
-        -------
-        Predicted target values for each row of `X`.
-
-        """
-
-        # Check the fit method has been called
-        utils.validation.check_is_fitted(self, attributes="instance_")
-
-        # Check the input
-        X = utils.check_array(X, **SKLEARN_INPUT_X_PARAMS)
-
-        if X.shape[1] != self.n_features_in_:
-            raise ValueError(
-                f"Expected {self.n_features_in_} features, got {X.shape[1]}"
-            )
-
-        # Make a prediction for each observation
-        y_pred = np.empty(shape=len(X))
-        for i, (x, _) in enumerate(stream.iter_array(X)):
-            y_pred[i] = self.instance_.predict_one(x)
-
-        return y_pred
-
-
-class River2SKLClassifier(River2SKLBase, sklearn_base.ClassifierMixin):
-    """Compatibility layer from River to scikit-learn for classification.
-
-    Parameters
-    ----------
-    river_estimator
-
-    """
-
-    def __init__(self, river_estimator: base.Classifier):
-
-        # Check the estimator is Classifier
-        if not isinstance(river_estimator, base.Classifier):
-            raise ValueError("estimator is not a Classifier")
-
-        self.river_estimator = river_estimator
-
-    def _more_tags(self):
-        return {"binary_only": not self.river_estimator._multiclass}
-
-    def _partial_fit(self, X, y, classes):
-
-        # If first _partial_fit call, set the classes, else check consistency
-        if not hasattr(self, "classes_"):
-            self.classes_ = classes
-
-        # Check the inputs
-        X, y = utils.check_X_y(X, y, **SKLEARN_INPUT_X_PARAMS, **SKLEARN_INPUT_Y_PARAMS)
-
-        # Check the number of classes agrees with the type of classifier
-        if len(self.classes_) > 2 and not self.river_estimator._multiclass:
-            # Only a warning for now so tests can pass, see scikit-learn issue
-            # https://github.com/scikit-learn/scikit-learn/issues/16798#issuecomment-651784267
-            # TODO: change to a ValueError when fixed
-            import warnings
-
-            warnings.warn(
-                f"more than 2 classes were given but {self.river_estimator} is a"
-                " binary classifier"
-            )
-
-        # Store the number of features so that future inputs can be checked
-        if hasattr(self, "n_features_in_") and X.shape[1] != self.n_features_in_:
-            raise ValueError(
-                f"Expected {self.n_features_in_} features, got {X.shape[1]}"
-            )
-        self.n_features_in_ = X.shape[1]
-
-        # Check the target
-        utils.multiclass.check_classification_targets(y)
-        if set(y) - set(self.classes_):
-            raise ValueError("classes should include all valid labels that can be in y")
-
-        # scikit-learn's convention is that fit shouldn't mutate the input parameters; we have to
-        # deep copy the provided estimator in order to respect this convention
-        if not hasattr(self, "instance_"):
-            self.instance_ = copy.deepcopy(self.river_estimator)
-
-        # river's binary classifiers expects bools or 0/1 values
-        if not self.river_estimator._multiclass:
-            if not hasattr(self, "label_encoder_"):
-                self.label_encoder_ = preprocessing.LabelEncoder().fit(self.classes_)
-            y = self.label_encoder_.transform(y)
-
-        # Call learn_one for each observation
-        for x, yi in STREAM_METHODS[type(X)](X, y):
-            self.instance_.learn_one(x, yi)
-
-        return self
-
-    def fit(self, X, y):
-        """Fits to an entire dataset contained in memory.
-
-        Parameters
-        ----------
-        X
-            array-like of shape (n_samples, n_features).
-        y
-            array-like of shape n_samples.
-
-        Returns
-        -------
-        self
-
-        """
-
-        # Reset the state if already fitted
-        for attr in ("classes_", "instance_", "label_encoder_", "n_features_in_"):
-            self.__dict__.pop(attr, None)
-
-        # Fit with one pass of the dataset
-        classes = utils.multiclass.unique_labels(y)
-        return self._partial_fit(X, y, classes)
-
-    def partial_fit(self, X, y, classes=None):
-        """Fits incrementally on a portion of a dataset.
-
-        Parameters
-        ----------
-        X
-            array-like of shape (n_samples, n_features).
-        y
-            array-like of shape n_samples.
-        classes
-            Classes across all calls to partial_fit. This argument is required for the first call
-            to partial_fit and can be omitted in the subsequent calls. Note that y doesn't need to
-            contain all labels in `classes`.
-
-        Returns
-        -------
-        self
-
-        """
-
-        # Fit with one pass of the dataset portion
-        return self._partial_fit(X, y, classes)
-
-    def predict_proba(self, X):
-        """Predicts the target probability of an entire dataset contained in memory.
-
-        Parameters
-        ----------
-        X
-            array-like of shape (n_samples, n_features).
-
-        Returns
-        -------
-        Predicted target values for each row of `X`.
-
-        """
-
-        # Check the fit method has been called
-        utils.validation.check_is_fitted(self, attributes="instance_")
-
-        # Check the input
-        X = utils.check_array(X, **SKLEARN_INPUT_X_PARAMS)
-
-        if X.shape[1] != self.n_features_in_:
-            raise ValueError(
-                f"Expected {self.n_features_in_} features, got {X.shape[1]}"
-            )
-
-        # river's predictions have to converted to follow the scikit-learn conventions
-        def reshape_probas(y_pred):
-            return [y_pred.get(c, 0) for c in self.classes_]
-
-        # Make a prediction for each observation
-        y_pred = np.empty(shape=(len(X), len(self.classes_)))
-        for i, (x, _) in enumerate(stream.iter_array(X)):
-            y_pred[i] = reshape_probas(self.instance_.predict_proba_one(x))
-
-        return y_pred
-
-    def predict(self, X):
-        """Predicts the target of an entire dataset contained in memory.
-
-        Parameters
-        ----------
-        X
-            array-like of shape (n_samples, n_features).
-
-        Returns
-        -------
-        Predicted target values for each row of `X`.
-
-        """
-
-        # Check the fit method has been called
-        utils.validation.check_is_fitted(self, attributes="instance_")
-
-        # Check the input
-        X = utils.check_array(X, **SKLEARN_INPUT_X_PARAMS)
-
-        if X.shape[1] != self.n_features_in_:
-            raise ValueError(
-                f"Expected {self.n_features_in_} features, got {X.shape[1]}"
-            )
-
-        # Make a prediction for each observation
-        y_pred = [None] * len(X)
-        for i, (x, _) in enumerate(stream.iter_array(X)):
-            y_pred[i] = self.instance_.predict_one(x)
-
-        # Convert back to the expected labels if an encoder was necessary for binary classification
-        y_pred = np.asarray(y_pred)
-        if hasattr(self, "label_encoder_"):
-            y_pred = self.label_encoder_.inverse_transform(y_pred.astype(int))
-
-        return y_pred
-
-
-class River2SKLTransformer(River2SKLBase, sklearn_base.TransformerMixin):
-    """Compatibility layer from River to scikit-learn for transformation.
-
-    Parameters
-    ----------
-    river_estimator
-
-    """
-
-    def __init__(self, river_estimator: base.Transformer):
-
-        # Check the estimator is a Transformer
-        if not isinstance(river_estimator, base.Transformer):
-            raise ValueError("estimator is not a Transformer")
-
-        self.river_estimator = river_estimator
-
-    def _partial_fit(self, X, y):
-
-        # Check the inputs
-        if y is None:
-            X = utils.check_array(X, **SKLEARN_INPUT_X_PARAMS)
-        else:
-            X, y = utils.check_X_y(
-                X, y, **SKLEARN_INPUT_X_PARAMS, **SKLEARN_INPUT_Y_PARAMS
-            )
-
-        # Store the number of features so that future inputs can be checked
-        if hasattr(self, "n_features_in_") and X.shape[1] != self.n_features_in_:
-            raise ValueError(
-                f"Expected {self.n_features_in_} features, got {X.shape[1]}"
-            )
-        self.n_features_in_ = X.shape[1]
-
-        # scikit-learn's convention is that fit shouldn't mutate the input parameters; we have to
-        # deep copy the provided estimator in order to respect this convention
-        if not hasattr(self, "instance_"):
-            self.instance_ = copy.deepcopy(self.river_estimator)
-
-        # Call learn_one for each observation
-        if isinstance(self.instance_, base.SupervisedTransformer):
-            for x, yi in STREAM_METHODS[type(X)](X, y):
-                self.instance_.learn_one(x, yi)
-        else:
-            for x, _ in STREAM_METHODS[type(X)](X):
-                self.instance_.learn_one(x)
-
-        return self
-
-    def fit(self, X, y=None):
-        """Fits to an entire dataset contained in memory.
-
-        Parameters
-        ----------
-        X
-            array-like of shape (n_samples, n_features).
-        y
-            array-like of shape n_samples.
-
-        Returns
-        -------
-        self
-
-        """
-
-        # Reset the state if already fitted
-        for attr in ("instance_", "n_features_in_"):
-            self.__dict__.pop(attr, None)
-
-        # Fit with one pass of the dataset
-        return self._partial_fit(X, y)
-
-    def partial_fit(self, X, y=None):
-        """Fits incrementally on a portion of a dataset.
-
-        Parameters
-        ----------
-        X
-            array-like of shape (n_samples, n_features).
-        y
-            array-like of shape n_samples.
-
-        Returns
-        -------
-        self
-
-        """
-
-        # Fit with one pass of the dataset
-        return self._partial_fit(X, y)
-
-    def transform(self, X):
-        """Predicts the target of an entire dataset contained in memory.
-
-        Parameters
-        ----------
-        X
-            array-like of shape (n_samples, n_features)
-
-        Returns
-        -------
-        Transformed output.
-
-        """
-
-        # Check the fit method has been called
-        utils.validation.check_is_fitted(self, attributes="instance_")
-
-        # Check the input
-        X = utils.check_array(X, **SKLEARN_INPUT_X_PARAMS)
-
-        if X.shape[1] != self.n_features_in_:
-            raise ValueError(
-                f"Expected {self.n_features_in_} features, got {X.shape[1]}"
-            )
-
-        # Call predict_proba_one for each observation
-        X_trans = [None] * len(X)
-        for i, (x, _) in enumerate(STREAM_METHODS[type(X)](X)):
-            X_trans[i] = list(self.instance_.transform_one(x).values())
-
-        return np.asarray(X_trans)
-
-
-class River2SKLClusterer(River2SKLBase, sklearn_base.ClusterMixin):
-    """Compatibility layer from River to scikit-learn for clustering.
-
-    Parameters
-    ----------
-    river_estimator
-
-    """
-
-    def __init__(self, river_estimator: base.Clusterer):
-
-        # Check the estimator is a Clusterer
-        if not isinstance(river_estimator, base.Clusterer):
-            raise ValueError("estimator is not a Clusterer")
-
-        self.river_estimator = river_estimator
-
-    def _partial_fit(self, X, y):
-
-        # Check the inputs
-        X = utils.check_array(X, **SKLEARN_INPUT_X_PARAMS)
-
-        # Store the number of features so that future inputs can be checked
-        if hasattr(self, "n_features_in_") and X.shape[1] != self.n_features_in_:
-            raise ValueError(
-                f"Expected {self.n_features_in_} features, got {X.shape[1]}"
-            )
-        self.n_features_in_ = X.shape[1]
-
-        # scikit-learn's convention is that fit shouldn't mutate the input parameters; we have to
-        # deep copy the provided estimator in order to respect this convention
-        if not hasattr(self, "instance_"):
-            self.instance_ = copy.deepcopy(self.river_estimator)
-
-        # Call learn_one for each observation
-        self.labels_ = np.empty(len(X), dtype=np.int32)
-        for i, (x, _) in enumerate(STREAM_METHODS[type(X)](X)):
-            label = self.instance_.learn_one(x).predict_one(x)
-            self.labels_[i] = label
-
-        return self
-
-    def fit(self, X, y=None):
-        """Fits to an entire dataset contained in memory.
-
-        Parameters
-        ----------
-        X
-            array-like of shape (n_samples, n_features).
-        y
-            array-like of shape n_samples.
-
-        Returns
-        -------
-        self
-
-        """
-
-        # Reset the state if already fitted
-        for attr in ("instance_", "n_features_in_"):
-            self.__dict__.pop(attr, None)
-
-        # Fit with one pass of the dataset
-        return self._partial_fit(X, y)
-
-    def partial_fit(self, X, y):
-        """Fits incrementally on a portion of a dataset.
-
-        Parameters
-        ----------
-        X
-            array-like of shape (n_samples, n_features).
-        y
-            array-like of shape n_samples.
-
-        Returns
-        -------
-        self
-
-        """
-
-        # Fit with one pass of the dataset
-        return self._partial_fit(X, y)
-
-    def predict(self, X):
-        """Predicts the target of an entire dataset contained in memory.
-
-        Parameters
-        ----------
-        X
-            array-like of shape (n_samples, n_features).
-
-        Returns
-        -------
-        Transformed output.
-
-        """
-
-        # Check the fit method has been called
-        utils.validation.check_is_fitted(self, attributes="instance_")
-
-        # Check the input
-        X = utils.check_array(X, **SKLEARN_INPUT_X_PARAMS)
-
-        # Call predict_proba_one for each observation
-        y_pred = np.empty(len(X), dtype=np.int32)
-        for i, (x, _) in enumerate(STREAM_METHODS[type(X)](X)):
-            y_pred[i] = self.instance_.predict_one(x)
-
-        return y_pred
+import copy
+import typing
+
+import numpy as np
+
+try:
+    import pandas as pd
+
+    PANDAS_INSTALLED = True
+except ImportError:
+    PANDAS_INSTALLED = False
+from sklearn import base as sklearn_base
+from sklearn import pipeline, preprocessing, utils
+
+from river import base, compose, stream
+
+__all__ = [
+    "convert_river_to_sklearn",
+    "River2SKLRegressor",
+    "River2SKLClassifier",
+    "River2SKLClusterer",
+    "River2SKLTransformer",
+]
+
+
+# Define a streaming method for each kind of batch input
+STREAM_METHODS: typing.Dict[typing.Type, typing.Callable] = {
+    np.ndarray: stream.iter_array
+}
+
+if PANDAS_INSTALLED:
+    STREAM_METHODS[pd.DataFrame] = stream.iter_pandas
+
+# Params passed to sklearn.utils.check_X_y and sklearn.utils.check_array
+SKLEARN_INPUT_X_PARAMS = {
+    "accept_sparse": False,
+    "accept_large_sparse": True,
+    "dtype": "numeric",
+    "order": None,
+    "copy": False,
+    "force_all_finite": True,
+    "ensure_2d": True,
+    "allow_nd": False,
+    "ensure_min_samples": 1,
+    "ensure_min_features": 1,
+}
+
+# Params passed to sklearn.utils.check_X_y in addition to SKLEARN_INPUT_X_PARAMS
+SKLEARN_INPUT_Y_PARAMS = {"multi_output": False, "y_numeric": False}
+
+
+def convert_river_to_sklearn(estimator: base.Estimator):
+    """Wraps a river estimator to make it compatible with scikit-learn.
+
+    Parameters
+    ----------
+    estimator
+
+    """
+
+    if isinstance(estimator, compose.Pipeline):
+        return pipeline.Pipeline(
+            [
+                (name, convert_river_to_sklearn(step))
+                for name, step in estimator.steps.items()
+            ]
+        )
+
+    wrappers = [
+        (base.Classifier, River2SKLClassifier),
+        (base.Clusterer, River2SKLClusterer),
+        (base.Regressor, River2SKLRegressor),
+        (base.Transformer, River2SKLTransformer),
+    ]
+
+    for base_type, wrapper in wrappers:
+        if isinstance(estimator, base_type):
+            obj = wrapper(estimator)
+            obj.instance_ = copy.deepcopy(estimator)
+            return obj
+
+    raise ValueError("Couldn't find an appropriate wrapper")
+
+
+class River2SKLBase(sklearn_base.BaseEstimator, base.Wrapper):
+    """This class is just here for house-keeping."""
+
+    @property
+    def _wrapped_model(self):
+        return self.river_estimator
+
+    _required_parameters = ["river_estimator"]
+
+
+class River2SKLRegressor(River2SKLBase, sklearn_base.RegressorMixin):
+    """Compatibility layer from River to scikit-learn for regression.
+
+    Parameters
+    ----------
+    river_estimator
+
+    """
+
+    def __init__(self, river_estimator: base.Regressor):
+
+        # Check the estimator is a Regressor
+        if not isinstance(river_estimator, base.Regressor):
+            raise ValueError("river_estimator is not a Regressor")
+
+        self.river_estimator = river_estimator
+
+    def _partial_fit(self, X, y):
+
+        # Check the inputs
+        X, y = utils.check_X_y(X, y, **SKLEARN_INPUT_X_PARAMS, **SKLEARN_INPUT_Y_PARAMS)
+
+        # Store the number of features so that future inputs can be checked
+        if hasattr(self, "n_features_in_") and X.shape[1] != self.n_features_in_:
+            raise ValueError(
+                f"Expected {self.n_features_in_} features, got {X.shape[1]}"
+            )
+        self.n_features_in_ = X.shape[1]
+
+        # scikit-learn's convention is that fit shouldn't mutate the input parameters; we have to
+        # deep copy the provided estimator in order to respect this convention
+        if not hasattr(self, "instance_"):
+            self.instance_ = copy.deepcopy(self.river_estimator)
+
+        # Call learn_one for each observation
+        for x, yi in STREAM_METHODS[type(X)](X, y):
+            self.instance_.learn_one(x, yi)
+
+        return self
+
+    def fit(self, X, y):
+        """Fits to an entire dataset contained in memory.
+
+        Parameters
+        ----------
+        X
+            array-like of shape (n_samples, n_features).
+        y
+            array-like of shape n_samples.
+
+        Returns
+        -------
+        self
+
+        """
+
+        # Reset the state if already fitted
+        for attr in ("instance_", "n_features_in_"):
+            self.__dict__.pop(attr, None)
+
+        # Fit with one pass of the dataset
+        return self._partial_fit(X, y)
+
+    def partial_fit(self, X, y):
+        """Fits incrementally on a portion of a dataset.
+
+        Parameters
+        ----------
+        X
+            array-like of shape (n_samples, n_features).
+        y
+            array-like of shape n_samples.
+
+        Returns
+        -------
+        self
+
+        """
+
+        # Fit with one pass of the dataset portion
+        return self._partial_fit(X, y)
+
+    def predict(self, X) -> np.ndarray:
+        """Predicts the target of an entire dataset contained in memory.
+
+        Parameters
+        ----------
+        X
+            array-like of shape (n_samples, n_features).
+
+        Returns
+        -------
+        Predicted target values for each row of `X`.
+
+        """
+
+        # Check the fit method has been called
+        utils.validation.check_is_fitted(self, attributes="instance_")
+
+        # Check the input
+        X = utils.check_array(X, **SKLEARN_INPUT_X_PARAMS)
+
+        if X.shape[1] != self.n_features_in_:
+            raise ValueError(
+                f"Expected {self.n_features_in_} features, got {X.shape[1]}"
+            )
+
+        # Make a prediction for each observation
+        y_pred = np.empty(shape=len(X))
+        for i, (x, _) in enumerate(stream.iter_array(X)):
+            y_pred[i] = self.instance_.predict_one(x)
+
+        return y_pred
+
+
+class River2SKLClassifier(River2SKLBase, sklearn_base.ClassifierMixin):
+    """Compatibility layer from River to scikit-learn for classification.
+
+    Parameters
+    ----------
+    river_estimator
+
+    """
+
+    def __init__(self, river_estimator: base.Classifier):
+
+        # Check the estimator is Classifier
+        if not isinstance(river_estimator, base.Classifier):
+            raise ValueError("estimator is not a Classifier")
+
+        self.river_estimator = river_estimator
+
+    def _more_tags(self):
+        return {"binary_only": not self.river_estimator._multiclass}
+
+    def _partial_fit(self, X, y, classes):
+
+        # If first _partial_fit call, set the classes, else check consistency
+        if not hasattr(self, "classes_"):
+            self.classes_ = classes
+
+        # Check the inputs
+        X, y = utils.check_X_y(X, y, **SKLEARN_INPUT_X_PARAMS, **SKLEARN_INPUT_Y_PARAMS)
+
+        # Check the number of classes agrees with the type of classifier
+        if len(self.classes_) > 2 and not self.river_estimator._multiclass:
+            # Only a warning for now so tests can pass, see scikit-learn issue
+            # https://github.com/scikit-learn/scikit-learn/issues/16798#issuecomment-651784267
+            # TODO: change to a ValueError when fixed
+            import warnings
+
+            warnings.warn(
+                f"more than 2 classes were given but {self.river_estimator} is a"
+                " binary classifier"
+            )
+
+        # Store the number of features so that future inputs can be checked
+        if hasattr(self, "n_features_in_") and X.shape[1] != self.n_features_in_:
+            raise ValueError(
+                f"Expected {self.n_features_in_} features, got {X.shape[1]}"
+            )
+        self.n_features_in_ = X.shape[1]
+
+        # Check the target
+        utils.multiclass.check_classification_targets(y)
+        if set(y) - set(self.classes_):
+            raise ValueError("classes should include all valid labels that can be in y")
+
+        # scikit-learn's convention is that fit shouldn't mutate the input parameters; we have to
+        # deep copy the provided estimator in order to respect this convention
+        if not hasattr(self, "instance_"):
+            self.instance_ = copy.deepcopy(self.river_estimator)
+
+        # river's binary classifiers expects bools or 0/1 values
+        if not self.river_estimator._multiclass:
+            if not hasattr(self, "label_encoder_"):
+                self.label_encoder_ = preprocessing.LabelEncoder().fit(self.classes_)
+            y = self.label_encoder_.transform(y)
+
+        # Call learn_one for each observation
+        for x, yi in STREAM_METHODS[type(X)](X, y):
+            self.instance_.learn_one(x, yi)
+
+        return self
+
+    def fit(self, X, y):
+        """Fits to an entire dataset contained in memory.
+
+        Parameters
+        ----------
+        X
+            array-like of shape (n_samples, n_features).
+        y
+            array-like of shape n_samples.
+
+        Returns
+        -------
+        self
+
+        """
+
+        # Reset the state if already fitted
+        for attr in ("classes_", "instance_", "label_encoder_", "n_features_in_"):
+            self.__dict__.pop(attr, None)
+
+        # Fit with one pass of the dataset
+        classes = utils.multiclass.unique_labels(y)
+        return self._partial_fit(X, y, classes)
+
+    def partial_fit(self, X, y, classes=None):
+        """Fits incrementally on a portion of a dataset.
+
+        Parameters
+        ----------
+        X
+            array-like of shape (n_samples, n_features).
+        y
+            array-like of shape n_samples.
+        classes
+            Classes across all calls to partial_fit. This argument is required for the first call
+            to partial_fit and can be omitted in the subsequent calls. Note that y doesn't need to
+            contain all labels in `classes`.
+
+        Returns
+        -------
+        self
+
+        """
+
+        # Fit with one pass of the dataset portion
+        return self._partial_fit(X, y, classes)
+
+    def predict_proba(self, X):
+        """Predicts the target probability of an entire dataset contained in memory.
+
+        Parameters
+        ----------
+        X
+            array-like of shape (n_samples, n_features).
+
+        Returns
+        -------
+        Predicted target values for each row of `X`.
+
+        """
+
+        # Check the fit method has been called
+        utils.validation.check_is_fitted(self, attributes="instance_")
+
+        # Check the input
+        X = utils.check_array(X, **SKLEARN_INPUT_X_PARAMS)
+
+        if X.shape[1] != self.n_features_in_:
+            raise ValueError(
+                f"Expected {self.n_features_in_} features, got {X.shape[1]}"
+            )
+
+        # river's predictions have to converted to follow the scikit-learn conventions
+        def reshape_probas(y_pred):
+            return [y_pred.get(c, 0) for c in self.classes_]
+
+        # Make a prediction for each observation
+        y_pred = np.empty(shape=(len(X), len(self.classes_)))
+        for i, (x, _) in enumerate(stream.iter_array(X)):
+            y_pred[i] = reshape_probas(self.instance_.predict_proba_one(x))
+
+        return y_pred
+
+    def predict(self, X):
+        """Predicts the target of an entire dataset contained in memory.
+
+        Parameters
+        ----------
+        X
+            array-like of shape (n_samples, n_features).
+
+        Returns
+        -------
+        Predicted target values for each row of `X`.
+
+        """
+
+        # Check the fit method has been called
+        utils.validation.check_is_fitted(self, attributes="instance_")
+
+        # Check the input
+        X = utils.check_array(X, **SKLEARN_INPUT_X_PARAMS)
+
+        if X.shape[1] != self.n_features_in_:
+            raise ValueError(
+                f"Expected {self.n_features_in_} features, got {X.shape[1]}"
+            )
+
+        # Make a prediction for each observation
+        y_pred = [None] * len(X)
+        for i, (x, _) in enumerate(stream.iter_array(X)):
+            y_pred[i] = self.instance_.predict_one(x)
+
+        # Convert back to the expected labels if an encoder was necessary for binary classification
+        y_pred = np.asarray(y_pred)
+        if hasattr(self, "label_encoder_"):
+            y_pred = self.label_encoder_.inverse_transform(y_pred.astype(int))
+
+        return y_pred
+
+
+class River2SKLTransformer(River2SKLBase, sklearn_base.TransformerMixin):
+    """Compatibility layer from River to scikit-learn for transformation.
+
+    Parameters
+    ----------
+    river_estimator
+
+    """
+
+    def __init__(self, river_estimator: base.Transformer):
+
+        # Check the estimator is a Transformer
+        if not isinstance(river_estimator, base.Transformer):
+            raise ValueError("estimator is not a Transformer")
+
+        self.river_estimator = river_estimator
+
+    def _partial_fit(self, X, y):
+
+        # Check the inputs
+        if y is None:
+            X = utils.check_array(X, **SKLEARN_INPUT_X_PARAMS)
+        else:
+            X, y = utils.check_X_y(
+                X, y, **SKLEARN_INPUT_X_PARAMS, **SKLEARN_INPUT_Y_PARAMS
+            )
+
+        # Store the number of features so that future inputs can be checked
+        if hasattr(self, "n_features_in_") and X.shape[1] != self.n_features_in_:
+            raise ValueError(
+                f"Expected {self.n_features_in_} features, got {X.shape[1]}"
+            )
+        self.n_features_in_ = X.shape[1]
+
+        # scikit-learn's convention is that fit shouldn't mutate the input parameters; we have to
+        # deep copy the provided estimator in order to respect this convention
+        if not hasattr(self, "instance_"):
+            self.instance_ = copy.deepcopy(self.river_estimator)
+
+        # Call learn_one for each observation
+        if isinstance(self.instance_, base.SupervisedTransformer):
+            for x, yi in STREAM_METHODS[type(X)](X, y):
+                self.instance_.learn_one(x, yi)
+        else:
+            for x, _ in STREAM_METHODS[type(X)](X):
+                self.instance_.learn_one(x)
+
+        return self
+
+    def fit(self, X, y=None):
+        """Fits to an entire dataset contained in memory.
+
+        Parameters
+        ----------
+        X
+            array-like of shape (n_samples, n_features).
+        y
+            array-like of shape n_samples.
+
+        Returns
+        -------
+        self
+
+        """
+
+        # Reset the state if already fitted
+        for attr in ("instance_", "n_features_in_"):
+            self.__dict__.pop(attr, None)
+
+        # Fit with one pass of the dataset
+        return self._partial_fit(X, y)
+
+    def partial_fit(self, X, y=None):
+        """Fits incrementally on a portion of a dataset.
+
+        Parameters
+        ----------
+        X
+            array-like of shape (n_samples, n_features).
+        y
+            array-like of shape n_samples.
+
+        Returns
+        -------
+        self
+
+        """
+
+        # Fit with one pass of the dataset
+        return self._partial_fit(X, y)
+
+    def transform(self, X):
+        """Predicts the target of an entire dataset contained in memory.
+
+        Parameters
+        ----------
+        X
+            array-like of shape (n_samples, n_features)
+
+        Returns
+        -------
+        Transformed output.
+
+        """
+
+        # Check the fit method has been called
+        utils.validation.check_is_fitted(self, attributes="instance_")
+
+        # Check the input
+        X = utils.check_array(X, **SKLEARN_INPUT_X_PARAMS)
+
+        if X.shape[1] != self.n_features_in_:
+            raise ValueError(
+                f"Expected {self.n_features_in_} features, got {X.shape[1]}"
+            )
+
+        # Call predict_proba_one for each observation
+        X_trans = [None] * len(X)
+        for i, (x, _) in enumerate(STREAM_METHODS[type(X)](X)):
+            X_trans[i] = list(self.instance_.transform_one(x).values())
+
+        return np.asarray(X_trans)
+
+
+class River2SKLClusterer(River2SKLBase, sklearn_base.ClusterMixin):
+    """Compatibility layer from River to scikit-learn for clustering.
+
+    Parameters
+    ----------
+    river_estimator
+
+    """
+
+    def __init__(self, river_estimator: base.Clusterer):
+
+        # Check the estimator is a Clusterer
+        if not isinstance(river_estimator, base.Clusterer):
+            raise ValueError("estimator is not a Clusterer")
+
+        self.river_estimator = river_estimator
+
+    def _partial_fit(self, X, y):
+
+        # Check the inputs
+        X = utils.check_array(X, **SKLEARN_INPUT_X_PARAMS)
+
+        # Store the number of features so that future inputs can be checked
+        if hasattr(self, "n_features_in_") and X.shape[1] != self.n_features_in_:
+            raise ValueError(
+                f"Expected {self.n_features_in_} features, got {X.shape[1]}"
+            )
+        self.n_features_in_ = X.shape[1]
+
+        # scikit-learn's convention is that fit shouldn't mutate the input parameters; we have to
+        # deep copy the provided estimator in order to respect this convention
+        if not hasattr(self, "instance_"):
+            self.instance_ = copy.deepcopy(self.river_estimator)
+
+        # Call learn_one for each observation
+        self.labels_ = np.empty(len(X), dtype=np.int32)
+        for i, (x, _) in enumerate(STREAM_METHODS[type(X)](X)):
+            label = self.instance_.learn_one(x).predict_one(x)
+            self.labels_[i] = label
+
+        return self
+
+    def fit(self, X, y=None):
+        """Fits to an entire dataset contained in memory.
+
+        Parameters
+        ----------
+        X
+            array-like of shape (n_samples, n_features).
+        y
+            array-like of shape n_samples.
+
+        Returns
+        -------
+        self
+
+        """
+
+        # Reset the state if already fitted
+        for attr in ("instance_", "n_features_in_"):
+            self.__dict__.pop(attr, None)
+
+        # Fit with one pass of the dataset
+        return self._partial_fit(X, y)
+
+    def partial_fit(self, X, y):
+        """Fits incrementally on a portion of a dataset.
+
+        Parameters
+        ----------
+        X
+            array-like of shape (n_samples, n_features).
+        y
+            array-like of shape n_samples.
+
+        Returns
+        -------
+        self
+
+        """
+
+        # Fit with one pass of the dataset
+        return self._partial_fit(X, y)
+
+    def predict(self, X):
+        """Predicts the target of an entire dataset contained in memory.
+
+        Parameters
+        ----------
+        X
+            array-like of shape (n_samples, n_features).
+
+        Returns
+        -------
+        Transformed output.
+
+        """
+
+        # Check the fit method has been called
+        utils.validation.check_is_fitted(self, attributes="instance_")
+
+        # Check the input
+        X = utils.check_array(X, **SKLEARN_INPUT_X_PARAMS)
+
+        # Call predict_proba_one for each observation
+        y_pred = np.empty(len(X), dtype=np.int32)
+        for i, (x, _) in enumerate(STREAM_METHODS[type(X)](X)):
+            y_pred[i] = self.instance_.predict_one(x)
+
+        return y_pred
```

### Comparing `river-0.8.0/river/compat/sklearn_to_river.py` & `river-0.9.0/river/compat/sklearn_to_river.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,183 +1,183 @@
-import copy
-import functools
-
-import pandas as pd
-from sklearn import base as sklearn_base
-from sklearn import exceptions
-
-from river import base
-
-__all__ = ["convert_sklearn_to_river", "SKL2RiverClassifier", "SKL2RiverRegressor"]
-
-
-def convert_sklearn_to_river(
-    estimator: sklearn_base.BaseEstimator, classes: list = None
-):
-    """Wraps a scikit-learn estimator to make it compatible with river.
-
-    Parameters
-    ----------
-    estimator
-    classes
-        Class names necessary for classifiers.
-
-    """
-
-    if not hasattr(estimator, "partial_fit"):
-        raise ValueError(f"{estimator} does not have a partial_fit method")
-
-    if isinstance(estimator, sklearn_base.ClassifierMixin) and classes is None:
-        raise ValueError("classes must be provided to convert a classifier")
-
-    wrappers = [
-        (sklearn_base.RegressorMixin, SKL2RiverRegressor),
-        (
-            sklearn_base.ClassifierMixin,
-            functools.partial(SKL2RiverClassifier, classes=classes),
-        ),
-    ]
-
-    for base_type, wrapper in wrappers:
-        if isinstance(estimator, base_type):
-            return wrapper(copy.deepcopy(estimator))
-
-    raise ValueError("Couldn't find an appropriate wrapper")
-
-
-class SKL2RiverBase:
-    def __init__(self, estimator: sklearn_base.BaseEstimator):
-        self.estimator = estimator
-
-
-class SKL2RiverRegressor(SKL2RiverBase, base.Regressor):
-    """Compatibility layer from scikit-learn to River for regression.
-
-    Parameters
-    ----------
-    estimator
-        A scikit-learn transformer which has a `partial_fit` method.
-
-    Examples
-    --------
-
-    >>> from river import compat
-    >>> from river import evaluate
-    >>> from river import metrics
-    >>> from river import preprocessing
-    >>> from river import stream
-    >>> from sklearn import linear_model
-    >>> from sklearn import datasets
-
-    >>> dataset = stream.iter_sklearn_dataset(
-    ...     dataset=datasets.load_boston(),
-    ...     shuffle=True,
-    ...     seed=42
-    ... )
-
-    >>> scaler = preprocessing.StandardScaler()
-    >>> sgd_reg = compat.convert_sklearn_to_river(linear_model.SGDRegressor())
-    >>> model = scaler | sgd_reg
-
-    >>> metric = metrics.MAE()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    MAE: 11.004415
-
-    """
-
-    def learn_one(self, x, y):
-        self.estimator.partial_fit(X=[list(x.values())], y=[y])
-        return self
-
-    def learn_many(self, X, y):
-        self.estimator.partial_fit(X=X.values, y=y.values)
-        return self
-
-    def predict_one(self, x):
-        try:
-            return self.estimator.predict(X=[list(x.values())])[0]
-        except exceptions.NotFittedError:
-            return 0
-
-    def predict_many(self, X):
-        return pd.Series(self.estimator.predict(X))
-
-
-class SKL2RiverClassifier(SKL2RiverBase, base.Classifier):
-    """Compatibility layer from scikit-learn to River for classification.
-
-    Parameters
-    ----------
-    estimator
-        A scikit-learn regressor which has a `partial_fit` method.
-    classes
-
-    Examples
-    --------
-
-    >>> from river import compat
-    >>> from river import evaluate
-    >>> from river import metrics
-    >>> from river import preprocessing
-    >>> from river import stream
-    >>> from sklearn import linear_model
-    >>> from sklearn import datasets
-
-    >>> dataset = stream.iter_sklearn_dataset(
-    ...     dataset=datasets.load_breast_cancer(),
-    ...     shuffle=True,
-    ...     seed=42
-    ... )
-
-    >>> model = preprocessing.StandardScaler()
-    >>> model |= compat.convert_sklearn_to_river(
-    ...     estimator=linear_model.SGDClassifier(
-    ...         loss='log',
-    ...         eta0=0.01,
-    ...         learning_rate='constant'
-    ...     ),
-    ...     classes=[False, True]
-    ... )
-
-    >>> metric = metrics.LogLoss()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    LogLoss: 0.199554
-
-    """
-
-    def __init__(self, estimator: sklearn_base.ClassifierMixin, classes: list):
-        super().__init__(estimator)
-        self.classes = classes
-
-    @property
-    def _multiclass(self):
-        return True
-
-    def learn_one(self, x, y):
-        self.estimator.partial_fit(X=[list(x.values())], y=[y], classes=self.classes)
-        return self
-
-    def learn_many(self, X, y):
-        self.estimator.partial_fit(X=X.values, y=y.values, classes=self.classes)
-        return self
-
-    def predict_proba_one(self, x):
-        try:
-            y_pred = self.estimator.predict_proba([list(x.values())])[0]
-            return {self.classes[i]: p for i, p in enumerate(y_pred)}
-        except exceptions.NotFittedError:
-            return {c: 1 / len(self.classes) for c in self.classes}
-
-    def predict_proba_many(self, X):
-        return pd.Series(self.estimator.predict_proba(X), columns=self.classes)
-
-    def predict_one(self, x):
-        try:
-            y_pred = self.estimator.predict(X=[list(x.values())])[0]
-            return y_pred
-        except exceptions.NotFittedError:
-            return self.classes[0]
-
-    def predict_many(self, X):
-        return pd.Series(self.estimator.predict(X))
+import copy
+import functools
+
+import pandas as pd
+from sklearn import base as sklearn_base
+from sklearn import exceptions
+
+from river import base
+
+__all__ = ["convert_sklearn_to_river", "SKL2RiverClassifier", "SKL2RiverRegressor"]
+
+
+def convert_sklearn_to_river(
+    estimator: sklearn_base.BaseEstimator, classes: list = None
+):
+    """Wraps a scikit-learn estimator to make it compatible with river.
+
+    Parameters
+    ----------
+    estimator
+    classes
+        Class names necessary for classifiers.
+
+    """
+
+    if not hasattr(estimator, "partial_fit"):
+        raise ValueError(f"{estimator} does not have a partial_fit method")
+
+    if isinstance(estimator, sklearn_base.ClassifierMixin) and classes is None:
+        raise ValueError("classes must be provided to convert a classifier")
+
+    wrappers = [
+        (sklearn_base.RegressorMixin, SKL2RiverRegressor),
+        (
+            sklearn_base.ClassifierMixin,
+            functools.partial(SKL2RiverClassifier, classes=classes),
+        ),
+    ]
+
+    for base_type, wrapper in wrappers:
+        if isinstance(estimator, base_type):
+            return wrapper(copy.deepcopy(estimator))
+
+    raise ValueError("Couldn't find an appropriate wrapper")
+
+
+class SKL2RiverBase:
+    def __init__(self, estimator: sklearn_base.BaseEstimator):
+        self.estimator = estimator
+
+
+class SKL2RiverRegressor(SKL2RiverBase, base.Regressor):
+    """Compatibility layer from scikit-learn to River for regression.
+
+    Parameters
+    ----------
+    estimator
+        A scikit-learn transformer which has a `partial_fit` method.
+
+    Examples
+    --------
+
+    >>> from river import compat
+    >>> from river import evaluate
+    >>> from river import metrics
+    >>> from river import preprocessing
+    >>> from river import stream
+    >>> from sklearn import linear_model
+    >>> from sklearn import datasets
+
+    >>> dataset = stream.iter_sklearn_dataset(
+    ...     dataset=datasets.load_diabetes(),
+    ...     shuffle=True,
+    ...     seed=42
+    ... )
+
+    >>> scaler = preprocessing.StandardScaler()
+    >>> sgd_reg = compat.convert_sklearn_to_river(linear_model.SGDRegressor())
+    >>> model = scaler | sgd_reg
+
+    >>> metric = metrics.MAE()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    MAE: 84.51947
+
+    """
+
+    def learn_one(self, x, y):
+        self.estimator.partial_fit(X=[list(x.values())], y=[y])
+        return self
+
+    def learn_many(self, X, y):
+        self.estimator.partial_fit(X=X.values, y=y.values)
+        return self
+
+    def predict_one(self, x):
+        try:
+            return self.estimator.predict(X=[list(x.values())])[0]
+        except exceptions.NotFittedError:
+            return 0
+
+    def predict_many(self, X):
+        return pd.Series(self.estimator.predict(X))
+
+
+class SKL2RiverClassifier(SKL2RiverBase, base.Classifier):
+    """Compatibility layer from scikit-learn to River for classification.
+
+    Parameters
+    ----------
+    estimator
+        A scikit-learn regressor which has a `partial_fit` method.
+    classes
+
+    Examples
+    --------
+
+    >>> from river import compat
+    >>> from river import evaluate
+    >>> from river import metrics
+    >>> from river import preprocessing
+    >>> from river import stream
+    >>> from sklearn import linear_model
+    >>> from sklearn import datasets
+
+    >>> dataset = stream.iter_sklearn_dataset(
+    ...     dataset=datasets.load_breast_cancer(),
+    ...     shuffle=True,
+    ...     seed=42
+    ... )
+
+    >>> model = preprocessing.StandardScaler()
+    >>> model |= compat.convert_sklearn_to_river(
+    ...     estimator=linear_model.SGDClassifier(
+    ...         loss='log',
+    ...         eta0=0.01,
+    ...         learning_rate='constant'
+    ...     ),
+    ...     classes=[False, True]
+    ... )
+
+    >>> metric = metrics.LogLoss()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    LogLoss: 0.199554
+
+    """
+
+    def __init__(self, estimator: sklearn_base.ClassifierMixin, classes: list):
+        super().__init__(estimator)
+        self.classes = classes
+
+    @property
+    def _multiclass(self):
+        return True
+
+    def learn_one(self, x, y):
+        self.estimator.partial_fit(X=[list(x.values())], y=[y], classes=self.classes)
+        return self
+
+    def learn_many(self, X, y):
+        self.estimator.partial_fit(X=X.values, y=y.values, classes=self.classes)
+        return self
+
+    def predict_proba_one(self, x):
+        try:
+            y_pred = self.estimator.predict_proba([list(x.values())])[0]
+            return {self.classes[i]: p for i, p in enumerate(y_pred)}
+        except exceptions.NotFittedError:
+            return {c: 1 / len(self.classes) for c in self.classes}
+
+    def predict_proba_many(self, X):
+        return pd.Series(self.estimator.predict_proba(X), columns=self.classes)
+
+    def predict_one(self, x):
+        try:
+            y_pred = self.estimator.predict(X=[list(x.values())])[0]
+            return y_pred
+        except exceptions.NotFittedError:
+            return self.classes[0]
+
+    def predict_many(self, X):
+        return pd.Series(self.estimator.predict(X))
```

### Comparing `river-0.8.0/river/compat/test_sklearn.py` & `river-0.9.0/river/compat/test_sklearn.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,31 +1,31 @@
-import pytest
-from sklearn import linear_model as sk_linear_model
-from sklearn.utils import estimator_checks
-
-from river import base, cluster, compat, linear_model, preprocessing
-
-
-@pytest.mark.parametrize(
-    "estimator",
-    [
-        pytest.param(estimator, id=str(estimator))
-        for estimator in [
-            linear_model.LinearRegression(),
-            linear_model.LogisticRegression(),
-            preprocessing.StandardScaler(),
-            cluster.KMeans(seed=42),
-        ]
-    ],
-)
-@pytest.mark.filterwarnings("ignore::sklearn.utils.estimator_checks.SkipTestWarning")
-def test_river_to_sklearn_check_estimator(estimator: base.Estimator):
-    skl_estimator = compat.convert_river_to_sklearn(estimator)
-    estimator_checks.check_estimator(skl_estimator)
-
-
-@pytest.mark.filterwarnings("ignore::sklearn.utils.estimator_checks.SkipTestWarning")
-def test_sklearn_check_twoway():
-    estimator = sk_linear_model.SGDRegressor()
-    river_estimator = compat.convert_sklearn_to_river(estimator)
-    skl_estimator = compat.convert_river_to_sklearn(river_estimator)
-    estimator_checks.check_estimator(skl_estimator)
+import pytest
+from sklearn import linear_model as sk_linear_model
+from sklearn.utils import estimator_checks
+
+from river import base, cluster, compat, linear_model, preprocessing
+
+
+@pytest.mark.parametrize(
+    "estimator",
+    [
+        pytest.param(estimator, id=str(estimator))
+        for estimator in [
+            linear_model.LinearRegression(),
+            linear_model.LogisticRegression(),
+            preprocessing.StandardScaler(),
+            cluster.KMeans(seed=42),
+        ]
+    ],
+)
+@pytest.mark.filterwarnings("ignore::sklearn.utils.estimator_checks.SkipTestWarning")
+def test_river_to_sklearn_check_estimator(estimator: base.Estimator):
+    skl_estimator = compat.convert_river_to_sklearn(estimator)
+    estimator_checks.check_estimator(skl_estimator)
+
+
+@pytest.mark.filterwarnings("ignore::sklearn.utils.estimator_checks.SkipTestWarning")
+def test_sklearn_check_twoway():
+    estimator = sk_linear_model.SGDRegressor()
+    river_estimator = compat.convert_sklearn_to_river(estimator)
+    skl_estimator = compat.convert_river_to_sklearn(river_estimator)
+    estimator_checks.check_estimator(skl_estimator)
```

### Comparing `river-0.8.0/river/compose/__init__.py` & `river-0.9.0/river/compose/__init__.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,23 +1,27 @@
-"""Model composition.
-
-This module contains utilities for merging multiple modeling steps into a single pipeline. Although
-pipelines are not the only way to process a stream of data, we highly encourage you to use them.
-
-"""
-from .func import FuncTransformer
-from .grouper import Grouper
-from .pipeline import Pipeline
-from .rename import Renamer
-from .select import Discard, Select, SelectType
-from .union import TransformerUnion
-
-__all__ = [
-    "Discard",
-    "FuncTransformer",
-    "Grouper",
-    "Pipeline",
-    "Renamer",
-    "TransformerUnion",
-    "Select",
-    "SelectType",
-]
+"""Model composition.
+
+This module contains utilities for merging multiple modeling steps into a single pipeline. Although
+pipelines are not the only way to process a stream of data, we highly encourage you to use them.
+
+"""
+from .func import FuncTransformer
+from .grouper import Grouper
+from .pipeline import Pipeline
+from .product import TransformerProduct
+from .rename import Renamer
+from .select import Discard, Select, SelectType
+from .target_transform import TargetTransformRegressor
+from .union import TransformerUnion
+
+__all__ = [
+    "Discard",
+    "FuncTransformer",
+    "Grouper",
+    "Pipeline",
+    "Renamer",
+    "Select",
+    "SelectType",
+    "TargetTransformRegressor",
+    "TransformerProduct",
+    "TransformerUnion",
+]
```

### Comparing `river-0.8.0/river/compose/func.py` & `river-0.9.0/river/compose/func.py`

 * *Ordering differences only*

 * *Files 9% similar despite different names*

```diff
@@ -1,100 +1,100 @@
-import typing
-
-from river import base
-
-__all__ = ["FuncTransformer"]
-
-
-class FuncTransformer(base.Transformer):
-    """Wraps a function to make it usable in a pipeline.
-
-    There is often a need to apply an arbitrary transformation to a set of features. For instance,
-    this could involve parsing a date and then extracting the hour from said date. If you're
-    processing a stream of data, then you can do this yourself by calling the necessary code at
-    your leisure. On the other hand, if you want to do this as part of a pipeline, then you need to
-    follow a simple convention.
-
-    To use a function as part of a pipeline, take as input a `dict` of features and output a `dict`.
-    Once you have initialized this class with your function, then you can use it like you would use
-    any other (unsupervised) transformer.
-
-    It is up to you if you want your function to be pure or not. By pure we refer to a function
-    that doesn't modify its input. However, we recommend writing pure functions because this
-    reduces the chances of inserting bugs into your pipeline.
-
-    Parameters
-    ----------
-    func
-        A function that takes as input a `dict` and outputs a `dict`.
-
-    Examples
-    --------
-
-    >>> from pprint import pprint
-    >>> import datetime as dt
-    >>> from river import compose
-
-    >>> x = {'date': '2019-02-14'}
-
-    >>> def parse_date(x):
-    ...     date = dt.datetime.strptime(x['date'], '%Y-%m-%d')
-    ...     x['is_weekend'] = date.day in (5, 6)
-    ...     x['hour'] = date.hour
-    ...     return x
-
-    >>> t = compose.FuncTransformer(parse_date)
-    >>> pprint(t.transform_one(x))
-    {'date': '2019-02-14', 'hour': 0, 'is_weekend': False}
-
-    The above example is not pure because it modifies the input. The following example is pure
-    and produces the same output:
-
-    >>> def parse_date(x):
-    ...     date = dt.datetime.strptime(x['date'], '%Y-%m-%d')
-    ...     return {'is_weekend': date.day in (5, 6), 'hour': date.hour}
-
-    >>> t = compose.FuncTransformer(parse_date)
-    >>> pprint(t.transform_one(x))
-    {'hour': 0, 'is_weekend': False}
-
-    The previous example doesn't include the `date` feature because it returns a new `dict`.
-    However, a common usecase is to add a feature to an existing set of features. You can do
-    this in a pure way by unpacking the input `dict` into the output `dict`:
-
-    >>> def parse_date(x):
-    ...     date = dt.datetime.strptime(x['date'], '%Y-%m-%d')
-    ...     return {'is_weekend': date.day in (5, 6), 'hour': date.hour, **x}
-
-    >>> t = compose.FuncTransformer(parse_date)
-    >>> pprint(t.transform_one(x))
-    {'date': '2019-02-14', 'hour': 0, 'is_weekend': False}
-
-    You can add `FuncTransformer` to a pipeline just like you would with any other transformer.
-
-    >>> from river import naive_bayes
-
-    >>> pipeline = compose.FuncTransformer(parse_date) | naive_bayes.MultinomialNB()
-    >>> pipeline
-    Pipeline (
-        FuncTransformer (
-        func="parse_date"
-        ),
-        MultinomialNB (
-        alpha=1.
-        )
-    )
-
-    If you provide a function with wrapping it, then the pipeline will do it for you:
-
-    >>> pipeline = parse_date | naive_bayes.MultinomialNB()
-
-    """
-
-    def __init__(self, func: typing.Callable[[dict], dict]):
-        self.func = func
-
-    def transform_one(self, x):
-        return self.func(x)
-
-    def __str__(self):
-        return self.func.__name__
+import typing
+
+from river import base
+
+__all__ = ["FuncTransformer"]
+
+
+class FuncTransformer(base.Transformer):
+    """Wraps a function to make it usable in a pipeline.
+
+    There is often a need to apply an arbitrary transformation to a set of features. For instance,
+    this could involve parsing a date and then extracting the hour from said date. If you're
+    processing a stream of data, then you can do this yourself by calling the necessary code at
+    your leisure. On the other hand, if you want to do this as part of a pipeline, then you need to
+    follow a simple convention.
+
+    To use a function as part of a pipeline, take as input a `dict` of features and output a `dict`.
+    Once you have initialized this class with your function, then you can use it like you would use
+    any other (unsupervised) transformer.
+
+    It is up to you if you want your function to be pure or not. By pure we refer to a function
+    that doesn't modify its input. However, we recommend writing pure functions because this
+    reduces the chances of inserting bugs into your pipeline.
+
+    Parameters
+    ----------
+    func
+        A function that takes as input a `dict` and outputs a `dict`.
+
+    Examples
+    --------
+
+    >>> from pprint import pprint
+    >>> import datetime as dt
+    >>> from river import compose
+
+    >>> x = {'date': '2019-02-14'}
+
+    >>> def parse_date(x):
+    ...     date = dt.datetime.strptime(x['date'], '%Y-%m-%d')
+    ...     x['is_weekend'] = date.day in (5, 6)
+    ...     x['hour'] = date.hour
+    ...     return x
+
+    >>> t = compose.FuncTransformer(parse_date)
+    >>> pprint(t.transform_one(x))
+    {'date': '2019-02-14', 'hour': 0, 'is_weekend': False}
+
+    The above example is not pure because it modifies the input. The following example is pure
+    and produces the same output:
+
+    >>> def parse_date(x):
+    ...     date = dt.datetime.strptime(x['date'], '%Y-%m-%d')
+    ...     return {'is_weekend': date.day in (5, 6), 'hour': date.hour}
+
+    >>> t = compose.FuncTransformer(parse_date)
+    >>> pprint(t.transform_one(x))
+    {'hour': 0, 'is_weekend': False}
+
+    The previous example doesn't include the `date` feature because it returns a new `dict`.
+    However, a common usecase is to add a feature to an existing set of features. You can do
+    this in a pure way by unpacking the input `dict` into the output `dict`:
+
+    >>> def parse_date(x):
+    ...     date = dt.datetime.strptime(x['date'], '%Y-%m-%d')
+    ...     return {'is_weekend': date.day in (5, 6), 'hour': date.hour, **x}
+
+    >>> t = compose.FuncTransformer(parse_date)
+    >>> pprint(t.transform_one(x))
+    {'date': '2019-02-14', 'hour': 0, 'is_weekend': False}
+
+    You can add `FuncTransformer` to a pipeline just like you would with any other transformer.
+
+    >>> from river import naive_bayes
+
+    >>> pipeline = compose.FuncTransformer(parse_date) | naive_bayes.MultinomialNB()
+    >>> pipeline
+    Pipeline (
+        FuncTransformer (
+        func="parse_date"
+        ),
+        MultinomialNB (
+        alpha=1.
+        )
+    )
+
+    If you provide a function with wrapping it, then the pipeline will do it for you:
+
+    >>> pipeline = parse_date | naive_bayes.MultinomialNB()
+
+    """
+
+    def __init__(self, func: typing.Callable[[dict], dict]):
+        self.func = func
+
+    def transform_one(self, x):
+        return self.func(x)
+
+    def __str__(self):
+        return self.func.__name__
```

### Comparing `river-0.8.0/river/compose/grouper.py` & `river-0.9.0/river/compose/grouper.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,50 +1,50 @@
-import collections
-import copy
-import functools
-import typing
-
-from river import base
-
-__all__ = ["Grouper"]
-
-
-class Grouper(base.Transformer):
-    """Applies a transformer within different groups.
-
-    This transformer allows you to split your data into groups and apply a transformer within each
-    group. This happens in a streaming manner, which means that the groups are discovered online.
-    A separate copy of the provided transformer is made whenever a new group appears. The groups
-    are defined according to one or more keys.
-
-    Parameters
-    ----------
-    transformer
-    by
-        The field on which to group the data. This can either by a single value, or a list of
-        values.
-
-    """
-
-    def __init__(
-        self,
-        transformer: base.Transformer,
-        by: typing.Union[base.typing.FeatureName, typing.List[base.typing.FeatureName]],
-    ):
-
-        self.transformer = transformer
-        self.by = by if isinstance(by, list) else [by]
-        self.transformers = collections.defaultdict(
-            functools.partial(copy.deepcopy, transformer)
-        )
-
-    def _get_key(self, x):
-        return "_".join(str(x[k]) for k in self.by)
-
-    def learn_one(self, x):
-        key = self._get_key(x)
-        self.transformers[key].learn_one(x)
-        return self
-
-    def transform_one(self, x):
-        key = self._get_key(x)
-        return self.transformers[key].transform_one(x)
+import collections
+import copy
+import functools
+import typing
+
+from river import base
+
+__all__ = ["Grouper"]
+
+
+class Grouper(base.Transformer):
+    """Applies a transformer within different groups.
+
+    This transformer allows you to split your data into groups and apply a transformer within each
+    group. This happens in a streaming manner, which means that the groups are discovered online.
+    A separate copy of the provided transformer is made whenever a new group appears. The groups
+    are defined according to one or more keys.
+
+    Parameters
+    ----------
+    transformer
+    by
+        The field on which to group the data. This can either by a single value, or a list of
+        values.
+
+    """
+
+    def __init__(
+        self,
+        transformer: base.Transformer,
+        by: typing.Union[base.typing.FeatureName, typing.List[base.typing.FeatureName]],
+    ):
+
+        self.transformer = transformer
+        self.by = by if isinstance(by, list) else [by]
+        self.transformers = collections.defaultdict(
+            functools.partial(copy.deepcopy, transformer)
+        )
+
+    def _get_key(self, x):
+        return "_".join(str(x[k]) for k in self.by)
+
+    def learn_one(self, x):
+        key = self._get_key(x)
+        self.transformers[key].learn_one(x)
+        return self
+
+    def transform_one(self, x):
+        key = self._get_key(x)
+        return self.transformers[key].transform_one(x)
```

### Comparing `river-0.8.0/river/datasets/airline-passengers.csv` & `river-0.9.0/river/datasets/airline-passengers.csv`

 * *Files identical despite different names*

### Comparing `river-0.8.0/river/datasets/banana.zip` & `river-0.9.0/river/datasets/banana.zip`

 * *Files identical despite different names*

### Comparing `river-0.8.0/river/datasets/bananas.py` & `river-0.9.0/river/datasets/bananas.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,24 +1,24 @@
-from river import stream
-
-from . import base
-
-
-class Bananas(base.FileDataset):
-    """Bananas dataset.
-
-    An artificial dataset where instances belongs to several clusters with a banana shape.
-    There are two attributes that correspond to the x and y axis, respectively.
-
-    References
-    ----------
-    [^1]: [OpenML page](https://www.openml.org/d/1460)
-
-    """
-
-    def __init__(self):
-        super().__init__(
-            filename="banana.zip", n_samples=5300, n_features=2, task=base.BINARY_CLF
-        )
-
-    def __iter__(self):
-        return stream.iter_libsvm(self.path, target_type=lambda x: x == "1")
+from river import stream
+
+from . import base
+
+
+class Bananas(base.FileDataset):
+    """Bananas dataset.
+
+    An artificial dataset where instances belongs to several clusters with a banana shape.
+    There are two attributes that correspond to the x and y axis, respectively.
+
+    References
+    ----------
+    [^1]: [OpenML page](https://www.openml.org/d/1460)
+
+    """
+
+    def __init__(self):
+        super().__init__(
+            filename="banana.zip", n_samples=5300, n_features=2, task=base.BINARY_CLF
+        )
+
+    def __iter__(self):
+        return stream.iter_libsvm(self.path, target_type=lambda x: x == "1")
```

### Comparing `river-0.8.0/river/datasets/base.py` & `river-0.9.0/river/datasets/base.py`

 * *Ordering differences only*

 * *Files 27% similar despite different names*

```diff
@@ -1,270 +1,270 @@
-import abc
-import inspect
-import itertools
-import os
-import pathlib
-import re
-import shutil
-import tarfile
-import typing
-import zipfile
-from urllib import request
-
-from river import utils
-
-REG = "Regression"
-BINARY_CLF = "Binary classification"
-MULTI_CLF = "Multi-class classification"
-MO_BINARY_CLF = "Multi-output binary classification"
-MO_REG = "Multi-output regression"
-
-
-def get_data_home():
-    """Return the location where remote datasets are to be stored."""
-
-    data_home = os.environ.get("RIVER_DATA", os.path.join("~", "river_data"))
-    data_home = os.path.expanduser(data_home)
-    if not os.path.exists(data_home):
-        os.makedirs(data_home)
-    return data_home
-
-
-class Dataset(abc.ABC):
-    """Base class for all datasets.
-
-    All datasets inherit from this class, be they stored in a file or generated on the fly.
-
-    """
-
-    def __init__(
-        self,
-        task,
-        n_features,
-        n_samples=None,
-        n_classes=None,
-        n_outputs=None,
-        sparse=False,
-    ):
-        self.task = task
-        self.n_features = n_features
-        self.n_samples = n_samples
-        self.n_outputs = n_outputs
-        self.n_classes = n_classes
-        self.sparse = sparse
-
-    @abc.abstractmethod
-    def __iter__(self):
-        raise NotImplementedError
-
-    def take(self, k: int):
-        """Iterate over the k samples."""
-        return itertools.islice(self, k)
-
-    @property
-    def desc(self):
-        """Return the description from the docstring."""
-        desc = re.split(pattern=r"\w+\n\s{4}\-{3,}", string=self.__doc__, maxsplit=0)[0]
-        return inspect.cleandoc(desc)
-
-    @property
-    def _repr_content(self):
-        """The items that are displayed in the __repr__ method.
-
-        This property can be overridden in order to modify the output of the __repr__ method.
-
-        """
-
-        content = {}
-        content["Name"] = self.__class__.__name__
-        content["Task"] = self.task
-        if isinstance(self, SyntheticDataset) and self.n_samples is None:
-            content["Samples"] = "∞"
-        elif self.n_samples:
-            content["Samples"] = f"{self.n_samples:,}"
-        if self.n_features:
-            content["Features"] = f"{self.n_features:,}"
-        if self.n_outputs:
-            content["Outputs"] = f"{self.n_outputs:,}"
-        if self.n_classes:
-            content["Classes"] = f"{self.n_classes:,}"
-        content["Sparse"] = str(self.sparse)
-
-        return content
-
-    def __repr__(self):
-
-        l_len = max(map(len, self._repr_content.keys()))
-        r_len = max(map(len, self._repr_content.values()))
-
-        out = f"{self.desc}\n\n" + "\n".join(
-            k.rjust(l_len) + "  " + v.ljust(r_len)
-            for k, v in self._repr_content.items()
-        )
-
-        if "Parameters\n    ----------" in self.__doc__:
-            params = re.split(
-                r"\w+\n\s{4}\-{3,}",
-                re.split("Parameters\n    ----------", self.__doc__)[1],
-            )[0].rstrip()
-            out += f"\n\nParameters\n----------{params}"
-
-        return out
-
-
-class SyntheticDataset(Dataset):
-    """A synthetic dataset."""
-
-    def __repr__(self):
-        l_len_prop = max(map(len, self._repr_content.keys()))
-        r_len_prop = max(map(len, self._repr_content.values()))
-        params = self._get_params()
-        l_len_config = max(map(len, params.keys()))
-        r_len_config = max(map(len, map(str, params.values())))
-
-        out = (
-            "Synthetic data generator\n\n"
-            + "\n".join(
-                k.rjust(l_len_prop) + "  " + v.ljust(r_len_prop)
-                for k, v in self._repr_content.items()
-            )
-            + "\n\nConfiguration\n-------------\n"
-            + "\n".join(
-                k.rjust(l_len_config) + "  " + str(v).ljust(r_len_config)
-                for k, v in params.items()
-            )
-        )
-
-        return out
-
-    def _get_params(self) -> typing.Dict[str, typing.Any]:
-        """Return the parameters that were used during initialization."""
-        return {
-            name: getattr(self, name)
-            for name, param in inspect.signature(self.__init__).parameters.items()  # type: ignore
-            if param.kind != param.VAR_KEYWORD
-        }
-
-
-class FileDataset(Dataset):
-    """Base class for datasets that are stored in a local file.
-
-    Small datasets that are part of the river package inherit from this class.
-
-    """
-
-    def __init__(self, filename, **desc):
-        super().__init__(**desc)
-        self.filename = filename
-
-    @property
-    def path(self):
-        return pathlib.Path(__file__).parent.joinpath(self.filename)
-
-    @property
-    def _repr_content(self):
-        content = super()._repr_content
-        content["Path"] = str(self.path)
-        return content
-
-
-class RemoteDataset(FileDataset):
-    """Base class for datasets that are stored in a remote file.
-
-    Medium and large datasets that are not part of the river package inherit from this class.
-
-    The filename doesn't have to be provided if unpack is False. Indeed in the latter case the
-    filename will be inferred from the URL.
-
-    """
-
-    def __init__(self, url, size, filename=None, unpack=True, **desc):
-
-        if filename is None:
-            filename = os.path.basename(url)
-
-        super().__init__(filename=filename, **desc)
-        self.url = url
-        self.size = size
-        self.unpack = unpack
-
-    @property
-    def path(self):
-        return pathlib.Path(get_data_home(), self.__class__.__name__, self.filename)
-
-    def download(self, force=False, verbose=True):
-
-        if not force and self.is_downloaded:
-            return
-
-        # Determine where to download the archive
-        directory = self.path.parent
-        directory.mkdir(parents=True, exist_ok=True)
-        archive_path = directory.joinpath(os.path.basename(self.url))
-
-        with request.urlopen(self.url) as r:
-
-            # Notify the user
-            if verbose:
-                meta = r.info()
-                try:
-                    n_bytes = int(meta["Content-Length"])
-                    msg = f"Downloading {self.url} ({utils.pretty.humanize_bytes(n_bytes)})"
-                except KeyError:
-                    msg = f"Downloading {self.url}"
-                print(msg)
-
-            # Now dump the contents of the requests
-            with open(archive_path, "wb") as f:
-                shutil.copyfileobj(r, f)
-
-        if not self.unpack:
-            return
-
-        if verbose:
-            print(f"Uncompressing into {directory}")
-
-        if archive_path.suffix.endswith("zip"):
-            with zipfile.ZipFile(archive_path, "r") as zf:
-                zf.extractall(directory)
-
-        elif archive_path.suffix.endswith(("gz", "tar")):
-            mode = "r:" if archive_path.suffix.endswith("tar") else "r:gz"
-            tar = tarfile.open(archive_path, mode)
-            tar.extractall(directory)
-            tar.close()
-
-        else:
-            raise RuntimeError(f"Unhandled extension type: {archive_path.suffix}")
-
-        # Delete the archive file now that it has been uncompressed
-        archive_path.unlink()
-
-    @abc.abstractmethod
-    def _iter(self):
-        pass
-
-    @property
-    def is_downloaded(self):
-        """Indicate whether or the data has been correctly downloaded."""
-        if self.path.exists():
-
-            if self.path.is_file():
-                return self.path.stat().st_size == self.size
-            return sum(f.stat().st_size for f in self.path.glob("**/*") if f.is_file())
-
-        return False
-
-    def __iter__(self):
-        if not self.is_downloaded:
-            self.download(verbose=True)
-        if not self.is_downloaded:
-            raise RuntimeError("Something went wrong during the download")
-        yield from self._iter()
-
-    @property
-    def _repr_content(self):
-        content = super()._repr_content
-        content["URL"] = self.url
-        content["Size"] = utils.pretty.humanize_bytes(self.size)
-        content["Downloaded"] = str(self.is_downloaded)
-        return content
+import abc
+import inspect
+import itertools
+import os
+import pathlib
+import re
+import shutil
+import tarfile
+import typing
+import zipfile
+from urllib import request
+
+from river import utils
+
+REG = "Regression"
+BINARY_CLF = "Binary classification"
+MULTI_CLF = "Multi-class classification"
+MO_BINARY_CLF = "Multi-output binary classification"
+MO_REG = "Multi-output regression"
+
+
+def get_data_home():
+    """Return the location where remote datasets are to be stored."""
+
+    data_home = os.environ.get("RIVER_DATA", os.path.join("~", "river_data"))
+    data_home = os.path.expanduser(data_home)
+    if not os.path.exists(data_home):
+        os.makedirs(data_home)
+    return data_home
+
+
+class Dataset(abc.ABC):
+    """Base class for all datasets.
+
+    All datasets inherit from this class, be they stored in a file or generated on the fly.
+
+    """
+
+    def __init__(
+        self,
+        task,
+        n_features,
+        n_samples=None,
+        n_classes=None,
+        n_outputs=None,
+        sparse=False,
+    ):
+        self.task = task
+        self.n_features = n_features
+        self.n_samples = n_samples
+        self.n_outputs = n_outputs
+        self.n_classes = n_classes
+        self.sparse = sparse
+
+    @abc.abstractmethod
+    def __iter__(self):
+        raise NotImplementedError
+
+    def take(self, k: int):
+        """Iterate over the k samples."""
+        return itertools.islice(self, k)
+
+    @property
+    def desc(self):
+        """Return the description from the docstring."""
+        desc = re.split(pattern=r"\w+\n\s{4}\-{3,}", string=self.__doc__, maxsplit=0)[0]
+        return inspect.cleandoc(desc)
+
+    @property
+    def _repr_content(self):
+        """The items that are displayed in the __repr__ method.
+
+        This property can be overridden in order to modify the output of the __repr__ method.
+
+        """
+
+        content = {}
+        content["Name"] = self.__class__.__name__
+        content["Task"] = self.task
+        if isinstance(self, SyntheticDataset) and self.n_samples is None:
+            content["Samples"] = "∞"
+        elif self.n_samples:
+            content["Samples"] = f"{self.n_samples:,}"
+        if self.n_features:
+            content["Features"] = f"{self.n_features:,}"
+        if self.n_outputs:
+            content["Outputs"] = f"{self.n_outputs:,}"
+        if self.n_classes:
+            content["Classes"] = f"{self.n_classes:,}"
+        content["Sparse"] = str(self.sparse)
+
+        return content
+
+    def __repr__(self):
+
+        l_len = max(map(len, self._repr_content.keys()))
+        r_len = max(map(len, self._repr_content.values()))
+
+        out = f"{self.desc}\n\n" + "\n".join(
+            k.rjust(l_len) + "  " + v.ljust(r_len)
+            for k, v in self._repr_content.items()
+        )
+
+        if "Parameters\n    ----------" in self.__doc__:
+            params = re.split(
+                r"\w+\n\s{4}\-{3,}",
+                re.split("Parameters\n    ----------", self.__doc__)[1],
+            )[0].rstrip()
+            out += f"\n\nParameters\n----------{params}"
+
+        return out
+
+
+class SyntheticDataset(Dataset):
+    """A synthetic dataset."""
+
+    def __repr__(self):
+        l_len_prop = max(map(len, self._repr_content.keys()))
+        r_len_prop = max(map(len, self._repr_content.values()))
+        params = self._get_params()
+        l_len_config = max(map(len, params.keys()))
+        r_len_config = max(map(len, map(str, params.values())))
+
+        out = (
+            "Synthetic data generator\n\n"
+            + "\n".join(
+                k.rjust(l_len_prop) + "  " + v.ljust(r_len_prop)
+                for k, v in self._repr_content.items()
+            )
+            + "\n\nConfiguration\n-------------\n"
+            + "\n".join(
+                k.rjust(l_len_config) + "  " + str(v).ljust(r_len_config)
+                for k, v in params.items()
+            )
+        )
+
+        return out
+
+    def _get_params(self) -> typing.Dict[str, typing.Any]:
+        """Return the parameters that were used during initialization."""
+        return {
+            name: getattr(self, name)
+            for name, param in inspect.signature(self.__init__).parameters.items()  # type: ignore
+            if param.kind != param.VAR_KEYWORD
+        }
+
+
+class FileDataset(Dataset):
+    """Base class for datasets that are stored in a local file.
+
+    Small datasets that are part of the river package inherit from this class.
+
+    """
+
+    def __init__(self, filename, **desc):
+        super().__init__(**desc)
+        self.filename = filename
+
+    @property
+    def path(self):
+        return pathlib.Path(__file__).parent.joinpath(self.filename)
+
+    @property
+    def _repr_content(self):
+        content = super()._repr_content
+        content["Path"] = str(self.path)
+        return content
+
+
+class RemoteDataset(FileDataset):
+    """Base class for datasets that are stored in a remote file.
+
+    Medium and large datasets that are not part of the river package inherit from this class.
+
+    The filename doesn't have to be provided if unpack is False. Indeed in the latter case the
+    filename will be inferred from the URL.
+
+    """
+
+    def __init__(self, url, size, filename=None, unpack=True, **desc):
+
+        if filename is None:
+            filename = os.path.basename(url)
+
+        super().__init__(filename=filename, **desc)
+        self.url = url
+        self.size = size
+        self.unpack = unpack
+
+    @property
+    def path(self):
+        return pathlib.Path(get_data_home(), self.__class__.__name__, self.filename)
+
+    def download(self, force=False, verbose=True):
+
+        if not force and self.is_downloaded:
+            return
+
+        # Determine where to download the archive
+        directory = self.path.parent
+        directory.mkdir(parents=True, exist_ok=True)
+        archive_path = directory.joinpath(os.path.basename(self.url))
+
+        with request.urlopen(self.url) as r:
+
+            # Notify the user
+            if verbose:
+                meta = r.info()
+                try:
+                    n_bytes = int(meta["Content-Length"])
+                    msg = f"Downloading {self.url} ({utils.pretty.humanize_bytes(n_bytes)})"
+                except KeyError:
+                    msg = f"Downloading {self.url}"
+                print(msg)
+
+            # Now dump the contents of the requests
+            with open(archive_path, "wb") as f:
+                shutil.copyfileobj(r, f)
+
+        if not self.unpack:
+            return
+
+        if verbose:
+            print(f"Uncompressing into {directory}")
+
+        if archive_path.suffix.endswith("zip"):
+            with zipfile.ZipFile(archive_path, "r") as zf:
+                zf.extractall(directory)
+
+        elif archive_path.suffix.endswith(("gz", "tar")):
+            mode = "r:" if archive_path.suffix.endswith("tar") else "r:gz"
+            tar = tarfile.open(archive_path, mode)
+            tar.extractall(directory)
+            tar.close()
+
+        else:
+            raise RuntimeError(f"Unhandled extension type: {archive_path.suffix}")
+
+        # Delete the archive file now that it has been uncompressed
+        archive_path.unlink()
+
+    @abc.abstractmethod
+    def _iter(self):
+        pass
+
+    @property
+    def is_downloaded(self):
+        """Indicate whether or the data has been correctly downloaded."""
+        if self.path.exists():
+
+            if self.path.is_file():
+                return self.path.stat().st_size == self.size
+            return sum(f.stat().st_size for f in self.path.glob("**/*") if f.is_file())
+
+        return False
+
+    def __iter__(self):
+        if not self.is_downloaded:
+            self.download(verbose=True)
+        if not self.is_downloaded:
+            raise RuntimeError("Something went wrong during the download")
+        yield from self._iter()
+
+    @property
+    def _repr_content(self):
+        content = super()._repr_content
+        content["URL"] = self.url
+        content["Size"] = utils.pretty.humanize_bytes(self.size)
+        content["Downloaded"] = str(self.is_downloaded)
+        return content
```

### Comparing `river-0.8.0/river/datasets/chick-weights.csv` & `river-0.9.0/river/datasets/chick-weights.csv`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,579 +1,579 @@
-weight,time,chick,diet
-42,0,1,1
-40,0,2,1
-43,0,3,1
-42,0,4,1
-41,0,5,1
-41,0,6,1
-41,0,7,1
-42,0,8,1
-42,0,9,1
-41,0,10,1
-43,0,11,1
-41,0,12,1
-41,0,13,1
-41,0,14,1
-41,0,15,1
-41,0,16,1
-42,0,17,1
-39,0,18,1
-43,0,19,1
-41,0,20,1
-40,0,21,2
-41,0,22,2
-43,0,23,2
-42,0,24,2
-40,0,25,2
-42,0,26,2
-39,0,27,2
-39,0,28,2
-39,0,29,2
-42,0,30,2
-42,0,31,3
-41,0,32,3
-39,0,33,3
-41,0,34,3
-41,0,35,3
-39,0,36,3
-41,0,37,3
-41,0,38,3
-42,0,39,3
-41,0,40,3
-42,0,41,4
-42,0,42,4
-42,0,43,4
-42,0,44,4
-41,0,45,4
-40,0,46,4
-41,0,47,4
-39,0,48,4
-40,0,49,4
-41,0,50,4
-51,2,1,1
-49,2,2,1
-39,2,3,1
-49,2,4,1
-42,2,5,1
-49,2,6,1
-49,2,7,1
-50,2,8,1
-51,2,9,1
-44,2,10,1
-51,2,11,1
-49,2,12,1
-48,2,13,1
-49,2,14,1
-49,2,15,1
-45,2,16,1
-51,2,17,1
-35,2,18,1
-48,2,19,1
-47,2,20,1
-50,2,21,2
-55,2,22,2
-52,2,23,2
-52,2,24,2
-49,2,25,2
-48,2,26,2
-46,2,27,2
-46,2,28,2
-48,2,29,2
-48,2,30,2
-53,2,31,3
-49,2,32,3
-50,2,33,3
-49,2,34,3
-53,2,35,3
-48,2,36,3
-48,2,37,3
-49,2,38,3
-50,2,39,3
-55,2,40,3
-51,2,41,4
-49,2,42,4
-55,2,43,4
-51,2,44,4
-50,2,45,4
-52,2,46,4
-53,2,47,4
-50,2,48,4
-53,2,49,4
-54,2,50,4
-59,4,1,1
-58,4,2,1
-55,4,3,1
-56,4,4,1
-48,4,5,1
-59,4,6,1
-57,4,7,1
-61,4,8,1
-59,4,9,1
-52,4,10,1
-63,4,11,1
-56,4,12,1
-53,4,13,1
-62,4,14,1
-56,4,15,1
-49,4,16,1
-61,4,17,1
-55,4,19,1
-54,4,20,1
-62,4,21,2
-64,4,22,2
-61,4,23,2
-58,4,24,2
-62,4,25,2
-57,4,26,2
-58,4,27,2
-58,4,28,2
-59,4,29,2
-59,4,30,2
-62,4,31,3
-65,4,32,3
-63,4,33,3
-63,4,34,3
-64,4,35,3
-61,4,36,3
-56,4,37,3
-61,4,38,3
-61,4,39,3
-66,4,40,3
-66,4,41,4
-63,4,42,4
-69,4,43,4
-65,4,44,4
-61,4,45,4
-62,4,46,4
-66,4,47,4
-62,4,48,4
-64,4,49,4
-67,4,50,4
-64,6,1,1
-72,6,2,1
-67,6,3,1
-67,6,4,1
-60,6,5,1
-74,6,6,1
-71,6,7,1
-71,6,8,1
-68,6,9,1
-63,6,10,1
-84,6,11,1
-62,6,12,1
-60,6,13,1
-79,6,14,1
-64,6,15,1
-51,6,16,1
-72,6,17,1
-62,6,19,1
-58,6,20,1
-86,6,21,2
-77,6,22,2
-73,6,23,2
-74,6,24,2
-78,6,25,2
-74,6,26,2
-73,6,27,2
-73,6,28,2
-74,6,29,2
-72,6,30,2
-73,6,31,3
-82,6,32,3
-77,6,33,3
-85,6,34,3
-87,6,35,3
-76,6,36,3
-68,6,37,3
-74,6,38,3
-78,6,39,3
-79,6,40,3
-85,6,41,4
-84,6,42,4
-96,6,43,4
-86,6,44,4
-78,6,45,4
-82,6,46,4
-79,6,47,4
-80,6,48,4
-85,6,49,4
-84,6,50,4
-76,8,1,1
-84,8,2,1
-84,8,3,1
-74,8,4,1
-79,8,5,1
-97,8,6,1
-89,8,7,1
-84,8,8,1
-85,8,9,1
-74,8,10,1
-112,8,11,1
-72,8,12,1
-65,8,13,1
-101,8,14,1
-68,8,15,1
-57,8,16,1
-83,8,17,1
-65,8,19,1
-65,8,20,1
-125,8,21,2
-90,8,22,2
-90,8,23,2
-66,8,24,2
-102,8,25,2
-93,8,26,2
-87,8,27,2
-92,8,28,2
-87,8,29,2
-85,8,30,2
-85,8,31,3
-107,8,32,3
-96,8,33,3
-107,8,34,3
-123,8,35,3
-98,8,36,3
-80,8,37,3
-98,8,38,3
-89,8,39,3
-101,8,40,3
-103,8,41,4
-103,8,42,4
-131,8,43,4
-103,8,44,4
-98,8,45,4
-101,8,46,4
-100,8,47,4
-104,8,48,4
-108,8,49,4
-105,8,50,4
-93,10,1,1
-103,10,2,1
-99,10,3,1
-87,10,4,1
-106,10,5,1
-124,10,6,1
-112,10,7,1
-93,10,8,1
-96,10,9,1
-81,10,10,1
-139,10,11,1
-88,10,12,1
-67,10,13,1
-128,10,14,1
-68,10,15,1
-51,10,16,1
-89,10,17,1
-71,10,19,1
-73,10,20,1
-163,10,21,2
-95,10,22,2
-103,10,23,2
-68,10,24,2
-124,10,25,2
-114,10,26,2
-100,10,27,2
-114,10,28,2
-106,10,29,2
-98,10,30,2
-102,10,31,3
-129,10,32,3
-111,10,33,3
-134,10,34,3
-158,10,35,3
-116,10,36,3
-83,10,37,3
-109,10,38,3
-109,10,39,3
-120,10,40,3
-124,10,41,4
-126,10,42,4
-157,10,43,4
-118,10,44,4
-117,10,45,4
-120,10,46,4
-123,10,47,4
-125,10,48,4
-128,10,49,4
-122,10,50,4
-106,12,1,1
-122,12,2,1
-115,12,3,1
-102,12,4,1
-141,12,5,1
-141,12,6,1
-146,12,7,1
-110,12,8,1
-90,12,9,1
-89,12,10,1
-168,12,11,1
-119,12,12,1
-71,12,13,1
-164,12,14,1
-67,12,15,1
-54,12,16,1
-98,12,17,1
-82,12,19,1
-77,12,20,1
-217,12,21,2
-108,12,22,2
-127,12,23,2
-70,12,24,2
-146,12,25,2
-136,12,26,2
-115,12,27,2
-145,12,28,2
-134,12,29,2
-115,12,30,2
-123,12,31,3
-159,12,32,3
-137,12,33,3
-164,12,34,3
-201,12,35,3
-145,12,36,3
-103,12,37,3
-128,12,38,3
-130,12,39,3
-154,12,40,3
-155,12,41,4
-160,12,42,4
-184,12,43,4
-127,12,44,4
-135,12,45,4
-144,12,46,4
-148,12,47,4
-154,12,48,4
-152,12,49,4
-155,12,50,4
-125,14,1,1
-138,14,2,1
-138,14,3,1
-108,14,4,1
-164,14,5,1
-148,14,6,1
-174,14,7,1
-116,14,8,1
-92,14,9,1
-96,14,10,1
-177,14,11,1
-135,14,12,1
-70,14,13,1
-192,14,14,1
-68,14,15,1
-103,14,17,1
-88,14,19,1
-89,14,20,1
-240,14,21,2
-111,14,22,2
-135,14,23,2
-71,14,24,2
-164,14,25,2
-147,14,26,2
-123,14,27,2
-156,14,28,2
-150,14,29,2
-122,14,30,2
-138,14,31,3
-179,14,32,3
-144,14,33,3
-186,14,34,3
-238,14,35,3
-166,14,36,3
-112,14,37,3
-154,14,38,3
-146,14,39,3
-182,14,40,3
-153,14,41,4
-174,14,42,4
-188,14,43,4
-138,14,44,4
-141,14,45,4
-156,14,46,4
-157,14,47,4
-170,14,48,4
-166,14,49,4
-175,14,50,4
-149,16,1,1
-162,16,2,1
-163,16,3,1
-136,16,4,1
-197,16,5,1
-155,16,6,1
-218,16,7,1
-126,16,8,1
-93,16,9,1
-101,16,10,1
-182,16,11,1
-162,16,12,1
-71,16,13,1
-227,16,14,1
-113,16,17,1
-106,16,19,1
-98,16,20,1
-275,16,21,2
-131,16,22,2
-145,16,23,2
-72,16,24,2
-197,16,25,2
-169,16,26,2
-144,16,27,2
-184,16,28,2
-187,16,29,2
-143,16,30,2
-170,16,31,3
-221,16,32,3
-151,16,33,3
-235,16,34,3
-287,16,35,3
-198,16,36,3
-135,16,37,3
-192,16,38,3
-170,16,39,3
-215,16,40,3
-175,16,41,4
-204,16,42,4
-197,16,43,4
-145,16,44,4
-147,16,45,4
-173,16,46,4
-168,16,47,4
-222,16,48,4
-184,16,49,4
-205,16,50,4
-171,18,1,1
-187,18,2,1
-187,18,3,1
-154,18,4,1
-199,18,5,1
-160,18,6,1
-250,18,7,1
-134,18,8,1
-100,18,9,1
-112,18,10,1
-184,18,11,1
-185,18,12,1
-81,18,13,1
-248,18,14,1
-123,18,17,1
-120,18,19,1
-107,18,20,1
-307,18,21,2
-148,18,22,2
-163,18,23,2
-72,18,24,2
-231,18,25,2
-205,18,26,2
-163,18,27,2
-207,18,28,2
-230,18,29,2
-151,18,30,2
-204,18,31,3
-263,18,32,3
-146,18,33,3
-294,18,34,3
-332,18,35,3
-227,18,36,3
-157,18,37,3
-232,18,38,3
-214,18,39,3
-262,18,40,3
-184,18,41,4
-234,18,42,4
-198,18,43,4
-146,18,44,4
-174,18,45,4
-210,18,46,4
-185,18,47,4
-261,18,48,4
-203,18,49,4
-234,18,50,4
-199,20,1,1
-209,20,2,1
-198,20,3,1
-160,20,4,1
-220,20,5,1
-160,20,6,1
-288,20,7,1
-125,20,8,1
-100,20,9,1
-120,20,10,1
-181,20,11,1
-195,20,12,1
-91,20,13,1
-259,20,14,1
-133,20,17,1
-144,20,19,1
-115,20,20,1
-318,20,21,2
-164,20,22,2
-170,20,23,2
-76,20,24,2
-259,20,25,2
-236,20,26,2
-185,20,27,2
-212,20,28,2
-279,20,29,2
-157,20,30,2
-235,20,31,3
-291,20,32,3
-156,20,33,3
-327,20,34,3
-361,20,35,3
-225,20,36,3
-169,20,37,3
-280,20,38,3
-250,20,39,3
-295,20,40,3
-199,20,41,4
-269,20,42,4
-199,20,43,4
-197,20,45,4
-231,20,46,4
-210,20,47,4
-303,20,48,4
-233,20,49,4
-264,20,50,4
-205,21,1,1
-215,21,2,1
-202,21,3,1
-157,21,4,1
-223,21,5,1
-157,21,6,1
-305,21,7,1
-98,21,9,1
-124,21,10,1
-175,21,11,1
-205,21,12,1
-96,21,13,1
-266,21,14,1
-142,21,17,1
-157,21,19,1
-117,21,20,1
-331,21,21,2
-167,21,22,2
-175,21,23,2
-74,21,24,2
-265,21,25,2
-251,21,26,2
-192,21,27,2
-233,21,28,2
-309,21,29,2
-150,21,30,2
-256,21,31,3
-305,21,32,3
-147,21,33,3
-341,21,34,3
-373,21,35,3
-220,21,36,3
-178,21,37,3
-290,21,38,3
-272,21,39,3
-321,21,40,3
-204,21,41,4
-281,21,42,4
-200,21,43,4
-196,21,45,4
-238,21,46,4
-205,21,47,4
-322,21,48,4
-237,21,49,4
-264,21,50,4
+weight,time,chick,diet
+42,0,1,1
+40,0,2,1
+43,0,3,1
+42,0,4,1
+41,0,5,1
+41,0,6,1
+41,0,7,1
+42,0,8,1
+42,0,9,1
+41,0,10,1
+43,0,11,1
+41,0,12,1
+41,0,13,1
+41,0,14,1
+41,0,15,1
+41,0,16,1
+42,0,17,1
+39,0,18,1
+43,0,19,1
+41,0,20,1
+40,0,21,2
+41,0,22,2
+43,0,23,2
+42,0,24,2
+40,0,25,2
+42,0,26,2
+39,0,27,2
+39,0,28,2
+39,0,29,2
+42,0,30,2
+42,0,31,3
+41,0,32,3
+39,0,33,3
+41,0,34,3
+41,0,35,3
+39,0,36,3
+41,0,37,3
+41,0,38,3
+42,0,39,3
+41,0,40,3
+42,0,41,4
+42,0,42,4
+42,0,43,4
+42,0,44,4
+41,0,45,4
+40,0,46,4
+41,0,47,4
+39,0,48,4
+40,0,49,4
+41,0,50,4
+51,2,1,1
+49,2,2,1
+39,2,3,1
+49,2,4,1
+42,2,5,1
+49,2,6,1
+49,2,7,1
+50,2,8,1
+51,2,9,1
+44,2,10,1
+51,2,11,1
+49,2,12,1
+48,2,13,1
+49,2,14,1
+49,2,15,1
+45,2,16,1
+51,2,17,1
+35,2,18,1
+48,2,19,1
+47,2,20,1
+50,2,21,2
+55,2,22,2
+52,2,23,2
+52,2,24,2
+49,2,25,2
+48,2,26,2
+46,2,27,2
+46,2,28,2
+48,2,29,2
+48,2,30,2
+53,2,31,3
+49,2,32,3
+50,2,33,3
+49,2,34,3
+53,2,35,3
+48,2,36,3
+48,2,37,3
+49,2,38,3
+50,2,39,3
+55,2,40,3
+51,2,41,4
+49,2,42,4
+55,2,43,4
+51,2,44,4
+50,2,45,4
+52,2,46,4
+53,2,47,4
+50,2,48,4
+53,2,49,4
+54,2,50,4
+59,4,1,1
+58,4,2,1
+55,4,3,1
+56,4,4,1
+48,4,5,1
+59,4,6,1
+57,4,7,1
+61,4,8,1
+59,4,9,1
+52,4,10,1
+63,4,11,1
+56,4,12,1
+53,4,13,1
+62,4,14,1
+56,4,15,1
+49,4,16,1
+61,4,17,1
+55,4,19,1
+54,4,20,1
+62,4,21,2
+64,4,22,2
+61,4,23,2
+58,4,24,2
+62,4,25,2
+57,4,26,2
+58,4,27,2
+58,4,28,2
+59,4,29,2
+59,4,30,2
+62,4,31,3
+65,4,32,3
+63,4,33,3
+63,4,34,3
+64,4,35,3
+61,4,36,3
+56,4,37,3
+61,4,38,3
+61,4,39,3
+66,4,40,3
+66,4,41,4
+63,4,42,4
+69,4,43,4
+65,4,44,4
+61,4,45,4
+62,4,46,4
+66,4,47,4
+62,4,48,4
+64,4,49,4
+67,4,50,4
+64,6,1,1
+72,6,2,1
+67,6,3,1
+67,6,4,1
+60,6,5,1
+74,6,6,1
+71,6,7,1
+71,6,8,1
+68,6,9,1
+63,6,10,1
+84,6,11,1
+62,6,12,1
+60,6,13,1
+79,6,14,1
+64,6,15,1
+51,6,16,1
+72,6,17,1
+62,6,19,1
+58,6,20,1
+86,6,21,2
+77,6,22,2
+73,6,23,2
+74,6,24,2
+78,6,25,2
+74,6,26,2
+73,6,27,2
+73,6,28,2
+74,6,29,2
+72,6,30,2
+73,6,31,3
+82,6,32,3
+77,6,33,3
+85,6,34,3
+87,6,35,3
+76,6,36,3
+68,6,37,3
+74,6,38,3
+78,6,39,3
+79,6,40,3
+85,6,41,4
+84,6,42,4
+96,6,43,4
+86,6,44,4
+78,6,45,4
+82,6,46,4
+79,6,47,4
+80,6,48,4
+85,6,49,4
+84,6,50,4
+76,8,1,1
+84,8,2,1
+84,8,3,1
+74,8,4,1
+79,8,5,1
+97,8,6,1
+89,8,7,1
+84,8,8,1
+85,8,9,1
+74,8,10,1
+112,8,11,1
+72,8,12,1
+65,8,13,1
+101,8,14,1
+68,8,15,1
+57,8,16,1
+83,8,17,1
+65,8,19,1
+65,8,20,1
+125,8,21,2
+90,8,22,2
+90,8,23,2
+66,8,24,2
+102,8,25,2
+93,8,26,2
+87,8,27,2
+92,8,28,2
+87,8,29,2
+85,8,30,2
+85,8,31,3
+107,8,32,3
+96,8,33,3
+107,8,34,3
+123,8,35,3
+98,8,36,3
+80,8,37,3
+98,8,38,3
+89,8,39,3
+101,8,40,3
+103,8,41,4
+103,8,42,4
+131,8,43,4
+103,8,44,4
+98,8,45,4
+101,8,46,4
+100,8,47,4
+104,8,48,4
+108,8,49,4
+105,8,50,4
+93,10,1,1
+103,10,2,1
+99,10,3,1
+87,10,4,1
+106,10,5,1
+124,10,6,1
+112,10,7,1
+93,10,8,1
+96,10,9,1
+81,10,10,1
+139,10,11,1
+88,10,12,1
+67,10,13,1
+128,10,14,1
+68,10,15,1
+51,10,16,1
+89,10,17,1
+71,10,19,1
+73,10,20,1
+163,10,21,2
+95,10,22,2
+103,10,23,2
+68,10,24,2
+124,10,25,2
+114,10,26,2
+100,10,27,2
+114,10,28,2
+106,10,29,2
+98,10,30,2
+102,10,31,3
+129,10,32,3
+111,10,33,3
+134,10,34,3
+158,10,35,3
+116,10,36,3
+83,10,37,3
+109,10,38,3
+109,10,39,3
+120,10,40,3
+124,10,41,4
+126,10,42,4
+157,10,43,4
+118,10,44,4
+117,10,45,4
+120,10,46,4
+123,10,47,4
+125,10,48,4
+128,10,49,4
+122,10,50,4
+106,12,1,1
+122,12,2,1
+115,12,3,1
+102,12,4,1
+141,12,5,1
+141,12,6,1
+146,12,7,1
+110,12,8,1
+90,12,9,1
+89,12,10,1
+168,12,11,1
+119,12,12,1
+71,12,13,1
+164,12,14,1
+67,12,15,1
+54,12,16,1
+98,12,17,1
+82,12,19,1
+77,12,20,1
+217,12,21,2
+108,12,22,2
+127,12,23,2
+70,12,24,2
+146,12,25,2
+136,12,26,2
+115,12,27,2
+145,12,28,2
+134,12,29,2
+115,12,30,2
+123,12,31,3
+159,12,32,3
+137,12,33,3
+164,12,34,3
+201,12,35,3
+145,12,36,3
+103,12,37,3
+128,12,38,3
+130,12,39,3
+154,12,40,3
+155,12,41,4
+160,12,42,4
+184,12,43,4
+127,12,44,4
+135,12,45,4
+144,12,46,4
+148,12,47,4
+154,12,48,4
+152,12,49,4
+155,12,50,4
+125,14,1,1
+138,14,2,1
+138,14,3,1
+108,14,4,1
+164,14,5,1
+148,14,6,1
+174,14,7,1
+116,14,8,1
+92,14,9,1
+96,14,10,1
+177,14,11,1
+135,14,12,1
+70,14,13,1
+192,14,14,1
+68,14,15,1
+103,14,17,1
+88,14,19,1
+89,14,20,1
+240,14,21,2
+111,14,22,2
+135,14,23,2
+71,14,24,2
+164,14,25,2
+147,14,26,2
+123,14,27,2
+156,14,28,2
+150,14,29,2
+122,14,30,2
+138,14,31,3
+179,14,32,3
+144,14,33,3
+186,14,34,3
+238,14,35,3
+166,14,36,3
+112,14,37,3
+154,14,38,3
+146,14,39,3
+182,14,40,3
+153,14,41,4
+174,14,42,4
+188,14,43,4
+138,14,44,4
+141,14,45,4
+156,14,46,4
+157,14,47,4
+170,14,48,4
+166,14,49,4
+175,14,50,4
+149,16,1,1
+162,16,2,1
+163,16,3,1
+136,16,4,1
+197,16,5,1
+155,16,6,1
+218,16,7,1
+126,16,8,1
+93,16,9,1
+101,16,10,1
+182,16,11,1
+162,16,12,1
+71,16,13,1
+227,16,14,1
+113,16,17,1
+106,16,19,1
+98,16,20,1
+275,16,21,2
+131,16,22,2
+145,16,23,2
+72,16,24,2
+197,16,25,2
+169,16,26,2
+144,16,27,2
+184,16,28,2
+187,16,29,2
+143,16,30,2
+170,16,31,3
+221,16,32,3
+151,16,33,3
+235,16,34,3
+287,16,35,3
+198,16,36,3
+135,16,37,3
+192,16,38,3
+170,16,39,3
+215,16,40,3
+175,16,41,4
+204,16,42,4
+197,16,43,4
+145,16,44,4
+147,16,45,4
+173,16,46,4
+168,16,47,4
+222,16,48,4
+184,16,49,4
+205,16,50,4
+171,18,1,1
+187,18,2,1
+187,18,3,1
+154,18,4,1
+199,18,5,1
+160,18,6,1
+250,18,7,1
+134,18,8,1
+100,18,9,1
+112,18,10,1
+184,18,11,1
+185,18,12,1
+81,18,13,1
+248,18,14,1
+123,18,17,1
+120,18,19,1
+107,18,20,1
+307,18,21,2
+148,18,22,2
+163,18,23,2
+72,18,24,2
+231,18,25,2
+205,18,26,2
+163,18,27,2
+207,18,28,2
+230,18,29,2
+151,18,30,2
+204,18,31,3
+263,18,32,3
+146,18,33,3
+294,18,34,3
+332,18,35,3
+227,18,36,3
+157,18,37,3
+232,18,38,3
+214,18,39,3
+262,18,40,3
+184,18,41,4
+234,18,42,4
+198,18,43,4
+146,18,44,4
+174,18,45,4
+210,18,46,4
+185,18,47,4
+261,18,48,4
+203,18,49,4
+234,18,50,4
+199,20,1,1
+209,20,2,1
+198,20,3,1
+160,20,4,1
+220,20,5,1
+160,20,6,1
+288,20,7,1
+125,20,8,1
+100,20,9,1
+120,20,10,1
+181,20,11,1
+195,20,12,1
+91,20,13,1
+259,20,14,1
+133,20,17,1
+144,20,19,1
+115,20,20,1
+318,20,21,2
+164,20,22,2
+170,20,23,2
+76,20,24,2
+259,20,25,2
+236,20,26,2
+185,20,27,2
+212,20,28,2
+279,20,29,2
+157,20,30,2
+235,20,31,3
+291,20,32,3
+156,20,33,3
+327,20,34,3
+361,20,35,3
+225,20,36,3
+169,20,37,3
+280,20,38,3
+250,20,39,3
+295,20,40,3
+199,20,41,4
+269,20,42,4
+199,20,43,4
+197,20,45,4
+231,20,46,4
+210,20,47,4
+303,20,48,4
+233,20,49,4
+264,20,50,4
+205,21,1,1
+215,21,2,1
+202,21,3,1
+157,21,4,1
+223,21,5,1
+157,21,6,1
+305,21,7,1
+98,21,9,1
+124,21,10,1
+175,21,11,1
+205,21,12,1
+96,21,13,1
+266,21,14,1
+142,21,17,1
+157,21,19,1
+117,21,20,1
+331,21,21,2
+167,21,22,2
+175,21,23,2
+74,21,24,2
+265,21,25,2
+251,21,26,2
+192,21,27,2
+233,21,28,2
+309,21,29,2
+150,21,30,2
+256,21,31,3
+305,21,32,3
+147,21,33,3
+341,21,34,3
+373,21,35,3
+220,21,36,3
+178,21,37,3
+290,21,38,3
+272,21,39,3
+321,21,40,3
+204,21,41,4
+281,21,42,4
+200,21,43,4
+196,21,45,4
+238,21,46,4
+205,21,47,4
+322,21,48,4
+237,21,49,4
+264,21,50,4
```

### Comparing `river-0.8.0/river/datasets/chick_weights.py` & `river-0.9.0/river/datasets/chick_weights.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,29 +1,29 @@
-from river import stream
-
-from . import base
-
-
-class ChickWeights(base.FileDataset):
-    """Chick weights along time.
-
-    The stream contains 578 items and 3 features. The goal is to predict the weight of each chick
-    along time, according to the diet the chick is on. The data is ordered by time and then by
-    chick.
-
-    References
-    ----------
-    [^1]: [Chick weight dataset overview](http://rstudio-pubs-static.s3.amazonaws.com/107631_131ad1c022df4f90aa2d214a5c5609b2.html)
-
-    """
-
-    def __init__(self):
-        super().__init__(
-            filename="chick-weights.csv", n_samples=578, n_features=3, task=base.REG
-        )
-
-    def __iter__(self):
-        return stream.iter_csv(
-            self.path,
-            target="weight",
-            converters={"time": int, "weight": int, "chick": int, "diet": int},
-        )
+from river import stream
+
+from . import base
+
+
+class ChickWeights(base.FileDataset):
+    """Chick weights along time.
+
+    The stream contains 578 items and 3 features. The goal is to predict the weight of each chick
+    along time, according to the diet the chick is on. The data is ordered by time and then by
+    chick.
+
+    References
+    ----------
+    [^1]: [Chick weight dataset overview](http://rstudio-pubs-static.s3.amazonaws.com/107631_131ad1c022df4f90aa2d214a5c5609b2.html)
+
+    """
+
+    def __init__(self):
+        super().__init__(
+            filename="chick-weights.csv", n_samples=578, n_features=3, task=base.REG
+        )
+
+    def __iter__(self):
+        return stream.iter_csv(
+            self.path,
+            target="weight",
+            converters={"time": int, "weight": int, "chick": int, "diet": int},
+        )
```

### Comparing `river-0.8.0/river/datasets/credit_card.py` & `river-0.9.0/river/datasets/credit_card.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,53 +1,53 @@
-from river import stream
-
-from . import base
-
-
-class CreditCard(base.RemoteDataset):
-    """Credit card frauds.
-
-    The datasets contains transactions made by credit cards in September 2013 by european
-    cardholders. This dataset presents transactions that occurred in two days, where we have 492
-    frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class
-    (frauds) account for 0.172% of all transactions.
-
-    It contains only numerical input variables which are the result of a PCA transformation.
-    Unfortunately, due to confidentiality issues, we cannot provide the original features and more
-    background information about the data. Features V1, V2, ... V28 are the principal components
-    obtained with PCA, the only features which have not been transformed with PCA are 'Time' and
-    'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first
-    transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be
-    used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and
-    it takes value 1 in case of fraud and 0 otherwise.
-
-    References
-    ----------
-    [^1]: Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015
-    [^2]: Dal Pozzolo, Andrea; Caelen, Olivier; Le Borgne, Yann-Ael; Waterschoot, Serge; Bontempi, Gianluca. Learned lessons in credit card fraud detection from a practitioner perspective, Expert systems with applications,41,10,4915-4928,2014, Pergamon
-    [^3]: Dal Pozzolo, Andrea; Boracchi, Giacomo; Caelen, Olivier; Alippi, Cesare; Bontempi, Gianluca. Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE transactions on neural networks and learning systems,29,8,3784-3797,2018,IEEE
-    [^4]: Dal Pozzolo, Andrea Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)
-    [^5]: Carcillo, Fabrizio; Dal Pozzolo, Andrea; Le Borgne, Yann-Ael; Caelen, Olivier; Mazzer, Yannis; Bontempi, Gianluca. Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information fusion,41, 182-194,2018,Elsevier
-    [^6]: Carcillo, Fabrizio; Le Borgne, Yann-Ael; Caelen, Olivier; Bontempi, Gianluca. Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics, 5,4,285-300,2018,Springer International Publishing
-    [^7]: Bertrand Lebichot, Yann-Ael Le Borgne, Liyun He, Frederic Oble, Gianluca Bontempi Deep-Learning Domain Adaptation Techniques for Credit Cards Fraud Detection, INNSBDDL 2019: Recent Advances in Big Data and Deep Learning, pp 78-88, 2019
-    [^8]: Fabrizio Carcillo, Yann-Ael Le Borgne, Olivier Caelen, Frederic Oble, Gianluca Bontempi Combining Unsupervised and Supervised Learning in Credit Card Fraud Detection Information Sciences, 2019
-
-    """
-
-    def __init__(self):
-        super().__init__(
-            n_samples=284_807,
-            n_features=30,
-            task=base.BINARY_CLF,
-            url="https://maxhalford.github.io/files/datasets/creditcardfraud.zip",
-            size=150828752,
-            filename="creditcard.csv",
-        )
-
-    def _iter(self):
-
-        converters = {f"V{i}": float for i in range(1, 29)}
-        converters["Class"] = int
-        converters["Time"] = float
-        converters["Amount"] = float
-
-        return stream.iter_csv(self.path, target="Class", converters=converters)
+from river import stream
+
+from . import base
+
+
+class CreditCard(base.RemoteDataset):
+    """Credit card frauds.
+
+    The datasets contains transactions made by credit cards in September 2013 by european
+    cardholders. This dataset presents transactions that occurred in two days, where we have 492
+    frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class
+    (frauds) account for 0.172% of all transactions.
+
+    It contains only numerical input variables which are the result of a PCA transformation.
+    Unfortunately, due to confidentiality issues, we cannot provide the original features and more
+    background information about the data. Features V1, V2, ... V28 are the principal components
+    obtained with PCA, the only features which have not been transformed with PCA are 'Time' and
+    'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first
+    transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be
+    used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and
+    it takes value 1 in case of fraud and 0 otherwise.
+
+    References
+    ----------
+    [^1]: Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015
+    [^2]: Dal Pozzolo, Andrea; Caelen, Olivier; Le Borgne, Yann-Ael; Waterschoot, Serge; Bontempi, Gianluca. Learned lessons in credit card fraud detection from a practitioner perspective, Expert systems with applications,41,10,4915-4928,2014, Pergamon
+    [^3]: Dal Pozzolo, Andrea; Boracchi, Giacomo; Caelen, Olivier; Alippi, Cesare; Bontempi, Gianluca. Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE transactions on neural networks and learning systems,29,8,3784-3797,2018,IEEE
+    [^4]: Dal Pozzolo, Andrea Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)
+    [^5]: Carcillo, Fabrizio; Dal Pozzolo, Andrea; Le Borgne, Yann-Ael; Caelen, Olivier; Mazzer, Yannis; Bontempi, Gianluca. Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information fusion,41, 182-194,2018,Elsevier
+    [^6]: Carcillo, Fabrizio; Le Borgne, Yann-Ael; Caelen, Olivier; Bontempi, Gianluca. Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics, 5,4,285-300,2018,Springer International Publishing
+    [^7]: Bertrand Lebichot, Yann-Ael Le Borgne, Liyun He, Frederic Oble, Gianluca Bontempi Deep-Learning Domain Adaptation Techniques for Credit Cards Fraud Detection, INNSBDDL 2019: Recent Advances in Big Data and Deep Learning, pp 78-88, 2019
+    [^8]: Fabrizio Carcillo, Yann-Ael Le Borgne, Olivier Caelen, Frederic Oble, Gianluca Bontempi Combining Unsupervised and Supervised Learning in Credit Card Fraud Detection Information Sciences, 2019
+
+    """
+
+    def __init__(self):
+        super().__init__(
+            n_samples=284_807,
+            n_features=30,
+            task=base.BINARY_CLF,
+            url="https://maxhalford.github.io/files/datasets/creditcardfraud.zip",
+            size=150_828_752,
+            filename="creditcard.csv",
+        )
+
+    def _iter(self):
+
+        converters = {f"V{i}": float for i in range(1, 29)}
+        converters["Class"] = int
+        converters["Time"] = float
+        converters["Amount"] = float
+
+        return stream.iter_csv(self.path, target="Class", converters=converters)
```

### Comparing `river-0.8.0/river/datasets/elec2.py` & `river-0.9.0/river/datasets/elec2.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,49 +1,49 @@
-from river import stream
-
-from . import base
-
-
-class Elec2(base.RemoteDataset):
-    """Electricity prices in New South Wales.
-
-    This is a binary classification task, where the goal is to predict if the price of electricity
-    will go up or down.
-
-    This data was collected from the Australian New South Wales Electricity Market. In this market,
-    prices are not fixed and are affected by demand and supply of the market. They are set every
-    five minutes. Electricity transfers to/from the neighboring state of Victoria were done to
-    alleviate fluctuations.
-
-    References
-    ----------
-    [^1]: [SPLICE-2 Comparative Evaluation: Electricity Pricing](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.12.9405)
-    [^2]: [DataHub description](https://datahub.io/machine-learning/electricity#readme)
-
-    """
-
-    def __init__(self):
-        super().__init__(
-            url="https://maxhalford.github.io/files/datasets/electricity.zip",
-            size=3091689,
-            task=base.BINARY_CLF,
-            n_samples=45_312,
-            n_features=8,
-            filename="electricity.csv",
-        )
-
-    def _iter(self):
-        return stream.iter_csv(
-            self.path,
-            target="class",
-            converters={
-                "date": float,
-                "day": int,
-                "period": float,
-                "nswprice": float,
-                "nswdemand": float,
-                "vicprice": float,
-                "vicdemand": float,
-                "transfer": float,
-                "class": lambda x: x == "UP",
-            },
-        )
+from river import stream
+
+from . import base
+
+
+class Elec2(base.RemoteDataset):
+    """Electricity prices in New South Wales.
+
+    This is a binary classification task, where the goal is to predict if the price of electricity
+    will go up or down.
+
+    This data was collected from the Australian New South Wales Electricity Market. In this market,
+    prices are not fixed and are affected by demand and supply of the market. They are set every
+    five minutes. Electricity transfers to/from the neighboring state of Victoria were done to
+    alleviate fluctuations.
+
+    References
+    ----------
+    [^1]: [SPLICE-2 Comparative Evaluation: Electricity Pricing](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.12.9405)
+    [^2]: [DataHub description](https://datahub.io/machine-learning/electricity#readme)
+
+    """
+
+    def __init__(self):
+        super().__init__(
+            url="https://maxhalford.github.io/files/datasets/electricity.zip",
+            size=3_091_689,
+            task=base.BINARY_CLF,
+            n_samples=45_312,
+            n_features=8,
+            filename="electricity.csv",
+        )
+
+    def _iter(self):
+        return stream.iter_csv(
+            self.path,
+            target="class",
+            converters={
+                "date": float,
+                "day": int,
+                "period": float,
+                "nswprice": float,
+                "nswdemand": float,
+                "vicprice": float,
+                "vicdemand": float,
+                "transfer": float,
+                "class": lambda x: x == "UP",
+            },
+        )
```

### Comparing `river-0.8.0/river/datasets/higgs.py` & `river-0.9.0/river/datasets/higgs.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,71 +1,71 @@
-from river import stream
-
-from . import base
-
-
-class Higgs(base.RemoteDataset):
-    """Higgs dataset.
-
-    The data has been produced using Monte Carlo simulations. The first 21 features (columns 2-22)
-    are kinematic properties measured by the particle detectors in the accelerator. The last seven
-    features are functions of the first 21 features; these are high-level features derived by
-    physicists to help discriminate between the two classes.
-
-    References
-    ----------
-    [^1]: [UCI page](https://archive.ics.uci.edu/ml/datasets/HIGGS)
-
-    """
-
-    def __init__(self):
-        super().__init__(
-            n_samples=11_000_000,
-            n_features=28,
-            task=base.BINARY_CLF,
-            url="https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz",
-            size=2816407858,
-            unpack=False,
-        )
-
-    def _iter(self):
-
-        features = [
-            "lepton pT",
-            "lepton eta",
-            "lepton phi",
-            "missing energy magnitude",
-            "missing energy phi",
-            "jet 1 pt",
-            "jet 1 eta",
-            "jet 1 phi",
-            "jet 1 b-tag",
-            "jet 2 pt",
-            "jet 2 eta",
-            "jet 2 phi",
-            "jet 2 b-tag",
-            "jet 3 pt",
-            "jet 3 eta",
-            "jet 3 phi",
-            "jet 3 b-tag",
-            "jet 4 pt",
-            "jet 4 eta",
-            "jet 4 phi",
-            "jet 4 b-tag",
-            "m_jj",
-            "m_jjj",
-            "m_lv",
-            "m_jlv",
-            "m_bb",
-            "m_wbb",
-            "m_wwbb",
-        ]
-
-        return stream.iter_csv(
-            self.path,
-            fieldnames=["is_signal", *features],
-            target="is_signal",
-            converters={
-                "is_signal": lambda x: x.startswith("1"),
-                **{f: float for f in features},
-            },
-        )
+from river import stream
+
+from . import base
+
+
+class Higgs(base.RemoteDataset):
+    """Higgs dataset.
+
+    The data has been produced using Monte Carlo simulations. The first 21 features (columns 2-22)
+    are kinematic properties measured by the particle detectors in the accelerator. The last seven
+    features are functions of the first 21 features; these are high-level features derived by
+    physicists to help discriminate between the two classes.
+
+    References
+    ----------
+    [^1]: [UCI page](https://archive.ics.uci.edu/ml/datasets/HIGGS)
+
+    """
+
+    def __init__(self):
+        super().__init__(
+            n_samples=11_000_000,
+            n_features=28,
+            task=base.BINARY_CLF,
+            url="https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz",
+            size=2_816_407_858,
+            unpack=False,
+        )
+
+    def _iter(self):
+
+        features = [
+            "lepton pT",
+            "lepton eta",
+            "lepton phi",
+            "missing energy magnitude",
+            "missing energy phi",
+            "jet 1 pt",
+            "jet 1 eta",
+            "jet 1 phi",
+            "jet 1 b-tag",
+            "jet 2 pt",
+            "jet 2 eta",
+            "jet 2 phi",
+            "jet 2 b-tag",
+            "jet 3 pt",
+            "jet 3 eta",
+            "jet 3 phi",
+            "jet 3 b-tag",
+            "jet 4 pt",
+            "jet 4 eta",
+            "jet 4 phi",
+            "jet 4 b-tag",
+            "m_jj",
+            "m_jjj",
+            "m_lv",
+            "m_jlv",
+            "m_bb",
+            "m_wbb",
+            "m_wwbb",
+        ]
+
+        return stream.iter_csv(
+            self.path,
+            fieldnames=["is_signal", *features],
+            target="is_signal",
+            converters={
+                "is_signal": lambda x: x.startswith("1"),
+                **{f: float for f in features},
+            },
+        )
```

### Comparing `river-0.8.0/river/datasets/insects.py` & `river-0.9.0/river/datasets/insects.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,79 +1,79 @@
-from river import stream
-
-from . import base
-
-
-class Insects(base.RemoteDataset):
-    """Insects dataset.
-
-    This dataset has different variants, which are:
-
-    - abrupt_balanced
-    - abrupt_imbalanced
-    - gradual_balanced
-    - gradual_imbalanced
-    - incremental-abrupt_balanced
-    - incremental-abrupt_imbalanced
-    - incremental-reoccurring_balanced
-    - incremental-reoccurring_imbalanced
-    - incremental_balanced
-    - incremental_imbalanced
-    - out-of-control
-
-    The number of samples and the difficulty change from one variant to another. The number of
-    classes is always the same (6), except for the last variant (24).
-
-    Parameters
-    ----------
-    variant
-        Indicates which variant of the dataset to load.
-
-    References
-    ----------
-    [^1]: [USP DS repository](https://sites.google.com/view/uspdsrepository)
-    [^2]: [Souza, V., Reis, D.M.D., Maletzke, A.G. and Batista, G.E., 2020. Challenges in Benchmarking Stream Learning Algorithms with Real-world Data. arXiv preprint arXiv:2005.00113.](https://arxiv.org/abs/2005.00113)
-
-    """
-
-    variant_sizes = {
-        "abrupt_balanced": (52848, 16419025),
-        "abrupt_imbalanced": (355275, 110043637),
-        "gradual_balanced": (24150, 7503750),
-        "gradual_imbalanced": (143323, 44371501),
-        "incremental-abrupt_balanced": (79986, 24849436),
-        "incremental-abrupt_imbalanced": (452044, 140004225),
-        "incremental-reoccurring_balanced": (79986, 24849092),
-        "incremental-reoccurring_imbalanced": (452044, 140004230),
-        "incremental_balanced": (57018, 17713574),
-        "incremental_imbalanced": (452044, 140004218),
-        "out-of-control": (905145, 277777854),
-    }
-
-    variants = list(variant_sizes.keys())
-
-    def __init__(self, variant="abrupt_balanced"):
-
-        try:
-            n_samples, size = self.variant_sizes[variant]
-        except KeyError:
-            variants = "\n".join(f"- {v}" for v in self.variant_sizes)
-            raise ValueError(f"Unknown variant, possible choices are:\n{variants}")
-        n_classes = 24 if variant == "out-of-control" else 6
-
-        super().__init__(
-            n_classes=n_classes,
-            n_samples=n_samples,
-            n_features=33,
-            task=base.MULTI_CLF,
-            url=f"https://sites.labic.icmc.usp.br/vsouza/repository/river/INSECTS-{variant}_norm.arff",
-            size=size,
-            unpack=False,
-        )
-        self.variant = variant
-
-    def _iter(self):
-        return stream.iter_arff(self.path, target="class")
-
-    @property
-    def _repr_content(self):
-        return {**super()._repr_content, "Variant": self.variant}
+from river import stream
+
+from . import base
+
+
+class Insects(base.RemoteDataset):
+    """Insects dataset.
+
+    This dataset has different variants, which are:
+
+    - abrupt_balanced
+    - abrupt_imbalanced
+    - gradual_balanced
+    - gradual_imbalanced
+    - incremental-abrupt_balanced
+    - incremental-abrupt_imbalanced
+    - incremental-reoccurring_balanced
+    - incremental-reoccurring_imbalanced
+    - incremental_balanced
+    - incremental_imbalanced
+    - out-of-control
+
+    The number of samples and the difficulty change from one variant to another. The number of
+    classes is always the same (6), except for the last variant (24).
+
+    Parameters
+    ----------
+    variant
+        Indicates which variant of the dataset to load.
+
+    References
+    ----------
+    [^1]: [USP DS repository](https://sites.google.com/view/uspdsrepository)
+    [^2]: [Souza, V., Reis, D.M.D., Maletzke, A.G. and Batista, G.E., 2020. Challenges in Benchmarking Stream Learning Algorithms with Real-world Data. arXiv preprint arXiv:2005.00113.](https://arxiv.org/abs/2005.00113)
+
+    """
+
+    variant_sizes = {
+        "abrupt_balanced": (52_848, 16_419_025),
+        "abrupt_imbalanced": (355_275, 110_043_637),
+        "gradual_balanced": (24_150, 7_503_750),
+        "gradual_imbalanced": (143_323, 44_371_501),
+        "incremental-abrupt_balanced": (79_986, 24_849_436),
+        "incremental-abrupt_imbalanced": (452_044, 140_004_225),
+        "incremental-reoccurring_balanced": (79_986, 24_849_092),
+        "incremental-reoccurring_imbalanced": (452_044, 140_004_230),
+        "incremental_balanced": (57_018, 17_713_574),
+        "incremental_imbalanced": (452_044, 140_004_218),
+        "out-of-control": (905_145, 277_777_854),
+    }
+
+    variants = list(variant_sizes.keys())
+
+    def __init__(self, variant="abrupt_balanced"):
+
+        try:
+            n_samples, size = self.variant_sizes[variant]
+        except KeyError:
+            variants = "\n".join(f"- {v}" for v in self.variant_sizes)
+            raise ValueError(f"Unknown variant, possible choices are:\n{variants}")
+        n_classes = 24 if variant == "out-of-control" else 6
+
+        super().__init__(
+            n_classes=n_classes,
+            n_samples=n_samples,
+            n_features=33,
+            task=base.MULTI_CLF,
+            url=f"http://sites.labic.icmc.usp.br/vsouza/repository/creme/INSECTS-{variant}_norm.arff",
+            size=size,
+            unpack=False,
+        )
+        self.variant = variant
+
+    def _iter(self):
+        return stream.iter_arff(self.path, target="class")
+
+    @property
+    def _repr_content(self):
+        return {**super()._repr_content, "Variant": self.variant}
```

### Comparing `river-0.8.0/river/datasets/movielens100k.py` & `river-0.9.0/river/datasets/movielens100k.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,42 +1,42 @@
-from river import stream
-
-from . import base
-
-
-class MovieLens100K(base.RemoteDataset):
-    """MovieLens 100K dataset.
-
-    MovieLens datasets were collected by the GroupLens Research Project at the University of
-    Minnesota. This dataset consists of 100,000 ratings (1-5) from 943 users on 1682 movies. Each
-    user has rated at least 20 movies. User and movie information are provided. The data was
-    collected through the MovieLens web site (movielens.umn.edu) during the seven-month period from
-    September 19th, 1997 through April 22nd, 1998.
-
-    References
-    ----------
-    [^1]: [The MovieLens Datasets: History and Context](http://dx.doi.org/10.1145/2827872)
-
-    """
-
-    def __init__(self):
-        super().__init__(
-            n_samples=100_000,
-            n_features=10,
-            task=base.REG,
-            url="https://maxhalford.github.io/files/datasets/ml_100k.zip",
-            size=11057876,
-            filename="ml_100k.csv",
-        )
-
-    def _iter(self):
-        return stream.iter_csv(
-            self.path,
-            target="rating",
-            converters={
-                "timestamp": int,
-                "release_date": int,
-                "age": float,
-                "rating": float,
-            },
-            delimiter="\t",
-        )
+from river import stream
+
+from . import base
+
+
+class MovieLens100K(base.RemoteDataset):
+    """MovieLens 100K dataset.
+
+    MovieLens datasets were collected by the GroupLens Research Project at the University of
+    Minnesota. This dataset consists of 100,000 ratings (1-5) from 943 users on 1682 movies. Each
+    user has rated at least 20 movies. User and movie information are provided. The data was
+    collected through the MovieLens web site (movielens.umn.edu) during the seven-month period from
+    September 19th, 1997 through April 22nd, 1998.
+
+    References
+    ----------
+    [^1]: [The MovieLens Datasets: History and Context](http://dx.doi.org/10.1145/2827872)
+
+    """
+
+    def __init__(self):
+        super().__init__(
+            n_samples=100_000,
+            n_features=10,
+            task=base.REG,
+            url="https://maxhalford.github.io/files/datasets/ml_100k.zip",
+            size=11_057_876,
+            filename="ml_100k.csv",
+        )
+
+    def _iter(self):
+        return stream.iter_csv(
+            self.path,
+            target="rating",
+            converters={
+                "timestamp": int,
+                "release_date": int,
+                "age": float,
+                "rating": float,
+            },
+            delimiter="\t",
+        )
```

### Comparing `river-0.8.0/river/datasets/phishing.csv.gz` & `river-0.9.0/river/datasets/phishing.csv.gz`

 * *Files identical despite different names*

### Comparing `river-0.8.0/river/datasets/restaurants.py` & `river-0.9.0/river/datasets/restaurants.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,41 +1,41 @@
-import ast
-
-from river import stream
-
-from . import base
-
-
-class Restaurants(base.RemoteDataset):
-    """Data from the Kaggle Recruit Restaurants challenge.
-
-    The goal is to predict the number of visitors in each of 829 Japanese restaurants over a priod
-    of roughly 16 weeks. The data is ordered by date and then by restaurant ID.
-
-    References
-    ----------
-    [^1]: [Recruit Restaurant Visitor Forecasting](https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting)
-
-    """
-
-    def __init__(self):
-        super().__init__(
-            n_samples=252_108,
-            n_features=7,
-            task=base.REG,
-            url="https://maxhalford.github.io/files/datasets/kaggle_recruit_restaurants.zip",
-            size=28881242,
-            filename="kaggle_recruit_restaurants.csv",
-        )
-
-    def _iter(self):
-        return stream.iter_csv(
-            self.path,
-            target="visitors",
-            converters={
-                "latitude": float,
-                "longitude": float,
-                "visitors": int,
-                "is_holiday": ast.literal_eval,
-            },
-            parse_dates={"date": "%Y-%m-%d"},
-        )
+import ast
+
+from river import stream
+
+from . import base
+
+
+class Restaurants(base.RemoteDataset):
+    """Data from the Kaggle Recruit Restaurants challenge.
+
+    The goal is to predict the number of visitors in each of 829 Japanese restaurants over a priod
+    of roughly 16 weeks. The data is ordered by date and then by restaurant ID.
+
+    References
+    ----------
+    [^1]: [Recruit Restaurant Visitor Forecasting](https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting)
+
+    """
+
+    def __init__(self):
+        super().__init__(
+            n_samples=252_108,
+            n_features=7,
+            task=base.REG,
+            url="https://maxhalford.github.io/files/datasets/kaggle_recruit_restaurants.zip",
+            size=28_881_242,
+            filename="kaggle_recruit_restaurants.csv",
+        )
+
+    def _iter(self):
+        return stream.iter_csv(
+            self.path,
+            target="visitors",
+            converters={
+                "latitude": float,
+                "longitude": float,
+                "visitors": int,
+                "is_holiday": ast.literal_eval,
+            },
+            parse_dates={"date": "%Y-%m-%d"},
+        )
```

### Comparing `river-0.8.0/river/datasets/segment.csv.zip` & `river-0.9.0/river/datasets/segment.csv.zip`

 * *Files identical despite different names*

### Comparing `river-0.8.0/river/datasets/sms_spam.py` & `river-0.9.0/river/datasets/sms_spam.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,30 +1,30 @@
-from . import base
-
-
-class SMSSpam(base.RemoteDataset):
-    """SMS Spam Collection dataset.
-
-    The data contains 5,574 items and 1 feature (i.e. SMS body). Spam messages represent
-    13.4% of the dataset. The goal is to predict whether an SMS is a spam or not.
-
-    References
-    ----------
-    [^1]: [Almeida, T.A., Hidalgo, J.M.G. and Yamakami, A., 2011, September. Contributions to the study of SMS spam filtering: new collection and results. In Proceedings of the 11th ACM symposium on Document engineering (pp. 259-262).](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/doceng11.pdf)
-
-    """
-
-    def __init__(self):
-        super().__init__(
-            n_samples=5_574,
-            n_features=1,
-            task=base.BINARY_CLF,
-            url="https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip",
-            size=477907,
-            filename="SMSSpamCollection",
-        )
-
-    def _iter(self):
-        with open(self.path) as f:
-            for row in f:
-                label, body = row.split("\t")
-                yield ({"body": body}, label == "spam")
+from . import base
+
+
+class SMSSpam(base.RemoteDataset):
+    """SMS Spam Collection dataset.
+
+    The data contains 5,574 items and 1 feature (i.e. SMS body). Spam messages represent
+    13.4% of the dataset. The goal is to predict whether an SMS is a spam or not.
+
+    References
+    ----------
+    [^1]: [Almeida, T.A., Hidalgo, J.M.G. and Yamakami, A., 2011, September. Contributions to the study of SMS spam filtering: new collection and results. In Proceedings of the 11th ACM symposium on Document engineering (pp. 259-262).](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/doceng11.pdf)
+
+    """
+
+    def __init__(self):
+        super().__init__(
+            n_samples=5_574,
+            n_features=1,
+            task=base.BINARY_CLF,
+            url="https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip",
+            size=477_907,
+            filename="SMSSpamCollection",
+        )
+
+    def _iter(self):
+        with open(self.path) as f:
+            for row in f:
+                label, body = row.split("\t")
+                yield ({"body": body}, label == "spam")
```

### Comparing `river-0.8.0/river/datasets/solar-flare.csv.zip` & `river-0.9.0/river/datasets/solar-flare.csv.zip`

 * *Files identical despite different names*

### Comparing `river-0.8.0/river/datasets/synth/agrawal.py` & `river-0.9.0/river/datasets/synth/agrawal.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,339 +1,339 @@
-import numpy as np
-
-from river.utils.skmultiflow_utils import check_random_state
-
-from .. import base
-
-
-class Agrawal(base.SyntheticDataset):
-    r"""Agrawal stream generator.
-
-    The generator was introduced by Agrawal et al. [^1], and was a common
-    source of data for early work on scaling up decision tree learners.
-    The generator produces a stream containing nine features, six numeric and
-    three categorical.
-    There are 10 functions defined for generating binary class labels from the
-    features. Presumably these determine whether the loan should be approved.
-    Classification functions are listed in the original paper [^1].
-
-    **Feature** | **Description** | **Values**
-
-    * `salary` | salary | uniformly distributed from 20k to 150k
-
-    * `commission` | commission | 0 if `salary` < 75k else uniformly distributed from 10k to 75k
-
-    * `age` | age | uniformly distributed from 20 to 80
-
-    * `elevel` | education level | uniformly chosen from 0 to 4
-
-    * `car` | car maker | uniformly chosen from 1 to 20
-
-    * `zipcode` | zip code of the town | uniformly chosen from 0 to 8
-
-    * `hvalue` | house value | uniformly distributed from 50k x zipcode to 100k x zipcode
-
-    * `hyears` | years house owned | uniformly distributed from 1 to 30
-
-    * `loan` | total loan amount | uniformly distributed from 0 to 500k
-
-    Parameters
-    ----------
-    classification_function
-        The classification function to use for the generation.
-        Valid values are from 0 to 9.
-    seed
-        If int, `seed` is used to seed the random number generator;
-        If RandomState instance, `seed` is the random number generator;
-        If None, the random number generator is the `RandomState` instance used
-        by `np.random`.
-    balance_classes
-        If True, the class distribution will converge to a uniform distribution.
-    perturbation
-        The probability that noise will happen in the generation. Each new
-        sample will be perturbed by the magnitude of `perturbation`.
-        Valid values are in the range [0.0 to 1.0].
-
-    Examples
-    --------
-
-    >>> from river import synth
-
-    >>> dataset = synth.Agrawal(
-    ...     classification_function=0,
-    ...     seed=42
-    ... )
-
-    >>> dataset
-    Synthetic data generator
-    <BLANKLINE>
-        Name  Agrawal
-        Task  Binary classification
-     Samples  ∞
-    Features  9
-     Outputs  1
-     Classes  2
-      Sparse  False
-    <BLANKLINE>
-    Configuration
-    -------------
-    classification_function  0
-                       seed  42
-            balance_classes  False
-               perturbation  0.0
-
-    >>> for x, y in dataset.take(5):
-    ...     print(list(x.values()), y)
-    [68690.2154, 81303.5729, 62, 4, 6, 2, 419982.4410, 11, 433088.0728] 1
-    [98144.9515, 0, 43, 2, 1, 7, 266488.5281, 6, 389.3829] 0
-    [148987.502, 0, 52, 3, 11, 8, 79122.9140, 27, 199930.4858] 0
-    [26066.5362, 83031.6639, 34, 2, 11, 6, 444969.2657, 25, 23225.2063] 1
-    [98980.8307, 0, 40, 0, 6, 1, 1159108.4298, 28, 281644.1089] 0
-
-    Notes
-    -----
-    The sample generation works as follows: The 9 features are generated
-    with the random generator, initialized with the seed passed by the
-    user. Then, the classification function decides, as a function of all
-    the attributes, whether to classify the instance as class 0 or class
-    1. The next step is to verify if the classes should be balanced, and
-    if so, balance the classes. Finally, add noise if `perturbation` > 0.0.
-
-    References
-    ----------
-    [^1]: Rakesh Agrawal, Tomasz Imielinksi, and Arun Swami. "Database Mining:
-          A Performance Perspective", IEEE Transactions on Knowledge and
-          Data Engineering, 5(6), December 1993.
-
-    """
-
-    def __init__(
-        self,
-        classification_function: int = 0,
-        seed: int or np.random.RandomState = None,
-        balance_classes: bool = False,
-        perturbation: float = 0.0,
-    ):
-        super().__init__(n_features=9, n_classes=2, n_outputs=1, task=base.BINARY_CLF)
-
-        # Classification functions to use
-        self._classification_functions = [
-            self._classification_function_0,
-            self._classification_function_1,
-            self._classification_function_2,
-            self._classification_function_3,
-            self._classification_function_4,
-            self._classification_function_5,
-            self._classification_function_6,
-            self._classification_function_7,
-            self._classification_function_8,
-            self._classification_function_9,
-        ]
-        if classification_function not in range(10):
-            raise ValueError(
-                f"classification_function takes values from 0 to 9 "
-                f"and {classification_function} was passed"
-            )
-        self.classification_function = classification_function
-        self.balance_classes = balance_classes
-        if not 0.0 <= perturbation <= 1.0:
-            raise ValueError(
-                f"noise percentage should be in [0.0..1.0] "
-                f"and {perturbation} was passed"
-            )
-        self.perturbation = perturbation
-        self.seed = seed
-        self.n_num_features = 6
-        self.n_cat_features = 3
-        self._next_class_should_be_zero = False
-        self.feature_names = [
-            "salary",
-            "commission",
-            "age",
-            "elevel",
-            "car",
-            "zipcode",
-            "hvalue",
-            "hyears",
-            "loan",
-        ]
-        self.target_values = [i for i in range(self.n_classes)]
-
-    def __iter__(self):
-        self._rng = check_random_state(self.seed)
-        self._next_class_should_be_zero = False
-
-        while True:
-            y = 0
-            desired_class_found = False
-            while not desired_class_found:
-                salary = 20000 + 130000 * self._rng.rand()
-                commission = (
-                    0 if (salary >= 75000) else (10000 + 75000 * self._rng.rand())
-                )
-                age = 20 + self._rng.randint(61)
-                elevel = self._rng.randint(5)
-                car = self._rng.randint(20)
-                zipcode = self._rng.randint(9)
-                hvalue = (9 - zipcode) * 100000 * (0.5 + self._rng.rand())
-                hyears = 1 + self._rng.randint(30)
-                loan = self._rng.rand() * 500000
-                y = self._classification_functions[self.classification_function](
-                    salary, commission, age, elevel, car, zipcode, hvalue, hyears, loan
-                )
-                if not self.balance_classes:
-                    desired_class_found = True
-                else:
-                    if (self._next_class_should_be_zero and (y == 0)) or (
-                        (not self._next_class_should_be_zero) and (y == 1)
-                    ):
-                        desired_class_found = True
-                        self._next_class_should_be_zero = (
-                            not self._next_class_should_be_zero
-                        )
-
-            if self.perturbation > 0.0:
-                salary = self._perturb_value(salary, 20000, 150000)
-                if commission > 0:
-                    commission = self._perturb_value(commission, 10000, 75000)
-                age = np.round(self._perturb_value(age, 20, 80))
-                hvalue = self._perturb_value(hvalue, (9 - zipcode) * 100000, 0, 135000)
-                hyears = np.round(self._perturb_value(hyears, 1, 30))
-                loan = self._perturb_value(loan, 0, 500000)
-
-            x = dict()
-            for feature in self.feature_names:
-                x[feature] = eval(feature)
-
-            yield x, y
-
-    def _perturb_value(self, val, val_min, val_max, val_range=None):
-        if val_range is None:
-            val_range = val_max - val_min
-        val += val_range * (2 * (self._rng.rand() - 0.5)) * self.perturbation
-        if val < val_min:
-            val = val_min
-        elif val > val_max:
-            val = val_max
-        return val
-
-    def generate_drift(self):
-        """
-        Generate drift by switching the classification function randomly.
-
-        """
-        new_function = self._rng.randint(10)
-        while new_function == self.classification_function:
-            new_function = self._rng.randint(10)
-        self.classification_function = new_function
-
-    @staticmethod
-    def _classification_function_0(
-        salary, commission, age, elevel, car, zipcode, hvalue, hyears, loan
-    ):
-        return int((age < 40) or (60 <= age))
-
-    @staticmethod
-    def _classification_function_1(
-        salary, commission, age, elevel, car, zipcode, hvalue, hyears, loan
-    ):
-        if age < 40:
-            return int((50000 <= salary) and (salary <= 100000))
-        elif age < 60:
-            return int((75000 <= salary) and (salary <= 125000))
-        else:
-            return int((25000 <= salary) and (salary <= 75000))
-
-    @staticmethod
-    def _classification_function_2(
-        salary, commission, age, elevel, car, zipcode, hvalue, hyears, loan
-    ):
-        if age < 40:
-            return int((elevel == 0) or (elevel == 1))
-        elif age < 60:
-            return int((elevel == 1) or (elevel == 2) or (elevel == 3))
-        else:
-            return int((elevel == 2) or (elevel == 3) or (elevel == 4))
-
-    @staticmethod
-    def _classification_function_3(
-        salary, commission, age, elevel, car, zipcode, hvalue, hyears, loan
-    ):
-        if age < 40:
-            if (elevel == 0) or (elevel == 1):
-                return int((25000 <= salary) and (salary <= 75000))
-            else:
-                return int((50000 <= salary) and (salary <= 100000))
-        elif age < 60:
-            if (elevel == 1) or (elevel == 2) or (elevel == 3):
-                return int((50000 <= salary) and (salary <= 100000))
-            else:
-                return int((75000 <= salary) and (salary <= 125000))
-        else:
-            if (elevel == 2) or (elevel == 3) or (elevel == 4):
-                return int((50000 <= salary) and (salary <= 100000))
-            else:
-                return int((25000 <= salary) and (salary <= 75000))
-
-    @staticmethod
-    def _classification_function_4(
-        salary, commission, age, elevel, car, zipcode, hvalue, hyears, loan
-    ):
-        if age < 40:
-            if (50000 <= salary) and (salary <= 100000):
-                return int((100000 <= loan) and (loan <= 300000))
-            else:
-                return int((200000 <= salary) and (salary <= 400000))
-        elif age < 60:
-            if (75000 <= salary) and (salary <= 125000):
-                return int((200000 <= salary) and (loan <= 400000))
-            else:
-                return int((300000 <= salary) and (salary <= 500000))
-        else:
-            if (25000 <= salary) and (salary <= 75000):
-                return int((300000 <= loan) and (loan <= 500000))
-            else:
-                return int((75000 <= loan) and (loan <= 300000))
-
-    @staticmethod
-    def _classification_function_5(
-        salary, commission, age, elevel, car, zipcode, hvalue, hyears, loan
-    ):
-        totalsalary = salary + commission
-
-        if age < 40:
-            return int((50000 <= totalsalary) and (totalsalary <= 100000))
-        elif age < 60:
-            return int((75000 <= totalsalary) and (totalsalary <= 125000))
-        else:
-            return int((25000 <= totalsalary) and (totalsalary <= 75000))
-
-    @staticmethod
-    def _classification_function_6(
-        salary, commission, age, elevel, car, zipcode, hvalue, hyears, loan
-    ):
-        disposable = 2 * (salary + commission) / 3 - loan / 5 - 20000
-        return 0 if disposable > 1 else 1
-
-    @staticmethod
-    def _classification_function_7(
-        salary, commission, age, elevel, car, zipcode, hvalue, hyears, loan
-    ):
-        disposable = 2 * (salary + commission) / 3 - 5000 * elevel - 20000
-        return 0 if disposable > 1 else 1
-
-    @staticmethod
-    def _classification_function_8(
-        salary, commission, age, elevel, car, zipcode, hvalue, hyears, loan
-    ):
-        disposable = 2 * (salary + commission) / 3 - 5000 * elevel - loan / 5 - 10000
-        return 0 if disposable > 1 else 1
-
-    @staticmethod
-    def _classification_function_9(
-        salary, commission, age, elevel, car, zipcode, hvalue, hyears, loan
-    ):
-        equity = 0
-        if hyears >= 20:
-            equity = hvalue * (hyears - 20) / 10
-        disposable = 2 * (salary + commission) / 3 - 5000 * elevel + equity / 5 - 10000
-        return 0 if disposable > 1 else 1
+import numpy as np
+
+from river.utils.skmultiflow_utils import check_random_state
+
+from .. import base
+
+
+class Agrawal(base.SyntheticDataset):
+    r"""Agrawal stream generator.
+
+    The generator was introduced by Agrawal et al. [^1], and was a common
+    source of data for early work on scaling up decision tree learners.
+    The generator produces a stream containing nine features, six numeric and
+    three categorical.
+    There are 10 functions defined for generating binary class labels from the
+    features. Presumably these determine whether the loan should be approved.
+    Classification functions are listed in the original paper [^1].
+
+    **Feature** | **Description** | **Values**
+
+    * `salary` | salary | uniformly distributed from 20k to 150k
+
+    * `commission` | commission | 0 if `salary` < 75k else uniformly distributed from 10k to 75k
+
+    * `age` | age | uniformly distributed from 20 to 80
+
+    * `elevel` | education level | uniformly chosen from 0 to 4
+
+    * `car` | car maker | uniformly chosen from 1 to 20
+
+    * `zipcode` | zip code of the town | uniformly chosen from 0 to 8
+
+    * `hvalue` | house value | uniformly distributed from 50k x zipcode to 100k x zipcode
+
+    * `hyears` | years house owned | uniformly distributed from 1 to 30
+
+    * `loan` | total loan amount | uniformly distributed from 0 to 500k
+
+    Parameters
+    ----------
+    classification_function
+        The classification function to use for the generation.
+        Valid values are from 0 to 9.
+    seed
+        If int, `seed` is used to seed the random number generator;
+        If RandomState instance, `seed` is the random number generator;
+        If None, the random number generator is the `RandomState` instance used
+        by `np.random`.
+    balance_classes
+        If True, the class distribution will converge to a uniform distribution.
+    perturbation
+        The probability that noise will happen in the generation. Each new
+        sample will be perturbed by the magnitude of `perturbation`.
+        Valid values are in the range [0.0 to 1.0].
+
+    Examples
+    --------
+
+    >>> from river import synth
+
+    >>> dataset = synth.Agrawal(
+    ...     classification_function=0,
+    ...     seed=42
+    ... )
+
+    >>> dataset
+    Synthetic data generator
+    <BLANKLINE>
+        Name  Agrawal
+        Task  Binary classification
+     Samples  ∞
+    Features  9
+     Outputs  1
+     Classes  2
+      Sparse  False
+    <BLANKLINE>
+    Configuration
+    -------------
+    classification_function  0
+                       seed  42
+            balance_classes  False
+               perturbation  0.0
+
+    >>> for x, y in dataset.take(5):
+    ...     print(list(x.values()), y)
+    [68690.2154, 81303.5729, 62, 4, 6, 2, 419982.4410, 11, 433088.0728] 1
+    [98144.9515, 0, 43, 2, 1, 7, 266488.5281, 6, 389.3829] 0
+    [148987.502, 0, 52, 3, 11, 8, 79122.9140, 27, 199930.4858] 0
+    [26066.5362, 83031.6639, 34, 2, 11, 6, 444969.2657, 25, 23225.2063] 1
+    [98980.8307, 0, 40, 0, 6, 1, 1159108.4298, 28, 281644.1089] 0
+
+    Notes
+    -----
+    The sample generation works as follows: The 9 features are generated
+    with the random generator, initialized with the seed passed by the
+    user. Then, the classification function decides, as a function of all
+    the attributes, whether to classify the instance as class 0 or class
+    1. The next step is to verify if the classes should be balanced, and
+    if so, balance the classes. Finally, add noise if `perturbation` > 0.0.
+
+    References
+    ----------
+    [^1]: Rakesh Agrawal, Tomasz Imielinksi, and Arun Swami. "Database Mining:
+          A Performance Perspective", IEEE Transactions on Knowledge and
+          Data Engineering, 5(6), December 1993.
+
+    """
+
+    def __init__(
+        self,
+        classification_function: int = 0,
+        seed: int or np.random.RandomState = None,
+        balance_classes: bool = False,
+        perturbation: float = 0.0,
+    ):
+        super().__init__(n_features=9, n_classes=2, n_outputs=1, task=base.BINARY_CLF)
+
+        # Classification functions to use
+        self._classification_functions = [
+            self._classification_function_0,
+            self._classification_function_1,
+            self._classification_function_2,
+            self._classification_function_3,
+            self._classification_function_4,
+            self._classification_function_5,
+            self._classification_function_6,
+            self._classification_function_7,
+            self._classification_function_8,
+            self._classification_function_9,
+        ]
+        if classification_function not in range(10):
+            raise ValueError(
+                f"classification_function takes values from 0 to 9 "
+                f"and {classification_function} was passed"
+            )
+        self.classification_function = classification_function
+        self.balance_classes = balance_classes
+        if not 0.0 <= perturbation <= 1.0:
+            raise ValueError(
+                f"noise percentage should be in [0.0..1.0] "
+                f"and {perturbation} was passed"
+            )
+        self.perturbation = perturbation
+        self.seed = seed
+        self.n_num_features = 6
+        self.n_cat_features = 3
+        self._next_class_should_be_zero = False
+        self.feature_names = [
+            "salary",
+            "commission",
+            "age",
+            "elevel",
+            "car",
+            "zipcode",
+            "hvalue",
+            "hyears",
+            "loan",
+        ]
+        self.target_values = [i for i in range(self.n_classes)]
+
+    def __iter__(self):
+        self._rng = check_random_state(self.seed)
+        self._next_class_should_be_zero = False
+
+        while True:
+            y = 0
+            desired_class_found = False
+            while not desired_class_found:
+                salary = 20000 + 130000 * self._rng.rand()
+                commission = (
+                    0 if (salary >= 75000) else (10000 + 75000 * self._rng.rand())
+                )
+                age = 20 + self._rng.randint(61)
+                elevel = self._rng.randint(5)
+                car = self._rng.randint(20)
+                zipcode = self._rng.randint(9)
+                hvalue = (9 - zipcode) * 100000 * (0.5 + self._rng.rand())
+                hyears = 1 + self._rng.randint(30)
+                loan = self._rng.rand() * 500000
+                y = self._classification_functions[self.classification_function](
+                    salary, commission, age, elevel, car, zipcode, hvalue, hyears, loan
+                )
+                if not self.balance_classes:
+                    desired_class_found = True
+                else:
+                    if (self._next_class_should_be_zero and (y == 0)) or (
+                        (not self._next_class_should_be_zero) and (y == 1)
+                    ):
+                        desired_class_found = True
+                        self._next_class_should_be_zero = (
+                            not self._next_class_should_be_zero
+                        )
+
+            if self.perturbation > 0.0:
+                salary = self._perturb_value(salary, 20000, 150000)
+                if commission > 0:
+                    commission = self._perturb_value(commission, 10000, 75000)
+                age = np.round(self._perturb_value(age, 20, 80))
+                hvalue = self._perturb_value(hvalue, (9 - zipcode) * 100000, 0, 135000)
+                hyears = np.round(self._perturb_value(hyears, 1, 30))
+                loan = self._perturb_value(loan, 0, 500000)
+
+            x = dict()
+            for feature in self.feature_names:
+                x[feature] = eval(feature)
+
+            yield x, y
+
+    def _perturb_value(self, val, val_min, val_max, val_range=None):
+        if val_range is None:
+            val_range = val_max - val_min
+        val += val_range * (2 * (self._rng.rand() - 0.5)) * self.perturbation
+        if val < val_min:
+            val = val_min
+        elif val > val_max:
+            val = val_max
+        return val
+
+    def generate_drift(self):
+        """
+        Generate drift by switching the classification function randomly.
+
+        """
+        new_function = self._rng.randint(10)
+        while new_function == self.classification_function:
+            new_function = self._rng.randint(10)
+        self.classification_function = new_function
+
+    @staticmethod
+    def _classification_function_0(
+        salary, commission, age, elevel, car, zipcode, hvalue, hyears, loan
+    ):
+        return int((age < 40) or (60 <= age))
+
+    @staticmethod
+    def _classification_function_1(
+        salary, commission, age, elevel, car, zipcode, hvalue, hyears, loan
+    ):
+        if age < 40:
+            return int((50000 <= salary) and (salary <= 100000))
+        elif age < 60:
+            return int((75000 <= salary) and (salary <= 125000))
+        else:
+            return int((25000 <= salary) and (salary <= 75000))
+
+    @staticmethod
+    def _classification_function_2(
+        salary, commission, age, elevel, car, zipcode, hvalue, hyears, loan
+    ):
+        if age < 40:
+            return int((elevel == 0) or (elevel == 1))
+        elif age < 60:
+            return int((elevel == 1) or (elevel == 2) or (elevel == 3))
+        else:
+            return int((elevel == 2) or (elevel == 3) or (elevel == 4))
+
+    @staticmethod
+    def _classification_function_3(
+        salary, commission, age, elevel, car, zipcode, hvalue, hyears, loan
+    ):
+        if age < 40:
+            if (elevel == 0) or (elevel == 1):
+                return int((25000 <= salary) and (salary <= 75000))
+            else:
+                return int((50000 <= salary) and (salary <= 100000))
+        elif age < 60:
+            if (elevel == 1) or (elevel == 2) or (elevel == 3):
+                return int((50000 <= salary) and (salary <= 100000))
+            else:
+                return int((75000 <= salary) and (salary <= 125000))
+        else:
+            if (elevel == 2) or (elevel == 3) or (elevel == 4):
+                return int((50000 <= salary) and (salary <= 100000))
+            else:
+                return int((25000 <= salary) and (salary <= 75000))
+
+    @staticmethod
+    def _classification_function_4(
+        salary, commission, age, elevel, car, zipcode, hvalue, hyears, loan
+    ):
+        if age < 40:
+            if (50000 <= salary) and (salary <= 100000):
+                return int((100000 <= loan) and (loan <= 300000))
+            else:
+                return int((200000 <= salary) and (salary <= 400000))
+        elif age < 60:
+            if (75000 <= salary) and (salary <= 125000):
+                return int((200000 <= salary) and (loan <= 400000))
+            else:
+                return int((300000 <= salary) and (salary <= 500000))
+        else:
+            if (25000 <= salary) and (salary <= 75000):
+                return int((300000 <= loan) and (loan <= 500000))
+            else:
+                return int((75000 <= loan) and (loan <= 300000))
+
+    @staticmethod
+    def _classification_function_5(
+        salary, commission, age, elevel, car, zipcode, hvalue, hyears, loan
+    ):
+        totalsalary = salary + commission
+
+        if age < 40:
+            return int((50000 <= totalsalary) and (totalsalary <= 100000))
+        elif age < 60:
+            return int((75000 <= totalsalary) and (totalsalary <= 125000))
+        else:
+            return int((25000 <= totalsalary) and (totalsalary <= 75000))
+
+    @staticmethod
+    def _classification_function_6(
+        salary, commission, age, elevel, car, zipcode, hvalue, hyears, loan
+    ):
+        disposable = 2 * (salary + commission) / 3 - loan / 5 - 20000
+        return 0 if disposable > 1 else 1
+
+    @staticmethod
+    def _classification_function_7(
+        salary, commission, age, elevel, car, zipcode, hvalue, hyears, loan
+    ):
+        disposable = 2 * (salary + commission) / 3 - 5000 * elevel - 20000
+        return 0 if disposable > 1 else 1
+
+    @staticmethod
+    def _classification_function_8(
+        salary, commission, age, elevel, car, zipcode, hvalue, hyears, loan
+    ):
+        disposable = 2 * (salary + commission) / 3 - 5000 * elevel - loan / 5 - 10000
+        return 0 if disposable > 1 else 1
+
+    @staticmethod
+    def _classification_function_9(
+        salary, commission, age, elevel, car, zipcode, hvalue, hyears, loan
+    ):
+        equity = 0
+        if hyears >= 20:
+            equity = hvalue * (hyears - 20) / 10
+        disposable = 2 * (salary + commission) / 3 - 5000 * elevel + equity / 5 - 10000
+        return 0 if disposable > 1 else 1
```

### Comparing `river-0.8.0/river/datasets/synth/anomaly_sine.py` & `river-0.9.0/river/datasets/synth/anomaly_sine.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,146 +1,146 @@
-import itertools
-
-import numpy as np
-
-from river.utils.skmultiflow_utils import check_random_state
-
-from .. import base
-
-
-class AnomalySine(base.SyntheticDataset):
-    """Simulate a stream with anomalies in sine waves
-
-    The data generated corresponds to sine (`attribute 1`) and cosine
-    (`attribute 2`) functions. Anomalies are induced by replacing values
-    from `attribute 2` with values from a sine function different to the one
-    used in `attribute 1`. The `contextual` flag can be used to introduce
-    contextual anomalies which are values in the normal global range,
-    but abnormal compared to the seasonal pattern. Contextual attributes
-    are introduced by replacing values in `attribute 2` with values from
-    `attribute 1`.
-
-    Parameters
-    ----------
-    n_samples
-        Number of samples
-    n_anomalies
-        Number of anomalies. Can't be larger than `n_samples`.
-    contextual
-        If True, will add contextual anomalies
-    n_contextual
-        Number of contextual anomalies. Can't be larger than `n_samples`.
-    shift
-        Shift in number of samples applied when retrieving contextual anomalies
-    noise
-        Amount of noise
-    replace
-        If True, anomalies are randomly sampled with replacement
-    seed
-        If int, `seed` is used to seed the random number generator;
-        If RandomState instance, `seed` is the random number generator;
-        If None, the random number generator is the `RandomState` instance used
-        by `np.random`.
-
-    Examples
-    --------
-
-    >>> from river import synth
-
-    >>> dataset = synth.AnomalySine(seed=12345,
-    ...                             n_samples=100,
-    ...                             n_anomalies=25,
-    ...                             contextual=True,
-    ...                             n_contextual=10)
-
-    >>> for x, y in dataset.take(5):
-    ...     print(x, y)
-    {'sine': -0.1023, 'cosine': 0.2171} 0.0
-    {'sine': 0.4868, 'cosine': 0.6876} 0.0
-    {'sine': 0.2197, 'cosine': 0.8612} 0.0
-    {'sine': 0.4037, 'cosine': 0.2671} 0.0
-    {'sine': 1.8243, 'cosine': 1.8268} 1.0
-
-    """
-
-    def __init__(
-        self,
-        n_samples: int = 10000,
-        n_anomalies: int = 2500,
-        contextual: bool = False,
-        n_contextual: int = 2500,
-        shift: int = 4,
-        noise: float = 0.5,
-        replace: bool = True,
-        seed: int or np.random.RandomState = None,
-    ):
-        super().__init__(
-            n_features=2,
-            n_classes=1,
-            n_outputs=1,
-            n_samples=n_samples,
-            task=base.BINARY_CLF,
-        )
-        if n_anomalies > self.n_samples:
-            raise ValueError(
-                f"n_anomalies ({n_anomalies}) can't be larger "
-                f"than n_samples ({self.n_samples})"
-            )
-        self.n_anomalies = n_anomalies
-        self.contextual = contextual
-        if contextual and n_contextual > self.n_samples:
-            raise ValueError(
-                f"n_contextual ({n_contextual}) can't be larger "
-                f"than n_samples ({self.n_samples})"
-            )
-        self.n_contextual = n_contextual
-        self.shift = abs(shift)
-        self.noise = noise
-        self.replace = replace
-        self.seed = seed
-
-        # Stream attributes
-        self.n_num_features = 2
-
-    def _generate_data(self):
-        # Generate anomaly data arrays
-        self._random_state = check_random_state(self.seed)
-        self.y = np.zeros(self.n_samples)
-        self.X = np.column_stack(
-            [
-                np.sin(np.arange(self.n_samples) / 4.0)
-                + self._random_state.randn(self.n_samples) * self.noise,
-                np.cos(np.arange(self.n_samples) / 4.0)
-                + self._random_state.randn(self.n_samples) * self.noise,
-            ]
-        )
-
-        if self.contextual:
-            # contextual anomaly indices
-            contextual_anomalies = self._random_state.choice(
-                self.n_samples - self.shift, self.n_contextual, replace=self.replace
-            )
-            # set contextual anomalies
-            contextual_idx = contextual_anomalies + self.shift
-            contextual_idx[contextual_idx >= self.n_samples] -= self.n_samples
-            self.X[contextual_idx, 1] = self.X[contextual_anomalies, 0]
-
-        # Anomaly indices
-        anomalies_idx = self._random_state.choice(
-            self.n_samples, self.n_anomalies, replace=self.replace
-        )
-        self.X[anomalies_idx, 1] = (
-            np.sin(self._random_state.choice(self.n_anomalies, replace=self.replace))
-            + self._random_state.randn(self.n_anomalies) * self.noise
-            + 2.0
-        )
-        # Mark sample as anomalous
-        self.y[anomalies_idx] = 1
-
-    def __iter__(self):
-
-        self._generate_data()
-
-        for xi, yi in itertools.zip_longest(
-            self.X, self.y if hasattr(self.y, "__iter__") else []
-        ):
-            yield dict(zip(["sine", "cosine"], xi)), yi
+import itertools
+
+import numpy as np
+
+from river.utils.skmultiflow_utils import check_random_state
+
+from .. import base
+
+
+class AnomalySine(base.SyntheticDataset):
+    """Simulate a stream with anomalies in sine waves
+
+    The data generated corresponds to sine (`attribute 1`) and cosine
+    (`attribute 2`) functions. Anomalies are induced by replacing values
+    from `attribute 2` with values from a sine function different to the one
+    used in `attribute 1`. The `contextual` flag can be used to introduce
+    contextual anomalies which are values in the normal global range,
+    but abnormal compared to the seasonal pattern. Contextual attributes
+    are introduced by replacing values in `attribute 2` with values from
+    `attribute 1`.
+
+    Parameters
+    ----------
+    n_samples
+        Number of samples
+    n_anomalies
+        Number of anomalies. Can't be larger than `n_samples`.
+    contextual
+        If True, will add contextual anomalies
+    n_contextual
+        Number of contextual anomalies. Can't be larger than `n_samples`.
+    shift
+        Shift in number of samples applied when retrieving contextual anomalies
+    noise
+        Amount of noise
+    replace
+        If True, anomalies are randomly sampled with replacement
+    seed
+        If int, `seed` is used to seed the random number generator;
+        If RandomState instance, `seed` is the random number generator;
+        If None, the random number generator is the `RandomState` instance used
+        by `np.random`.
+
+    Examples
+    --------
+
+    >>> from river import synth
+
+    >>> dataset = synth.AnomalySine(seed=12345,
+    ...                             n_samples=100,
+    ...                             n_anomalies=25,
+    ...                             contextual=True,
+    ...                             n_contextual=10)
+
+    >>> for x, y in dataset.take(5):
+    ...     print(x, y)
+    {'sine': -0.1023, 'cosine': 0.2171} 0.0
+    {'sine': 0.4868, 'cosine': 0.6876} 0.0
+    {'sine': 0.2197, 'cosine': 0.8612} 0.0
+    {'sine': 0.4037, 'cosine': 0.2671} 0.0
+    {'sine': 1.8243, 'cosine': 1.8268} 1.0
+
+    """
+
+    def __init__(
+        self,
+        n_samples: int = 10000,
+        n_anomalies: int = 2500,
+        contextual: bool = False,
+        n_contextual: int = 2500,
+        shift: int = 4,
+        noise: float = 0.5,
+        replace: bool = True,
+        seed: int or np.random.RandomState = None,
+    ):
+        super().__init__(
+            n_features=2,
+            n_classes=1,
+            n_outputs=1,
+            n_samples=n_samples,
+            task=base.BINARY_CLF,
+        )
+        if n_anomalies > self.n_samples:
+            raise ValueError(
+                f"n_anomalies ({n_anomalies}) can't be larger "
+                f"than n_samples ({self.n_samples})"
+            )
+        self.n_anomalies = n_anomalies
+        self.contextual = contextual
+        if contextual and n_contextual > self.n_samples:
+            raise ValueError(
+                f"n_contextual ({n_contextual}) can't be larger "
+                f"than n_samples ({self.n_samples})"
+            )
+        self.n_contextual = n_contextual
+        self.shift = abs(shift)
+        self.noise = noise
+        self.replace = replace
+        self.seed = seed
+
+        # Stream attributes
+        self.n_num_features = 2
+
+    def _generate_data(self):
+        # Generate anomaly data arrays
+        self._random_state = check_random_state(self.seed)
+        self.y = np.zeros(self.n_samples)
+        self.X = np.column_stack(
+            [
+                np.sin(np.arange(self.n_samples) / 4.0)
+                + self._random_state.randn(self.n_samples) * self.noise,
+                np.cos(np.arange(self.n_samples) / 4.0)
+                + self._random_state.randn(self.n_samples) * self.noise,
+            ]
+        )
+
+        if self.contextual:
+            # contextual anomaly indices
+            contextual_anomalies = self._random_state.choice(
+                self.n_samples - self.shift, self.n_contextual, replace=self.replace
+            )
+            # set contextual anomalies
+            contextual_idx = contextual_anomalies + self.shift
+            contextual_idx[contextual_idx >= self.n_samples] -= self.n_samples
+            self.X[contextual_idx, 1] = self.X[contextual_anomalies, 0]
+
+        # Anomaly indices
+        anomalies_idx = self._random_state.choice(
+            self.n_samples, self.n_anomalies, replace=self.replace
+        )
+        self.X[anomalies_idx, 1] = (
+            np.sin(self._random_state.choice(self.n_anomalies, replace=self.replace))
+            + self._random_state.randn(self.n_anomalies) * self.noise
+            + 2.0
+        )
+        # Mark sample as anomalous
+        self.y[anomalies_idx] = 1
+
+    def __iter__(self):
+
+        self._generate_data()
+
+        for xi, yi in itertools.zip_longest(
+            self.X, self.y if hasattr(self.y, "__iter__") else []
+        ):
+            yield dict(zip(["sine", "cosine"], xi)), yi
```

### Comparing `river-0.8.0/river/datasets/synth/concept_drift_stream.py` & `river-0.9.0/river/datasets/synth/concept_drift_stream.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,184 +1,184 @@
-import textwrap
-
-import numpy as np
-
-from river.utils.skmultiflow_utils import check_random_state
-
-from .. import base
-from ..synth import Agrawal
-
-
-class ConceptDriftStream(base.SyntheticDataset):
-    """Generates a stream with concept drift.
-
-    A stream generator that adds concept drift or change by joining two
-    streams. This is done by building a weighted combination of two pure
-    distributions that characterizes the target concepts before and after
-    the change.
-
-    The sigmoid function is an elegant and practical solution to define the
-    probability that each new instance of the stream belongs to the new
-    concept after the drift. The sigmoid function introduces a gradual, smooth
-    transition whose duration is controlled with two parameters:
-
-    - $p$, the position of the change.
-
-    - $w$, the width of the transition.
-
-    The sigmoid function at sample $t$ is
-
-    $$f(t) = 1/(1+e^{-4(t-p)/w})$$
-
-    Parameters
-    ----------
-    stream
-        Original stream
-    drift_stream
-        Drift stream
-    seed
-        If int, `seed` is used to seed the random number generator;
-        If RandomState instance, `seed` is the random number generator;
-        If None, the random number generator is the `RandomState` instance used
-        by `np.random`.
-    alpha
-        Angle of change used to estimate the width of concept drift change.
-        If set, it will override the width parameter. Valid values are in the
-        range (0.0, 90.0].
-    position
-        Central position of the concept drift change.
-    width
-        Width of concept drift change.
-
-    Examples
-    --------
-    >>> from river import synth
-
-    >>> dataset = synth.ConceptDriftStream(stream=synth.SEA(seed=42, variant=0),
-    ...                                    drift_stream=synth.SEA(seed=42, variant=1),
-    ...                                    seed=1, position=5, width=2)
-
-    >>> for x, y in dataset.take(10):
-    ...     print(x, y)
-    {0: 6.3942, 1: 0.2501, 2: 2.7502} False
-    {0: 2.2321, 1: 7.3647, 2: 6.7669} True
-    {0: 6.3942, 1: 0.2501, 2: 2.7502} False
-    {0: 8.9217, 1: 0.8693, 2: 4.2192} True
-    {0: 2.2321, 1: 7.3647, 2: 6.7669} True
-    {0: 8.9217, 1: 0.8693, 2: 4.2192} True
-    {0: 0.2979, 1: 2.1863, 2: 5.0535} False
-    {0: 0.2653, 1: 1.9883, 2: 6.4988} False
-    {0: 5.4494, 1: 2.2044, 2: 5.8926} False
-    {0: 8.0943, 1: 0.0649, 2: 8.0581} False
-
-    Notes
-    -----
-    An optional way to estimate the width of the transition $w$ is based on
-    the angle $\alpha$, $w = 1/ tan(\alpha)$. Since width corresponds to
-    the number of samples for the transition, the width is rounded to the
-    nearest smaller integer. Notice that larger values of $\alpha$ result in
-    smaller widths. For $\alpha > 45.0$, the width is smaller than 1 so values
-    are rounded to 1 to avoid division by zero errors.
-
-    """
-
-    def __init__(
-        self,
-        stream: base.SyntheticDataset = None,
-        drift_stream: base.SyntheticDataset = None,
-        position: int = 5000,
-        width: int = 1000,
-        seed: int = None,
-        alpha: float = None,
-    ):
-
-        if stream is None:
-            stream = Agrawal(seed=seed)
-
-        if drift_stream is None:
-            drift_stream = Agrawal(seed=seed, classification_function=2)
-
-        # Fairly simple check for consistent number of features
-        if stream.n_features != drift_stream.n_features:
-            raise AttributeError(
-                f"Inconsistent number of features between "
-                f"{stream.__name__} ({stream.n_features}) and "
-                f"{drift_stream.__name__} ({drift_stream.n_features})."
-            )
-        super().__init__(
-            n_features=stream.n_features,
-            n_classes=stream.n_classes,
-            n_outputs=stream.n_outputs,
-            task=stream.task,
-        )
-
-        self.n_samples = stream.n_samples
-
-        self.seed = seed
-        self.alpha = alpha
-        if self.alpha is not None:
-            if 0 < self.alpha <= 90.0:
-                w = int(1 / np.tan(self.alpha * np.pi / 180))
-                self.width = w if w > 0 else 1
-            else:
-                raise ValueError(
-                    f"Invalid alpha value: {alpha}. "
-                    f"Valid values are in the range (0.0, 90.0]"
-                )
-        else:
-            self.width = width
-        self.position = position
-        self.stream = stream
-        self.drift_stream = drift_stream
-
-    def __iter__(self):
-        rng = check_random_state(self.seed)
-        stream_generator = iter(self.stream)
-        drift_stream_generator = iter(self.drift_stream)
-        sample_idx = 0
-
-        while True:
-            sample_idx += 1
-            v = -4.0 * float(sample_idx - self.position) / float(self.width)
-            probability_drift = 1.0 / (1.0 + np.exp(v))
-            try:
-                if rng.rand() > probability_drift:
-                    x, y = next(stream_generator)
-                else:
-                    x, y = next(drift_stream_generator)
-            except StopIteration:
-                break
-            yield x, y
-
-    def __repr__(self):
-        params = self._get_params()
-        l_len_config = max(map(len, params.keys()))
-        r_len_config = max(map(len, map(str, params.values())))
-
-        config = "\n\nConfiguration:\n"
-        for k, v in params.items():
-            if not isinstance(v, base.SyntheticDataset):
-                indent = 0
-            else:
-                indent = l_len_config + 2
-            config += (
-                "".join(
-                    k.rjust(l_len_config)
-                    + "  "
-                    + textwrap.indent(str(v).ljust(r_len_config), " " * indent)
-                )
-                + "\n"
-            )
-
-        l_len_prop = max(map(len, self._repr_content.keys()))
-        r_len_prop = max(map(len, self._repr_content.values()))
-
-        out = (
-            "Synthetic data generator\n\n"
-            + "\n".join(
-                k.rjust(l_len_prop) + "  " + v.ljust(r_len_prop)
-                for k, v in self._repr_content.items()
-            )
-            + config
-        )
-
-        return out
+import textwrap
+
+import numpy as np
+
+from river.utils.skmultiflow_utils import check_random_state
+
+from .. import base
+from ..synth import Agrawal
+
+
+class ConceptDriftStream(base.SyntheticDataset):
+    """Generates a stream with concept drift.
+
+    A stream generator that adds concept drift or change by joining two
+    streams. This is done by building a weighted combination of two pure
+    distributions that characterizes the target concepts before and after
+    the change.
+
+    The sigmoid function is an elegant and practical solution to define the
+    probability that each new instance of the stream belongs to the new
+    concept after the drift. The sigmoid function introduces a gradual, smooth
+    transition whose duration is controlled with two parameters:
+
+    - $p$, the position of the change.
+
+    - $w$, the width of the transition.
+
+    The sigmoid function at sample $t$ is
+
+    $$f(t) = 1/(1+e^{-4(t-p)/w})$$
+
+    Parameters
+    ----------
+    stream
+        Original stream
+    drift_stream
+        Drift stream
+    seed
+        If int, `seed` is used to seed the random number generator;
+        If RandomState instance, `seed` is the random number generator;
+        If None, the random number generator is the `RandomState` instance used
+        by `np.random`.
+    alpha
+        Angle of change used to estimate the width of concept drift change.
+        If set, it will override the width parameter. Valid values are in the
+        range (0.0, 90.0].
+    position
+        Central position of the concept drift change.
+    width
+        Width of concept drift change.
+
+    Examples
+    --------
+    >>> from river import synth
+
+    >>> dataset = synth.ConceptDriftStream(stream=synth.SEA(seed=42, variant=0),
+    ...                                    drift_stream=synth.SEA(seed=42, variant=1),
+    ...                                    seed=1, position=5, width=2)
+
+    >>> for x, y in dataset.take(10):
+    ...     print(x, y)
+    {0: 6.3942, 1: 0.2501, 2: 2.7502} False
+    {0: 2.2321, 1: 7.3647, 2: 6.7669} True
+    {0: 6.3942, 1: 0.2501, 2: 2.7502} False
+    {0: 8.9217, 1: 0.8693, 2: 4.2192} True
+    {0: 2.2321, 1: 7.3647, 2: 6.7669} True
+    {0: 8.9217, 1: 0.8693, 2: 4.2192} True
+    {0: 0.2979, 1: 2.1863, 2: 5.0535} False
+    {0: 0.2653, 1: 1.9883, 2: 6.4988} False
+    {0: 5.4494, 1: 2.2044, 2: 5.8926} False
+    {0: 8.0943, 1: 0.0649, 2: 8.0581} False
+
+    Notes
+    -----
+    An optional way to estimate the width of the transition $w$ is based on
+    the angle $\alpha$, $w = 1/ tan(\alpha)$. Since width corresponds to
+    the number of samples for the transition, the width is rounded to the
+    nearest smaller integer. Notice that larger values of $\alpha$ result in
+    smaller widths. For $\alpha > 45.0$, the width is smaller than 1 so values
+    are rounded to 1 to avoid division by zero errors.
+
+    """
+
+    def __init__(
+        self,
+        stream: base.SyntheticDataset = None,
+        drift_stream: base.SyntheticDataset = None,
+        position: int = 5000,
+        width: int = 1000,
+        seed: int = None,
+        alpha: float = None,
+    ):
+
+        if stream is None:
+            stream = Agrawal(seed=seed)
+
+        if drift_stream is None:
+            drift_stream = Agrawal(seed=seed, classification_function=2)
+
+        # Fairly simple check for consistent number of features
+        if stream.n_features != drift_stream.n_features:
+            raise AttributeError(
+                f"Inconsistent number of features between "
+                f"{stream.__name__} ({stream.n_features}) and "
+                f"{drift_stream.__name__} ({drift_stream.n_features})."
+            )
+        super().__init__(
+            n_features=stream.n_features,
+            n_classes=stream.n_classes,
+            n_outputs=stream.n_outputs,
+            task=stream.task,
+        )
+
+        self.n_samples = stream.n_samples
+
+        self.seed = seed
+        self.alpha = alpha
+        if self.alpha is not None:
+            if 0 < self.alpha <= 90.0:
+                w = int(1 / np.tan(self.alpha * np.pi / 180))
+                self.width = w if w > 0 else 1
+            else:
+                raise ValueError(
+                    f"Invalid alpha value: {alpha}. "
+                    f"Valid values are in the range (0.0, 90.0]"
+                )
+        else:
+            self.width = width
+        self.position = position
+        self.stream = stream
+        self.drift_stream = drift_stream
+
+    def __iter__(self):
+        rng = check_random_state(self.seed)
+        stream_generator = iter(self.stream)
+        drift_stream_generator = iter(self.drift_stream)
+        sample_idx = 0
+
+        while True:
+            sample_idx += 1
+            v = -4.0 * float(sample_idx - self.position) / float(self.width)
+            probability_drift = 1.0 / (1.0 + np.exp(v))
+            try:
+                if rng.rand() > probability_drift:
+                    x, y = next(stream_generator)
+                else:
+                    x, y = next(drift_stream_generator)
+            except StopIteration:
+                break
+            yield x, y
+
+    def __repr__(self):
+        params = self._get_params()
+        l_len_config = max(map(len, params.keys()))
+        r_len_config = max(map(len, map(str, params.values())))
+
+        config = "\n\nConfiguration:\n"
+        for k, v in params.items():
+            if not isinstance(v, base.SyntheticDataset):
+                indent = 0
+            else:
+                indent = l_len_config + 2
+            config += (
+                "".join(
+                    k.rjust(l_len_config)
+                    + "  "
+                    + textwrap.indent(str(v).ljust(r_len_config), " " * indent)
+                )
+                + "\n"
+            )
+
+        l_len_prop = max(map(len, self._repr_content.keys()))
+        r_len_prop = max(map(len, self._repr_content.values()))
+
+        out = (
+            "Synthetic data generator\n\n"
+            + "\n".join(
+                k.rjust(l_len_prop) + "  " + v.ljust(r_len_prop)
+                for k, v in self._repr_content.items()
+            )
+            + config
+        )
+
+        return out
```

### Comparing `river-0.8.0/river/datasets/synth/hyper_plane.py` & `river-0.9.0/river/datasets/synth/hyper_plane.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,142 +1,142 @@
-import numpy as np
-
-from river.utils.skmultiflow_utils import check_random_state
-
-from .. import base
-
-
-class Hyperplane(base.SyntheticDataset):
-    r"""Hyperplane stream generator.
-
-    Generates a problem of prediction class of a rotation hyperplane. It was
-    used as testbed for CVFDT and VFDT in [^1].
-
-    A hyperplane in d-dimensional space is the set of points $x$ that satisfy
-
-    $$\sum^{d}_{i=1} w_i x_i = w_0 = \sum^{d}_{i=1} w_i$$
-
-    where $x_i$ is the i-th coordinate of $x$.
-
-    - Examples for which $\sum^{d}_{i=1} w_i x_i > w_0$, are labeled positive.
-
-    - Examples for which $\sum^{d}_{i=1} w_i x_i \leq w_0$, are labeled negative.
-
-    Hyperplanes are useful for simulating time-changing concepts because we
-    can change the orientation and position of the hyperplane in a smooth
-    manner by changing the relative size of the weights. We introduce change
-    to this dataset by adding drift to each weighted feature
-    $w_i = w_i + d \sigma$, where $\sigma$ is the probability that the
-    direction of change is reversed and $d$ is the change applied to each
-    example.
-
-    Parameters
-    ----------
-    seed
-        If int, `seed` is used to seed the random number generator;
-        If RandomState instance, `seed` is the random number generator;
-        If None, the random number generator is the `RandomState` instance used
-        by `np.random`.
-    n_features
-        The number of attributes to generate. Higher than 2.
-    n_drift_features
-        The number of attributes with drift. Higher than 2.
-    mag_change
-        Magnitude of the change for every example. From 0.0 to 1.0.
-    noise_percentage
-        Percentage of noise to add to the data. From 0.0 to 1.0.
-    sigma
-        Probability that the direction of change is reversed. From 0.0 to 1.0.
-
-    Examples
-    --------
-
-    >>> from river import synth
-
-    >>> dataset = synth.Hyperplane(seed=42, n_features=2)
-
-    >>> for x, y in dataset.take(5):
-    ...     print(x, y)
-    {0: 0.7319, 1: 0.5986} 1
-    {0: 0.8661, 1: 0.6011} 1
-    {0: 0.8324, 1: 0.2123} 0
-    {0: 0.5247, 1: 0.4319} 0
-    {0: 0.2921, 1: 0.3663} 0
-
-    Notes
-    -----
-    The sample generation works as follows: The features are generated
-    with the random number generator, initialized with the seed passed by
-    the user. Then the classification function decides, as a function of
-    the sum of the weighted features and the sum of the weights, whether
-    the instance belongs to class 0 or class 1. The last step is to add
-    noise and generate drift.
-
-    References
-    ----------
-    [^1]: G. Hulten, L. Spencer, and P. Domingos. Mining time-changing data streams.
-          In KDD’01, pages 97–106, San Francisco, CA, 2001. ACM Press.
-
-    """
-
-    def __init__(
-        self,
-        seed: int or np.random.RandomState = None,
-        n_features: int = 10,
-        n_drift_features: int = 2,
-        mag_change: float = 0.0,
-        noise_percentage: float = 0.05,
-        sigma: float = 0.1,
-    ):
-        super().__init__(
-            n_features=n_features, n_classes=2, n_outputs=1, task=base.BINARY_CLF
-        )
-
-        self.seed = seed
-        self.n_drift_features = n_drift_features
-        if not (0.0 <= mag_change <= 1.0):
-            raise ValueError(
-                f"Invalid mag_change ({mag_change}). " "Valid range is [0.0, 1.0]"
-            )
-        self.mag_change = mag_change
-        if not (0.0 <= sigma <= 1.0):
-            raise ValueError(
-                f"Invalid sigma_percentage ({sigma}). " "Valid range is [0.0, 1.0]"
-            )
-        self.sigma = sigma
-        if not (0.0 <= noise_percentage <= 1.0):
-            raise ValueError(
-                f"Invalid noise_percentage ({noise_percentage}). "
-                "Valid range is [0.0, 1.0]"
-            )
-        self.noise_percentage = noise_percentage
-        self.target_values = [0, 1]
-
-    def __iter__(self):
-        self._rng = check_random_state(self.seed)
-        self._weights = self._rng.rand(self.n_features)
-        self._change_direction = np.zeros(self.n_features)
-        self._change_direction[: self.n_drift_features] = 1
-
-        while True:
-            x = dict()
-
-            sum_weights = np.sum(self._weights)
-            sum_value = 0
-            for i in range(self.n_features):
-                x[i] = self._rng.rand()
-                sum_value += self._weights[i] * x[i]
-
-            y = 1 if sum_value >= sum_weights * 0.5 else 0
-
-            if 0.01 + self._rng.rand() <= self.noise_percentage:
-                y = 1 if (y == 0) else 0
-
-            self._generate_drift()
-
-            yield x, y
-
-    def _generate_drift(self):
-        for i in range(self.n_drift_features):
-            self._weights[i] += self._change_direction[i] * self.mag_change
-            if (0.01 + self._rng.rand()) <= self.sigma:
-                self._change_direction[i] *= -1
+import numpy as np
+
+from river.utils.skmultiflow_utils import check_random_state
+
+from .. import base
+
+
+class Hyperplane(base.SyntheticDataset):
+    r"""Hyperplane stream generator.
+
+    Generates a problem of prediction class of a rotation hyperplane. It was
+    used as testbed for CVFDT and VFDT in [^1].
+
+    A hyperplane in d-dimensional space is the set of points $x$ that satisfy
+
+    $$\sum^{d}_{i=1} w_i x_i = w_0 = \sum^{d}_{i=1} w_i$$
+
+    where $x_i$ is the i-th coordinate of $x$.
+
+    - Examples for which $\sum^{d}_{i=1} w_i x_i > w_0$, are labeled positive.
+
+    - Examples for which $\sum^{d}_{i=1} w_i x_i \leq w_0$, are labeled negative.
+
+    Hyperplanes are useful for simulating time-changing concepts because we
+    can change the orientation and position of the hyperplane in a smooth
+    manner by changing the relative size of the weights. We introduce change
+    to this dataset by adding drift to each weighted feature
+    $w_i = w_i + d \sigma$, where $\sigma$ is the probability that the
+    direction of change is reversed and $d$ is the change applied to each
+    example.
+
+    Parameters
+    ----------
+    seed
+        If int, `seed` is used to seed the random number generator;
+        If RandomState instance, `seed` is the random number generator;
+        If None, the random number generator is the `RandomState` instance used
+        by `np.random`.
+    n_features
+        The number of attributes to generate. Higher than 2.
+    n_drift_features
+        The number of attributes with drift. Higher than 2.
+    mag_change
+        Magnitude of the change for every example. From 0.0 to 1.0.
+    noise_percentage
+        Percentage of noise to add to the data. From 0.0 to 1.0.
+    sigma
+        Probability that the direction of change is reversed. From 0.0 to 1.0.
+
+    Examples
+    --------
+
+    >>> from river import synth
+
+    >>> dataset = synth.Hyperplane(seed=42, n_features=2)
+
+    >>> for x, y in dataset.take(5):
+    ...     print(x, y)
+    {0: 0.7319, 1: 0.5986} 1
+    {0: 0.8661, 1: 0.6011} 1
+    {0: 0.8324, 1: 0.2123} 0
+    {0: 0.5247, 1: 0.4319} 0
+    {0: 0.2921, 1: 0.3663} 0
+
+    Notes
+    -----
+    The sample generation works as follows: The features are generated
+    with the random number generator, initialized with the seed passed by
+    the user. Then the classification function decides, as a function of
+    the sum of the weighted features and the sum of the weights, whether
+    the instance belongs to class 0 or class 1. The last step is to add
+    noise and generate drift.
+
+    References
+    ----------
+    [^1]: G. Hulten, L. Spencer, and P. Domingos. Mining time-changing data streams.
+          In KDD’01, pages 97–106, San Francisco, CA, 2001. ACM Press.
+
+    """
+
+    def __init__(
+        self,
+        seed: int or np.random.RandomState = None,
+        n_features: int = 10,
+        n_drift_features: int = 2,
+        mag_change: float = 0.0,
+        noise_percentage: float = 0.05,
+        sigma: float = 0.1,
+    ):
+        super().__init__(
+            n_features=n_features, n_classes=2, n_outputs=1, task=base.BINARY_CLF
+        )
+
+        self.seed = seed
+        self.n_drift_features = n_drift_features
+        if not (0.0 <= mag_change <= 1.0):
+            raise ValueError(
+                f"Invalid mag_change ({mag_change}). " "Valid range is [0.0, 1.0]"
+            )
+        self.mag_change = mag_change
+        if not (0.0 <= sigma <= 1.0):
+            raise ValueError(
+                f"Invalid sigma_percentage ({sigma}). " "Valid range is [0.0, 1.0]"
+            )
+        self.sigma = sigma
+        if not (0.0 <= noise_percentage <= 1.0):
+            raise ValueError(
+                f"Invalid noise_percentage ({noise_percentage}). "
+                "Valid range is [0.0, 1.0]"
+            )
+        self.noise_percentage = noise_percentage
+        self.target_values = [0, 1]
+
+    def __iter__(self):
+        self._rng = check_random_state(self.seed)
+        self._weights = self._rng.rand(self.n_features)
+        self._change_direction = np.zeros(self.n_features)
+        self._change_direction[: self.n_drift_features] = 1
+
+        while True:
+            x = dict()
+
+            sum_weights = np.sum(self._weights)
+            sum_value = 0
+            for i in range(self.n_features):
+                x[i] = self._rng.rand()
+                sum_value += self._weights[i] * x[i]
+
+            y = 1 if sum_value >= sum_weights * 0.5 else 0
+
+            if 0.01 + self._rng.rand() <= self.noise_percentage:
+                y = 1 if (y == 0) else 0
+
+            self._generate_drift()
+
+            yield x, y
+
+    def _generate_drift(self):
+        for i in range(self.n_drift_features):
+            self._weights[i] += self._change_direction[i] * self.mag_change
+            if (0.01 + self._rng.rand()) <= self.sigma:
+                self._change_direction[i] *= -1
```

### Comparing `river-0.8.0/river/datasets/synth/led.py` & `river-0.9.0/river/datasets/synth/led.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,225 +1,225 @@
-import numpy as np
-
-from river.utils.skmultiflow_utils import check_random_state
-
-from .. import base
-
-
-class LED(base.SyntheticDataset):
-    """LED stream generator.
-
-    This data source originates from the CART book [^1]. An implementation
-    in C was donated to the UCI [^2] machine learning repository by David Aha.
-    The goal is to predict the digit displayed on a seven-segment LED display,
-    where each attribute has a 10% chance of being inverted. It has an optimal
-    Bayes classification rate of 74%. The particular configuration of the
-    generator used for experiments (LED) produces 24 binary attributes,
-    17 of which are irrelevant.
-
-    Parameters
-    ----------
-    seed
-        If int, `seed` is used to seed the random number generator;
-        If RandomState instance, `seed` is the random number generator;
-        If None, the random number generator is the `RandomState` instance used
-        by `np.random`.
-    noise_percentage
-        The probability that noise will happen in the generation. At each
-        new sample generated, a random number is generated, and if it is equal
-        or less than the noise_percentage, the led value  will be switched
-    irrelevant_features
-        Adds 17 non-relevant attributes to the stream.
-
-    Examples
-    --------
-    >>> from river import synth
-
-    >>> dataset = synth.LED(seed = 112, noise_percentage = 0.28, irrelevant_features= False)
-
-    >>> for x, y in dataset.take(5):
-    ...     print(x, y)
-    {0: 0, 1: 1, 2: 1, 3: 1, 4: 0, 5: 0, 6: 0} 4
-    {0: 0, 1: 1, 2: 0, 3: 1, 4: 0, 5: 0, 6: 0} 4
-    {0: 1, 1: 0, 2: 1, 3: 1, 4: 0, 5: 0, 6: 1} 3
-    {0: 0, 1: 1, 2: 1, 3: 0, 4: 0, 5: 1, 6: 1} 0
-    {0: 1, 1: 1, 2: 1, 3: 1, 4: 0, 5: 1, 6: 0} 4
-
-    Notes
-    -----
-    An instance is generated based on the parameters passed. If `has_noise`
-    is set then the total number of attributes will be 24, otherwise there will
-    be 7 attributes.
-
-    References
-    ----------
-
-    [^1]: Leo Breiman, Jerome Friedman, R. Olshen, and Charles J. Stone.
-          Classification and Regression Trees. Wadsworth and Brooks,
-          Monterey, CA,1984.
-
-    [^2]: A. Asuncion and D. J. Newman. UCI Machine Learning Repository
-          [http://www.ics.uci.edu/∼mlearn/mlrepository.html].
-          University of California, Irvine, School of Information and
-          Computer Sciences,2007.
-
-    """
-
-    _N_RELEVANT_FEATURES = 7
-    _N_FEATURES_INCLUDING_NOISE = 24
-    _ORIGINAL_INSTANCES = np.array(
-        [
-            [1, 1, 1, 0, 1, 1, 1],
-            [0, 0, 1, 0, 0, 1, 0],
-            [1, 0, 1, 1, 1, 0, 1],
-            [1, 0, 1, 1, 0, 1, 1],
-            [0, 1, 1, 1, 0, 1, 0],
-            [1, 1, 0, 1, 0, 1, 1],
-            [1, 1, 0, 1, 1, 1, 1],
-            [1, 0, 1, 0, 0, 1, 0],
-            [1, 1, 1, 1, 1, 1, 1],
-            [1, 1, 1, 1, 0, 1, 1],
-        ],
-        dtype=int,
-    )
-
-    def __init__(
-        self,
-        seed: int or np.random.RandomState = None,
-        noise_percentage: float = 0.0,
-        irrelevant_features: bool = False,
-    ):
-        super().__init__(
-            n_features=self._N_FEATURES_INCLUDING_NOISE
-            if irrelevant_features
-            else self._N_RELEVANT_FEATURES,
-            n_classes=10,
-            n_outputs=1,
-            task=base.MULTI_CLF,
-        )
-        self.seed = seed
-        self._rng = None  # This is the actual random_state object used internally
-        if not (0.0 <= noise_percentage <= 1.0):
-            raise ValueError(
-                f"Invalid noise_percentage ({noise_percentage}). "
-                "Valid range is [0.0, 1.0]"
-            )
-        self.noise_percentage = noise_percentage
-        self.irrelevant_features = irrelevant_features
-        self.n_cat_features = self.n_features
-        self.target_values = [i for i in range(self.n_classes)]
-
-    def __iter__(self):
-        self._rng = check_random_state(self.seed)
-
-        while True:
-            x = dict()
-            y = self._rng.randint(self.n_classes)
-
-            for i in range(self._N_RELEVANT_FEATURES):
-                if (0.01 + self._rng.rand()) <= self.noise_percentage:
-                    x[i] = int(self._ORIGINAL_INSTANCES[y, i] == 0)
-                else:
-                    x[i] = self._ORIGINAL_INSTANCES[y, i]
-
-            if self.irrelevant_features:
-                for i in range(
-                    self._N_RELEVANT_FEATURES, self._N_FEATURES_INCLUDING_NOISE
-                ):
-                    x[i] = self._rng.randint(2)
-
-            yield x, y
-
-
-class LEDDrift(LED):
-    """LED stream generator with concept drift.
-
-    This class is an extension of the `LED` generator whose purpose is to add
-    concept drift to the stream.
-
-    Parameters
-    ----------
-    seed
-        If int, `seed` is used to seed the random number generator;
-        If RandomState instance, `seed` is the random number generator;
-        If None, the random number generator is the `RandomState` instance used
-        by `np.random`.
-    noise_percentage
-        The probability that noise will happen in the generation. At each
-        new sample generated, a random number is generated, and if it is equal
-        or less than the noise_percentage, the led value  will be switched
-    irrelevant_features
-        Adds 17 non-relevant attributes to the stream.
-    n_drift_features
-        The number of attributes that have drift.
-
-    Examples
-    --------
-    >>> from river import synth
-
-    >>> dataset = synth.LEDDrift(seed = 112, noise_percentage = 0.28,
-    ...                          irrelevant_features= True, n_drift_features=4)
-
-    >>> for x, y in dataset.take(5):
-    ...     print(list(x.values()), y)
-    [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1] 8
-    [0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1] 5
-    [1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1] 8
-    [0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0] 3
-    [0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0] 5
-
-    Notes
-    -----
-    An instance is generated based on the parameters passed. If `has_noise`
-    is set then the total number of attributes will be 24, otherwise there will
-    be 7 attributes.
-
-    """
-
-    _N_IRRELEVANT_ATTRIBUTES = 17
-
-    def __init__(
-        self,
-        seed: int or np.random.RandomState = None,
-        noise_percentage: float = 0.0,
-        irrelevant_features: bool = False,
-        n_drift_features: int = 0,
-    ):
-        super().__init__(
-            seed=seed,
-            noise_percentage=noise_percentage,
-            irrelevant_features=irrelevant_features,
-        )
-        self.n_drift_features = n_drift_features
-
-    def __iter__(self):
-        self._rng = check_random_state(self.seed)
-        self._attr_idx = np.arange(self._N_FEATURES_INCLUDING_NOISE)
-
-        # Change attributes
-        if self.irrelevant_features and self.n_drift_features > 0:
-            random_int = self._rng.randint(7)
-            offset = self._rng.randint(self._N_IRRELEVANT_ATTRIBUTES)
-            for i in range(self.n_drift_features):
-                value_1 = (i + random_int) % 7
-                value_2 = 7 + (i + offset) % self._N_IRRELEVANT_ATTRIBUTES
-                self._attr_idx[value_1] = value_2
-                self._attr_idx[value_2] = value_1
-
-        while True:
-            x = {
-                i: -1 for i in range(self.n_features)
-            }  # Initialize to keep order in dictionary
-            y = self._rng.randint(self.n_classes)
-
-            for i in range(self._N_RELEVANT_FEATURES):
-                if (0.01 + self._rng.rand()) <= self.noise_percentage:
-                    x[self._attr_idx[i]] = int(self._ORIGINAL_INSTANCES[y, i] == 0)
-                else:
-                    x[self._attr_idx[i]] = self._ORIGINAL_INSTANCES[y, i]
-            if self.irrelevant_features:
-                for i in range(
-                    self._N_RELEVANT_FEATURES, self._N_FEATURES_INCLUDING_NOISE
-                ):
-                    x[self._attr_idx[i]] = self._rng.randint(2)
-
-            yield x, y
+import numpy as np
+
+from river.utils.skmultiflow_utils import check_random_state
+
+from .. import base
+
+
+class LED(base.SyntheticDataset):
+    """LED stream generator.
+
+    This data source originates from the CART book [^1]. An implementation
+    in C was donated to the UCI [^2] machine learning repository by David Aha.
+    The goal is to predict the digit displayed on a seven-segment LED display,
+    where each attribute has a 10% chance of being inverted. It has an optimal
+    Bayes classification rate of 74%. The particular configuration of the
+    generator used for experiments (LED) produces 24 binary attributes,
+    17 of which are irrelevant.
+
+    Parameters
+    ----------
+    seed
+        If int, `seed` is used to seed the random number generator;
+        If RandomState instance, `seed` is the random number generator;
+        If None, the random number generator is the `RandomState` instance used
+        by `np.random`.
+    noise_percentage
+        The probability that noise will happen in the generation. At each
+        new sample generated, a random number is generated, and if it is equal
+        or less than the noise_percentage, the led value  will be switched
+    irrelevant_features
+        Adds 17 non-relevant attributes to the stream.
+
+    Examples
+    --------
+    >>> from river import synth
+
+    >>> dataset = synth.LED(seed = 112, noise_percentage = 0.28, irrelevant_features= False)
+
+    >>> for x, y in dataset.take(5):
+    ...     print(x, y)
+    {0: 0, 1: 1, 2: 1, 3: 1, 4: 0, 5: 0, 6: 0} 4
+    {0: 0, 1: 1, 2: 0, 3: 1, 4: 0, 5: 0, 6: 0} 4
+    {0: 1, 1: 0, 2: 1, 3: 1, 4: 0, 5: 0, 6: 1} 3
+    {0: 0, 1: 1, 2: 1, 3: 0, 4: 0, 5: 1, 6: 1} 0
+    {0: 1, 1: 1, 2: 1, 3: 1, 4: 0, 5: 1, 6: 0} 4
+
+    Notes
+    -----
+    An instance is generated based on the parameters passed. If `has_noise`
+    is set then the total number of attributes will be 24, otherwise there will
+    be 7 attributes.
+
+    References
+    ----------
+
+    [^1]: Leo Breiman, Jerome Friedman, R. Olshen, and Charles J. Stone.
+          Classification and Regression Trees. Wadsworth and Brooks,
+          Monterey, CA,1984.
+
+    [^2]: A. Asuncion and D. J. Newman. UCI Machine Learning Repository
+          [http://www.ics.uci.edu/∼mlearn/mlrepository.html].
+          University of California, Irvine, School of Information and
+          Computer Sciences,2007.
+
+    """
+
+    _N_RELEVANT_FEATURES = 7
+    _N_FEATURES_INCLUDING_NOISE = 24
+    _ORIGINAL_INSTANCES = np.array(
+        [
+            [1, 1, 1, 0, 1, 1, 1],
+            [0, 0, 1, 0, 0, 1, 0],
+            [1, 0, 1, 1, 1, 0, 1],
+            [1, 0, 1, 1, 0, 1, 1],
+            [0, 1, 1, 1, 0, 1, 0],
+            [1, 1, 0, 1, 0, 1, 1],
+            [1, 1, 0, 1, 1, 1, 1],
+            [1, 0, 1, 0, 0, 1, 0],
+            [1, 1, 1, 1, 1, 1, 1],
+            [1, 1, 1, 1, 0, 1, 1],
+        ],
+        dtype=int,
+    )
+
+    def __init__(
+        self,
+        seed: int or np.random.RandomState = None,
+        noise_percentage: float = 0.0,
+        irrelevant_features: bool = False,
+    ):
+        super().__init__(
+            n_features=self._N_FEATURES_INCLUDING_NOISE
+            if irrelevant_features
+            else self._N_RELEVANT_FEATURES,
+            n_classes=10,
+            n_outputs=1,
+            task=base.MULTI_CLF,
+        )
+        self.seed = seed
+        self._rng = None  # This is the actual random_state object used internally
+        if not (0.0 <= noise_percentage <= 1.0):
+            raise ValueError(
+                f"Invalid noise_percentage ({noise_percentage}). "
+                "Valid range is [0.0, 1.0]"
+            )
+        self.noise_percentage = noise_percentage
+        self.irrelevant_features = irrelevant_features
+        self.n_cat_features = self.n_features
+        self.target_values = [i for i in range(self.n_classes)]
+
+    def __iter__(self):
+        self._rng = check_random_state(self.seed)
+
+        while True:
+            x = dict()
+            y = self._rng.randint(self.n_classes)
+
+            for i in range(self._N_RELEVANT_FEATURES):
+                if (0.01 + self._rng.rand()) <= self.noise_percentage:
+                    x[i] = int(self._ORIGINAL_INSTANCES[y, i] == 0)
+                else:
+                    x[i] = self._ORIGINAL_INSTANCES[y, i]
+
+            if self.irrelevant_features:
+                for i in range(
+                    self._N_RELEVANT_FEATURES, self._N_FEATURES_INCLUDING_NOISE
+                ):
+                    x[i] = self._rng.randint(2)
+
+            yield x, y
+
+
+class LEDDrift(LED):
+    """LED stream generator with concept drift.
+
+    This class is an extension of the `LED` generator whose purpose is to add
+    concept drift to the stream.
+
+    Parameters
+    ----------
+    seed
+        If int, `seed` is used to seed the random number generator;
+        If RandomState instance, `seed` is the random number generator;
+        If None, the random number generator is the `RandomState` instance used
+        by `np.random`.
+    noise_percentage
+        The probability that noise will happen in the generation. At each
+        new sample generated, a random number is generated, and if it is equal
+        or less than the noise_percentage, the led value  will be switched
+    irrelevant_features
+        Adds 17 non-relevant attributes to the stream.
+    n_drift_features
+        The number of attributes that have drift.
+
+    Examples
+    --------
+    >>> from river import synth
+
+    >>> dataset = synth.LEDDrift(seed = 112, noise_percentage = 0.28,
+    ...                          irrelevant_features= True, n_drift_features=4)
+
+    >>> for x, y in dataset.take(5):
+    ...     print(list(x.values()), y)
+    [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1] 8
+    [0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1] 5
+    [1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1] 8
+    [0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0] 3
+    [0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0] 5
+
+    Notes
+    -----
+    An instance is generated based on the parameters passed. If `has_noise`
+    is set then the total number of attributes will be 24, otherwise there will
+    be 7 attributes.
+
+    """
+
+    _N_IRRELEVANT_ATTRIBUTES = 17
+
+    def __init__(
+        self,
+        seed: int or np.random.RandomState = None,
+        noise_percentage: float = 0.0,
+        irrelevant_features: bool = False,
+        n_drift_features: int = 0,
+    ):
+        super().__init__(
+            seed=seed,
+            noise_percentage=noise_percentage,
+            irrelevant_features=irrelevant_features,
+        )
+        self.n_drift_features = n_drift_features
+
+    def __iter__(self):
+        self._rng = check_random_state(self.seed)
+        self._attr_idx = np.arange(self._N_FEATURES_INCLUDING_NOISE)
+
+        # Change attributes
+        if self.irrelevant_features and self.n_drift_features > 0:
+            random_int = self._rng.randint(7)
+            offset = self._rng.randint(self._N_IRRELEVANT_ATTRIBUTES)
+            for i in range(self.n_drift_features):
+                value_1 = (i + random_int) % 7
+                value_2 = 7 + (i + offset) % self._N_IRRELEVANT_ATTRIBUTES
+                self._attr_idx[value_1] = value_2
+                self._attr_idx[value_2] = value_1
+
+        while True:
+            x = {
+                i: -1 for i in range(self.n_features)
+            }  # Initialize to keep order in dictionary
+            y = self._rng.randint(self.n_classes)
+
+            for i in range(self._N_RELEVANT_FEATURES):
+                if (0.01 + self._rng.rand()) <= self.noise_percentage:
+                    x[self._attr_idx[i]] = int(self._ORIGINAL_INSTANCES[y, i] == 0)
+                else:
+                    x[self._attr_idx[i]] = self._ORIGINAL_INSTANCES[y, i]
+            if self.irrelevant_features:
+                for i in range(
+                    self._N_RELEVANT_FEATURES, self._N_FEATURES_INCLUDING_NOISE
+                ):
+                    x[self._attr_idx[i]] = self._rng.randint(2)
+
+            yield x, y
```

### Comparing `river-0.8.0/river/datasets/synth/logical.py` & `river-0.9.0/river/datasets/synth/logical.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,104 +1,104 @@
-import itertools
-
-import numpy as np
-
-from river.utils.skmultiflow_utils import check_random_state
-
-from .. import base
-
-
-class Logical(base.SyntheticDataset):
-    """Logical functions stream generator.
-
-    Make a toy dataset with three labels that represent the logical
-    functions: `OR`, `XOR`, `AND` (functions of the 2D input).
-
-    Data is generated in 'tiles' which contain the complete set of
-    logical operations results. The tiles are repeated `n_tiles` times.
-    Optionally, the generated data can be shuffled.
-
-    Parameters
-    ----------
-    n_tiles
-        Number of tiles to generate.
-    shuffle
-        If set, generated data will be shuffled.
-    seed
-        If int, `seed` is used to seed the random number generator;
-        If RandomState instance, `seed` is the random number generator;
-        If None, the random number generator is the `RandomState` instance used
-        by `np.random`.
-
-    Examples
-    --------
-    >>> from river import synth
-
-    >>> dataset = synth.Logical(n_tiles=2, shuffle=True, seed=42)
-
-    >>> for x, y in dataset.take(5):
-    ...     print(x, y)
-    {'A': 0, 'B': 1} {'OR': 1, 'XOR': 1, 'AND': 0}
-    {'A': 0, 'B': 1} {'OR': 1, 'XOR': 1, 'AND': 0}
-    {'A': 0, 'B': 0} {'OR': 0, 'XOR': 0, 'AND': 0}
-    {'A': 1, 'B': 1} {'OR': 1, 'XOR': 0, 'AND': 1}
-    {'A': 1, 'B': 0} {'OR': 1, 'XOR': 1, 'AND': 0}
-
-    """
-
-    def __init__(
-        self,
-        n_tiles: int = 1,
-        shuffle: bool = True,
-        seed: int or np.random.RandomState = None,
-    ):
-        super().__init__(
-            n_features=2, n_outputs=3, n_samples=4 * n_tiles, task=base.MO_BINARY_CLF
-        )
-        self.n_tiles = n_tiles
-        self.shuffle = shuffle
-        self.seed = seed
-        self.feature_names = ["A", "B"]
-        self.target_names = ["OR", "XOR", "AND"]
-
-    def __iter__(self):
-        rng = check_random_state(self.seed)
-        X, Y = self._make_logical(
-            n_tiles=self.n_tiles, shuffle=self.shuffle, random_state=rng
-        )
-
-        for xi, yi in itertools.zip_longest(X, Y if hasattr(Y, "__iter__") else []):
-            yield dict(zip(self.feature_names, xi)), dict(zip(self.target_names, yi))
-
-    @staticmethod
-    def _make_logical(
-        n_tiles: int = 1,
-        shuffle: bool = True,
-        random_state: np.random.RandomState = None,
-    ):
-        """Make toy dataset"""
-        base_pattern = np.array(
-            [
-                # A  B  OR  XOR  AND
-                [0, 0, 0, 0, 0],
-                [0, 1, 1, 1, 0],
-                [1, 0, 1, 1, 0],
-                [1, 1, 1, 0, 1],
-            ],
-            dtype=int,
-        )
-
-        N, E = base_pattern.shape
-        D = 2
-        L = E - D
-
-        pattern = np.zeros((N, E))
-        pattern[:, 0:L] = base_pattern[:, D:E]
-        pattern[:, L:E] = base_pattern[:, 0:D]
-        pattern = np.tile(pattern, (n_tiles, 1))
-        if shuffle:
-            random_state.shuffle(pattern)
-        # return X, Y
-        return (
-            np.array(pattern[:, L:E], dtype=int),
-            np.array(pattern[:, 0:L], dtype=int),
-        )
+import itertools
+
+import numpy as np
+
+from river.utils.skmultiflow_utils import check_random_state
+
+from .. import base
+
+
+class Logical(base.SyntheticDataset):
+    """Logical functions stream generator.
+
+    Make a toy dataset with three labels that represent the logical
+    functions: `OR`, `XOR`, `AND` (functions of the 2D input).
+
+    Data is generated in 'tiles' which contain the complete set of
+    logical operations results. The tiles are repeated `n_tiles` times.
+    Optionally, the generated data can be shuffled.
+
+    Parameters
+    ----------
+    n_tiles
+        Number of tiles to generate.
+    shuffle
+        If set, generated data will be shuffled.
+    seed
+        If int, `seed` is used to seed the random number generator;
+        If RandomState instance, `seed` is the random number generator;
+        If None, the random number generator is the `RandomState` instance used
+        by `np.random`.
+
+    Examples
+    --------
+    >>> from river import synth
+
+    >>> dataset = synth.Logical(n_tiles=2, shuffle=True, seed=42)
+
+    >>> for x, y in dataset.take(5):
+    ...     print(x, y)
+    {'A': 0, 'B': 1} {'OR': 1, 'XOR': 1, 'AND': 0}
+    {'A': 0, 'B': 1} {'OR': 1, 'XOR': 1, 'AND': 0}
+    {'A': 0, 'B': 0} {'OR': 0, 'XOR': 0, 'AND': 0}
+    {'A': 1, 'B': 1} {'OR': 1, 'XOR': 0, 'AND': 1}
+    {'A': 1, 'B': 0} {'OR': 1, 'XOR': 1, 'AND': 0}
+
+    """
+
+    def __init__(
+        self,
+        n_tiles: int = 1,
+        shuffle: bool = True,
+        seed: int or np.random.RandomState = None,
+    ):
+        super().__init__(
+            n_features=2, n_outputs=3, n_samples=4 * n_tiles, task=base.MO_BINARY_CLF
+        )
+        self.n_tiles = n_tiles
+        self.shuffle = shuffle
+        self.seed = seed
+        self.feature_names = ["A", "B"]
+        self.target_names = ["OR", "XOR", "AND"]
+
+    def __iter__(self):
+        rng = check_random_state(self.seed)
+        X, Y = self._make_logical(
+            n_tiles=self.n_tiles, shuffle=self.shuffle, random_state=rng
+        )
+
+        for xi, yi in itertools.zip_longest(X, Y if hasattr(Y, "__iter__") else []):
+            yield dict(zip(self.feature_names, xi)), dict(zip(self.target_names, yi))
+
+    @staticmethod
+    def _make_logical(
+        n_tiles: int = 1,
+        shuffle: bool = True,
+        random_state: np.random.RandomState = None,
+    ):
+        """Make toy dataset"""
+        base_pattern = np.array(
+            [
+                # A  B  OR  XOR  AND
+                [0, 0, 0, 0, 0],
+                [0, 1, 1, 1, 0],
+                [1, 0, 1, 1, 0],
+                [1, 1, 1, 0, 1],
+            ],
+            dtype=int,
+        )
+
+        N, E = base_pattern.shape
+        D = 2
+        L = E - D
+
+        pattern = np.zeros((N, E))
+        pattern[:, 0:L] = base_pattern[:, D:E]
+        pattern[:, L:E] = base_pattern[:, 0:D]
+        pattern = np.tile(pattern, (n_tiles, 1))
+        if shuffle:
+            random_state.shuffle(pattern)
+        # return X, Y
+        return (
+            np.array(pattern[:, L:E], dtype=int),
+            np.array(pattern[:, 0:L], dtype=int),
+        )
```

### Comparing `river-0.8.0/river/datasets/synth/mixed.py` & `river-0.9.0/river/datasets/synth/mixed.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,152 +1,152 @@
-import numpy as np
-
-from river.utils.skmultiflow_utils import check_random_state
-
-from .. import base
-
-
-class Mixed(base.SyntheticDataset):
-    r"""Mixed data stream generator.
-
-    This generator is an implementation of a data stream with abrupt concept
-    drift and boolean noise-free examples as described in [^1].
-
-    It has four relevant attributes, two boolean attributes $v, w$ and two
-    numeric attributes $x, y$ uniformly distributed from 0 to 1. The examples
-    are labeled depending on the classification function chosen from below.
-
-    * `function 0`:
-      if $v$ and $w$ are true or $v$ and $z$ are true or $w$ and $z$ are true
-      then 0 else 1, where $z$ is $y < 0.5 + 0.3 sin(3 \pi  x)$
-
-    * `function 1`:
-       The opposite of `function 0`.
-
-    Concept drift can be introduced by changing the classification function.
-    This can be done manually or using `ConceptDriftStream`.
-
-    Parameters
-    ----------
-    classification_function
-        Which of the two classification functions to use for the generation.
-        Valid options are 0 or 1.
-    seed
-        If int, `seed` is used to seed the random number generator;
-        If RandomState instance, `seed` is the random number generator;
-        If None, the random number generator is the `RandomState` instance used
-        by `np.random`.
-    balance_classes
-        Whether to balance classes or not. If balanced, the class distribution
-        will converge to a uniform distribution.
-
-    Examples
-    --------
-    >>> from river import synth
-    >>>
-    >>> dataset = synth.Mixed(seed = 42, classification_function=1, balance_classes = True)
-    >>>
-    >>> for x, y in dataset.take(5):
-    ...     print(x, y)
-    {0: False, 1: True, 2: 0.7319, 3: 0.5986} 1
-    {0: False, 1: False, 2: 0.0580, 3: 0.8661} 0
-    {0: True, 1: True, 2: 0.0205, 3: 0.9699} 1
-    {0: False, 1: True, 2: 0.4319, 3: 0.2912} 0
-    {0: True, 1: False, 2: 0.2921, 3: 0.3663} 1
-
-    Notes
-    -----
-    The sample generation works as follows: The two numeric attributes are
-    generated with the random  generator initialized with the seed passed by
-    the user (optional). The boolean attributes are either 0 or 1
-    based on the comparison of the random number generator and 0.5 ,
-    the classification function decides whether to classify the instance
-    as class 0 or class 1. The next step is to verify if the classes should
-    be balanced, and if so, balance the classes.
-
-    The generated sample will have 4 relevant features and 1 label (it is a
-    binary-classification task).
-
-    References
-    ----------
-    [^1]: Gama, Joao, et al. "Learning with drift detection." Advances in
-          artificial intelligence–SBIA 2004. Springer Berlin Heidelberg,
-          2004. 286-295"
-
-    """
-
-    def __init__(
-        self,
-        classification_function: int = 0,
-        seed: int or np.random.RandomState = None,
-        balance_classes: bool = False,
-    ):
-        super().__init__(n_features=4, n_classes=2, n_outputs=1, task=base.BINARY_CLF)
-
-        # Classification functions to use
-        self._functions = [
-            self._classification_function_zero,
-            self._classification_function_one,
-        ]
-        self.seed = seed
-        if classification_function not in [0, 1]:
-            raise ValueError(
-                f"Invalid classification_function ({classification_function}). "
-                "Valid values are 0 or 1."
-            )
-        self.classification_function = classification_function
-        self._rng = None  # This is the actual random_state object used internally
-        self.balance_classes = balance_classes
-        self.n_cat_features = 2
-        self.n_num_features = 2
-        self.cat_features_idx = [0, 1]
-        self.next_class_should_be_zero = False
-
-    def __iter__(self):
-        self._rng = check_random_state(self.seed)
-        self.next_class_should_be_zero = False
-
-        while True:
-            att_0 = False
-            att_1 = False
-            att_2 = 0.0
-            att_3 = 0.0
-            y = 0
-            desired_class_found = False
-            while not desired_class_found:
-                att_0 = self._rng.rand() >= 0.5
-                att_1 = self._rng.rand() >= 0.5
-                att_2 = self._rng.rand()
-                att_3 = self._rng.rand()
-
-                y = self._functions[self.classification_function](
-                    att_0, att_1, att_2, att_3
-                )
-
-                if not self.balance_classes:
-                    desired_class_found = True
-                else:
-                    if (self.next_class_should_be_zero and (y == 0)) or (
-                        (not self.next_class_should_be_zero) and (y == 1)
-                    ):
-                        desired_class_found = True
-                        self.next_class_should_be_zero = (
-                            not self.next_class_should_be_zero
-                        )
-
-            x = {0: att_0, 1: att_1, 2: att_2, 3: att_3}
-
-            yield x, y
-
-    @staticmethod
-    def _classification_function_zero(v: bool, w: bool, x: float, y: float):
-        z = y < 0.5 + 0.3 * np.sin(3 * np.pi * x)
-        return 0 if (v and w) or (v and z) or (w and z) else 1
-
-    @staticmethod
-    def _classification_function_one(v: bool, w: bool, x: float, y: float):
-        z = y < 0.5 + 0.3 * np.sin(3 * np.pi * x)
-        return 1 if (v == 1 and w == 1) or (v == 1 and z) or (w == 1 and z) else 0
-
-    def generate_drift(self):
-        """Generate drift by switching the classification function."""
-        self.classification_function = 1 - self.classification_function
+import numpy as np
+
+from river.utils.skmultiflow_utils import check_random_state
+
+from .. import base
+
+
+class Mixed(base.SyntheticDataset):
+    r"""Mixed data stream generator.
+
+    This generator is an implementation of a data stream with abrupt concept
+    drift and boolean noise-free examples as described in [^1].
+
+    It has four relevant attributes, two boolean attributes $v, w$ and two
+    numeric attributes $x, y$ uniformly distributed from 0 to 1. The examples
+    are labeled depending on the classification function chosen from below.
+
+    * `function 0`:
+      if $v$ and $w$ are true or $v$ and $z$ are true or $w$ and $z$ are true
+      then 0 else 1, where $z$ is $y < 0.5 + 0.3 sin(3 \pi  x)$
+
+    * `function 1`:
+       The opposite of `function 0`.
+
+    Concept drift can be introduced by changing the classification function.
+    This can be done manually or using `ConceptDriftStream`.
+
+    Parameters
+    ----------
+    classification_function
+        Which of the two classification functions to use for the generation.
+        Valid options are 0 or 1.
+    seed
+        If int, `seed` is used to seed the random number generator;
+        If RandomState instance, `seed` is the random number generator;
+        If None, the random number generator is the `RandomState` instance used
+        by `np.random`.
+    balance_classes
+        Whether to balance classes or not. If balanced, the class distribution
+        will converge to a uniform distribution.
+
+    Examples
+    --------
+    >>> from river import synth
+    >>>
+    >>> dataset = synth.Mixed(seed = 42, classification_function=1, balance_classes = True)
+    >>>
+    >>> for x, y in dataset.take(5):
+    ...     print(x, y)
+    {0: False, 1: True, 2: 0.7319, 3: 0.5986} 1
+    {0: False, 1: False, 2: 0.0580, 3: 0.8661} 0
+    {0: True, 1: True, 2: 0.0205, 3: 0.9699} 1
+    {0: False, 1: True, 2: 0.4319, 3: 0.2912} 0
+    {0: True, 1: False, 2: 0.2921, 3: 0.3663} 1
+
+    Notes
+    -----
+    The sample generation works as follows: The two numeric attributes are
+    generated with the random  generator initialized with the seed passed by
+    the user (optional). The boolean attributes are either 0 or 1
+    based on the comparison of the random number generator and 0.5 ,
+    the classification function decides whether to classify the instance
+    as class 0 or class 1. The next step is to verify if the classes should
+    be balanced, and if so, balance the classes.
+
+    The generated sample will have 4 relevant features and 1 label (it is a
+    binary-classification task).
+
+    References
+    ----------
+    [^1]: Gama, Joao, et al. "Learning with drift detection." Advances in
+          artificial intelligence–SBIA 2004. Springer Berlin Heidelberg,
+          2004. 286-295"
+
+    """
+
+    def __init__(
+        self,
+        classification_function: int = 0,
+        seed: int or np.random.RandomState = None,
+        balance_classes: bool = False,
+    ):
+        super().__init__(n_features=4, n_classes=2, n_outputs=1, task=base.BINARY_CLF)
+
+        # Classification functions to use
+        self._functions = [
+            self._classification_function_zero,
+            self._classification_function_one,
+        ]
+        self.seed = seed
+        if classification_function not in [0, 1]:
+            raise ValueError(
+                f"Invalid classification_function ({classification_function}). "
+                "Valid values are 0 or 1."
+            )
+        self.classification_function = classification_function
+        self._rng = None  # This is the actual random_state object used internally
+        self.balance_classes = balance_classes
+        self.n_cat_features = 2
+        self.n_num_features = 2
+        self.cat_features_idx = [0, 1]
+        self.next_class_should_be_zero = False
+
+    def __iter__(self):
+        self._rng = check_random_state(self.seed)
+        self.next_class_should_be_zero = False
+
+        while True:
+            att_0 = False
+            att_1 = False
+            att_2 = 0.0
+            att_3 = 0.0
+            y = 0
+            desired_class_found = False
+            while not desired_class_found:
+                att_0 = self._rng.rand() >= 0.5
+                att_1 = self._rng.rand() >= 0.5
+                att_2 = self._rng.rand()
+                att_3 = self._rng.rand()
+
+                y = self._functions[self.classification_function](
+                    att_0, att_1, att_2, att_3
+                )
+
+                if not self.balance_classes:
+                    desired_class_found = True
+                else:
+                    if (self.next_class_should_be_zero and (y == 0)) or (
+                        (not self.next_class_should_be_zero) and (y == 1)
+                    ):
+                        desired_class_found = True
+                        self.next_class_should_be_zero = (
+                            not self.next_class_should_be_zero
+                        )
+
+            x = {0: att_0, 1: att_1, 2: att_2, 3: att_3}
+
+            yield x, y
+
+    @staticmethod
+    def _classification_function_zero(v: bool, w: bool, x: float, y: float):
+        z = y < 0.5 + 0.3 * np.sin(3 * np.pi * x)
+        return 0 if (v and w) or (v and z) or (w and z) else 1
+
+    @staticmethod
+    def _classification_function_one(v: bool, w: bool, x: float, y: float):
+        z = y < 0.5 + 0.3 * np.sin(3 * np.pi * x)
+        return 1 if (v == 1 and w == 1) or (v == 1 and z) or (w == 1 and z) else 0
+
+    def generate_drift(self):
+        """Generate drift by switching the classification function."""
+        self.classification_function = 1 - self.classification_function
```

### Comparing `river-0.8.0/river/datasets/synth/mv.py` & `river-0.9.0/river/datasets/synth/mv.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,133 +1,133 @@
-import random
-
-from .. import base
-
-
-class Mv(base.SyntheticDataset):
-    """Mv artificial dataset.
-
-    Artificial dataset composed of both nominal and numeric features, whose features
-    present co-dependencies. Originally described in [^1].
-
-    The features are generated using the following expressions:
-
-    - $x_1$: uniformly distributed over `[-5, 5]`.
-
-    - $x_2$: uniformly distributed over `[-15, -10]`.
-
-    - $x_3$:
-
-        * if $x_1 > 0$, $x_3 \\leftarrow$ `'green'`
-
-        * else $x_3 \\leftarrow$ `'red'` with probability $0.4$ and $x_3 \\leftarrow$ `'brown'`
-        with probability $0.6$.
-
-    - $x_4$:
-
-        * if $x_3 =$ `'green'`, $x_4 \\leftarrow x_1 + 2 x_2$
-
-        * else $x_4 = \\frac{x_1}{2}$ with probability $0.3$ and $x_4 = \\frac{x_2}{2}$
-        with probability $0.7$.
-
-    - $x_5$: uniformly distributed over `[-1, 1]`.
-
-    - $x_6 \\leftarrow x_4 \\times \\epsilon$, where $\\epsilon$ is uniformly distributed
-    over `[0, 5]`.
-
-    - $x_7$: `'yes'` with probability $0.3$, and `'no'` with probability $0.7$.
-
-    - $x_8$: `'normal'` if $x_5 < 0.5$ else `'large'`.
-
-    - $x_9$: uniformly distributed over `[100, 500]`.
-
-    - $x_{10}$: uniformly distributed integer over the interval `[1000, 1200]`.
-
-    The target value is generated using the following rules:
-
-    - if $x_2 > 2$, $y \\leftarrow 35 - 0.5 x_4$
-
-    - else if $-2 \\le x_4 \\le 2$, $y \\leftarrow 10 - 2 x_1$
-
-    - else if $x_7 =$ `'yes'`, $y \\leftarrow 3 - \\frac{x_1}{x_4}$
-
-    - else if $x_8 =$ `'normal'`, $y \\leftarrow x_6 + x_1$
-
-    - else $y \\leftarrow \\frac{x_1}{2}$.
-
-    Parameters
-    ----------
-    seed
-        Random seed number used for reproducibility.
-
-    Examples
-    --------
-    >>> from river import synth
-
-    >>> dataset = synth.Mv(seed=42)
-
-    >>> for x, y in dataset.take(5):
-    ...     print(list(x.values()), y)
-    [1.39, -14.87, 'green', -28.35, -0.44, -31.64, 'no', 'normal', 370.67, 1178.43] -30.25
-    [-4.13, -12.89, 'red', -2.06, 0.01, -0.27, 'yes', 'normal', 359.95, 1108.98] 1.00
-    [-2.79, -12.05, 'brown', -1.39, 0.61, -4.87, 'no', 'large', 162.19, 1191.44] 15.59
-    [-1.63, -14.53, 'red', -7.26, 0.20, -29.33, 'no', 'normal', 314.49, 1194.62] -30.96
-    [-1.21, -12.23, 'brown', -6.11, 0.72, -17.66, 'no', 'large', 118.32, 1045.57] -0.60
-
-
-    References
-    ----------
-    [^1]: [Mv in Luís Torgo regression datasets](https://www.dcc.fc.up.pt/~ltorgo/Regression/mv.html)
-
-    """
-
-    def __init__(self, seed: int = None):
-        super().__init__(task=base.REG, n_features=10)
-        self.seed = seed
-
-    def __iter__(self):
-
-        rng = random.Random(self.seed)
-
-        while True:
-            x = {1: rng.uniform(-5, 5), 2: rng.uniform(-15, -10)}
-
-            if x[1] > 0:
-                x[3] = "green"
-            else:
-                x[3] = rng.choices(population=["red", "brown"], weights=[0.4, 0.6])[0]
-
-            if x[3] == "green":
-                x[4] = x[1] + 2 * x[2]
-            else:
-                choice = rng.choices(population=[True, False], weights=[0.3, 0.7])[0]
-
-                if choice:
-                    x[4] = x[1] / 2
-                else:
-                    x[4] = x[2] / 2
-
-            x[5] = rng.uniform(-1, 1)
-
-            epsilon = rng.uniform(0, 5)
-            x[6] = x[4] * epsilon
-
-            x[7] = rng.choices(population=["yes", "no"], weights=[0.3, 0.7])[0]
-
-            x[8] = "normal" if x[5] < 0.5 else "large"
-
-            x[9] = rng.uniform(100, 500)
-
-            x[10] = rng.uniform(1000, 1200)
-
-            if x[2] > 2:
-                y = 35 - 0.5 * x[4]
-            elif -2 <= x[4] <= 2:
-                y = 10 - 2 * x[1]
-            elif x[7] == "yes":
-                y = 3 - (x[1] / x[4] if x[4] != 0 else 0)
-            elif x[8] == "normal":
-                y = x[6] + x[1]
-            else:
-                y = x[1] / 2
-
-            yield x, y
+import random
+
+from .. import base
+
+
+class Mv(base.SyntheticDataset):
+    """Mv artificial dataset.
+
+    Artificial dataset composed of both nominal and numeric features, whose features
+    present co-dependencies. Originally described in [^1].
+
+    The features are generated using the following expressions:
+
+    - $x_1$: uniformly distributed over `[-5, 5]`.
+
+    - $x_2$: uniformly distributed over `[-15, -10]`.
+
+    - $x_3$:
+
+        * if $x_1 > 0$, $x_3 \\leftarrow$ `'green'`
+
+        * else $x_3 \\leftarrow$ `'red'` with probability $0.4$ and $x_3 \\leftarrow$ `'brown'`
+        with probability $0.6$.
+
+    - $x_4$:
+
+        * if $x_3 =$ `'green'`, $x_4 \\leftarrow x_1 + 2 x_2$
+
+        * else $x_4 = \\frac{x_1}{2}$ with probability $0.3$ and $x_4 = \\frac{x_2}{2}$
+        with probability $0.7$.
+
+    - $x_5$: uniformly distributed over `[-1, 1]`.
+
+    - $x_6 \\leftarrow x_4 \\times \\epsilon$, where $\\epsilon$ is uniformly distributed
+    over `[0, 5]`.
+
+    - $x_7$: `'yes'` with probability $0.3$, and `'no'` with probability $0.7$.
+
+    - $x_8$: `'normal'` if $x_5 < 0.5$ else `'large'`.
+
+    - $x_9$: uniformly distributed over `[100, 500]`.
+
+    - $x_{10}$: uniformly distributed integer over the interval `[1000, 1200]`.
+
+    The target value is generated using the following rules:
+
+    - if $x_2 > 2$, $y \\leftarrow 35 - 0.5 x_4$
+
+    - else if $-2 \\le x_4 \\le 2$, $y \\leftarrow 10 - 2 x_1$
+
+    - else if $x_7 =$ `'yes'`, $y \\leftarrow 3 - \\frac{x_1}{x_4}$
+
+    - else if $x_8 =$ `'normal'`, $y \\leftarrow x_6 + x_1$
+
+    - else $y \\leftarrow \\frac{x_1}{2}$.
+
+    Parameters
+    ----------
+    seed
+        Random seed number used for reproducibility.
+
+    Examples
+    --------
+    >>> from river import synth
+
+    >>> dataset = synth.Mv(seed=42)
+
+    >>> for x, y in dataset.take(5):
+    ...     print(list(x.values()), y)
+    [1.39, -14.87, 'green', -28.35, -0.44, -31.64, 'no', 'normal', 370.67, 1178.43] -30.25
+    [-4.13, -12.89, 'red', -2.06, 0.01, -0.27, 'yes', 'normal', 359.95, 1108.98] 1.00
+    [-2.79, -12.05, 'brown', -1.39, 0.61, -4.87, 'no', 'large', 162.19, 1191.44] 15.59
+    [-1.63, -14.53, 'red', -7.26, 0.20, -29.33, 'no', 'normal', 314.49, 1194.62] -30.96
+    [-1.21, -12.23, 'brown', -6.11, 0.72, -17.66, 'no', 'large', 118.32, 1045.57] -0.60
+
+
+    References
+    ----------
+    [^1]: [Mv in Luís Torgo regression datasets](https://www.dcc.fc.up.pt/~ltorgo/Regression/mv.html)
+
+    """
+
+    def __init__(self, seed: int = None):
+        super().__init__(task=base.REG, n_features=10)
+        self.seed = seed
+
+    def __iter__(self):
+
+        rng = random.Random(self.seed)
+
+        while True:
+            x = {1: rng.uniform(-5, 5), 2: rng.uniform(-15, -10)}
+
+            if x[1] > 0:
+                x[3] = "green"
+            else:
+                x[3] = rng.choices(population=["red", "brown"], weights=[0.4, 0.6])[0]
+
+            if x[3] == "green":
+                x[4] = x[1] + 2 * x[2]
+            else:
+                choice = rng.choices(population=[True, False], weights=[0.3, 0.7])[0]
+
+                if choice:
+                    x[4] = x[1] / 2
+                else:
+                    x[4] = x[2] / 2
+
+            x[5] = rng.uniform(-1, 1)
+
+            epsilon = rng.uniform(0, 5)
+            x[6] = x[4] * epsilon
+
+            x[7] = rng.choices(population=["yes", "no"], weights=[0.3, 0.7])[0]
+
+            x[8] = "normal" if x[5] < 0.5 else "large"
+
+            x[9] = rng.uniform(100, 500)
+
+            x[10] = rng.uniform(1000, 1200)
+
+            if x[2] > 2:
+                y = 35 - 0.5 * x[4]
+            elif -2 <= x[4] <= 2:
+                y = 10 - 2 * x[1]
+            elif x[7] == "yes":
+                y = 3 - (x[1] / x[4] if x[4] != 0 else 0)
+            elif x[8] == "normal":
+                y = x[6] + x[1]
+            else:
+                y = x[1] / 2
+
+            yield x, y
```

### Comparing `river-0.8.0/river/datasets/synth/planes_2d.py` & `river-0.9.0/river/datasets/synth/planes_2d.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,73 +1,73 @@
-import random
-
-from .. import base
-
-
-class Planes2D(base.SyntheticDataset):
-    """2D Planes synthetic dataset.
-
-    This dataset is described in [^1] and was adapted from [^2]. The features are generated
-    using the following probabilities:
-
-    $$P(x_1 = -1) = P(x_1 = 1) = \\frac{1}{2}$$
-
-    $$P(x_m = -1) = P(x_m = 0) = P(x_m = 1) = \\frac{1}{3}, m=2,\\ldots, 10$$
-
-    The target value is defined by the following rule:
-
-    $$\\text{if}~x_1 = 1, y \\leftarrow 3 + 3x_2 + 2x_3 + x_4 + \\epsilon$$
-
-    $$\\text{if}~x_1 = -1, y \\leftarrow -3 + 3x_5 + 2x_6 + x_7 + \\epsilon$$
-
-    In the expressions, $\\epsilon \\sim \\mathcal{N}(0, 1)$, is the noise.
-
-    Parameters
-    ----------
-    seed
-        Random seed number used for reproducibility.
-
-    Examples
-    --------
-    >>> from river import synth
-
-    >>> dataset = synth.Planes2D(seed=42)
-
-    >>> for x, y in dataset.take(5):
-    ...     print(list(x.values()), y)
-    [-1, -1, 1, 0, -1, -1, -1, 1, -1, 1] -9.07
-    [1, -1, -1, -1, -1, -1, 1, 1, -1, 1] -4.25
-    [-1, 1, 1, 1, 1, 0, -1, 0, 1, 0] -0.95
-    [-1, 1, 0, 0, 0, -1, -1, 0, -1, -1] -6.10
-    [1, -1, 0, 0, 1, 0, -1, 1, 0, 1] 1.60
-
-    References
-    ----------
-    [^1]: [2DPlanes in Luís Torgo regression datasets](https://www.dcc.fc.up.pt/~ltorgo/Regression/2dplanes.html)
-    [^2]: Breiman, L., Friedman, J., Stone, C.J. and Olshen, R.A., 1984. Classification and
-    regression trees. CRC press.
-
-    """
-
-    def __init__(self, seed: int = None):
-        super().__init__(task=base.REG, n_features=10)
-        self.seed = seed
-
-    def __iter__(self):
-
-        rng = random.Random(self.seed)
-
-        while True:
-            x = {1: rng.choice([-1, 1])}
-
-            for m in range(2, 11):
-                x[m] = rng.randint(-1, 1)
-
-            if x[1] == 1:
-                y = 3 + 3 * x[2] + 2 * x[3] + x[4]
-            else:
-                y = -3 + 3 * x[5] + 2 * x[6] + x[7]
-
-            # Add noise
-            y += rng.gauss(mu=0, sigma=1)
-
-            yield x, y
+import random
+
+from .. import base
+
+
+class Planes2D(base.SyntheticDataset):
+    """2D Planes synthetic dataset.
+
+    This dataset is described in [^1] and was adapted from [^2]. The features are generated
+    using the following probabilities:
+
+    $$P(x_1 = -1) = P(x_1 = 1) = \\frac{1}{2}$$
+
+    $$P(x_m = -1) = P(x_m = 0) = P(x_m = 1) = \\frac{1}{3}, m=2,\\ldots, 10$$
+
+    The target value is defined by the following rule:
+
+    $$\\text{if}~x_1 = 1, y \\leftarrow 3 + 3x_2 + 2x_3 + x_4 + \\epsilon$$
+
+    $$\\text{if}~x_1 = -1, y \\leftarrow -3 + 3x_5 + 2x_6 + x_7 + \\epsilon$$
+
+    In the expressions, $\\epsilon \\sim \\mathcal{N}(0, 1)$, is the noise.
+
+    Parameters
+    ----------
+    seed
+        Random seed number used for reproducibility.
+
+    Examples
+    --------
+    >>> from river import synth
+
+    >>> dataset = synth.Planes2D(seed=42)
+
+    >>> for x, y in dataset.take(5):
+    ...     print(list(x.values()), y)
+    [-1, -1, 1, 0, -1, -1, -1, 1, -1, 1] -9.07
+    [1, -1, -1, -1, -1, -1, 1, 1, -1, 1] -4.25
+    [-1, 1, 1, 1, 1, 0, -1, 0, 1, 0] -0.95
+    [-1, 1, 0, 0, 0, -1, -1, 0, -1, -1] -6.10
+    [1, -1, 0, 0, 1, 0, -1, 1, 0, 1] 1.60
+
+    References
+    ----------
+    [^1]: [2DPlanes in Luís Torgo regression datasets](https://www.dcc.fc.up.pt/~ltorgo/Regression/2dplanes.html)
+    [^2]: Breiman, L., Friedman, J., Stone, C.J. and Olshen, R.A., 1984. Classification and
+    regression trees. CRC press.
+
+    """
+
+    def __init__(self, seed: int = None):
+        super().__init__(task=base.REG, n_features=10)
+        self.seed = seed
+
+    def __iter__(self):
+
+        rng = random.Random(self.seed)
+
+        while True:
+            x = {1: rng.choice([-1, 1])}
+
+            for m in range(2, 11):
+                x[m] = rng.randint(-1, 1)
+
+            if x[1] == 1:
+                y = 3 + 3 * x[2] + 2 * x[3] + x[4]
+            else:
+                y = -3 + 3 * x[5] + 2 * x[6] + x[7]
+
+            # Add noise
+            y += rng.gauss(mu=0, sigma=1)
+
+            yield x, y
```

### Comparing `river-0.8.0/river/datasets/synth/random_rbf.py` & `river-0.9.0/river/datasets/synth/random_rbf.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,295 +1,295 @@
-import warnings
-
-import numpy as np
-
-from river.utils.skmultiflow_utils import check_random_state
-
-from .. import base
-
-
-class RandomRBF(base.SyntheticDataset):
-    """Random Radial Basis Function generator.
-
-    Produces a radial basis function stream. A number of centroids, having a
-    random central position, a standard deviation, a class label and weight
-    are generated. A new sample is created by choosing one of the centroids at
-    random, taking into account their weights, and offsetting the attributes
-    in a random direction from the centroid's center. The offset length is
-    drawn from a Gaussian distribution.
-
-    This process will create a normally distributed hypersphere of samples on
-    the surrounds of each centroid.
-
-    Parameters
-    ----------
-    seed_model
-        Model's seed to generate centroids
-        If int, `seed` is used to seed the random number generator;
-        If RandomState instance, `seed` is the random number generator;
-        If None, the random number generator is the `RandomState` instance used
-        by `np.random`.
-    seed_sample
-        Sample's seed
-        If int, `seed` is used to seed the random number generator;
-        If RandomState instance, `seed` is the random number generator;
-        If None, the random number generator is the `RandomState` instance used
-        by `np.random`.
-    n_classes
-        The number of class labels to generate.
-    n_features
-        The number of numerical features to generate.
-    n_centroids
-        The number of centroids to generate.
-
-    Examples
-    --------
-    >>> from river import synth
-    >>>
-    >>> dataset = synth.RandomRBF(seed_model=42, seed_sample=42,
-    ...                           n_classes=4, n_features=4, n_centroids=20)
-    >>>
-    >>> for x, y in dataset.take(5):
-    ...     print(x, y)
-    {0: 0.9518, 1: 0.5263, 2: 0.2509, 3: 0.4177} 0
-    {0: 0.3383, 1: 0.8072, 2: 0.8051, 3: 0.4140} 3
-    {0: -0.2640, 1: 0.2275, 2: 0.6286, 3: -0.0532} 2
-    {0: 0.9050, 1: 0.6443, 2: 0.1270, 3: 0.4520} 2
-    {0: 0.1874, 1: 0.4348, 2: 0.9819, 3: -0.0459} 2
-
-    """
-
-    def __init__(
-        self,
-        seed_model: int or np.random.RandomState = None,
-        seed_sample: int or np.random.RandomState = None,
-        n_classes: int = 2,
-        n_features: int = 10,
-        n_centroids: int = 50,
-    ):
-        super().__init__(
-            n_features=n_features, n_classes=n_classes, n_outputs=1, task=base.MULTI_CLF
-        )
-        self.seed_sample = seed_sample
-        self.seed_model = seed_model
-        self.n_num_features = n_features
-        self.n_centroids = n_centroids
-        self.centroids = None
-        self.centroid_weights = None
-        self.target_values = [i for i in range(self.n_classes)]
-
-    def __iter__(self):
-        self._generate_centroids()
-        rng_sample = check_random_state(self.seed_sample)
-
-        while True:
-            x, y = self._generate_sample(rng_sample)
-            yield x, y
-
-    def _generate_sample(self, rng_sample: np.random.RandomState):
-        idx = random_index_based_on_weights(self.centroid_weights, rng_sample)
-        current_centroid = self.centroids[idx]
-        att_vals = dict()
-        magnitude = 0.0
-        for i in range(self.n_features):
-            att_vals[i] = (rng_sample.rand() * 2.0) - 1.0
-            magnitude += att_vals[i] * att_vals[i]
-        magnitude = np.sqrt(magnitude)
-        desired_mag = rng_sample.normal() * current_centroid.std_dev
-        scale = desired_mag / magnitude
-        x = {
-            i: current_centroid.centre[i] + att_vals[i] * scale
-            for i in range(self.n_features)
-        }
-        y = current_centroid.class_label
-        return x, y
-
-    def _generate_centroids(self):
-        """Generates centroids
-
-        Sequentially creates all the centroids, choosing at random a center,
-        a label, a standard deviation and a weight.
-
-        """
-        rng_model = check_random_state(self.seed_model)
-        self.centroids = []
-        self.centroid_weights = []
-        for i in range(self.n_centroids):
-            self.centroids.append(Centroid())
-            rand_centre = []
-            for j in range(self.n_num_features):
-                rand_centre.append(rng_model.rand())
-            self.centroids[i].centre = rand_centre
-            self.centroids[i].class_label = rng_model.randint(self.n_classes)
-            self.centroids[i].std_dev = rng_model.rand()
-            self.centroid_weights.append(rng_model.rand())
-
-
-class RandomRBFDrift(RandomRBF):
-    """Random Radial Basis Function generator with concept drift.
-
-    This class is an extension from the `RandomRBF` generator. Concept drift
-    can be introduced in instances of this class.
-
-    The drift is created by adding a "speed" to certain centroids. As the
-    samples are generated each of the moving centroids' centers is
-    changed by an amount determined by its speed.
-
-    Parameters
-    ----------
-    seed_model
-        Model's seed to generate centroids
-        If int, `seed` is used to seed the random number generator;
-        If RandomState instance, `seed` is the random number generator;
-        If None, the random number generator is the `RandomState` instance used
-        by `np.random`.
-    seed_sample
-        Sample's seed
-        If int, `seed` is used to seed the random number generator;
-        If RandomState instance, `seed` is the random number generator;
-        If None, the random number generator is the `RandomState` instance used
-        by `np.random`.
-    n_classes
-        The number of class labels to generate.
-    n_features
-        The number of numerical features to generate.
-    n_centroids
-        The number of centroids to generate.
-    change_speed
-        The concept drift speed.
-    n_drift_centroids
-        The number of centroids that will drift.
-
-    Examples
-    --------
-    >>> from river import synth
-    >>>
-    >>> dataset = synth.RandomRBFDrift(seed_model=42, seed_sample=42,
-    ...                                n_classes=4, n_features=4, n_centroids=20,
-    ...                                change_speed=0.87, n_drift_centroids=10)
-    >>>
-    >>> for x, y in dataset.take(5):
-    ...     print(x, y)
-    {0: 1.1965, 1: 0.5729, 2: 0.8607, 3: 0.5888} 0
-    {0: 0.3383, 1: 0.8072, 2: 0.8051, 3: 0.4140} 3
-    {0: 0.5362, 1: -0.2867, 2: 0.0962, 3: 0.8974} 2
-    {0: 1.1875, 1: 1.0385, 2: 0.8323, 3: -0.0553} 2
-    {0: 0.3256, 1: 0.9206, 2: 0.8595, 3: 0.5907} 2
-
-
-    """
-
-    def __init__(
-        self,
-        seed_model: int or np.random.RandomState = None,
-        seed_sample: int or np.random.RandomState = None,
-        n_classes: int = 2,
-        n_features: int = 10,
-        n_centroids: int = 50,
-        change_speed: float = 0.0,
-        n_drift_centroids: int = 50,
-    ):
-        super().__init__(
-            seed_model=seed_model,
-            seed_sample=seed_sample,
-            n_classes=n_classes,
-            n_features=n_features,
-            n_centroids=n_centroids,
-        )
-        self.change_speed = change_speed
-        if n_drift_centroids <= n_centroids:
-            self.n_drift_centroids = n_drift_centroids
-        else:
-            warnings.warn(
-                f"n_drift_centroids ({n_drift_centroids}) can not be larger than"
-                f"n_centroids ({n_centroids}). Will use n_centroids instead."
-            )
-            self.n_drift_centroids = n_centroids
-        self.centroid_speed = None
-
-    def __iter__(self):
-        self._generate_centroids()
-        rng_sample = check_random_state(self.seed_sample)
-
-        while True:
-            # Move centroids
-            for i in range(self.n_drift_centroids):
-                for j in range(self.n_features):
-                    self.centroids[i].centre[j] += (
-                        self.centroid_speed[i][j] * self.change_speed
-                    )
-
-                    if (self.centroids[i].centre[j] > 1) or (
-                        self.centroids[i].centre[j] < 0
-                    ):
-                        self.centroids[i].centre[j] = (
-                            1 if (self.centroids[i].centre[j] > 1) else 0
-                        )
-                        self.centroid_speed[i][j] = -self.centroid_speed[i][j]
-
-            x, y = self._generate_sample(rng_sample)
-            yield x, y
-
-    def _generate_centroids(self):
-        """Generates centroids
-
-        The centroids are generated just as it is done in the parent class,
-        an extra step is taken to introduce drift, if there is any.
-
-        To configure the drift, random offset speeds are chosen for
-        `n_drift_centroids` centroids. Finally, the speed is normalized.
-
-        """
-        super()._generate_centroids()
-        rng_model = check_random_state(self.seed_model)
-        self.centroid_speed = []
-
-        for i in range(self.n_drift_centroids):
-            rand_speed = np.zeros(self.n_features)
-            norm_speed = 0.0
-
-            for j in range(self.n_features):
-                rand_speed[j] = rng_model.rand()
-                norm_speed += rand_speed[j] * rand_speed[j]
-
-            norm_speed = np.sqrt(norm_speed)
-
-            for j in range(self.n_features):
-                rand_speed[j] /= norm_speed
-
-            self.centroid_speed.append(rand_speed)
-
-
-class Centroid:
-    """ Class that stores a centroid's attributes. """
-
-    def __init__(self):
-        self.centre = None
-        self.class_label = None
-        self.std_dev = None
-
-
-def random_index_based_on_weights(weights: list, random_state: np.random.RandomState):
-    """Generate a random index, based on index weights and a random number generator.
-
-    Parameters
-    ----------
-    weights
-        The weights of the centroid's indexes.
-
-    random_state
-        Random number generator instance.
-
-    Returns
-    -------
-    int
-        The generated index.
-
-    """
-    prob_sum = np.sum(weights)
-    val = random_state.rand() * prob_sum
-    index = 0
-    sum_value = 0.0
-    while (sum_value <= val) & (index < len(weights)):
-        sum_value += weights[index]
-        index += 1
-    return index - 1
+import warnings
+
+import numpy as np
+
+from river.utils.skmultiflow_utils import check_random_state
+
+from .. import base
+
+
+class RandomRBF(base.SyntheticDataset):
+    """Random Radial Basis Function generator.
+
+    Produces a radial basis function stream. A number of centroids, having a
+    random central position, a standard deviation, a class label and weight
+    are generated. A new sample is created by choosing one of the centroids at
+    random, taking into account their weights, and offsetting the attributes
+    in a random direction from the centroid's center. The offset length is
+    drawn from a Gaussian distribution.
+
+    This process will create a normally distributed hypersphere of samples on
+    the surrounds of each centroid.
+
+    Parameters
+    ----------
+    seed_model
+        Model's seed to generate centroids
+        If int, `seed` is used to seed the random number generator;
+        If RandomState instance, `seed` is the random number generator;
+        If None, the random number generator is the `RandomState` instance used
+        by `np.random`.
+    seed_sample
+        Sample's seed
+        If int, `seed` is used to seed the random number generator;
+        If RandomState instance, `seed` is the random number generator;
+        If None, the random number generator is the `RandomState` instance used
+        by `np.random`.
+    n_classes
+        The number of class labels to generate.
+    n_features
+        The number of numerical features to generate.
+    n_centroids
+        The number of centroids to generate.
+
+    Examples
+    --------
+    >>> from river import synth
+    >>>
+    >>> dataset = synth.RandomRBF(seed_model=42, seed_sample=42,
+    ...                           n_classes=4, n_features=4, n_centroids=20)
+    >>>
+    >>> for x, y in dataset.take(5):
+    ...     print(x, y)
+    {0: 0.9518, 1: 0.5263, 2: 0.2509, 3: 0.4177} 0
+    {0: 0.3383, 1: 0.8072, 2: 0.8051, 3: 0.4140} 3
+    {0: -0.2640, 1: 0.2275, 2: 0.6286, 3: -0.0532} 2
+    {0: 0.9050, 1: 0.6443, 2: 0.1270, 3: 0.4520} 2
+    {0: 0.1874, 1: 0.4348, 2: 0.9819, 3: -0.0459} 2
+
+    """
+
+    def __init__(
+        self,
+        seed_model: int or np.random.RandomState = None,
+        seed_sample: int or np.random.RandomState = None,
+        n_classes: int = 2,
+        n_features: int = 10,
+        n_centroids: int = 50,
+    ):
+        super().__init__(
+            n_features=n_features, n_classes=n_classes, n_outputs=1, task=base.MULTI_CLF
+        )
+        self.seed_sample = seed_sample
+        self.seed_model = seed_model
+        self.n_num_features = n_features
+        self.n_centroids = n_centroids
+        self.centroids = None
+        self.centroid_weights = None
+        self.target_values = [i for i in range(self.n_classes)]
+
+    def __iter__(self):
+        self._generate_centroids()
+        rng_sample = check_random_state(self.seed_sample)
+
+        while True:
+            x, y = self._generate_sample(rng_sample)
+            yield x, y
+
+    def _generate_sample(self, rng_sample: np.random.RandomState):
+        idx = random_index_based_on_weights(self.centroid_weights, rng_sample)
+        current_centroid = self.centroids[idx]
+        att_vals = dict()
+        magnitude = 0.0
+        for i in range(self.n_features):
+            att_vals[i] = (rng_sample.rand() * 2.0) - 1.0
+            magnitude += att_vals[i] * att_vals[i]
+        magnitude = np.sqrt(magnitude)
+        desired_mag = rng_sample.normal() * current_centroid.std_dev
+        scale = desired_mag / magnitude
+        x = {
+            i: current_centroid.centre[i] + att_vals[i] * scale
+            for i in range(self.n_features)
+        }
+        y = current_centroid.class_label
+        return x, y
+
+    def _generate_centroids(self):
+        """Generates centroids
+
+        Sequentially creates all the centroids, choosing at random a center,
+        a label, a standard deviation and a weight.
+
+        """
+        rng_model = check_random_state(self.seed_model)
+        self.centroids = []
+        self.centroid_weights = []
+        for i in range(self.n_centroids):
+            self.centroids.append(Centroid())
+            rand_centre = []
+            for j in range(self.n_num_features):
+                rand_centre.append(rng_model.rand())
+            self.centroids[i].centre = rand_centre
+            self.centroids[i].class_label = rng_model.randint(self.n_classes)
+            self.centroids[i].std_dev = rng_model.rand()
+            self.centroid_weights.append(rng_model.rand())
+
+
+class RandomRBFDrift(RandomRBF):
+    """Random Radial Basis Function generator with concept drift.
+
+    This class is an extension from the `RandomRBF` generator. Concept drift
+    can be introduced in instances of this class.
+
+    The drift is created by adding a "speed" to certain centroids. As the
+    samples are generated each of the moving centroids' centers is
+    changed by an amount determined by its speed.
+
+    Parameters
+    ----------
+    seed_model
+        Model's seed to generate centroids
+        If int, `seed` is used to seed the random number generator;
+        If RandomState instance, `seed` is the random number generator;
+        If None, the random number generator is the `RandomState` instance used
+        by `np.random`.
+    seed_sample
+        Sample's seed
+        If int, `seed` is used to seed the random number generator;
+        If RandomState instance, `seed` is the random number generator;
+        If None, the random number generator is the `RandomState` instance used
+        by `np.random`.
+    n_classes
+        The number of class labels to generate.
+    n_features
+        The number of numerical features to generate.
+    n_centroids
+        The number of centroids to generate.
+    change_speed
+        The concept drift speed.
+    n_drift_centroids
+        The number of centroids that will drift.
+
+    Examples
+    --------
+    >>> from river import synth
+    >>>
+    >>> dataset = synth.RandomRBFDrift(seed_model=42, seed_sample=42,
+    ...                                n_classes=4, n_features=4, n_centroids=20,
+    ...                                change_speed=0.87, n_drift_centroids=10)
+    >>>
+    >>> for x, y in dataset.take(5):
+    ...     print(x, y)
+    {0: 1.1965, 1: 0.5729, 2: 0.8607, 3: 0.5888} 0
+    {0: 0.3383, 1: 0.8072, 2: 0.8051, 3: 0.4140} 3
+    {0: 0.5362, 1: -0.2867, 2: 0.0962, 3: 0.8974} 2
+    {0: 1.1875, 1: 1.0385, 2: 0.8323, 3: -0.0553} 2
+    {0: 0.3256, 1: 0.9206, 2: 0.8595, 3: 0.5907} 2
+
+
+    """
+
+    def __init__(
+        self,
+        seed_model: int or np.random.RandomState = None,
+        seed_sample: int or np.random.RandomState = None,
+        n_classes: int = 2,
+        n_features: int = 10,
+        n_centroids: int = 50,
+        change_speed: float = 0.0,
+        n_drift_centroids: int = 50,
+    ):
+        super().__init__(
+            seed_model=seed_model,
+            seed_sample=seed_sample,
+            n_classes=n_classes,
+            n_features=n_features,
+            n_centroids=n_centroids,
+        )
+        self.change_speed = change_speed
+        if n_drift_centroids <= n_centroids:
+            self.n_drift_centroids = n_drift_centroids
+        else:
+            warnings.warn(
+                f"n_drift_centroids ({n_drift_centroids}) can not be larger than"
+                f"n_centroids ({n_centroids}). Will use n_centroids instead."
+            )
+            self.n_drift_centroids = n_centroids
+        self.centroid_speed = None
+
+    def __iter__(self):
+        self._generate_centroids()
+        rng_sample = check_random_state(self.seed_sample)
+
+        while True:
+            # Move centroids
+            for i in range(self.n_drift_centroids):
+                for j in range(self.n_features):
+                    self.centroids[i].centre[j] += (
+                        self.centroid_speed[i][j] * self.change_speed
+                    )
+
+                    if (self.centroids[i].centre[j] > 1) or (
+                        self.centroids[i].centre[j] < 0
+                    ):
+                        self.centroids[i].centre[j] = (
+                            1 if (self.centroids[i].centre[j] > 1) else 0
+                        )
+                        self.centroid_speed[i][j] = -self.centroid_speed[i][j]
+
+            x, y = self._generate_sample(rng_sample)
+            yield x, y
+
+    def _generate_centroids(self):
+        """Generates centroids
+
+        The centroids are generated just as it is done in the parent class,
+        an extra step is taken to introduce drift, if there is any.
+
+        To configure the drift, random offset speeds are chosen for
+        `n_drift_centroids` centroids. Finally, the speed is normalized.
+
+        """
+        super()._generate_centroids()
+        rng_model = check_random_state(self.seed_model)
+        self.centroid_speed = []
+
+        for i in range(self.n_drift_centroids):
+            rand_speed = np.zeros(self.n_features)
+            norm_speed = 0.0
+
+            for j in range(self.n_features):
+                rand_speed[j] = rng_model.rand()
+                norm_speed += rand_speed[j] * rand_speed[j]
+
+            norm_speed = np.sqrt(norm_speed)
+
+            for j in range(self.n_features):
+                rand_speed[j] /= norm_speed
+
+            self.centroid_speed.append(rand_speed)
+
+
+class Centroid:
+    """ Class that stores a centroid's attributes. """
+
+    def __init__(self):
+        self.centre = None
+        self.class_label = None
+        self.std_dev = None
+
+
+def random_index_based_on_weights(weights: list, random_state: np.random.RandomState):
+    """Generate a random index, based on index weights and a random number generator.
+
+    Parameters
+    ----------
+    weights
+        The weights of the centroid's indexes.
+
+    random_state
+        Random number generator instance.
+
+    Returns
+    -------
+    int
+        The generated index.
+
+    """
+    prob_sum = np.sum(weights)
+    val = random_state.rand() * prob_sum
+    index = 0
+    sum_value = 0.0
+    while (sum_value <= val) & (index < len(weights)):
+        sum_value += weights[index]
+        index += 1
+    return index - 1
```

### Comparing `river-0.8.0/river/datasets/synth/random_tree.py` & `river-0.9.0/river/datasets/synth/random_tree.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,278 +1,278 @@
-import numpy as np
-
-from river.utils.skmultiflow_utils import check_random_state
-
-from .. import base
-
-
-class RandomTree(base.SyntheticDataset):
-    """Random Tree generator.
-
-    This generator is based on [^1]. The generator creates a random
-    tree by splitting features at random and setting labels at its leaves.
-
-    The tree structure is composed of node objects, which can be either inner
-    nodes or leaf nodes. The choice comes as a function of the parameters
-    passed to its initializer.
-
-    Since the concepts are generated and classified according to a tree
-    structure, in theory, it should favor decision tree learners.
-
-    Parameters
-    ----------
-    seed_tree
-        Seed for random generation of tree.
-    seed_sample
-        Seed for random generation of instances.
-    n_classes
-        The number of classes to generate.
-    n_num_features
-        The number of numerical features to generate.
-    n_cat_features
-        The number of categorical features to generate.
-    n_categories_per_feature
-        The number of values to generate per categorical feature.
-    max_tree_depth
-        The maximum depth of the tree concept.
-    first_leaf_level
-        The first level of the tree above `max_tree_depth` that can have leaves.
-    fraction_leaves_per_level
-        The fraction of leaves per level from `first_leaf_level` onwards.
-
-    Examples
-    --------
-
-    >>> from river import synth
-
-    >>> dataset = synth.RandomTree(seed_tree=42, seed_sample=42, n_classes=2,
-    ...                            n_num_features=2, n_cat_features=2,
-    ...                            n_categories_per_feature=2, max_tree_depth=6,
-    ...                            first_leaf_level=3, fraction_leaves_per_level=0.15)
-
-    >>> for x, y in dataset.take(5):
-    ...     print(x, y)
-    {'x_num_0': 0.3745, 'x_num_1': 0.9507, 'x_cat_0': 0, 'x_cat_1': 1} 1
-    {'x_num_0': 0.5986, 'x_num_1': 0.1560, 'x_cat_0': 0, 'x_cat_1': 0} 1
-    {'x_num_0': 0.0580, 'x_num_1': 0.8661, 'x_cat_0': 1, 'x_cat_1': 1} 0
-    {'x_num_0': 0.7080, 'x_num_1': 0.0205, 'x_cat_0': 1, 'x_cat_1': 1} 0
-    {'x_num_0': 0.8324, 'x_num_1': 0.2123, 'x_cat_0': 1, 'x_cat_1': 1} 0
-
-    References
-    ----------
-    [^1]: Domingos, Pedro, and Geoff Hulten. "Mining high-speed data streams."
-          In Proceedings of the sixth ACM SIGKDD international conference on
-          Knowledge discovery and data mining, pp. 71-80. 2000.
-
-    """
-
-    def __init__(
-        self,
-        seed_tree: int or np.random.RandomState = None,
-        seed_sample: int or np.random.RandomState = None,
-        n_classes: int = 2,
-        n_num_features: int = 5,
-        n_cat_features: int = 5,
-        n_categories_per_feature: int = 5,
-        max_tree_depth: int = 5,
-        first_leaf_level: int = 3,
-        fraction_leaves_per_level: float = 0.15,
-    ):
-        super().__init__(
-            n_features=n_num_features + n_cat_features,
-            n_classes=n_classes,
-            n_outputs=1,
-            task=base.MULTI_CLF,
-        )
-
-        self.seed_tree = seed_tree
-        self.seed_sample = seed_sample
-        self.n_num_features = n_num_features
-        self.n_cat_features = n_cat_features
-        self.n_categories_per_feature = n_categories_per_feature
-        self.max_tree_depth = max_tree_depth
-        self.first_leaf_level = first_leaf_level
-        self.fraction_leaves_per_level = fraction_leaves_per_level
-        self.tree_root = None
-
-        self.features_num = [f"x_num_{i}" for i in range(self.n_num_features)]
-        self.features_cat = [f"x_cat_{i}" for i in range(self.n_cat_features)]
-        self.feature_names = self.features_num + self.features_cat
-        self.target_values = [i for i in range(self.n_classes)]
-
-    def _generate_random_tree(self):
-        """
-        Generates the random tree, starting from the root node and following
-        the constraints passed as parameters to the initializer.
-
-        The tree is recursively generated, node by node, until it reaches the
-        maximum tree depth.
-        """
-        rng_tree = check_random_state(self.seed_tree)
-        candidate_features = np.arange(self.n_num_features + self.n_cat_features)
-        min_numeric_values = np.zeros(self.n_num_features)
-        max_numeric_values = np.ones(self.n_num_features)
-
-        self.tree_root = self._generate_random_tree_node(
-            0, candidate_features, min_numeric_values, max_numeric_values, rng_tree
-        )
-
-    def _generate_random_tree_node(
-        self,
-        current_depth: int,
-        candidate_features: np.ndarray,
-        min_numeric_value: np.ndarray,
-        max_numeric_value: np.ndarray,
-        rng: np.random.RandomState,
-    ):
-        """
-        Creates a node, choosing at random the splitting feature and value.
-        Then recursively generates its children. If the split feature is a
-        numerical feature there will be two children nodes (left and right),
-        one for samples where the value for the split feature is smaller than
-        the split value, and one for the other case. For categorical features,
-        the number of children generated is equal to the number of categories
-        per categorical feature.
-
-        Once the recursion passes the leaf_node minimum depth, it probabilistic
-        chooses if the node is a leaf_node or not. If not, the recursion follow
-        the same way as before. If it decides the node is a leaf_node, a class
-        label is chosen for the leaf_node at random.
-
-        Finally, if the current_depth is equal or higher than the tree
-        maximum depth, a leaf_node node is immediately returned.
-
-        Parameters
-        ----------
-        current_depth
-            The current tree depth.
-        candidate_features
-            Candidate features
-        min_numeric_value
-            The minimum numeric feature value, on this branch of the tree.
-        max_numeric_value
-            The minimum numeric feature value, on this branch of the tree.
-        rng
-            A numpy random number generator instance.
-
-        Notes
-        -----
-        If the splitting attribute of a node happens to be a nominal attribute
-        we guarantee that none of its children will split on the same attribute,
-        as it would have no use for that split.
-
-        """
-        # Stop recursive call
-        if (current_depth >= self.max_tree_depth) or (
-            (current_depth >= self.first_leaf_level)
-            and (self.fraction_leaves_per_level >= (1.0 - rng.rand()))
-        ):
-            leaf_node = TreeNode()
-            leaf_node.class_label = rng.randint(self.n_classes)
-            return leaf_node
-        # Create a new node
-        split_node = TreeNode()
-        chosen_feature = rng.randint(candidate_features.size)
-        if chosen_feature < self.n_num_features:
-            # Chosen feature is numeric
-            split_node.split_feature_idx = chosen_feature
-            min_val = min_numeric_value[chosen_feature]
-            max_val = max_numeric_value[chosen_feature]
-            split_node.split_feature_val = (max_val - min_val) * rng.rand() + min_val
-            # Left node
-            new_max_value = np.array(max_numeric_value)
-            new_max_value[chosen_feature] = split_node.split_feature_val
-            split_node.children.append(
-                self._generate_random_tree_node(
-                    current_depth + 1,
-                    candidate_features,
-                    min_numeric_value,
-                    new_max_value,
-                    rng,
-                )
-            )
-            # Right node
-            new_min_value = np.array(min_numeric_value)
-            new_min_value[chosen_feature] = split_node.split_feature_val
-            split_node.children.append(
-                self._generate_random_tree_node(
-                    current_depth + 1,
-                    candidate_features,
-                    new_min_value,
-                    max_numeric_value,
-                    rng,
-                )
-            )
-        else:
-            # Chosen feature is categorical
-            split_node.split_feature_idx = candidate_features[chosen_feature]
-            # Remove chosen features from candidates
-            new_candidates = np.delete(
-                candidate_features,
-                np.argwhere(candidate_features == split_node.split_feature_idx),
-            )
-            # Generate children per category
-            for i in range(self.n_categories_per_feature):
-                split_node.children.append(
-                    self._generate_random_tree_node(
-                        current_depth + 1,
-                        new_candidates,
-                        min_numeric_value,
-                        max_numeric_value,
-                        rng,
-                    )
-                )
-        return split_node
-
-    def _classify_instance(self, node, x):
-        if len(node.children) == 0:
-            # Reached a leaf node
-            return node.class_label
-        feature = self.feature_names[node.split_feature_idx]
-        if node.split_feature_idx < self.n_num_features:
-            idx = 0 if x[feature] < node.split_feature_val else 1
-            return self._classify_instance(node.children[idx], x)
-        else:
-            idx = x[feature]
-            return self._classify_instance(node.children[idx], x)
-
-    def __iter__(self):
-        rng_sample = check_random_state(self.seed_sample)
-        # Generate random tree model which will be used to classify instances
-        self._generate_random_tree()
-
-        # Randomly generate features, and then classify the resulting instance.
-        while True:
-            x = dict()
-            for feature in self.features_num:
-                x[feature] = rng_sample.rand()
-            for feature in self.features_cat:
-                x[feature] = rng_sample.randint(self.n_categories_per_feature)
-            y = self._classify_instance(self.tree_root, x)
-            yield x, y
-
-
-class TreeNode:
-    """Class that stores the attributes of a tree node.
-
-    Parameters
-    ----------
-    class_label
-        Class label if the node is a leaf node.
-
-    split_feature_idx
-        Feature index for the split, if the node is a split node.
-
-    split_feature_val
-        Feature value for the split, if the node is a split node.
-    """
-
-    def __init__(
-        self,
-        class_label: int = None,
-        split_feature_idx: int = None,
-        split_feature_val: int or float = None,
-    ):
-        self.class_label = class_label
-        self.split_feature_idx = split_feature_idx
-        self.split_feature_val = split_feature_val
-        self.children = []
+import numpy as np
+
+from river.utils.skmultiflow_utils import check_random_state
+
+from .. import base
+
+
+class RandomTree(base.SyntheticDataset):
+    """Random Tree generator.
+
+    This generator is based on [^1]. The generator creates a random
+    tree by splitting features at random and setting labels at its leaves.
+
+    The tree structure is composed of node objects, which can be either inner
+    nodes or leaf nodes. The choice comes as a function of the parameters
+    passed to its initializer.
+
+    Since the concepts are generated and classified according to a tree
+    structure, in theory, it should favor decision tree learners.
+
+    Parameters
+    ----------
+    seed_tree
+        Seed for random generation of tree.
+    seed_sample
+        Seed for random generation of instances.
+    n_classes
+        The number of classes to generate.
+    n_num_features
+        The number of numerical features to generate.
+    n_cat_features
+        The number of categorical features to generate.
+    n_categories_per_feature
+        The number of values to generate per categorical feature.
+    max_tree_depth
+        The maximum depth of the tree concept.
+    first_leaf_level
+        The first level of the tree above `max_tree_depth` that can have leaves.
+    fraction_leaves_per_level
+        The fraction of leaves per level from `first_leaf_level` onwards.
+
+    Examples
+    --------
+
+    >>> from river import synth
+
+    >>> dataset = synth.RandomTree(seed_tree=42, seed_sample=42, n_classes=2,
+    ...                            n_num_features=2, n_cat_features=2,
+    ...                            n_categories_per_feature=2, max_tree_depth=6,
+    ...                            first_leaf_level=3, fraction_leaves_per_level=0.15)
+
+    >>> for x, y in dataset.take(5):
+    ...     print(x, y)
+    {'x_num_0': 0.3745, 'x_num_1': 0.9507, 'x_cat_0': 0, 'x_cat_1': 1} 1
+    {'x_num_0': 0.5986, 'x_num_1': 0.1560, 'x_cat_0': 0, 'x_cat_1': 0} 1
+    {'x_num_0': 0.0580, 'x_num_1': 0.8661, 'x_cat_0': 1, 'x_cat_1': 1} 0
+    {'x_num_0': 0.7080, 'x_num_1': 0.0205, 'x_cat_0': 1, 'x_cat_1': 1} 0
+    {'x_num_0': 0.8324, 'x_num_1': 0.2123, 'x_cat_0': 1, 'x_cat_1': 1} 0
+
+    References
+    ----------
+    [^1]: Domingos, Pedro, and Geoff Hulten. "Mining high-speed data streams."
+          In Proceedings of the sixth ACM SIGKDD international conference on
+          Knowledge discovery and data mining, pp. 71-80. 2000.
+
+    """
+
+    def __init__(
+        self,
+        seed_tree: int or np.random.RandomState = None,
+        seed_sample: int or np.random.RandomState = None,
+        n_classes: int = 2,
+        n_num_features: int = 5,
+        n_cat_features: int = 5,
+        n_categories_per_feature: int = 5,
+        max_tree_depth: int = 5,
+        first_leaf_level: int = 3,
+        fraction_leaves_per_level: float = 0.15,
+    ):
+        super().__init__(
+            n_features=n_num_features + n_cat_features,
+            n_classes=n_classes,
+            n_outputs=1,
+            task=base.MULTI_CLF,
+        )
+
+        self.seed_tree = seed_tree
+        self.seed_sample = seed_sample
+        self.n_num_features = n_num_features
+        self.n_cat_features = n_cat_features
+        self.n_categories_per_feature = n_categories_per_feature
+        self.max_tree_depth = max_tree_depth
+        self.first_leaf_level = first_leaf_level
+        self.fraction_leaves_per_level = fraction_leaves_per_level
+        self.tree_root = None
+
+        self.features_num = [f"x_num_{i}" for i in range(self.n_num_features)]
+        self.features_cat = [f"x_cat_{i}" for i in range(self.n_cat_features)]
+        self.feature_names = self.features_num + self.features_cat
+        self.target_values = [i for i in range(self.n_classes)]
+
+    def _generate_random_tree(self):
+        """
+        Generates the random tree, starting from the root node and following
+        the constraints passed as parameters to the initializer.
+
+        The tree is recursively generated, node by node, until it reaches the
+        maximum tree depth.
+        """
+        rng_tree = check_random_state(self.seed_tree)
+        candidate_features = np.arange(self.n_num_features + self.n_cat_features)
+        min_numeric_values = np.zeros(self.n_num_features)
+        max_numeric_values = np.ones(self.n_num_features)
+
+        self.tree_root = self._generate_random_tree_node(
+            0, candidate_features, min_numeric_values, max_numeric_values, rng_tree
+        )
+
+    def _generate_random_tree_node(
+        self,
+        current_depth: int,
+        candidate_features: np.ndarray,
+        min_numeric_value: np.ndarray,
+        max_numeric_value: np.ndarray,
+        rng: np.random.RandomState,
+    ):
+        """
+        Creates a node, choosing at random the splitting feature and value.
+        Then recursively generates its children. If the split feature is a
+        numerical feature there will be two children nodes (left and right),
+        one for samples where the value for the split feature is smaller than
+        the split value, and one for the other case. For categorical features,
+        the number of children generated is equal to the number of categories
+        per categorical feature.
+
+        Once the recursion passes the leaf_node minimum depth, it probabilistic
+        chooses if the node is a leaf_node or not. If not, the recursion follow
+        the same way as before. If it decides the node is a leaf_node, a class
+        label is chosen for the leaf_node at random.
+
+        Finally, if the current_depth is equal or higher than the tree
+        maximum depth, a leaf_node node is immediately returned.
+
+        Parameters
+        ----------
+        current_depth
+            The current tree depth.
+        candidate_features
+            Candidate features
+        min_numeric_value
+            The minimum numeric feature value, on this branch of the tree.
+        max_numeric_value
+            The minimum numeric feature value, on this branch of the tree.
+        rng
+            A numpy random number generator instance.
+
+        Notes
+        -----
+        If the splitting attribute of a node happens to be a nominal attribute
+        we guarantee that none of its children will split on the same attribute,
+        as it would have no use for that split.
+
+        """
+        # Stop recursive call
+        if (current_depth >= self.max_tree_depth) or (
+            (current_depth >= self.first_leaf_level)
+            and (self.fraction_leaves_per_level >= (1.0 - rng.rand()))
+        ):
+            leaf_node = TreeNode()
+            leaf_node.class_label = rng.randint(self.n_classes)
+            return leaf_node
+        # Create a new node
+        split_node = TreeNode()
+        chosen_feature = rng.randint(candidate_features.size)
+        if chosen_feature < self.n_num_features:
+            # Chosen feature is numeric
+            split_node.split_feature_idx = chosen_feature
+            min_val = min_numeric_value[chosen_feature]
+            max_val = max_numeric_value[chosen_feature]
+            split_node.split_feature_val = (max_val - min_val) * rng.rand() + min_val
+            # Left node
+            new_max_value = np.array(max_numeric_value)
+            new_max_value[chosen_feature] = split_node.split_feature_val
+            split_node.children.append(
+                self._generate_random_tree_node(
+                    current_depth + 1,
+                    candidate_features,
+                    min_numeric_value,
+                    new_max_value,
+                    rng,
+                )
+            )
+            # Right node
+            new_min_value = np.array(min_numeric_value)
+            new_min_value[chosen_feature] = split_node.split_feature_val
+            split_node.children.append(
+                self._generate_random_tree_node(
+                    current_depth + 1,
+                    candidate_features,
+                    new_min_value,
+                    max_numeric_value,
+                    rng,
+                )
+            )
+        else:
+            # Chosen feature is categorical
+            split_node.split_feature_idx = candidate_features[chosen_feature]
+            # Remove chosen features from candidates
+            new_candidates = np.delete(
+                candidate_features,
+                np.argwhere(candidate_features == split_node.split_feature_idx),
+            )
+            # Generate children per category
+            for i in range(self.n_categories_per_feature):
+                split_node.children.append(
+                    self._generate_random_tree_node(
+                        current_depth + 1,
+                        new_candidates,
+                        min_numeric_value,
+                        max_numeric_value,
+                        rng,
+                    )
+                )
+        return split_node
+
+    def _classify_instance(self, node, x):
+        if len(node.children) == 0:
+            # Reached a leaf node
+            return node.class_label
+        feature = self.feature_names[node.split_feature_idx]
+        if node.split_feature_idx < self.n_num_features:
+            idx = 0 if x[feature] < node.split_feature_val else 1
+            return self._classify_instance(node.children[idx], x)
+        else:
+            idx = x[feature]
+            return self._classify_instance(node.children[idx], x)
+
+    def __iter__(self):
+        rng_sample = check_random_state(self.seed_sample)
+        # Generate random tree model which will be used to classify instances
+        self._generate_random_tree()
+
+        # Randomly generate features, and then classify the resulting instance.
+        while True:
+            x = dict()
+            for feature in self.features_num:
+                x[feature] = rng_sample.rand()
+            for feature in self.features_cat:
+                x[feature] = rng_sample.randint(self.n_categories_per_feature)
+            y = self._classify_instance(self.tree_root, x)
+            yield x, y
+
+
+class TreeNode:
+    """Class that stores the attributes of a tree node.
+
+    Parameters
+    ----------
+    class_label
+        Class label if the node is a leaf node.
+
+    split_feature_idx
+        Feature index for the split, if the node is a split node.
+
+    split_feature_val
+        Feature value for the split, if the node is a split node.
+    """
+
+    def __init__(
+        self,
+        class_label: int = None,
+        split_feature_idx: int = None,
+        split_feature_val: int or float = None,
+    ):
+        self.class_label = class_label
+        self.split_feature_idx = split_feature_idx
+        self.split_feature_val = split_feature_val
+        self.children = []
```

### Comparing `river-0.8.0/river/datasets/synth/sea.py` & `river-0.9.0/river/datasets/synth/sea.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,81 +1,81 @@
-import random
-
-from .. import base
-
-
-class SEA(base.SyntheticDataset):
-    """SEA synthetic dataset.
-
-    Implementation of the data stream with abrupt drift described in [^1]. Each observation is
-    composed of 3 features. Only the first two features are relevant. The target is binary, and is
-    positive if the sum of the features exceeds a certain threshold. There are 4 thresholds to
-    choose from. Concept drift can be introduced by switching the threshold anytime during the
-    stream.
-
-    * **Variant 0**: `True` if $att1 + att2 > 8$
-
-    * **Variant 1**: `True` if $att1 + att2 > 9$
-
-    * **Variant 2**: `True` if $att1 + att2 > 7$
-
-    * **Variant 3**: `True` if $att1 + att2 > 9.5$
-
-    Parameters
-    ----------
-    variant
-        Determines the classification function to use. Possible choices are 0, 1, 2, 3.
-    noise
-        Determines the amount of observations for which the target sign will be flipped.
-    seed
-        Random seed number used for reproducibility.
-
-    Examples
-    --------
-
-    >>> from river import synth
-
-    >>> dataset = synth.SEA(variant=0, seed=42)
-
-    >>> for x, y in dataset.take(5):
-    ...     print(x, y)
-    {0: 6.39426, 1: 0.25010, 2: 2.75029} False
-    {0: 2.23210, 1: 7.36471, 2: 6.76699} True
-    {0: 8.92179, 1: 0.86938, 2: 4.21921} True
-    {0: 0.29797, 1: 2.18637, 2: 5.05355} False
-    {0: 0.26535, 1: 1.98837, 2: 6.49884} False
-
-    References
-    ----------
-    [^1]: [A Streaming Ensemble Algorithm (SEA) for Large-Scale Classification](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.482.3991&rep=rep1&type=pdf)
-
-    """
-
-    def __init__(self, variant=0, noise=0.0, seed: int = None):
-
-        super().__init__(n_features=3, task=base.BINARY_CLF)
-
-        if variant not in (0, 1, 2, 3):
-            raise ValueError("Unknown variant, possible choices are: 0, 1, 2, 3")
-
-        self.variant = variant
-        self.noise = noise
-        self.seed = seed
-        self._threshold = {0: 8, 1: 9, 2: 7, 3: 9.5}[variant]
-
-    def __iter__(self):
-
-        rng = random.Random(self.seed)
-
-        while True:
-
-            x = {i: rng.uniform(0, 10) for i in range(3)}
-            y = x[0] + x[1] > self._threshold
-
-            if self.noise and rng.random() < self.noise:
-                y = not y
-
-            yield x, y
-
-    @property
-    def _repr_content(self):
-        return {**super()._repr_content, "Variant": str(self.variant)}
+import random
+
+from .. import base
+
+
+class SEA(base.SyntheticDataset):
+    """SEA synthetic dataset.
+
+    Implementation of the data stream with abrupt drift described in [^1]. Each observation is
+    composed of 3 features. Only the first two features are relevant. The target is binary, and is
+    positive if the sum of the features exceeds a certain threshold. There are 4 thresholds to
+    choose from. Concept drift can be introduced by switching the threshold anytime during the
+    stream.
+
+    * **Variant 0**: `True` if $att1 + att2 > 8$
+
+    * **Variant 1**: `True` if $att1 + att2 > 9$
+
+    * **Variant 2**: `True` if $att1 + att2 > 7$
+
+    * **Variant 3**: `True` if $att1 + att2 > 9.5$
+
+    Parameters
+    ----------
+    variant
+        Determines the classification function to use. Possible choices are 0, 1, 2, 3.
+    noise
+        Determines the amount of observations for which the target sign will be flipped.
+    seed
+        Random seed number used for reproducibility.
+
+    Examples
+    --------
+
+    >>> from river import synth
+
+    >>> dataset = synth.SEA(variant=0, seed=42)
+
+    >>> for x, y in dataset.take(5):
+    ...     print(x, y)
+    {0: 6.39426, 1: 0.25010, 2: 2.75029} False
+    {0: 2.23210, 1: 7.36471, 2: 6.76699} True
+    {0: 8.92179, 1: 0.86938, 2: 4.21921} True
+    {0: 0.29797, 1: 2.18637, 2: 5.05355} False
+    {0: 0.26535, 1: 1.98837, 2: 6.49884} False
+
+    References
+    ----------
+    [^1]: [A Streaming Ensemble Algorithm (SEA) for Large-Scale Classification](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.482.3991&rep=rep1&type=pdf)
+
+    """
+
+    def __init__(self, variant=0, noise=0.0, seed: int = None):
+
+        super().__init__(n_features=3, task=base.BINARY_CLF)
+
+        if variant not in (0, 1, 2, 3):
+            raise ValueError("Unknown variant, possible choices are: 0, 1, 2, 3")
+
+        self.variant = variant
+        self.noise = noise
+        self.seed = seed
+        self._threshold = {0: 8, 1: 9, 2: 7, 3: 9.5}[variant]
+
+    def __iter__(self):
+
+        rng = random.Random(self.seed)
+
+        while True:
+
+            x = {i: rng.uniform(0, 10) for i in range(3)}
+            y = x[0] + x[1] > self._threshold
+
+            if self.noise and rng.random() < self.noise:
+                y = not y
+
+            yield x, y
+
+    @property
+    def _repr_content(self):
+        return {**super()._repr_content, "Variant": str(self.variant)}
```

### Comparing `river-0.8.0/river/datasets/synth/stagger.py` & `river-0.9.0/river/datasets/synth/stagger.py`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,158 +1,158 @@
-import numpy as np
-
-from river.utils.skmultiflow_utils import check_random_state
-
-from .. import base
-
-
-class STAGGER(base.SyntheticDataset):
-    """STAGGER concepts stream generator.
-
-    This generator is an implementation of the dara stream with abrupt concept
-    drift, as described in [^1].
-
-    The STAGGER concepts are boolean functions `f` with three features
-    describing objects: size (small, medium and large), shape (circle, square
-    and triangle) and colour (red, blue and green).
-
-    `f` options:
-
-    0. `True` if the size is small and the color is red.
-
-    1. `True` if the color is green or the shape is a circle.
-
-    2. `True` if the size is medium or large
-
-    Concept drift can be introduced by changing the classification function.
-    This can be done manually or using `ConceptDriftStream`.
-
-    One important feature is the possibility to balance classes, which
-    means the class distribution will tend to a uniform one.
-
-    Parameters
-    ----------
-    classification_function
-        Classification functions to use. From 0 to 2.
-    seed
-        If int, `seed` is used to seed the random number generator;
-        If RandomState instance, `seed` is the random number generator;
-        If None, the random number generator is the `RandomState` instance used
-        by `np.random`.
-    balance_classes
-        Whether to balance classes or not. If balanced, the class
-        distribution will converge to an uniform distribution.
-
-    Examples
-    --------
-    >>> from river import synth
-
-    >>> dataset = synth.STAGGER(classification_function = 2, seed = 112,
-    ...                      balance_classes = False)
-
-    >>> for x, y in dataset.take(5):
-    ...     print(x, y)
-    {'size': 0, 'color': 0, 'shape': 2} 0
-    {'size': 1, 'color': 0, 'shape': 1} 1
-    {'size': 0, 'color': 0, 'shape': 0} 0
-    {'size': 1, 'color': 2, 'shape': 0} 1
-    {'size': 1, 'color': 0, 'shape': 2} 1
-
-    Notes
-    -----
-    The sample generation works as follows: The 3 attributes are
-    generated with the random number generator. The classification function
-    defines whether to classify the instance as class 0 or class 1. Finally,
-    data is balanced, if this option is set by the user.
-
-    References
-    ----------
-    [^1]: Schlimmer, J. C., & Granger, R. H. (1986). Incremental learning
-          from noisy data. Machine learning, 1(3), 317-354.
-
-    """
-
-    def __init__(
-        self,
-        classification_function: int = 0,
-        seed: int or np.random.RandomState = None,
-        balance_classes: bool = False,
-    ):
-        super().__init__(n_features=3, n_classes=2, n_outputs=1, task=base.BINARY_CLF)
-
-        # Classification functions to use
-        self._functions = [
-            self._classification_function_zero,
-            self._classification_function_one,
-            self._classification_function_two,
-        ]
-        if classification_function not in range(3):
-            raise ValueError(
-                f"Invalid classification_function {classification_function}. "
-                "Valid values are: 0, 1, 2."
-            )
-        self.classification_function = classification_function
-        self.seed = seed
-        self.balance_classes = balance_classes
-        self.n_cat_features = 3
-        self._rng = None  # This is the actual random_state object used internally
-        self.next_class_should_be_zero = False
-
-        self.feature_names = ["size", "color", "shape"]
-        self.size_labels = {0: "small", 1: "medium", 2: "large"}
-        self.color_labels = {0: "red", 1: "blue", 2: "green"}
-        self.shape_labels = {0: "circle", 1: "square", 2: "triangle"}
-        self.target_values = [i for i in range(self.n_classes)]
-
-    def __iter__(self):
-        self._rng = check_random_state(self.seed)
-        self.next_class_should_be_zero = False
-
-        while True:
-            size = 0
-            color = 0
-            shape = 0
-            y = 0
-            desired_class_found = False
-            while not desired_class_found:
-                size = self._rng.randint(3)
-                color = self._rng.randint(3)
-                shape = self._rng.randint(3)
-
-                y = self._functions[self.classification_function](size, color, shape)
-
-                if not self.balance_classes:
-                    desired_class_found = True
-                else:
-                    if (self.next_class_should_be_zero and (y == 0)) or (
-                        (not self.next_class_should_be_zero) and (y == 1)
-                    ):
-                        desired_class_found = True
-                        self.next_class_should_be_zero = (
-                            not self.next_class_should_be_zero
-                        )
-
-            x = {"size": size, "color": color, "shape": shape}
-
-            yield x, y
-
-    def generate_drift(self):
-        """Generate drift by switching the classification function at random."""
-        new_function = self._rng.randint(3)
-        while new_function == self.classification_function:
-            new_function = self._rng.randint(3)
-        self.classification_function = new_function
-
-    @staticmethod
-    def _classification_function_zero(size, color, shape):
-        # Class label 1 if the color is red and size is small.
-        return 1 if (size == 0 and color == 0) else 0
-
-    @staticmethod
-    def _classification_function_one(size, color, shape):
-        # Class label 1 if the color is green or shape is a circle.
-        return 1 if (color == 2 or shape == 0) else 0
-
-    @staticmethod
-    def _classification_function_two(size, color, shape):
-        # Class label 1 if the size is medium or large.
-        return 1 if (size == 1 or size == 2) else 0
+import numpy as np
+
+from river.utils.skmultiflow_utils import check_random_state
+
+from .. import base
+
+
+class STAGGER(base.SyntheticDataset):
+    """STAGGER concepts stream generator.
+
+    This generator is an implementation of the dara stream with abrupt concept
+    drift, as described in [^1].
+
+    The STAGGER concepts are boolean functions `f` with three features
+    describing objects: size (small, medium and large), shape (circle, square
+    and triangle) and colour (red, blue and green).
+
+    `f` options:
+
+    0. `True` if the size is small and the color is red.
+
+    1. `True` if the color is green or the shape is a circle.
+
+    2. `True` if the size is medium or large
+
+    Concept drift can be introduced by changing the classification function.
+    This can be done manually or using `ConceptDriftStream`.
+
+    One important feature is the possibility to balance classes, which
+    means the class distribution will tend to a uniform one.
+
+    Parameters
+    ----------
+    classification_function
+        Classification functions to use. From 0 to 2.
+    seed
+        If int, `seed` is used to seed the random number generator;
+        If RandomState instance, `seed` is the random number generator;
+        If None, the random number generator is the `RandomState` instance used
+        by `np.random`.
+    balance_classes
+        Whether to balance classes or not. If balanced, the class
+        distribution will converge to an uniform distribution.
+
+    Examples
+    --------
+    >>> from river import synth
+
+    >>> dataset = synth.STAGGER(classification_function = 2, seed = 112,
+    ...                      balance_classes = False)
+
+    >>> for x, y in dataset.take(5):
+    ...     print(x, y)
+    {'size': 0, 'color': 0, 'shape': 2} 0
+    {'size': 1, 'color': 0, 'shape': 1} 1
+    {'size': 0, 'color': 0, 'shape': 0} 0
+    {'size': 1, 'color': 2, 'shape': 0} 1
+    {'size': 1, 'color': 0, 'shape': 2} 1
+
+    Notes
+    -----
+    The sample generation works as follows: The 3 attributes are
+    generated with the random number generator. The classification function
+    defines whether to classify the instance as class 0 or class 1. Finally,
+    data is balanced, if this option is set by the user.
+
+    References
+    ----------
+    [^1]: Schlimmer, J. C., & Granger, R. H. (1986). Incremental learning
+          from noisy data. Machine learning, 1(3), 317-354.
+
+    """
+
+    def __init__(
+        self,
+        classification_function: int = 0,
+        seed: int or np.random.RandomState = None,
+        balance_classes: bool = False,
+    ):
+        super().__init__(n_features=3, n_classes=2, n_outputs=1, task=base.BINARY_CLF)
+
+        # Classification functions to use
+        self._functions = [
+            self._classification_function_zero,
+            self._classification_function_one,
+            self._classification_function_two,
+        ]
+        if classification_function not in range(3):
+            raise ValueError(
+                f"Invalid classification_function {classification_function}. "
+                "Valid values are: 0, 1, 2."
+            )
+        self.classification_function = classification_function
+        self.seed = seed
+        self.balance_classes = balance_classes
+        self.n_cat_features = 3
+        self._rng = None  # This is the actual random_state object used internally
+        self.next_class_should_be_zero = False
+
+        self.feature_names = ["size", "color", "shape"]
+        self.size_labels = {0: "small", 1: "medium", 2: "large"}
+        self.color_labels = {0: "red", 1: "blue", 2: "green"}
+        self.shape_labels = {0: "circle", 1: "square", 2: "triangle"}
+        self.target_values = [i for i in range(self.n_classes)]
+
+    def __iter__(self):
+        self._rng = check_random_state(self.seed)
+        self.next_class_should_be_zero = False
+
+        while True:
+            size = 0
+            color = 0
+            shape = 0
+            y = 0
+            desired_class_found = False
+            while not desired_class_found:
+                size = self._rng.randint(3)
+                color = self._rng.randint(3)
+                shape = self._rng.randint(3)
+
+                y = self._functions[self.classification_function](size, color, shape)
+
+                if not self.balance_classes:
+                    desired_class_found = True
+                else:
+                    if (self.next_class_should_be_zero and (y == 0)) or (
+                        (not self.next_class_should_be_zero) and (y == 1)
+                    ):
+                        desired_class_found = True
+                        self.next_class_should_be_zero = (
+                            not self.next_class_should_be_zero
+                        )
+
+            x = {"size": size, "color": color, "shape": shape}
+
+            yield x, y
+
+    def generate_drift(self):
+        """Generate drift by switching the classification function at random."""
+        new_function = self._rng.randint(3)
+        while new_function == self.classification_function:
+            new_function = self._rng.randint(3)
+        self.classification_function = new_function
+
+    @staticmethod
+    def _classification_function_zero(size, color, shape):
+        # Class label 1 if the color is red and size is small.
+        return 1 if (size == 0 and color == 0) else 0
+
+    @staticmethod
+    def _classification_function_one(size, color, shape):
+        # Class label 1 if the color is green or shape is a circle.
+        return 1 if (color == 2 or shape == 0) else 0
+
+    @staticmethod
+    def _classification_function_two(size, color, shape):
+        # Class label 1 if the size is medium or large.
+        return 1 if (size == 1 or size == 2) else 0
```

### Comparing `river-0.8.0/river/datasets/synth/waveform.py` & `river-0.9.0/river/datasets/synth/waveform.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,117 +1,117 @@
-import numpy as np
-
-from river.utils.skmultiflow_utils import check_random_state
-
-from .. import base
-
-
-class Waveform(base.SyntheticDataset):
-    """Waveform stream generator.
-
-    Generates samples with 21 numeric features and 3 classes, based
-    on a random differentiation of some base waveforms. Supports noise
-    addition, in this case the samples will have 40 features.
-
-    Parameters
-    ----------
-    seed
-        If int, `seed` is used to seed the random number generator;
-        If RandomState instance, `seed` is the random number generator;
-        If None, the random number generator is the `RandomState` instance used
-        by `np.random`.
-    has_noise
-        Adds 19 unrelated features to the stream.
-
-    Examples
-    --------
-    >>> from river import synth
-
-    >>> dataset = synth.Waveform(seed=42, has_noise=True)
-
-    >>> for x, y in dataset.take(5):
-    ...     print(list(x.values()), y)
-    [0.5437, -0.6154, -1.1978, 2.1417, -0.0946, -0.7254, -0.4783, 0.1982, 0.3312, 1.9780, \
-    3.0469, 3.5249, 5.4624, 6.1318, 2.7471, 4.7896, 2.9351, 2.2258, 0.1168, 2.2835, -0.0245, \
-    0.3556, 0.4170, 0.8325, -0.2934, -0.0298, 0.0951, 0.6647, -0.1402, -0.0332, -0.7491, \
-    -0.7784, 0.9488, 1.5809, -0.3682, 0.3756, -1.1932, -0.4091, -0.4467, 1.5242] 2
-    [0.2186, 1.4285, 0.0843, 0.0568, 2.9605, 2.6487, 2.8402, 3.2128, 2.8694, 4.0410, \
-    4.3953, 3.7009, 2.7075, 2.1149, 0.6994, -0.1702, -1.5082, 1.0996, -0.1777, -0.4104, \
-    1.1797, -0.8982, 0.8348, 0.2966, -1.0378, -0.0758, 0.9730, 0.7956, 1.4954, 0.3382, \
-    3.3723, -0.9204, -0.3986, -0.0609, -1.4188, 1.0425, 0.9035, 0.0190, -0.5344, -1.4951] 1
-    [0.1358, 0.6081, 0.7050, 0.3609, -1.4670, 1.6896, 1.4886, 1.4355, 2.7730, 2.7890, \
-    4.8437, 5.3447, 3.6724, 2.5445, 2.5541, 2.2732, -0.5371, -0.4099, 0.5331, -1.0464, \
-    1.9451, -0.1533, -0.9070, -0.8174, -0.4831, -0.5698, -2.0916, 1.2637, -0.0155, -0.0274, \
-    0.8179, -1.0546, -0.7583, 0.4574, -0.0644, 0.3449, -0.0801, -0.2414, 1.4335, 1.0658] 2
-    [1.1428, 1.2414, 1.7699, 0.5590, 3.3606, 1.0454, 3.5236, 4.6377, 0.9673, 1.4126, \
-    2.0997, 1.5176, 0.4915, 2.6213, 2.0010, 3.0263, 1.1228, 3.0816, 0.2378, 0.1885, \
-    0.8135, -1.2309, 0.2275, 1.3071, -1.6075, 0.1846, 0.2599, 0.7818, -1.2370, -1.3205, \
-    0.5219, 0.2970, 0.2505, 0.3464, -0.6800, 0.2323, 0.2931, -0.7144, 1.8658, 0.4738] 0
-    [-0.9747, 1.0114, 1.6071, -0.1479, 1.8605, 1.5341, 2.1677, 3.0181, 0.6517, 0.6948, \
-    1.1105, 1.7357, 3.0258, 4.2198, 4.9311, 4.7058, 3.1159, 3.7807, 1.2868, 3.4959, \
-    0.6257, -0.8572, -1.0709, 0.4825, -0.2235, 0.7140, 0.4732, -0.0728, -0.8468, -1.5148, \
-    -0.4465, 0.8564, 0.2141, -1.2457, 0.1732, 0.3853, -0.8839, 0.1537, 0.0582, -1.1430] 0
-
-    Notes
-    -----
-    An instance is generated based on the parameters passed.
-    The generator will randomly choose one of the hard coded waveforms, as
-    well as random multipliers. For each feature, the actual value generated
-    will be a a combination of the hard coded functions, with the multipliers
-    and a random value.
-
-    If noise is added then the features 21 to 40 will be replaced with a
-    random normal value.
-
-    """
-
-    _N_CLASSES = 3
-    _N_BASE_FEATURES = 21
-    _N_FEATURES_INCLUDING_NOISE = 40
-    _H_FUNCTION = np.array(
-        [
-            [0, 1, 2, 3, 4, 5, 6, 5, 4, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
-            [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 5, 4, 3, 2, 1, 0],
-            [0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 5, 4, 3, 2, 1, 0, 0, 0, 0, 0],
-        ]
-    )
-
-    def __init__(
-        self, seed: int or np.random.RandomState = None, has_noise: bool = False
-    ):
-        super().__init__(
-            n_features=self._N_BASE_FEATURES
-            if not has_noise
-            else self._N_FEATURES_INCLUDING_NOISE,
-            n_classes=self._N_CLASSES,
-            n_outputs=1,
-            task=base.MULTI_CLF,
-        )
-        self.seed = seed
-        self.has_noise = has_noise
-        self.n_num_features = self._N_BASE_FEATURES
-
-        self.target_values = [i for i in range(self.n_classes)]
-
-    def __iter__(self):
-        rng = check_random_state(self.seed)
-
-        while True:
-            x = dict()
-            y = rng.randint(self.n_classes)
-            choice_a = 1 if (y == 2) else 0
-            choice_b = 1 if (y == 0) else 2
-            multiplier_a = rng.rand()
-            multiplier_b = 1.0 - multiplier_a
-
-            for i in range(self._N_BASE_FEATURES):
-                x[i] = (
-                    multiplier_a * self._H_FUNCTION[choice_a][i]
-                    + multiplier_b * self._H_FUNCTION[choice_b][i]
-                    + rng.normal()
-                )
-
-            if self.has_noise:
-                for i in range(self._N_BASE_FEATURES, self._N_FEATURES_INCLUDING_NOISE):
-                    x[i] = rng.normal()
-
-            yield x, y
+import numpy as np
+
+from river.utils.skmultiflow_utils import check_random_state
+
+from .. import base
+
+
+class Waveform(base.SyntheticDataset):
+    """Waveform stream generator.
+
+    Generates samples with 21 numeric features and 3 classes, based
+    on a random differentiation of some base waveforms. Supports noise
+    addition, in this case the samples will have 40 features.
+
+    Parameters
+    ----------
+    seed
+        If int, `seed` is used to seed the random number generator;
+        If RandomState instance, `seed` is the random number generator;
+        If None, the random number generator is the `RandomState` instance used
+        by `np.random`.
+    has_noise
+        Adds 19 unrelated features to the stream.
+
+    Examples
+    --------
+    >>> from river import synth
+
+    >>> dataset = synth.Waveform(seed=42, has_noise=True)
+
+    >>> for x, y in dataset.take(5):
+    ...     print(list(x.values()), y)
+    [0.5437, -0.6154, -1.1978, 2.1417, -0.0946, -0.7254, -0.4783, 0.1982, 0.3312, 1.9780, \
+    3.0469, 3.5249, 5.4624, 6.1318, 2.7471, 4.7896, 2.9351, 2.2258, 0.1168, 2.2835, -0.0245, \
+    0.3556, 0.4170, 0.8325, -0.2934, -0.0298, 0.0951, 0.6647, -0.1402, -0.0332, -0.7491, \
+    -0.7784, 0.9488, 1.5809, -0.3682, 0.3756, -1.1932, -0.4091, -0.4467, 1.5242] 2
+    [0.2186, 1.4285, 0.0843, 0.0568, 2.9605, 2.6487, 2.8402, 3.2128, 2.8694, 4.0410, \
+    4.3953, 3.7009, 2.7075, 2.1149, 0.6994, -0.1702, -1.5082, 1.0996, -0.1777, -0.4104, \
+    1.1797, -0.8982, 0.8348, 0.2966, -1.0378, -0.0758, 0.9730, 0.7956, 1.4954, 0.3382, \
+    3.3723, -0.9204, -0.3986, -0.0609, -1.4188, 1.0425, 0.9035, 0.0190, -0.5344, -1.4951] 1
+    [0.1358, 0.6081, 0.7050, 0.3609, -1.4670, 1.6896, 1.4886, 1.4355, 2.7730, 2.7890, \
+    4.8437, 5.3447, 3.6724, 2.5445, 2.5541, 2.2732, -0.5371, -0.4099, 0.5331, -1.0464, \
+    1.9451, -0.1533, -0.9070, -0.8174, -0.4831, -0.5698, -2.0916, 1.2637, -0.0155, -0.0274, \
+    0.8179, -1.0546, -0.7583, 0.4574, -0.0644, 0.3449, -0.0801, -0.2414, 1.4335, 1.0658] 2
+    [1.1428, 1.2414, 1.7699, 0.5590, 3.3606, 1.0454, 3.5236, 4.6377, 0.9673, 1.4126, \
+    2.0997, 1.5176, 0.4915, 2.6213, 2.0010, 3.0263, 1.1228, 3.0816, 0.2378, 0.1885, \
+    0.8135, -1.2309, 0.2275, 1.3071, -1.6075, 0.1846, 0.2599, 0.7818, -1.2370, -1.3205, \
+    0.5219, 0.2970, 0.2505, 0.3464, -0.6800, 0.2323, 0.2931, -0.7144, 1.8658, 0.4738] 0
+    [-0.9747, 1.0114, 1.6071, -0.1479, 1.8605, 1.5341, 2.1677, 3.0181, 0.6517, 0.6948, \
+    1.1105, 1.7357, 3.0258, 4.2198, 4.9311, 4.7058, 3.1159, 3.7807, 1.2868, 3.4959, \
+    0.6257, -0.8572, -1.0709, 0.4825, -0.2235, 0.7140, 0.4732, -0.0728, -0.8468, -1.5148, \
+    -0.4465, 0.8564, 0.2141, -1.2457, 0.1732, 0.3853, -0.8839, 0.1537, 0.0582, -1.1430] 0
+
+    Notes
+    -----
+    An instance is generated based on the parameters passed.
+    The generator will randomly choose one of the hard coded waveforms, as
+    well as random multipliers. For each feature, the actual value generated
+    will be a a combination of the hard coded functions, with the multipliers
+    and a random value.
+
+    If noise is added then the features 21 to 40 will be replaced with a
+    random normal value.
+
+    """
+
+    _N_CLASSES = 3
+    _N_BASE_FEATURES = 21
+    _N_FEATURES_INCLUDING_NOISE = 40
+    _H_FUNCTION = np.array(
+        [
+            [0, 1, 2, 3, 4, 5, 6, 5, 4, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
+            [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 5, 4, 3, 2, 1, 0],
+            [0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 5, 4, 3, 2, 1, 0, 0, 0, 0, 0],
+        ]
+    )
+
+    def __init__(
+        self, seed: int or np.random.RandomState = None, has_noise: bool = False
+    ):
+        super().__init__(
+            n_features=self._N_BASE_FEATURES
+            if not has_noise
+            else self._N_FEATURES_INCLUDING_NOISE,
+            n_classes=self._N_CLASSES,
+            n_outputs=1,
+            task=base.MULTI_CLF,
+        )
+        self.seed = seed
+        self.has_noise = has_noise
+        self.n_num_features = self._N_BASE_FEATURES
+
+        self.target_values = [i for i in range(self.n_classes)]
+
+    def __iter__(self):
+        rng = check_random_state(self.seed)
+
+        while True:
+            x = dict()
+            y = rng.randint(self.n_classes)
+            choice_a = 1 if (y == 2) else 0
+            choice_b = 1 if (y == 0) else 2
+            multiplier_a = rng.rand()
+            multiplier_b = 1.0 - multiplier_a
+
+            for i in range(self._N_BASE_FEATURES):
+                x[i] = (
+                    multiplier_a * self._H_FUNCTION[choice_a][i]
+                    + multiplier_b * self._H_FUNCTION[choice_b][i]
+                    + rng.normal()
+                )
+
+            if self.has_noise:
+                for i in range(self._N_BASE_FEATURES, self._N_FEATURES_INCLUDING_NOISE):
+                    x[i] = rng.normal()
+
+            yield x, y
```

### Comparing `river-0.8.0/river/datasets/taxis.py` & `river-0.9.0/river/datasets/taxis.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,41 +1,41 @@
-from river import stream
-
-from . import base
-
-
-class Taxis(base.RemoteDataset):
-    """Taxi ride durations in New York City.
-
-    The goal is to predict the duration of taxi rides in New York City.
-
-    References
-    ----------
-    [^1]: [New York City Taxi Trip Duration competition on Kaggle](https://www.kaggle.com/c/nyc-taxi-trip-duration)
-
-    """
-
-    def __init__(self):
-        super().__init__(
-            n_samples=1_458_644,
-            n_features=8,
-            task=base.REG,
-            url="https://maxhalford.github.io/files/datasets/nyc_taxis.zip",
-            size=195271696,
-            filename="train.csv",
-        )
-
-    def _iter(self):
-        return stream.iter_csv(
-            self.path,
-            target="trip_duration",
-            converters={
-                "passenger_count": int,
-                "pickup_longitude": float,
-                "pickup_latitude": float,
-                "dropoff_longitude": float,
-                "dropoff_latitude": float,
-                "trip_duration": int,
-            },
-            parse_dates={"pickup_datetime": "%Y-%m-%d %H:%M:%S"},
-            drop=["dropoff_datetime", "id"],
-        )
+from river import stream
+
+from . import base
+
+
+class Taxis(base.RemoteDataset):
+    """Taxi ride durations in New York City.
+
+    The goal is to predict the duration of taxi rides in New York City.
+
+    References
+    ----------
+    [^1]: [New York City Taxi Trip Duration competition on Kaggle](https://www.kaggle.com/c/nyc-taxi-trip-duration)
+
+    """
+
+    def __init__(self):
+        super().__init__(
+            n_samples=1_458_644,
+            n_features=8,
+            task=base.REG,
+            url="https://maxhalford.github.io/files/datasets/nyc_taxis.zip",
+            size=195_271_696,
+            filename="train.csv",
+        )
+
+    def _iter(self):
+        return stream.iter_csv(
+            self.path,
+            target="trip_duration",
+            converters={
+                "passenger_count": int,
+                "pickup_longitude": float,
+                "pickup_latitude": float,
+                "dropoff_longitude": float,
+                "dropoff_latitude": float,
+                "trip_duration": int,
+            },
+            parse_dates={"pickup_datetime": "%Y-%m-%d %H:%M:%S"},
+            drop=["dropoff_datetime", "id"],
+        )
```

### Comparing `river-0.8.0/river/datasets/test_datasets.py` & `river-0.9.0/river/datasets/test_datasets.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,121 +1,120 @@
-import functools
-import importlib
-import inspect
-import itertools
-from urllib import request
-
-import pytest
-
-from river import datasets
-
-from . import base
-
-
-def _iter_datasets():
-
-    for variant in datasets.Insects.variants:
-        yield datasets.Insects(variant=variant)
-
-    for _, dataset in inspect.getmembers(
-        importlib.import_module("river.datasets"), inspect.isclass
-    ):
-        if not issubclass(dataset, datasets.Insects):
-            yield dataset()
-
-
-@pytest.mark.parametrize(
-    "dataset",
-    [
-        pytest.param(dataset, id=dataset.__class__.__name__)
-        for dataset in _iter_datasets()
-        if isinstance(dataset, base.RemoteDataset)
-    ],
-)
-@pytest.mark.datasets
-def test_remote_url(dataset):
-    with request.urlopen(dataset.url) as r:
-        assert r.status == 200
-
-
-@pytest.mark.parametrize(
-    "dataset",
-    [
-        pytest.param(dataset, id=dataset.__class__.__name__)
-        for dataset in _iter_datasets()
-        if not isinstance(dataset, base.SyntheticDataset)
-    ],
-)
-@pytest.mark.datasets
-def test_dimensions(dataset):
-    n = 0
-    for x, _ in dataset:
-        if not dataset.sparse:
-            assert len(x) == dataset.n_features
-        n += 1
-    assert n == dataset.n_samples
-
-
-@pytest.mark.parametrize(
-    "dataset",
-    [
-        pytest.param(dataset, id=dataset.__class__.__name__)
-        for dataset in _iter_datasets()
-    ],
-)
-def test_repr(dataset):
-    assert repr(dataset)
-
-
-def _iter_synth_datasets():
-
-    for variant in range(10):
-        dataset = functools.partial(
-            datasets.synth.Agrawal, classification_function=variant
-        )
-        functools.update_wrapper(dataset, datasets.synth.Agrawal)
-        yield dataset
-
-    synth = importlib.import_module("river.datasets.synth")
-    for name, dataset in inspect.getmembers(synth, inspect.isclass):
-        # TODO: test the following synth datasets also
-        if name in ("RandomRBF", "RandomRBFDrift", "RandomTree"):
-            continue
-        yield dataset
-
-
-@pytest.mark.parametrize(
-    "dataset",
-    [
-        pytest.param(dataset(seed=42), id=dataset.__name__)
-        for dataset in _iter_synth_datasets()
-    ],
-)
-def test_synth_idempotent(dataset):
-    """Checks that a synthetic dataset produces identical results when seeded."""
-    assert list(dataset.take(5)) == list(dataset.take(5))
-
-
-@pytest.mark.parametrize(
-    "dataset",
-    [
-        pytest.param(dataset(seed=None), id=dataset.__name__)
-        for dataset in _iter_synth_datasets()
-    ],
-)
-def test_synth_non_idempotent(dataset):
-    """Checks that a synthetic dataset produces different results when not seeded."""
-    assert list(dataset.take(5)) != list(dataset.take(5))
-
-
-@pytest.mark.parametrize(
-    "dataset",
-    [
-        pytest.param(dataset(seed=42), id=dataset.__name__)
-        for dataset in _iter_synth_datasets()
-    ],
-)
-def test_synth_pausable(dataset):
-    stream = iter(dataset)
-    s1 = itertools.islice(stream, 3)
-    s2 = itertools.islice(stream, 2)
-    assert list(dataset.take(5)) == list(itertools.chain(s1, s2))
+import functools
+import importlib
+import inspect
+import itertools
+from urllib import request
+
+import pytest
+
+from river import datasets
+
+from . import base
+
+
+def _iter_datasets():
+    for _, dataset in inspect.getmembers(
+        importlib.import_module("river.datasets"), inspect.isclass
+    ):
+        if issubclass(dataset, datasets.Insects):
+            for variant in dataset.variants:
+                yield dataset(variant=variant)
+            continue
+        yield dataset()
+
+
+@pytest.mark.parametrize(
+    "dataset",
+    [
+        pytest.param(dataset, id=dataset.__class__.__name__)
+        for dataset in _iter_datasets()
+        if isinstance(dataset, base.RemoteDataset)
+    ],
+)
+@pytest.mark.datasets
+def test_remote_url(dataset):
+    with request.urlopen(dataset.url) as r:
+        assert r.status == 200
+
+
+@pytest.mark.parametrize(
+    "dataset",
+    [
+        pytest.param(dataset, id=dataset.__class__.__name__)
+        for dataset in _iter_datasets()
+        if not isinstance(dataset, base.SyntheticDataset)
+    ],
+)
+@pytest.mark.datasets
+def test_dimensions(dataset):
+    n = 0
+    for x, _ in dataset:
+        if not dataset.sparse:
+            assert len(x) == dataset.n_features
+        n += 1
+    assert n == dataset.n_samples
+
+
+@pytest.mark.parametrize(
+    "dataset",
+    [
+        pytest.param(dataset, id=dataset.__class__.__name__)
+        for dataset in _iter_datasets()
+    ],
+)
+def test_repr(dataset):
+    assert repr(dataset)
+
+
+def _iter_synth_datasets():
+
+    for variant in range(10):
+        dataset = functools.partial(
+            datasets.synth.Agrawal, classification_function=variant
+        )
+        functools.update_wrapper(dataset, datasets.synth.Agrawal)
+        yield dataset
+
+    synth = importlib.import_module("river.datasets.synth")
+    for name, dataset in inspect.getmembers(synth, inspect.isclass):
+        # TODO: test the following synth datasets also
+        if name in ("RandomRBF", "RandomRBFDrift", "RandomTree"):
+            continue
+        yield dataset
+
+
+@pytest.mark.parametrize(
+    "dataset",
+    [
+        pytest.param(dataset(seed=42), id=dataset.__name__)
+        for dataset in _iter_synth_datasets()
+    ],
+)
+def test_synth_idempotent(dataset):
+    """Checks that a synthetic dataset produces identical results when seeded."""
+    assert list(dataset.take(5)) == list(dataset.take(5))
+
+
+@pytest.mark.parametrize(
+    "dataset",
+    [
+        pytest.param(dataset(seed=None), id=dataset.__name__)
+        for dataset in _iter_synth_datasets()
+    ],
+)
+def test_synth_non_idempotent(dataset):
+    """Checks that a synthetic dataset produces different results when not seeded."""
+    assert list(dataset.take(5)) != list(dataset.take(5))
+
+
+@pytest.mark.parametrize(
+    "dataset",
+    [
+        pytest.param(dataset(seed=42), id=dataset.__name__)
+        for dataset in _iter_synth_datasets()
+    ],
+)
+def test_synth_pausable(dataset):
+    stream = iter(dataset)
+    s1 = itertools.islice(stream, 3)
+    s2 = itertools.islice(stream, 2)
+    assert list(dataset.take(5)) == list(itertools.chain(s1, s2))
```

### Comparing `river-0.8.0/river/datasets/trec07.py` & `river-0.9.0/river/datasets/trec07.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,39 +1,39 @@
-from river import stream
-
-from . import base
-
-
-class TREC07(base.RemoteDataset):
-    """TREC's 2007 Spam Track dataset.
-
-    The data contains 75,419 chronologically ordered items, i.e. 3 months of emails delivered
-    to a particular server in 2007. Spam messages represent 66.6% of the dataset.
-    The goal is to predict whether an email is a spam or not.
-
-    The available raw features are: sender, recipients, date, subject, body.
-
-    References
-    ----------
-    [^1]: [TREC 2007 Spam Track Overview](https://trec.nist.gov/pubs/trec16/papers/SPAM.OVERVIEW16.pdf)
-    [^2]: [Code ran to parse the dataset](https://gist.github.com/gbolmier/b6a942699aaaedec54041a32e4f34d40)
-
-    """
-
-    def __init__(self):
-        super().__init__(
-            n_samples=75_419,
-            n_features=5,
-            task=base.BINARY_CLF,
-            url="https://maxhalford.github.io/files/datasets/trec07p.zip",
-            size=144504829,
-            filename="trec07p.csv",
-        )
-
-    def _iter(self):
-        return stream.iter_csv(
-            self.path,
-            target="y",
-            delimiter=",",
-            quotechar='"',
-            field_size_limit=1_000_000,
-        )
+from river import stream
+
+from . import base
+
+
+class TREC07(base.RemoteDataset):
+    """TREC's 2007 Spam Track dataset.
+
+    The data contains 75,419 chronologically ordered items, i.e. 3 months of emails delivered
+    to a particular server in 2007. Spam messages represent 66.6% of the dataset.
+    The goal is to predict whether an email is a spam or not.
+
+    The available raw features are: sender, recipients, date, subject, body.
+
+    References
+    ----------
+    [^1]: [TREC 2007 Spam Track Overview](https://trec.nist.gov/pubs/trec16/papers/SPAM.OVERVIEW16.pdf)
+    [^2]: [Code ran to parse the dataset](https://gist.github.com/gbolmier/b6a942699aaaedec54041a32e4f34d40)
+
+    """
+
+    def __init__(self):
+        super().__init__(
+            n_samples=75_419,
+            n_features=5,
+            task=base.BINARY_CLF,
+            url="https://maxhalford.github.io/files/datasets/trec07p.zip",
+            size=144_504_829,
+            filename="trec07p.csv",
+        )
+
+    def _iter(self):
+        return stream.iter_csv(
+            self.path,
+            target="y",
+            delimiter=",",
+            quotechar='"',
+            field_size_limit=1_000_000,
+        )
```

### Comparing `river-0.8.0/river/datasets/trump_approval.csv.gz` & `river-0.9.0/river/datasets/trump_approval.csv.gz`

 * *Files identical despite different names*

### Comparing `river-0.8.0/river/drift/adwin.py` & `river-0.9.0/river/drift/adwin.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,122 +1,122 @@
-from river.base import DriftDetector
-
-from .adwin_c import AdaptiveWindowing
-
-
-class ADWIN(DriftDetector):
-    r"""Adaptive Windowing method for concept drift detection.
-
-    ADWIN (ADaptive WINdowing) is a popular drift detection method with
-    mathematical guarantees. ADWIN efficiently keeps a variable-length window
-    of recent items; such that it holds that there has no been change in the
-    data distribution. This window is further divided into two sub-windows
-    $(W_0, W_1)$ used to determine if a change has happened. ADWIN compares
-    the average of $W_0$ and $W_1$ to confirm that they correspond to the
-    same distribution. Concept drift is detected if the distribution equality
-    no longer holds. Upon detecting a drift, $W_0$ is replaced by $W_1$ and a
-    new $W_1$ is initialized. ADWIN uses a confidence value
-    $\delta=\in(0,1)$ to determine if the two sub-windows correspond to the
-    same distribution.
-
-    **Input**: `value` can be any numeric value related to the definition of
-    concept change for the data analyzed. For example, using 0's or 1's
-    to track drift in a classifier's performance as follows:
-
-    - 0: Means the learners prediction was wrong
-
-    - 1: Means the learners prediction was correct
-
-    Parameters
-    ----------
-    delta
-        Confidence value.
-
-    Examples
-    --------
-    >>> import numpy as np
-    >>> from river.drift import ADWIN
-    >>> np.random.seed(12345)
-
-    >>> adwin = ADWIN()
-
-    >>> # Simulate a data stream composed by two data distributions
-    >>> data_stream = np.concatenate((np.random.randint(2, size=1000),
-    ...                               np.random.randint(4, high=8, size=1000)))
-
-    >>> # Update drift detector and verify if change is detected
-    >>> for i, val in enumerate(data_stream):
-    ...     in_drift, in_warning = adwin.update(val)
-    ...     if in_drift:
-    ...         print(f"Change detected at index {i}, input value: {val}")
-    Change detected at index 1023, input value: 5
-    Change detected at index 1055, input value: 7
-    Change detected at index 1087, input value: 5
-    Change detected at index 1119, input value: 7
-
-    References
-    ----------
-    [^1]: Albert Bifet and Ricard Gavalda.
-    "Learning from time-changing data with adaptive windowing."
-    In Proceedings of the 2007 SIAM international conference on data mining,
-    pp. 443-448. Society for Industrial and Applied Mathematics, 2007.
-
-    """
-
-    def __init__(self, delta=0.002):
-        super().__init__()
-        self._helper = AdaptiveWindowing(delta)
-
-    @property
-    def delta(self):
-        return self._helper.get_delta()
-
-    @property
-    def width(self):
-        """Window size"""
-        return self._helper.get_width()
-
-    @property
-    def n_detections(self):
-        return self._helper.get_n_detections()
-
-    @property
-    def variance(self):
-        return self._helper.get_variance()
-
-    @property
-    def total(self):
-        return self._helper.get_total()
-
-    @property
-    def estimation(self):
-        """Error estimation"""
-        if self.width == 0:
-            return 0.0
-        return self.total / self.width
-
-    def update(self, value):
-        """Update the change detector with a single data point.
-
-        Apart from adding the element value to the window, by inserting it in
-        the correct bucket, it will also update the relevant statistics, in
-        this case the total sum of all values, the window width and the total
-        variance.
-
-        Parameters
-        ----------
-        value
-            Input value
-
-        Returns
-        -------
-        tuple
-            A tuple (drift, warning) where its elements indicate if a drift or a warning is
-            detected.
-
-        """
-        self._in_concept_change = self._helper.update(value)
-        return self._in_concept_change, self._in_warning_zone
-
-    def reset(self):
-        """Reset the change detector."""
-        self._helper = AdaptiveWindowing(delta=self.delta)
+from river.base import DriftDetector
+
+from .adwin_c import AdaptiveWindowing
+
+
+class ADWIN(DriftDetector):
+    r"""Adaptive Windowing method for concept drift detection.
+
+    ADWIN (ADaptive WINdowing) is a popular drift detection method with
+    mathematical guarantees. ADWIN efficiently keeps a variable-length window
+    of recent items; such that it holds that there has no been change in the
+    data distribution. This window is further divided into two sub-windows
+    $(W_0, W_1)$ used to determine if a change has happened. ADWIN compares
+    the average of $W_0$ and $W_1$ to confirm that they correspond to the
+    same distribution. Concept drift is detected if the distribution equality
+    no longer holds. Upon detecting a drift, $W_0$ is replaced by $W_1$ and a
+    new $W_1$ is initialized. ADWIN uses a confidence value
+    $\delta=\in(0,1)$ to determine if the two sub-windows correspond to the
+    same distribution.
+
+    **Input**: `value` can be any numeric value related to the definition of
+    concept change for the data analyzed. For example, using 0's or 1's
+    to track drift in a classifier's performance as follows:
+
+    - 0: Means the learners prediction was wrong
+
+    - 1: Means the learners prediction was correct
+
+    Parameters
+    ----------
+    delta
+        Confidence value.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from river.drift import ADWIN
+    >>> np.random.seed(12345)
+
+    >>> adwin = ADWIN()
+
+    >>> # Simulate a data stream composed by two data distributions
+    >>> data_stream = np.concatenate((np.random.randint(2, size=1000),
+    ...                               np.random.randint(4, high=8, size=1000)))
+
+    >>> # Update drift detector and verify if change is detected
+    >>> for i, val in enumerate(data_stream):
+    ...     in_drift, in_warning = adwin.update(val)
+    ...     if in_drift:
+    ...         print(f"Change detected at index {i}, input value: {val}")
+    Change detected at index 1023, input value: 5
+    Change detected at index 1055, input value: 7
+    Change detected at index 1087, input value: 5
+    Change detected at index 1119, input value: 7
+
+    References
+    ----------
+    [^1]: Albert Bifet and Ricard Gavalda.
+    "Learning from time-changing data with adaptive windowing."
+    In Proceedings of the 2007 SIAM international conference on data mining,
+    pp. 443-448. Society for Industrial and Applied Mathematics, 2007.
+
+    """
+
+    def __init__(self, delta=0.002):
+        super().__init__()
+        self._helper = AdaptiveWindowing(delta)
+
+    @property
+    def delta(self):
+        return self._helper.get_delta()
+
+    @property
+    def width(self):
+        """Window size"""
+        return self._helper.get_width()
+
+    @property
+    def n_detections(self):
+        return self._helper.get_n_detections()
+
+    @property
+    def variance(self):
+        return self._helper.get_variance()
+
+    @property
+    def total(self):
+        return self._helper.get_total()
+
+    @property
+    def estimation(self):
+        """Error estimation"""
+        if self.width == 0:
+            return 0.0
+        return self.total / self.width
+
+    def update(self, value):
+        """Update the change detector with a single data point.
+
+        Apart from adding the element value to the window, by inserting it in
+        the correct bucket, it will also update the relevant statistics, in
+        this case the total sum of all values, the window width and the total
+        variance.
+
+        Parameters
+        ----------
+        value
+            Input value
+
+        Returns
+        -------
+        tuple
+            A tuple (drift, warning) where its elements indicate if a drift or a warning is
+            detected.
+
+        """
+        self._in_concept_change = self._helper.update(value)
+        return self._in_concept_change, self._in_warning_zone
+
+    def reset(self):
+        """Reset the change detector."""
+        self._helper = AdaptiveWindowing(delta=self.delta)
```

### Comparing `river-0.8.0/river/drift/adwin_c.c` & `river-0.9.0/river/drift/adwin_c.c`

 * *Files 2% similar despite different names*

```diff
@@ -6,29 +6,26 @@
         "define_macros": [
             [
                 "NPY_NO_DEPRECATED_API",
                 "NPY_1_7_API_VERSION"
             ]
         ],
         "depends": [
-            "/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/core/include/numpy/arrayobject.h",
-            "/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/core/include/numpy/arrayscalars.h",
-            "/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/core/include/numpy/ndarrayobject.h",
-            "/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/core/include/numpy/ndarraytypes.h",
-            "/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/core/include/numpy/ufuncobject.h"
+            "C:\\cibw\\python\\python.3.9.9\\tools\\lib\\site-packages\\numpy\\core\\include\\numpy\\arrayobject.h",
+            "C:\\cibw\\python\\python.3.9.9\\tools\\lib\\site-packages\\numpy\\core\\include\\numpy\\arrayscalars.h",
+            "C:\\cibw\\python\\python.3.9.9\\tools\\lib\\site-packages\\numpy\\core\\include\\numpy\\ndarrayobject.h",
+            "C:\\cibw\\python\\python.3.9.9\\tools\\lib\\site-packages\\numpy\\core\\include\\numpy\\ndarraytypes.h",
+            "C:\\cibw\\python\\python.3.9.9\\tools\\lib\\site-packages\\numpy\\core\\include\\numpy\\ufuncobject.h"
         ],
         "include_dirs": [
-            "/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/core/include"
-        ],
-        "libraries": [
-            "m"
+            "C:\\cibw\\python\\python.3.9.9\\tools\\lib\\site-packages\\numpy\\core\\include"
         ],
         "name": "river.drift.adwin_c",
         "sources": [
-            "river/drift/adwin_c.pyx"
+            "river\\drift\\adwin_c.pyx"
         ]
     },
     "module_name": "river.drift.adwin_c"
 }
 END: Cython Metadata */
 
 #ifndef PY_SSIZE_T_CLEAN
@@ -883,201 +880,201 @@
 #if CYTHON_CCOMPLEX && !defined(__cplusplus) && defined(__sun__) && defined(__GNUC__)
   #undef _Complex_I
   #define _Complex_I 1.0fj
 #endif
 
 
 static const char *__pyx_f[] = {
-  "river/drift/adwin_c.pyx",
+  "river\\drift\\adwin_c.pyx",
   "stringsource",
   "__init__.pxd",
   "type.pxd",
 };
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":690
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":690
  * # in Cython to enable them only on the right systems.
  * 
  * ctypedef npy_int8       int8_t             # <<<<<<<<<<<<<<
  * ctypedef npy_int16      int16_t
  * ctypedef npy_int32      int32_t
  */
 typedef npy_int8 __pyx_t_5numpy_int8_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":691
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":691
  * 
  * ctypedef npy_int8       int8_t
  * ctypedef npy_int16      int16_t             # <<<<<<<<<<<<<<
  * ctypedef npy_int32      int32_t
  * ctypedef npy_int64      int64_t
  */
 typedef npy_int16 __pyx_t_5numpy_int16_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":692
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":692
  * ctypedef npy_int8       int8_t
  * ctypedef npy_int16      int16_t
  * ctypedef npy_int32      int32_t             # <<<<<<<<<<<<<<
  * ctypedef npy_int64      int64_t
  * #ctypedef npy_int96      int96_t
  */
 typedef npy_int32 __pyx_t_5numpy_int32_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":693
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":693
  * ctypedef npy_int16      int16_t
  * ctypedef npy_int32      int32_t
  * ctypedef npy_int64      int64_t             # <<<<<<<<<<<<<<
  * #ctypedef npy_int96      int96_t
  * #ctypedef npy_int128     int128_t
  */
 typedef npy_int64 __pyx_t_5numpy_int64_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":697
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":697
  * #ctypedef npy_int128     int128_t
  * 
  * ctypedef npy_uint8      uint8_t             # <<<<<<<<<<<<<<
  * ctypedef npy_uint16     uint16_t
  * ctypedef npy_uint32     uint32_t
  */
 typedef npy_uint8 __pyx_t_5numpy_uint8_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":698
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":698
  * 
  * ctypedef npy_uint8      uint8_t
  * ctypedef npy_uint16     uint16_t             # <<<<<<<<<<<<<<
  * ctypedef npy_uint32     uint32_t
  * ctypedef npy_uint64     uint64_t
  */
 typedef npy_uint16 __pyx_t_5numpy_uint16_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":699
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":699
  * ctypedef npy_uint8      uint8_t
  * ctypedef npy_uint16     uint16_t
  * ctypedef npy_uint32     uint32_t             # <<<<<<<<<<<<<<
  * ctypedef npy_uint64     uint64_t
  * #ctypedef npy_uint96     uint96_t
  */
 typedef npy_uint32 __pyx_t_5numpy_uint32_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":700
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":700
  * ctypedef npy_uint16     uint16_t
  * ctypedef npy_uint32     uint32_t
  * ctypedef npy_uint64     uint64_t             # <<<<<<<<<<<<<<
  * #ctypedef npy_uint96     uint96_t
  * #ctypedef npy_uint128    uint128_t
  */
 typedef npy_uint64 __pyx_t_5numpy_uint64_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":704
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":704
  * #ctypedef npy_uint128    uint128_t
  * 
  * ctypedef npy_float32    float32_t             # <<<<<<<<<<<<<<
  * ctypedef npy_float64    float64_t
  * #ctypedef npy_float80    float80_t
  */
 typedef npy_float32 __pyx_t_5numpy_float32_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":705
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":705
  * 
  * ctypedef npy_float32    float32_t
  * ctypedef npy_float64    float64_t             # <<<<<<<<<<<<<<
  * #ctypedef npy_float80    float80_t
  * #ctypedef npy_float128   float128_t
  */
 typedef npy_float64 __pyx_t_5numpy_float64_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":714
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":714
  * # The int types are mapped a bit surprising --
  * # numpy.int corresponds to 'l' and numpy.long to 'q'
  * ctypedef npy_long       int_t             # <<<<<<<<<<<<<<
  * ctypedef npy_longlong   long_t
  * ctypedef npy_longlong   longlong_t
  */
 typedef npy_long __pyx_t_5numpy_int_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":715
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":715
  * # numpy.int corresponds to 'l' and numpy.long to 'q'
  * ctypedef npy_long       int_t
  * ctypedef npy_longlong   long_t             # <<<<<<<<<<<<<<
  * ctypedef npy_longlong   longlong_t
  * 
  */
 typedef npy_longlong __pyx_t_5numpy_long_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":716
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":716
  * ctypedef npy_long       int_t
  * ctypedef npy_longlong   long_t
  * ctypedef npy_longlong   longlong_t             # <<<<<<<<<<<<<<
  * 
  * ctypedef npy_ulong      uint_t
  */
 typedef npy_longlong __pyx_t_5numpy_longlong_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":718
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":718
  * ctypedef npy_longlong   longlong_t
  * 
  * ctypedef npy_ulong      uint_t             # <<<<<<<<<<<<<<
  * ctypedef npy_ulonglong  ulong_t
  * ctypedef npy_ulonglong  ulonglong_t
  */
 typedef npy_ulong __pyx_t_5numpy_uint_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":719
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":719
  * 
  * ctypedef npy_ulong      uint_t
  * ctypedef npy_ulonglong  ulong_t             # <<<<<<<<<<<<<<
  * ctypedef npy_ulonglong  ulonglong_t
  * 
  */
 typedef npy_ulonglong __pyx_t_5numpy_ulong_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":720
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":720
  * ctypedef npy_ulong      uint_t
  * ctypedef npy_ulonglong  ulong_t
  * ctypedef npy_ulonglong  ulonglong_t             # <<<<<<<<<<<<<<
  * 
  * ctypedef npy_intp       intp_t
  */
 typedef npy_ulonglong __pyx_t_5numpy_ulonglong_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":722
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":722
  * ctypedef npy_ulonglong  ulonglong_t
  * 
  * ctypedef npy_intp       intp_t             # <<<<<<<<<<<<<<
  * ctypedef npy_uintp      uintp_t
  * 
  */
 typedef npy_intp __pyx_t_5numpy_intp_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":723
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":723
  * 
  * ctypedef npy_intp       intp_t
  * ctypedef npy_uintp      uintp_t             # <<<<<<<<<<<<<<
  * 
  * ctypedef npy_double     float_t
  */
 typedef npy_uintp __pyx_t_5numpy_uintp_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":725
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":725
  * ctypedef npy_uintp      uintp_t
  * 
  * ctypedef npy_double     float_t             # <<<<<<<<<<<<<<
  * ctypedef npy_double     double_t
  * ctypedef npy_longdouble longdouble_t
  */
 typedef npy_double __pyx_t_5numpy_float_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":726
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":726
  * 
  * ctypedef npy_double     float_t
  * ctypedef npy_double     double_t             # <<<<<<<<<<<<<<
  * ctypedef npy_longdouble longdouble_t
  * 
  */
 typedef npy_double __pyx_t_5numpy_double_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":727
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":727
  * ctypedef npy_double     float_t
  * ctypedef npy_double     double_t
  * ctypedef npy_longdouble longdouble_t             # <<<<<<<<<<<<<<
  * 
  * ctypedef npy_cfloat      cfloat_t
  */
 typedef npy_longdouble __pyx_t_5numpy_longdouble_t;
@@ -1106,42 +1103,42 @@
 static CYTHON_INLINE __pyx_t_double_complex __pyx_t_double_complex_from_parts(double, double);
 
 
 /*--- Type declarations ---*/
 struct __pyx_obj_5river_5drift_7adwin_c_AdaptiveWindowing;
 struct __pyx_obj_5river_5drift_7adwin_c_Bucket;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":729
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":729
  * ctypedef npy_longdouble longdouble_t
  * 
  * ctypedef npy_cfloat      cfloat_t             # <<<<<<<<<<<<<<
  * ctypedef npy_cdouble     cdouble_t
  * ctypedef npy_clongdouble clongdouble_t
  */
 typedef npy_cfloat __pyx_t_5numpy_cfloat_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":730
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":730
  * 
  * ctypedef npy_cfloat      cfloat_t
  * ctypedef npy_cdouble     cdouble_t             # <<<<<<<<<<<<<<
  * ctypedef npy_clongdouble clongdouble_t
  * 
  */
 typedef npy_cdouble __pyx_t_5numpy_cdouble_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":731
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":731
  * ctypedef npy_cfloat      cfloat_t
  * ctypedef npy_cdouble     cdouble_t
  * ctypedef npy_clongdouble clongdouble_t             # <<<<<<<<<<<<<<
  * 
  * ctypedef npy_cdouble     complex_t
  */
 typedef npy_clongdouble __pyx_t_5numpy_clongdouble_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":733
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":733
  * ctypedef npy_clongdouble clongdouble_t
  * 
  * ctypedef npy_cdouble     complex_t             # <<<<<<<<<<<<<<
  * 
  * cdef inline object PyArray_MultiIterNew1(a):
  */
 typedef npy_cdouble __pyx_t_5numpy_complex_t;
@@ -2032,15 +2029,15 @@
 static const char __pyx_k_cline_in_traceback[] = "cline_in_traceback";
 static const char __pyx_k_variance_in_window[] = "variance_in_window";
 static const char __pyx_k_pyx_unpickle_Bucket[] = "__pyx_unpickle_Bucket";
 static const char __pyx_k_river_drift_adwin_c[] = "river.drift.adwin_c";
 static const char __pyx_k_calculate_bucket_size[] = "_calculate_bucket_size";
 static const char __pyx_k_Bucket___reduce_cython[] = "Bucket.__reduce_cython__";
 static const char __pyx_k_AdaptiveWindowing_reset[] = "AdaptiveWindowing.reset";
-static const char __pyx_k_river_drift_adwin_c_pyx[] = "river/drift/adwin_c.pyx";
+static const char __pyx_k_river_drift_adwin_c_pyx[] = "river\\drift\\adwin_c.pyx";
 static const char __pyx_k_AdaptiveWindowing_update[] = "AdaptiveWindowing.update";
 static const char __pyx_k_Bucket___setstate_cython[] = "Bucket.__setstate_cython__";
 static const char __pyx_k_AdaptiveWindowing_get_delta[] = "AdaptiveWindowing.get_delta";
 static const char __pyx_k_AdaptiveWindowing_get_total[] = "AdaptiveWindowing.get_total";
 static const char __pyx_k_AdaptiveWindowing_get_width[] = "AdaptiveWindowing.get_width";
 static const char __pyx_k_AdaptiveWindowing_get_variance[] = "AdaptiveWindowing.get_variance";
 static const char __pyx_k_pyx_unpickle_AdaptiveWindowing[] = "__pyx_unpickle_AdaptiveWindowing";
@@ -7292,15 +7289,15 @@
   __pyx_r = 0;
   __pyx_L0:;
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":735
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":735
  * ctypedef npy_cdouble     complex_t
  * 
  * cdef inline object PyArray_MultiIterNew1(a):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(1, <void*>a)
  * 
  */
 
@@ -7309,29 +7306,29 @@
   __Pyx_RefNannyDeclarations
   PyObject *__pyx_t_1 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("PyArray_MultiIterNew1", 0);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":736
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":736
  * 
  * cdef inline object PyArray_MultiIterNew1(a):
  *     return PyArray_MultiIterNew(1, <void*>a)             # <<<<<<<<<<<<<<
  * 
  * cdef inline object PyArray_MultiIterNew2(a, b):
  */
   __Pyx_XDECREF(__pyx_r);
   __pyx_t_1 = PyArray_MultiIterNew(1, ((void *)__pyx_v_a)); if (unlikely(!__pyx_t_1)) __PYX_ERR(2, 736, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_1);
   __pyx_r = __pyx_t_1;
   __pyx_t_1 = 0;
   goto __pyx_L0;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":735
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":735
  * ctypedef npy_cdouble     complex_t
  * 
  * cdef inline object PyArray_MultiIterNew1(a):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(1, <void*>a)
  * 
  */
 
@@ -7342,15 +7339,15 @@
   __pyx_r = 0;
   __pyx_L0:;
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":738
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":738
  *     return PyArray_MultiIterNew(1, <void*>a)
  * 
  * cdef inline object PyArray_MultiIterNew2(a, b):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(2, <void*>a, <void*>b)
  * 
  */
 
@@ -7359,29 +7356,29 @@
   __Pyx_RefNannyDeclarations
   PyObject *__pyx_t_1 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("PyArray_MultiIterNew2", 0);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":739
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":739
  * 
  * cdef inline object PyArray_MultiIterNew2(a, b):
  *     return PyArray_MultiIterNew(2, <void*>a, <void*>b)             # <<<<<<<<<<<<<<
  * 
  * cdef inline object PyArray_MultiIterNew3(a, b, c):
  */
   __Pyx_XDECREF(__pyx_r);
   __pyx_t_1 = PyArray_MultiIterNew(2, ((void *)__pyx_v_a), ((void *)__pyx_v_b)); if (unlikely(!__pyx_t_1)) __PYX_ERR(2, 739, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_1);
   __pyx_r = __pyx_t_1;
   __pyx_t_1 = 0;
   goto __pyx_L0;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":738
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":738
  *     return PyArray_MultiIterNew(1, <void*>a)
  * 
  * cdef inline object PyArray_MultiIterNew2(a, b):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(2, <void*>a, <void*>b)
  * 
  */
 
@@ -7392,15 +7389,15 @@
   __pyx_r = 0;
   __pyx_L0:;
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":741
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":741
  *     return PyArray_MultiIterNew(2, <void*>a, <void*>b)
  * 
  * cdef inline object PyArray_MultiIterNew3(a, b, c):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(3, <void*>a, <void*>b, <void*> c)
  * 
  */
 
@@ -7409,29 +7406,29 @@
   __Pyx_RefNannyDeclarations
   PyObject *__pyx_t_1 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("PyArray_MultiIterNew3", 0);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":742
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":742
  * 
  * cdef inline object PyArray_MultiIterNew3(a, b, c):
  *     return PyArray_MultiIterNew(3, <void*>a, <void*>b, <void*> c)             # <<<<<<<<<<<<<<
  * 
  * cdef inline object PyArray_MultiIterNew4(a, b, c, d):
  */
   __Pyx_XDECREF(__pyx_r);
   __pyx_t_1 = PyArray_MultiIterNew(3, ((void *)__pyx_v_a), ((void *)__pyx_v_b), ((void *)__pyx_v_c)); if (unlikely(!__pyx_t_1)) __PYX_ERR(2, 742, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_1);
   __pyx_r = __pyx_t_1;
   __pyx_t_1 = 0;
   goto __pyx_L0;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":741
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":741
  *     return PyArray_MultiIterNew(2, <void*>a, <void*>b)
  * 
  * cdef inline object PyArray_MultiIterNew3(a, b, c):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(3, <void*>a, <void*>b, <void*> c)
  * 
  */
 
@@ -7442,15 +7439,15 @@
   __pyx_r = 0;
   __pyx_L0:;
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":744
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":744
  *     return PyArray_MultiIterNew(3, <void*>a, <void*>b, <void*> c)
  * 
  * cdef inline object PyArray_MultiIterNew4(a, b, c, d):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(4, <void*>a, <void*>b, <void*>c, <void*> d)
  * 
  */
 
@@ -7459,29 +7456,29 @@
   __Pyx_RefNannyDeclarations
   PyObject *__pyx_t_1 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("PyArray_MultiIterNew4", 0);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":745
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":745
  * 
  * cdef inline object PyArray_MultiIterNew4(a, b, c, d):
  *     return PyArray_MultiIterNew(4, <void*>a, <void*>b, <void*>c, <void*> d)             # <<<<<<<<<<<<<<
  * 
  * cdef inline object PyArray_MultiIterNew5(a, b, c, d, e):
  */
   __Pyx_XDECREF(__pyx_r);
   __pyx_t_1 = PyArray_MultiIterNew(4, ((void *)__pyx_v_a), ((void *)__pyx_v_b), ((void *)__pyx_v_c), ((void *)__pyx_v_d)); if (unlikely(!__pyx_t_1)) __PYX_ERR(2, 745, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_1);
   __pyx_r = __pyx_t_1;
   __pyx_t_1 = 0;
   goto __pyx_L0;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":744
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":744
  *     return PyArray_MultiIterNew(3, <void*>a, <void*>b, <void*> c)
  * 
  * cdef inline object PyArray_MultiIterNew4(a, b, c, d):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(4, <void*>a, <void*>b, <void*>c, <void*> d)
  * 
  */
 
@@ -7492,15 +7489,15 @@
   __pyx_r = 0;
   __pyx_L0:;
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":747
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":747
  *     return PyArray_MultiIterNew(4, <void*>a, <void*>b, <void*>c, <void*> d)
  * 
  * cdef inline object PyArray_MultiIterNew5(a, b, c, d, e):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(5, <void*>a, <void*>b, <void*>c, <void*> d, <void*> e)
  * 
  */
 
@@ -7509,29 +7506,29 @@
   __Pyx_RefNannyDeclarations
   PyObject *__pyx_t_1 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("PyArray_MultiIterNew5", 0);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":748
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":748
  * 
  * cdef inline object PyArray_MultiIterNew5(a, b, c, d, e):
  *     return PyArray_MultiIterNew(5, <void*>a, <void*>b, <void*>c, <void*> d, <void*> e)             # <<<<<<<<<<<<<<
  * 
  * cdef inline tuple PyDataType_SHAPE(dtype d):
  */
   __Pyx_XDECREF(__pyx_r);
   __pyx_t_1 = PyArray_MultiIterNew(5, ((void *)__pyx_v_a), ((void *)__pyx_v_b), ((void *)__pyx_v_c), ((void *)__pyx_v_d), ((void *)__pyx_v_e)); if (unlikely(!__pyx_t_1)) __PYX_ERR(2, 748, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_1);
   __pyx_r = __pyx_t_1;
   __pyx_t_1 = 0;
   goto __pyx_L0;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":747
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":747
  *     return PyArray_MultiIterNew(4, <void*>a, <void*>b, <void*>c, <void*> d)
  * 
  * cdef inline object PyArray_MultiIterNew5(a, b, c, d, e):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(5, <void*>a, <void*>b, <void*>c, <void*> d, <void*> e)
  * 
  */
 
@@ -7542,212 +7539,212 @@
   __pyx_r = 0;
   __pyx_L0:;
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":750
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":750
  *     return PyArray_MultiIterNew(5, <void*>a, <void*>b, <void*>c, <void*> d, <void*> e)
  * 
  * cdef inline tuple PyDataType_SHAPE(dtype d):             # <<<<<<<<<<<<<<
  *     if PyDataType_HASSUBARRAY(d):
  *         return <tuple>d.subarray.shape
  */
 
 static CYTHON_INLINE PyObject *__pyx_f_5numpy_PyDataType_SHAPE(PyArray_Descr *__pyx_v_d) {
   PyObject *__pyx_r = NULL;
   __Pyx_RefNannyDeclarations
   int __pyx_t_1;
   __Pyx_RefNannySetupContext("PyDataType_SHAPE", 0);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":751
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":751
  * 
  * cdef inline tuple PyDataType_SHAPE(dtype d):
  *     if PyDataType_HASSUBARRAY(d):             # <<<<<<<<<<<<<<
  *         return <tuple>d.subarray.shape
  *     else:
  */
   __pyx_t_1 = (PyDataType_HASSUBARRAY(__pyx_v_d) != 0);
   if (__pyx_t_1) {
 
-    /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":752
+    /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":752
  * cdef inline tuple PyDataType_SHAPE(dtype d):
  *     if PyDataType_HASSUBARRAY(d):
  *         return <tuple>d.subarray.shape             # <<<<<<<<<<<<<<
  *     else:
  *         return ()
  */
     __Pyx_XDECREF(__pyx_r);
     __Pyx_INCREF(((PyObject*)__pyx_v_d->subarray->shape));
     __pyx_r = ((PyObject*)__pyx_v_d->subarray->shape);
     goto __pyx_L0;
 
-    /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":751
+    /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":751
  * 
  * cdef inline tuple PyDataType_SHAPE(dtype d):
  *     if PyDataType_HASSUBARRAY(d):             # <<<<<<<<<<<<<<
  *         return <tuple>d.subarray.shape
  *     else:
  */
   }
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":754
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":754
  *         return <tuple>d.subarray.shape
  *     else:
  *         return ()             # <<<<<<<<<<<<<<
  * 
  * 
  */
   /*else*/ {
     __Pyx_XDECREF(__pyx_r);
     __Pyx_INCREF(__pyx_empty_tuple);
     __pyx_r = __pyx_empty_tuple;
     goto __pyx_L0;
   }
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":750
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":750
  *     return PyArray_MultiIterNew(5, <void*>a, <void*>b, <void*>c, <void*> d, <void*> e)
  * 
  * cdef inline tuple PyDataType_SHAPE(dtype d):             # <<<<<<<<<<<<<<
  *     if PyDataType_HASSUBARRAY(d):
  *         return <tuple>d.subarray.shape
  */
 
   /* function exit code */
   __pyx_L0:;
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":931
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":929
  *     int _import_umath() except -1
  * 
  * cdef inline void set_array_base(ndarray arr, object base):             # <<<<<<<<<<<<<<
  *     Py_INCREF(base) # important to do this before stealing the reference below!
  *     PyArray_SetBaseObject(arr, base)
  */
 
 static CYTHON_INLINE void __pyx_f_5numpy_set_array_base(PyArrayObject *__pyx_v_arr, PyObject *__pyx_v_base) {
   __Pyx_RefNannyDeclarations
   __Pyx_RefNannySetupContext("set_array_base", 0);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":932
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":930
  * 
  * cdef inline void set_array_base(ndarray arr, object base):
  *     Py_INCREF(base) # important to do this before stealing the reference below!             # <<<<<<<<<<<<<<
  *     PyArray_SetBaseObject(arr, base)
  * 
  */
   Py_INCREF(__pyx_v_base);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":933
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":931
  * cdef inline void set_array_base(ndarray arr, object base):
  *     Py_INCREF(base) # important to do this before stealing the reference below!
  *     PyArray_SetBaseObject(arr, base)             # <<<<<<<<<<<<<<
  * 
  * cdef inline object get_array_base(ndarray arr):
  */
   (void)(PyArray_SetBaseObject(__pyx_v_arr, __pyx_v_base));
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":931
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":929
  *     int _import_umath() except -1
  * 
  * cdef inline void set_array_base(ndarray arr, object base):             # <<<<<<<<<<<<<<
  *     Py_INCREF(base) # important to do this before stealing the reference below!
  *     PyArray_SetBaseObject(arr, base)
  */
 
   /* function exit code */
   __Pyx_RefNannyFinishContext();
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":935
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":933
  *     PyArray_SetBaseObject(arr, base)
  * 
  * cdef inline object get_array_base(ndarray arr):             # <<<<<<<<<<<<<<
  *     base = PyArray_BASE(arr)
  *     if base is NULL:
  */
 
 static CYTHON_INLINE PyObject *__pyx_f_5numpy_get_array_base(PyArrayObject *__pyx_v_arr) {
   PyObject *__pyx_v_base;
   PyObject *__pyx_r = NULL;
   __Pyx_RefNannyDeclarations
   int __pyx_t_1;
   __Pyx_RefNannySetupContext("get_array_base", 0);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":936
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":934
  * 
  * cdef inline object get_array_base(ndarray arr):
  *     base = PyArray_BASE(arr)             # <<<<<<<<<<<<<<
  *     if base is NULL:
  *         return None
  */
   __pyx_v_base = PyArray_BASE(__pyx_v_arr);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":937
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":935
  * cdef inline object get_array_base(ndarray arr):
  *     base = PyArray_BASE(arr)
  *     if base is NULL:             # <<<<<<<<<<<<<<
  *         return None
  *     return <object>base
  */
   __pyx_t_1 = ((__pyx_v_base == NULL) != 0);
   if (__pyx_t_1) {
 
-    /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":938
+    /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":936
  *     base = PyArray_BASE(arr)
  *     if base is NULL:
  *         return None             # <<<<<<<<<<<<<<
  *     return <object>base
  * 
  */
     __Pyx_XDECREF(__pyx_r);
     __pyx_r = Py_None; __Pyx_INCREF(Py_None);
     goto __pyx_L0;
 
-    /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":937
+    /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":935
  * cdef inline object get_array_base(ndarray arr):
  *     base = PyArray_BASE(arr)
  *     if base is NULL:             # <<<<<<<<<<<<<<
  *         return None
  *     return <object>base
  */
   }
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":939
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":937
  *     if base is NULL:
  *         return None
  *     return <object>base             # <<<<<<<<<<<<<<
  * 
  * # Versions of the import_* functions which are more suitable for
  */
   __Pyx_XDECREF(__pyx_r);
   __Pyx_INCREF(((PyObject *)__pyx_v_base));
   __pyx_r = ((PyObject *)__pyx_v_base);
   goto __pyx_L0;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":935
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":933
  *     PyArray_SetBaseObject(arr, base)
  * 
  * cdef inline object get_array_base(ndarray arr):             # <<<<<<<<<<<<<<
  *     base = PyArray_BASE(arr)
  *     if base is NULL:
  */
 
   /* function exit code */
   __pyx_L0:;
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":943
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":941
  * # Versions of the import_* functions which are more suitable for
  * # Cython code.
  * cdef inline int import_array() except -1:             # <<<<<<<<<<<<<<
  *     try:
  *         __pyx_import_array()
  */
 
@@ -7763,15 +7760,15 @@
   PyObject *__pyx_t_7 = NULL;
   PyObject *__pyx_t_8 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("import_array", 0);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":944
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":942
  * # Cython code.
  * cdef inline int import_array() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         __pyx_import_array()
  *     except Exception:
  */
   {
@@ -7779,84 +7776,84 @@
     __Pyx_PyThreadState_assign
     __Pyx_ExceptionSave(&__pyx_t_1, &__pyx_t_2, &__pyx_t_3);
     __Pyx_XGOTREF(__pyx_t_1);
     __Pyx_XGOTREF(__pyx_t_2);
     __Pyx_XGOTREF(__pyx_t_3);
     /*try:*/ {
 
-      /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":945
+      /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":943
  * cdef inline int import_array() except -1:
  *     try:
  *         __pyx_import_array()             # <<<<<<<<<<<<<<
  *     except Exception:
  *         raise ImportError("numpy.core.multiarray failed to import")
  */
-      __pyx_t_4 = _import_array(); if (unlikely(__pyx_t_4 == ((int)-1))) __PYX_ERR(2, 945, __pyx_L3_error)
+      __pyx_t_4 = _import_array(); if (unlikely(__pyx_t_4 == ((int)-1))) __PYX_ERR(2, 943, __pyx_L3_error)
 
-      /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":944
+      /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":942
  * # Cython code.
  * cdef inline int import_array() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         __pyx_import_array()
  *     except Exception:
  */
     }
     __Pyx_XDECREF(__pyx_t_1); __pyx_t_1 = 0;
     __Pyx_XDECREF(__pyx_t_2); __pyx_t_2 = 0;
     __Pyx_XDECREF(__pyx_t_3); __pyx_t_3 = 0;
     goto __pyx_L8_try_end;
     __pyx_L3_error:;
 
-    /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":946
+    /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":944
  *     try:
  *         __pyx_import_array()
  *     except Exception:             # <<<<<<<<<<<<<<
  *         raise ImportError("numpy.core.multiarray failed to import")
  * 
  */
     __pyx_t_4 = __Pyx_PyErr_ExceptionMatches(((PyObject *)(&((PyTypeObject*)PyExc_Exception)[0])));
     if (__pyx_t_4) {
       __Pyx_AddTraceback("numpy.import_array", __pyx_clineno, __pyx_lineno, __pyx_filename);
-      if (__Pyx_GetException(&__pyx_t_5, &__pyx_t_6, &__pyx_t_7) < 0) __PYX_ERR(2, 946, __pyx_L5_except_error)
+      if (__Pyx_GetException(&__pyx_t_5, &__pyx_t_6, &__pyx_t_7) < 0) __PYX_ERR(2, 944, __pyx_L5_except_error)
       __Pyx_GOTREF(__pyx_t_5);
       __Pyx_GOTREF(__pyx_t_6);
       __Pyx_GOTREF(__pyx_t_7);
 
-      /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":947
+      /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":945
  *         __pyx_import_array()
  *     except Exception:
  *         raise ImportError("numpy.core.multiarray failed to import")             # <<<<<<<<<<<<<<
  * 
  * cdef inline int import_umath() except -1:
  */
-      __pyx_t_8 = __Pyx_PyObject_Call(__pyx_builtin_ImportError, __pyx_tuple_, NULL); if (unlikely(!__pyx_t_8)) __PYX_ERR(2, 947, __pyx_L5_except_error)
+      __pyx_t_8 = __Pyx_PyObject_Call(__pyx_builtin_ImportError, __pyx_tuple_, NULL); if (unlikely(!__pyx_t_8)) __PYX_ERR(2, 945, __pyx_L5_except_error)
       __Pyx_GOTREF(__pyx_t_8);
       __Pyx_Raise(__pyx_t_8, 0, 0, 0);
       __Pyx_DECREF(__pyx_t_8); __pyx_t_8 = 0;
-      __PYX_ERR(2, 947, __pyx_L5_except_error)
+      __PYX_ERR(2, 945, __pyx_L5_except_error)
     }
     goto __pyx_L5_except_error;
     __pyx_L5_except_error:;
 
-    /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":944
+    /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":942
  * # Cython code.
  * cdef inline int import_array() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         __pyx_import_array()
  *     except Exception:
  */
     __Pyx_XGIVEREF(__pyx_t_1);
     __Pyx_XGIVEREF(__pyx_t_2);
     __Pyx_XGIVEREF(__pyx_t_3);
     __Pyx_ExceptionReset(__pyx_t_1, __pyx_t_2, __pyx_t_3);
     goto __pyx_L1_error;
     __pyx_L8_try_end:;
   }
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":943
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":941
  * # Versions of the import_* functions which are more suitable for
  * # Cython code.
  * cdef inline int import_array() except -1:             # <<<<<<<<<<<<<<
  *     try:
  *         __pyx_import_array()
  */
 
@@ -7871,15 +7868,15 @@
   __Pyx_AddTraceback("numpy.import_array", __pyx_clineno, __pyx_lineno, __pyx_filename);
   __pyx_r = -1;
   __pyx_L0:;
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":949
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":947
  *         raise ImportError("numpy.core.multiarray failed to import")
  * 
  * cdef inline int import_umath() except -1:             # <<<<<<<<<<<<<<
  *     try:
  *         _import_umath()
  */
 
@@ -7895,15 +7892,15 @@
   PyObject *__pyx_t_7 = NULL;
   PyObject *__pyx_t_8 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("import_umath", 0);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":950
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":948
  * 
  * cdef inline int import_umath() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         _import_umath()
  *     except Exception:
  */
   {
@@ -7911,84 +7908,84 @@
     __Pyx_PyThreadState_assign
     __Pyx_ExceptionSave(&__pyx_t_1, &__pyx_t_2, &__pyx_t_3);
     __Pyx_XGOTREF(__pyx_t_1);
     __Pyx_XGOTREF(__pyx_t_2);
     __Pyx_XGOTREF(__pyx_t_3);
     /*try:*/ {
 
-      /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":951
+      /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":949
  * cdef inline int import_umath() except -1:
  *     try:
  *         _import_umath()             # <<<<<<<<<<<<<<
  *     except Exception:
  *         raise ImportError("numpy.core.umath failed to import")
  */
-      __pyx_t_4 = _import_umath(); if (unlikely(__pyx_t_4 == ((int)-1))) __PYX_ERR(2, 951, __pyx_L3_error)
+      __pyx_t_4 = _import_umath(); if (unlikely(__pyx_t_4 == ((int)-1))) __PYX_ERR(2, 949, __pyx_L3_error)
 
-      /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":950
+      /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":948
  * 
  * cdef inline int import_umath() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         _import_umath()
  *     except Exception:
  */
     }
     __Pyx_XDECREF(__pyx_t_1); __pyx_t_1 = 0;
     __Pyx_XDECREF(__pyx_t_2); __pyx_t_2 = 0;
     __Pyx_XDECREF(__pyx_t_3); __pyx_t_3 = 0;
     goto __pyx_L8_try_end;
     __pyx_L3_error:;
 
-    /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":952
+    /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":950
  *     try:
  *         _import_umath()
  *     except Exception:             # <<<<<<<<<<<<<<
  *         raise ImportError("numpy.core.umath failed to import")
  * 
  */
     __pyx_t_4 = __Pyx_PyErr_ExceptionMatches(((PyObject *)(&((PyTypeObject*)PyExc_Exception)[0])));
     if (__pyx_t_4) {
       __Pyx_AddTraceback("numpy.import_umath", __pyx_clineno, __pyx_lineno, __pyx_filename);
-      if (__Pyx_GetException(&__pyx_t_5, &__pyx_t_6, &__pyx_t_7) < 0) __PYX_ERR(2, 952, __pyx_L5_except_error)
+      if (__Pyx_GetException(&__pyx_t_5, &__pyx_t_6, &__pyx_t_7) < 0) __PYX_ERR(2, 950, __pyx_L5_except_error)
       __Pyx_GOTREF(__pyx_t_5);
       __Pyx_GOTREF(__pyx_t_6);
       __Pyx_GOTREF(__pyx_t_7);
 
-      /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":953
+      /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":951
  *         _import_umath()
  *     except Exception:
  *         raise ImportError("numpy.core.umath failed to import")             # <<<<<<<<<<<<<<
  * 
  * cdef inline int import_ufunc() except -1:
  */
-      __pyx_t_8 = __Pyx_PyObject_Call(__pyx_builtin_ImportError, __pyx_tuple__2, NULL); if (unlikely(!__pyx_t_8)) __PYX_ERR(2, 953, __pyx_L5_except_error)
+      __pyx_t_8 = __Pyx_PyObject_Call(__pyx_builtin_ImportError, __pyx_tuple__2, NULL); if (unlikely(!__pyx_t_8)) __PYX_ERR(2, 951, __pyx_L5_except_error)
       __Pyx_GOTREF(__pyx_t_8);
       __Pyx_Raise(__pyx_t_8, 0, 0, 0);
       __Pyx_DECREF(__pyx_t_8); __pyx_t_8 = 0;
-      __PYX_ERR(2, 953, __pyx_L5_except_error)
+      __PYX_ERR(2, 951, __pyx_L5_except_error)
     }
     goto __pyx_L5_except_error;
     __pyx_L5_except_error:;
 
-    /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":950
+    /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":948
  * 
  * cdef inline int import_umath() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         _import_umath()
  *     except Exception:
  */
     __Pyx_XGIVEREF(__pyx_t_1);
     __Pyx_XGIVEREF(__pyx_t_2);
     __Pyx_XGIVEREF(__pyx_t_3);
     __Pyx_ExceptionReset(__pyx_t_1, __pyx_t_2, __pyx_t_3);
     goto __pyx_L1_error;
     __pyx_L8_try_end:;
   }
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":949
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":947
  *         raise ImportError("numpy.core.multiarray failed to import")
  * 
  * cdef inline int import_umath() except -1:             # <<<<<<<<<<<<<<
  *     try:
  *         _import_umath()
  */
 
@@ -8003,15 +8000,15 @@
   __Pyx_AddTraceback("numpy.import_umath", __pyx_clineno, __pyx_lineno, __pyx_filename);
   __pyx_r = -1;
   __pyx_L0:;
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":955
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":953
  *         raise ImportError("numpy.core.umath failed to import")
  * 
  * cdef inline int import_ufunc() except -1:             # <<<<<<<<<<<<<<
  *     try:
  *         _import_umath()
  */
 
@@ -8027,15 +8024,15 @@
   PyObject *__pyx_t_7 = NULL;
   PyObject *__pyx_t_8 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("import_ufunc", 0);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":956
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":954
  * 
  * cdef inline int import_ufunc() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         _import_umath()
  *     except Exception:
  */
   {
@@ -8043,84 +8040,84 @@
     __Pyx_PyThreadState_assign
     __Pyx_ExceptionSave(&__pyx_t_1, &__pyx_t_2, &__pyx_t_3);
     __Pyx_XGOTREF(__pyx_t_1);
     __Pyx_XGOTREF(__pyx_t_2);
     __Pyx_XGOTREF(__pyx_t_3);
     /*try:*/ {
 
-      /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":957
+      /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":955
  * cdef inline int import_ufunc() except -1:
  *     try:
  *         _import_umath()             # <<<<<<<<<<<<<<
  *     except Exception:
  *         raise ImportError("numpy.core.umath failed to import")
  */
-      __pyx_t_4 = _import_umath(); if (unlikely(__pyx_t_4 == ((int)-1))) __PYX_ERR(2, 957, __pyx_L3_error)
+      __pyx_t_4 = _import_umath(); if (unlikely(__pyx_t_4 == ((int)-1))) __PYX_ERR(2, 955, __pyx_L3_error)
 
-      /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":956
+      /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":954
  * 
  * cdef inline int import_ufunc() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         _import_umath()
  *     except Exception:
  */
     }
     __Pyx_XDECREF(__pyx_t_1); __pyx_t_1 = 0;
     __Pyx_XDECREF(__pyx_t_2); __pyx_t_2 = 0;
     __Pyx_XDECREF(__pyx_t_3); __pyx_t_3 = 0;
     goto __pyx_L8_try_end;
     __pyx_L3_error:;
 
-    /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":958
+    /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":956
  *     try:
  *         _import_umath()
  *     except Exception:             # <<<<<<<<<<<<<<
  *         raise ImportError("numpy.core.umath failed to import")
  * 
  */
     __pyx_t_4 = __Pyx_PyErr_ExceptionMatches(((PyObject *)(&((PyTypeObject*)PyExc_Exception)[0])));
     if (__pyx_t_4) {
       __Pyx_AddTraceback("numpy.import_ufunc", __pyx_clineno, __pyx_lineno, __pyx_filename);
-      if (__Pyx_GetException(&__pyx_t_5, &__pyx_t_6, &__pyx_t_7) < 0) __PYX_ERR(2, 958, __pyx_L5_except_error)
+      if (__Pyx_GetException(&__pyx_t_5, &__pyx_t_6, &__pyx_t_7) < 0) __PYX_ERR(2, 956, __pyx_L5_except_error)
       __Pyx_GOTREF(__pyx_t_5);
       __Pyx_GOTREF(__pyx_t_6);
       __Pyx_GOTREF(__pyx_t_7);
 
-      /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":959
+      /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":957
  *         _import_umath()
  *     except Exception:
  *         raise ImportError("numpy.core.umath failed to import")             # <<<<<<<<<<<<<<
  * 
  * cdef extern from *:
  */
-      __pyx_t_8 = __Pyx_PyObject_Call(__pyx_builtin_ImportError, __pyx_tuple__2, NULL); if (unlikely(!__pyx_t_8)) __PYX_ERR(2, 959, __pyx_L5_except_error)
+      __pyx_t_8 = __Pyx_PyObject_Call(__pyx_builtin_ImportError, __pyx_tuple__2, NULL); if (unlikely(!__pyx_t_8)) __PYX_ERR(2, 957, __pyx_L5_except_error)
       __Pyx_GOTREF(__pyx_t_8);
       __Pyx_Raise(__pyx_t_8, 0, 0, 0);
       __Pyx_DECREF(__pyx_t_8); __pyx_t_8 = 0;
-      __PYX_ERR(2, 959, __pyx_L5_except_error)
+      __PYX_ERR(2, 957, __pyx_L5_except_error)
     }
     goto __pyx_L5_except_error;
     __pyx_L5_except_error:;
 
-    /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":956
+    /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":954
  * 
  * cdef inline int import_ufunc() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         _import_umath()
  *     except Exception:
  */
     __Pyx_XGIVEREF(__pyx_t_1);
     __Pyx_XGIVEREF(__pyx_t_2);
     __Pyx_XGIVEREF(__pyx_t_3);
     __Pyx_ExceptionReset(__pyx_t_1, __pyx_t_2, __pyx_t_3);
     goto __pyx_L1_error;
     __pyx_L8_try_end:;
   }
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":955
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":953
  *         raise ImportError("numpy.core.umath failed to import")
  * 
  * cdef inline int import_ufunc() except -1:             # <<<<<<<<<<<<<<
  *     try:
  *         _import_umath()
  */
 
@@ -8135,176 +8132,176 @@
   __Pyx_AddTraceback("numpy.import_ufunc", __pyx_clineno, __pyx_lineno, __pyx_filename);
   __pyx_r = -1;
   __pyx_L0:;
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":969
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":967
  * 
  * 
  * cdef inline bint is_timedelta64_object(object obj):             # <<<<<<<<<<<<<<
  *     """
  *     Cython equivalent of `isinstance(obj, np.timedelta64)`
  */
 
 static CYTHON_INLINE int __pyx_f_5numpy_is_timedelta64_object(PyObject *__pyx_v_obj) {
   int __pyx_r;
   __Pyx_RefNannyDeclarations
   __Pyx_RefNannySetupContext("is_timedelta64_object", 0);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":981
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":979
  *     bool
  *     """
  *     return PyObject_TypeCheck(obj, &PyTimedeltaArrType_Type)             # <<<<<<<<<<<<<<
  * 
  * 
  */
   __pyx_r = PyObject_TypeCheck(__pyx_v_obj, (&PyTimedeltaArrType_Type));
   goto __pyx_L0;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":969
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":967
  * 
  * 
  * cdef inline bint is_timedelta64_object(object obj):             # <<<<<<<<<<<<<<
  *     """
  *     Cython equivalent of `isinstance(obj, np.timedelta64)`
  */
 
   /* function exit code */
   __pyx_L0:;
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":984
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":982
  * 
  * 
  * cdef inline bint is_datetime64_object(object obj):             # <<<<<<<<<<<<<<
  *     """
  *     Cython equivalent of `isinstance(obj, np.datetime64)`
  */
 
 static CYTHON_INLINE int __pyx_f_5numpy_is_datetime64_object(PyObject *__pyx_v_obj) {
   int __pyx_r;
   __Pyx_RefNannyDeclarations
   __Pyx_RefNannySetupContext("is_datetime64_object", 0);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":996
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":994
  *     bool
  *     """
  *     return PyObject_TypeCheck(obj, &PyDatetimeArrType_Type)             # <<<<<<<<<<<<<<
  * 
  * 
  */
   __pyx_r = PyObject_TypeCheck(__pyx_v_obj, (&PyDatetimeArrType_Type));
   goto __pyx_L0;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":984
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":982
  * 
  * 
  * cdef inline bint is_datetime64_object(object obj):             # <<<<<<<<<<<<<<
  *     """
  *     Cython equivalent of `isinstance(obj, np.datetime64)`
  */
 
   /* function exit code */
   __pyx_L0:;
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":999
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":997
  * 
  * 
  * cdef inline npy_datetime get_datetime64_value(object obj) nogil:             # <<<<<<<<<<<<<<
  *     """
  *     returns the int64 value underlying scalar numpy datetime64 object
  */
 
 static CYTHON_INLINE npy_datetime __pyx_f_5numpy_get_datetime64_value(PyObject *__pyx_v_obj) {
   npy_datetime __pyx_r;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":1006
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":1004
  *     also needed.  That can be found using `get_datetime64_unit`.
  *     """
  *     return (<PyDatetimeScalarObject*>obj).obval             # <<<<<<<<<<<<<<
  * 
  * 
  */
   __pyx_r = ((PyDatetimeScalarObject *)__pyx_v_obj)->obval;
   goto __pyx_L0;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":999
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":997
  * 
  * 
  * cdef inline npy_datetime get_datetime64_value(object obj) nogil:             # <<<<<<<<<<<<<<
  *     """
  *     returns the int64 value underlying scalar numpy datetime64 object
  */
 
   /* function exit code */
   __pyx_L0:;
   return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":1009
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":1007
  * 
  * 
  * cdef inline npy_timedelta get_timedelta64_value(object obj) nogil:             # <<<<<<<<<<<<<<
  *     """
  *     returns the int64 value underlying scalar numpy timedelta64 object
  */
 
 static CYTHON_INLINE npy_timedelta __pyx_f_5numpy_get_timedelta64_value(PyObject *__pyx_v_obj) {
   npy_timedelta __pyx_r;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":1013
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":1011
  *     returns the int64 value underlying scalar numpy timedelta64 object
  *     """
  *     return (<PyTimedeltaScalarObject*>obj).obval             # <<<<<<<<<<<<<<
  * 
  * 
  */
   __pyx_r = ((PyTimedeltaScalarObject *)__pyx_v_obj)->obval;
   goto __pyx_L0;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":1009
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":1007
  * 
  * 
  * cdef inline npy_timedelta get_timedelta64_value(object obj) nogil:             # <<<<<<<<<<<<<<
  *     """
  *     returns the int64 value underlying scalar numpy timedelta64 object
  */
 
   /* function exit code */
   __pyx_L0:;
   return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":1016
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":1014
  * 
  * 
  * cdef inline NPY_DATETIMEUNIT get_datetime64_unit(object obj) nogil:             # <<<<<<<<<<<<<<
  *     """
  *     returns the unit part of the dtype for a numpy datetime64 object.
  */
 
 static CYTHON_INLINE NPY_DATETIMEUNIT __pyx_f_5numpy_get_datetime64_unit(PyObject *__pyx_v_obj) {
   NPY_DATETIMEUNIT __pyx_r;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":1020
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":1018
  *     returns the unit part of the dtype for a numpy datetime64 object.
  *     """
  *     return <NPY_DATETIMEUNIT>(<PyDatetimeScalarObject*>obj).obmeta.base             # <<<<<<<<<<<<<<
  */
   __pyx_r = ((NPY_DATETIMEUNIT)((PyDatetimeScalarObject *)__pyx_v_obj)->obmeta.base);
   goto __pyx_L0;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":1016
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":1014
  * 
  * 
  * cdef inline NPY_DATETIMEUNIT get_datetime64_unit(object obj) nogil:             # <<<<<<<<<<<<<<
  *     """
  *     returns the unit part of the dtype for a numpy datetime64 object.
  */
 
@@ -8720,43 +8717,43 @@
   {&__pyx_n_s_zeros, __pyx_k_zeros, sizeof(__pyx_k_zeros), 0, 0, 1, 1},
   {0, 0, 0, 0, 0, 0, 0}
 };
 static CYTHON_SMALL_CODE int __Pyx_InitCachedBuiltins(void) {
   __pyx_builtin_staticmethod = __Pyx_GetBuiltinName(__pyx_n_s_staticmethod); if (!__pyx_builtin_staticmethod) __PYX_ERR(0, 116, __pyx_L1_error)
   __pyx_builtin_IndexError = __Pyx_GetBuiltinName(__pyx_n_s_IndexError); if (!__pyx_builtin_IndexError) __PYX_ERR(0, 160, __pyx_L1_error)
   __pyx_builtin_range = __Pyx_GetBuiltinName(__pyx_n_s_range); if (!__pyx_builtin_range) __PYX_ERR(0, 233, __pyx_L1_error)
-  __pyx_builtin_ImportError = __Pyx_GetBuiltinName(__pyx_n_s_ImportError); if (!__pyx_builtin_ImportError) __PYX_ERR(2, 947, __pyx_L1_error)
+  __pyx_builtin_ImportError = __Pyx_GetBuiltinName(__pyx_n_s_ImportError); if (!__pyx_builtin_ImportError) __PYX_ERR(2, 945, __pyx_L1_error)
   return 0;
   __pyx_L1_error:;
   return -1;
 }
 
 static CYTHON_SMALL_CODE int __Pyx_InitCachedConstants(void) {
   __Pyx_RefNannyDeclarations
   __Pyx_RefNannySetupContext("__Pyx_InitCachedConstants", 0);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":947
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":945
  *         __pyx_import_array()
  *     except Exception:
  *         raise ImportError("numpy.core.multiarray failed to import")             # <<<<<<<<<<<<<<
  * 
  * cdef inline int import_umath() except -1:
  */
-  __pyx_tuple_ = PyTuple_Pack(1, __pyx_kp_u_numpy_core_multiarray_failed_to); if (unlikely(!__pyx_tuple_)) __PYX_ERR(2, 947, __pyx_L1_error)
+  __pyx_tuple_ = PyTuple_Pack(1, __pyx_kp_u_numpy_core_multiarray_failed_to); if (unlikely(!__pyx_tuple_)) __PYX_ERR(2, 945, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_tuple_);
   __Pyx_GIVEREF(__pyx_tuple_);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":953
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":951
  *         _import_umath()
  *     except Exception:
  *         raise ImportError("numpy.core.umath failed to import")             # <<<<<<<<<<<<<<
  * 
  * cdef inline int import_ufunc() except -1:
  */
-  __pyx_tuple__2 = PyTuple_Pack(1, __pyx_kp_u_numpy_core_umath_failed_to_impor); if (unlikely(!__pyx_tuple__2)) __PYX_ERR(2, 953, __pyx_L1_error)
+  __pyx_tuple__2 = PyTuple_Pack(1, __pyx_kp_u_numpy_core_umath_failed_to_impor); if (unlikely(!__pyx_tuple__2)) __PYX_ERR(2, 951, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_tuple__2);
   __Pyx_GIVEREF(__pyx_tuple__2);
 
   /* "river/drift/adwin_c.pyx":42
  *         self.min_window_length = 5
  * 
  *     def reset(self):             # <<<<<<<<<<<<<<
@@ -9553,15 +9550,15 @@
  * from libc.math cimport sqrt, log, fabs, pow
  */
   __pyx_t_2 = __Pyx_PyDict_NewPresized(0); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 1, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_2);
   if (PyDict_SetItem(__pyx_d, __pyx_n_s_test, __pyx_t_2) < 0) __PYX_ERR(0, 1, __pyx_L1_error)
   __Pyx_DECREF(__pyx_t_2); __pyx_t_2 = 0;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":1016
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":1014
  * 
  * 
  * cdef inline NPY_DATETIMEUNIT get_datetime64_unit(object obj) nogil:             # <<<<<<<<<<<<<<
  *     """
  *     returns the unit part of the dtype for a numpy datetime64 object.
  */
```

### Comparing `river-0.8.0/river/drift/adwin_c.pyx` & `river-0.9.0/river/drift/adwin_c.pyx`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,361 +1,361 @@
-# cython: boundscheck=False
-
-from libc.math cimport sqrt, log, fabs, pow
-import numpy as np
-cimport numpy as np
-from typing import Deque
-from collections import deque
-
-
-cdef class AdaptiveWindowing:
-    """ The helper class for ADWIN
-
-    Parameters
-    ----------
-    delta
-        Confidence value.
-
-    """
-    cdef:
-        dict __dict__
-        double delta, total, variance, total_width, width
-        int n_buckets, grace_period, min_window_length, tick, n_detections,\
-            clock, max_n_buckets, detect, detect_twice
-
-    MAX_BUCKETS = 5
-
-    def __init__(self, delta=.002):
-        self.delta = delta
-        self.bucket_deque: Deque['Bucket'] = deque([Bucket()])
-        self.total = 0.
-        self.variance = 0.
-        self.width = 0.
-        self.n_buckets = 0
-        self.grace_period = 10
-        self.tick = 0
-        self.total_width = 0
-        self.n_detections = 0
-        self.clock = 32
-        self.max_n_buckets = 0
-        self.min_window_length = 5
-
-    def reset(self):
-        """Reset the change detector.
-        """
-        self.__init__(delta=self.delta)
-
-    def get_delta(self):
-        return self.delta
-
-    def get_n_detections(self):
-        return self.n_detections
-
-    def get_width(self):
-        return self.width
-
-    def get_total(self):
-        return self.total
-
-    def get_variance(self):
-        return self.variance
-
-    @property
-    def variance_in_window(self):
-        return self.variance / self.width
-
-    def update(self, value: float):
-        """Update the change detector with a single data point.
-
-        Apart from adding the element value to the window, by inserting it in
-        the correct bucket, it will also update the relevant statistics, in
-        this case the total sum of all values, the window width and the total
-        variance.
-
-        Parameters
-        ----------
-        value
-            Input value
-
-        Returns
-        -------
-        bool
-            If True then a change is detected.
-
-        """
-        return self._update(value)
-
-    cdef bint _update(self, double value):
-        # Increment window with one element
-        self._insert_element(value, 0.0)
-
-        return self._detect_change()
-
-    cdef void _insert_element(self, double value, double variance):
-        cdef Bucket bucket = self.bucket_deque[0]
-        bucket.insert_data(value, variance)
-        self.n_buckets += 1
-
-        if self.n_buckets > self.max_n_buckets:
-            self.max_n_buckets = self.n_buckets
-
-        # Update width, variance and total
-        self.width += 1
-        cdef double incremental_variance = 0.0
-        if self.width > 1.0:
-            incremental_variance = (
-                    (self.width - 1.0)
-                    * (value - self.total / (self.width - 1.0))
-                    * (value - self.total / (self.width - 1.0))
-                    / self.width
-            )
-        self.variance += incremental_variance
-        self.total += value
-
-        self._compress_buckets()
-
-    @staticmethod
-    def _calculate_bucket_size(row: int):
-        return pow(2, row)
-
-    cdef double _delete_element(self):
-        cdef Bucket bucket = self.bucket_deque[-1]
-        cdef double n = self._calculate_bucket_size(len(self.bucket_deque) - 1) # length of bucket
-        cdef double u = bucket.get_total_at(0)     # total of bucket
-        cdef double mu = u / n                   # mean of bucket
-        cdef double v = bucket.get_variance_at(0)  # variance of bucket
-
-        # Update width, total and variance
-        self.width -= n
-        self.total -= u
-        mu_window = self.total / self.width     # mean of the window
-        cdef double incremental_variance = (
-                v + n * self.width * (mu - mu_window) * (mu - mu_window)
-                / (n + self.width)
-        )
-        self.variance -= incremental_variance
-
-        bucket.remove()
-        self.n_buckets -= 1
-
-        if bucket.current_idx == 0:
-            self.bucket_deque.pop()
-
-        return n
-
-    cdef void _compress_buckets(self):
-
-        cdef:
-            unsigned int idx, k
-            double n1, n2, mu1, mu2, temp, total12
-            Bucket bucket, next_bucket
-
-        bucket = self.bucket_deque[0]
-        idx = 0
-        while bucket is not None:
-            k = bucket.current_idx
-            # Merge buckets if there are more than MAX_BUCKETS
-            if k == self.MAX_BUCKETS + 1:
-                try:
-                    next_bucket = self.bucket_deque[idx + 1]
-                except IndexError:
-                    self.bucket_deque.append(Bucket())
-                    next_bucket = self.bucket_deque[-1]
-                n1 = self._calculate_bucket_size(idx)   # length of bucket 1
-                n2 = self._calculate_bucket_size(idx)   # length of bucket 2
-                mu1 = bucket.get_total_at(0) / n1       # mean of bucket 1
-                mu2 = bucket.get_total_at(1) / n2       # mean of bucket 2
-
-                # Combine total and variance of adjacent buckets
-                total12 = bucket.get_total_at(0) + bucket.get_total_at(1)
-                temp = n1 * n2 * (mu1 - mu2) * (mu1 - mu2) / (n1 + n2)
-                v12 = bucket.get_variance_at(0) + bucket.get_variance_at(1) + temp
-                next_bucket.insert_data(total12, v12)
-                self.n_buckets += 1
-                bucket.compress(2)
-
-                if next_bucket.current_idx <= self.MAX_BUCKETS:
-                    break
-            else:
-                break
-
-            try:
-                bucket = self.bucket_deque[idx + 1]
-            except IndexError:
-                bucket = None
-            idx += 1
-
-    cdef bint _detect_change(self):
-        """Detect concept change.
-
-        This function is responsible for analysing different cutting points in
-        the sliding window, to verify if there is a significant change.
-
-        Returns
-        -------
-        bint
-            If True then a change is detected.
-
-        Notes
-        -----
-        Variance calculation is based on:
-
-        Babcock, B., Datar, M., Motwani, R., & O’Callaghan, L. (2003).
-        Maintaining Variance and k-Medians over Data Stream Windows.
-        Proceedings of the ACM SIGACT-SIGMOD-SIGART
-        Symposium on Principles of Database Systems, 22, 234–243.
-        https://doi.org/10.1145/773153.773176
-
-        """
-        cdef:
-            unsigned int idx, k
-            bint change_detected, exit_flag
-            double n0, n1, n2, u0, u1, u2, v0, v1
-            Bucket bucket
-
-        change_detected = False
-        exit_flag = False
-        self.tick += 1
-
-        # Reduce window
-        if (self.tick % self.clock == 0) and (self.width > self.grace_period):
-            reduce_width = True
-            while reduce_width:
-                reduce_width = False
-                exit_flag = False
-                n0 = 0.0            # length of window 0
-                n1 = self.width     # length of window 1
-                u0 = 0.0            # total of window 0
-                u1 = self.total     # total of window 1
-                v0 = 0              # variance of window 0
-                v1 = self.variance  # variance of window 1
-
-                # Evaluate each window cut (W_0, W_1)
-                for idx in range(len(self.bucket_deque) - 1, -1 , -1):
-                    if exit_flag:
-                        break
-                    bucket = self.bucket_deque[idx]
-
-                    for k in range(bucket.current_idx - 1):
-                        n2 = self._calculate_bucket_size(idx)   # length of window 2
-                        u2 = bucket.get_total_at(k)             # total of window 2
-                        # Warning: means are calculated inside the loop to get updated values.
-                        mu2 = u2 / n2   # mean of window 2
-
-                        if n0 > 0.0:
-                            mu0 = u0 / n0  # mean of window 0
-                            v0 += (
-                                    bucket.get_variance_at(k) + n0 * n2
-                                    * (mu0 - mu2) * (mu0 - mu2)
-                                    / (n0 + n2)
-                            )
-
-                        if n1 > 0.0:
-                            mu1 = u1 / n1  # mean of window 1
-                            v1 -= (
-                                    bucket.get_variance_at(k) + n1 * n2
-                                    * (mu1 - mu2) * (mu1 - mu2)
-                                    / (n1 + n2)
-                            )
-
-                        # Update window 0 and 1
-                        n0 += self._calculate_bucket_size(idx)
-                        n1 -= self._calculate_bucket_size(idx)
-                        u0 += bucket.get_total_at(k)
-                        u1 -= bucket.get_total_at(k)
-
-                        if (idx == 0) and (k == bucket.current_idx - 1):
-                            exit_flag = True    # We are done
-                            break
-
-                        # Check if delta_mean < epsilon_cut holds
-                        # Note: Must re-calculate means per updated values
-                        delta_mean = (u0 / n0) - (u1 / n1)
-                        if (
-                                n1 >= self.min_window_length
-                                and n0 >= self.min_window_length
-                                and self._evaluate_cut(n0, n1, delta_mean, self.delta)
-                        ):
-                            # Change detected
-
-                            reduce_width = True
-                            change_detected = True
-                            if self.width > 0:
-                                # Reduce the width of the window
-                                n0 -= self._delete_element()
-                                exit_flag = True    # We are done
-                                break
-
-        self.total_width += self.width
-        if change_detected:
-            self.n_detections += 1
-
-        return change_detected
-
-    cdef bint _evaluate_cut(self, double n0, double n1,
-                            double delta_mean, double delta):
-        cdef:
-            double delta_prime, m_recip, epsilon
-        delta_prime = log(2 * log(self.width) / delta)
-        # Use reciprocal of m to avoid extra divisions when calculating epsilon
-        m_recip = ((1.0 / (n0 - self.min_window_length + 1))
-                   + (1.0 / (n1 - self.min_window_length + 1)))
-        epsilon = (sqrt(2 * m_recip * self.variance_in_window * delta_prime)
-                   + 2 / 3 * delta_prime * m_recip)
-        return fabs(delta_mean) > epsilon
-
-
-cdef class Bucket:
-    """ A bucket class to keep statistics.
-
-    A bucket stores the summary structure for a contiguous set of data elements.
-    In this implementation fixed-size arrays are used for efficiency. The index
-    of the "current" element is used to simulate the dynamic size of the bucket.
-
-    """
-    cdef:
-        int current_idx, max_size
-        np.ndarray total_array, variance_array
-
-    def __init__(self):
-        self.max_size = AdaptiveWindowing.MAX_BUCKETS
-
-        self.current_idx = 0
-        self.total_array = np.zeros(self.max_size + 1, dtype=float)
-        self.variance_array = np.zeros(self.max_size + 1, dtype=float)
-
-    cdef void clear_at(self, int index):
-        self.set_total_at(0.0, index)
-        self.set_variance_at(0.0, index)
-
-    cdef void insert_data(self, double value, double variance):
-        self.set_total_at(value, self.current_idx)
-        self.set_variance_at(variance, self.current_idx)
-        self.current_idx += 1
-
-    cdef void remove(self):
-        self.compress(1)
-
-    cdef void compress(self, int n_elements):
-        cdef unsigned int i
-        cdef int window_len = len(self.total_array)
-        # Remove first n_elements by shifting elements to the left
-        for i in range(n_elements, window_len):
-            self.total_array[i - n_elements] = self.total_array[i]
-            self.variance_array[i - n_elements] = self.variance_array[i]
-        # Clear remaining elements
-        for i in range(window_len - n_elements, window_len):
-            self.clear_at(i)
-
-        self.current_idx -= n_elements
-
-    cdef double get_total_at(self, int index):
-        return self.total_array[index]
-
-    cdef double get_variance_at(self, int index):
-        return self.variance_array[index]
-
-    cdef void set_total_at(self, double value, int index):
-        self.total_array[index] = value
-
-    cdef void set_variance_at(self, double value, int index):
-        self.variance_array[index] = value
+# cython: boundscheck=False
+
+from libc.math cimport sqrt, log, fabs, pow
+import numpy as np
+cimport numpy as np
+from typing import Deque
+from collections import deque
+
+
+cdef class AdaptiveWindowing:
+    """ The helper class for ADWIN
+
+    Parameters
+    ----------
+    delta
+        Confidence value.
+
+    """
+    cdef:
+        dict __dict__
+        double delta, total, variance, total_width, width
+        int n_buckets, grace_period, min_window_length, tick, n_detections,\
+            clock, max_n_buckets, detect, detect_twice
+
+    MAX_BUCKETS = 5
+
+    def __init__(self, delta=.002):
+        self.delta = delta
+        self.bucket_deque: Deque['Bucket'] = deque([Bucket()])
+        self.total = 0.
+        self.variance = 0.
+        self.width = 0.
+        self.n_buckets = 0
+        self.grace_period = 10
+        self.tick = 0
+        self.total_width = 0
+        self.n_detections = 0
+        self.clock = 32
+        self.max_n_buckets = 0
+        self.min_window_length = 5
+
+    def reset(self):
+        """Reset the change detector.
+        """
+        self.__init__(delta=self.delta)
+
+    def get_delta(self):
+        return self.delta
+
+    def get_n_detections(self):
+        return self.n_detections
+
+    def get_width(self):
+        return self.width
+
+    def get_total(self):
+        return self.total
+
+    def get_variance(self):
+        return self.variance
+
+    @property
+    def variance_in_window(self):
+        return self.variance / self.width
+
+    def update(self, value: float):
+        """Update the change detector with a single data point.
+
+        Apart from adding the element value to the window, by inserting it in
+        the correct bucket, it will also update the relevant statistics, in
+        this case the total sum of all values, the window width and the total
+        variance.
+
+        Parameters
+        ----------
+        value
+            Input value
+
+        Returns
+        -------
+        bool
+            If True then a change is detected.
+
+        """
+        return self._update(value)
+
+    cdef bint _update(self, double value):
+        # Increment window with one element
+        self._insert_element(value, 0.0)
+
+        return self._detect_change()
+
+    cdef void _insert_element(self, double value, double variance):
+        cdef Bucket bucket = self.bucket_deque[0]
+        bucket.insert_data(value, variance)
+        self.n_buckets += 1
+
+        if self.n_buckets > self.max_n_buckets:
+            self.max_n_buckets = self.n_buckets
+
+        # Update width, variance and total
+        self.width += 1
+        cdef double incremental_variance = 0.0
+        if self.width > 1.0:
+            incremental_variance = (
+                    (self.width - 1.0)
+                    * (value - self.total / (self.width - 1.0))
+                    * (value - self.total / (self.width - 1.0))
+                    / self.width
+            )
+        self.variance += incremental_variance
+        self.total += value
+
+        self._compress_buckets()
+
+    @staticmethod
+    def _calculate_bucket_size(row: int):
+        return pow(2, row)
+
+    cdef double _delete_element(self):
+        cdef Bucket bucket = self.bucket_deque[-1]
+        cdef double n = self._calculate_bucket_size(len(self.bucket_deque) - 1) # length of bucket
+        cdef double u = bucket.get_total_at(0)     # total of bucket
+        cdef double mu = u / n                   # mean of bucket
+        cdef double v = bucket.get_variance_at(0)  # variance of bucket
+
+        # Update width, total and variance
+        self.width -= n
+        self.total -= u
+        mu_window = self.total / self.width     # mean of the window
+        cdef double incremental_variance = (
+                v + n * self.width * (mu - mu_window) * (mu - mu_window)
+                / (n + self.width)
+        )
+        self.variance -= incremental_variance
+
+        bucket.remove()
+        self.n_buckets -= 1
+
+        if bucket.current_idx == 0:
+            self.bucket_deque.pop()
+
+        return n
+
+    cdef void _compress_buckets(self):
+
+        cdef:
+            unsigned int idx, k
+            double n1, n2, mu1, mu2, temp, total12
+            Bucket bucket, next_bucket
+
+        bucket = self.bucket_deque[0]
+        idx = 0
+        while bucket is not None:
+            k = bucket.current_idx
+            # Merge buckets if there are more than MAX_BUCKETS
+            if k == self.MAX_BUCKETS + 1:
+                try:
+                    next_bucket = self.bucket_deque[idx + 1]
+                except IndexError:
+                    self.bucket_deque.append(Bucket())
+                    next_bucket = self.bucket_deque[-1]
+                n1 = self._calculate_bucket_size(idx)   # length of bucket 1
+                n2 = self._calculate_bucket_size(idx)   # length of bucket 2
+                mu1 = bucket.get_total_at(0) / n1       # mean of bucket 1
+                mu2 = bucket.get_total_at(1) / n2       # mean of bucket 2
+
+                # Combine total and variance of adjacent buckets
+                total12 = bucket.get_total_at(0) + bucket.get_total_at(1)
+                temp = n1 * n2 * (mu1 - mu2) * (mu1 - mu2) / (n1 + n2)
+                v12 = bucket.get_variance_at(0) + bucket.get_variance_at(1) + temp
+                next_bucket.insert_data(total12, v12)
+                self.n_buckets += 1
+                bucket.compress(2)
+
+                if next_bucket.current_idx <= self.MAX_BUCKETS:
+                    break
+            else:
+                break
+
+            try:
+                bucket = self.bucket_deque[idx + 1]
+            except IndexError:
+                bucket = None
+            idx += 1
+
+    cdef bint _detect_change(self):
+        """Detect concept change.
+
+        This function is responsible for analysing different cutting points in
+        the sliding window, to verify if there is a significant change.
+
+        Returns
+        -------
+        bint
+            If True then a change is detected.
+
+        Notes
+        -----
+        Variance calculation is based on:
+
+        Babcock, B., Datar, M., Motwani, R., & O’Callaghan, L. (2003).
+        Maintaining Variance and k-Medians over Data Stream Windows.
+        Proceedings of the ACM SIGACT-SIGMOD-SIGART
+        Symposium on Principles of Database Systems, 22, 234–243.
+        https://doi.org/10.1145/773153.773176
+
+        """
+        cdef:
+            unsigned int idx, k
+            bint change_detected, exit_flag
+            double n0, n1, n2, u0, u1, u2, v0, v1
+            Bucket bucket
+
+        change_detected = False
+        exit_flag = False
+        self.tick += 1
+
+        # Reduce window
+        if (self.tick % self.clock == 0) and (self.width > self.grace_period):
+            reduce_width = True
+            while reduce_width:
+                reduce_width = False
+                exit_flag = False
+                n0 = 0.0            # length of window 0
+                n1 = self.width     # length of window 1
+                u0 = 0.0            # total of window 0
+                u1 = self.total     # total of window 1
+                v0 = 0              # variance of window 0
+                v1 = self.variance  # variance of window 1
+
+                # Evaluate each window cut (W_0, W_1)
+                for idx in range(len(self.bucket_deque) - 1, -1 , -1):
+                    if exit_flag:
+                        break
+                    bucket = self.bucket_deque[idx]
+
+                    for k in range(bucket.current_idx - 1):
+                        n2 = self._calculate_bucket_size(idx)   # length of window 2
+                        u2 = bucket.get_total_at(k)             # total of window 2
+                        # Warning: means are calculated inside the loop to get updated values.
+                        mu2 = u2 / n2   # mean of window 2
+
+                        if n0 > 0.0:
+                            mu0 = u0 / n0  # mean of window 0
+                            v0 += (
+                                    bucket.get_variance_at(k) + n0 * n2
+                                    * (mu0 - mu2) * (mu0 - mu2)
+                                    / (n0 + n2)
+                            )
+
+                        if n1 > 0.0:
+                            mu1 = u1 / n1  # mean of window 1
+                            v1 -= (
+                                    bucket.get_variance_at(k) + n1 * n2
+                                    * (mu1 - mu2) * (mu1 - mu2)
+                                    / (n1 + n2)
+                            )
+
+                        # Update window 0 and 1
+                        n0 += self._calculate_bucket_size(idx)
+                        n1 -= self._calculate_bucket_size(idx)
+                        u0 += bucket.get_total_at(k)
+                        u1 -= bucket.get_total_at(k)
+
+                        if (idx == 0) and (k == bucket.current_idx - 1):
+                            exit_flag = True    # We are done
+                            break
+
+                        # Check if delta_mean < epsilon_cut holds
+                        # Note: Must re-calculate means per updated values
+                        delta_mean = (u0 / n0) - (u1 / n1)
+                        if (
+                                n1 >= self.min_window_length
+                                and n0 >= self.min_window_length
+                                and self._evaluate_cut(n0, n1, delta_mean, self.delta)
+                        ):
+                            # Change detected
+
+                            reduce_width = True
+                            change_detected = True
+                            if self.width > 0:
+                                # Reduce the width of the window
+                                n0 -= self._delete_element()
+                                exit_flag = True    # We are done
+                                break
+
+        self.total_width += self.width
+        if change_detected:
+            self.n_detections += 1
+
+        return change_detected
+
+    cdef bint _evaluate_cut(self, double n0, double n1,
+                            double delta_mean, double delta):
+        cdef:
+            double delta_prime, m_recip, epsilon
+        delta_prime = log(2 * log(self.width) / delta)
+        # Use reciprocal of m to avoid extra divisions when calculating epsilon
+        m_recip = ((1.0 / (n0 - self.min_window_length + 1))
+                   + (1.0 / (n1 - self.min_window_length + 1)))
+        epsilon = (sqrt(2 * m_recip * self.variance_in_window * delta_prime)
+                   + 2 / 3 * delta_prime * m_recip)
+        return fabs(delta_mean) > epsilon
+
+
+cdef class Bucket:
+    """ A bucket class to keep statistics.
+
+    A bucket stores the summary structure for a contiguous set of data elements.
+    In this implementation fixed-size arrays are used for efficiency. The index
+    of the "current" element is used to simulate the dynamic size of the bucket.
+
+    """
+    cdef:
+        int current_idx, max_size
+        np.ndarray total_array, variance_array
+
+    def __init__(self):
+        self.max_size = AdaptiveWindowing.MAX_BUCKETS
+
+        self.current_idx = 0
+        self.total_array = np.zeros(self.max_size + 1, dtype=float)
+        self.variance_array = np.zeros(self.max_size + 1, dtype=float)
+
+    cdef void clear_at(self, int index):
+        self.set_total_at(0.0, index)
+        self.set_variance_at(0.0, index)
+
+    cdef void insert_data(self, double value, double variance):
+        self.set_total_at(value, self.current_idx)
+        self.set_variance_at(variance, self.current_idx)
+        self.current_idx += 1
+
+    cdef void remove(self):
+        self.compress(1)
+
+    cdef void compress(self, int n_elements):
+        cdef unsigned int i
+        cdef int window_len = len(self.total_array)
+        # Remove first n_elements by shifting elements to the left
+        for i in range(n_elements, window_len):
+            self.total_array[i - n_elements] = self.total_array[i]
+            self.variance_array[i - n_elements] = self.variance_array[i]
+        # Clear remaining elements
+        for i in range(window_len - n_elements, window_len):
+            self.clear_at(i)
+
+        self.current_idx -= n_elements
+
+    cdef double get_total_at(self, int index):
+        return self.total_array[index]
+
+    cdef double get_variance_at(self, int index):
+        return self.variance_array[index]
+
+    cdef void set_total_at(self, double value, int index):
+        self.total_array[index] = value
+
+    cdef void set_variance_at(self, double value, int index):
+        self.variance_array[index] = value
```

### Comparing `river-0.8.0/river/drift/ddm.py` & `river-0.9.0/river/drift/ddm.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,166 +1,166 @@
-import math
-import numbers
-
-from river.base import DriftDetector
-
-
-class DDM(DriftDetector):
-    r"""Drift Detection Method.
-
-    DDM (Drift Detection Method) is a concept change detection method
-    based on the PAC learning model premise, that the learner's error rate
-    will decrease as the number of analysed samples increase, as long as the
-    data distribution is stationary.
-
-    If the algorithm detects an increase in the error rate, that surpasses
-    a calculated threshold, either change is detected or the algorithm will
-    warn the user that change may occur in the near future, which is called
-    the warning zone.
-
-    The detection threshold is calculated in function of two statistics,
-    obtained when $(p_i + s_i)$ is minimum:
-
-    * $p_{min}$: The minimum recorded error rate.
-
-    * $s_{min}$: The minimum recorded standard deviation.
-
-    At instant $i$, the detection algorithm uses:
-
-    * $p_i$: The error rate at instant $i$.
-
-    * $s_i$: The standard deviation at instant $i$.
-
-    The conditions for entering the warning zone and detecting change are
-    as follows [see implementation note bellow]:
-
-    * if $p_i + s_i \geq p_{min} + 2 * s_{min}$ -> Warning zone
-
-    * if $p_i + s_i \geq p_{min} + 3 * s_{min}$ -> Change detected
-
-    **Input:** `value` must be a binary signal, where 0 indicates error.
-    For example, if a classifier's prediction $y'$ is right or wrong w.r.t the
-    true target label $y$:
-
-    - 0: Correct, $y=y'$
-
-    - 1: Error, $y \neq y'$
-
-    Parameters
-    ----------
-    min_num_instances
-        The minimum required number of analyzed samples so change can be detected. This is used to
-        avoid false detections during the early moments of the detector, when the weight of one
-        sample is important.
-    warning_level
-        Warning level.
-    out_control_level
-        Out-control level.
-
-    Examples
-    --------
-    >>> import numpy as np
-    >>> from river.drift import DDM
-    >>> np.random.seed(12345)
-
-    >>> ddm = DDM()
-
-    >>> # Simulate a data stream as a normal distribution of 1's and 0's
-    >>> data_stream = np.random.randint(2, size=2000)
-    >>> # Change the data distribution from index 999 to 1500, simulating an
-    >>> # increase in error rate (1 indicates error)
-    >>> data_stream[1000:1200] = [np.random.binomial(1, .8) for _ in range(200)]
-
-    >>> # Update drift detector and verify if change is detected
-    >>> for i, val in enumerate(data_stream):
-    ...     in_drift, in_warning = ddm.update(val)
-    ...     if in_drift:
-    ...         print(f"Change detected at index {i}, input value: {val}")
-    Change detected at index 1157, input value: 1
-
-    Notes
-    -----
-    In this implementation, the conditions to signal drift and warning are
-    $p_i + s_i > thershold$ instead of $p_i + s_i \geq thershold$. This is to
-    avoid a corner case when a classifier is consistently wrong (`value=1`)
-    that results in DDM indicating a drift every `min_num_instances`. This
-    modification is consistent with the implementation in MOA.
-
-    References
-    ----------
-    [^1]: João Gama, Pedro Medas, Gladys Castillo, Pedro Pereira Rodrigues: Learning with Drift Detection. SBIA 2004: 286-295
-
-    """
-
-    def __init__(self, min_num_instances=30, warning_level=2.0, out_control_level=3.0):
-        super().__init__()
-        self.sample_count = None
-        self.miss_prob = None
-        self.miss_std = None
-        self.miss_prob_sd_min = None
-        self.miss_prob_min = None
-        self.miss_sd_min = None
-        self.min_num_instances = min_num_instances
-        self.warning_level = warning_level
-        self.out_control_level = out_control_level
-        self.estimation = None
-        self.reset()
-
-    def reset(self):
-        """Reset the change detector."""
-        super().reset()
-        self.sample_count = 1
-        self.miss_prob = 1.0
-        self.miss_std = 0.0
-        self.miss_prob_sd_min = float("inf")
-        self.miss_prob_min = float("inf")
-        self.miss_sd_min = float("inf")
-
-    def update(self, value: numbers.Number):
-        """Update the change detector with a single data point.
-
-        Parameters
-        ----------
-        value
-            This parameter indicates whether the last sample analyzed was correctly classified or
-            not. 1 indicates an error (miss-classification).
-
-        """
-        if self._in_concept_change:
-            self.reset()
-
-        self.miss_prob = self.miss_prob + (value - self.miss_prob) / float(
-            self.sample_count
-        )
-        self.miss_std = math.sqrt(
-            self.miss_prob * (1 - self.miss_prob) / float(self.sample_count)
-        )
-        self.sample_count += 1
-
-        self.estimation = self.miss_prob
-        self._in_concept_change = False
-        self._in_warning_zone = False
-
-        if self.sample_count <= self.min_num_instances:
-            return self._in_concept_change, self._in_warning_zone
-
-        if self.miss_prob + self.miss_std <= self.miss_prob_sd_min:
-            self.miss_prob_min = self.miss_prob
-            self.miss_sd_min = self.miss_std
-            self.miss_prob_sd_min = self.miss_prob + self.miss_std
-
-        if (
-            self.miss_prob + self.miss_std
-            > self.miss_prob_min + self.out_control_level * self.miss_sd_min
-        ):
-            self._in_concept_change = True
-
-        elif (
-            self.miss_prob + self.miss_std
-            > self.miss_prob_min + self.warning_level * self.miss_sd_min
-        ):
-            self._in_warning_zone = True
-
-        else:
-            self._in_warning_zone = False
-
-        return self._in_concept_change, self._in_warning_zone
+import math
+import numbers
+
+from river.base import DriftDetector
+
+
+class DDM(DriftDetector):
+    r"""Drift Detection Method.
+
+    DDM (Drift Detection Method) is a concept change detection method
+    based on the PAC learning model premise, that the learner's error rate
+    will decrease as the number of analysed samples increase, as long as the
+    data distribution is stationary.
+
+    If the algorithm detects an increase in the error rate, that surpasses
+    a calculated threshold, either change is detected or the algorithm will
+    warn the user that change may occur in the near future, which is called
+    the warning zone.
+
+    The detection threshold is calculated in function of two statistics,
+    obtained when $(p_i + s_i)$ is minimum:
+
+    * $p_{min}$: The minimum recorded error rate.
+
+    * $s_{min}$: The minimum recorded standard deviation.
+
+    At instant $i$, the detection algorithm uses:
+
+    * $p_i$: The error rate at instant $i$.
+
+    * $s_i$: The standard deviation at instant $i$.
+
+    The conditions for entering the warning zone and detecting change are
+    as follows [see implementation note bellow]:
+
+    * if $p_i + s_i \geq p_{min} + 2 * s_{min}$ -> Warning zone
+
+    * if $p_i + s_i \geq p_{min} + 3 * s_{min}$ -> Change detected
+
+    **Input:** `value` must be a binary signal, where 1 indicates error.
+    For example, if a classifier's prediction $y'$ is right or wrong w.r.t the
+    true target label $y$:
+
+    - 0: Correct, $y=y'$
+
+    - 1: Error, $y \neq y'$
+
+    Parameters
+    ----------
+    min_num_instances
+        The minimum required number of analyzed samples so change can be detected. This is used to
+        avoid false detections during the early moments of the detector, when the weight of one
+        sample is important.
+    warning_level
+        Warning level.
+    out_control_level
+        Out-control level.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from river.drift import DDM
+    >>> np.random.seed(12345)
+
+    >>> ddm = DDM()
+
+    >>> # Simulate a data stream as a normal distribution of 1's and 0's
+    >>> data_stream = np.random.randint(2, size=2000)
+    >>> # Change the data distribution from index 999 to 1500, simulating an
+    >>> # increase in error rate (1 indicates error)
+    >>> data_stream[1000:1200] = [np.random.binomial(1, .8) for _ in range(200)]
+
+    >>> # Update drift detector and verify if change is detected
+    >>> for i, val in enumerate(data_stream):
+    ...     in_drift, in_warning = ddm.update(val)
+    ...     if in_drift:
+    ...         print(f"Change detected at index {i}, input value: {val}")
+    Change detected at index 1157, input value: 1
+
+    Notes
+    -----
+    In this implementation, the conditions to signal drift and warning are
+    $p_i + s_i > thershold$ instead of $p_i + s_i \geq thershold$. This is to
+    avoid a corner case when a classifier is consistently wrong (`value=1`)
+    that results in DDM indicating a drift every `min_num_instances`. This
+    modification is consistent with the implementation in MOA.
+
+    References
+    ----------
+    [^1]: João Gama, Pedro Medas, Gladys Castillo, Pedro Pereira Rodrigues: Learning with Drift Detection. SBIA 2004: 286-295
+
+    """
+
+    def __init__(self, min_num_instances=30, warning_level=2.0, out_control_level=3.0):
+        super().__init__()
+        self.sample_count = None
+        self.miss_prob = None
+        self.miss_std = None
+        self.miss_prob_sd_min = None
+        self.miss_prob_min = None
+        self.miss_sd_min = None
+        self.min_num_instances = min_num_instances
+        self.warning_level = warning_level
+        self.out_control_level = out_control_level
+        self.estimation = None
+        self.reset()
+
+    def reset(self):
+        """Reset the change detector."""
+        super().reset()
+        self.sample_count = 1
+        self.miss_prob = 1.0
+        self.miss_std = 0.0
+        self.miss_prob_sd_min = float("inf")
+        self.miss_prob_min = float("inf")
+        self.miss_sd_min = float("inf")
+
+    def update(self, value: numbers.Number):
+        """Update the change detector with a single data point.
+
+        Parameters
+        ----------
+        value
+            This parameter indicates whether the last sample analyzed was correctly classified or
+            not. 1 indicates an error (miss-classification).
+
+        """
+        if self._in_concept_change:
+            self.reset()
+
+        self.miss_prob = self.miss_prob + (value - self.miss_prob) / float(
+            self.sample_count
+        )
+        self.miss_std = math.sqrt(
+            self.miss_prob * (1 - self.miss_prob) / float(self.sample_count)
+        )
+        self.sample_count += 1
+
+        self.estimation = self.miss_prob
+        self._in_concept_change = False
+        self._in_warning_zone = False
+
+        if self.sample_count <= self.min_num_instances:
+            return self._in_concept_change, self._in_warning_zone
+
+        if self.miss_prob + self.miss_std <= self.miss_prob_sd_min:
+            self.miss_prob_min = self.miss_prob
+            self.miss_sd_min = self.miss_std
+            self.miss_prob_sd_min = self.miss_prob + self.miss_std
+
+        if (
+            self.miss_prob + self.miss_std
+            > self.miss_prob_min + self.out_control_level * self.miss_sd_min
+        ):
+            self._in_concept_change = True
+
+        elif (
+            self.miss_prob + self.miss_std
+            > self.miss_prob_min + self.warning_level * self.miss_sd_min
+        ):
+            self._in_warning_zone = True
+
+        else:
+            self._in_warning_zone = False
+
+        return self._in_concept_change, self._in_warning_zone
```

### Comparing `river-0.8.0/river/drift/eddm.py` & `river-0.9.0/river/drift/eddm.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,171 +1,169 @@
-import numpy as np
-
-from river.base import DriftDetector
-
-
-class EDDM(DriftDetector):
-    r"""Early Drift Detection Method.
-
-    EDDM (Early Drift Detection Method) aims to improve the
-    detection rate of gradual concept drift in DDM, while keeping
-    a good performance against abrupt concept drift.
-
-    This method works by keeping track of the average distance
-    between two errors instead of only the error rate. For this,
-    it is necessary to keep track of the running average distance
-    and the running standard deviation, as well as the maximum
-    distance and the maximum standard deviation.
-
-    The algorithm works similarly to the DDM algorithm, by keeping
-    track of statistics only. It works with the running average
-    distance ($p_i'$) and the running standard deviation ($s_i'$), as
-    well as $p'_{max}$ and $s'_{max}$, which are the values of $p_i'$
-    and $s_i'$ when $(p_i' + 2 * s_i')$ reaches its maximum.
-
-    Like DDM, there are two threshold values that define the
-    borderline between no change, warning zone, and drift detected.
-    These are as follows:
-
-    * if $(p_i' + 2 * s_i')/(p'_{max} + 2 * s'_{max}) < \alpha$ -> Warning zone
-
-    * if $(p_i' + 2 * s_i')/(p'_{max} + 2 * s'_{max}) < \beta$ -> Change detected
-
-    $\alpha$ and $\beta$ are set to 0.95 and 0.9, respectively.
-
-    **Input:** `value` must be a binary signal, where 0 indicates error.
-    For example, if a classifier's prediction $y'$ is right or wrong w.r.t the
-    true target label $y$:
-
-    - 0: Correct, $y=y'$
-
-    - 1: Error, $y \neq y'$
-
-
-    Examples
-    --------
-    >>> import numpy as np
-    >>> from river.drift import EDDM
-    >>> np.random.seed(12345)
-
-    >>> eddm = EDDM()
-
-    >>> # Simulate a data stream as a normal distribution of 1's and 0's
-    >>> data_stream = np.random.randint(2, size=2000)
-    >>> # Change the data distribution from index 999 to 1500, simulating an
-    >>> # increase in error rate (1 indicates error)
-    >>> data_stream[999:1500] = 1
-
-    >>> # Update drift detector and verify if change is detected
-    >>> for i, val in enumerate(data_stream):
-    ...     in_drift, in_warning = eddm.update(val)
-    ...     if in_drift:
-    ...         print(f"Change detected at index {i}, input value: {val}")
-    Change detected at index 53, input value: 1
-    Change detected at index 121, input value: 1
-    Change detected at index 185, input value: 1
-    Change detected at index 272, input value: 1
-    Change detected at index 336, input value: 1
-    Change detected at index 391, input value: 1
-    Change detected at index 571, input value: 1
-    Change detected at index 627, input value: 1
-    Change detected at index 686, input value: 1
-    Change detected at index 754, input value: 1
-    Change detected at index 1033, input value: 1
-
-    References
-    ----------
-    [^1]: Early Drift Detection Method. Manuel Baena-Garcia, Jose Del Campo-Avila, Raúl Fidalgo, Albert Bifet, Ricard Gavalda, Rafael Morales-Bueno. In Fourth International Workshop on Knowledge Discovery from Data Streams, 2006.
-
-    """
-    FDDM_OUTCONTROL = 0.9
-    FDDM_WARNING = 0.95
-    FDDM_MIN_NUM_INSTANCES = 30
-
-    def __init__(self):
-        super().__init__()
-        self.m_num_errors = None
-        self.m_min_num_errors = 30
-        self.m_n = None
-        self.m_d = None
-        self.m_lastd = None
-        self.m_mean = None
-        self.m_std_temp = None
-        self.m_m2s_max = None
-        self.m_last_level = None
-        self.estimation = None
-        self.delay = None
-        self.reset()
-
-    def reset(self):
-        """Reset the change detector."""
-        super().reset()
-        self.m_n = 1
-        self.m_num_errors = 0
-        self.m_d = 0
-        self.m_lastd = 0
-        self.m_mean = 0.0
-        self.m_std_temp = 0.0
-        self.m_m2s_max = 0.0
-        self.estimation = 0.0
-
-    def update(self, value) -> tuple:
-        """Update the change detector with a single data point.
-
-        Parameters
-        ----------
-        value
-            This parameter indicates whether the last sample analyzed was correctly classified or
-            not. 1 indicates an error (miss-classification).
-
-        Returns
-        -------
-        A tuple (drift, warning) where its elements indicate if a drift or a warning is detected.
-
-        """
-
-        if self._in_concept_change:
-            self.reset()
-
-        self._in_concept_change = False
-
-        self.m_n += 1
-
-        if value == 1.0:
-            self._in_warning_zone = False
-            self.delay = 0
-            self.m_num_errors += 1
-            self.m_lastd = self.m_d
-            self.m_d = self.m_n - 1
-            distance = self.m_d - self.m_lastd
-            old_mean = self.m_mean
-            self.m_mean = (
-                self.m_mean + (float(distance) - self.m_mean) / self.m_num_errors
-            )
-            self.estimation = self.m_mean
-            self.m_std_temp = self.m_std_temp + (distance - self.m_mean) * (
-                distance - old_mean
-            )
-            std = np.sqrt(self.m_std_temp / self.m_num_errors)
-            m2s = self.m_mean + 2 * std
-
-            if self.m_n < self.FDDM_MIN_NUM_INSTANCES:
-                return self._in_concept_change, self._in_warning_zone
-
-            if m2s > self.m_m2s_max:
-                self.m_m2s_max = m2s
-            else:
-                p = m2s / self.m_m2s_max
-                if (self.m_num_errors > self.m_min_num_errors) and (
-                    p < self.FDDM_OUTCONTROL
-                ):
-                    self._in_concept_change = True
-
-                elif (self.m_num_errors > self.m_min_num_errors) and (
-                    p < self.FDDM_WARNING
-                ):
-                    self._in_warning_zone = True
-
-                else:
-                    self._in_warning_zone = False
-
-        return self._in_concept_change, self._in_warning_zone
+import numpy as np
+
+from river.base import DriftDetector
+
+
+class EDDM(DriftDetector):
+    r"""Early Drift Detection Method.
+
+    EDDM (Early Drift Detection Method) aims to improve the
+    detection rate of gradual concept drift in DDM, while keeping
+    a good performance against abrupt concept drift.
+
+    This method works by keeping track of the average distance
+    between two errors instead of only the error rate. For this,
+    it is necessary to keep track of the running average distance
+    and the running standard deviation, as well as the maximum
+    distance and the maximum standard deviation.
+
+    The algorithm works similarly to the DDM algorithm, by keeping
+    track of statistics only. It works with the running average
+    distance ($p_i'$) and the running standard deviation ($s_i'$), as
+    well as $p'_{max}$ and $s'_{max}$, which are the values of $p_i'$
+    and $s_i'$ when $(p_i' + 2 * s_i')$ reaches its maximum.
+
+    Like DDM, there are two threshold values that define the
+    borderline between no change, warning zone, and drift detected.
+    These are as follows:
+
+    * if $(p_i' + 2 * s_i')/(p'_{max} + 2 * s'_{max}) < \alpha$ -> Warning zone
+
+    * if $(p_i' + 2 * s_i')/(p'_{max} + 2 * s'_{max}) < \beta$ -> Change detected
+
+    $\alpha$ and $\beta$ are set to 0.95 and 0.9, respectively.
+
+    **Input:** `value` must be a binary signal, where 1 indicates error.
+    For example, if a classifier's prediction $y'$ is right or wrong w.r.t the
+    true target label $y$:
+
+    - 0: Correct, $y=y'$
+
+    - 1: Error, $y \neq y'$
+
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from river.drift import EDDM
+    >>> np.random.seed(12345)
+
+    >>> eddm = EDDM()
+
+    >>> # Simulate a data stream as a normal distribution of 1's and 0's
+    >>> data_stream = np.random.randint(2, size=2000)
+    >>> # Change the data distribution from index 999 to 1500, simulating an
+    >>> # increase in error rate (1 indicates error)
+    >>> data_stream[999:1500] = 1
+
+    >>> # Update drift detector and verify if change is detected
+    >>> for i, val in enumerate(data_stream):
+    ...     in_drift, in_warning = eddm.update(val)
+    ...     if in_drift:
+    ...         print(f"Change detected at index {i}, input value: {val}")
+    Change detected at index 53, input value: 1
+    Change detected at index 121, input value: 1
+    Change detected at index 185, input value: 1
+    Change detected at index 272, input value: 1
+    Change detected at index 336, input value: 1
+    Change detected at index 391, input value: 1
+    Change detected at index 571, input value: 1
+    Change detected at index 627, input value: 1
+    Change detected at index 686, input value: 1
+    Change detected at index 754, input value: 1
+    Change detected at index 1033, input value: 1
+
+    References
+    ----------
+    [^1]: Early Drift Detection Method. Manuel Baena-Garcia, Jose Del Campo-Avila, Raúl Fidalgo, Albert Bifet, Ricard Gavalda, Rafael Morales-Bueno. In Fourth International Workshop on Knowledge Discovery from Data Streams, 2006.
+
+    """
+    FDDM_OUTCONTROL = 0.9
+    FDDM_WARNING = 0.95
+    FDDM_MIN_NUM_INSTANCES = 30
+
+    def __init__(self):
+        super().__init__()
+        self.m_num_errors = None
+        self.m_min_num_errors = 30
+        self.m_n = None
+        self.m_d = None
+        self.m_lastd = None
+        self.m_mean = None
+        self.m_std_temp = None
+        self.m_m2s_max = None
+        self.m_last_level = None
+        self.estimation = None
+        self.delay = None
+        self.reset()
+
+    def reset(self):
+        """Reset the change detector."""
+        super().reset()
+        self.m_n = 1
+        self.m_num_errors = 0
+        self.m_d = 0
+        self.m_lastd = 0
+        self.m_mean = 0.0
+        self.m_std_temp = 0.0
+        self.m_m2s_max = 0.0
+        self.estimation = 0.0
+
+    def update(self, value) -> tuple:
+        """Update the change detector with a single data point.
+
+        Parameters
+        ----------
+        value
+            This parameter indicates whether the last sample analyzed was correctly classified or
+            not. 1 indicates an error (miss-classification).
+
+        Returns
+        -------
+        A tuple (drift, warning) where its elements indicate if a drift or a warning is detected.
+
+        """
+
+        if self._in_concept_change:
+            self.reset()
+
+        self._in_concept_change = False
+
+        self.m_n += 1
+
+        if value == 1.0:
+            self._in_warning_zone = False
+            self.delay = 0
+            self.m_num_errors += 1
+            self.m_lastd = self.m_d
+            self.m_d = self.m_n - 1
+            distance = self.m_d - self.m_lastd
+            old_mean = self.m_mean
+            self.m_mean = (
+                self.m_mean + (float(distance) - self.m_mean) / self.m_num_errors
+            )
+            self.estimation = self.m_mean
+            self.m_std_temp += (distance - self.m_mean) * (distance - old_mean)
+            std = np.sqrt(self.m_std_temp / self.m_num_errors)
+            m2s = self.m_mean + 2 * std
+
+            if self.m_n < self.FDDM_MIN_NUM_INSTANCES:
+                return self._in_concept_change, self._in_warning_zone
+
+            if m2s > self.m_m2s_max:
+                self.m_m2s_max = m2s
+            else:
+                p = m2s / self.m_m2s_max
+                if (self.m_num_errors > self.m_min_num_errors) and (
+                    p < self.FDDM_OUTCONTROL
+                ):
+                    self._in_concept_change = True
+
+                elif (self.m_num_errors > self.m_min_num_errors) and (
+                    p < self.FDDM_WARNING
+                ):
+                    self._in_warning_zone = True
+
+                else:
+                    self._in_warning_zone = False
+
+        return self._in_concept_change, self._in_warning_zone
```

### Comparing `river-0.8.0/river/drift/hddm_a.py` & `river-0.9.0/river/drift/hddm_a.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,197 +1,197 @@
-from math import log, sqrt
-
-from river.base import DriftDetector
-
-
-class HDDM_A(DriftDetector):
-    r"""Drift Detection Method based on Hoeffding’s bounds with moving average-test.
-
-    HDDM_A is a drift detection method based on the Hoeffding’s inequality.
-    HDDM_A uses the average as estimator. It receives as input a stream of real
-    values and returns the estimated status of the stream: STABLE, WARNING or
-    DRIFT.
-
-    **Input:** `value` must be a binary signal, where 0 indicates error.
-    For example, if a classifier's prediction $y'$ is right or wrong w.r.t the
-    true target label $y$:
-
-    - 0: Correct, $y=y'$
-
-    - 1: Error, $y \neq y'$
-
-    *Implementation based on MOA.*
-
-    Parameters
-    ----------
-    drift_confidence
-        Confidence to the drift
-    warning_confidence
-        Confidence to the warning
-    two_sided_test
-        If `True`, will monitor error increments and decrements (two-sided). By default will only
-        monitor increments (one-sided).
-
-    Examples
-    --------
-    >>> import numpy as np
-    >>> from river.drift import HDDM_A
-    >>> np.random.seed(12345)
-
-    >>> hddm_a = HDDM_A()
-
-    >>> # Simulate a data stream as a normal distribution of 1's and 0's
-    >>> data_stream = np.random.randint(2, size=2000)
-    >>> # Change the data distribution from index 999 to 1500, simulating an
-    >>> # increase in error rate (1 indicates error)
-    >>> data_stream[999:1500] = 1
-
-    >>> # Update drift detector and verify if change is detected
-    >>> for i, val in enumerate(data_stream):
-    ...     in_drift, in_warning = hddm_a.update(val)
-    ...     if in_drift:
-    ...         print(f"Change detected at index {i}, input value: {val}")
-    Change detected at index 1013, input value: 1
-
-    References
-    ----------
-    [^1]: Frías-Blanco I, del Campo-Ávila J, Ramos-Jimenez G, et al. Online and non-parametric drift detection methods based on Hoeffding’s bounds. IEEE Transactions on Knowledge and Data Engineering, 2014, 27(3): 810-823.
-    [^2]: Albert Bifet, Geoff Holmes, Richard Kirkby, Bernhard Pfahringer. MOA: Massive Online Analysis; Journal of Machine Learning Research 11: 1601-1604, 2010.
-
-    """
-
-    def __init__(
-        self, drift_confidence=0.001, warning_confidence=0.005, two_sided_test=False
-    ):
-        super().__init__()
-        super().reset()
-        self.n_min = 0
-        self.c_min = 0
-        self.total_n = 0
-        self.total_c = 0
-        self.n_max = 0
-        self.c_max = 0
-        self.n_estimation = 0
-        self.c_estimation = 0
-        self.estimation = None
-        self.delay = None
-
-        self.drift_confidence = drift_confidence
-        self.warning_confidence = warning_confidence
-        self.two_sided_test = two_sided_test
-
-    def update(self, value) -> tuple:
-        """Update the change detector with a single data point.
-
-        Parameters
-        ----------
-        value
-            This parameter indicates whether the last sample analyzed was correctly classified or
-            not. 1 indicates an error (miss-classification).
-
-        Returns
-        -------
-        A tuple (drift, warning) where its elements indicate if a drift or a warning is detected.
-
-        """
-        self.total_n += 1
-        self.total_c += value
-        if self.n_min == 0:
-            self.n_min = self.total_n
-            self.c_min = self.total_c
-        if self.n_max == 0:
-            self.n_max = self.total_n
-            self.c_max = self.total_c
-
-        cota = sqrt(1.0 / (2 * self.n_min) * log(1.0 / self.drift_confidence))
-        cota1 = sqrt(1.0 / (2 * self.total_n) * log(1.0 / self.drift_confidence))
-
-        if self.c_min / self.n_min + cota >= self.total_c / self.total_n + cota1:
-            self.c_min = self.total_c
-            self.n_min = self.total_n
-
-        cota = sqrt(1.0 / (2 * self.n_max) * log(1.0 / self.drift_confidence))
-        if self.c_max / self.n_max - cota <= self.total_c / self.total_n - cota1:
-            self.c_max = self.total_c
-            self.n_max = self.total_n
-
-        if self._mean_incr(
-            self.c_min, self.n_min, self.total_c, self.total_n, self.drift_confidence
-        ):
-            self.n_estimation = self.total_n - self.n_min
-            self.c_estimation = self.total_c - self.c_min
-            self.n_min = self.n_max = self.total_n = 0
-            self.c_min = self.c_max = self.total_c = 0
-            self._in_concept_change = True
-            self._in_warning_zone = False
-        elif self._mean_incr(
-            self.c_min, self.n_min, self.total_c, self.total_n, self.warning_confidence
-        ):
-            self._in_concept_change = False
-            self._in_warning_zone = True
-        else:
-            self._in_concept_change = False
-            self._in_warning_zone = False
-
-        if self.two_sided_test:
-            if self._mean_decr(
-                self.c_max,
-                self.n_max,
-                self.total_c,
-                self.total_n,
-                self.drift_confidence,
-            ):
-                self.n_estimation = self.total_n - self.n_max
-                self.c_estimation = self.total_c - self.c_max
-                self.n_min = self.n_max = self.total_n = 0
-                self.c_min = self.c_max = self.total_c = 0
-                self._in_concept_change = True
-            elif self._mean_decr(
-                self.c_max,
-                self.n_max,
-                self.total_c,
-                self.total_n,
-                self.warning_confidence,
-            ):
-                self._in_warning_zone = True
-
-        self._update_estimations()
-
-        return self._in_concept_change, self._in_warning_zone
-
-    @staticmethod
-    def _mean_incr(c_min, n_min, total_c, total_n, confidence):
-        if n_min == total_n:
-            return False
-
-        m = (total_n - n_min) / n_min * (1.0 / total_n)
-        cota = sqrt(m / 2 * log(2.0 / confidence))
-        return total_c / total_n - c_min / n_min >= cota
-
-    def _mean_decr(self, c_max, n_max, total_c, total_n, confidence):
-        if n_max == total_n:
-            return False
-
-        m = (total_n - n_max) / n_max * (1.0 / total_n)
-        cota = sqrt(m / 2 * log(2.0 / confidence))
-        return c_max / n_max - total_c / total_n >= cota
-
-    def reset(self):
-        """Reset the change detector."""
-        super().reset()
-        self.n_min = 0
-        self.c_min = 0
-        self.total_n = 0
-        self.total_c = 0
-        self.n_max = 0
-        self.c_max = 0
-        self.c_estimation = 0
-        self.n_estimation = 0
-
-    def _update_estimations(self):
-        if self.total_n >= self.n_estimation:
-            self.c_estimation = self.n_estimation = 0
-            self.estimation = self.total_c / self.total_n
-            self.delay = self.total_n
-        else:
-            self.estimation = self.c_estimation / self.n_estimation
-            self.delay = self.n_estimation
+from math import log, sqrt
+
+from river.base import DriftDetector
+
+
+class HDDM_A(DriftDetector):
+    r"""Drift Detection Method based on Hoeffding’s bounds with moving average-test.
+
+    HDDM_A is a drift detection method based on the Hoeffding’s inequality.
+    HDDM_A uses the average as estimator. It receives as input a stream of real
+    values and returns the estimated status of the stream: STABLE, WARNING or
+    DRIFT.
+
+    **Input:** `value` must be a binary signal, where 1 indicates error.
+    For example, if a classifier's prediction $y'$ is right or wrong w.r.t the
+    true target label $y$:
+
+    - 0: Correct, $y=y'$
+
+    - 1: Error, $y \neq y'$
+
+    *Implementation based on MOA.*
+
+    Parameters
+    ----------
+    drift_confidence
+        Confidence to the drift
+    warning_confidence
+        Confidence to the warning
+    two_sided_test
+        If `True`, will monitor error increments and decrements (two-sided). By default will only
+        monitor increments (one-sided).
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from river.drift import HDDM_A
+    >>> np.random.seed(12345)
+
+    >>> hddm_a = HDDM_A()
+
+    >>> # Simulate a data stream as a normal distribution of 1's and 0's
+    >>> data_stream = np.random.randint(2, size=2000)
+    >>> # Change the data distribution from index 999 to 1500, simulating an
+    >>> # increase in error rate (1 indicates error)
+    >>> data_stream[999:1500] = 1
+
+    >>> # Update drift detector and verify if change is detected
+    >>> for i, val in enumerate(data_stream):
+    ...     in_drift, in_warning = hddm_a.update(val)
+    ...     if in_drift:
+    ...         print(f"Change detected at index {i}, input value: {val}")
+    Change detected at index 1013, input value: 1
+
+    References
+    ----------
+    [^1]: Frías-Blanco I, del Campo-Ávila J, Ramos-Jimenez G, et al. Online and non-parametric drift detection methods based on Hoeffding’s bounds. IEEE Transactions on Knowledge and Data Engineering, 2014, 27(3): 810-823.
+    [^2]: Albert Bifet, Geoff Holmes, Richard Kirkby, Bernhard Pfahringer. MOA: Massive Online Analysis; Journal of Machine Learning Research 11: 1601-1604, 2010.
+
+    """
+
+    def __init__(
+        self, drift_confidence=0.001, warning_confidence=0.005, two_sided_test=False
+    ):
+        super().__init__()
+        super().reset()
+        self.n_min = 0
+        self.c_min = 0
+        self.total_n = 0
+        self.total_c = 0
+        self.n_max = 0
+        self.c_max = 0
+        self.n_estimation = 0
+        self.c_estimation = 0
+        self.estimation = None
+        self.delay = None
+
+        self.drift_confidence = drift_confidence
+        self.warning_confidence = warning_confidence
+        self.two_sided_test = two_sided_test
+
+    def update(self, value) -> tuple:
+        """Update the change detector with a single data point.
+
+        Parameters
+        ----------
+        value
+            This parameter indicates whether the last sample analyzed was correctly classified or
+            not. 1 indicates an error (miss-classification).
+
+        Returns
+        -------
+        A tuple (drift, warning) where its elements indicate if a drift or a warning is detected.
+
+        """
+        self.total_n += 1
+        self.total_c += value
+        if self.n_min == 0:
+            self.n_min = self.total_n
+            self.c_min = self.total_c
+        if self.n_max == 0:
+            self.n_max = self.total_n
+            self.c_max = self.total_c
+
+        cota = sqrt(1.0 / (2 * self.n_min) * log(1.0 / self.drift_confidence))
+        cota1 = sqrt(1.0 / (2 * self.total_n) * log(1.0 / self.drift_confidence))
+
+        if self.c_min / self.n_min + cota >= self.total_c / self.total_n + cota1:
+            self.c_min = self.total_c
+            self.n_min = self.total_n
+
+        cota = sqrt(1.0 / (2 * self.n_max) * log(1.0 / self.drift_confidence))
+        if self.c_max / self.n_max - cota <= self.total_c / self.total_n - cota1:
+            self.c_max = self.total_c
+            self.n_max = self.total_n
+
+        if self._mean_incr(
+            self.c_min, self.n_min, self.total_c, self.total_n, self.drift_confidence
+        ):
+            self.n_estimation = self.total_n - self.n_min
+            self.c_estimation = self.total_c - self.c_min
+            self.n_min = self.n_max = self.total_n = 0
+            self.c_min = self.c_max = self.total_c = 0
+            self._in_concept_change = True
+            self._in_warning_zone = False
+        elif self._mean_incr(
+            self.c_min, self.n_min, self.total_c, self.total_n, self.warning_confidence
+        ):
+            self._in_concept_change = False
+            self._in_warning_zone = True
+        else:
+            self._in_concept_change = False
+            self._in_warning_zone = False
+
+        if self.two_sided_test:
+            if self._mean_decr(
+                self.c_max,
+                self.n_max,
+                self.total_c,
+                self.total_n,
+                self.drift_confidence,
+            ):
+                self.n_estimation = self.total_n - self.n_max
+                self.c_estimation = self.total_c - self.c_max
+                self.n_min = self.n_max = self.total_n = 0
+                self.c_min = self.c_max = self.total_c = 0
+                self._in_concept_change = True
+            elif self._mean_decr(
+                self.c_max,
+                self.n_max,
+                self.total_c,
+                self.total_n,
+                self.warning_confidence,
+            ):
+                self._in_warning_zone = True
+
+        self._update_estimations()
+
+        return self._in_concept_change, self._in_warning_zone
+
+    @staticmethod
+    def _mean_incr(c_min, n_min, total_c, total_n, confidence):
+        if n_min == total_n:
+            return False
+
+        m = (total_n - n_min) / n_min * (1.0 / total_n)
+        cota = sqrt(m / 2 * log(2.0 / confidence))
+        return total_c / total_n - c_min / n_min >= cota
+
+    def _mean_decr(self, c_max, n_max, total_c, total_n, confidence):
+        if n_max == total_n:
+            return False
+
+        m = (total_n - n_max) / n_max * (1.0 / total_n)
+        cota = sqrt(m / 2 * log(2.0 / confidence))
+        return c_max / n_max - total_c / total_n >= cota
+
+    def reset(self):
+        """Reset the change detector."""
+        super().reset()
+        self.n_min = 0
+        self.c_min = 0
+        self.total_n = 0
+        self.total_c = 0
+        self.n_max = 0
+        self.c_max = 0
+        self.c_estimation = 0
+        self.n_estimation = 0
+
+    def _update_estimations(self):
+        if self.total_n >= self.n_estimation:
+            self.c_estimation = self.n_estimation = 0
+            self.estimation = self.total_c / self.total_n
+            self.delay = self.total_n
+        else:
+            self.estimation = self.c_estimation / self.n_estimation
+            self.delay = self.n_estimation
```

### Comparing `river-0.8.0/river/drift/hddm_w.py` & `river-0.9.0/river/drift/hddm_w.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,241 +1,241 @@
-from math import log, sqrt
-
-from river.base import DriftDetector
-
-
-class HDDM_W(DriftDetector):
-    r"""Drift Detection Method based on Hoeffding’s bounds with moving weighted average-test.
-
-    HDDM_W is an online drift detection method based on McDiarmid's bounds.
-    HDDM_W uses the Exponentially Weighted Moving Average (EWMA) statistic as
-    estimator. It receives as input a stream of real predictions and returns
-    the estimated status of the stream: STABLE, WARNING or DRIFT.
-
-    **Input:** `value` must be a binary signal, where 0 indicates error.
-    For example, if a classifier's prediction $y'$ is right or wrong w.r.t the
-    true target label $y$:
-
-    - 0: Correct, $y=y'$
-
-    - 1: Error, $y \neq y'$
-
-    *Implementation based on MOA.*
-
-    Parameters
-    ----------
-    drift_confidence
-        Confidence to the drift
-    warning_confidence
-        Confidence to the warning
-    lambda_option
-        The weight given to recent data. Smaller values mean less weight given to recent data.
-    two_sided_test
-        If True, will monitor error increments and decrements (two-sided). By default will only
-        monitor increments (one-sided).
-
-    Examples
-    --------
-    >>> import numpy as np
-    >>> from river.drift import HDDM_W
-    >>> np.random.seed(12345)
-
-    >>> hddm_w = HDDM_W()
-
-    >>> # Simulate a data stream as a normal distribution of 1's and 0's
-    >>> data_stream = np.random.randint(2, size=2000)
-    >>> # Change the data distribution from index 999 to 1500, simulating an
-    >>> # increase in error rate (1 indicates error)
-    >>> data_stream[999:1500] = 1
-
-    >>> # Update drift detector and verify if change is detected
-    >>> for i, val in enumerate(data_stream):
-    ...     in_drift, in_warning = hddm_w.update(val)
-    ...     if in_drift:
-    ...         print(f"Change detected at index {i}, input value: {val}")
-    Change detected at index 1011, input value: 1
-
-    References
-    ----------
-    [^1]: Frías-Blanco I, del Campo-Ávila J, Ramos-Jimenez G, et al. Online and non-parametric drift detection methods based on Hoeffding’s bounds. IEEE Transactions on Knowledge and Data Engineering, 2014, 27(3): 810-823.
-    [^2]: Albert Bifet, Geoff Holmes, Richard Kirkby, Bernhard Pfahringer. MOA: Massive Online Analysis; Journal of Machine Learning Research 11: 1601-1604, 2010.
-
-    """
-
-    class SampleInfo:
-        def __init__(self):
-            self.EWMA_estimator = -1.0
-            self.independent_bounded_condition_sum = 0.0
-
-    def __init__(
-        self,
-        drift_confidence=0.001,
-        warning_confidence=0.005,
-        lambda_option=0.050,
-        two_sided_test=False,
-    ):
-        super().__init__()
-        super().reset()
-        self.total = self.SampleInfo()
-        self.sample1_decr_monitor = self.SampleInfo()
-        self.sample1_incr_monitor = self.SampleInfo()
-        self.sample2_decr_monitor = self.SampleInfo()
-        self.sample2_incr_monitor = self.SampleInfo()
-        self.incr_cutpoint = float("inf")
-        self.decr_cutpoint = float("inf")
-        self.width = 0
-        self.delay = 0
-        self.drift_confidence = drift_confidence
-        self.warning_confidence = warning_confidence
-        self.lambda_option = lambda_option
-        self.two_sided_test = two_sided_test
-        self.estimation = None
-
-    def update(self, value):
-        """Update the change detector with a single data point.
-
-        Parameters
-        ----------
-        value: Input value (0 or 1)
-            This parameter indicates whether the last sample analyzed was
-            correctly classified or not. 1 indicates an error (miss-classification).
-
-        Returns
-        -------
-        tuple
-            A tuple (drift, warning) where its elements indicate if a drift or a warning is
-            detected.
-
-        """
-        aux_decay_rate = 1.0 - self.lambda_option
-        self.width += 1
-        if self.total.EWMA_estimator < 0:
-            self.total.EWMA_estimator = value
-            self.total.independent_bounded_condition_sum = 1
-        else:
-            self.total.EWMA_estimator = (
-                self.lambda_option * value + aux_decay_rate * self.total.EWMA_estimator
-            )
-            self.total.independent_bounded_condition_sum = (
-                self.lambda_option * self.lambda_option
-                + aux_decay_rate
-                * aux_decay_rate
-                * self.total.independent_bounded_condition_sum
-            )
-
-        self._update_incr_statistics(value, self.drift_confidence)
-        if self._monitor_mean_incr(self.drift_confidence):
-            self.reset()
-            self._in_concept_change = True
-            self._in_warning_zone = False
-        elif self._monitor_mean_incr(self.warning_confidence):
-            self._in_concept_change = False
-            self._in_warning_zone = True
-        else:
-            self._in_concept_change = False
-            self._in_warning_zone = False
-
-        self._update_decr_statistics(value, self.drift_confidence)
-        if self.two_sided_test:
-            if self._monitor_mean_decr(self.drift_confidence):
-                self.reset()
-                self._in_concept_change = True
-            elif self._monitor_mean_decr(self.warning_confidence):
-                self._in_warning_zone = True
-
-        self.estimation = self.total.EWMA_estimator
-
-        return self._in_concept_change, self._in_warning_zone
-
-    @staticmethod
-    def _detect_mean_increment(sample1, sample2, confidence):
-        if sample1.EWMA_estimator < 0 or sample2.EWMA_estimator < 0:
-            return False
-        ibc_sum = (
-            sample1.independent_bounded_condition_sum
-            + sample2.independent_bounded_condition_sum
-        )
-        bound = sqrt(ibc_sum * log(1 / confidence) / 2)
-        return sample2.EWMA_estimator - sample1.EWMA_estimator > bound
-
-    def _monitor_mean_incr(self, confidence):
-        return self._detect_mean_increment(
-            self.sample1_incr_monitor, self.sample2_incr_monitor, confidence
-        )
-
-    def _monitor_mean_decr(self, confidence):
-        return self._detect_mean_increment(
-            self.sample2_decr_monitor, self.sample1_decr_monitor, confidence
-        )
-
-    def _update_incr_statistics(self, value, confidence):
-        aux_decay = 1.0 - self.lambda_option
-        bound = sqrt(
-            self.total.independent_bounded_condition_sum * log(1.0 / confidence) / 2
-        )
-
-        if self.total.EWMA_estimator + bound < self.incr_cutpoint:
-            self.incr_cutpoint = self.total.EWMA_estimator + bound
-            self.sample1_incr_monitor.EWMA_estimator = self.total.EWMA_estimator
-            self.sample1_incr_monitor.independent_bounded_condition_sum = (
-                self.total.independent_bounded_condition_sum
-            )
-            self.sample2_incr_monitor = self.SampleInfo()
-            self.delay = 0
-        else:
-            self.delay += 1
-            if self.sample2_incr_monitor.EWMA_estimator < 0:
-                self.sample2_incr_monitor.EWMA_estimator = value
-                self.sample2_incr_monitor.independent_bounded_condition_sum = 1
-            else:
-                self.sample2_incr_monitor.EWMA_estimator = (
-                    self.lambda_option * value
-                    + aux_decay * self.sample2_incr_monitor.EWMA_estimator
-                )
-                self.sample2_incr_monitor.independent_bounded_condition_sum = (
-                    self.lambda_option * self.lambda_option
-                    + aux_decay
-                    * aux_decay
-                    * self.sample2_incr_monitor.independent_bounded_condition_sum
-                )
-
-    def _update_decr_statistics(self, value, confidence):
-        aux_decay = 1.0 - self.lambda_option
-        epsilon = sqrt(
-            self.total.independent_bounded_condition_sum * log(1.0 / confidence) / 2
-        )
-
-        if self.total.EWMA_estimator - epsilon > self.decr_cutpoint:
-            self.decr_cutpoint = self.total.EWMA_estimator - epsilon
-            self.sample1_decr_monitor.EWMA_estimator = self.total.EWMA_estimator
-            self.sample1_decr_monitor.independent_bounded_condition_sum = (
-                self.total.independent_bounded_condition_sum
-            )
-            self.sample2_decr_monitor = self.SampleInfo()
-        else:
-            if self.sample2_decr_monitor.EWMA_estimator < 0:
-                self.sample2_decr_monitor.EWMA_estimator = value
-                self.sample2_decr_monitor.independent_bounded_condition_sum = 1
-            else:
-                self.sample2_decr_monitor.EWMA_estimator = (
-                    self.lambda_option * value
-                    + aux_decay * self.sample2_decr_monitor.EWMA_estimator
-                )
-                self.sample2_decr_monitor.independent_bounded_condition_sum = (
-                    self.lambda_option * self.lambda_option
-                    + aux_decay
-                    * aux_decay
-                    * self.sample2_decr_monitor.independent_bounded_condition_sum
-                )
-
-    def reset(self):
-        """Reset the change detector."""
-        super().reset()
-        self.total = self.SampleInfo()
-        self.sample1_decr_monitor = self.SampleInfo()
-        self.sample1_incr_monitor = self.SampleInfo()
-        self.sample2_decr_monitor = self.SampleInfo()
-        self.sample2_incr_monitor = self.SampleInfo()
-        self.incr_cutpoint = float("inf")
-        self.decr_cutpoint = float("inf")
-        self.width = 0
-        self.delay = 0
+from math import log, sqrt
+
+from river.base import DriftDetector
+
+
+class HDDM_W(DriftDetector):
+    r"""Drift Detection Method based on Hoeffding’s bounds with moving weighted average-test.
+
+    HDDM_W is an online drift detection method based on McDiarmid's bounds.
+    HDDM_W uses the Exponentially Weighted Moving Average (EWMA) statistic as
+    estimator. It receives as input a stream of real predictions and returns
+    the estimated status of the stream: STABLE, WARNING or DRIFT.
+
+    **Input:** `value` must be a binary signal, where 1 indicates error.
+    For example, if a classifier's prediction $y'$ is right or wrong w.r.t the
+    true target label $y$:
+
+    - 0: Correct, $y=y'$
+
+    - 1: Error, $y \neq y'$
+
+    *Implementation based on MOA.*
+
+    Parameters
+    ----------
+    drift_confidence
+        Confidence to the drift
+    warning_confidence
+        Confidence to the warning
+    lambda_option
+        The weight given to recent data. Smaller values mean less weight given to recent data.
+    two_sided_test
+        If True, will monitor error increments and decrements (two-sided). By default will only
+        monitor increments (one-sided).
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from river.drift import HDDM_W
+    >>> np.random.seed(12345)
+
+    >>> hddm_w = HDDM_W()
+
+    >>> # Simulate a data stream as a normal distribution of 1's and 0's
+    >>> data_stream = np.random.randint(2, size=2000)
+    >>> # Change the data distribution from index 999 to 1500, simulating an
+    >>> # increase in error rate (1 indicates error)
+    >>> data_stream[999:1500] = 1
+
+    >>> # Update drift detector and verify if change is detected
+    >>> for i, val in enumerate(data_stream):
+    ...     in_drift, in_warning = hddm_w.update(val)
+    ...     if in_drift:
+    ...         print(f"Change detected at index {i}, input value: {val}")
+    Change detected at index 1011, input value: 1
+
+    References
+    ----------
+    [^1]: Frías-Blanco I, del Campo-Ávila J, Ramos-Jimenez G, et al. Online and non-parametric drift detection methods based on Hoeffding’s bounds. IEEE Transactions on Knowledge and Data Engineering, 2014, 27(3): 810-823.
+    [^2]: Albert Bifet, Geoff Holmes, Richard Kirkby, Bernhard Pfahringer. MOA: Massive Online Analysis; Journal of Machine Learning Research 11: 1601-1604, 2010.
+
+    """
+
+    class SampleInfo:
+        def __init__(self):
+            self.EWMA_estimator = -1.0
+            self.independent_bounded_condition_sum = 0.0
+
+    def __init__(
+        self,
+        drift_confidence=0.001,
+        warning_confidence=0.005,
+        lambda_option=0.050,
+        two_sided_test=False,
+    ):
+        super().__init__()
+        super().reset()
+        self.total = self.SampleInfo()
+        self.sample1_decr_monitor = self.SampleInfo()
+        self.sample1_incr_monitor = self.SampleInfo()
+        self.sample2_decr_monitor = self.SampleInfo()
+        self.sample2_incr_monitor = self.SampleInfo()
+        self.incr_cutpoint = float("inf")
+        self.decr_cutpoint = float("inf")
+        self.width = 0
+        self.delay = 0
+        self.drift_confidence = drift_confidence
+        self.warning_confidence = warning_confidence
+        self.lambda_option = lambda_option
+        self.two_sided_test = two_sided_test
+        self.estimation = None
+
+    def update(self, value):
+        """Update the change detector with a single data point.
+
+        Parameters
+        ----------
+        value: Input value (0 or 1)
+            This parameter indicates whether the last sample analyzed was
+            correctly classified or not. 1 indicates an error (miss-classification).
+
+        Returns
+        -------
+        tuple
+            A tuple (drift, warning) where its elements indicate if a drift or a warning is
+            detected.
+
+        """
+        aux_decay_rate = 1.0 - self.lambda_option
+        self.width += 1
+        if self.total.EWMA_estimator < 0:
+            self.total.EWMA_estimator = value
+            self.total.independent_bounded_condition_sum = 1
+        else:
+            self.total.EWMA_estimator = (
+                self.lambda_option * value + aux_decay_rate * self.total.EWMA_estimator
+            )
+            self.total.independent_bounded_condition_sum = (
+                self.lambda_option * self.lambda_option
+                + aux_decay_rate
+                * aux_decay_rate
+                * self.total.independent_bounded_condition_sum
+            )
+
+        self._update_incr_statistics(value, self.drift_confidence)
+        if self._monitor_mean_incr(self.drift_confidence):
+            self.reset()
+            self._in_concept_change = True
+            self._in_warning_zone = False
+        elif self._monitor_mean_incr(self.warning_confidence):
+            self._in_concept_change = False
+            self._in_warning_zone = True
+        else:
+            self._in_concept_change = False
+            self._in_warning_zone = False
+
+        self._update_decr_statistics(value, self.drift_confidence)
+        if self.two_sided_test:
+            if self._monitor_mean_decr(self.drift_confidence):
+                self.reset()
+                self._in_concept_change = True
+            elif self._monitor_mean_decr(self.warning_confidence):
+                self._in_warning_zone = True
+
+        self.estimation = self.total.EWMA_estimator
+
+        return self._in_concept_change, self._in_warning_zone
+
+    @staticmethod
+    def _detect_mean_increment(sample1, sample2, confidence):
+        if sample1.EWMA_estimator < 0 or sample2.EWMA_estimator < 0:
+            return False
+        ibc_sum = (
+            sample1.independent_bounded_condition_sum
+            + sample2.independent_bounded_condition_sum
+        )
+        bound = sqrt(ibc_sum * log(1 / confidence) / 2)
+        return sample2.EWMA_estimator - sample1.EWMA_estimator > bound
+
+    def _monitor_mean_incr(self, confidence):
+        return self._detect_mean_increment(
+            self.sample1_incr_monitor, self.sample2_incr_monitor, confidence
+        )
+
+    def _monitor_mean_decr(self, confidence):
+        return self._detect_mean_increment(
+            self.sample2_decr_monitor, self.sample1_decr_monitor, confidence
+        )
+
+    def _update_incr_statistics(self, value, confidence):
+        aux_decay = 1.0 - self.lambda_option
+        bound = sqrt(
+            self.total.independent_bounded_condition_sum * log(1.0 / confidence) / 2
+        )
+
+        if self.total.EWMA_estimator + bound < self.incr_cutpoint:
+            self.incr_cutpoint = self.total.EWMA_estimator + bound
+            self.sample1_incr_monitor.EWMA_estimator = self.total.EWMA_estimator
+            self.sample1_incr_monitor.independent_bounded_condition_sum = (
+                self.total.independent_bounded_condition_sum
+            )
+            self.sample2_incr_monitor = self.SampleInfo()
+            self.delay = 0
+        else:
+            self.delay += 1
+            if self.sample2_incr_monitor.EWMA_estimator < 0:
+                self.sample2_incr_monitor.EWMA_estimator = value
+                self.sample2_incr_monitor.independent_bounded_condition_sum = 1
+            else:
+                self.sample2_incr_monitor.EWMA_estimator = (
+                    self.lambda_option * value
+                    + aux_decay * self.sample2_incr_monitor.EWMA_estimator
+                )
+                self.sample2_incr_monitor.independent_bounded_condition_sum = (
+                    self.lambda_option * self.lambda_option
+                    + aux_decay
+                    * aux_decay
+                    * self.sample2_incr_monitor.independent_bounded_condition_sum
+                )
+
+    def _update_decr_statistics(self, value, confidence):
+        aux_decay = 1.0 - self.lambda_option
+        epsilon = sqrt(
+            self.total.independent_bounded_condition_sum * log(1.0 / confidence) / 2
+        )
+
+        if self.total.EWMA_estimator - epsilon > self.decr_cutpoint:
+            self.decr_cutpoint = self.total.EWMA_estimator - epsilon
+            self.sample1_decr_monitor.EWMA_estimator = self.total.EWMA_estimator
+            self.sample1_decr_monitor.independent_bounded_condition_sum = (
+                self.total.independent_bounded_condition_sum
+            )
+            self.sample2_decr_monitor = self.SampleInfo()
+        else:
+            if self.sample2_decr_monitor.EWMA_estimator < 0:
+                self.sample2_decr_monitor.EWMA_estimator = value
+                self.sample2_decr_monitor.independent_bounded_condition_sum = 1
+            else:
+                self.sample2_decr_monitor.EWMA_estimator = (
+                    self.lambda_option * value
+                    + aux_decay * self.sample2_decr_monitor.EWMA_estimator
+                )
+                self.sample2_decr_monitor.independent_bounded_condition_sum = (
+                    self.lambda_option * self.lambda_option
+                    + aux_decay
+                    * aux_decay
+                    * self.sample2_decr_monitor.independent_bounded_condition_sum
+                )
+
+    def reset(self):
+        """Reset the change detector."""
+        super().reset()
+        self.total = self.SampleInfo()
+        self.sample1_decr_monitor = self.SampleInfo()
+        self.sample1_incr_monitor = self.SampleInfo()
+        self.sample2_decr_monitor = self.SampleInfo()
+        self.sample2_incr_monitor = self.SampleInfo()
+        self.incr_cutpoint = float("inf")
+        self.decr_cutpoint = float("inf")
+        self.width = 0
+        self.delay = 0
```

### Comparing `river-0.8.0/river/drift/kswin.py` & `river-0.9.0/river/drift/kswin.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,137 +1,137 @@
-import numpy as np
-from scipy import stats
-
-from river.base import DriftDetector
-
-
-class KSWIN(DriftDetector):
-    r"""Kolmogorov-Smirnov Windowing method for concept drift detection.
-
-    Parameters
-    ----------
-    alpha
-        Probability for the test statistic of the Kolmogorov-Smirnov-Test. The alpha parameter is
-        very sensitive, therefore should be set below 0.01.
-    window_size
-        Size of the sliding window.
-    stat_size
-        Size of the statistic window.
-    window
-        Already collected data to avoid cold start.
-
-    Notes
-    -----
-    KSWIN (Kolmogorov-Smirnov Windowing) is a concept change detection method based
-    on the Kolmogorov-Smirnov (KS) statistical test. KS-test is a statistical test with
-    no assumption of underlying data distribution. KSWIN can monitor data or performance
-    distributions. Note that the detector accepts one dimensional input as array.
-
-    KSWIN maintains a sliding window $\Psi$ of fixed size $n$ (window_size). The
-    last $r$ (stat_size) samples of $\Psi$ are assumed to represent the last
-    concept considered as $R$. From the first $n-r$ samples of $\Psi$,
-    $r$ samples are uniformly drawn, representing an approximated last concept $W$.
-
-    The KS-test is performed on the windows $R$ and $W$ of the same size. KS
-    -test compares the distance of the empirical cumulative data distribution $dist(R,W)$.
-
-    A concept drift is detected by KSWIN if:
-
-    $$
-    dist(R,W) > \sqrt{-\frac{ln\alpha}{r}}
-    $$
-
-    The difference in empirical data distributions between the windows $R$ and $W$ is too large
-    since $R$ and $W$ come from the same distribution.
-
-    Examples
-    --------
-    >>> import numpy as np
-    >>> from river.drift import KSWIN
-    >>> np.random.seed(12345)
-
-    >>> kswin = KSWIN()
-
-    >>> # Simulate a data stream composed by two data distributions
-    >>> data_stream = np.concatenate((np.random.randint(2, size=1000),
-    ...                               np.random.randint(4, high=8, size=1000)))
-
-    >>> # Update drift detector and verify if change is detected
-    >>> for i, val in enumerate(data_stream):
-    ...     in_drift, in_warning = kswin.update(val)
-    ...     if in_drift:
-    ...         print(f"Change detected at index {i}, input value: {val}")
-    Change detected at index 1014, input value: 5
-    Change detected at index 1828, input value: 6
-
-    References
-    ----------
-    [^1]: Christoph Raab, Moritz Heusinger, Frank-Michael Schleif, Reactive Soft Prototype Computing for Concept Drift Streams, Neurocomputing, 2020,
-
-    """
-
-    def __init__(self, alpha=0.005, window_size=100, stat_size=30, window=None):
-        super().__init__()
-        self.window_size = window_size
-        self.stat_size = stat_size
-        self.alpha = alpha
-        self.p_value = 0
-        self.n = 0
-        if self.alpha < 0 or self.alpha > 1:
-            raise ValueError("Alpha must be between 0 and 1")
-
-        if self.window_size < 0:
-            raise ValueError("window_size must be greater than 0")
-
-        if self.window_size < self.stat_size:
-            raise ValueError("stat_size must be smaller than window_size")
-
-        if not isinstance(window, np.ndarray) or window is None:
-            self.window = np.array([])
-        else:
-            self.window = window
-
-    def update(self, value) -> tuple:
-        """Update the change detector with a single data point.
-
-        Adds an element on top of the sliding window and removes the oldest one from the window.
-        Afterwards, the KS-test is performed.
-
-        Parameters
-        ----------
-        value
-            New data sample the sliding window should add.
-
-        Returns
-        -------
-        A tuple (drift, warning) where its elements indicate if a drift or a warning is detected.
-
-        """
-        self.n += 1
-        currentLength = self.window.shape[0]
-        if currentLength >= self.window_size:
-            self.window = np.delete(self.window, 0)
-            rnd_window = np.random.choice(
-                self.window[: -self.stat_size], self.stat_size
-            )
-
-            (st, self.p_value) = stats.ks_2samp(
-                rnd_window, self.window[-self.stat_size :], mode="exact"
-            )
-
-            if self.p_value <= self.alpha and st > 0.1:
-                self._in_concept_change = True
-                self.window = self.window[-self.stat_size :]
-            else:
-                self._in_concept_change = False
-        else:  # Not enough samples in sliding window for a valid test
-            self._in_concept_change = False
-
-        self.window = np.concatenate([self.window, [value]])
-
-        return self._in_concept_change, self._in_warning_zone
-
-    def reset(self):
-        """Reset the change detector."""
-        self.p_value = 0
-        self.window = np.array([])
-        self._in_concept_change = False
+import numpy as np
+from scipy import stats
+
+from river.base import DriftDetector
+
+
+class KSWIN(DriftDetector):
+    r"""Kolmogorov-Smirnov Windowing method for concept drift detection.
+
+    Parameters
+    ----------
+    alpha
+        Probability for the test statistic of the Kolmogorov-Smirnov-Test. The alpha parameter is
+        very sensitive, therefore should be set below 0.01.
+    window_size
+        Size of the sliding window.
+    stat_size
+        Size of the statistic window.
+    window
+        Already collected data to avoid cold start.
+
+    Notes
+    -----
+    KSWIN (Kolmogorov-Smirnov Windowing) is a concept change detection method based
+    on the Kolmogorov-Smirnov (KS) statistical test. KS-test is a statistical test with
+    no assumption of underlying data distribution. KSWIN can monitor data or performance
+    distributions. Note that the detector accepts one dimensional input as array.
+
+    KSWIN maintains a sliding window $\Psi$ of fixed size $n$ (window_size). The
+    last $r$ (stat_size) samples of $\Psi$ are assumed to represent the last
+    concept considered as $R$. From the first $n-r$ samples of $\Psi$,
+    $r$ samples are uniformly drawn, representing an approximated last concept $W$.
+
+    The KS-test is performed on the windows $R$ and $W$ of the same size. KS
+    -test compares the distance of the empirical cumulative data distribution $dist(R,W)$.
+
+    A concept drift is detected by KSWIN if:
+
+    $$
+    dist(R,W) > \sqrt{-\frac{ln\alpha}{r}}
+    $$
+
+    The difference in empirical data distributions between the windows $R$ and $W$ is too large
+    since $R$ and $W$ come from the same distribution.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from river.drift import KSWIN
+    >>> np.random.seed(12345)
+
+    >>> kswin = KSWIN()
+
+    >>> # Simulate a data stream composed by two data distributions
+    >>> data_stream = np.concatenate((np.random.randint(2, size=1000),
+    ...                               np.random.randint(4, high=8, size=1000)))
+
+    >>> # Update drift detector and verify if change is detected
+    >>> for i, val in enumerate(data_stream):
+    ...     in_drift, in_warning = kswin.update(val)
+    ...     if in_drift:
+    ...         print(f"Change detected at index {i}, input value: {val}")
+    Change detected at index 1014, input value: 5
+    Change detected at index 1828, input value: 6
+
+    References
+    ----------
+    [^1]: Christoph Raab, Moritz Heusinger, Frank-Michael Schleif, Reactive Soft Prototype Computing for Concept Drift Streams, Neurocomputing, 2020,
+
+    """
+
+    def __init__(self, alpha=0.005, window_size=100, stat_size=30, window=None):
+        super().__init__()
+        self.window_size = window_size
+        self.stat_size = stat_size
+        self.alpha = alpha
+        self.p_value = 0
+        self.n = 0
+        if self.alpha < 0 or self.alpha > 1:
+            raise ValueError("Alpha must be between 0 and 1")
+
+        if self.window_size < 0:
+            raise ValueError("window_size must be greater than 0")
+
+        if self.window_size < self.stat_size:
+            raise ValueError("stat_size must be smaller than window_size")
+
+        if not isinstance(window, np.ndarray) or window is None:
+            self.window = np.array([])
+        else:
+            self.window = window
+
+    def update(self, value) -> tuple:
+        """Update the change detector with a single data point.
+
+        Adds an element on top of the sliding window and removes the oldest one from the window.
+        Afterwards, the KS-test is performed.
+
+        Parameters
+        ----------
+        value
+            New data sample the sliding window should add.
+
+        Returns
+        -------
+        A tuple (drift, warning) where its elements indicate if a drift or a warning is detected.
+
+        """
+        self.n += 1
+        currentLength = self.window.shape[0]
+        if currentLength >= self.window_size:
+            self.window = np.delete(self.window, 0)
+            rnd_window = np.random.choice(
+                self.window[: -self.stat_size], self.stat_size
+            )
+
+            (st, self.p_value) = stats.ks_2samp(
+                rnd_window, self.window[-self.stat_size :], mode="exact"
+            )
+
+            if self.p_value <= self.alpha and st > 0.1:
+                self._in_concept_change = True
+                self.window = self.window[-self.stat_size :]
+            else:
+                self._in_concept_change = False
+        else:  # Not enough samples in sliding window for a valid test
+            self._in_concept_change = False
+
+        self.window = np.concatenate([self.window, [value]])
+
+        return self._in_concept_change, self._in_warning_zone
+
+    def reset(self):
+        """Reset the change detector."""
+        self.p_value = 0
+        self.window = np.array([])
+        self._in_concept_change = False
```

### Comparing `river-0.8.0/river/drift/page_hinkley.py` & `river-0.9.0/river/drift/page_hinkley.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,97 +1,97 @@
-from river.base import DriftDetector
-
-
-class PageHinkley(DriftDetector):
-    """Page-Hinkley method for concept drift detection.
-
-    This change detection method works by computing the observed
-    values and their mean up to the current moment. Page-Hinkley
-    does not signal warning zones, only change detections.
-    The method works by means of the Page-Hinkley test. In general
-    lines it will detect a concept drift if the observed mean at
-    some instant is greater then a threshold value lambda.
-
-    Parameters
-    ----------
-    min_instances
-        The minimum number of instances before detecting change.
-    delta
-        The delta factor for the Page Hinkley test.
-    threshold
-        The change detection threshold (lambda).
-    alpha
-        The forgetting factor, used to weight the observed value and the mean.
-
-    Examples
-    --------
-    >>> import numpy as np
-    >>> from river.drift import PageHinkley
-    >>> np.random.seed(12345)
-
-    >>> ph = PageHinkley()
-
-    >>> # Simulate a data stream composed by two data distributions
-    >>> data_stream = np.concatenate((np.random.randint(2, size=1000),
-    ...                               np.random.randint(4, high=8, size=1000)))
-
-    >>> # Update drift detector and verify if change is detected
-    >>> for i, val in enumerate(data_stream):
-    ...     in_drift, in_warning = ph.update(val)
-    ...     if in_drift:
-    ...         print(f"Change detected at index {i}, input value: {val}")
-    Change detected at index 1009, input value: 5
-
-    References
-    ----------
-    [^1]: E. S. Page. 1954. Continuous Inspection Schemes. Biometrika 41, 1/2 (1954), 100–115.
-
-    """
-
-    def __init__(self, min_instances=30, delta=0.005, threshold=50, alpha=1 - 0.0001):
-        super().__init__()
-        self.min_instances = min_instances
-        self.delta = delta
-        self.threshold = threshold
-        self.alpha = alpha
-        self.x_mean = None
-        self.sample_count = None
-        self.sum = None
-        self.reset()
-
-    def reset(self):
-        """Reset the change detector."""
-        super().reset()
-        self.sample_count = 1
-        self.x_mean = 0.0
-        self.sum = 0.0
-
-    def update(self, value) -> tuple:
-        """Update the change detector with a single data point.
-
-        Parameters
-        ----------
-        value
-            Input value
-
-        Returns
-        -------
-        A tuple (drift, warning) where its elements indicate if a drift or a warning is detected.
-
-        """
-        if self._in_concept_change:
-            self.reset()
-
-        self.x_mean = self.x_mean + (value - self.x_mean) / float(self.sample_count)
-        self.sum = max(0.0, self.alpha * self.sum + (value - self.x_mean - self.delta))
-
-        self.sample_count += 1
-
-        self._in_concept_change = False
-
-        if self.sample_count < self.min_instances:
-            return False, False
-
-        if self.sum >= self.threshold:
-            self._in_concept_change = True
-
-        return self._in_concept_change, self._in_warning_zone
+from river.base import DriftDetector
+
+
+class PageHinkley(DriftDetector):
+    """Page-Hinkley method for concept drift detection.
+
+    This change detection method works by computing the observed
+    values and their mean up to the current moment. Page-Hinkley
+    does not signal warning zones, only change detections.
+    The method works by means of the Page-Hinkley test. In general
+    lines it will detect a concept drift if the observed mean at
+    some instant is greater then a threshold value lambda.
+
+    Parameters
+    ----------
+    min_instances
+        The minimum number of instances before detecting change.
+    delta
+        The delta factor for the Page Hinkley test.
+    threshold
+        The change detection threshold (lambda).
+    alpha
+        The forgetting factor, used to weight the observed value and the mean.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from river.drift import PageHinkley
+    >>> np.random.seed(12345)
+
+    >>> ph = PageHinkley()
+
+    >>> # Simulate a data stream composed by two data distributions
+    >>> data_stream = np.concatenate((np.random.randint(2, size=1000),
+    ...                               np.random.randint(4, high=8, size=1000)))
+
+    >>> # Update drift detector and verify if change is detected
+    >>> for i, val in enumerate(data_stream):
+    ...     in_drift, in_warning = ph.update(val)
+    ...     if in_drift:
+    ...         print(f"Change detected at index {i}, input value: {val}")
+    Change detected at index 1009, input value: 5
+
+    References
+    ----------
+    [^1]: E. S. Page. 1954. Continuous Inspection Schemes. Biometrika 41, 1/2 (1954), 100–115.
+
+    """
+
+    def __init__(self, min_instances=30, delta=0.005, threshold=50, alpha=1 - 0.0001):
+        super().__init__()
+        self.min_instances = min_instances
+        self.delta = delta
+        self.threshold = threshold
+        self.alpha = alpha
+        self.x_mean = None
+        self.sample_count = None
+        self.sum = None
+        self.reset()
+
+    def reset(self):
+        """Reset the change detector."""
+        super().reset()
+        self.sample_count = 1
+        self.x_mean = 0.0
+        self.sum = 0.0
+
+    def update(self, value) -> tuple:
+        """Update the change detector with a single data point.
+
+        Parameters
+        ----------
+        value
+            Input value
+
+        Returns
+        -------
+        A tuple (drift, warning) where its elements indicate if a drift or a warning is detected.
+
+        """
+        if self._in_concept_change:
+            self.reset()
+
+        self.x_mean += (value - self.x_mean) / float(self.sample_count)
+        self.sum = max(0.0, self.alpha * self.sum + (value - self.x_mean - self.delta))
+
+        self.sample_count += 1
+
+        self._in_concept_change = False
+
+        if self.sample_count < self.min_instances:
+            return False, False
+
+        if self.sum >= self.threshold:
+            self._in_concept_change = True
+
+        return self._in_concept_change, self._in_warning_zone
```

### Comparing `river-0.8.0/river/drift/test_drift_detectors.py` & `river-0.9.0/river/drift/test_drift_detectors.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,131 +1,131 @@
-import numpy as np
-import pytest
-
-from river.drift import ADWIN, DDM, EDDM, HDDM_A, HDDM_W, KSWIN, PageHinkley
-
-np.random.seed(12345)
-data_stream_1 = np.concatenate(
-    (np.random.randint(2, size=1000), np.random.randint(8, size=1000))
-)
-
-np.random.seed(12345)
-data_stream_2 = np.concatenate(
-    [
-        [np.random.binomial(1, 0.2) for _ in range(1000)],
-        [np.random.binomial(1, 0.8) for _ in range(1000)],
-    ]
-).astype(int)
-
-np.random.seed(12345)
-data_stream_3 = np.concatenate(
-    (
-        np.random.normal(0.0, 0.1, 500) > 0,
-        np.random.normal(0.25, 0.1, 500) > 0,
-        np.random.normal(0.0, 0.1, 500) > 0,
-        np.random.normal(0.25, 0.1, 500) > 0,
-    )
-).astype(int)
-
-
-def test_adwin():
-    expected_indices = [1055, 1087, 1215]
-    detected_indices = perform_test(ADWIN(), data_stream_1)
-
-    assert detected_indices == expected_indices
-
-
-def test_ddm():
-    expected_indices = [1049]
-    detected_indices = perform_test(DDM(), data_stream_2)
-    assert detected_indices == expected_indices
-
-
-def test_eddm():
-    expected_indices = [681, 848, 983, 1042, 1089]
-    detected_indices = perform_test(EDDM(), data_stream_2)
-    assert detected_indices == expected_indices
-
-
-def test_hddm_a():
-    hddm_a = HDDM_A()
-    expected_indices = [1047]
-    detected_indices = perform_test(hddm_a, data_stream_2)
-    assert detected_indices == expected_indices
-
-    # Second test, more abrupt drifts
-    hddm_a = HDDM_A(two_sided_test=True)
-    expected_indices = [531, 1015, 1545]
-    detected_indices = perform_test(hddm_a, data_stream_3)
-    assert detected_indices == expected_indices
-
-
-def test_hddm_w():
-    hddm_w = HDDM_W()
-    expected_indices = [1018]
-    detected_indices = perform_test(hddm_w, data_stream_2)
-    assert detected_indices == expected_indices
-
-    # Second test, more abrupt drifts
-    hddm_w = HDDM_W(two_sided_test=True)
-    expected_indices = [507, 1508]
-    detected_indices = perform_test(hddm_w, data_stream_3)
-    assert detected_indices == expected_indices
-
-
-def test_kswin():
-    kswin = KSWIN(alpha=0.0001, window_size=200, stat_size=100)
-    expected_indices = [1043, 1143]
-    detected_indices = perform_test(kswin, data_stream_1)
-    assert detected_indices == expected_indices
-
-
-def test_kswin_coverage():
-    with pytest.raises(ValueError):
-        KSWIN(alpha=-0.1)
-
-    with pytest.raises(ValueError):
-        KSWIN(alpha=1.1)
-
-    kswin = KSWIN(alpha=0.5)
-    assert kswin.alpha == 0.5
-
-    kswin = KSWIN(window="st")
-    assert isinstance(kswin.window, np.ndarray)
-
-    kswin = KSWIN(window=np.array([0.75, 0.80, 1, -1]))
-    assert isinstance(kswin.window, np.ndarray)
-
-    try:
-        KSWIN(window_size=-10)
-    except ValueError:
-        assert True
-    else:
-        assert False
-    try:
-        KSWIN(window_size=10, stat_size=30)
-    except ValueError:
-        assert True
-    else:
-        assert False
-
-    kswin = KSWIN()
-    kswin.reset()
-    assert kswin.p_value == 0
-    assert kswin.window.shape[0] == 0
-    assert kswin.change_detected is False
-
-
-def test_page_hinkley():
-    expected_indices = [1020, 1991]
-    detected_indices = perform_test(PageHinkley(), data_stream_1)
-
-    assert detected_indices == expected_indices
-
-
-def perform_test(drift_detector, data_stream):
-    detected_indices = []
-    for i, val in enumerate(data_stream):
-        in_drift, in_warning = drift_detector.update(val)
-        if in_drift:
-            detected_indices.append(i)
-    return detected_indices
+import numpy as np
+import pytest
+
+from river.drift import ADWIN, DDM, EDDM, HDDM_A, HDDM_W, KSWIN, PageHinkley
+
+np.random.seed(12345)
+data_stream_1 = np.concatenate(
+    (np.random.randint(2, size=1000), np.random.randint(8, size=1000))
+)
+
+np.random.seed(12345)
+data_stream_2 = np.concatenate(
+    [
+        [np.random.binomial(1, 0.2) for _ in range(1000)],
+        [np.random.binomial(1, 0.8) for _ in range(1000)],
+    ]
+).astype(int)
+
+np.random.seed(12345)
+data_stream_3 = np.concatenate(
+    (
+        np.random.normal(0.0, 0.1, 500) > 0,
+        np.random.normal(0.25, 0.1, 500) > 0,
+        np.random.normal(0.0, 0.1, 500) > 0,
+        np.random.normal(0.25, 0.1, 500) > 0,
+    )
+).astype(int)
+
+
+def test_adwin():
+    expected_indices = [1055, 1087, 1215]
+    detected_indices = perform_test(ADWIN(), data_stream_1)
+
+    assert detected_indices == expected_indices
+
+
+def test_ddm():
+    expected_indices = [1049]
+    detected_indices = perform_test(DDM(), data_stream_2)
+    assert detected_indices == expected_indices
+
+
+def test_eddm():
+    expected_indices = [681, 848, 983, 1042, 1089]
+    detected_indices = perform_test(EDDM(), data_stream_2)
+    assert detected_indices == expected_indices
+
+
+def test_hddm_a():
+    hddm_a = HDDM_A()
+    expected_indices = [1047]
+    detected_indices = perform_test(hddm_a, data_stream_2)
+    assert detected_indices == expected_indices
+
+    # Second test, more abrupt drifts
+    hddm_a = HDDM_A(two_sided_test=True)
+    expected_indices = [531, 1015, 1545]
+    detected_indices = perform_test(hddm_a, data_stream_3)
+    assert detected_indices == expected_indices
+
+
+def test_hddm_w():
+    hddm_w = HDDM_W()
+    expected_indices = [1018]
+    detected_indices = perform_test(hddm_w, data_stream_2)
+    assert detected_indices == expected_indices
+
+    # Second test, more abrupt drifts
+    hddm_w = HDDM_W(two_sided_test=True)
+    expected_indices = [507, 1508]
+    detected_indices = perform_test(hddm_w, data_stream_3)
+    assert detected_indices == expected_indices
+
+
+def test_kswin():
+    kswin = KSWIN(alpha=0.0001, window_size=200, stat_size=100)
+    expected_indices = [1043, 1143]
+    detected_indices = perform_test(kswin, data_stream_1)
+    assert detected_indices == expected_indices
+
+
+def test_kswin_coverage():
+    with pytest.raises(ValueError):
+        KSWIN(alpha=-0.1)
+
+    with pytest.raises(ValueError):
+        KSWIN(alpha=1.1)
+
+    kswin = KSWIN(alpha=0.5)
+    assert kswin.alpha == 0.5
+
+    kswin = KSWIN(window="st")
+    assert isinstance(kswin.window, np.ndarray)
+
+    kswin = KSWIN(window=np.array([0.75, 0.80, 1, -1]))
+    assert isinstance(kswin.window, np.ndarray)
+
+    try:
+        KSWIN(window_size=-10)
+    except ValueError:
+        assert True
+    else:
+        assert False
+    try:
+        KSWIN(window_size=10, stat_size=30)
+    except ValueError:
+        assert True
+    else:
+        assert False
+
+    kswin = KSWIN()
+    kswin.reset()
+    assert kswin.p_value == 0
+    assert kswin.window.shape[0] == 0
+    assert kswin.change_detected is False
+
+
+def test_page_hinkley():
+    expected_indices = [1020, 1991]
+    detected_indices = perform_test(PageHinkley(), data_stream_1)
+
+    assert detected_indices == expected_indices
+
+
+def perform_test(drift_detector, data_stream):
+    detected_indices = []
+    for i, val in enumerate(data_stream):
+        in_drift, in_warning = drift_detector.update(val)
+        if in_drift:
+            detected_indices.append(i)
+    return detected_indices
```

### Comparing `river-0.8.0/river/dummy.py` & `river-0.9.0/river/multiclass/ovr.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,180 +1,134 @@
-"""Dummy estimators.
-
-This module is here for testing purposes, as well as providing baseline performances.
-
-"""
-import collections
-
-from river import base, stats
-
-__all__ = ["NoChangeClassifier", "PriorClassifier", "StatisticRegressor"]
-
-
-class NoChangeClassifier(base.Classifier):
-    """Dummy classifier which returns the last class seen.
-
-    The predict_one method will output the last class seen whilst predict_proba_one will
-    return 1 for the last class seen and 0 for the others.
-
-    Attributes
-    ----------
-    last_class
-        The last class seen.
-    classes
-        The set of classes seen.
-
-    Examples
-    --------
-
-    Taken from example 2.1 from
-    [this page](https://www.cms.waikato.ac.nz/~abifet/book/chapter_2.html).
-
-    >>> import pprint
-    >>> from river import dummy
-
-    >>> sentences = [
-    ...     ('glad happy glad', '+'),
-    ...     ('glad glad joyful', '+'),
-    ...     ('glad pleasant', '+'),
-    ...     ('miserable sad glad', '−')
-    ... ]
-
-    >>> model = dummy.NoChangeClassifier()
-
-    >>> for sentence, label in sentences:
-    ...     model = model.learn_one(sentence, label)
-
-    >>> new_sentence = 'glad sad miserable pleasant glad'
-    >>> model.predict_one(new_sentence)
-    '−'
-
-    >>> pprint.pprint(model.predict_proba_one(new_sentence))
-    {'+': 0, '−': 1}
-
-    """
-
-    def __init__(self):
-        self.last_class = None
-        self.classes = set()
-
-    @property
-    def _multiclass(self):
-        return True
-
-    def learn_one(self, x, y):
-        self.last_class = y
-        self.classes.add(y)
-        return self
-
-    def predict_one(self, x):
-        return self.last_class
-
-    def predict_proba_one(self, x):
-        probas = {c: 0 for c in self.classes}
-        probas[self.last_class] = 1
-        return probas
-
-
-class PriorClassifier(base.Classifier):
-    """Dummy classifier which uses the prior distribution.
-
-    The `predict_one` method will output the most common class whilst `predict_proba_one` will
-    return the normalized class counts.
-
-    Attributes
-    ----------
-    counts : collections.Counter
-        Class counts.
-    n : int
-        Total number of seen instances.
-
-    Examples
-    --------
-
-    Taken from example 2.1 from
-    [this page](https://www.cms.waikato.ac.nz/~abifet/book/chapter_2.html)
-
-    >>> from river import dummy
-
-    >>> sentences = [
-    ...     ('glad happy glad', '+'),
-    ...     ('glad glad joyful', '+'),
-    ...     ('glad pleasant', '+'),
-    ...     ('miserable sad glad', '−')
-    ... ]
-
-    >>> model = dummy.PriorClassifier()
-
-    >>> for sentence, label in sentences:
-    ...     model = model.learn_one(sentence, label)
-
-    >>> new_sentence = 'glad sad miserable pleasant glad'
-    >>> model.predict_one(new_sentence)
-    '+'
-    >>> model.predict_proba_one(new_sentence)
-    {'+': 0.75, '−': 0.25}
-
-    """
-
-    def __init__(self):
-        self.counts = collections.Counter()
-        self.n = 0
-
-    @property
-    def _multiclass(self):
-        return True
-
-    def learn_one(self, x, y):
-        self.counts.update([y])
-        self.n += 1
-        return self
-
-    def predict_proba_one(self, x):
-        return {label: count / self.n for label, count in self.counts.items()}
-
-
-class StatisticRegressor(base.Regressor):
-    """Dummy regressor that uses a univariate statistic to make predictions.
-
-    Parameters
-    ----------
-    statistic
-
-    Examples
-    --------
-
-    >>> from pprint import pprint
-    >>> from river import dummy
-    >>> from river import stats
-
-    >>> sentences = [
-    ...     ('glad happy glad', 3),
-    ...     ('glad glad joyful', 3),
-    ...     ('glad pleasant', 2),
-    ...     ('miserable sad glad', -3)
-    ... ]
-
-    >>> model = dummy.StatisticRegressor(stats.Mean())
-
-    >>> for sentence, score in sentences:
-    ...     model = model.learn_one(sentence, score)
-
-    >>> new_sentence = 'glad sad miserable pleasant glad'
-    >>> model.predict_one(new_sentence)
-    1.25
-
-    """
-
-    def __init__(self, statistic: stats.Univariate):
-        self.statistic = statistic
-
-    @classmethod
-    def _unit_test_params(cls):
-        return {"statistic": stats.Mean()}
-
-    def learn_one(self, x, y):
-        self.statistic.update(y)
-        return self
-
-    def predict_one(self, x):
-        return self.statistic.get()
+import copy
+
+import pandas as pd
+
+from river import base, linear_model
+
+__all__ = ["OneVsRestClassifier"]
+
+
+class OneVsRestClassifier(base.Wrapper, base.Classifier):
+    """One-vs-the-rest (OvR) multiclass strategy.
+
+    This strategy consists in fitting one binary classifier per class. Because we are in a
+    streaming context, the number of classes isn't known from the start. Hence, new classifiers are
+    instantiated on the fly. Likewise, the predicted probabilities will only include the classes
+    seen up to a given point in time.
+
+    Note that this classifier supports mini-batches as well as single instances.
+
+    The computational complexity for both learning and predicting grows linearly with the number of
+    classes. If you have a very large number of classes, then you might want to consider using an
+    `multiclass.OutputCodeClassifier` instead.
+
+    Parameters
+    ----------
+    classifier
+        A binary classifier, although a multi-class classifier will work too.
+
+    Attributes
+    ----------
+    classifiers : dict
+        A mapping between classes and classifiers.
+
+    Examples
+    --------
+
+    >>> from river import datasets
+    >>> from river import evaluate
+    >>> from river import linear_model
+    >>> from river import metrics
+    >>> from river import multiclass
+    >>> from river import preprocessing
+
+    >>> dataset = datasets.ImageSegments()
+
+    >>> scaler = preprocessing.StandardScaler()
+    >>> ovr = multiclass.OneVsRestClassifier(linear_model.LogisticRegression())
+    >>> model = scaler | ovr
+
+    >>> metric = metrics.MacroF1()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    MacroF1: 0.774573
+
+    This estimator also also supports mini-batching.
+
+    >>> for X in pd.read_csv(dataset.path, chunksize=64):
+    ...     y = X.pop('category')
+    ...     y_pred = model.predict_many(X)
+    ...     model = model.learn_many(X, y)
+
+    """
+
+    def __init__(self, classifier: base.Classifier):
+        self.classifier = classifier
+        self.classifiers = {}
+        self._y_name = None
+
+    @property
+    def _wrapped_model(self):
+        return self.classifier
+
+    @property
+    def _multiclass(self):
+        return True
+
+    @classmethod
+    def _unit_test_params(cls):
+        yield {"classifier": linear_model.LogisticRegression()}
+
+    def learn_one(self, x, y):
+
+        # Instantiate a new binary classifier if the class is new
+        if y not in self.classifiers:
+            self.classifiers[y] = copy.deepcopy(self.classifier)
+
+        # Train each label's associated classifier
+        for label, model in self.classifiers.items():
+            model.learn_one(x, bool(y == label))
+
+        return self
+
+    def predict_proba_one(self, x):
+
+        y_pred = {}
+        total = 0.0
+
+        for label, model in self.classifiers.items():
+            yp = model.predict_proba_one(x)[True]
+            y_pred[label] = yp
+            total += yp
+
+        if total:
+            return {label: votes / total for label, votes in y_pred.items()}
+        return {label: 1 / len(y_pred) for label in y_pred}
+
+    def learn_many(self, X, y, **params):
+
+        self._y_name = y.name
+
+        # Instantiate a new binary classifier for the classes that have not yet been seen
+        for label in y.unique():
+            if label not in self.classifiers:
+                self.classifiers[label] = copy.deepcopy(self.classifier)
+
+        # Train each label's associated classifier
+        for label, model in self.classifiers.items():
+            model.learn_many(X, y == label, **params)
+
+        return self
+
+    def predict_proba_many(self, X):
+
+        y_pred = pd.DataFrame(columns=self.classifiers.keys(), index=X.index)
+
+        for label, clf in self.classifiers.items():
+            y_pred[label] = clf.predict_proba_many(X)[True]
+
+        return y_pred.div(y_pred.sum(axis="columns"), axis="rows")
+
+    def predict_many(self, X):
+        if not self.classifiers:
+            return pd.Series([None] * len(X), index=X.index, dtype="object")
+        return self.predict_proba_many(X).idxmax(axis="columns").rename(self._y_name)
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `river-0.8.0/river/ensemble/adaptive_random_forest.py` & `river-0.9.0/river/ensemble/adaptive_random_forest.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,1086 +1,1089 @@
-import abc
-import collections
-import copy
-import math
-import typing
-
-import numpy as np
-
-from river import base, metrics, stats, tree
-from river.drift import ADWIN
-from river.tree.nodes.arf_htc_nodes import (
-    RandomLeafMajorityClass,
-    RandomLeafNaiveBayes,
-    RandomLeafNaiveBayesAdaptive,
-)
-from river.tree.nodes.arf_htr_nodes import (
-    RandomLeafAdaptive,
-    RandomLeafMean,
-    RandomLeafModel,
-)
-from river.tree.splitter import Splitter
-from river.utils.skmultiflow_utils import check_random_state
-
-
-class BaseForest(base.EnsembleMixin):
-
-    _FEATURES_SQRT = "sqrt"
-    _FEATURES_LOG2 = "log2"
-
-    def __init__(
-        self,
-        n_models: int,
-        max_features: typing.Union[bool, str, int],
-        lambda_value: int,
-        drift_detector: typing.Union[base.DriftDetector, None],
-        warning_detector: typing.Union[base.DriftDetector, None],
-        metric: typing.Union[metrics.MultiClassMetric, metrics.RegressionMetric],
-        disable_weighted_vote,
-        seed,
-    ):
-        super().__init__([None])  # List of models is properly initialized later
-        self.models = []
-        self.n_models = n_models
-        self.max_features = max_features
-        self.lambda_value = lambda_value
-        self.metric = metric
-        self.disable_weighted_vote = disable_weighted_vote
-        self.drift_detector = drift_detector
-        self.warning_detector = warning_detector
-        self.seed = seed
-        self._rng = check_random_state(self.seed)  # Actual random number generator
-
-        # Internal parameters
-        self._n_samples_seen = 0
-        self._base_member_class = None
-
-    def learn_one(self, x: dict, y: base.typing.Target, **kwargs):
-        self._n_samples_seen += 1
-
-        if not self.models:
-            self._init_ensemble(list(x.keys()))
-
-        for model in self.models:
-            # Get prediction for instance
-            y_pred = model.predict_one(x)
-
-            # Update performance evaluator
-            model.metric.update(y_true=y, y_pred=y_pred)
-
-            k = self._rng.poisson(lam=self.lambda_value)
-            if k > 0:
-                # print(self._n_samples_seen)
-                model.learn_one(
-                    x=x, y=y, sample_weight=k, n_samples_seen=self._n_samples_seen
-                )
-
-        return self
-
-    def _init_ensemble(self, features: list):
-        self._set_max_features(len(features))
-
-        # Generate a different random seed per tree
-        seeds = self._rng.randint(0, 4294967295, size=self.n_models, dtype="u8")
-
-        self.models = [
-            self._base_member_class(
-                index_original=i,
-                model=self._new_base_model(seed=seeds[i]),
-                created_on=self._n_samples_seen,
-                drift_detector=self.drift_detector,
-                warning_detector=self.warning_detector,
-                is_background_learner=False,
-                metric=self.metric,
-            )
-            for i in range(self.n_models)
-        ]
-
-    @abc.abstractmethod
-    def _new_base_model(self, seed: int):
-        raise NotImplementedError
-
-    def _set_max_features(self, n_features):
-        if self.max_features == "sqrt":
-            self.max_features = round(math.sqrt(n_features))
-        elif self.max_features == "log2":
-            self.max_features = round(math.log2(n_features))
-        elif isinstance(self.max_features, int):
-            # Consider 'max_features' features at each split.
-            pass
-        elif isinstance(self.max_features, float):
-            # Consider 'max_features' as a percentage
-            self.max_features = int(self.max_features * n_features)
-        elif self.max_features is None:
-            self.max_features = n_features
-        else:
-            raise AttributeError(
-                f"Invalid max_features: {self.max_features}.\n"
-                f"Valid options are: int [2, M], float (0., 1.],"
-                f" {self._FEATURES_SQRT}, {self._FEATURES_LOG2}"
-            )
-        # Sanity checks
-        # max_features is negative, use max_features + n
-        if self.max_features < 0:
-            self.max_features += n_features
-        # max_features <= 0
-        # (m can be negative if max_features is negative and abs(max_features) > n),
-        # use max_features = 1
-        if self.max_features <= 0:
-            self.max_features = 1
-        # max_features > n, then use n
-        if self.max_features > n_features:
-            self.max_features = n_features
-
-    def reset(self):
-        """Reset the forest."""
-        self.models = []
-        self._n_samples_seen = 0
-        self._rng = check_random_state(self.seed)
-
-
-class BaseTreeClassifier(tree.HoeffdingTreeClassifier):
-    """Adaptive Random Forest Hoeffding Tree Classifier.
-
-    This is the base-estimator of the Adaptive Random Forest classifier.
-    This variant of the Hoeffding Tree classifier includes the `max_features`
-    parameter, which defines the number of randomly selected features to be
-    considered at each split.
-
-    """
-
-    def __init__(
-        self,
-        max_features: int = 2,
-        grace_period: int = 200,
-        max_depth: int = None,
-        split_criterion: str = "info_gain",
-        split_confidence: float = 1e-7,
-        tie_threshold: float = 0.05,
-        leaf_prediction: str = "nba",
-        nb_threshold: int = 0,
-        nominal_attributes: list = None,
-        splitter: Splitter = None,
-        binary_split: bool = False,
-        max_size: int = 100,
-        memory_estimate_period: int = 1000000,
-        stop_mem_management: bool = False,
-        remove_poor_attrs: bool = False,
-        merit_preprune: bool = True,
-        seed=None,
-    ):
-        super().__init__(
-            grace_period=grace_period,
-            max_depth=max_depth,
-            split_criterion=split_criterion,
-            split_confidence=split_confidence,
-            tie_threshold=tie_threshold,
-            leaf_prediction=leaf_prediction,
-            nb_threshold=nb_threshold,
-            nominal_attributes=nominal_attributes,
-            splitter=splitter,
-            binary_split=binary_split,
-            max_size=max_size,
-            memory_estimate_period=memory_estimate_period,
-            stop_mem_management=stop_mem_management,
-            remove_poor_attrs=remove_poor_attrs,
-            merit_preprune=merit_preprune,
-        )
-
-        self.max_features = max_features
-        self.seed = seed
-        self._rng = check_random_state(self.seed)
-
-    def _new_leaf(self, initial_stats=None, parent=None):
-        if initial_stats is None:
-            initial_stats = {}
-
-        if parent is None:
-            depth = 0
-        else:
-            depth = parent.depth + 1
-
-        # Generate a random seed for the new learning node
-        seed = self._rng.randint(0, 4294967295, dtype="u8")
-
-        if self._leaf_prediction == self._MAJORITY_CLASS:
-            return RandomLeafMajorityClass(
-                initial_stats, depth, self.splitter, self.max_features, seed,
-            )
-        elif self._leaf_prediction == self._NAIVE_BAYES:
-            return RandomLeafNaiveBayes(
-                initial_stats, depth, self.splitter, self.max_features, seed,
-            )
-        else:  # NAIVE BAYES ADAPTIVE (default)
-            return RandomLeafNaiveBayesAdaptive(
-                initial_stats, depth, self.splitter, self.max_features, seed,
-            )
-
-    def new_instance(self):
-        new_instance = self.clone()
-        # Use existing rng to enforce a different model
-        new_instance._rng = self._rng
-        return new_instance
-
-
-class BaseTreeRegressor(tree.HoeffdingTreeRegressor):
-    """ARF Hoeffding Tree regressor.
-
-    This is the base-estimator of the Adaptive Random Forest regressor.
-    This variant of the Hoeffding Tree regressor includes the `max_features`
-    parameter, which defines the number of randomly selected features to be
-    considered at each split.
-
-    """
-
-    def __init__(
-        self,
-        max_features: int = 2,
-        grace_period: int = 200,
-        max_depth: int = None,
-        split_confidence: float = 1e-7,
-        tie_threshold: float = 0.05,
-        leaf_prediction: str = "model",
-        leaf_model: base.Regressor = None,
-        model_selector_decay: float = 0.95,
-        nominal_attributes: list = None,
-        splitter: Splitter = None,
-        min_samples_split: int = 5,
-        binary_split: bool = False,
-        max_size: int = 100,
-        memory_estimate_period: int = 1000000,
-        stop_mem_management: bool = False,
-        remove_poor_attrs: bool = False,
-        merit_preprune: bool = True,
-        seed=None,
-    ):
-        super().__init__(
-            grace_period=grace_period,
-            max_depth=max_depth,
-            split_confidence=split_confidence,
-            tie_threshold=tie_threshold,
-            leaf_prediction=leaf_prediction,
-            leaf_model=leaf_model,
-            model_selector_decay=model_selector_decay,
-            nominal_attributes=nominal_attributes,
-            splitter=splitter,
-            min_samples_split=min_samples_split,
-            binary_split=binary_split,
-            max_size=max_size,
-            memory_estimate_period=memory_estimate_period,
-            stop_mem_management=stop_mem_management,
-            remove_poor_attrs=remove_poor_attrs,
-            merit_preprune=merit_preprune,
-        )
-
-        self.max_features = max_features
-        self.seed = seed
-        self._rng = check_random_state(self.seed)
-
-    def _new_leaf(self, initial_stats=None, parent=None):  # noqa
-        """Create a new learning node.
-
-        The type of learning node depends on the tree configuration.
-        """
-
-        if parent is not None:
-            depth = parent.depth + 1
-        else:
-            depth = 0
-
-        # Generate a random seed for the new learning node
-        seed = self._rng.randint(0, 4294967295, dtype="u8")
-
-        leaf_model = None
-        if self.leaf_prediction in {self._MODEL, self._ADAPTIVE}:
-            if parent is None:
-                leaf_model = copy.deepcopy(self.leaf_model)
-            else:
-                try:
-                    leaf_model = copy.deepcopy(parent._leaf_model)  # noqa
-                except AttributeError:
-                    leaf_model = copy.deepcopy(self.leaf_model)
-
-        if self.leaf_prediction == self._TARGET_MEAN:
-            return RandomLeafMean(
-                initial_stats, depth, self.splitter, self.max_features, seed,
-            )
-        elif self.leaf_prediction == self._MODEL:
-            return RandomLeafModel(
-                initial_stats,
-                depth,
-                self.splitter,
-                self.max_features,
-                seed,
-                leaf_model=leaf_model,
-            )
-        else:  # adaptive learning node
-            new_adaptive = RandomLeafAdaptive(
-                initial_stats,
-                depth,
-                self.splitter,
-                self.max_features,
-                seed,
-                leaf_model=leaf_model,
-            )
-            if parent is not None and isinstance(parent, RandomLeafAdaptive):
-                new_adaptive._fmse_mean = parent._fmse_mean  # noqa
-                new_adaptive._fmse_model = parent._fmse_model  # noqa
-
-            return new_adaptive
-
-    def new_instance(self):
-        new_instance = self.clone()
-        # Use existing rng to enforce a different model
-        new_instance._rng = self._rng
-        return new_instance
-
-
-class AdaptiveRandomForestClassifier(BaseForest, base.Classifier):
-    """Adaptive Random Forest classifier.
-
-    The 3 most important aspects of Adaptive Random Forest [^1] are:
-
-    1. inducing diversity through re-sampling
-
-    2. inducing diversity through randomly selecting subsets of features for
-       node splits
-
-    3. drift detectors per base tree, which cause selective resets in response
-       to drifts
-
-    It also allows training background trees, which start training if a
-    warning is detected and replace the active tree if the warning escalates
-    to a drift.
-
-    Parameters
-    ----------
-    n_models
-        Number of trees in the ensemble.
-    max_features
-        Max number of attributes for each node split.<br/>
-        - If `int`, then consider `max_features` at each split.<br/>
-        - If `float`, then `max_features` is a percentage and
-          `int(max_features * n_features)` features are considered per split.<br/>
-        - If "sqrt", then `max_features=sqrt(n_features)`.<br/>
-        - If "log2", then `max_features=log2(n_features)`.<br/>
-        - If None, then ``max_features=n_features``.
-    lambda_value
-        The lambda value for bagging (lambda=6 corresponds to Leveraging Bagging).
-    metric
-        Metric used to track trees performance within the ensemble.
-    disable_weighted_vote
-        If `True`, disables the weighted vote prediction.
-    drift_detector
-        Drift Detection method. Set to None to disable Drift detection.
-    warning_detector
-        Warning Detection method. Set to None to disable warning detection.
-    grace_period
-        [*Tree parameter*] Number of instances a leaf should observe between
-        split attempts.
-    max_depth
-        [*Tree parameter*] The maximum depth a tree can reach. If `None`, the
-        tree will grow indefinitely.
-    split_criterion
-        [*Tree parameter*] Split criterion to use.<br/>
-        - 'gini' - Gini<br/>
-        - 'info_gain' - Information Gain<br/>
-        - 'hellinger' - Hellinger Distance
-    split_confidence
-        [*Tree parameter*] Allowed error in split decision, a value closer to 0
-        takes longer to decide.
-    tie_threshold
-        [*Tree parameter*] Threshold below which a split will be forced to break
-        ties.
-    leaf_prediction
-        [*Tree parameter*] Prediction mechanism used at leafs.<br/>
-        - 'mc' - Majority Class<br/>
-        - 'nb' - Naive Bayes<br/>
-        - 'nba' - Naive Bayes Adaptive
-    nb_threshold
-        [*Tree parameter*] Number of instances a leaf should observe before
-        allowing Naive Bayes.
-    nominal_attributes
-        [*Tree parameter*] List of Nominal attributes. If empty, then assume that
-        all attributes are numerical.
-    splitter
-        [*Tree parameter*] The Splitter or Attribute Observer (AO) used to monitor the class
-        statistics of numeric features and perform splits. Splitters are available in the
-        `tree.splitter` module. Different splitters are available for classification and
-        regression tasks. Classification and regression splitters can be distinguished by their
-        property `is_target_class`. This is an advanced option. Special care must be taken when
-        choosing different splitters. By default, `tree.splitter.GaussianSplitter` is used
-        if `splitter` is `None`.
-    binary_split
-        [*Tree parameter*] If True, only allow binary splits.
-    max_size
-        [*Tree parameter*] Maximum memory (MB) consumed by the tree.
-    memory_estimate_period
-        [*Tree parameter*] Number of instances between memory consumption checks.
-    stop_mem_management
-        [*Tree parameter*] If True, stop growing as soon as memory limit is hit.
-    remove_poor_attrs
-        [*Tree parameter*] If True, disable poor attributes to reduce memory usage.
-    merit_preprune
-        [*Tree parameter*] If True, enable merit-based tree pre-pruning.
-    seed
-        If `int`, `seed` is used to seed the random number generator;
-        If `RandomState`, `seed` is the random number generator;
-        If `None`, the random number generator is the `RandomState` instance
-        used by `np.random`.
-
-    Examples
-    --------
-    >>> from river import synth
-    >>> from river import ensemble
-    >>> from river import evaluate
-    >>> from river import metrics
-
-    >>> dataset = synth.ConceptDriftStream(seed=42, position=500,
-    ...                                    width=40).take(1000)
-
-    >>> model = ensemble.AdaptiveRandomForestClassifier(
-    ...     n_models=3,
-    ...     seed=42
-    ... )
-
-    >>> metric = metrics.Accuracy()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    Accuracy: 70.47%
-
-    References
-    ----------
-    [^1]: Heitor Murilo Gomes, Albert Bifet, Jesse Read, Jean Paul Barddal,
-         Fabricio Enembreck, Bernhard Pfharinger, Geoff Holmes, Talel Abdessalem.
-         Adaptive random forests for evolving data stream classification.
-         In Machine Learning, DOI: 10.1007/s10994-017-5642-8, Springer, 2017.
-
-    """
-
-    def __init__(
-        self,
-        n_models: int = 10,
-        max_features: typing.Union[bool, str, int] = "sqrt",
-        lambda_value: int = 6,
-        metric: metrics.MultiClassMetric = metrics.Accuracy(),
-        disable_weighted_vote=False,
-        drift_detector: typing.Union[base.DriftDetector, None] = ADWIN(delta=0.001),
-        warning_detector: typing.Union[base.DriftDetector, None] = ADWIN(delta=0.01),
-        # Tree parameters
-        grace_period: int = 50,
-        max_depth: int = None,
-        split_criterion: str = "info_gain",
-        split_confidence: float = 0.01,
-        tie_threshold: float = 0.05,
-        leaf_prediction: str = "nba",
-        nb_threshold: int = 0,
-        nominal_attributes: list = None,
-        splitter: Splitter = None,
-        binary_split: bool = False,
-        max_size: int = 32,
-        memory_estimate_period: int = 2_000_000,
-        stop_mem_management: bool = False,
-        remove_poor_attrs: bool = False,
-        merit_preprune: bool = True,
-        seed: int = None,
-    ):
-        super().__init__(
-            n_models=n_models,
-            max_features=max_features,
-            lambda_value=lambda_value,
-            metric=metric,
-            disable_weighted_vote=disable_weighted_vote,
-            drift_detector=drift_detector,
-            warning_detector=warning_detector,
-            seed=seed,
-        )
-
-        self._n_samples_seen = 0
-        self._base_member_class = ForestMemberClassifier
-
-        # Tree parameters
-        self.grace_period = grace_period
-        self.max_depth = max_depth
-        self.split_criterion = split_criterion
-        self.split_confidence = split_confidence
-        self.tie_threshold = tie_threshold
-        self.leaf_prediction = leaf_prediction
-        self.nb_threshold = nb_threshold
-        self.nominal_attributes = nominal_attributes
-        self.splitter = splitter
-        self.binary_split = binary_split
-        self.max_size = max_size
-        self.memory_estimate_period = memory_estimate_period
-        self.stop_mem_management = stop_mem_management
-        self.remove_poor_attrs = remove_poor_attrs
-        self.merit_preprune = merit_preprune
-
-    @classmethod
-    def _unit_test_params(cls):
-        return {"n_models": 3}
-
-    def _unit_test_skips(self):
-        return {"check_shuffle_features_no_impact"}
-
-    def _multiclass(self):
-        return True
-
-    def predict_proba_one(self, x: dict) -> typing.Dict[base.typing.ClfTarget, float]:
-
-        y_pred = collections.Counter()
-
-        if not self.models:
-            self._init_ensemble(features=list(x.keys()))
-            return y_pred
-
-        for model in self.models:
-            y_proba_temp = model.predict_proba_one(x)
-            metric_value = model.metric.get()
-            if not self.disable_weighted_vote and metric_value > 0.0:
-                y_proba_temp = {
-                    k: val * metric_value for k, val in y_proba_temp.items()
-                }
-            y_pred.update(y_proba_temp)
-
-        total = sum(y_pred.values())
-        if total > 0:
-            return {label: proba / total for label, proba in y_pred.items()}
-        return y_pred
-
-    def _new_base_model(self, seed: int):
-        return BaseTreeClassifier(
-            max_features=self.max_features,
-            grace_period=self.grace_period,
-            split_criterion=self.split_criterion,
-            split_confidence=self.split_confidence,
-            tie_threshold=self.tie_threshold,
-            leaf_prediction=self.leaf_prediction,
-            nb_threshold=self.nb_threshold,
-            nominal_attributes=self.nominal_attributes,
-            splitter=self.splitter,
-            max_depth=self.max_depth,
-            binary_split=self.binary_split,
-            max_size=self.max_size,
-            memory_estimate_period=self.memory_estimate_period,
-            stop_mem_management=self.stop_mem_management,
-            remove_poor_attrs=self.remove_poor_attrs,
-            merit_preprune=self.merit_preprune,
-            seed=seed,
-        )
-
-
-class AdaptiveRandomForestRegressor(BaseForest, base.Regressor):
-    r"""Adaptive Random Forest regressor.
-
-    The 3 most important aspects of Adaptive Random Forest [^1] are:
-
-    1. inducing diversity through re-sampling
-
-    2. inducing diversity through randomly selecting subsets of features for
-       node splits
-
-    3. drift detectors per base tree, which cause selective resets in response
-       to drifts
-
-    Notice that this implementation is slightly different from the original
-    algorithm proposed in [^2]. The `HoeffdingTreeRegressor` is used as base
-    learner, instead of `FIMT-DD`. It also adds a new strategy to monitor the
-    predictions and check for concept drifts. The deviations of the predictions
-    to the target are monitored and normalized in the [0, 1] range to fulfill ADWIN's
-    requirements. We assume that the data subjected to the normalization follows
-    a normal distribution, and thus, lies within the interval of the mean $\pm3\sigma$.
-
-    Parameters
-    ----------
-    n_models
-        Number of trees in the ensemble.
-    max_features
-        Max number of attributes for each node split.<br/>
-        - If `int`, then consider `max_features` at each split.<br/>
-        - If `float`, then `max_features` is a percentage and
-          `int(max_features * n_features)` features are considered per split.<br/>
-        - If "sqrt", then `max_features=sqrt(n_features)`.<br/>
-        - If "log2", then `max_features=log2(n_features)`.<br/>
-        - If None, then ``max_features=n_features``.
-    lambda_value
-        The lambda value for bagging (lambda=6 corresponds to Leveraging Bagging).
-    metric
-        Metric used to track trees performance within the ensemble. Depending,
-        on the configuration, this metric is also used to weight predictions
-        from the members of the ensemble.
-    aggregation_method
-        The method to use to aggregate predictions in the ensemble.<br/>
-        - 'mean'<br/>
-        - 'median' - If selected will disable the weighted vote.
-    disable_weighted_vote
-        If `True`, disables the weighted vote prediction, i.e. does not assign
-        weights to individual tree's predictions and uses the arithmetic mean
-        instead. Otherwise will use the `metric` value to weight predictions.
-    drift_detector
-        Drift Detection method. Set to None to disable Drift detection.
-    warning_detector
-        Warning Detection method. Set to None to disable warning detection.
-    grace_period
-        [*Tree parameter*] Number of instances a leaf should observe between
-        split attempts.
-    max_depth
-        [*Tree parameter*] The maximum depth a tree can reach. If `None`, the
-        tree will grow indefinitely.
-    split_confidence
-        [*Tree parameter*] Allowed error in split decision, a value closer to 0
-        takes longer to decide.
-    tie_threshold
-        [*Tree parameter*] Threshold below which a split will be forced to break
-        ties.
-    leaf_prediction
-        [*Tree parameter*] Prediction mechanism used at leaves.</br>
-        - 'mean' - Target mean</br>
-        - 'model' - Uses the model defined in `leaf_model`</br>
-        - 'adaptive' - Chooses between 'mean' and 'model' dynamically</br>
-    leaf_model
-        [*Tree parameter*] The regression model used to provide responses if
-        `leaf_prediction='model'`. If not provided, an instance of
-        `river.linear_model.LinearRegression` with the default hyperparameters
-         is used.
-    model_selector_decay
-        [*Tree parameter*] The exponential decaying factor applied to the learning models'
-        squared errors, that are monitored if `leaf_prediction='adaptive'`. Must be
-        between `0` and `1`. The closer to `1`, the more importance is going to
-        be given to past observations. On the other hand, if its value
-        approaches `0`, the recent observed errors are going to have more
-        influence on the final decision.
-    nominal_attributes
-        [*Tree parameter*] List of Nominal attributes. If empty, then assume that
-        all attributes are numerical.
-    splitter
-        [*Tree parameter*] The Splitter or Attribute Observer (AO) used to monitor the class
-        statistics of numeric features and perform splits. Splitters are available in the
-        `tree.splitter` module. Different splitters are available for classification and
-        regression tasks. Classification and regression splitters can be distinguished by their
-        property `is_target_class`. This is an advanced option. Special care must be taken when
-        choosing different splitters.By default, `tree.splitter.EBSTSplitter` is used if
-        `splitter` is `None`.
-    min_samples_split
-        [*Tree parameter*] The minimum number of samples every branch resulting from a split
-        candidate must have to be considered valid.
-    binary_split
-        [*Tree parameter*] If True, only allow binary splits.
-    max_size
-        [*Tree parameter*] Maximum memory (MB) consumed by the tree.
-    memory_estimate_period
-        [*Tree parameter*] Number of instances between memory consumption checks.
-    stop_mem_management
-        [*Tree parameter*] If True, stop growing as soon as memory limit is hit.
-    remove_poor_attrs
-        [*Tree parameter*] If True, disable poor attributes to reduce memory usage.
-    merit_preprune
-        [*Tree parameter*] If True, enable merit-based tree pre-pruning.
-    seed
-        If `int`, `seed` is used to seed the random number generator;
-        If `RandomState`, `seed` is the random number generator;
-        If `None`, the random number generator is the `RandomState` instance
-        used by `np.random`.
-
-    References
-    ----------
-    [^1]: Gomes, H.M., Bifet, A., Read, J., Barddal, J.P., Enembreck, F.,
-          Pfharinger, B., Holmes, G. and Abdessalem, T., 2017. Adaptive random
-          forests for evolving data stream classification. Machine Learning,
-          106(9-10), pp.1469-1495.
-
-    [^2]: Gomes, H.M., Barddal, J.P., Boiko, L.E., Bifet, A., 2018.
-          Adaptive random forests for data stream regression. ESANN 2018.
-
-    Examples
-    --------
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import metrics
-    >>> from river import ensemble
-    >>> from river import preprocessing
-
-    >>> dataset = datasets.TrumpApproval()
-
-    >>> model = (
-    ...     preprocessing.StandardScaler() |
-    ...     ensemble.AdaptiveRandomForestRegressor(n_models=3, seed=42)
-    ... )
-
-    >>> metric = metrics.MAE()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    MAE: 1.870913
-
-    """
-
-    _MEAN = "mean"
-    _MEDIAN = "median"
-    _VALID_AGGREGATION_METHOD = [_MEAN, _MEDIAN]
-
-    def __init__(
-        self,
-        # Forest parameters
-        n_models: int = 10,
-        max_features="sqrt",
-        aggregation_method: str = "median",
-        lambda_value: int = 6,
-        metric: metrics.RegressionMetric = metrics.MSE(),
-        disable_weighted_vote=True,
-        drift_detector: base.DriftDetector = ADWIN(0.001),
-        warning_detector: base.DriftDetector = ADWIN(0.01),
-        # Tree parameters
-        grace_period: int = 50,
-        max_depth: int = None,
-        split_confidence: float = 0.01,
-        tie_threshold: float = 0.05,
-        leaf_prediction: str = "model",
-        leaf_model: base.Regressor = None,
-        model_selector_decay: float = 0.95,
-        nominal_attributes: list = None,
-        splitter: Splitter = None,
-        min_samples_split: int = 5,
-        binary_split: bool = False,
-        max_size: int = 500,
-        memory_estimate_period: int = 2_000_000,
-        stop_mem_management: bool = False,
-        remove_poor_attrs: bool = False,
-        merit_preprune: bool = True,
-        seed: int = None,
-    ):
-        super().__init__(
-            n_models=n_models,
-            max_features=max_features,
-            lambda_value=lambda_value,
-            metric=metric,
-            disable_weighted_vote=disable_weighted_vote,
-            drift_detector=drift_detector,
-            warning_detector=warning_detector,
-            seed=seed,
-        )
-
-        self._n_samples_seen = 0
-        self._base_member_class = ForestMemberRegressor
-
-        # Tree parameters
-        self.grace_period = grace_period
-        self.max_depth = max_depth
-        self.split_confidence = split_confidence
-        self.tie_threshold = tie_threshold
-        self.leaf_prediction = leaf_prediction
-        self.leaf_model = leaf_model
-        self.model_selector_decay = model_selector_decay
-        self.nominal_attributes = nominal_attributes
-        self.splitter = splitter
-        self.min_samples_split = min_samples_split
-        self.binary_split = binary_split
-        self.max_size = max_size
-        self.memory_estimate_period = memory_estimate_period
-        self.stop_mem_management = stop_mem_management
-        self.remove_poor_attrs = remove_poor_attrs
-        self.merit_preprune = merit_preprune
-
-        if aggregation_method in self._VALID_AGGREGATION_METHOD:
-            self.aggregation_method = aggregation_method
-        else:
-            raise ValueError(
-                f"Invalid aggregation_method: {aggregation_method}.\n"
-                f"Valid values are: {self._VALID_AGGREGATION_METHOD}"
-            )
-
-    @classmethod
-    def _unit_test_params(cls):
-        return {"n_models": 3}
-
-    def _unit_test_skips(self):
-        return {"check_shuffle_features_no_impact"}
-
-    def predict_one(self, x: dict) -> base.typing.RegTarget:
-
-        if not self.models:
-            self._init_ensemble(features=list(x.keys()))
-            return 0.0
-
-        y_pred = np.zeros(self.n_models)
-
-        if not self.disable_weighted_vote and self.aggregation_method != self._MEDIAN:
-            weights = np.zeros(self.n_models)
-            sum_weights = 0.0
-            for idx, model in enumerate(self.models):
-                y_pred[idx] = model.predict_one(x)
-                weights[idx] = model.metric.get()
-                sum_weights += weights[idx]
-
-            if sum_weights != 0:
-                # The higher the error, the worse is the tree
-                weights = sum_weights - weights
-                # Normalize weights to sum up to 1
-                weights = weights / weights.sum()
-                y_pred *= weights
-        else:
-            for idx, model in enumerate(self.models):
-                y_pred[idx] = model.predict_one(x)
-
-        if self.aggregation_method == self._MEAN:
-            y_pred = y_pred.mean()
-        else:
-            y_pred = np.median(y_pred)
-
-        return y_pred
-
-    def _new_base_model(self, seed: int):
-        return BaseTreeRegressor(
-            max_features=self.max_features,
-            grace_period=self.grace_period,
-            max_depth=self.max_depth,
-            split_confidence=self.split_confidence,
-            tie_threshold=self.tie_threshold,
-            leaf_prediction=self.leaf_prediction,
-            leaf_model=self.leaf_model,
-            model_selector_decay=self.model_selector_decay,
-            nominal_attributes=self.nominal_attributes,
-            splitter=self.splitter,
-            binary_split=self.binary_split,
-            max_size=self.max_size,
-            memory_estimate_period=self.memory_estimate_period,
-            stop_mem_management=self.stop_mem_management,
-            remove_poor_attrs=self.remove_poor_attrs,
-            merit_preprune=self.merit_preprune,
-            seed=seed,
-        )
-
-    @property
-    def valid_aggregation_method(self):
-        """Valid aggregation_method values."""
-        return self._VALID_AGGREGATION_METHOD
-
-
-class BaseForestMember:
-    """Base forest member class.
-
-    This class represents a tree member of the forest. It includes a
-    base tree model, the background learner, drift detectors and performance
-    tracking parameters.
-
-    The main purpose of this class is to train the foreground model.
-    Optionally, it monitors drift detection. Depending on the configuration,
-    if drift is detected then the foreground model is reset or replaced by a
-    background model.
-
-    Parameters
-    ----------
-    index_original
-        Tree index within the ensemble.
-    model
-        Tree learner.
-    created_on
-        Number of instances seen by the tree.
-    drift_detector
-        Drift Detection method.
-    warning_detector
-        Warning Detection method.
-    is_background_learner
-        True if the tree is a background learner.
-    metric
-        Metric to track performance.
-
-    """
-
-    def __init__(
-        self,
-        index_original: int,
-        model: typing.Union[BaseTreeClassifier, BaseTreeRegressor],
-        created_on: int,
-        drift_detector: base.DriftDetector,
-        warning_detector: base.DriftDetector,
-        is_background_learner,
-        metric: typing.Union[metrics.MultiClassMetric, metrics.RegressionMetric],
-    ):
-        self.index_original = index_original
-        self.model = model.clone()
-        self.created_on = created_on
-        self.is_background_learner = is_background_learner
-        self.metric = copy.deepcopy(metric)
-        # Make sure that the metric is not initialized, e.g. when creating background learners.
-        if isinstance(self.metric, metrics.MultiClassMetric):
-            self.metric.cm.reset()
-        # Keep a copy of the original metric for background learners or reset
-        self._original_metric = copy.deepcopy(metric)
-
-        self.background_learner = None
-
-        # Drift and warning detection
-        self.last_drift_on = 0
-        self.last_warning_on = 0
-        self.n_drifts_detected = 0
-        self.n_warnings_detected = 0
-
-        # Initialize drift and warning detectors
-        if drift_detector is not None:
-            self._use_drift_detector = True
-            self.drift_detector = drift_detector.clone()
-        else:
-            self._use_drift_detector = False
-            self.drift_detector = None
-
-        if warning_detector is not None:
-            self._use_background_learner = True
-            self.warning_detector = warning_detector.clone()
-        else:
-            self._use_background_learner = False
-            self.warning_detector = None
-
-    def reset(self, n_samples_seen):
-        if self._use_background_learner and self.background_learner is not None:
-            # Replace foreground model with background model
-            self.model = self.background_learner.model
-            self.warning_detector = self.background_learner.warning_detector
-            self.drift_detector = self.background_learner.drift_detector
-            self.metric = self.background_learner.metric
-            self.created_on = self.background_learner.created_on
-            self.background_learner = None
-        else:
-            # Reset model
-            self.model = self.model.clone()
-            self.metric = copy.deepcopy(self._original_metric)
-            self.created_on = n_samples_seen
-            self.drift_detector = self.drift_detector.clone()
-        # Make sure that the metric is not initialized, e.g. when creating background learners.
-        if isinstance(self.metric, metrics.MultiClassMetric):
-            self.metric.cm.reset()
-
-    def learn_one(
-        self, x: dict, y: base.typing.Target, *, sample_weight: int, n_samples_seen: int
-    ):
-
-        self.model.learn_one(x, y, sample_weight=sample_weight)
-
-        if self.background_learner:
-            # Train the background learner
-            self.background_learner.model.learn_one(
-                x=x, y=y, sample_weight=sample_weight
-            )
-
-        if self._use_drift_detector and not self.is_background_learner:
-            drift_detector_input = self._drift_detector_input(
-                y_true=y, y_pred=self.model.predict_one(x)
-            )
-
-            # Check for warning only if use_background_learner is set
-            if self._use_background_learner:
-                self.warning_detector.update(drift_detector_input)
-                # Check if there was a (warning) change
-                if self.warning_detector.change_detected:
-                    self.last_warning_on = n_samples_seen
-                    self.n_warnings_detected += 1
-                    # Create a new background learner object
-                    self.background_learner = self.__class__(
-                        index_original=self.index_original,
-                        model=self.model.new_instance(),
-                        created_on=n_samples_seen,
-                        drift_detector=self.drift_detector,
-                        warning_detector=self.warning_detector,
-                        is_background_learner=True,
-                        metric=self.metric,
-                    )
-                    # Reset the warning detector for the current object
-                    self.warning_detector = self.warning_detector.clone()
-
-            # Update the drift detector
-            self.drift_detector.update(drift_detector_input)
-
-            # Check if there was a change
-            if self.drift_detector.change_detected:
-                self.last_drift_on = n_samples_seen
-                self.n_drifts_detected += 1
-                self.reset(n_samples_seen)
-
-    @abc.abstractmethod
-    def _drift_detector_input(
-        self,
-        y_true: typing.Union[base.typing.ClfTarget, base.typing.RegTarget],
-        y_pred: typing.Union[base.typing.ClfTarget, base.typing.RegTarget],
-    ):
-        raise NotImplementedError
-
-
-class ForestMemberClassifier(BaseForestMember, base.Classifier):
-    """Forest member class for classification"""
-
-    def __init__(
-        self,
-        index_original: int,
-        model: BaseTreeClassifier,
-        created_on: int,
-        drift_detector: base.DriftDetector,
-        warning_detector: base.DriftDetector,
-        is_background_learner,
-        metric: metrics.MultiClassMetric,
-    ):
-        super().__init__(
-            index_original=index_original,
-            model=model,
-            created_on=created_on,
-            drift_detector=drift_detector,
-            warning_detector=warning_detector,
-            is_background_learner=is_background_learner,
-            metric=metric,
-        )
-
-    def _drift_detector_input(
-        self, y_true: base.typing.ClfTarget, y_pred: base.typing.ClfTarget
-    ):
-        return int(not y_true == y_pred)  # Not correctly_classifies
-
-    def predict_one(self, x):
-        return self.model.predict_one(x)
-
-    def predict_proba_one(self, x):
-        return self.model.predict_proba_one(x)
-
-
-class ForestMemberRegressor(BaseForestMember, base.Regressor):
-    """Forest member class for regression"""
-
-    def __init__(
-        self,
-        index_original: int,
-        model: BaseTreeRegressor,
-        created_on: int,
-        drift_detector: base.DriftDetector,
-        warning_detector: base.DriftDetector,
-        is_background_learner,
-        metric: metrics.RegressionMetric,
-    ):
-        super().__init__(
-            index_original=index_original,
-            model=model,
-            created_on=created_on,
-            drift_detector=drift_detector,
-            warning_detector=warning_detector,
-            is_background_learner=is_background_learner,
-            metric=metric,
-        )
-        self._var = stats.Var()  # Used to track drift
-
-    def _drift_detector_input(self, y_true: float, y_pred: float):
-        drift_input = y_true - y_pred
-        self._var.update(drift_input)
-
-        if self._var.mean.n == 1:
-            return 0.5  # The expected error is the normalized mean error
-
-        sd = math.sqrt(self._var.get())
-
-        # We assume the error follows a normal distribution -> (empirical rule)
-        # 99.73% of the values lie  between [mean - 3*sd, mean + 3*sd]. We
-        # assume this range for the normalized data. Hence, we can apply the
-        # min-max norm to cope with  ADWIN's requirements
-        return (drift_input + 3 * sd) / (6 * sd) if sd > 0 else 0.5
-
-    def reset(self, n_samples_seen):
-        super().reset(n_samples_seen)
-        # Reset the stats for the drift detector
-        self._var = stats.Var()
-
-    def predict_one(self, x):
-        return self.model.predict_one(x)
+import abc
+import collections
+import copy
+import math
+import typing
+
+import numpy as np
+
+from river import base, metrics, stats, tree
+from river.drift import ADWIN
+from river.tree.nodes.arf_htc_nodes import (
+    RandomLeafMajorityClass,
+    RandomLeafNaiveBayes,
+    RandomLeafNaiveBayesAdaptive,
+)
+from river.tree.nodes.arf_htr_nodes import (
+    RandomLeafAdaptive,
+    RandomLeafMean,
+    RandomLeafModel,
+)
+from river.tree.splitter import Splitter
+from river.utils.skmultiflow_utils import check_random_state
+
+
+class BaseForest(base.Ensemble):
+
+    _FEATURES_SQRT = "sqrt"
+    _FEATURES_LOG2 = "log2"
+
+    def __init__(
+        self,
+        n_models: int,
+        max_features: typing.Union[bool, str, int],
+        lambda_value: int,
+        drift_detector: typing.Optional[base.DriftDetector],
+        warning_detector: typing.Optional[base.DriftDetector],
+        metric: typing.Union[metrics.MultiClassMetric, metrics.RegressionMetric],
+        disable_weighted_vote,
+        seed,
+    ):
+        super().__init__([])  # List of models is properly initialized later
+        self.n_models = n_models
+        self.max_features = max_features
+        self.lambda_value = lambda_value
+        self.metric = metric
+        self.disable_weighted_vote = disable_weighted_vote
+        self.drift_detector = drift_detector
+        self.warning_detector = warning_detector
+        self.seed = seed
+        self._rng = check_random_state(self.seed)  # Actual random number generator
+
+        # Internal parameters
+        self._n_samples_seen = 0
+        self._base_member_class = None
+
+    @property
+    def _min_number_of_models(self):
+        return 0
+
+    def learn_one(self, x: dict, y: base.typing.Target, **kwargs):
+        self._n_samples_seen += 1
+
+        if not self:
+            self._init_ensemble(list(x.keys()))
+
+        for model in self:
+            # Get prediction for instance
+            y_pred = model.predict_one(x)
+
+            # Update performance evaluator
+            model.metric.update(y_true=y, y_pred=y_pred)
+
+            k = self._rng.poisson(lam=self.lambda_value)
+            if k > 0:
+                # print(self._n_samples_seen)
+                model.learn_one(
+                    x=x, y=y, sample_weight=k, n_samples_seen=self._n_samples_seen
+                )
+
+        return self
+
+    def _init_ensemble(self, features: list):
+        self._set_max_features(len(features))
+
+        # Generate a different random seed per tree
+        seeds = self._rng.randint(0, 4294967295, size=self.n_models, dtype="u8")
+
+        self.data = [
+            self._base_member_class(
+                index_original=i,
+                model=self._new_base_model(seed=seeds[i]),
+                created_on=self._n_samples_seen,
+                drift_detector=self.drift_detector,
+                warning_detector=self.warning_detector,
+                is_background_learner=False,
+                metric=self.metric,
+            )
+            for i in range(self.n_models)
+        ]
+
+    @abc.abstractmethod
+    def _new_base_model(self, seed: int):
+        raise NotImplementedError
+
+    def _set_max_features(self, n_features):
+        if self.max_features == "sqrt":
+            self.max_features = round(math.sqrt(n_features))
+        elif self.max_features == "log2":
+            self.max_features = round(math.log2(n_features))
+        elif isinstance(self.max_features, int):
+            # Consider 'max_features' features at each split.
+            pass
+        elif isinstance(self.max_features, float):
+            # Consider 'max_features' as a percentage
+            self.max_features = int(self.max_features * n_features)
+        elif self.max_features is None:
+            self.max_features = n_features
+        else:
+            raise AttributeError(
+                f"Invalid max_features: {self.max_features}.\n"
+                f"Valid options are: int [2, M], float (0., 1.],"
+                f" {self._FEATURES_SQRT}, {self._FEATURES_LOG2}"
+            )
+        # Sanity checks
+        # max_features is negative, use max_features + n
+        if self.max_features < 0:
+            self.max_features += n_features
+        # max_features <= 0
+        # (m can be negative if max_features is negative and abs(max_features) > n),
+        # use max_features = 1
+        if self.max_features <= 0:
+            self.max_features = 1
+        # max_features > n, then use n
+        if self.max_features > n_features:
+            self.max_features = n_features
+
+    def reset(self):
+        """Reset the forest."""
+        self.models = []
+        self._n_samples_seen = 0
+        self._rng = check_random_state(self.seed)
+
+
+class BaseTreeClassifier(tree.HoeffdingTreeClassifier):
+    """Adaptive Random Forest Hoeffding Tree Classifier.
+
+    This is the base-estimator of the Adaptive Random Forest classifier.
+    This variant of the Hoeffding Tree classifier includes the `max_features`
+    parameter, which defines the number of randomly selected features to be
+    considered at each split.
+
+    """
+
+    def __init__(
+        self,
+        max_features: int = 2,
+        grace_period: int = 200,
+        max_depth: int = None,
+        split_criterion: str = "info_gain",
+        split_confidence: float = 1e-7,
+        tie_threshold: float = 0.05,
+        leaf_prediction: str = "nba",
+        nb_threshold: int = 0,
+        nominal_attributes: list = None,
+        splitter: Splitter = None,
+        binary_split: bool = False,
+        max_size: int = 100,
+        memory_estimate_period: int = 1000000,
+        stop_mem_management: bool = False,
+        remove_poor_attrs: bool = False,
+        merit_preprune: bool = True,
+        seed=None,
+    ):
+        super().__init__(
+            grace_period=grace_period,
+            max_depth=max_depth,
+            split_criterion=split_criterion,
+            split_confidence=split_confidence,
+            tie_threshold=tie_threshold,
+            leaf_prediction=leaf_prediction,
+            nb_threshold=nb_threshold,
+            nominal_attributes=nominal_attributes,
+            splitter=splitter,
+            binary_split=binary_split,
+            max_size=max_size,
+            memory_estimate_period=memory_estimate_period,
+            stop_mem_management=stop_mem_management,
+            remove_poor_attrs=remove_poor_attrs,
+            merit_preprune=merit_preprune,
+        )
+
+        self.max_features = max_features
+        self.seed = seed
+        self._rng = check_random_state(self.seed)
+
+    def _new_leaf(self, initial_stats=None, parent=None):
+        if initial_stats is None:
+            initial_stats = {}
+
+        if parent is None:
+            depth = 0
+        else:
+            depth = parent.depth + 1
+
+        # Generate a random seed for the new learning node
+        seed = self._rng.randint(0, 4294967295, dtype="u8")
+
+        if self._leaf_prediction == self._MAJORITY_CLASS:
+            return RandomLeafMajorityClass(
+                initial_stats, depth, self.splitter, self.max_features, seed,
+            )
+        elif self._leaf_prediction == self._NAIVE_BAYES:
+            return RandomLeafNaiveBayes(
+                initial_stats, depth, self.splitter, self.max_features, seed,
+            )
+        else:  # NAIVE BAYES ADAPTIVE (default)
+            return RandomLeafNaiveBayesAdaptive(
+                initial_stats, depth, self.splitter, self.max_features, seed,
+            )
+
+    def new_instance(self):
+        new_instance = self.clone()
+        # Use existing rng to enforce a different model
+        new_instance._rng = self._rng
+        return new_instance
+
+
+class BaseTreeRegressor(tree.HoeffdingTreeRegressor):
+    """ARF Hoeffding Tree regressor.
+
+    This is the base-estimator of the Adaptive Random Forest regressor.
+    This variant of the Hoeffding Tree regressor includes the `max_features`
+    parameter, which defines the number of randomly selected features to be
+    considered at each split.
+
+    """
+
+    def __init__(
+        self,
+        max_features: int = 2,
+        grace_period: int = 200,
+        max_depth: int = None,
+        split_confidence: float = 1e-7,
+        tie_threshold: float = 0.05,
+        leaf_prediction: str = "model",
+        leaf_model: base.Regressor = None,
+        model_selector_decay: float = 0.95,
+        nominal_attributes: list = None,
+        splitter: Splitter = None,
+        min_samples_split: int = 5,
+        binary_split: bool = False,
+        max_size: int = 100,
+        memory_estimate_period: int = 1000000,
+        stop_mem_management: bool = False,
+        remove_poor_attrs: bool = False,
+        merit_preprune: bool = True,
+        seed=None,
+    ):
+        super().__init__(
+            grace_period=grace_period,
+            max_depth=max_depth,
+            split_confidence=split_confidence,
+            tie_threshold=tie_threshold,
+            leaf_prediction=leaf_prediction,
+            leaf_model=leaf_model,
+            model_selector_decay=model_selector_decay,
+            nominal_attributes=nominal_attributes,
+            splitter=splitter,
+            min_samples_split=min_samples_split,
+            binary_split=binary_split,
+            max_size=max_size,
+            memory_estimate_period=memory_estimate_period,
+            stop_mem_management=stop_mem_management,
+            remove_poor_attrs=remove_poor_attrs,
+            merit_preprune=merit_preprune,
+        )
+
+        self.max_features = max_features
+        self.seed = seed
+        self._rng = check_random_state(self.seed)
+
+    def _new_leaf(self, initial_stats=None, parent=None):  # noqa
+        """Create a new learning node.
+
+        The type of learning node depends on the tree configuration.
+        """
+
+        if parent is not None:
+            depth = parent.depth + 1
+        else:
+            depth = 0
+
+        # Generate a random seed for the new learning node
+        seed = self._rng.randint(0, 4294967295, dtype="u8")
+
+        leaf_model = None
+        if self.leaf_prediction in {self._MODEL, self._ADAPTIVE}:
+            if parent is None:
+                leaf_model = copy.deepcopy(self.leaf_model)
+            else:
+                try:
+                    leaf_model = copy.deepcopy(parent._leaf_model)  # noqa
+                except AttributeError:
+                    leaf_model = copy.deepcopy(self.leaf_model)
+
+        if self.leaf_prediction == self._TARGET_MEAN:
+            return RandomLeafMean(
+                initial_stats, depth, self.splitter, self.max_features, seed,
+            )
+        elif self.leaf_prediction == self._MODEL:
+            return RandomLeafModel(
+                initial_stats,
+                depth,
+                self.splitter,
+                self.max_features,
+                seed,
+                leaf_model=leaf_model,
+            )
+        else:  # adaptive learning node
+            new_adaptive = RandomLeafAdaptive(
+                initial_stats,
+                depth,
+                self.splitter,
+                self.max_features,
+                seed,
+                leaf_model=leaf_model,
+            )
+            if parent is not None and isinstance(parent, RandomLeafAdaptive):
+                new_adaptive._fmse_mean = parent._fmse_mean  # noqa
+                new_adaptive._fmse_model = parent._fmse_model  # noqa
+
+            return new_adaptive
+
+    def new_instance(self):
+        new_instance = self.clone()
+        # Use existing rng to enforce a different model
+        new_instance._rng = self._rng
+        return new_instance
+
+
+class AdaptiveRandomForestClassifier(BaseForest, base.Classifier):
+    """Adaptive Random Forest classifier.
+
+    The 3 most important aspects of Adaptive Random Forest [^1] are:
+
+    1. inducing diversity through re-sampling
+
+    2. inducing diversity through randomly selecting subsets of features for
+       node splits
+
+    3. drift detectors per base tree, which cause selective resets in response
+       to drifts
+
+    It also allows training background trees, which start training if a
+    warning is detected and replace the active tree if the warning escalates
+    to a drift.
+
+    Parameters
+    ----------
+    n_models
+        Number of trees in the ensemble.
+    max_features
+        Max number of attributes for each node split.<br/>
+        - If `int`, then consider `max_features` at each split.<br/>
+        - If `float`, then `max_features` is a percentage and
+          `int(max_features * n_features)` features are considered per split.<br/>
+        - If "sqrt", then `max_features=sqrt(n_features)`.<br/>
+        - If "log2", then `max_features=log2(n_features)`.<br/>
+        - If None, then ``max_features=n_features``.
+    lambda_value
+        The lambda value for bagging (lambda=6 corresponds to Leveraging Bagging).
+    metric
+        Metric used to track trees performance within the ensemble.
+    disable_weighted_vote
+        If `True`, disables the weighted vote prediction.
+    drift_detector
+        Drift Detection method. Set to None to disable Drift detection.
+    warning_detector
+        Warning Detection method. Set to None to disable warning detection.
+    grace_period
+        [*Tree parameter*] Number of instances a leaf should observe between
+        split attempts.
+    max_depth
+        [*Tree parameter*] The maximum depth a tree can reach. If `None`, the
+        tree will grow indefinitely.
+    split_criterion
+        [*Tree parameter*] Split criterion to use.<br/>
+        - 'gini' - Gini<br/>
+        - 'info_gain' - Information Gain<br/>
+        - 'hellinger' - Hellinger Distance
+    split_confidence
+        [*Tree parameter*] Allowed error in split decision, a value closer to 0
+        takes longer to decide.
+    tie_threshold
+        [*Tree parameter*] Threshold below which a split will be forced to break
+        ties.
+    leaf_prediction
+        [*Tree parameter*] Prediction mechanism used at leafs.<br/>
+        - 'mc' - Majority Class<br/>
+        - 'nb' - Naive Bayes<br/>
+        - 'nba' - Naive Bayes Adaptive
+    nb_threshold
+        [*Tree parameter*] Number of instances a leaf should observe before
+        allowing Naive Bayes.
+    nominal_attributes
+        [*Tree parameter*] List of Nominal attributes. If empty, then assume that
+        all attributes are numerical.
+    splitter
+        [*Tree parameter*] The Splitter or Attribute Observer (AO) used to monitor the class
+        statistics of numeric features and perform splits. Splitters are available in the
+        `tree.splitter` module. Different splitters are available for classification and
+        regression tasks. Classification and regression splitters can be distinguished by their
+        property `is_target_class`. This is an advanced option. Special care must be taken when
+        choosing different splitters. By default, `tree.splitter.GaussianSplitter` is used
+        if `splitter` is `None`.
+    binary_split
+        [*Tree parameter*] If True, only allow binary splits.
+    max_size
+        [*Tree parameter*] Maximum memory (MB) consumed by the tree.
+    memory_estimate_period
+        [*Tree parameter*] Number of instances between memory consumption checks.
+    stop_mem_management
+        [*Tree parameter*] If True, stop growing as soon as memory limit is hit.
+    remove_poor_attrs
+        [*Tree parameter*] If True, disable poor attributes to reduce memory usage.
+    merit_preprune
+        [*Tree parameter*] If True, enable merit-based tree pre-pruning.
+    seed
+        If `int`, `seed` is used to seed the random number generator;
+        If `RandomState`, `seed` is the random number generator;
+        If `None`, the random number generator is the `RandomState` instance
+        used by `np.random`.
+
+    Examples
+    --------
+    >>> from river import synth
+    >>> from river import ensemble
+    >>> from river import evaluate
+    >>> from river import metrics
+
+    >>> dataset = synth.ConceptDriftStream(seed=42, position=500,
+    ...                                    width=40).take(1000)
+
+    >>> model = ensemble.AdaptiveRandomForestClassifier(
+    ...     n_models=3,
+    ...     seed=42
+    ... )
+
+    >>> metric = metrics.Accuracy()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    Accuracy: 70.47%
+
+    References
+    ----------
+    [^1]: Heitor Murilo Gomes, Albert Bifet, Jesse Read, Jean Paul Barddal,
+         Fabricio Enembreck, Bernhard Pfharinger, Geoff Holmes, Talel Abdessalem.
+         Adaptive random forests for evolving data stream classification.
+         In Machine Learning, DOI: 10.1007/s10994-017-5642-8, Springer, 2017.
+
+    """
+
+    def __init__(
+        self,
+        n_models: int = 10,
+        max_features: typing.Union[bool, str, int] = "sqrt",
+        lambda_value: int = 6,
+        metric: metrics.MultiClassMetric = metrics.Accuracy(),
+        disable_weighted_vote=False,
+        drift_detector: typing.Union[base.DriftDetector, None] = ADWIN(delta=0.001),
+        warning_detector: typing.Union[base.DriftDetector, None] = ADWIN(delta=0.01),
+        # Tree parameters
+        grace_period: int = 50,
+        max_depth: int = None,
+        split_criterion: str = "info_gain",
+        split_confidence: float = 0.01,
+        tie_threshold: float = 0.05,
+        leaf_prediction: str = "nba",
+        nb_threshold: int = 0,
+        nominal_attributes: list = None,
+        splitter: Splitter = None,
+        binary_split: bool = False,
+        max_size: int = 32,
+        memory_estimate_period: int = 2_000_000,
+        stop_mem_management: bool = False,
+        remove_poor_attrs: bool = False,
+        merit_preprune: bool = True,
+        seed: int = None,
+    ):
+        super().__init__(
+            n_models=n_models,
+            max_features=max_features,
+            lambda_value=lambda_value,
+            metric=metric,
+            disable_weighted_vote=disable_weighted_vote,
+            drift_detector=drift_detector,
+            warning_detector=warning_detector,
+            seed=seed,
+        )
+
+        self._n_samples_seen = 0
+        self._base_member_class = ForestMemberClassifier
+
+        # Tree parameters
+        self.grace_period = grace_period
+        self.max_depth = max_depth
+        self.split_criterion = split_criterion
+        self.split_confidence = split_confidence
+        self.tie_threshold = tie_threshold
+        self.leaf_prediction = leaf_prediction
+        self.nb_threshold = nb_threshold
+        self.nominal_attributes = nominal_attributes
+        self.splitter = splitter
+        self.binary_split = binary_split
+        self.max_size = max_size
+        self.memory_estimate_period = memory_estimate_period
+        self.stop_mem_management = stop_mem_management
+        self.remove_poor_attrs = remove_poor_attrs
+        self.merit_preprune = merit_preprune
+
+    @classmethod
+    def _unit_test_params(cls):
+        yield {"n_models": 3}
+
+    def _unit_test_skips(self):
+        return {"check_shuffle_features_no_impact"}
+
+    def _multiclass(self):
+        return True
+
+    def predict_proba_one(self, x: dict) -> typing.Dict[base.typing.ClfTarget, float]:
+
+        y_pred = collections.Counter()
+
+        if not self.models:
+            self._init_ensemble(features=list(x.keys()))
+            return y_pred
+
+        for model in self.models:
+            y_proba_temp = model.predict_proba_one(x)
+            metric_value = model.metric.get()
+            if not self.disable_weighted_vote and metric_value > 0.0:
+                y_proba_temp = {
+                    k: val * metric_value for k, val in y_proba_temp.items()
+                }
+            y_pred.update(y_proba_temp)
+
+        total = sum(y_pred.values())
+        if total > 0:
+            return {label: proba / total for label, proba in y_pred.items()}
+        return y_pred
+
+    def _new_base_model(self, seed: int):
+        return BaseTreeClassifier(
+            max_features=self.max_features,
+            grace_period=self.grace_period,
+            split_criterion=self.split_criterion,
+            split_confidence=self.split_confidence,
+            tie_threshold=self.tie_threshold,
+            leaf_prediction=self.leaf_prediction,
+            nb_threshold=self.nb_threshold,
+            nominal_attributes=self.nominal_attributes,
+            splitter=self.splitter,
+            max_depth=self.max_depth,
+            binary_split=self.binary_split,
+            max_size=self.max_size,
+            memory_estimate_period=self.memory_estimate_period,
+            stop_mem_management=self.stop_mem_management,
+            remove_poor_attrs=self.remove_poor_attrs,
+            merit_preprune=self.merit_preprune,
+            seed=seed,
+        )
+
+
+class AdaptiveRandomForestRegressor(BaseForest, base.Regressor):
+    r"""Adaptive Random Forest regressor.
+
+    The 3 most important aspects of Adaptive Random Forest [^1] are:
+
+    1. inducing diversity through re-sampling
+
+    2. inducing diversity through randomly selecting subsets of features for
+       node splits
+
+    3. drift detectors per base tree, which cause selective resets in response
+       to drifts
+
+    Notice that this implementation is slightly different from the original
+    algorithm proposed in [^2]. The `HoeffdingTreeRegressor` is used as base
+    learner, instead of `FIMT-DD`. It also adds a new strategy to monitor the
+    predictions and check for concept drifts. The deviations of the predictions
+    to the target are monitored and normalized in the [0, 1] range to fulfill ADWIN's
+    requirements. We assume that the data subjected to the normalization follows
+    a normal distribution, and thus, lies within the interval of the mean $\pm3\sigma$.
+
+    Parameters
+    ----------
+    n_models
+        Number of trees in the ensemble.
+    max_features
+        Max number of attributes for each node split.<br/>
+        - If `int`, then consider `max_features` at each split.<br/>
+        - If `float`, then `max_features` is a percentage and
+          `int(max_features * n_features)` features are considered per split.<br/>
+        - If "sqrt", then `max_features=sqrt(n_features)`.<br/>
+        - If "log2", then `max_features=log2(n_features)`.<br/>
+        - If None, then ``max_features=n_features``.
+    lambda_value
+        The lambda value for bagging (lambda=6 corresponds to Leveraging Bagging).
+    metric
+        Metric used to track trees performance within the ensemble. Depending,
+        on the configuration, this metric is also used to weight predictions
+        from the members of the ensemble.
+    aggregation_method
+        The method to use to aggregate predictions in the ensemble.<br/>
+        - 'mean'<br/>
+        - 'median' - If selected will disable the weighted vote.
+    disable_weighted_vote
+        If `True`, disables the weighted vote prediction, i.e. does not assign
+        weights to individual tree's predictions and uses the arithmetic mean
+        instead. Otherwise will use the `metric` value to weight predictions.
+    drift_detector
+        Drift Detection method. Set to None to disable Drift detection.
+    warning_detector
+        Warning Detection method. Set to None to disable warning detection.
+    grace_period
+        [*Tree parameter*] Number of instances a leaf should observe between
+        split attempts.
+    max_depth
+        [*Tree parameter*] The maximum depth a tree can reach. If `None`, the
+        tree will grow indefinitely.
+    split_confidence
+        [*Tree parameter*] Allowed error in split decision, a value closer to 0
+        takes longer to decide.
+    tie_threshold
+        [*Tree parameter*] Threshold below which a split will be forced to break
+        ties.
+    leaf_prediction
+        [*Tree parameter*] Prediction mechanism used at leaves.</br>
+        - 'mean' - Target mean</br>
+        - 'model' - Uses the model defined in `leaf_model`</br>
+        - 'adaptive' - Chooses between 'mean' and 'model' dynamically</br>
+    leaf_model
+        [*Tree parameter*] The regression model used to provide responses if
+        `leaf_prediction='model'`. If not provided, an instance of
+        `river.linear_model.LinearRegression` with the default hyperparameters
+         is used.
+    model_selector_decay
+        [*Tree parameter*] The exponential decaying factor applied to the learning models'
+        squared errors, that are monitored if `leaf_prediction='adaptive'`. Must be
+        between `0` and `1`. The closer to `1`, the more importance is going to
+        be given to past observations. On the other hand, if its value
+        approaches `0`, the recent observed errors are going to have more
+        influence on the final decision.
+    nominal_attributes
+        [*Tree parameter*] List of Nominal attributes. If empty, then assume that
+        all attributes are numerical.
+    splitter
+        [*Tree parameter*] The Splitter or Attribute Observer (AO) used to monitor the class
+        statistics of numeric features and perform splits. Splitters are available in the
+        `tree.splitter` module. Different splitters are available for classification and
+        regression tasks. Classification and regression splitters can be distinguished by their
+        property `is_target_class`. This is an advanced option. Special care must be taken when
+        choosing different splitters.By default, `tree.splitter.EBSTSplitter` is used if
+        `splitter` is `None`.
+    min_samples_split
+        [*Tree parameter*] The minimum number of samples every branch resulting from a split
+        candidate must have to be considered valid.
+    binary_split
+        [*Tree parameter*] If True, only allow binary splits.
+    max_size
+        [*Tree parameter*] Maximum memory (MB) consumed by the tree.
+    memory_estimate_period
+        [*Tree parameter*] Number of instances between memory consumption checks.
+    stop_mem_management
+        [*Tree parameter*] If True, stop growing as soon as memory limit is hit.
+    remove_poor_attrs
+        [*Tree parameter*] If True, disable poor attributes to reduce memory usage.
+    merit_preprune
+        [*Tree parameter*] If True, enable merit-based tree pre-pruning.
+    seed
+        If `int`, `seed` is used to seed the random number generator;
+        If `RandomState`, `seed` is the random number generator;
+        If `None`, the random number generator is the `RandomState` instance
+        used by `np.random`.
+
+    References
+    ----------
+    [^1]: Gomes, H.M., Bifet, A., Read, J., Barddal, J.P., Enembreck, F.,
+          Pfharinger, B., Holmes, G. and Abdessalem, T., 2017. Adaptive random
+          forests for evolving data stream classification. Machine Learning,
+          106(9-10), pp.1469-1495.
+
+    [^2]: Gomes, H.M., Barddal, J.P., Boiko, L.E., Bifet, A., 2018.
+          Adaptive random forests for data stream regression. ESANN 2018.
+
+    Examples
+    --------
+    >>> from river import datasets
+    >>> from river import evaluate
+    >>> from river import metrics
+    >>> from river import ensemble
+    >>> from river import preprocessing
+
+    >>> dataset = datasets.TrumpApproval()
+
+    >>> model = (
+    ...     preprocessing.StandardScaler() |
+    ...     ensemble.AdaptiveRandomForestRegressor(n_models=3, seed=42)
+    ... )
+
+    >>> metric = metrics.MAE()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    MAE: 1.874094
+
+    """
+
+    _MEAN = "mean"
+    _MEDIAN = "median"
+    _VALID_AGGREGATION_METHOD = [_MEAN, _MEDIAN]
+
+    def __init__(
+        self,
+        # Forest parameters
+        n_models: int = 10,
+        max_features="sqrt",
+        aggregation_method: str = "median",
+        lambda_value: int = 6,
+        metric: metrics.RegressionMetric = metrics.MSE(),
+        disable_weighted_vote=True,
+        drift_detector: base.DriftDetector = ADWIN(0.001),
+        warning_detector: base.DriftDetector = ADWIN(0.01),
+        # Tree parameters
+        grace_period: int = 50,
+        max_depth: int = None,
+        split_confidence: float = 0.01,
+        tie_threshold: float = 0.05,
+        leaf_prediction: str = "model",
+        leaf_model: base.Regressor = None,
+        model_selector_decay: float = 0.95,
+        nominal_attributes: list = None,
+        splitter: Splitter = None,
+        min_samples_split: int = 5,
+        binary_split: bool = False,
+        max_size: int = 500,
+        memory_estimate_period: int = 2_000_000,
+        stop_mem_management: bool = False,
+        remove_poor_attrs: bool = False,
+        merit_preprune: bool = True,
+        seed: int = None,
+    ):
+        super().__init__(
+            n_models=n_models,
+            max_features=max_features,
+            lambda_value=lambda_value,
+            metric=metric,
+            disable_weighted_vote=disable_weighted_vote,
+            drift_detector=drift_detector,
+            warning_detector=warning_detector,
+            seed=seed,
+        )
+
+        self._n_samples_seen = 0
+        self._base_member_class = ForestMemberRegressor
+
+        # Tree parameters
+        self.grace_period = grace_period
+        self.max_depth = max_depth
+        self.split_confidence = split_confidence
+        self.tie_threshold = tie_threshold
+        self.leaf_prediction = leaf_prediction
+        self.leaf_model = leaf_model
+        self.model_selector_decay = model_selector_decay
+        self.nominal_attributes = nominal_attributes
+        self.splitter = splitter
+        self.min_samples_split = min_samples_split
+        self.binary_split = binary_split
+        self.max_size = max_size
+        self.memory_estimate_period = memory_estimate_period
+        self.stop_mem_management = stop_mem_management
+        self.remove_poor_attrs = remove_poor_attrs
+        self.merit_preprune = merit_preprune
+
+        if aggregation_method in self._VALID_AGGREGATION_METHOD:
+            self.aggregation_method = aggregation_method
+        else:
+            raise ValueError(
+                f"Invalid aggregation_method: {aggregation_method}.\n"
+                f"Valid values are: {self._VALID_AGGREGATION_METHOD}"
+            )
+
+    @classmethod
+    def _unit_test_params(cls):
+        yield {"n_models": 3}
+
+    def _unit_test_skips(self):
+        return {"check_shuffle_features_no_impact"}
+
+    def predict_one(self, x: dict) -> base.typing.RegTarget:
+
+        if not self.models:
+            self._init_ensemble(features=list(x.keys()))
+            return 0.0
+
+        y_pred = np.zeros(self.n_models)
+
+        if not self.disable_weighted_vote and self.aggregation_method != self._MEDIAN:
+            weights = np.zeros(self.n_models)
+            sum_weights = 0.0
+            for idx, model in enumerate(self.models):
+                y_pred[idx] = model.predict_one(x)
+                weights[idx] = model.metric.get()
+                sum_weights += weights[idx]
+
+            if sum_weights != 0:
+                # The higher the error, the worse is the tree
+                weights = sum_weights - weights
+                # Normalize weights to sum up to 1
+                weights /= weights.sum()
+                y_pred *= weights
+        else:
+            for idx, model in enumerate(self.models):
+                y_pred[idx] = model.predict_one(x)
+
+        if self.aggregation_method == self._MEAN:
+            y_pred = y_pred.mean()
+        else:
+            y_pred = np.median(y_pred)
+
+        return y_pred
+
+    def _new_base_model(self, seed: int):
+        return BaseTreeRegressor(
+            max_features=self.max_features,
+            grace_period=self.grace_period,
+            max_depth=self.max_depth,
+            split_confidence=self.split_confidence,
+            tie_threshold=self.tie_threshold,
+            leaf_prediction=self.leaf_prediction,
+            leaf_model=self.leaf_model,
+            model_selector_decay=self.model_selector_decay,
+            nominal_attributes=self.nominal_attributes,
+            splitter=self.splitter,
+            binary_split=self.binary_split,
+            max_size=self.max_size,
+            memory_estimate_period=self.memory_estimate_period,
+            stop_mem_management=self.stop_mem_management,
+            remove_poor_attrs=self.remove_poor_attrs,
+            merit_preprune=self.merit_preprune,
+            seed=seed,
+        )
+
+    @property
+    def valid_aggregation_method(self):
+        """Valid aggregation_method values."""
+        return self._VALID_AGGREGATION_METHOD
+
+
+class BaseForestMember:
+    """Base forest member class.
+
+    This class represents a tree member of the forest. It includes a
+    base tree model, the background learner, drift detectors and performance
+    tracking parameters.
+
+    The main purpose of this class is to train the foreground model.
+    Optionally, it monitors drift detection. Depending on the configuration,
+    if drift is detected then the foreground model is reset or replaced by a
+    background model.
+
+    Parameters
+    ----------
+    index_original
+        Tree index within the ensemble.
+    model
+        Tree learner.
+    created_on
+        Number of instances seen by the tree.
+    drift_detector
+        Drift Detection method.
+    warning_detector
+        Warning Detection method.
+    is_background_learner
+        True if the tree is a background learner.
+    metric
+        Metric to track performance.
+
+    """
+
+    def __init__(
+        self,
+        index_original: int,
+        model: typing.Union[BaseTreeClassifier, BaseTreeRegressor],
+        created_on: int,
+        drift_detector: base.DriftDetector,
+        warning_detector: base.DriftDetector,
+        is_background_learner,
+        metric: typing.Union[metrics.MultiClassMetric, metrics.RegressionMetric],
+    ):
+        self.index_original = index_original
+        self.model = model.clone()
+        self.created_on = created_on
+        self.is_background_learner = is_background_learner
+        self.metric = copy.deepcopy(metric)
+        # Make sure that the metric is not initialized, e.g. when creating background learners.
+        if isinstance(self.metric, metrics.MultiClassMetric):
+            self.metric.cm.reset()
+        # Keep a copy of the original metric for background learners or reset
+        self._original_metric = copy.deepcopy(metric)
+
+        self.background_learner = None
+
+        # Drift and warning detection
+        self.last_drift_on = 0
+        self.last_warning_on = 0
+        self.n_drifts_detected = 0
+        self.n_warnings_detected = 0
+
+        # Initialize drift and warning detectors
+        if drift_detector is not None:
+            self._use_drift_detector = True
+            self.drift_detector = drift_detector.clone()
+        else:
+            self._use_drift_detector = False
+            self.drift_detector = None
+
+        if warning_detector is not None:
+            self._use_background_learner = True
+            self.warning_detector = warning_detector.clone()
+        else:
+            self._use_background_learner = False
+            self.warning_detector = None
+
+    def reset(self, n_samples_seen):
+        if self._use_background_learner and self.background_learner is not None:
+            # Replace foreground model with background model
+            self.model = self.background_learner.model
+            self.warning_detector = self.background_learner.warning_detector
+            self.drift_detector = self.background_learner.drift_detector
+            self.metric = self.background_learner.metric
+            self.created_on = self.background_learner.created_on
+            self.background_learner = None
+        else:
+            # Reset model
+            self.model = self.model.clone()
+            self.metric = copy.deepcopy(self._original_metric)
+            self.created_on = n_samples_seen
+            self.drift_detector = self.drift_detector.clone()
+        # Make sure that the metric is not initialized, e.g. when creating background learners.
+        if isinstance(self.metric, metrics.MultiClassMetric):
+            self.metric.cm.reset()
+
+    def learn_one(
+        self, x: dict, y: base.typing.Target, *, sample_weight: int, n_samples_seen: int
+    ):
+
+        self.model.learn_one(x, y, sample_weight=sample_weight)
+
+        if self.background_learner:
+            # Train the background learner
+            self.background_learner.model.learn_one(
+                x=x, y=y, sample_weight=sample_weight
+            )
+
+        if self._use_drift_detector and not self.is_background_learner:
+            drift_detector_input = self._drift_detector_input(
+                y_true=y, y_pred=self.model.predict_one(x)
+            )
+
+            # Check for warning only if use_background_learner is set
+            if self._use_background_learner:
+                self.warning_detector.update(drift_detector_input)
+                # Check if there was a (warning) change
+                if self.warning_detector.change_detected:
+                    self.last_warning_on = n_samples_seen
+                    self.n_warnings_detected += 1
+                    # Create a new background learner object
+                    self.background_learner = self.__class__(
+                        index_original=self.index_original,
+                        model=self.model.new_instance(),
+                        created_on=n_samples_seen,
+                        drift_detector=self.drift_detector,
+                        warning_detector=self.warning_detector,
+                        is_background_learner=True,
+                        metric=self.metric,
+                    )
+                    # Reset the warning detector for the current object
+                    self.warning_detector = self.warning_detector.clone()
+
+            # Update the drift detector
+            self.drift_detector.update(drift_detector_input)
+
+            # Check if there was a change
+            if self.drift_detector.change_detected:
+                self.last_drift_on = n_samples_seen
+                self.n_drifts_detected += 1
+                self.reset(n_samples_seen)
+
+    @abc.abstractmethod
+    def _drift_detector_input(
+        self,
+        y_true: typing.Union[base.typing.ClfTarget, base.typing.RegTarget],
+        y_pred: typing.Union[base.typing.ClfTarget, base.typing.RegTarget],
+    ):
+        raise NotImplementedError
+
+
+class ForestMemberClassifier(BaseForestMember, base.Classifier):
+    """Forest member class for classification"""
+
+    def __init__(
+        self,
+        index_original: int,
+        model: BaseTreeClassifier,
+        created_on: int,
+        drift_detector: base.DriftDetector,
+        warning_detector: base.DriftDetector,
+        is_background_learner,
+        metric: metrics.MultiClassMetric,
+    ):
+        super().__init__(
+            index_original=index_original,
+            model=model,
+            created_on=created_on,
+            drift_detector=drift_detector,
+            warning_detector=warning_detector,
+            is_background_learner=is_background_learner,
+            metric=metric,
+        )
+
+    def _drift_detector_input(
+        self, y_true: base.typing.ClfTarget, y_pred: base.typing.ClfTarget
+    ):
+        return int(not y_true == y_pred)  # Not correctly_classifies
+
+    def predict_one(self, x):
+        return self.model.predict_one(x)
+
+    def predict_proba_one(self, x):
+        return self.model.predict_proba_one(x)
+
+
+class ForestMemberRegressor(BaseForestMember, base.Regressor):
+    """Forest member class for regression"""
+
+    def __init__(
+        self,
+        index_original: int,
+        model: BaseTreeRegressor,
+        created_on: int,
+        drift_detector: base.DriftDetector,
+        warning_detector: base.DriftDetector,
+        is_background_learner,
+        metric: metrics.RegressionMetric,
+    ):
+        super().__init__(
+            index_original=index_original,
+            model=model,
+            created_on=created_on,
+            drift_detector=drift_detector,
+            warning_detector=warning_detector,
+            is_background_learner=is_background_learner,
+            metric=metric,
+        )
+        self._var = stats.Var()  # Used to track drift
+
+    def _drift_detector_input(self, y_true: float, y_pred: float):
+        drift_input = y_true - y_pred
+        self._var.update(drift_input)
+
+        if self._var.mean.n == 1:
+            return 0.5  # The expected error is the normalized mean error
+
+        sd = math.sqrt(self._var.get())
+
+        # We assume the error follows a normal distribution -> (empirical rule)
+        # 99.73% of the values lie  between [mean - 3*sd, mean + 3*sd]. We
+        # assume this range for the normalized data. Hence, we can apply the
+        # min-max norm to cope with  ADWIN's requirements
+        return (drift_input + 3 * sd) / (6 * sd) if sd > 0 else 0.5
+
+    def reset(self, n_samples_seen):
+        super().reset(n_samples_seen)
+        # Reset the stats for the drift detector
+        self._var = stats.Var()
+
+    def predict_one(self, x):
+        return self.model.predict_one(x)
```

### Comparing `river-0.8.0/river/ensemble/bagging.py` & `river-0.9.0/river/ensemble/bagging.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,435 +1,423 @@
-import collections
-import copy
-import statistics
-
-import numpy as np
-
-from river import base, linear_model
-from river.drift import ADWIN
-
-__all__ = [
-    "BaggingClassifier",
-    "BaggingRegressor",
-    "ADWINBaggingClassifier",
-    "LeveragingBaggingClassifier",
-]
-
-
-class BaseBagging(base.WrapperMixin, base.EnsembleMixin):
-    def __init__(self, model, n_models=10, seed=None):
-        super().__init__(copy.deepcopy(model) for _ in range(n_models))
-        self.n_models = n_models
-        self.model = model
-        self.seed = seed
-        self._rng = np.random.RandomState(seed)
-
-    @property
-    def _wrapped_model(self):
-        return self.model
-
-    def learn_one(self, x, y):
-
-        for model in self:
-            for _ in range(self._rng.poisson(1)):
-                model.learn_one(x, y)
-
-        return self
-
-
-class BaggingClassifier(BaseBagging, base.Classifier):
-    """Online bootstrap aggregation for classification.
-
-    For each incoming observation, each model's `learn_one` method is called `k` times where
-    `k` is sampled from a Poisson distribution of parameter 1. `k` thus has a 36% chance of
-    being equal to 0, a 36% chance of being equal to 1, an 18% chance of being equal to 2, a 6%
-    chance of being equal to 3, a 1% chance of being equal to 4, etc. You can do
-    `scipy.stats.poisson(1).pmf(k)` to obtain more detailed values.
-
-    Parameters
-    ----------
-    model
-        The classifier to bag.
-    n_models
-        The number of models in the ensemble.
-    seed
-        Random number generator seed for reproducibility.
-
-    Examples
-    --------
-
-    In the following example three logistic regressions are bagged together. The performance is
-    slightly better than when using a single logistic regression.
-
-    >>> from river import datasets
-    >>> from river import ensemble
-    >>> from river import evaluate
-    >>> from river import linear_model
-    >>> from river import metrics
-    >>> from river import optim
-    >>> from river import preprocessing
-
-    >>> dataset = datasets.Phishing()
-
-    >>> model = ensemble.BaggingClassifier(
-    ...     model=(
-    ...         preprocessing.StandardScaler() |
-    ...         linear_model.LogisticRegression()
-    ...     ),
-    ...     n_models=3,
-    ...     seed=42
-    ... )
-
-    >>> metric = metrics.F1()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    F1: 0.877788
-
-    >>> print(model)
-    BaggingClassifier(StandardScaler | LogisticRegression)
-
-    References
-    ----------
-    [^1]: [Oza, N.C., 2005, October. Online bagging and boosting. In 2005 IEEE international conference on systems, man and cybernetics (Vol. 3, pp. 2340-2345). Ieee.](https://ti.arc.nasa.gov/m/profile/oza/files/ozru01a.pdf)
-
-    """
-
-    def __init__(self, model: base.Classifier, n_models=10, seed: int = None):
-        super().__init__(model, n_models, seed)
-
-    @classmethod
-    def _unit_test_params(cls):
-        return {"model": linear_model.LogisticRegression()}
-
-    def predict_proba_one(self, x):
-        """Averages the predictions of each classifier."""
-
-        y_pred = collections.Counter()
-        for classifier in self:
-            y_pred.update(classifier.predict_proba_one(x))
-
-        total = sum(y_pred.values())
-        if total > 0:
-            return {label: proba / total for label, proba in y_pred.items()}
-        return y_pred
-
-
-class BaggingRegressor(BaseBagging, base.Regressor):
-    """Online bootstrap aggregation for regression.
-
-    For each incoming observation, each model's `learn_one` method is called `k` times where
-    `k` is sampled from a Poisson distribution of parameter 1. `k` thus has a 36% chance of
-    being equal to 0, a 36% chance of being equal to 1, an 18% chance of being equal to 2, a 6%
-    chance of being equal to 3, a 1% chance of being equal to 4, etc. You can do
-    `scipy.stats.poisson(1).pmf(k)` for more detailed values.
-
-    Parameters
-    ----------
-    model
-        The regressor to bag.
-    n_models
-        The number of models in the ensemble.
-    seed
-        Random number generator seed for reproducibility.
-
-    Examples
-    --------
-
-    In the following example three logistic regressions are bagged together. The performance is
-    slightly better than when using a single logistic regression.
-
-    >>> from river import datasets
-    >>> from river import ensemble
-    >>> from river import evaluate
-    >>> from river import linear_model
-    >>> from river import metrics
-    >>> from river import optim
-    >>> from river import preprocessing
-
-    >>> dataset = datasets.TrumpApproval()
-
-    >>> model = preprocessing.StandardScaler()
-    >>> model |= ensemble.BaggingRegressor(
-    ...     model=linear_model.LinearRegression(intercept_lr=0.1),
-    ...     n_models=3,
-    ...     seed=42
-    ... )
-
-    >>> metric = metrics.MAE()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    MAE: 0.641799
-
-    References
-    ----------
-    [^1]: [Oza, N.C., 2005, October. Online bagging and boosting. In 2005 IEEE international conference on systems, man and cybernetics (Vol. 3, pp. 2340-2345). Ieee.](https://ti.arc.nasa.gov/m/profile/oza/files/ozru01a.pdf)
-
-    """
-
-    def __init__(self, model: base.Regressor, n_models=10, seed: int = None):
-        super().__init__(model, n_models, seed)
-
-    @classmethod
-    def _unit_test_params(cls):
-        return {"model": linear_model.LinearRegression()}
-
-    def predict_one(self, x):
-        """Averages the predictions of each regressor."""
-        return statistics.mean((regressor.predict_one(x) for regressor in self))
-
-
-class ADWINBaggingClassifier(BaggingClassifier):
-    """ADWIN Bagging classifier.
-
-    ADWIN Bagging [^1] is the online bagging method of Oza and Russell [^2]
-    with the addition of the `ADWIN` algorithm as a change detector. If concept
-    drift is detected, the worst member of the ensemble (based on the error
-    estimation by ADWIN) is replaced by a new (empty) classifier.
-
-    Parameters
-    ----------
-    model
-        The classifier to bag.
-    n_models
-        The number of models in the ensemble.
-    seed
-        Random number generator seed for reproducibility.
-
-    Examples
-    --------
-    >>> from river import datasets
-    >>> from river import ensemble
-    >>> from river import evaluate
-    >>> from river import linear_model
-    >>> from river import metrics
-    >>> from river import optim
-    >>> from river import preprocessing
-
-    >>> dataset = datasets.Phishing()
-
-    >>> model = ensemble.ADWINBaggingClassifier(
-    ...     model=(
-    ...         preprocessing.StandardScaler() |
-    ...         linear_model.LogisticRegression()
-    ...     ),
-    ...     n_models=3,
-    ...     seed=42
-    ... )
-
-    >>> metric = metrics.F1()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    F1: 0.878788
-
-    References
-    ----------
-    [^1]: Albert Bifet, Geoff Holmes, Bernhard Pfahringer, Richard Kirkby,
-    and Ricard Gavaldà. "New ensemble methods for evolving data streams."
-    In 15th ACM SIGKDD International Conference on Knowledge Discovery and
-    Data Mining, 2009.
-
-    [^2]: Oza, N., Russell, S. "Online bagging and boosting."
-    In: Artificial Intelligence and Statistics 2001, pp. 105–112.
-    Morgan Kaufmann, 2001.
-
-    """
-
-    def __init__(self, model: base.Classifier, n_models=10, seed: int = None):
-        super().__init__(model=model, n_models=n_models, seed=seed)
-        self._drift_detectors = [copy.deepcopy(ADWIN()) for _ in range(self.n_models)]
-
-    def learn_one(self, x, y):
-
-        change_detected = False
-        for i, model in enumerate(self):
-            for _ in range(self._rng.poisson(1)):
-                model.learn_one(x, y)
-
-            try:
-                y_pred = model.predict_one(x)
-                error_estimation = self._drift_detectors[i].estimation
-                self._drift_detectors[i].update(int(y_pred == y))
-                if self._drift_detectors[i].change_detected:
-                    if self._drift_detectors[i].estimation > error_estimation:
-                        change_detected = True
-            except ValueError:
-                change_detected = False
-
-        if change_detected:
-            max_error_idx = max(
-                range(len(self._drift_detectors)),
-                key=lambda j: self._drift_detectors[j].estimation,
-            )
-            self.models[max_error_idx] = copy.deepcopy(self.model)
-            self._drift_detectors[max_error_idx] = ADWIN()
-
-        return self
-
-
-class LeveragingBaggingClassifier(BaggingClassifier):
-    """Leveraging Bagging ensemble classifier.
-
-    Leveraging Bagging [^1] is an improvement over the Oza Bagging algorithm.
-    The bagging performance is leveraged by increasing the re-sampling.
-    It uses a poisson distribution to simulate the re-sampling process.
-    To increase re-sampling it uses a higher `w` value of the Poisson
-    distribution (agerage number of events), 6 by default, increasing the
-    input space diversity, by attributing a different range of weights to the
-    data samples.
-
-    To deal with concept drift, Leveraging Bagging uses the ADWIN algorithm to
-    monitor the performance of each member of the enemble If concept drift is
-    detected, the worst member of the ensemble (based on the error estimation
-    by ADWIN) is replaced by a new (empty) classifier.
-
-    Parameters
-    ----------
-    model
-        The classifier to bag.
-    n_models
-        The number of models in the ensemble.
-    w
-        Indicates the average number of events. This is the lambda parameter
-        of the Poisson distribution used to compute the re-sampling weight.
-    adwin_delta
-        The delta parameter for the ADWIN change detector.
-    bagging_method
-        The bagging method to use. Can be one of the following:<br/>
-        * 'bag' - Leveraging Bagging using ADWIN.<br/>
-        * 'me' - Assigns $weight=1$ if sample is misclassified,
-          otherwise $weight=error/(1-error)$.<br/>
-        * 'half' - Use resampling without replacement for half of the instances.<br/>
-        * 'wt' - Resample without taking out all instances.<br/>
-        * 'subag' - Resampling without replacement.<br/>
-    seed
-        Random number generator seed for reproducibility.
-
-    Examples
-    --------
-
-    >>> from river import datasets
-    >>> from river import ensemble
-    >>> from river import evaluate
-    >>> from river import linear_model
-    >>> from river import metrics
-    >>> from river import optim
-    >>> from river import preprocessing
-
-    >>> dataset = datasets.Phishing()
-
-    >>> model = ensemble.LeveragingBaggingClassifier(
-    ...     model=(
-    ...         preprocessing.StandardScaler() |
-    ...         linear_model.LogisticRegression()
-    ...     ),
-    ...     n_models=3,
-    ...     seed=42
-    ... )
-
-    >>> metric = metrics.F1()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    F1: 0.886282
-
-    """
-
-    _BAGGING_METHODS = ("bag", "me", "half", "wt", "subag")
-
-    def __init__(
-        self,
-        model: base.Classifier,
-        n_models: int = 10,
-        w: float = 6,
-        adwin_delta: float = 0.002,
-        bagging_method: str = "bag",
-        seed: int = None,
-    ):
-        super().__init__(model=model, n_models=n_models, seed=seed)
-        self.n_detected_changes = 0
-        self.w = w
-        self.adwin_delta = adwin_delta
-        self.bagging_method = bagging_method
-        self._drift_detectors = [
-            copy.deepcopy(ADWIN(delta=self.adwin_delta)) for _ in range(self.n_models)
-        ]
-
-        # Set bagging function
-        if bagging_method == "bag":
-            self._bagging_fct = self._leveraging_bag
-        elif bagging_method == "me":
-            self._bagging_fct = self._leveraging_bag_me
-        elif bagging_method == "half":
-            self._bagging_fct = self._leveraging_bag_half
-        elif bagging_method == "wt":
-            self._bagging_fct = self._leveraging_bag_wt
-        elif bagging_method == "subag":
-            self._bagging_fct = self._leveraging_subag
-        else:
-            raise ValueError(
-                f"Invalid bagging_method: {bagging_method}\n"
-                f"Valid options: {self._BAGGING_METHODS}"
-            )
-
-    def _leveraging_bag(self, **kwargs):
-        # Leveraging bagging
-        return self._rng.poisson(self.w)
-
-    def _leveraging_bag_me(self, **kwargs):
-        # Miss-classification error using weight=1 if misclassified.
-        # Otherwise using weight=error/(1-error)
-        x = kwargs["x"]
-        y = kwargs["y"]
-        i = kwargs["model_idx"]
-        error = self._drift_detectors[i].estimation
-        y_pred = self.models[i].predict_one(x)
-        if y_pred != y:
-            k = 1
-        elif error != 1.0 and self._rng.rand() < (error / (1.0 - error)):
-            k = 1
-        else:
-            k = 0
-        return k
-
-    def _leveraging_bag_half(self, **kwargs):
-        # Resampling without replacement for half of the instances
-        return int(not self._rng.randint(2))
-
-    def _leveraging_bag_wt(self, **kwargs):
-        # Resampling without taking out all instances
-        return 1 + self._rng.poisson(1.0)
-
-    def _leveraging_subag(self, **kwargs):
-        # Subagging using resampling without replacement
-        return int(self._rng.poisson(1) > 0)
-
-    def learn_one(self, x, y):
-        change_detected = False
-        for i, model in enumerate(self):
-            k = self._bagging_fct(x=x, y=y, model_idx=i)
-
-            for _ in range(k):
-                model.learn_one(x, y)
-
-            y_pred = self.models[i].predict_one(x)
-            if y_pred is not None:
-                incorrectly_classifies = int(y_pred != y)
-                error = self._drift_detectors[i].estimation
-                self._drift_detectors[i].update(incorrectly_classifies)
-                if self._drift_detectors[i].change_detected:
-                    if self._drift_detectors[i].estimation > error:
-                        change_detected = True
-
-        if change_detected:
-            self.n_detected_changes += 1
-            max_error_idx = max(
-                range(len(self._drift_detectors)),
-                key=lambda j: self._drift_detectors[j].estimation,
-            )
-            self.models[max_error_idx] = copy.deepcopy(self.model)
-            self._drift_detectors[max_error_idx] = ADWIN(delta=self.adwin_delta)
-
-        return self
-
-    @property
-    def bagging_methods(self):
-        """Valid bagging_method options."""
-        return self._BAGGING_METHODS
+import collections
+import copy
+import statistics
+
+from river import base, linear_model
+from river.drift import ADWIN
+from river.utils import poisson
+
+__all__ = [
+    "BaggingClassifier",
+    "BaggingRegressor",
+    "ADWINBaggingClassifier",
+    "LeveragingBaggingClassifier",
+]
+
+
+class BaseBagging(base.WrapperEnsemble):
+    def learn_one(self, x, y):
+
+        for model in self:
+            for _ in range(poisson(1, self._rng)):
+                model.learn_one(x, y)
+
+        return self
+
+
+class BaggingClassifier(BaseBagging, base.Classifier):
+    """Online bootstrap aggregation for classification.
+
+    For each incoming observation, each model's `learn_one` method is called `k` times where
+    `k` is sampled from a Poisson distribution of parameter 1. `k` thus has a 36% chance of
+    being equal to 0, a 36% chance of being equal to 1, an 18% chance of being equal to 2, a 6%
+    chance of being equal to 3, a 1% chance of being equal to 4, etc. You can do
+    `scipy.stats.poisson(1).pmf(k)` to obtain more detailed values.
+
+    Parameters
+    ----------
+    model
+        The classifier to bag.
+    n_models
+        The number of models in the ensemble.
+    seed
+        Random number generator seed for reproducibility.
+
+    Examples
+    --------
+
+    In the following example three logistic regressions are bagged together. The performance is
+    slightly better than when using a single logistic regression.
+
+    >>> from river import datasets
+    >>> from river import ensemble
+    >>> from river import evaluate
+    >>> from river import linear_model
+    >>> from river import metrics
+    >>> from river import optim
+    >>> from river import preprocessing
+
+    >>> dataset = datasets.Phishing()
+
+    >>> model = ensemble.BaggingClassifier(
+    ...     model=(
+    ...         preprocessing.StandardScaler() |
+    ...         linear_model.LogisticRegression()
+    ...     ),
+    ...     n_models=3,
+    ...     seed=42
+    ... )
+
+    >>> metric = metrics.F1()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    F1: 0.878269
+
+    >>> print(model)
+    BaggingClassifier(StandardScaler | LogisticRegression)
+
+    References
+    ----------
+    [^1]: [Oza, N.C., 2005, October. Online bagging and boosting. In 2005 IEEE international conference on systems, man and cybernetics (Vol. 3, pp. 2340-2345). Ieee.](https://ti.arc.nasa.gov/m/profile/oza/files/ozru01a.pdf)
+
+    """
+
+    def __init__(self, model: base.Classifier, n_models=10, seed: int = None):
+        super().__init__(model, n_models, seed)
+
+    @classmethod
+    def _unit_test_params(cls):
+        yield {"model": linear_model.LogisticRegression()}
+
+    def predict_proba_one(self, x):
+        """Averages the predictions of each classifier."""
+
+        y_pred = collections.Counter()
+        for classifier in self:
+            y_pred.update(classifier.predict_proba_one(x))
+
+        total = sum(y_pred.values())
+        if total > 0:
+            return {label: proba / total for label, proba in y_pred.items()}
+        return y_pred
+
+
+class BaggingRegressor(BaseBagging, base.Regressor):
+    """Online bootstrap aggregation for regression.
+
+    For each incoming observation, each model's `learn_one` method is called `k` times where
+    `k` is sampled from a Poisson distribution of parameter 1. `k` thus has a 36% chance of
+    being equal to 0, a 36% chance of being equal to 1, an 18% chance of being equal to 2, a 6%
+    chance of being equal to 3, a 1% chance of being equal to 4, etc. You can do
+    `scipy.stats.poisson(1).pmf(k)` for more detailed values.
+
+    Parameters
+    ----------
+    model
+        The regressor to bag.
+    n_models
+        The number of models in the ensemble.
+    seed
+        Random number generator seed for reproducibility.
+
+    Examples
+    --------
+
+    In the following example three logistic regressions are bagged together. The performance is
+    slightly better than when using a single logistic regression.
+
+    >>> from river import datasets
+    >>> from river import ensemble
+    >>> from river import evaluate
+    >>> from river import linear_model
+    >>> from river import metrics
+    >>> from river import optim
+    >>> from river import preprocessing
+
+    >>> dataset = datasets.TrumpApproval()
+
+    >>> model = preprocessing.StandardScaler()
+    >>> model |= ensemble.BaggingRegressor(
+    ...     model=linear_model.LinearRegression(intercept_lr=0.1),
+    ...     n_models=3,
+    ...     seed=42
+    ... )
+
+    >>> metric = metrics.MAE()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    MAE: 0.68886
+
+    References
+    ----------
+    [^1]: [Oza, N.C., 2005, October. Online bagging and boosting. In 2005 IEEE international conference on systems, man and cybernetics (Vol. 3, pp. 2340-2345). Ieee.](https://ti.arc.nasa.gov/m/profile/oza/files/ozru01a.pdf)
+
+    """
+
+    def __init__(self, model: base.Regressor, n_models=10, seed: int = None):
+        super().__init__(model, n_models, seed)
+
+    @classmethod
+    def _unit_test_params(cls):
+        yield {"model": linear_model.LinearRegression()}
+
+    def predict_one(self, x):
+        """Averages the predictions of each regressor."""
+        return statistics.mean((regressor.predict_one(x) for regressor in self))
+
+
+class ADWINBaggingClassifier(BaggingClassifier):
+    """ADWIN Bagging classifier.
+
+    ADWIN Bagging [^1] is the online bagging method of Oza and Russell [^2]
+    with the addition of the `ADWIN` algorithm as a change detector. If concept
+    drift is detected, the worst member of the ensemble (based on the error
+    estimation by ADWIN) is replaced by a new (empty) classifier.
+
+    Parameters
+    ----------
+    model
+        The classifier to bag.
+    n_models
+        The number of models in the ensemble.
+    seed
+        Random number generator seed for reproducibility.
+
+    Examples
+    --------
+    >>> from river import datasets
+    >>> from river import ensemble
+    >>> from river import evaluate
+    >>> from river import linear_model
+    >>> from river import metrics
+    >>> from river import optim
+    >>> from river import preprocessing
+
+    >>> dataset = datasets.Phishing()
+
+    >>> model = ensemble.ADWINBaggingClassifier(
+    ...     model=(
+    ...         preprocessing.StandardScaler() |
+    ...         linear_model.LogisticRegression()
+    ...     ),
+    ...     n_models=3,
+    ...     seed=42
+    ... )
+
+    >>> metric = metrics.F1()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    F1: 0.878269
+
+    References
+    ----------
+    [^1]: Albert Bifet, Geoff Holmes, Bernhard Pfahringer, Richard Kirkby,
+    and Ricard Gavaldà. "New ensemble methods for evolving data streams."
+    In 15th ACM SIGKDD International Conference on Knowledge Discovery and
+    Data Mining, 2009.
+
+    [^2]: Oza, N., Russell, S. "Online bagging and boosting."
+    In: Artificial Intelligence and Statistics 2001, pp. 105–112.
+    Morgan Kaufmann, 2001.
+
+    """
+
+    def __init__(self, model: base.Classifier, n_models=10, seed: int = None):
+        super().__init__(model=model, n_models=n_models, seed=seed)
+        self._drift_detectors = [copy.deepcopy(ADWIN()) for _ in range(self.n_models)]
+
+    def learn_one(self, x, y):
+
+        change_detected = False
+        for i, model in enumerate(self):
+            for _ in range(poisson(1, self._rng)):
+                model.learn_one(x, y)
+
+            try:
+                y_pred = model.predict_one(x)
+                error_estimation = self._drift_detectors[i].estimation
+                self._drift_detectors[i].update(int(y_pred == y))
+                if self._drift_detectors[i].change_detected:
+                    if self._drift_detectors[i].estimation > error_estimation:
+                        change_detected = True
+            except ValueError:
+                change_detected = False
+
+        if change_detected:
+            max_error_idx = max(
+                range(len(self._drift_detectors)),
+                key=lambda j: self._drift_detectors[j].estimation,
+            )
+            self.models[max_error_idx] = copy.deepcopy(self.model)
+            self._drift_detectors[max_error_idx] = ADWIN()
+
+        return self
+
+
+class LeveragingBaggingClassifier(BaggingClassifier):
+    """Leveraging Bagging ensemble classifier.
+
+    Leveraging Bagging [^1] is an improvement over the Oza Bagging algorithm.
+    The bagging performance is leveraged by increasing the re-sampling.
+    It uses a poisson distribution to simulate the re-sampling process.
+    To increase re-sampling it uses a higher `w` value of the Poisson
+    distribution (agerage number of events), 6 by default, increasing the
+    input space diversity, by attributing a different range of weights to the
+    data samples.
+
+    To deal with concept drift, Leveraging Bagging uses the ADWIN algorithm to
+    monitor the performance of each member of the enemble If concept drift is
+    detected, the worst member of the ensemble (based on the error estimation
+    by ADWIN) is replaced by a new (empty) classifier.
+
+    Parameters
+    ----------
+    model
+        The classifier to bag.
+    n_models
+        The number of models in the ensemble.
+    w
+        Indicates the average number of events. This is the lambda parameter
+        of the Poisson distribution used to compute the re-sampling weight.
+    adwin_delta
+        The delta parameter for the ADWIN change detector.
+    bagging_method
+        The bagging method to use. Can be one of the following:<br/>
+        * 'bag' - Leveraging Bagging using ADWIN.<br/>
+        * 'me' - Assigns $weight=1$ if sample is misclassified,
+          otherwise $weight=error/(1-error)$.<br/>
+        * 'half' - Use resampling without replacement for half of the instances.<br/>
+        * 'wt' - Resample without taking out all instances.<br/>
+        * 'subag' - Resampling without replacement.<br/>
+    seed
+        Random number generator seed for reproducibility.
+
+    Examples
+    --------
+
+    >>> from river import datasets
+    >>> from river import ensemble
+    >>> from river import evaluate
+    >>> from river import linear_model
+    >>> from river import metrics
+    >>> from river import optim
+    >>> from river import preprocessing
+
+    >>> dataset = datasets.Phishing()
+
+    >>> model = ensemble.LeveragingBaggingClassifier(
+    ...     model=(
+    ...         preprocessing.StandardScaler() |
+    ...         linear_model.LogisticRegression()
+    ...     ),
+    ...     n_models=3,
+    ...     seed=42
+    ... )
+
+    >>> metric = metrics.F1()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    F1: 0.887286
+
+    """
+
+    _BAGGING_METHODS = ("bag", "me", "half", "wt", "subag")
+
+    def __init__(
+        self,
+        model: base.Classifier,
+        n_models: int = 10,
+        w: float = 6,
+        adwin_delta: float = 0.002,
+        bagging_method: str = "bag",
+        seed: int = None,
+    ):
+        super().__init__(model=model, n_models=n_models, seed=seed)
+        self.n_detected_changes = 0
+        self.w = w
+        self.adwin_delta = adwin_delta
+        self.bagging_method = bagging_method
+        self._drift_detectors = [
+            copy.deepcopy(ADWIN(delta=self.adwin_delta)) for _ in range(self.n_models)
+        ]
+
+        # Set bagging function
+        if bagging_method == "bag":
+            self._bagging_fct = self._leveraging_bag
+        elif bagging_method == "me":
+            self._bagging_fct = self._leveraging_bag_me
+        elif bagging_method == "half":
+            self._bagging_fct = self._leveraging_bag_half
+        elif bagging_method == "wt":
+            self._bagging_fct = self._leveraging_bag_wt
+        elif bagging_method == "subag":
+            self._bagging_fct = self._leveraging_subag
+        else:
+            raise ValueError(
+                f"Invalid bagging_method: {bagging_method}\n"
+                f"Valid options: {self._BAGGING_METHODS}"
+            )
+
+    def _leveraging_bag(self, **kwargs):
+        # Leveraging bagging
+        return poisson(self.w, self._rng)
+
+    def _leveraging_bag_me(self, **kwargs):
+        # Miss-classification error using weight=1 if misclassified.
+        # Otherwise using weight=error/(1-error)
+        x = kwargs["x"]
+        y = kwargs["y"]
+        i = kwargs["model_idx"]
+        error = self._drift_detectors[i].estimation
+        y_pred = self.models[i].predict_one(x)
+        if y_pred != y:
+            k = 1
+        elif error != 1.0 and self._rng.rand() < (error / (1.0 - error)):
+            k = 1
+        else:
+            k = 0
+        return k
+
+    def _leveraging_bag_half(self, **kwargs):
+        # Resampling without replacement for half of the instances
+        return int(not self._rng.randint(2))
+
+    def _leveraging_bag_wt(self, **kwargs):
+        # Resampling without taking out all instances
+        return 1 + self._rng.poisson(1.0)
+
+    def _leveraging_subag(self, **kwargs):
+        # Subagging using resampling without replacement
+        return int(self._rng.poisson(1) > 0)
+
+    def learn_one(self, x, y):
+        change_detected = False
+        for i, model in enumerate(self):
+            k = self._bagging_fct(x=x, y=y, model_idx=i)
+
+            for _ in range(k):
+                model.learn_one(x, y)
+
+            y_pred = self[i].predict_one(x)
+            if y_pred is not None:
+                incorrectly_classifies = int(y_pred != y)
+                error = self._drift_detectors[i].estimation
+                self._drift_detectors[i].update(incorrectly_classifies)
+                if self._drift_detectors[i].change_detected:
+                    if self._drift_detectors[i].estimation > error:
+                        change_detected = True
+
+        if change_detected:
+            self.n_detected_changes += 1
+            max_error_idx = max(
+                range(len(self._drift_detectors)),
+                key=lambda j: self._drift_detectors[j].estimation,
+            )
+            self[max_error_idx] = copy.deepcopy(self.model)
+            self._drift_detectors[max_error_idx] = ADWIN(delta=self.adwin_delta)
+
+        return self
+
+    @property
+    def bagging_methods(self):
+        """Valid bagging_method options."""
+        return self._BAGGING_METHODS
```

### Comparing `river-0.8.0/river/ensemble/boosting.py` & `river-0.9.0/river/ensemble/boosting.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,130 +1,120 @@
-import collections
-import copy
-import math
-
-import numpy as np
-
-from river import base, linear_model
-from river.utils.skmultiflow_utils import normalize_values_in_dict, scale_values_in_dict
-
-__all__ = ["AdaBoostClassifier"]
-
-
-class AdaBoostClassifier(base.WrapperMixin, base.EnsembleMixin, base.Classifier):
-    """Boosting for classification
-
-    For each incoming observation, each model's `learn_one` method is called `k` times where
-    `k` is sampled from a Poisson distribution of parameter lambda. The lambda parameter is
-    updated when the weaks learners fit successively the same observation.
-
-    Parameters
-    ----------
-    model
-        The classifier to boost.
-    n_models
-        The number of models in the ensemble.
-    seed
-        Random number generator seed for reproducibility.
-
-    Attributes
-    ----------
-    wrong_weight : collections.defaultdict
-        Number of times a model has made a mistake when making predictions.
-    correct_weight : collections.defaultdict
-        Number of times a model has predicted the right label when making predictions.
-
-    Examples
-    --------
-
-    In the following example three tree classifiers are boosted together. The performance is
-    slightly better than when using a single tree.
-
-    >>> from river import datasets
-    >>> from river import ensemble
-    >>> from river import evaluate
-    >>> from river import metrics
-    >>> from river import tree
-
-    >>> dataset = datasets.Phishing()
-
-    >>> metric = metrics.LogLoss()
-
-    >>> model = ensemble.AdaBoostClassifier(
-    ...     model=(
-    ...         tree.HoeffdingTreeClassifier(
-    ...             split_criterion='gini',
-    ...             split_confidence=1e-5,
-    ...             grace_period=2000
-    ...         )
-    ...     ),
-    ...     n_models=5,
-    ...     seed=42
-    ... )
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    LogLoss: 0.364558
-
-    >>> print(model)
-    AdaBoostClassifier(HoeffdingTreeClassifier)
-
-    References
-    ----------
-    [^1]: [Oza, N.C., 2005, October. Online bagging and boosting. In 2005 IEEE international conference on systems, man and cybernetics (Vol. 3, pp. 2340-2345). Ieee.](https://ti.arc.nasa.gov/m/profile/oza/files/ozru01a.pdf)
-
-    """
-
-    def __init__(self, model: base.Classifier, n_models=10, seed: int = None):
-        super().__init__(copy.deepcopy(model) for _ in range(n_models))
-        self.n_models = n_models
-        self.model = model
-        self.seed = seed
-        self._rng = np.random.RandomState(seed)
-        self.wrong_weight = collections.defaultdict(int)
-        self.correct_weight = collections.defaultdict(int)
-
-    @property
-    def _wrapped_model(self):
-        return self.model
-
-    @classmethod
-    def _unit_test_params(cls):
-        return {"model": linear_model.LogisticRegression()}
-
-    def learn_one(self, x, y):
-        lambda_poisson = 1
-
-        for i, model in enumerate(self):
-            for _ in range(self._rng.poisson(lambda_poisson)):
-                model.learn_one(x, y)
-
-            if model.predict_one(x) == y:
-                self.correct_weight[i] += lambda_poisson
-                lambda_poisson *= (self.correct_weight[i] + self.wrong_weight[i]) / (
-                    2 * self.correct_weight[i]
-                )
-
-            else:
-                self.wrong_weight[i] += lambda_poisson
-                lambda_poisson *= (self.correct_weight[i] + self.wrong_weight[i]) / (
-                    2 * self.wrong_weight[i]
-                )
-        return self
-
-    def predict_proba_one(self, x):
-        y_proba = collections.Counter()
-
-        for i, model in enumerate(self):
-            epsilon = self.wrong_weight[i] + 1e-16
-            epsilon /= (self.correct_weight[i] + self.wrong_weight[i]) + 1e-16
-            if epsilon == 0 or epsilon > 0.5:
-                model_weight = 1.0
-            else:
-                beta_inv = (1 - epsilon) / epsilon
-                model_weight = math.log(beta_inv) if beta_inv != 0 else 0
-            predictions = model.predict_proba_one(x)
-            normalize_values_in_dict(predictions, inplace=True)
-            scale_values_in_dict(predictions, model_weight, inplace=True)
-            y_proba.update(predictions)
-
-        normalize_values_in_dict(y_proba, inplace=True)
-        return y_proba
+import collections
+import math
+
+from river import base, linear_model
+from river.utils import poisson
+from river.utils.skmultiflow_utils import normalize_values_in_dict, scale_values_in_dict
+
+__all__ = ["AdaBoostClassifier"]
+
+
+class AdaBoostClassifier(base.WrapperEnsemble, base.Classifier):
+    """Boosting for classification
+
+    For each incoming observation, each model's `learn_one` method is called `k` times where
+    `k` is sampled from a Poisson distribution of parameter lambda. The lambda parameter is
+    updated when the weaks learners fit successively the same observation.
+
+    Parameters
+    ----------
+    model
+        The classifier to boost.
+    n_models
+        The number of models in the ensemble.
+    seed
+        Random number generator seed for reproducibility.
+
+    Attributes
+    ----------
+    wrong_weight : collections.defaultdict
+        Number of times a model has made a mistake when making predictions.
+    correct_weight : collections.defaultdict
+        Number of times a model has predicted the right label when making predictions.
+
+    Examples
+    --------
+
+    In the following example three tree classifiers are boosted together. The performance is
+    slightly better than when using a single tree.
+
+    >>> from river import datasets
+    >>> from river import ensemble
+    >>> from river import evaluate
+    >>> from river import metrics
+    >>> from river import tree
+
+    >>> dataset = datasets.Phishing()
+
+    >>> metric = metrics.LogLoss()
+
+    >>> model = ensemble.AdaBoostClassifier(
+    ...     model=(
+    ...         tree.HoeffdingTreeClassifier(
+    ...             split_criterion='gini',
+    ...             split_confidence=1e-5,
+    ...             grace_period=2000
+    ...         )
+    ...     ),
+    ...     n_models=5,
+    ...     seed=42
+    ... )
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    LogLoss: 0.370805
+
+    >>> print(model)
+    AdaBoostClassifier(HoeffdingTreeClassifier)
+
+    References
+    ----------
+    [^1]: [Oza, N.C., 2005, October. Online bagging and boosting. In 2005 IEEE international conference on systems, man and cybernetics (Vol. 3, pp. 2340-2345). Ieee.](https://ti.arc.nasa.gov/m/profile/oza/files/ozru01a.pdf)
+
+    """
+
+    def __init__(self, model: base.Classifier, n_models=10, seed: int = None):
+        super().__init__(model, n_models, seed)
+        self.wrong_weight = collections.defaultdict(int)
+        self.correct_weight = collections.defaultdict(int)
+
+    @classmethod
+    def _unit_test_params(cls):
+        yield {"model": linear_model.LogisticRegression()}
+
+    def learn_one(self, x, y):
+        lambda_poisson = 1
+
+        for i, model in enumerate(self):
+            for _ in range(poisson(lambda_poisson, self._rng)):
+                model.learn_one(x, y)
+
+            if model.predict_one(x) == y:
+                self.correct_weight[i] += lambda_poisson
+                lambda_poisson *= (self.correct_weight[i] + self.wrong_weight[i]) / (
+                    2 * self.correct_weight[i]
+                )
+
+            else:
+                self.wrong_weight[i] += lambda_poisson
+                lambda_poisson *= (self.correct_weight[i] + self.wrong_weight[i]) / (
+                    2 * self.wrong_weight[i]
+                )
+        return self
+
+    def predict_proba_one(self, x):
+        y_proba = collections.Counter()
+
+        for i, model in enumerate(self):
+            epsilon = self.wrong_weight[i] + 1e-16
+            epsilon /= (self.correct_weight[i] + self.wrong_weight[i]) + 1e-16
+            if epsilon == 0 or epsilon > 0.5:
+                model_weight = 1.0
+            else:
+                beta_inv = (1 - epsilon) / epsilon
+                model_weight = math.log(beta_inv) if beta_inv != 0 else 0
+            predictions = model.predict_proba_one(x)
+            normalize_values_in_dict(predictions, inplace=True)
+            scale_values_in_dict(predictions, model_weight, inplace=True)
+            y_proba.update(predictions)
+
+        normalize_values_in_dict(y_proba, inplace=True)
+        return y_proba
```

### Comparing `river-0.8.0/river/ensemble/voting.py` & `river-0.9.0/river/ensemble/voting.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,86 +1,84 @@
-import collections
-from typing import List
-
-from river import base, linear_model, naive_bayes, tree
-
-
-class VotingClassifier(base.Classifier, base.EnsembleMixin):
-    """Voting classifier.
-
-    A classification is made by aggregating the predictions of each model in the ensemble. The
-    probabilities for each class are summed up if `use_probabilities` is set to `True`. If not,
-    the probabilities are ignored and each prediction is weighted the same. In this case, it's
-    important that you use an odd number of classifiers. A random class will be picked if the
-    number of classifiers is even.
-
-    Parameters
-    ----------
-    models
-        The classifiers.
-    use_probabilities
-        Whether or to weight each prediction with its associated probability.
-
-    Examples
-    --------
-
-    >>> from river import datasets
-    >>> from river import ensemble
-    >>> from river import evaluate
-    >>> from river import linear_model
-    >>> from river import metrics
-    >>> from river import naive_bayes
-    >>> from river import preprocessing
-    >>> from river import tree
-
-    >>> dataset = datasets.Phishing()
-
-    >>> model = (
-    ...     preprocessing.StandardScaler() |
-    ...     ensemble.VotingClassifier(
-    ...         models=[
-    ...             linear_model.LogisticRegression(),
-    ...             tree.HoeffdingTreeClassifier(),
-    ...             naive_bayes.GaussianNB()
-    ...         ]
-    ...     )
-    ... )
-
-    >>> metric = metrics.F1()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    F1: 0.871429
-
-    """
-
-    def __init__(self, models: List[base.Classifier], use_probabilities=True):
-        super().__init__(models)
-        self.use_probabilities = use_probabilities
-
-    @property
-    def _multiclass(self):
-        return all(model._multiclass for model in self)
-
-    def learn_one(self, x, y):
-        for model in self:
-            model.learn_one(x, y)
-        return self
-
-    def predict_one(self, x):
-        if self.use_probabilities:
-            votes = (model.predict_proba_one(x) for model in self)
-        else:
-            votes = ({model.predict_one(x): 1} for model in self)
-        agg = collections.Counter()
-        for vote in votes:
-            agg.update(vote)
-        return agg.most_common(1)[0][0]
-
-    @classmethod
-    def _unit_test_params(cls):
-        return {
-            "models": [
-                linear_model.LogisticRegression(),
-                tree.HoeffdingTreeClassifier(),
-                naive_bayes.GaussianNB(),
-            ]
-        }
+import collections
+from typing import List
+
+from river import base, linear_model, naive_bayes, tree
+
+
+class VotingClassifier(base.Classifier, base.Ensemble):
+    """Voting classifier.
+
+    A classification is made by aggregating the predictions of each model in the ensemble. The
+    probabilities for each class are summed up if `use_probabilities` is set to `True`. If not,
+    the probabilities are ignored and each prediction is weighted the same. In this case, it's
+    important that you use an odd number of classifiers. A random class will be picked if the
+    number of classifiers is even.
+
+    Parameters
+    ----------
+    models
+        The classifiers.
+    use_probabilities
+        Whether or to weight each prediction with its associated probability.
+
+    Examples
+    --------
+
+    >>> from river import datasets
+    >>> from river import ensemble
+    >>> from river import evaluate
+    >>> from river import linear_model
+    >>> from river import metrics
+    >>> from river import naive_bayes
+    >>> from river import preprocessing
+    >>> from river import tree
+
+    >>> dataset = datasets.Phishing()
+
+    >>> model = (
+    ...     preprocessing.StandardScaler() |
+    ...     ensemble.VotingClassifier([
+    ...         linear_model.LogisticRegression(),
+    ...         tree.HoeffdingTreeClassifier(),
+    ...         naive_bayes.GaussianNB()
+    ...     ])
+    ... )
+
+    >>> metric = metrics.F1()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    F1: 0.871429
+
+    """
+
+    def __init__(self, models: List[base.Classifier], use_probabilities=True):
+        super().__init__(models)
+        self.use_probabilities = use_probabilities
+
+    @property
+    def _multiclass(self):
+        return all(model._multiclass for model in self)
+
+    def learn_one(self, x, y):
+        for model in self:
+            model.learn_one(x, y)
+        return self
+
+    def predict_one(self, x):
+        if self.use_probabilities:
+            votes = (model.predict_proba_one(x) for model in self)
+        else:
+            votes = ({model.predict_one(x): 1} for model in self)
+        agg = collections.Counter()
+        for vote in votes:
+            agg.update(vote)
+        return agg.most_common(1)[0][0]
+
+    @classmethod
+    def _unit_test_params(cls):
+        yield {
+            "models": [
+                linear_model.LogisticRegression(),
+                tree.HoeffdingTreeClassifier(),
+                naive_bayes.GaussianNB(),
+            ]
+        }
```

### Comparing `river-0.8.0/river/evaluate/__init__.py` & `river-0.9.0/river/evaluate/__init__.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,18 +1,18 @@
-"""Model evaluation.
-
-This module provides utilities to evaluate an online model. The goal is to reproduce a real-world
-scenario with high fidelity. The core function of this module is `progressive_val_score`, which
-allows to evaluate a model via progressive validation.
-
-This module also exposes "tracks". A track is a predefined combination of a dataset and one or more
-metrics. This allows a principled manner to compare models with each other. For instance,
-the `load_binary_clf_tracks` returns tracks that are to be used to evaluate the performance of
-a binary classification model.
-
-The `benchmarks` directory at the root of the River repository uses these tracks.
-
-"""
-from .progressive_validation import progressive_val_score
-from .tracks import Track, load_binary_clf_tracks
-
-__all__ = ["load_binary_clf_tracks", "progressive_val_score", "Track"]
+"""Model evaluation.
+
+This module provides utilities to evaluate an online model. The goal is to reproduce a real-world
+scenario with high fidelity. The core function of this module is `progressive_val_score`, which
+allows to evaluate a model via progressive validation.
+
+This module also exposes "tracks". A track is a predefined combination of a dataset and one or more
+metrics. This allows a principled manner to compare models with each other. For instance,
+the `load_binary_clf_tracks` returns tracks that are to be used to evaluate the performance of
+a binary classification model.
+
+The `benchmarks` directory at the root of the River repository uses these tracks.
+
+"""
+from .progressive_validation import progressive_val_score
+from .tracks import Track, load_binary_clf_tracks
+
+__all__ = ["load_binary_clf_tracks", "progressive_val_score", "Track"]
```

### Comparing `river-0.8.0/river/evaluate/progressive_validation.py` & `river-0.9.0/river/evaluate/progressive_validation.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,241 +1,241 @@
-import datetime as dt
-import time
-import typing
-from itertools import accumulate, cycle
-
-from river import base, metrics, stream, utils
-
-__all__ = ["progressive_val_score"]
-
-
-def _progressive_validation(
-    dataset: base.typing.Stream,
-    model,
-    metric: metrics.Metric,
-    checkpoints: typing.Iterator[int],
-    moment: typing.Union[str, typing.Callable] = None,
-    delay: typing.Union[str, int, dt.timedelta, typing.Callable] = None,
-    measure_time=False,
-    measure_memory=False,
-):
-
-    # Check that the model and the metric are in accordance
-    if not metric.works_with(model):
-        raise ValueError(
-            f"{metric.__class__.__name__} metric is not compatible with {model}"
-        )
-
-    # Determine if predict_one or predict_proba_one should be used in case of a classifier
-    pred_func = model.predict_one
-    if utils.inspect.isclassifier(model) and not metric.requires_labels:
-        pred_func = model.predict_proba_one
-
-    preds = {}
-
-    next_checkpoint = next(checkpoints, None)
-    n_total_answers = 0
-    if measure_time:
-        start = time.perf_counter()
-
-    for i, x, y in stream.simulate_qa(dataset, moment, delay, copy=True):
-
-        # Question
-        if y is None:
-            preds[i] = pred_func(x=x)
-            continue
-
-        # Answer
-        y_pred = preds.pop(i)
-        if y_pred != {} and y_pred is not None:
-            metric.update(y_true=y, y_pred=y_pred)
-        model.learn_one(x=x, y=y)
-
-        # Update the answer counter
-        n_total_answers += 1
-        if n_total_answers == next_checkpoint:
-            if isinstance(metric, metrics.Metrics):
-                results = {m.__class__.__name__: m.get() for m in metric}
-            else:
-                results = {metric.__class__.__name__: metric.get()}
-            results["Step"] = n_total_answers
-            if measure_time:
-                now = time.perf_counter()
-                results["Time"] = dt.timedelta(seconds=now - start)
-            if measure_memory:
-                results["Memory"] = model._memory_usage
-            yield results
-            next_checkpoint = next(checkpoints, None)
-
-
-def progressive_val_score(
-    dataset: base.typing.Stream,
-    model,
-    metric: metrics.Metric,
-    moment: typing.Union[str, typing.Callable] = None,
-    delay: typing.Union[str, int, dt.timedelta, typing.Callable] = None,
-    print_every=0,
-    show_time=False,
-    show_memory=False,
-    **print_kwargs,
-) -> metrics.Metric:
-    """Evaluates the performance of a model on a streaming dataset.
-
-    This method is the canonical way to evaluate a model's performance. When used correctly, it
-    allows you to exactly assess how a model would have performed in a production scenario.
-
-    `dataset` is converted into a stream of questions and answers. At each step the model is either
-    asked to predict an observation, or is either updated. The target is only revealed to the model
-    after a certain amount of time, which is determined by the `delay` parameter. Note that under
-    the hood this uses the `stream.simulate_qa` function to go through the data in arrival order.
-
-    By default, there is no delay, which means that the samples are processed one after the other.
-    When there is no delay, this function essentially performs progressive validation. When there
-    is a delay, then we refer to it as delayed progressive validation.
-
-    It is recommended to use this method when you want to determine a model's performance on a
-    dataset. In particular, it is advised to use the `delay` parameter in order to get a reliable
-    assessment. Indeed, in a production scenario, it is often the case that ground truths are made
-    available after a certain amount of time. By using this method, you can reproduce this scenario
-    and therefore truthfully assess what would have been the performance of a model on a given
-    dataset.
-
-    Parameters
-    ----------
-    dataset
-        The stream of observations against which the model will be evaluated.
-    model
-        The model to evaluate.
-    metric
-        The metric used to evaluate the model's predictions.
-    moment
-        The attribute used for measuring time. If a callable is passed, then it is expected to take
-        as input a `dict` of features. If `None`, then the observations are implicitly timestamped
-        in the order in which they arrive.
-    delay
-        The amount to wait before revealing the target associated with each observation to the
-        model. This value is expected to be able to sum with the `moment` value. For instance, if
-        `moment` is a `datetime.date`, then `delay` is expected to be a `datetime.timedelta`. If a
-        callable is passed, then it is expected to take as input a `dict` of features and the
-        target. If a `str` is passed, then it will be used to access the relevant field from the
-        features. If `None` is passed, then no delay will be used, which leads to doing standard
-        online validation.
-    print_every
-        Iteration number at which to print the current metric. This only takes into account the
-        predictions, and not the training steps.
-    show_time
-        Whether or not to display the elapsed time.
-    show_memory
-        Whether or not to display the memory usage of the model.
-    print_kwargs
-        Extra keyword arguments are passed to the `print` function. For instance, this allows
-        providing a `file` argument, which indicates where to output progress.
-
-    Examples
-    --------
-
-    Take the following model:
-
-    >>> from river import linear_model
-    >>> from river import preprocessing
-
-    >>> model = (
-    ...     preprocessing.StandardScaler() |
-    ...     linear_model.LogisticRegression()
-    ... )
-
-    We can evaluate it on the `Phishing` dataset as so:
-
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import metrics
-
-    >>> evaluate.progressive_val_score(
-    ...     model=model,
-    ...     dataset=datasets.Phishing(),
-    ...     metric=metrics.ROCAUC(),
-    ...     print_every=200
-    ... )
-    [200] ROCAUC: 0.897995
-    [400] ROCAUC: 0.920896
-    [600] ROCAUC: 0.931339
-    [800] ROCAUC: 0.939909
-    [1,000] ROCAUC: 0.947417
-    [1,200] ROCAUC: 0.950304
-    ROCAUC: 0.950363
-
-    We haven't specified a delay, therefore this is strictly equivalent to the following piece
-    of code:
-
-    >>> model = (
-    ...     preprocessing.StandardScaler() |
-    ...     linear_model.LogisticRegression()
-    ... )
-
-    >>> metric = metrics.ROCAUC()
-
-    >>> for x, y in datasets.Phishing():
-    ...     y_pred = model.predict_proba_one(x)
-    ...     metric = metric.update(y, y_pred)
-    ...     model = model.learn_one(x, y)
-
-    >>> metric
-    ROCAUC: 0.950363
-
-    When `print_every` is specified, the current state is printed at regular intervals. Under
-    the hood, Python's `print` method is being used. You can pass extra keyword arguments to
-    modify its behavior. For instance, you may use the `file` argument if you want to log the
-    progress to a file of your choice.
-
-    >>> with open('progress.log', 'w') as f:
-    ...     metric = evaluate.progressive_val_score(
-    ...         model=model,
-    ...         dataset=datasets.Phishing(),
-    ...         metric=metrics.ROCAUC(),
-    ...         print_every=200,
-    ...         file=f
-    ...     )
-
-    >>> with open('progress.log') as f:
-    ...     for line in f.read().splitlines():
-    ...         print(line)
-    [200] ROCAUC: 0.94
-    [400] ROCAUC: 0.946969
-    [600] ROCAUC: 0.9517
-    [800] ROCAUC: 0.954238
-    [1,000] ROCAUC: 0.958207
-    [1,200] ROCAUC: 0.96002
-
-    Note that the performance is slightly better than above because we haven't used a fresh
-    copy of the model. Instead, we've reused the existing model which has already done a full
-    pass on the data.
-
-    >>> import os; os.remove('progress.log')
-
-    References
-    ----------
-    [^1]: [Beating the Hold-Out: Bounds for K-fold and Progressive Cross-Validation](http://hunch.net/~jl/projects/prediction_bounds/progressive_validation/coltfinal.pdf)
-    [^2]: [Grzenda, M., Gomes, H.M. and Bifet, A., 2019. Delayed labelling evaluation for data streams. Data Mining and Knowledge Discovery, pp.1-30](https://link.springer.com/content/pdf/10.1007%2Fs10618-019-00654-y.pdf)
-
-    """
-
-    checkpoints = _progressive_validation(
-        dataset,
-        model,
-        metric,
-        checkpoints=accumulate(cycle([print_every])) if print_every else iter([]),
-        moment=moment,
-        delay=delay,
-        measure_time=show_time,
-        measure_memory=show_memory,
-    )
-
-    for checkpoint in checkpoints:
-
-        msg = f"[{checkpoint['Step']:,d}] {metric}"
-        if show_time:
-            msg += f" – {checkpoint['Time']}"
-        if show_memory:
-            msg += f" – {checkpoint['Memory']}"
-        print(msg, **print_kwargs)
-
-    return metric
+import datetime as dt
+import time
+import typing
+from itertools import accumulate, cycle
+
+from river import base, metrics, stream, utils
+
+__all__ = ["progressive_val_score"]
+
+
+def _progressive_validation(
+    dataset: base.typing.Dataset,
+    model,
+    metric: metrics.Metric,
+    checkpoints: typing.Iterator[int],
+    moment: typing.Union[str, typing.Callable] = None,
+    delay: typing.Union[str, int, dt.timedelta, typing.Callable] = None,
+    measure_time=False,
+    measure_memory=False,
+):
+
+    # Check that the model and the metric are in accordance
+    if not metric.works_with(model):
+        raise ValueError(
+            f"{metric.__class__.__name__} metric is not compatible with {model}"
+        )
+
+    # Determine if predict_one or predict_proba_one should be used in case of a classifier
+    pred_func = model.predict_one
+    if utils.inspect.isclassifier(model) and not metric.requires_labels:
+        pred_func = model.predict_proba_one
+
+    preds = {}
+
+    next_checkpoint = next(checkpoints, None)
+    n_total_answers = 0
+    if measure_time:
+        start = time.perf_counter()
+
+    for i, x, y in stream.simulate_qa(dataset, moment, delay, copy=True):
+
+        # Question
+        if y is None:
+            preds[i] = pred_func(x=x)
+            continue
+
+        # Answer
+        y_pred = preds.pop(i)
+        if y_pred != {} and y_pred is not None:
+            metric.update(y_true=y, y_pred=y_pred)
+        model.learn_one(x=x, y=y)
+
+        # Update the answer counter
+        n_total_answers += 1
+        if n_total_answers == next_checkpoint:
+            if isinstance(metric, metrics.Metrics):
+                results = {m.__class__.__name__: m.get() for m in metric}
+            else:
+                results = {metric.__class__.__name__: metric.get()}
+            results["Step"] = n_total_answers
+            if measure_time:
+                now = time.perf_counter()
+                results["Time"] = dt.timedelta(seconds=now - start)
+            if measure_memory:
+                results["Memory"] = model._memory_usage
+            yield results
+            next_checkpoint = next(checkpoints, None)
+
+
+def progressive_val_score(
+    dataset: base.typing.Dataset,
+    model,
+    metric: metrics.Metric,
+    moment: typing.Union[str, typing.Callable] = None,
+    delay: typing.Union[str, int, dt.timedelta, typing.Callable] = None,
+    print_every=0,
+    show_time=False,
+    show_memory=False,
+    **print_kwargs,
+) -> metrics.Metric:
+    """Evaluates the performance of a model on a streaming dataset.
+
+    This method is the canonical way to evaluate a model's performance. When used correctly, it
+    allows you to exactly assess how a model would have performed in a production scenario.
+
+    `dataset` is converted into a stream of questions and answers. At each step the model is either
+    asked to predict an observation, or is either updated. The target is only revealed to the model
+    after a certain amount of time, which is determined by the `delay` parameter. Note that under
+    the hood this uses the `stream.simulate_qa` function to go through the data in arrival order.
+
+    By default, there is no delay, which means that the samples are processed one after the other.
+    When there is no delay, this function essentially performs progressive validation. When there
+    is a delay, then we refer to it as delayed progressive validation.
+
+    It is recommended to use this method when you want to determine a model's performance on a
+    dataset. In particular, it is advised to use the `delay` parameter in order to get a reliable
+    assessment. Indeed, in a production scenario, it is often the case that ground truths are made
+    available after a certain amount of time. By using this method, you can reproduce this scenario
+    and therefore truthfully assess what would have been the performance of a model on a given
+    dataset.
+
+    Parameters
+    ----------
+    dataset
+        The stream of observations against which the model will be evaluated.
+    model
+        The model to evaluate.
+    metric
+        The metric used to evaluate the model's predictions.
+    moment
+        The attribute used for measuring time. If a callable is passed, then it is expected to take
+        as input a `dict` of features. If `None`, then the observations are implicitly timestamped
+        in the order in which they arrive.
+    delay
+        The amount to wait before revealing the target associated with each observation to the
+        model. This value is expected to be able to sum with the `moment` value. For instance, if
+        `moment` is a `datetime.date`, then `delay` is expected to be a `datetime.timedelta`. If a
+        callable is passed, then it is expected to take as input a `dict` of features and the
+        target. If a `str` is passed, then it will be used to access the relevant field from the
+        features. If `None` is passed, then no delay will be used, which leads to doing standard
+        online validation.
+    print_every
+        Iteration number at which to print the current metric. This only takes into account the
+        predictions, and not the training steps.
+    show_time
+        Whether or not to display the elapsed time.
+    show_memory
+        Whether or not to display the memory usage of the model.
+    print_kwargs
+        Extra keyword arguments are passed to the `print` function. For instance, this allows
+        providing a `file` argument, which indicates where to output progress.
+
+    Examples
+    --------
+
+    Take the following model:
+
+    >>> from river import linear_model
+    >>> from river import preprocessing
+
+    >>> model = (
+    ...     preprocessing.StandardScaler() |
+    ...     linear_model.LogisticRegression()
+    ... )
+
+    We can evaluate it on the `Phishing` dataset as so:
+
+    >>> from river import datasets
+    >>> from river import evaluate
+    >>> from river import metrics
+
+    >>> evaluate.progressive_val_score(
+    ...     model=model,
+    ...     dataset=datasets.Phishing(),
+    ...     metric=metrics.ROCAUC(),
+    ...     print_every=200
+    ... )
+    [200] ROCAUC: 0.897995
+    [400] ROCAUC: 0.920896
+    [600] ROCAUC: 0.931339
+    [800] ROCAUC: 0.939909
+    [1,000] ROCAUC: 0.947417
+    [1,200] ROCAUC: 0.950304
+    ROCAUC: 0.950363
+
+    We haven't specified a delay, therefore this is strictly equivalent to the following piece
+    of code:
+
+    >>> model = (
+    ...     preprocessing.StandardScaler() |
+    ...     linear_model.LogisticRegression()
+    ... )
+
+    >>> metric = metrics.ROCAUC()
+
+    >>> for x, y in datasets.Phishing():
+    ...     y_pred = model.predict_proba_one(x)
+    ...     metric = metric.update(y, y_pred)
+    ...     model = model.learn_one(x, y)
+
+    >>> metric
+    ROCAUC: 0.950363
+
+    When `print_every` is specified, the current state is printed at regular intervals. Under
+    the hood, Python's `print` method is being used. You can pass extra keyword arguments to
+    modify its behavior. For instance, you may use the `file` argument if you want to log the
+    progress to a file of your choice.
+
+    >>> with open('progress.log', 'w') as f:
+    ...     metric = evaluate.progressive_val_score(
+    ...         model=model,
+    ...         dataset=datasets.Phishing(),
+    ...         metric=metrics.ROCAUC(),
+    ...         print_every=200,
+    ...         file=f
+    ...     )
+
+    >>> with open('progress.log') as f:
+    ...     for line in f.read().splitlines():
+    ...         print(line)
+    [200] ROCAUC: 0.94
+    [400] ROCAUC: 0.946969
+    [600] ROCAUC: 0.9517
+    [800] ROCAUC: 0.954238
+    [1,000] ROCAUC: 0.958207
+    [1,200] ROCAUC: 0.96002
+
+    Note that the performance is slightly better than above because we haven't used a fresh
+    copy of the model. Instead, we've reused the existing model which has already done a full
+    pass on the data.
+
+    >>> import os; os.remove('progress.log')
+
+    References
+    ----------
+    [^1]: [Beating the Hold-Out: Bounds for K-fold and Progressive Cross-Validation](http://hunch.net/~jl/projects/prediction_bounds/progressive_validation/coltfinal.pdf)
+    [^2]: [Grzenda, M., Gomes, H.M. and Bifet, A., 2019. Delayed labelling evaluation for data streams. Data Mining and Knowledge Discovery, pp.1-30](https://link.springer.com/content/pdf/10.1007%2Fs10618-019-00654-y.pdf)
+
+    """
+
+    checkpoints = _progressive_validation(
+        dataset,
+        model,
+        metric,
+        checkpoints=accumulate(cycle([print_every])) if print_every else iter([]),
+        moment=moment,
+        delay=delay,
+        measure_time=show_time,
+        measure_memory=show_memory,
+    )
+
+    for checkpoint in checkpoints:
+
+        msg = f"[{checkpoint['Step']:,d}] {metric}"
+        if show_time:
+            msg += f" – {checkpoint['Time']}"
+        if show_memory:
+            msg += f" – {checkpoint['Memory']}"
+        print(msg, **print_kwargs)
+
+    return metric
```

### Comparing `river-0.8.0/river/evaluate/tracks.py` & `river-0.9.0/river/evaluate/tracks.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,77 +1,77 @@
-import typing
-
-from river import datasets, metrics
-from river.evaluate.progressive_validation import _progressive_validation
-
-
-class Track:
-    """A track evaluate a model's performance.
-
-    The following metrics are recorded:
-
-    # - FLOPS: floating point operations per second.
-    - Time, which should be interpreted with wisdom. Indeed time can depend on the architecture
-        and local resource situations. Comparison via FLOPS should be preferred.
-    - The model's memory footprint.
-    - The model's predictive performance on the track's dataset.
-
-    Parameters
-    ----------
-    name
-        The name of the track.
-    dataset
-        The dataset from which samples will be retrieved. A slice must be used if the dataset
-        is a data generator.
-    metric
-        The metric(s) used to track performance.
-    n_samples
-        The number of samples that are going to be processed by the track.
-
-    """
-
-    def __init__(self, name: str, dataset, metric, n_samples: int = None):
-
-        if n_samples is None:
-            n_samples = dataset.n_samples
-
-        self.name = name
-        self.dataset = dataset
-        self.metric = metric
-        self.n_samples = n_samples
-
-    def run(self, model, n_checkpoints=10):
-
-        # Do the checkpoint logic
-        step = self.n_samples // n_checkpoints
-        checkpoints = range(0, self.n_samples, step)
-        checkpoints = list(checkpoints)[1:] + [self.n_samples]
-
-        # A model might be used in multiple tracks. It's a sane idea to keep things pure and clone
-        # the model so that there's no side effects.
-        model = model.clone()
-
-        yield from _progressive_validation(
-            dataset=self.dataset,
-            model=model,
-            metric=self.metric,
-            checkpoints=iter(checkpoints),
-            measure_time=True,
-            measure_memory=True,
-        )
-
-
-def load_binary_clf_tracks() -> typing.List[Track]:
-    """Return binary classification tracks."""
-
-    return [
-        Track(
-            name="Phishing",
-            dataset=datasets.Phishing(),
-            metric=metrics.Accuracy() + metrics.F1(),
-        ),
-        Track(
-            name="Bananas",
-            dataset=datasets.Bananas(),
-            metric=metrics.Accuracy() + metrics.F1(),
-        ),
-    ]
+import typing
+
+from river import datasets, metrics
+from river.evaluate.progressive_validation import _progressive_validation
+
+
+class Track:
+    """A track evaluate a model's performance.
+
+    The following metrics are recorded:
+
+    # - FLOPS: floating point operations per second.
+    - Time, which should be interpreted with wisdom. Indeed time can depend on the architecture
+        and local resource situations. Comparison via FLOPS should be preferred.
+    - The model's memory footprint.
+    - The model's predictive performance on the track's dataset.
+
+    Parameters
+    ----------
+    name
+        The name of the track.
+    dataset
+        The dataset from which samples will be retrieved. A slice must be used if the dataset
+        is a data generator.
+    metric
+        The metric(s) used to track performance.
+    n_samples
+        The number of samples that are going to be processed by the track.
+
+    """
+
+    def __init__(self, name: str, dataset, metric, n_samples: int = None):
+
+        if n_samples is None:
+            n_samples = dataset.n_samples
+
+        self.name = name
+        self.dataset = dataset
+        self.metric = metric
+        self.n_samples = n_samples
+
+    def run(self, model, n_checkpoints=10):
+
+        # Do the checkpoint logic
+        step = self.n_samples // n_checkpoints
+        checkpoints = range(0, self.n_samples, step)
+        checkpoints = list(checkpoints)[1:] + [self.n_samples]
+
+        # A model might be used in multiple tracks. It's a sane idea to keep things pure and clone
+        # the model so that there's no side effects.
+        model = model.clone()
+
+        yield from _progressive_validation(
+            dataset=self.dataset,
+            model=model,
+            metric=self.metric,
+            checkpoints=iter(checkpoints),
+            measure_time=True,
+            measure_memory=True,
+        )
+
+
+def load_binary_clf_tracks() -> typing.List[Track]:
+    """Return binary classification tracks."""
+
+    return [
+        Track(
+            name="Phishing",
+            dataset=datasets.Phishing(),
+            metric=metrics.Accuracy() + metrics.F1(),
+        ),
+        Track(
+            name="Bananas",
+            dataset=datasets.Bananas(),
+            metric=metrics.Accuracy() + metrics.F1(),
+        ),
+    ]
```

### Comparing `river-0.8.0/river/expert/__init__.py` & `river-0.9.0/river/model_selection/__init__.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,31 +1,29 @@
-"""Expert learning.
-
-This module regroups a variety of methods that may be used for performing model selection. An
-expert learner is provided with a list of models, which are also called experts, and is tasked with
-performing at least as well as the best expert. Indeed, initially the best model is not known. The
-performance of each model becomes more apparent as time goes by. Different strategies are possible,
-each one offering a different tradeoff in terms of accuracy and computational performance.
-
-Expert learning can be used for tuning the hyperparameters of a model. This may be done by creating
-a copy of the model for each set of hyperparameters, and treating each copy as a separate model.
-The `utils.expand_param_grid` function can be used for this purpose.
-
-Note that this differs from the `ensemble` module in that methods from the latter are designed to
-improve the performance of a single model. Both modules may thus be used in conjunction with one
-another.
-
-"""
-
-from .bandit import EpsilonGreedyRegressor, UCBRegressor
-from .ewa import EWARegressor
-from .sh import SuccessiveHalvingClassifier, SuccessiveHalvingRegressor
-from .stacking import StackingClassifier
-
-__all__ = [
-    "EpsilonGreedyRegressor",
-    "EWARegressor",
-    "SuccessiveHalvingClassifier",
-    "SuccessiveHalvingRegressor",
-    "StackingClassifier",
-    "UCBRegressor",
-]
+"""Model selection.
+
+This module regroups a variety of methods that may be used for performing model selection. An
+model selector is provided with a list of models. These are called "experts" in the expert learning
+literature. The model selector's goal is to perform at least as well as the best model. Indeed,
+initially, the best model is not known. The performance of each model becomes more apparent as time
+goes by. Different strategies are possible, each one offering a different tradeoff in terms of
+accuracy and computational performance.
+
+Model selection can be used for tuning the hyperparameters of a model. This may be done by creating
+a copy of the model for each set of hyperparameters, and treating each copy as a separate model.
+The `utils.expand_param_grid` function can be used for this purpose.
+
+"""
+
+from .base import ModelSelector
+from .epsilon_greedy import EpsilonGreedyRegressor
+from .greedy import GreedyRegressor
+from .sh import SuccessiveHalvingClassifier, SuccessiveHalvingRegressor
+from .ucb import UCBRegressor
+
+__all__ = [
+    "EpsilonGreedyRegressor",
+    "GreedyRegressor",
+    "ModelSelector",
+    "SuccessiveHalvingClassifier",
+    "SuccessiveHalvingRegressor",
+    "UCBRegressor",
+]
```

### Comparing `river-0.8.0/river/expert/ewa.py` & `river-0.9.0/river/ensemble/ewa.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,140 +1,131 @@
-import math
-import typing
-
-from river import base
-from river import linear_model as lm
-from river import optim
-from river import preprocessing as pp
-from river.expert.exceptions import NotEnoughModels
-
-__all__ = ["EWARegressor"]
-
-
-class EWARegressor(base.EnsembleMixin, base.Regressor):
-    """Exponentially Weighted Average regressor.
-
-    Parameters
-    ----------
-    regressors
-        The regressors to hedge.
-    loss
-        The loss function that has to be minimized. Defaults to `optim.losses.Squared`.
-    learning_rate
-        The learning rate by which the model weights are multiplied at each iteration.
-
-    Examples
-    --------
-
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import expert
-    >>> from river import linear_model
-    >>> from river import metrics
-    >>> from river import optim
-    >>> from river import preprocessing
-    >>> from river import stream
-
-    >>> optimizers = [
-    ...     optim.SGD(0.01),
-    ...     optim.RMSProp(),
-    ...     optim.AdaGrad()
-    ... ]
-
-    >>> for optimizer in optimizers:
-    ...
-    ...     dataset = datasets.TrumpApproval()
-    ...     metric = metrics.MAE()
-    ...     model = (
-    ...         preprocessing.StandardScaler() |
-    ...         linear_model.LinearRegression(
-    ...             optimizer=optimizer,
-    ...             intercept_lr=.1
-    ...         )
-    ...     )
-    ...
-    ...     print(optimizer, evaluate.progressive_val_score(dataset, model, metric))
-    SGD MAE: 0.555971
-    RMSProp MAE: 0.528284
-    AdaGrad MAE: 0.481461
-
-    >>> dataset = datasets.TrumpApproval()
-    >>> metric = metrics.MAE()
-    >>> hedge = (
-    ...     preprocessing.StandardScaler() |
-    ...     expert.EWARegressor(
-    ...         regressors=[
-    ...             linear_model.LinearRegression(optimizer=o, intercept_lr=.1)
-    ...             for o in optimizers
-    ...         ],
-    ...         learning_rate=0.005
-    ...     )
-    ... )
-
-    >>> evaluate.progressive_val_score(dataset, hedge, metric)
-    MAE: 0.494832
-
-    References
-    ----------
-    [^1]: [Online Learning from Experts: Weighed Majority and Hedge](https://www.shivani-agarwal.net/Teaching/E0370/Aug-2011/Lectures/20-scribe1.pdf)
-    [^2]: [Wikipedia page on the multiplicative weight update method](https://www.wikiwand.com/en/Multiplicative_weight_update_method)
-    [^3]: [Kivinen, J. and Warmuth, M.K., 1997. Exponentiated gradient versus gradient descent for linear predictors. information and computation, 132(1), pp.1-63.](https://users.soe.ucsc.edu/~manfred/pubs/J36.pdf)
-
-    """
-
-    def __init__(
-        self,
-        regressors: typing.List[base.Regressor],
-        loss: optim.losses.RegressionLoss = None,
-        learning_rate=0.5,
-    ):
-
-        if len(regressors) < 2:
-            raise NotEnoughModels(n_expected=2, n_obtained=len(regressors))
-
-        super().__init__(regressors)
-        self.loss = optim.losses.Squared() if loss is None else loss
-        self.learning_rate = learning_rate
-        self.weights = [1.0] * len(regressors)
-
-    @classmethod
-    def _unit_test_params(cls):
-        return {
-            "regressors": [
-                pp.StandardScaler() | lm.LinearRegression(intercept_lr=0.1),
-                pp.StandardScaler() | lm.PARegressor(),
-            ]
-        }
-
-    @property
-    def regressors(self):
-        return self.models
-
-    def learn_predict_one(self, x, y):
-
-        y_pred_mean = 0.0
-
-        # Make a prediction and update the weights accordingly for each model
-        total = 0
-        for i, regressor in enumerate(self):
-            y_pred = regressor.predict_one(x=x)
-            y_pred_mean += self.weights[i] * (y_pred - y_pred_mean) / len(self)
-            loss = self.loss(y_true=y, y_pred=y_pred)
-            self.weights[i] *= math.exp(-self.learning_rate * loss)
-            total += self.weights[i]
-            regressor.learn_one(x, y)
-
-        # Normalize the weights so that they sum up to 1
-        if total:
-            for i, _ in enumerate(self.weights):
-                self.weights[i] /= total
-
-        return y_pred_mean
-
-    def learn_one(self, x, y):
-        self.learn_predict_one(x, y)
-        return self
-
-    def predict_one(self, x):
-        return sum(
-            model.predict_one(x) * weight for model, weight in zip(self, self.weights)
-        )
+import math
+import typing
+
+from river import base
+from river import linear_model as lm
+from river import optim
+from river import preprocessing as pp
+
+__all__ = ["EWARegressor"]
+
+
+class EWARegressor(base.Ensemble, base.Regressor):
+    """Exponentially Weighted Average regressor.
+
+    Parameters
+    ----------
+    models
+        The regressors to hedge.
+    loss
+        The loss function that has to be minimized. Defaults to `optim.losses.Squared`.
+    learning_rate
+        The learning rate by which the model weights are multiplied at each iteration.
+
+    Examples
+    --------
+
+    >>> from river import datasets
+    >>> from river import ensemble
+    >>> from river import evaluate
+    >>> from river import linear_model
+    >>> from river import metrics
+    >>> from river import optim
+    >>> from river import preprocessing
+    >>> from river import stream
+
+    >>> optimizers = [
+    ...     optim.SGD(0.01),
+    ...     optim.RMSProp(),
+    ...     optim.AdaGrad()
+    ... ]
+
+    >>> for optimizer in optimizers:
+    ...
+    ...     dataset = datasets.TrumpApproval()
+    ...     metric = metrics.MAE()
+    ...     model = (
+    ...         preprocessing.StandardScaler() |
+    ...         linear_model.LinearRegression(
+    ...             optimizer=optimizer,
+    ...             intercept_lr=.1
+    ...         )
+    ...     )
+    ...
+    ...     print(optimizer, evaluate.progressive_val_score(dataset, model, metric))
+    SGD MAE: 0.555971
+    RMSProp MAE: 0.528284
+    AdaGrad MAE: 0.481461
+
+    >>> dataset = datasets.TrumpApproval()
+    >>> metric = metrics.MAE()
+    >>> hedge = (
+    ...     preprocessing.StandardScaler() |
+    ...     ensemble.EWARegressor(
+    ...         [
+    ...             linear_model.LinearRegression(optimizer=o, intercept_lr=.1)
+    ...             for o in optimizers
+    ...         ],
+    ...         learning_rate=0.005
+    ...     )
+    ... )
+
+    >>> evaluate.progressive_val_score(dataset, hedge, metric)
+    MAE: 0.494832
+
+    References
+    ----------
+    [^1]: [Online Learning from Experts: Weighed Majority and Hedge](https://www.shivani-agarwal.net/Teaching/E0370/Aug-2011/Lectures/20-scribe1.pdf)
+    [^2]: [Wikipedia page on the multiplicative weight update method](https://www.wikiwand.com/en/Multiplicative_weight_update_method)
+    [^3]: [Kivinen, J. and Warmuth, M.K., 1997. Exponentiated gradient versus gradient descent for linear predictors. information and computation, 132(1), pp.1-63.](https://users.soe.ucsc.edu/~manfred/pubs/J36.pdf)
+
+    """
+
+    def __init__(
+        self,
+        models: typing.List[base.Regressor],
+        loss: optim.losses.RegressionLoss = None,
+        learning_rate=0.5,
+    ):
+        super().__init__(models)
+        self.loss = optim.losses.Squared() if loss is None else loss
+        self.learning_rate = learning_rate
+        self.weights = [1.0] * len(models)
+
+    @classmethod
+    def _unit_test_params(cls):
+        yield {
+            "models": [
+                pp.StandardScaler() | lm.LinearRegression(intercept_lr=0.1),
+                pp.StandardScaler() | lm.PARegressor(),
+            ]
+        }
+
+    def learn_predict_one(self, x, y):
+
+        y_pred_mean = 0.0
+
+        # Make a prediction and update the weights accordingly for each model
+        total = 0
+        for i, regressor in enumerate(self):
+            y_pred = regressor.predict_one(x=x)
+            y_pred_mean += self.weights[i] * (y_pred - y_pred_mean) / len(self)
+            loss = self.loss(y_true=y, y_pred=y_pred)
+            self.weights[i] *= math.exp(-self.learning_rate * loss)
+            total += self.weights[i]
+            regressor.learn_one(x, y)
+
+        # Normalize the weights so that they sum up to 1
+        if total:
+            for i, _ in enumerate(self.weights):
+                self.weights[i] /= total
+
+        return y_pred_mean
+
+    def learn_one(self, x, y):
+        self.learn_predict_one(x, y)
+        return self
+
+    def predict_one(self, x):
+        return sum(
+            model.predict_one(x) * weight for model, weight in zip(self, self.weights)
+        )
```

### Comparing `river-0.8.0/river/expert/sh.py` & `river-0.9.0/river/model_selection/sh.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,418 +1,415 @@
-import copy
-import math
-import operator
-
-from river import base, metrics
-from river.expert.exceptions import NotEnoughModels
-
-__all__ = ["SuccessiveHalvingClassifier", "SuccessiveHalvingRegressor"]
-
-
-class SuccessiveHalving:
-    def __init__(
-        self,
-        models,
-        metric: metrics.Metric,
-        budget: int,
-        eta=2,
-        verbose=False,
-        **print_kwargs,
-    ):
-
-        if len(models) < 2:
-            raise NotEnoughModels(n_expected=2, n_obtained=len(models))
-
-        # Check that the model and the metric are in accordance
-        for model in models:
-            if not metric.works_with(model):
-                raise ValueError(
-                    f"{metric.__class__.__name__} metric can't be used to evaluate a "
-                    + f"{model.__class__.__name__}"
-                )
-
-        self.models = models
-        self.metric = metric
-        self.budget = budget
-        self.eta = eta
-        self.verbose = verbose
-        self.print_kwargs = print_kwargs
-
-        self._n = len(models)
-        self._metrics = [copy.deepcopy(metric) for _ in range(self._n)]
-        self._s = self._n
-        self._n_rungs = 0
-        self._rankings = list(range(self._n))
-        self._r = math.floor(budget / (self._s * math.ceil(math.log(self._n, eta))))
-        self._budget_used = 0
-        self._n_iterations = 0
-        self._best_model_idx = 0
-
-        if isinstance(model, base.Classifier) and not metric.requires_labels:
-            self._pred_func = lambda model: model.predict_proba_one
-        else:
-            self._pred_func = lambda model: model.predict_one
-
-    @property
-    def best_model(self):
-        """The current best model."""
-        return self.models[self._best_model_idx]
-
-    def learn_one(self, x, y):
-
-        for i in self._rankings[: self._s]:
-            model = self.models[i]
-            metric = self._metrics[i]
-            y_pred = self._pred_func(model)(x)
-            metric.update(y_true=y, y_pred=y_pred)
-            model.learn_one(x, y)
-
-            # Check for a new best model
-            if i != self._best_model_idx:
-                op = operator.gt if self.metric.bigger_is_better else operator.lt
-                if op(
-                    self._metrics[i].get(), self._metrics[self._best_model_idx].get()
-                ):
-                    self._best_model_idx = i
-
-        self._n_iterations += 1
-
-        if self._s > 1 and self._n_iterations == self._r:
-
-            self._n_rungs += 1
-            self._budget_used += self._s * self._r
-
-            # Update the rankings of the current models based on their respective metric values
-            self._rankings[: self._s] = sorted(
-                self._rankings[: self._s],
-                key=lambda i: self._metrics[i].get(),
-                reverse=self.metric.bigger_is_better,
-            )
-
-            # Determine how many models to keep for the current rung
-            cutoff = math.ceil(self._s / self.eta)
-
-            if self.verbose:
-                print(
-                    "\t".join(
-                        (
-                            f"[{self._n_rungs}]",
-                            f"{self._s - cutoff} removed",
-                            f"{cutoff} left",
-                            f"{self._r} iterations",
-                            f"budget used: {self._budget_used}",
-                            f"budget left: {self.budget - self._budget_used}",
-                            f"best {self._metrics[self._rankings[0]]}",
-                        )
-                    ),
-                    **self.print_kwargs,
-                )
-
-            # Determine where the next rung is located
-            self._s = cutoff
-            self._r = math.floor(
-                self.budget / (self._s * math.ceil(math.log(self._n, self.eta)))
-            )
-
-        return self
-
-
-class SuccessiveHalvingRegressor(SuccessiveHalving, base.Regressor):
-    r"""Successive halving algorithm for regression.
-
-    Successive halving is a method for performing model selection without having to train each
-    model on all the dataset. At certain points in time (called "rungs"), the worst performing will
-    be discarded and the best ones will keep competing between each other. The rung values are
-    designed so that at most `budget` model updates will be performed in total.
-
-    If you have `k` combinations of hyperparameters and that your dataset contains `n`
-    observations, then the maximal budget you can allocate is:
-
-    $$\frac{2kn}{eta}$$
-
-    It is recommended that you check this beforehand. This bound can't be checked by the function
-    because the size of the dataset is not known. In fact it is potentially infinite, in which case
-    the algorithm will terminate once all the budget has been spent.
-
-    If you have a budget of `B`, and that your dataset contains `n` observations, then the
-    number of hyperparameter combinations that will spend all the budget and go through all the
-    data is:
-
-    $$\ceil(\floor(\frac{B}{2n}) \times eta)$$
-
-    Parameters
-    ----------
-    models
-        The models to compare.
-    metric
-        Metric used for comparing models with.
-    budget
-        Total number of model updates you wish to allocate.
-    eta
-        Rate of elimination. At every rung, `math.ceil(k / eta)` models are kept, where `k` is the
-        number of models that have reached the rung. A higher `eta` value will focus on less models
-        but will allocate more iterations to the best models.
-    verbose
-        Whether to display progress or not.
-    print_kwargs
-        Extra keyword arguments are passed to the `print` function. For instance, this allows
-        providing a `file` argument, which indicates where to output progress.
-
-    Examples
-    --------
-
-    As an example, let's use successive halving to tune the optimizer of a linear regression.
-    We'll first define the model.
-
-    >>> from river import linear_model
-    >>> from river import preprocessing
-
-    >>> model = (
-    ...     preprocessing.StandardScaler() |
-    ...     linear_model.LinearRegression(intercept_lr=.1)
-    ... )
-
-    Let's now define a grid of parameters which we would like to compare. We'll try
-    different optimizers with various learning rates.
-
-    >>> from river import optim
-    >>> from river import utils
-
-    >>> models = utils.expand_param_grid(model, {
-    ...     'LinearRegression': {
-    ...         'optimizer': [
-    ...             (optim.SGD, {'lr': [.1, .01, .005]}),
-    ...             (optim.Adam, {'beta_1': [.01, .001], 'lr': [.1, .01, .001]}),
-    ...             (optim.Adam, {'beta_1': [.1], 'lr': [.001]}),
-    ...         ]
-    ...     }
-    ... })
-
-    We can check how many models we've created.
-
-    >>> len(models)
-    10
-
-    We can now pass these models to a `SuccessiveHalvingRegressor`. We also need to pick a
-    metric to compare the models, and a budget which indicates how many iterations to run
-    before picking the best model and discarding the rest.
-
-    >>> from river import expert
-
-    >>> sh = expert.SuccessiveHalvingRegressor(
-    ...     models=models,
-    ...     metric=metrics.MAE(),
-    ...     budget=2000,
-    ...     eta=2,
-    ...     verbose=True
-    ... )
-
-    A `SuccessiveHalvingRegressor` is also a regressor with a `learn_one` and a `predict_one`
-    method. We can therefore evaluate it like any other classifier with
-    `evaluate.progressive_val_score`.
-
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import metrics
-
-    >>> evaluate.progressive_val_score(
-    ...     dataset=datasets.TrumpApproval(),
-    ...     model=sh,
-    ...     metric=metrics.MAE()
-    ... )
-    [1]	5 removed	5 left	50 iterations	budget used: 500	budget left: 1500	best MAE: 4.540491
-    [2]	2 removed	3 left	100 iterations	budget used: 1000	budget left: 1000	best MAE: 2.458765
-    [3]	1 removed	2 left	166 iterations	budget used: 1498	budget left: 502	best MAE: 1.583751
-    [4]	1 removed	1 left	250 iterations	budget used: 1998	budget left: 2	best MAE: 1.147296
-    MAE: 0.488387
-
-    We can now view the best model.
-
-    >>> sh.best_model
-    Pipeline (
-      StandardScaler (
-        with_std=True
-      ),
-      LinearRegression (
-        optimizer=Adam (
-          lr=Constant (
-            learning_rate=0.1
-          )
-          beta_1=0.01
-          beta_2=0.999
-          eps=1e-08
-        )
-        loss=Squared ()
-        l2=0.
-        intercept_init=0.
-        intercept_lr=Constant (
-          learning_rate=0.1
-        )
-        clip_gradient=1e+12
-        initializer=Zeros ()
-      )
-    )
-
-    References
-    ----------
-    [^1]: [Jamieson, K. and Talwalkar, A., 2016, May. Non-stochastic best arm identification and hyperparameter optimization. In Artificial Intelligence and Statistics (pp. 240-248).](http://proceedings.mlr.press/v51/jamieson16.pdf)
-    [^2]: [Li, L., Jamieson, K., Rostamizadeh, A., Gonina, E., Hardt, M., Recht, B. and Talwalkar, A., 2018. Massively parallel hyperparameter tuning. arXiv preprint arXiv:1810.05934.](https://arxiv.org/pdf/1810.05934.pdf)
-    [^3]: [Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A. and Talwalkar, A., 2017. Hyperband: A novel bandit-based approach to hyperparameter optimization. The Journal of Machine Learning Research, 18(1), pp.6765-6816.](https://arxiv.org/pdf/1603.06560.pdf)
-
-    """
-
-    def predict_one(self, x):
-        return self.best_model.predict_one(x)
-
-
-class SuccessiveHalvingClassifier(SuccessiveHalving, base.Classifier):
-    r"""Successive halving algorithm for classification.
-
-    Successive halving is a method for performing model selection without having to train each
-    model on all the dataset. At certain points in time (called "rungs"), the worst performing will
-    be discarded and the best ones will keep competing between each other. The rung values are
-    designed so that at most `budget` model updates will be performed in total.
-
-    If you have `k` combinations of hyperparameters and that your dataset contains `n`
-    observations, then the maximal budget you can allocate is:
-
-    $$\frac{2kn}{eta}$$
-
-    It is recommended that you check this beforehand. This bound can't be checked by the function
-    because the size of the dataset is not known. In fact it is potentially infinite, in which case
-    the algorithm will terminate once all the budget has been spent.
-
-    If you have a budget of `B`, and that your dataset contains `n` observations, then the
-    number of hyperparameter combinations that will spend all the budget and go through all the
-    data is:
-
-    $$\ceil(\floor(\frac{B}{(2n)}) \times eta)$$
-
-    Parameters
-    ----------
-    models
-        The models to compare.
-    metric
-        Metric used for comparing models with.
-    budget
-        Total number of model updates you wish to allocate.
-    eta
-        Rate of elimination. At every rung, `math.ceil(k / eta)` models are kept, where `k` is the
-        number of models that have reached the rung. A higher `eta` value will focus on less models
-        but will allocate more iterations to the best models.
-    verbose
-        Whether to display progress or not.
-    print_kwargs
-        Extra keyword arguments are passed to the `print` function. For instance, this allows
-        providing a `file` argument, which indicates where to output progress.
-
-    Examples
-    --------
-
-    As an example, let's use successive halving to tune the optimizer of a logistic regression.
-    We'll first define the model.
-
-    >>> from river import linear_model
-    >>> from river import preprocessing
-
-    >>> model = (
-    ...     preprocessing.StandardScaler() |
-    ...     linear_model.LogisticRegression()
-    ... )
-
-    Let's now define a grid of parameters which we would like to compare. We'll try
-    different optimizers with various learning rates.
-
-    >>> from river import utils
-    >>> from river import optim
-
-    >>> models = utils.expand_param_grid(model, {
-    ...     'LogisticRegression': {
-    ...         'optimizer': [
-    ...             (optim.SGD, {'lr': [.1, .01, .005]}),
-    ...             (optim.Adam, {'beta_1': [.01, .001], 'lr': [.1, .01, .001]}),
-    ...             (optim.Adam, {'beta_1': [.1], 'lr': [.001]}),
-    ...         ]
-    ...     }
-    ... })
-
-    We can check how many models we've created.
-
-    >>> len(models)
-    10
-
-    We can now pass these models to a `SuccessiveHalvingClassifier`. We also need to pick a
-    metric to compare the models, and a budget which indicates how many iterations to run
-    before picking the best model and discarding the rest.
-
-    >>> from river import expert
-
-    >>> sh = expert.SuccessiveHalvingClassifier(
-    ...     models=models,
-    ...     metric=metrics.Accuracy(),
-    ...     budget=2000,
-    ...     eta=2,
-    ...     verbose=True
-    ... )
-
-    A `SuccessiveHalvingClassifier` is also a classifier with a `learn_one` and a
-    `predict_proba_one` method. We can therefore evaluate it like any other classifier with
-    `evaluate.progressive_val_score`.
-
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import metrics
-
-    >>> evaluate.progressive_val_score(
-    ...     dataset=datasets.Phishing(),
-    ...     model=sh,
-    ...     metric=metrics.ROCAUC()
-    ... )
-    [1] 5 removed       5 left  50 iterations   budget used: 500        budget left: 1500       best Accuracy: 80.00%
-    [2] 2 removed       3 left  100 iterations  budget used: 1000       budget left: 1000       best Accuracy: 84.00%
-    [3] 1 removed       2 left  166 iterations  budget used: 1498       budget left: 502        best Accuracy: 86.14%
-    [4] 1 removed       1 left  250 iterations  budget used: 1998       budget left: 2  best Accuracy: 84.80%
-    ROCAUC: 0.952889
-
-    We can now view the best model.
-
-    >>> sh.best_model
-    Pipeline (
-      StandardScaler (
-        with_std=True
-      ),
-      LogisticRegression (
-        optimizer=Adam (
-          lr=Constant (
-            learning_rate=0.01
-          )
-          beta_1=0.01
-          beta_2=0.999
-          eps=1e-08
-        )
-        loss=Log (
-          weight_pos=1.
-          weight_neg=1.
-        )
-        l2=0.
-        intercept_init=0.
-        intercept_lr=Constant (
-          learning_rate=0.01
-        )
-        clip_gradient=1e+12
-        initializer=Zeros ()
-      )
-    )
-
-    References
-    ----------
-    [^1]: [Jamieson, K. and Talwalkar, A., 2016, May. Non-stochastic best arm identification and hyperparameter optimization. In Artificial Intelligence and Statistics (pp. 240-248).](http://proceedings.mlr.press/v51/jamieson16.pdf)
-    [^2]: [Li, L., Jamieson, K., Rostamizadeh, A., Gonina, E., Hardt, M., Recht, B. and Talwalkar, A., 2018. Massively parallel hyperparameter tuning. arXiv preprint arXiv:1810.05934.](https://arxiv.org/pdf/1810.05934.pdf)
-    [^3]: [Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A. and Talwalkar, A., 2017. Hyperband: A novel bandit-based approach to hyperparameter optimization. The Journal of Machine Learning Research, 18(1), pp.6765-6816.](https://arxiv.org/pdf/1603.06560.pdf)
-
-    """
-
-    def predict_proba_one(self, x):
-        return self.best_model.predict_proba_one(x)
-
-    def _multiclass(self):
-        return all(model._multiclass for model in self.models)
+import abc
+import copy
+import math
+
+from river import metrics
+
+from .base import ModelSelectionClassifier, ModelSelectionRegressor
+
+__all__ = ["SuccessiveHalvingClassifier", "SuccessiveHalvingRegressor"]
+
+
+class SuccessiveHalving(abc.ABC):
+    def __init__(
+        self,
+        models,
+        metric: metrics.Metric,
+        budget: int,
+        eta=2,
+        verbose=False,
+        **print_kwargs,
+    ):
+
+        super().__init__(models=models, metric=metric)
+        self.budget = budget
+        self.eta = eta
+        self.verbose = verbose
+        self.print_kwargs = print_kwargs
+
+        self._n = len(models)
+        self._metrics = [copy.deepcopy(metric) for _ in range(self._n)]
+        self._s = self._n
+        self._n_rungs = 0
+        self._rankings = list(range(self._n))
+        self._r = math.floor(budget / (self._s * math.ceil(math.log(self._n, eta))))
+        self._budget_used = 0
+        self._n_iterations = 0
+        self._best_model_idx = 0
+
+    @abc.abstractmethod
+    def _pred_func(self, model):
+        ...
+
+    @property
+    def best_model(self):
+        """The current best model."""
+        return self.models[self._best_model_idx]
+
+    def learn_one(self, x, y):
+
+        for i in self._rankings[: self._s]:
+            model = self.models[i]
+            metric = self._metrics[i]
+            y_pred = self._pred_func(model)(x)
+            metric.update(y_true=y, y_pred=y_pred)
+            model.learn_one(x, y)
+
+            # Check for a new best model
+            if metric.is_better_than(self._metrics[self._best_model_idx]):
+                self._best_model_idx = i
+
+        self._n_iterations += 1
+
+        if self._s > 1 and self._n_iterations == self._r:
+
+            self._n_rungs += 1
+            self._budget_used += self._s * self._r
+
+            # Update the rankings of the current models based on their respective metric values
+            self._rankings[: self._s] = sorted(
+                self._rankings[: self._s],
+                key=lambda i: self._metrics[i].get(),
+                reverse=self.metric.bigger_is_better,
+            )
+
+            # Determine how many models to keep for the current rung
+            cutoff = math.ceil(self._s / self.eta)
+
+            if self.verbose:
+                print(
+                    "\t".join(
+                        (
+                            f"[{self._n_rungs}]",
+                            f"{self._s - cutoff} removed",
+                            f"{cutoff} left",
+                            f"{self._r} iterations",
+                            f"budget used: {self._budget_used}",
+                            f"budget left: {self.budget - self._budget_used}",
+                            f"best {self._metrics[self._rankings[0]]}",
+                        )
+                    ),
+                    **self.print_kwargs,
+                )
+
+            # Determine where the next rung is located
+            self._s = cutoff
+            self._r = math.floor(
+                self.budget / (self._s * math.ceil(math.log(self._n, self.eta)))
+            )
+
+        return self
+
+
+class SuccessiveHalvingRegressor(SuccessiveHalving, ModelSelectionRegressor):
+    r"""Successive halving algorithm for regression.
+
+    Successive halving is a method for performing model selection without having to train each
+    model on all the dataset. At certain points in time (called "rungs"), the worst performing will
+    be discarded and the best ones will keep competing between each other. The rung values are
+    designed so that at most `budget` model updates will be performed in total.
+
+    If you have `k` combinations of hyperparameters and that your dataset contains `n`
+    observations, then the maximal budget you can allocate is:
+
+    $$\frac{2kn}{eta}$$
+
+    It is recommended that you check this beforehand. This bound can't be checked by the function
+    because the size of the dataset is not known. In fact it is potentially infinite, in which case
+    the algorithm will terminate once all the budget has been spent.
+
+    If you have a budget of `B`, and that your dataset contains `n` observations, then the
+    number of hyperparameter combinations that will spend all the budget and go through all the
+    data is:
+
+    $$\ceil(\floor(\frac{B}{2n}) \times eta)$$
+
+    Parameters
+    ----------
+    models
+        The models to compare.
+    metric
+        Metric used for comparing models with.
+    budget
+        Total number of model updates you wish to allocate.
+    eta
+        Rate of elimination. At every rung, `math.ceil(k / eta)` models are kept, where `k` is the
+        number of models that have reached the rung. A higher `eta` value will focus on less models
+        but will allocate more iterations to the best models.
+    verbose
+        Whether to display progress or not.
+    print_kwargs
+        Extra keyword arguments are passed to the `print` function. For instance, this allows
+        providing a `file` argument, which indicates where to output progress.
+
+    Examples
+    --------
+
+    As an example, let's use successive halving to tune the optimizer of a linear regression.
+    We'll first define the model.
+
+    >>> from river import linear_model
+    >>> from river import preprocessing
+
+    >>> model = (
+    ...     preprocessing.StandardScaler() |
+    ...     linear_model.LinearRegression(intercept_lr=.1)
+    ... )
+
+    Let's now define a grid of parameters which we would like to compare. We'll try
+    different optimizers with various learning rates.
+
+    >>> from river import optim
+    >>> from river import utils
+
+    >>> models = utils.expand_param_grid(model, {
+    ...     'LinearRegression': {
+    ...         'optimizer': [
+    ...             (optim.SGD, {'lr': [.1, .01, .005]}),
+    ...             (optim.Adam, {'beta_1': [.01, .001], 'lr': [.1, .01, .001]}),
+    ...             (optim.Adam, {'beta_1': [.1], 'lr': [.001]}),
+    ...         ]
+    ...     }
+    ... })
+
+    We can check how many models we've created.
+
+    >>> len(models)
+    10
+
+    We can now pass these models to a `SuccessiveHalvingRegressor`. We also need to pick a
+    metric to compare the models, and a budget which indicates how many iterations to run
+    before picking the best model and discarding the rest.
+
+    >>> from river import model_selection
+
+    >>> sh = model_selection.SuccessiveHalvingRegressor(
+    ...     models,
+    ...     metric=metrics.MAE(),
+    ...     budget=2000,
+    ...     eta=2,
+    ...     verbose=True
+    ... )
+
+    A `SuccessiveHalvingRegressor` is also a regressor with a `learn_one` and a `predict_one`
+    method. We can therefore evaluate it like any other classifier with
+    `evaluate.progressive_val_score`.
+
+    >>> from river import datasets
+    >>> from river import evaluate
+    >>> from river import metrics
+
+    >>> evaluate.progressive_val_score(
+    ...     dataset=datasets.TrumpApproval(),
+    ...     model=sh,
+    ...     metric=metrics.MAE()
+    ... )
+    [1]	5 removed	5 left	50 iterations	budget used: 500	budget left: 1500	best MAE: 4.540491
+    [2]	2 removed	3 left	100 iterations	budget used: 1000	budget left: 1000	best MAE: 2.458765
+    [3]	1 removed	2 left	166 iterations	budget used: 1498	budget left: 502	best MAE: 1.583751
+    [4]	1 removed	1 left	250 iterations	budget used: 1998	budget left: 2	best MAE: 1.147296
+    MAE: 0.488387
+
+    We can now view the best model.
+
+    >>> sh.best_model
+    Pipeline (
+      StandardScaler (
+        with_std=True
+      ),
+      LinearRegression (
+        optimizer=Adam (
+          lr=Constant (
+            learning_rate=0.1
+          )
+          beta_1=0.01
+          beta_2=0.999
+          eps=1e-08
+        )
+        loss=Squared ()
+        l2=0.
+        intercept_init=0.
+        intercept_lr=Constant (
+          learning_rate=0.1
+        )
+        clip_gradient=1e+12
+        initializer=Zeros ()
+      )
+    )
+
+    References
+    ----------
+    [^1]: [Jamieson, K. and Talwalkar, A., 2016, May. Non-stochastic best arm identification and hyperparameter optimization. In Artificial Intelligence and Statistics (pp. 240-248).](http://proceedings.mlr.press/v51/jamieson16.pdf)
+    [^2]: [Li, L., Jamieson, K., Rostamizadeh, A., Gonina, E., Hardt, M., Recht, B. and Talwalkar, A., 2018. Massively parallel hyperparameter tuning. arXiv preprint arXiv:1810.05934.](https://arxiv.org/pdf/1810.05934.pdf)
+    [^3]: [Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A. and Talwalkar, A., 2017. Hyperband: A novel bandit-based approach to hyperparameter optimization. The Journal of Machine Learning Research, 18(1), pp.6765-6816.](https://arxiv.org/pdf/1603.06560.pdf)
+
+    """
+
+    def _pred_func(self, model):
+        return model.predict_one
+
+    def predict_one(self, x):
+        return self.best_model.predict_one(x)
+
+    @classmethod
+    def _unit_test_params(cls):
+        for params in super()._unit_test_params():
+            yield {**params, "budget": 500}
+
+
+class SuccessiveHalvingClassifier(SuccessiveHalving, ModelSelectionClassifier):
+    r"""Successive halving algorithm for classification.
+
+    Successive halving is a method for performing model selection without having to train each
+    model on all the dataset. At certain points in time (called "rungs"), the worst performing will
+    be discarded and the best ones will keep competing between each other. The rung values are
+    designed so that at most `budget` model updates will be performed in total.
+
+    If you have `k` combinations of hyperparameters and that your dataset contains `n`
+    observations, then the maximal budget you can allocate is:
+
+    $$\frac{2kn}{eta}$$
+
+    It is recommended that you check this beforehand. This bound can't be checked by the function
+    because the size of the dataset is not known. In fact it is potentially infinite, in which case
+    the algorithm will terminate once all the budget has been spent.
+
+    If you have a budget of `B`, and that your dataset contains `n` observations, then the
+    number of hyperparameter combinations that will spend all the budget and go through all the
+    data is:
+
+    $$\ceil(\floor(\frac{B}{(2n)}) \times eta)$$
+
+    Parameters
+    ----------
+    models
+        The models to compare.
+    metric
+        Metric used for comparing models with.
+    budget
+        Total number of model updates you wish to allocate.
+    eta
+        Rate of elimination. At every rung, `math.ceil(k / eta)` models are kept, where `k` is the
+        number of models that have reached the rung. A higher `eta` value will focus on less models
+        but will allocate more iterations to the best models.
+    verbose
+        Whether to display progress or not.
+    print_kwargs
+        Extra keyword arguments are passed to the `print` function. For instance, this allows
+        providing a `file` argument, which indicates where to output progress.
+
+    Examples
+    --------
+
+    As an example, let's use successive halving to tune the optimizer of a logistic regression.
+    We'll first define the model.
+
+    >>> from river import linear_model
+    >>> from river import preprocessing
+
+    >>> model = (
+    ...     preprocessing.StandardScaler() |
+    ...     linear_model.LogisticRegression()
+    ... )
+
+    Let's now define a grid of parameters which we would like to compare. We'll try
+    different optimizers with various learning rates.
+
+    >>> from river import utils
+    >>> from river import optim
+
+    >>> models = utils.expand_param_grid(model, {
+    ...     'LogisticRegression': {
+    ...         'optimizer': [
+    ...             (optim.SGD, {'lr': [.1, .01, .005]}),
+    ...             (optim.Adam, {'beta_1': [.01, .001], 'lr': [.1, .01, .001]}),
+    ...             (optim.Adam, {'beta_1': [.1], 'lr': [.001]}),
+    ...         ]
+    ...     }
+    ... })
+
+    We can check how many models we've created.
+
+    >>> len(models)
+    10
+
+    We can now pass these models to a `SuccessiveHalvingClassifier`. We also need to pick a
+    metric to compare the models, and a budget which indicates how many iterations to run
+    before picking the best model and discarding the rest.
+
+    >>> from river import model_selection
+
+    >>> sh = model_selection.SuccessiveHalvingClassifier(
+    ...     models,
+    ...     metric=metrics.Accuracy(),
+    ...     budget=2000,
+    ...     eta=2,
+    ...     verbose=True
+    ... )
+
+    A `SuccessiveHalvingClassifier` is also a classifier with a `learn_one` and a
+    `predict_proba_one` method. We can therefore evaluate it like any other classifier with
+    `evaluate.progressive_val_score`.
+
+    >>> from river import datasets
+    >>> from river import evaluate
+    >>> from river import metrics
+
+    >>> evaluate.progressive_val_score(
+    ...     dataset=datasets.Phishing(),
+    ...     model=sh,
+    ...     metric=metrics.ROCAUC()
+    ... )
+    [1] 5 removed       5 left  50 iterations   budget used: 500        budget left: 1500       best Accuracy: 80.00%
+    [2] 2 removed       3 left  100 iterations  budget used: 1000       budget left: 1000       best Accuracy: 84.00%
+    [3] 1 removed       2 left  166 iterations  budget used: 1498       budget left: 502        best Accuracy: 86.14%
+    [4] 1 removed       1 left  250 iterations  budget used: 1998       budget left: 2  best Accuracy: 84.80%
+    ROCAUC: 0.952889
+
+    We can now view the best model.
+
+    >>> sh.best_model
+    Pipeline (
+      StandardScaler (
+        with_std=True
+      ),
+      LogisticRegression (
+        optimizer=Adam (
+          lr=Constant (
+            learning_rate=0.01
+          )
+          beta_1=0.01
+          beta_2=0.999
+          eps=1e-08
+        )
+        loss=Log (
+          weight_pos=1.
+          weight_neg=1.
+        )
+        l2=0.
+        intercept_init=0.
+        intercept_lr=Constant (
+          learning_rate=0.01
+        )
+        clip_gradient=1e+12
+        initializer=Zeros ()
+      )
+    )
+
+    References
+    ----------
+    [^1]: [Jamieson, K. and Talwalkar, A., 2016, May. Non-stochastic best arm identification and hyperparameter optimization. In Artificial Intelligence and Statistics (pp. 240-248).](http://proceedings.mlr.press/v51/jamieson16.pdf)
+    [^2]: [Li, L., Jamieson, K., Rostamizadeh, A., Gonina, E., Hardt, M., Recht, B. and Talwalkar, A., 2018. Massively parallel hyperparameter tuning. arXiv preprint arXiv:1810.05934.](https://arxiv.org/pdf/1810.05934.pdf)
+    [^3]: [Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A. and Talwalkar, A., 2017. Hyperband: A novel bandit-based approach to hyperparameter optimization. The Journal of Machine Learning Research, 18(1), pp.6765-6816.](https://arxiv.org/pdf/1603.06560.pdf)
+
+    """
+
+    def _pred_func(self, model):
+        if self.metric.requires_labels:
+            return model.predict_one
+        return model.predict_proba_one
+
+    def predict_proba_one(self, x):
+        return self.best_model.predict_proba_one(x)
+
+    def _multiclass(self):
+        return all(model._multiclass for model in self.models)
```

### Comparing `river-0.8.0/river/expert/stacking.py` & `river-0.9.0/river/ensemble/stacking.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,103 +1,109 @@
-import typing
-
-from river import base
-from river.expert.exceptions import NotEnoughModels
-
-__all__ = ["StackingClassifier"]
-
-
-class StackingClassifier(base.EnsembleMixin, base.Classifier):
-    """Stacking for binary classification.
-
-    Parameters
-    ----------
-    classifiers
-    meta_classifier
-    include_features
-        Indicates whether or not the original features should be provided to the meta-model along
-        with the predictions from each model.
-
-    Examples
-    --------
-
-    >>> from river import compose
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import expert
-    >>> from river import linear_model as lm
-    >>> from river import metrics
-    >>> from river import preprocessing as pp
-
-    >>> dataset = datasets.Phishing()
-
-    >>> model = compose.Pipeline(
-    ...     ('scale', pp.StandardScaler()),
-    ...     ('stack', expert.StackingClassifier(
-    ...         classifiers=[
-    ...             lm.LogisticRegression(),
-    ...             lm.PAClassifier(mode=1, C=0.01),
-    ...             lm.PAClassifier(mode=2, C=0.01)
-    ...         ],
-    ...         meta_classifier=lm.LogisticRegression()
-    ...     ))
-    ... )
-
-    >>> metric = metrics.F1()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    F1: 0.881387
-
-    References
-    ----------
-    [^1]: [A Kaggler's Guide to Model Stacking in Practice](http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/)
-
-    """
-
-    def __init__(
-        self,
-        classifiers: typing.List[base.Classifier],
-        meta_classifier: base.Classifier,
-        include_features=True,
-    ):
-
-        if len(classifiers) < 2:
-            raise NotEnoughModels(n_expected=2, n_obtained=len(classifiers))
-
-        super().__init__(classifiers)
-        self.meta_classifier = meta_classifier
-        self.include_features = include_features
-
-    @property
-    def _multiclass(self):
-        return self.meta_classifier._multiclass
-
-    def learn_one(self, x, y):
-
-        # Ask each model to make a prediction and then update it
-        oof = {}
-        for i, clf in enumerate(self):
-            for k, p in clf.predict_proba_one(x).items():
-                oof[f"oof_{i}_{k}"] = p
-            clf.learn_one(x, y)
-
-        # Optionally, add the base features
-        if self.include_features:
-            oof.update(x)
-
-        # Update the meta-classifier using the predictions from the base classifiers
-        self.meta_classifier.learn_one(oof, y)
-
-        return self
-
-    def predict_proba_one(self, x):
-
-        oof = {
-            f"oof_{i}_{k}": p
-            for i, clf in enumerate(self)
-            for k, p in clf.predict_proba_one(x).items()
-        }
-
-        if self.include_features:
-            oof.update(x)
-
-        return self.meta_classifier.predict_proba_one(oof)
+import typing
+
+from river import base, linear_model, naive_bayes, tree
+
+__all__ = ["StackingClassifier"]
+
+
+class StackingClassifier(base.Ensemble, base.Classifier):
+    """Stacking for binary classification.
+
+    Parameters
+    ----------
+    models
+    meta_classifier
+    include_features
+        Indicates whether or not the original features should be provided to the meta-model along
+        with the predictions from each model.
+
+    Examples
+    --------
+
+    >>> from river import compose
+    >>> from river import datasets
+    >>> from river import ensemble
+    >>> from river import evaluate
+    >>> from river import linear_model as lm
+    >>> from river import metrics
+    >>> from river import preprocessing as pp
+
+    >>> dataset = datasets.Phishing()
+
+    >>> model = compose.Pipeline(
+    ...     ('scale', pp.StandardScaler()),
+    ...     ('stack', ensemble.StackingClassifier(
+    ...         [
+    ...             lm.LogisticRegression(),
+    ...             lm.PAClassifier(mode=1, C=0.01),
+    ...             lm.PAClassifier(mode=2, C=0.01),
+    ...         ],
+    ...         meta_classifier=lm.LogisticRegression()
+    ...     ))
+    ... )
+
+    >>> metric = metrics.F1()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    F1: 0.881387
+
+    References
+    ----------
+    [^1]: [A Kaggler's Guide to Model Stacking in Practice](http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/)
+
+    """
+
+    def __init__(
+        self,
+        models: typing.List[base.Classifier],
+        meta_classifier: base.Classifier,
+        include_features=True,
+    ):
+        super().__init__(models)
+        self.meta_classifier = meta_classifier
+        self.include_features = include_features
+
+    @property
+    def _multiclass(self):
+        return self.meta_classifier._multiclass
+
+    def learn_one(self, x, y):
+
+        # Ask each model to make a prediction and then update it
+        oof = {}
+        for i, clf in enumerate(self):
+            for k, p in clf.predict_proba_one(x).items():
+                oof[f"oof_{i}_{k}"] = p
+            clf.learn_one(x, y)
+
+        # Optionally, add the base features
+        if self.include_features:
+            oof.update(x)
+
+        # Update the meta-classifier using the predictions from the base classifiers
+        self.meta_classifier.learn_one(oof, y)
+
+        return self
+
+    def predict_proba_one(self, x):
+
+        oof = {
+            f"oof_{i}_{k}": p
+            for i, clf in enumerate(self)
+            for k, p in clf.predict_proba_one(x).items()
+        }
+
+        if self.include_features:
+            oof.update(x)
+
+        return self.meta_classifier.predict_proba_one(oof)
+
+    @classmethod
+    def _unit_test_params(cls):
+        yield {
+            "models": [
+                linear_model.LogisticRegression(),
+                tree.HoeffdingTreeClassifier(),
+                naive_bayes.GaussianNB(),
+            ],
+            "meta_classifier": linear_model.LogisticRegression(),
+        }
```

### Comparing `river-0.8.0/river/facto/base.py` & `river-0.9.0/river/facto/base.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,135 +1,135 @@
-import abc
-import collections
-import numbers
-
-from .. import optim, utils
-
-
-class BaseFM:
-    """Factorization Machines base class."""
-
-    def __init__(
-        self,
-        n_factors,
-        weight_optimizer,
-        latent_optimizer,
-        loss,
-        sample_normalization,
-        l1_weight,
-        l2_weight,
-        l1_latent,
-        l2_latent,
-        intercept,
-        intercept_lr,
-        weight_initializer,
-        latent_initializer,
-        clip_gradient,
-        seed,
-    ):
-        self.n_factors = n_factors
-        self.weight_optimizer = (
-            optim.SGD(0.01) if weight_optimizer is None else weight_optimizer
-        )
-        self.latent_optimizer = (
-            optim.SGD(0.01) if latent_optimizer is None else latent_optimizer
-        )
-        self.loss = loss
-        self.sample_normalization = sample_normalization
-        self.l1_weight = l1_weight
-        self.l2_weight = l2_weight
-        self.l1_latent = l1_latent
-        self.l2_latent = l2_latent
-        self.intercept = intercept
-
-        self.intercept_lr = (
-            optim.schedulers.Constant(intercept_lr)
-            if isinstance(intercept_lr, numbers.Number)
-            else intercept_lr
-        )
-
-        if weight_initializer is None:
-            weight_initializer = optim.initializers.Zeros()
-        self.weight_initializer = weight_initializer
-        self.weights = collections.defaultdict(weight_initializer)
-
-        if latent_initializer is None:
-            latent_initializer = optim.initializers.Normal(sigma=0.1, seed=seed)
-        self.latent_initializer = latent_initializer
-        self.latents = self._init_latents()
-
-        self.clip_gradient = clip_gradient
-        self.seed = seed
-
-    @abc.abstractmethod
-    def _init_latents(self) -> collections.defaultdict:
-        """Initializes latent weights dict."""
-
-    def learn_one(self, x, y, sample_weight=1.0):
-        x = self._ohe_cat_features(x)
-
-        if self.sample_normalization:
-            x_l2_norm = sum((xj ** 2 for xj in x.values())) ** 0.5
-            x = {j: xj / x_l2_norm for j, xj in x.items()}
-
-        return self._learn_one(x, y, sample_weight=sample_weight)
-
-    def _ohe_cat_features(self, x):
-        """One hot encodes string features considering them as categorical."""
-        return dict(
-            (f"{j}_{xj}", 1) if isinstance(xj, str) else (j, xj) for j, xj in x.items()
-        )
-
-    def _learn_one(self, x, y, sample_weight=1.0):
-
-        # Calculate the gradient of the loss with respect to the raw output
-        g_loss = self.loss.gradient(y_true=y, y_pred=self._raw_dot(x))
-
-        # Clamp the gradient to avoid numerical instability
-        g_loss = utils.math.clamp(
-            g_loss, minimum=-self.clip_gradient, maximum=self.clip_gradient
-        )
-
-        # Apply the sample weight
-        g_loss *= sample_weight
-
-        # Update the intercept
-        intercept_lr = self.intercept_lr.get(self.weight_optimizer.n_iterations)
-        self.intercept -= intercept_lr * g_loss
-
-        # Update the weights
-        weights_gradient = self._calculate_weights_gradients(x, g_loss)
-        self.weights = self.weight_optimizer.step(w=self.weights, g=weights_gradient)
-
-        # Update the latent weights
-        self._update_latents(x, g_loss)
-
-        return self
-
-    def _raw_dot(self, x):
-
-        # Start with the intercept
-        y_pred = self.intercept
-
-        # Add the unary interactions
-        y_pred += utils.math.dot(x, self.weights)
-
-        # Add greater than unary interactions
-        y_pred += self._calculate_interactions(x)
-
-        return y_pred
-
-    def _field(self, j):
-        """Infers feature field name."""
-        return j.split("_")[0]
-
-    @abc.abstractmethod
-    def _calculate_interactions(self, x: dict) -> float:
-        """Calculates greater than unary interactions."""
-
-    @abc.abstractmethod
-    def _calculate_weights_gradients(self, x: dict, g_loss: float) -> dict:
-        """Calculates weights gradient."""
-
-    @abc.abstractmethod
-    def _update_latents(self, x: dict, g_loss: float):
-        """Updates latent weights."""
+import abc
+import collections
+import numbers
+
+from .. import optim, utils
+
+
+class BaseFM:
+    """Factorization Machines base class."""
+
+    def __init__(
+        self,
+        n_factors,
+        weight_optimizer,
+        latent_optimizer,
+        loss,
+        sample_normalization,
+        l1_weight,
+        l2_weight,
+        l1_latent,
+        l2_latent,
+        intercept,
+        intercept_lr,
+        weight_initializer,
+        latent_initializer,
+        clip_gradient,
+        seed,
+    ):
+        self.n_factors = n_factors
+        self.weight_optimizer = (
+            optim.SGD(0.01) if weight_optimizer is None else weight_optimizer
+        )
+        self.latent_optimizer = (
+            optim.SGD(0.01) if latent_optimizer is None else latent_optimizer
+        )
+        self.loss = loss
+        self.sample_normalization = sample_normalization
+        self.l1_weight = l1_weight
+        self.l2_weight = l2_weight
+        self.l1_latent = l1_latent
+        self.l2_latent = l2_latent
+        self.intercept = intercept
+
+        self.intercept_lr = (
+            optim.schedulers.Constant(intercept_lr)
+            if isinstance(intercept_lr, numbers.Number)
+            else intercept_lr
+        )
+
+        if weight_initializer is None:
+            weight_initializer = optim.initializers.Zeros()
+        self.weight_initializer = weight_initializer
+        self.weights = collections.defaultdict(weight_initializer)
+
+        if latent_initializer is None:
+            latent_initializer = optim.initializers.Normal(sigma=0.1, seed=seed)
+        self.latent_initializer = latent_initializer
+        self.latents = self._init_latents()
+
+        self.clip_gradient = clip_gradient
+        self.seed = seed
+
+    @abc.abstractmethod
+    def _init_latents(self) -> collections.defaultdict:
+        """Initializes latent weights dict."""
+
+    def learn_one(self, x, y, sample_weight=1.0):
+        x = self._ohe_cat_features(x)
+
+        if self.sample_normalization:
+            x_l2_norm = sum((xj ** 2 for xj in x.values())) ** 0.5
+            x = {j: xj / x_l2_norm for j, xj in x.items()}
+
+        return self._learn_one(x, y, sample_weight=sample_weight)
+
+    def _ohe_cat_features(self, x):
+        """One hot encodes string features considering them as categorical."""
+        return dict(
+            (f"{j}_{xj}", 1) if isinstance(xj, str) else (j, xj) for j, xj in x.items()
+        )
+
+    def _learn_one(self, x, y, sample_weight=1.0):
+
+        # Calculate the gradient of the loss with respect to the raw output
+        g_loss = self.loss.gradient(y_true=y, y_pred=self._raw_dot(x))
+
+        # Clamp the gradient to avoid numerical instability
+        g_loss = utils.math.clamp(
+            g_loss, minimum=-self.clip_gradient, maximum=self.clip_gradient
+        )
+
+        # Apply the sample weight
+        g_loss *= sample_weight
+
+        # Update the intercept
+        intercept_lr = self.intercept_lr.get(self.weight_optimizer.n_iterations)
+        self.intercept -= intercept_lr * g_loss
+
+        # Update the weights
+        weights_gradient = self._calculate_weights_gradients(x, g_loss)
+        self.weights = self.weight_optimizer.step(w=self.weights, g=weights_gradient)
+
+        # Update the latent weights
+        self._update_latents(x, g_loss)
+
+        return self
+
+    def _raw_dot(self, x):
+
+        # Start with the intercept
+        y_pred = self.intercept
+
+        # Add the unary interactions
+        y_pred += utils.math.dot(x, self.weights)
+
+        # Add greater than unary interactions
+        y_pred += self._calculate_interactions(x)
+
+        return y_pred
+
+    def _field(self, j):
+        """Infers feature field name."""
+        return j.split("_")[0]
+
+    @abc.abstractmethod
+    def _calculate_interactions(self, x: dict) -> float:
+        """Calculates greater than unary interactions."""
+
+    @abc.abstractmethod
+    def _calculate_weights_gradients(self, x: dict, g_loss: float) -> dict:
+        """Calculates weights gradient."""
+
+    @abc.abstractmethod
+    def _update_latents(self, x: dict, g_loss: float):
+        """Updates latent weights."""
```

### Comparing `river-0.8.0/river/facto/ffm.py` & `river-0.9.0/river/facto/fwfm.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,381 +1,408 @@
-import collections
-import functools
-import itertools
-import typing
-
-import numpy as np
-
-from river import base, optim, utils
-
-from .base import BaseFM
-
-__all__ = ["FFMClassifier", "FFMRegressor"]
-
-
-class FFM(BaseFM):
-    """Field-aware Factorization Machine base class."""
-
-    def __init__(
-        self,
-        n_factors,
-        weight_optimizer,
-        latent_optimizer,
-        loss,
-        sample_normalization,
-        l1_weight,
-        l2_weight,
-        l1_latent,
-        l2_latent,
-        intercept,
-        intercept_lr,
-        weight_initializer,
-        latent_initializer,
-        clip_gradient,
-        seed,
-    ):
-        super().__init__(
-            n_factors=n_factors,
-            weight_optimizer=weight_optimizer,
-            latent_optimizer=latent_optimizer,
-            loss=loss,
-            sample_normalization=sample_normalization,
-            l1_weight=l1_weight,
-            l2_weight=l2_weight,
-            l1_latent=l1_latent,
-            l2_latent=l2_latent,
-            intercept=intercept,
-            intercept_lr=intercept_lr,
-            weight_initializer=weight_initializer,
-            latent_initializer=latent_initializer,
-            clip_gradient=clip_gradient,
-            seed=seed,
-        )
-
-    def _init_latents(self):
-        random_latents = functools.partial(
-            self.latent_initializer, shape=self.n_factors
-        )
-        field_latents_dict = functools.partial(collections.defaultdict, random_latents)
-        return collections.defaultdict(field_latents_dict)
-
-    def _calculate_interactions(self, x):
-        """Calculates pairwise interactions."""
-        field = self._field
-        return sum(
-            x[j1]
-            * x[j2]
-            * np.dot(self.latents[j1][field(j2)], self.latents[j2][field(j1)])
-            for j1, j2 in itertools.combinations(x.keys(), 2)
-        )
-
-    def _calculate_weights_gradients(self, x, g_loss):
-
-        # For notational convenience
-        w, l1, l2, sign = self.weights, self.l1_weight, self.l2_weight, utils.math.sign
-
-        return {j: g_loss * xj + l1 * sign(w[j]) + l2 * w[j] for j, xj in x.items()}
-
-    def _update_latents(self, x, g_loss):
-
-        # For notational convenience
-        v, l1, l2 = self.latents, self.l1_latent, self.l2_latent
-        sign, field = utils.math.sign, self._field
-
-        # Calculate each latent factor gradient before updating any
-        latent_gradient = collections.defaultdict(
-            lambda: collections.defaultdict(lambda: collections.defaultdict(float))
-        )
-
-        for j1, j2 in itertools.combinations(x.keys(), 2):
-            xj1_xj2 = x[j1] * x[j2]
-            field_j1, field_j2 = field(j1), field(j2)
-
-            for f in range(self.n_factors):
-                latent_gradient[j1][field_j2][f] += xj1_xj2 * v[j2][field_j1][f]
-                latent_gradient[j2][field_j1][f] += xj1_xj2 * v[j1][field_j2][f]
-
-        # Finally update the latent weights
-        for j in x.keys():
-            for field in latent_gradient[j].keys():
-                self.latents[j][field] = self.latent_optimizer.step(
-                    w=v[j][field],
-                    g={
-                        f: g_loss * latent_gradient[j][field][f]
-                        + l1 * sign(v[j][field][f])
-                        + 2.0 * l2 * v[j][field][f]
-                        for f in range(self.n_factors)
-                    },
-                )
-
-
-class FFMRegressor(FFM, base.Regressor):
-    """Field-aware Factorization Machine for regression.
-
-    The model equation is defined by:
-
-    $$\\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j}  + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} \\langle \\mathbf{v}_{j, f_{j'}}, \\mathbf{v}_{j', f_j} \\rangle x_{j} x_{j'}$$
-
-    Where \\mathbf{v}_{j, f_{j'}} is the latent vector corresponding to $j$ feature for $f_{j'}$
-    field, and \\mathbf{v}_{j', f_j} is the latent vector corresponding to $j'$ feature for $f_j$
-    field.
-
-    For more efficiency, this model automatically one-hot encodes strings features considering them
-    as categorical variables. Field names are inferred from feature names by taking everything
-    before the first underscore: `feature_name.split('_')[0]`.
-
-    Parameters
-    ----------
-    n_factors
-        Dimensionality of the factorization or number of latent factors.
-    weight_optimizer
-        The sequential optimizer used for updating the feature weights. Note that the intercept is
-        handled separately.
-    latent_optimizer
-        The sequential optimizer used for updating the latent factors.
-    loss
-        The loss function to optimize for.
-    sample_normalization
-        Whether to divide each element of `x` by `x`'s L2-norm.
-    l1_weight
-        Amount of L1 regularization used to push weights towards 0.
-    l2_weight
-        Amount of L2 regularization used to push weights towards 0.
-    l1_latent
-        Amount of L1 regularization used to push latent weights towards 0.
-    l2_latent
-        Amount of L2 regularization used to push latent weights towards 0.
-    intercept
-        Initial intercept value.
-    intercept_lr
-        Learning rate scheduler used for updating the intercept. An instance of
-        `optim.schedulers.Constant` is used if a `float` is passed. No intercept will be used
-        if this is set to 0.
-    weight_initializer
-        Weights initialization scheme. Defaults to `optim.initializers.Zeros()`.
-    latent_initializer
-        Latent factors initialization scheme. Defaults to
-        `optim.initializers.Normal(mu=.0, sigma=.1, random_state=self.random_state)`.
-    clip_gradient
-        Clips the absolute value of each gradient value.
-    seed
-        Randomization seed used for reproducibility.
-
-    Attributes
-    ----------
-    weights
-        The current weights assigned to the features.
-    latents
-        The current latent weights assigned to the features.
-
-    Examples
-    --------
-
-    >>> from river import facto
-
-    >>> dataset = (
-    ...     ({'user': 'Alice', 'item': 'Superman', 'time': .12}, 8),
-    ...     ({'user': 'Alice', 'item': 'Terminator', 'time': .13}, 9),
-    ...     ({'user': 'Alice', 'item': 'Star Wars', 'time': .14}, 8),
-    ...     ({'user': 'Alice', 'item': 'Notting Hill', 'time': .15}, 2),
-    ...     ({'user': 'Alice', 'item': 'Harry Potter ', 'time': .16}, 5),
-    ...     ({'user': 'Bob', 'item': 'Superman', 'time': .13}, 8),
-    ...     ({'user': 'Bob', 'item': 'Terminator', 'time': .12}, 9),
-    ...     ({'user': 'Bob', 'item': 'Star Wars', 'time': .16}, 8),
-    ...     ({'user': 'Bob', 'item': 'Notting Hill', 'time': .10}, 2)
-    ... )
-
-    >>> model = facto.FFMRegressor(
-    ...     n_factors=10,
-    ...     intercept=5,
-    ...     seed=42,
-    ... )
-
-    >>> for x, y in dataset:
-    ...     model = model.learn_one(x, y)
-
-    >>> model.predict_one({'user': 'Bob', 'item': 'Harry Potter', 'time': .14})
-    5.319945
-
-    References
-    ----------
-    [^1]: [Juan, Y., Zhuang, Y., Chin, W.S. and Lin, C.J., 2016, September. Field-aware factorization machines for CTR prediction. In Proceedings of the 10th ACM Conference on Recommender Systems (pp. 43-50).](https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf)
-
-    """
-
-    def __init__(
-        self,
-        n_factors=10,
-        weight_optimizer: optim.Optimizer = None,
-        latent_optimizer: optim.Optimizer = None,
-        loss: optim.losses.RegressionLoss = None,
-        sample_normalization=False,
-        l1_weight=0.0,
-        l2_weight=0.0,
-        l1_latent=0.0,
-        l2_latent=0.0,
-        intercept=0.0,
-        intercept_lr: typing.Union[optim.schedulers.Scheduler, float] = 0.01,
-        weight_initializer: optim.initializers.Initializer = None,
-        latent_initializer: optim.initializers.Initializer = None,
-        clip_gradient=1e12,
-        seed: int = None,
-    ):
-
-        super().__init__(
-            n_factors=n_factors,
-            weight_optimizer=weight_optimizer,
-            latent_optimizer=latent_optimizer,
-            loss=optim.losses.Squared() if loss is None else loss,
-            sample_normalization=sample_normalization,
-            l1_weight=l1_weight,
-            l2_weight=l2_weight,
-            l1_latent=l1_latent,
-            l2_latent=l2_latent,
-            intercept=intercept,
-            intercept_lr=intercept_lr,
-            weight_initializer=weight_initializer,
-            latent_initializer=latent_initializer,
-            clip_gradient=clip_gradient,
-            seed=seed,
-        )
-
-    def predict_one(self, x):
-        x = self._ohe_cat_features(x)
-        return self._raw_dot(x)
-
-
-class FFMClassifier(FFM, base.Classifier):
-    """Field-aware Factorization Machine for binary classification.
-
-    The model equation is defined by:
-
-    $$\\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j}  + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} \\langle \\mathbf{v}_{j, f_{j'}}, \\mathbf{v}_{j', f_j} \\rangle x_{j} x_{j'}$$
-
-    Where \\mathbf{v}_{j, f_{j'}} is the latent vector corresponding to $j$ feature for $f_{j'}$
-    field, and \\mathbf{v}_{j', f_j} is the latent vector corresponding to $j'$ feature for $f_j$
-    field.
-
-    For more efficiency, this model automatically one-hot encodes strings features considering them
-    as categorical variables. Field names are inferred from feature names by taking everything
-    before the first underscore: `feature_name.split('_')[0]`.
-
-    Parameters
-    ----------
-    n_factors
-        Dimensionality of the factorization or number of latent factors.
-    weight_optimizer
-        The sequential optimizer used for updating the feature weights. Note that the intercept is
-        handled separately.
-    latent_optimizer
-        The sequential optimizer used for updating the latent factors.
-    loss
-        The loss function to optimize for.
-    sample_normalization
-        Whether to divide each element of `x` by `x`'s L2-norm.
-    l1_weight
-        Amount of L1 regularization used to push weights towards 0.
-    l2_weight
-        Amount of L2 regularization used to push weights towards 0.
-    l1_latent
-        Amount of L1 regularization used to push latent weights towards 0.
-    l2_latent
-        Amount of L2 regularization used to push latent weights towards 0.
-    intercept
-        Initial intercept value.
-    intercept_lr
-        Learning rate scheduler used for updating the intercept. An instance of
-        `optim.schedulers.Constant` is used if a `float` is passed. No intercept will be used
-        if this is set to 0.
-    weight_initializer
-        Weights initialization scheme. Defaults to `optim.initializers.Zeros()`.
-    latent_initializer
-        Latent factors initialization scheme. Defaults to
-        `optim.initializers.Normal(mu=.0, sigma=.1, random_state=self.random_state)`.
-    clip_gradient
-        Clips the absolute value of each gradient value.
-    seed
-        Randomization seed used for reproducibility.
-
-    Attributes
-    ----------
-    weights
-        The current weights assigned to the features.
-    latents
-        The current latent weights assigned to the features.
-
-    Examples
-    --------
-
-    >>> from river import facto
-
-    >>> dataset = (
-    ...     ({'user': 'Alice', 'item': 'Superman', 'time': .12}, True),
-    ...     ({'user': 'Alice', 'item': 'Terminator', 'time': .13}, True),
-    ...     ({'user': 'Alice', 'item': 'Star Wars', 'time': .14}, True),
-    ...     ({'user': 'Alice', 'item': 'Notting Hill', 'time': .15}, False),
-    ...     ({'user': 'Alice', 'item': 'Harry Potter ', 'time': .16}, True),
-    ...     ({'user': 'Bob', 'item': 'Superman', 'time': .13}, True),
-    ...     ({'user': 'Bob', 'item': 'Terminator', 'time': .12}, True),
-    ...     ({'user': 'Bob', 'item': 'Star Wars', 'time': .16}, True),
-    ...     ({'user': 'Bob', 'item': 'Notting Hill', 'time': .10}, False)
-    ... )
-
-    >>> model = facto.FFMClassifier(
-    ...     n_factors=10,
-    ...     intercept=.5,
-    ...     seed=42,
-    ... )
-
-    >>> for x, y in dataset:
-    ...     model = model.learn_one(x, y)
-
-    >>> model.predict_one({'user': 'Bob', 'item': 'Harry Potter', 'time': .14})
-    True
-
-    References
-    ----------
-    1. [Juan, Y., Zhuang, Y., Chin, W.S. and Lin, C.J., 2016, September. Field-aware factorization machines for CTR prediction. In Proceedings of the 10th ACM Conference on Recommender Systems (pp. 43-50).](https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf)
-
-    """
-
-    def __init__(
-        self,
-        n_factors=10,
-        weight_optimizer: optim.Optimizer = None,
-        latent_optimizer: optim.Optimizer = None,
-        loss: optim.losses.BinaryLoss = None,
-        sample_normalization=False,
-        l1_weight=0.0,
-        l2_weight=0.0,
-        l1_latent=0.0,
-        l2_latent=0.0,
-        intercept=0.0,
-        intercept_lr: typing.Union[optim.schedulers.Scheduler, float] = 0.01,
-        weight_initializer: optim.initializers.Initializer = None,
-        latent_initializer: optim.initializers.Initializer = None,
-        clip_gradient=1e12,
-        seed: int = None,
-    ):
-
-        super().__init__(
-            n_factors=n_factors,
-            weight_optimizer=weight_optimizer,
-            latent_optimizer=latent_optimizer,
-            loss=optim.losses.Log() if loss is None else loss,
-            sample_normalization=sample_normalization,
-            l1_weight=l1_weight,
-            l2_weight=l2_weight,
-            l1_latent=l1_latent,
-            l2_latent=l2_latent,
-            intercept=intercept,
-            intercept_lr=intercept_lr,
-            weight_initializer=weight_initializer,
-            latent_initializer=latent_initializer,
-            clip_gradient=clip_gradient,
-            seed=seed,
-        )
-
-    def predict_proba_one(self, x):
-        x = self._ohe_cat_features(x)
-        p = utils.math.sigmoid(self._raw_dot(x))  # Convert logit to probability
-        return {False: 1.0 - p, True: p}
+import collections
+import functools
+import itertools
+import typing
+
+import numpy as np
+
+from river import base, optim, utils
+
+from .base import BaseFM
+
+__all__ = ["FwFMClassifier", "FwFMRegressor"]
+
+
+class FwFM(BaseFM):
+    """Field-Weighted Factorization Machine base class."""
+
+    def __init__(
+        self,
+        n_factors,
+        weight_optimizer,
+        latent_optimizer,
+        int_weight_optimizer,
+        loss,
+        sample_normalization,
+        l1_weight,
+        l2_weight,
+        l1_latent,
+        l2_latent,
+        intercept,
+        intercept_lr,
+        weight_initializer,
+        latent_initializer,
+        clip_gradient,
+        seed,
+    ):
+        super().__init__(
+            n_factors=n_factors,
+            weight_optimizer=weight_optimizer,
+            latent_optimizer=latent_optimizer,
+            loss=loss,
+            sample_normalization=sample_normalization,
+            l1_weight=l1_weight,
+            l2_weight=l2_weight,
+            l1_latent=l1_latent,
+            l2_latent=l2_latent,
+            intercept=intercept,
+            intercept_lr=intercept_lr,
+            weight_initializer=weight_initializer,
+            latent_initializer=latent_initializer,
+            clip_gradient=clip_gradient,
+            seed=seed,
+        )
+        if int_weight_optimizer is None:
+            self.int_weight_optimizer = optim.SGD(0.01)
+        else:
+            self.int_weight_optimizer = int_weight_optimizer
+
+        one = functools.partial(float, 1)
+        self.interaction_weights = collections.defaultdict(one)
+
+    def _init_latents(self):
+        random_latents = functools.partial(
+            self.latent_initializer, shape=self.n_factors
+        )
+        return collections.defaultdict(random_latents)
+
+    def _calculate_interactions(self, x):
+        """Calculates pairwise interactions."""
+
+        # For notational convenience
+        v, w_int, field = self.latents, self.interaction_weights, self._field
+
+        return sum(
+            x[j1] * x[j2] * np.dot(v[j1], v[j2]) * w_int[field(j1) + field(j2)]
+            for j1, j2 in itertools.combinations(x.keys(), 2)
+        )
+
+    def _calculate_weights_gradients(self, x, g_loss):
+
+        # For notational convenience
+        w, l1, l2, sign = self.weights, self.l1_weight, self.l2_weight, utils.math.sign
+
+        return {j: g_loss * xj + l1 * sign(w[j]) + l2 * w[j] for j, xj in x.items()}
+
+    def _update_latents(
+        self, x, g_loss
+    ):  # also updates interaction weights as both updates depends of each other
+
+        # For notational convenience
+        v, w_int, field = self.latents, self.interaction_weights, self._field
+        l1, l2, sign = self.l1_latent, self.l2_latent, utils.math.sign
+
+        # Precompute feature independent sum for time efficiency
+        precomputed_sum = {
+            f"{j1}_{f}": sum(
+                v[j2][f] * xj2 * w_int[field(j1) + field(j2)] for j2, xj2 in x.items()
+            )
+            for j1, xj1 in x.items()
+            for f in range(self.n_factors)
+        }
+
+        # Calculate each latent and interaction weights gradients before updating any of them
+        latent_gradients = {}
+        for j, xj in x.items():
+            latent_gradients[j] = {
+                f: g_loss
+                * (
+                    xj * precomputed_sum[f"{j}_{f}"]
+                    - v[j][f] * xj * w_int[field(j) + field(j)] ** 2
+                )
+                + l1 * sign(v[j][f])
+                + l2 * v[j][f]
+                for f in range(self.n_factors)
+            }
+
+        int_gradients = {
+            field(j1) + field(j2): g_loss * (x[j1] * x[j2] * np.dot(v[j1], v[j2]))
+            for j1, j2 in itertools.combinations(x.keys(), 2)
+        }
+
+        # Finally update the latent and interaction weights
+        for j in x.keys():
+            self.latents[j] = self.latent_optimizer.step(w=v[j], g=latent_gradients[j])
+
+        self.int_weights = self.int_weight_optimizer.step(w=w_int, g=int_gradients)
+
+
+class FwFMRegressor(FwFM, base.Regressor):
+    """Field-weighted Factorization Machine for regression.
+
+    The model equation is defined as:
+
+    $$\\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j}  + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} r_{f_j, f_{j'}} \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle x_{j} x_{j'}$$
+
+    Where $f_j$ and $f_{j'}$ are $j$ and $j'$ fields, respectively, and $\\mathbf{v}_j$ and
+    $\\mathbf{v}_{j'}$ are $j$ and $j'$ latent vectors, respectively.
+
+    For more efficiency, this model automatically one-hot encodes strings features considering them
+    as categorical variables. Field names are inferred from feature names by taking everything
+    before the first underscore: `feature_name.split('_')[0]`.
+
+    Parameters
+    ----------
+    n_factors
+        Dimensionality of the factorization or number of latent factors.
+    weight_optimizer
+        The sequential optimizer used for updating the feature weights. Note that the intercept is
+        handled separately.
+    latent_optimizer
+        The sequential optimizer used for updating the latent factors.
+    int_weight_optimizer
+        The sequential optimizer used for updating the field pairs interaction weights.
+    loss
+        The loss function to optimize for.
+    sample_normalization
+        Whether to divide each element of `x` by `x`'s L2-norm.
+    l1_weight
+        Amount of L1 regularization used to push weights towards 0.
+    l2_weight
+        Amount of L2 regularization used to push weights towards 0.
+    l1_latent
+        Amount of L1 regularization used to push latent weights towards 0.
+    l2_latent
+        Amount of L2 regularization used to push latent weights towards 0.
+    intercept
+        Initial intercept value.
+    intercept_lr
+        Learning rate scheduler used for updating the intercept. An instance of
+        `optim.schedulers.Constant` is used if a `float` is passed. No intercept will be used
+        if this is set to 0.
+    weight_initializer
+        Weights initialization scheme. Defaults to `optim.initializers.Zeros()`.
+    latent_initializer
+        Latent factors initialization scheme. Defaults to
+        `optim.initializers.Normal(mu=.0, sigma=.1, random_state=self.random_state)`.
+    clip_gradient
+        Clips the absolute value of each gradient value.
+    seed
+        Randomization seed used for reproducibility.
+
+    Attributes
+    ----------
+    weights
+        The current weights assigned to the features.
+    latents
+        The current latent weights assigned to the features.
+    interaction_weights
+        The current interaction strengths of field pairs.
+
+    Examples
+    --------
+
+    >>> from river import facto
+
+    >>> dataset = (
+    ...     ({'user': 'Alice', 'item': 'Superman'}, 8),
+    ...     ({'user': 'Alice', 'item': 'Terminator'}, 9),
+    ...     ({'user': 'Alice', 'item': 'Star Wars'}, 8),
+    ...     ({'user': 'Alice', 'item': 'Notting Hill'}, 2),
+    ...     ({'user': 'Alice', 'item': 'Harry Potter '}, 5),
+    ...     ({'user': 'Bob', 'item': 'Superman'}, 8),
+    ...     ({'user': 'Bob', 'item': 'Terminator'}, 9),
+    ...     ({'user': 'Bob', 'item': 'Star Wars'}, 8),
+    ...     ({'user': 'Bob', 'item': 'Notting Hill'}, 2)
+    ... )
+
+    >>> model = facto.FwFMRegressor(
+    ...     n_factors=10,
+    ...     intercept=5,
+    ...     seed=42,
+    ... )
+
+    >>> for x, y in dataset:
+    ...     model = model.learn_one(x, y)
+
+    >>> model.predict_one({'Bob': 1, 'Harry Potter': 1})
+    5.236501
+
+    References
+    ----------
+    [^1]: [Junwei Pan, Jian Xu, Alfonso Lobos Ruiz, Wenliang Zhao, Shengjun Pan, Yu Sun, and Quan Lu, 2018, April. Field-weighted Factorization Machines for Click-Through Rate Prediction in Display Advertising. In Proceedings of the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee, (pp. 1349–1357).](https://arxiv.org/abs/1806.03514)
+
+    """
+
+    def __init__(
+        self,
+        n_factors=10,
+        weight_optimizer: optim.Optimizer = None,
+        latent_optimizer: optim.Optimizer = None,
+        int_weight_optimizer: optim.Optimizer = None,
+        loss: optim.losses.RegressionLoss = None,
+        sample_normalization=False,
+        l1_weight=0.0,
+        l2_weight=0.0,
+        l1_latent=0.0,
+        l2_latent=0.0,
+        intercept=0.0,
+        intercept_lr: typing.Union[optim.schedulers.Scheduler, float] = 0.01,
+        weight_initializer: optim.initializers.Initializer = None,
+        latent_initializer: optim.initializers.Initializer = None,
+        clip_gradient=1e12,
+        seed: int = None,
+    ):
+
+        super().__init__(
+            n_factors=n_factors,
+            weight_optimizer=weight_optimizer,
+            int_weight_optimizer=int_weight_optimizer,
+            latent_optimizer=latent_optimizer,
+            loss=optim.losses.Squared() if loss is None else loss,
+            sample_normalization=sample_normalization,
+            l1_weight=l1_weight,
+            l2_weight=l2_weight,
+            l1_latent=l1_latent,
+            l2_latent=l2_latent,
+            intercept=intercept,
+            intercept_lr=intercept_lr,
+            weight_initializer=weight_initializer,
+            latent_initializer=latent_initializer,
+            clip_gradient=clip_gradient,
+            seed=seed,
+        )
+
+    def predict_one(self, x):
+        x = self._ohe_cat_features(x)
+        return self._raw_dot(x)
+
+
+class FwFMClassifier(FwFM, base.Classifier):
+    """Field-weighted Factorization Machine for binary classification.
+
+    The model equation is defined as:
+
+    $$\\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j}  + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} r_{f_j, f_{j'}} \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle x_{j} x_{j'}$$
+
+    Where $f_j$ and $f_{j'}$ are $j$ and $j'$ fields, respectively, and $\\mathbf{v}_j$ and
+    $\\mathbf{v}_{j'}$ are $j$ and $j'$ latent vectors, respectively.
+
+    For more efficiency, this model automatically one-hot encodes strings features considering them
+    as categorical variables. Field names are inferred from feature names by taking everything
+    before the first underscore: `feature_name.split('_')[0]`.
+
+    Parameters
+    ----------
+    n_factors
+        Dimensionality of the factorization or number of latent factors.
+    weight_optimizer
+        The sequential optimizer used for updating the feature weights. Note that the intercept is
+        handled separately.
+    latent_optimizer
+        The sequential optimizer used for updating the latent factors.
+    int_weight_optimizer
+        The sequential optimizer used for updating the field pairs interaction weights.
+    loss
+        The loss function to optimize for.
+    sample_normalization
+        Whether to divide each element of `x` by `x`'s L2-norm.
+    l1_weight
+        Amount of L1 regularization used to push weights towards 0.
+    l2_weight
+        Amount of L2 regularization used to push weights towards 0.
+    l1_latent
+        Amount of L1 regularization used to push latent weights towards 0.
+    l2_latent
+        Amount of L2 regularization used to push latent weights towards 0.
+    intercept
+        Initial intercept value.
+    intercept_lr
+        Learning rate scheduler used for updating the intercept. An instance of
+        `optim.schedulers.Constant` is used if a `float` is passed. No intercept will be used
+        if this is set to 0.
+    weight_initializer
+        Weights initialization scheme. Defaults to `optim.initializers.Zeros()`.
+    latent_initializer
+        Latent factors initialization scheme. Defaults to
+        `optim.initializers.Normal(mu=.0, sigma=.1, random_state=self.random_state)`.
+    clip_gradient
+        Clips the absolute value of each gradient value.
+    seed
+        Randomization seed used for reproducibility.
+
+    Attributes
+    ----------
+    weights
+        The current weights assigned to the features.
+    latents
+        The current latent weights assigned to the features.
+    interaction_weights
+        The current interaction strengths of field pairs.
+
+    Examples
+    --------
+
+    >>> from river import facto
+
+    >>> dataset = (
+    ...     ({'user': 'Alice', 'item': 'Superman'}, True),
+    ...     ({'user': 'Alice', 'item': 'Terminator'}, True),
+    ...     ({'user': 'Alice', 'item': 'Star Wars'}, True),
+    ...     ({'user': 'Alice', 'item': 'Notting Hill'}, False),
+    ...     ({'user': 'Alice', 'item': 'Harry Potter '}, True),
+    ...     ({'user': 'Bob', 'item': 'Superman'}, True),
+    ...     ({'user': 'Bob', 'item': 'Terminator'}, True),
+    ...     ({'user': 'Bob', 'item': 'Star Wars'}, True),
+    ...     ({'user': 'Bob', 'item': 'Notting Hill'}, False)
+    ... )
+
+    >>> model = facto.FwFMClassifier(
+    ...     n_factors=10,
+    ...     seed=42,
+    ... )
+
+    >>> for x, y in dataset:
+    ...     model = model.learn_one(x, y)
+
+    >>> model.predict_one({'Bob': 1, 'Harry Potter': 1})
+    True
+
+    References
+    ----------
+    [^1]: [Junwei Pan, Jian Xu, Alfonso Lobos Ruiz, Wenliang Zhao, Shengjun Pan, Yu Sun, and Quan Lu, 2018, April. Field-weighted Factorization Machines for Click-Through Rate Prediction in Display Advertising. In Proceedings of the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee, (pp. 1349–1357).](https://arxiv.org/abs/1806.03514)
+
+    """
+
+    def __init__(
+        self,
+        n_factors=10,
+        weight_optimizer: optim.Optimizer = None,
+        latent_optimizer: optim.Optimizer = None,
+        int_weight_optimizer: optim.Optimizer = None,
+        loss: optim.losses.BinaryLoss = None,
+        sample_normalization=False,
+        l1_weight=0.0,
+        l2_weight=0.0,
+        l1_latent=0.0,
+        l2_latent=0.0,
+        intercept=0.0,
+        intercept_lr: typing.Union[optim.schedulers.Scheduler, float] = 0.01,
+        weight_initializer: optim.initializers.Initializer = None,
+        latent_initializer: optim.initializers.Initializer = None,
+        clip_gradient=1e12,
+        seed: int = None,
+    ):
+
+        super().__init__(
+            n_factors=n_factors,
+            weight_optimizer=weight_optimizer,
+            int_weight_optimizer=int_weight_optimizer,
+            latent_optimizer=latent_optimizer,
+            loss=optim.losses.Log() if loss is None else loss,
+            sample_normalization=sample_normalization,
+            l1_weight=l1_weight,
+            l2_weight=l2_weight,
+            l1_latent=l1_latent,
+            l2_latent=l2_latent,
+            intercept=intercept,
+            intercept_lr=intercept_lr,
+            weight_initializer=weight_initializer,
+            latent_initializer=latent_initializer,
+            clip_gradient=clip_gradient,
+            seed=seed,
+        )
+
+    def predict_proba_one(self, x):
+        x = self._ohe_cat_features(x)
+        p = utils.math.sigmoid(self._raw_dot(x))  # Convert logit to probability
+        return {False: 1.0 - p, True: p}
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `river-0.8.0/river/facto/fm.py` & `river-0.9.0/river/facto/ffm.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,364 +1,381 @@
-import collections
-import functools
-import itertools
-import typing
-
-import numpy as np
-
-from river import base, optim, utils
-
-from .base import BaseFM
-
-__all__ = ["FMClassifier", "FMRegressor"]
-
-
-class FM(BaseFM):
-    """Factorization machine base class."""
-
-    def __init__(
-        self,
-        n_factors,
-        weight_optimizer,
-        latent_optimizer,
-        loss,
-        sample_normalization,
-        l1_weight,
-        l2_weight,
-        l1_latent,
-        l2_latent,
-        intercept,
-        intercept_lr,
-        weight_initializer,
-        latent_initializer,
-        clip_gradient,
-        seed,
-    ):
-        super().__init__(
-            n_factors=n_factors,
-            weight_optimizer=weight_optimizer,
-            latent_optimizer=latent_optimizer,
-            loss=loss,
-            sample_normalization=sample_normalization,
-            l1_weight=l1_weight,
-            l2_weight=l2_weight,
-            l1_latent=l1_latent,
-            l2_latent=l2_latent,
-            intercept=intercept,
-            intercept_lr=intercept_lr,
-            weight_initializer=weight_initializer,
-            latent_initializer=latent_initializer,
-            clip_gradient=clip_gradient,
-            seed=seed,
-        )
-
-    def _init_latents(self):
-        random_latents = functools.partial(
-            self.latent_initializer, shape=self.n_factors
-        )
-        return collections.defaultdict(random_latents)
-
-    def _calculate_interactions(self, x):
-        """Calculates pairwise interactions."""
-        return sum(
-            x[j1] * x[j2] * np.dot(self.latents[j1], self.latents[j2])
-            for j1, j2 in itertools.combinations(x.keys(), 2)
-        )
-
-    def _calculate_weights_gradients(self, x, g_loss):
-
-        # For notational convenience
-        w, l1, l2, sign = self.weights, self.l1_weight, self.l2_weight, utils.math.sign
-
-        return {j: g_loss * xj + l1 * sign(w[j]) + l2 * w[j] for j, xj in x.items()}
-
-    def _update_latents(self, x, g_loss):
-
-        # For notational convenience
-        v, l1, l2, sign = self.latents, self.l1_latent, self.l2_latent, utils.math.sign
-
-        # Precompute feature independent sum for time efficiency
-        precomputed_sum = {
-            f: sum(v[j][f] * xj for j, xj in x.items()) for f in range(self.n_factors)
-        }
-
-        # Calculate each latent factor gradient before updating any
-        gradients = {}
-        for j, xj in x.items():
-            gradients[j] = {
-                f: g_loss * (xj * precomputed_sum[f] - v[j][f] * xj ** 2)
-                + l1 * sign(v[j][f])
-                + l2 * v[j][f]
-                for f in range(self.n_factors)
-            }
-
-        # Finally update the latent weights
-        for j in x.keys():
-            self.latents[j] = self.latent_optimizer.step(w=v[j], g=gradients[j])
-
-
-class FMRegressor(FM, base.Regressor):
-    """Factorization Machine for regression.
-
-    The model equation is defined as:
-
-    $$\\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j}  + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle x_{j} x_{j'}$$
-
-    Where $\\mathbf{v}_j$ and $\\mathbf{v}_{j'}$ are $j$ and $j'$ latent vectors, respectively.
-
-    For more efficiency, this model automatically one-hot encodes strings features considering them
-    as categorical variables.
-
-    Parameters
-    ----------
-    n_factors
-        Dimensionality of the factorization or number of latent factors.
-    weight_optimizer
-        The sequential optimizer used for updating the feature weights. Note that
-        the intercept is handled separately.
-    latent_optimizer
-        The sequential optimizer used for updating the latent factors.
-    loss
-        The loss function to optimize for.
-    sample_normalization
-        Whether to divide each element of `x` by `x`'s L2-norm.
-    l1_weight
-        Amount of L1 regularization used to push weights towards 0.
-    l2_weight
-        Amount of L2 regularization used to push weights towards 0.
-    l1_latent
-        Amount of L1 regularization used to push latent weights towards 0.
-    l2_latent
-        Amount of L2 regularization used to push latent weights towards 0.
-    intercept
-        Initial intercept value.
-    intercept_lr
-        Learning rate scheduler used for updating the intercept. An instance of
-        `optim.schedulers.Constant` is used if a `float` is passed. No intercept will be used
-        if this is set to 0.
-    weight_initializer
-        Weights initialization scheme. Defaults to `optim.initializers.Zeros()`.
-    latent_initializer
-        Latent factors initialization scheme. Defaults to
-        `optim.initializers.Normal(mu=.0, sigma=.1, random_state=self.random_state)`.
-    clip_gradient
-        Clips the absolute value of each gradient value.
-    seed
-        Randomization seed used for reproducibility.
-
-    Attributes
-    ----------
-    weights
-        The current weights assigned to the features.
-    latents
-        The current latent weights assigned to the features.
-
-    Examples
-    --------
-
-    >>> from river import facto
-
-    >>> dataset = (
-    ...     ({'user': 'Alice', 'item': 'Superman'}, 8),
-    ...     ({'user': 'Alice', 'item': 'Terminator'}, 9),
-    ...     ({'user': 'Alice', 'item': 'Star Wars'}, 8),
-    ...     ({'user': 'Alice', 'item': 'Notting Hill'}, 2),
-    ...     ({'user': 'Alice', 'item': 'Harry Potter '}, 5),
-    ...     ({'user': 'Bob', 'item': 'Superman'}, 8),
-    ...     ({'user': 'Bob', 'item': 'Terminator'}, 9),
-    ...     ({'user': 'Bob', 'item': 'Star Wars'}, 8),
-    ...     ({'user': 'Bob', 'item': 'Notting Hill'}, 2)
-    ... )
-
-    >>> model = facto.FMRegressor(
-    ...     n_factors=10,
-    ...     intercept=5,
-    ...     seed=42,
-    ... )
-
-    >>> for x, y in dataset:
-    ...     _ = model.learn_one(x, y)
-
-    >>> model.predict_one({'Bob': 1, 'Harry Potter': 1})
-    5.236504
-
-    References
-    ----------
-    [^1]: [Rendle, S., 2010, December. Factorization machines. In 2010 IEEE International Conference on Data Mining (pp. 995-1000). IEEE.](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)
-    [^2]: [Rendle, S., 2012, May. Factorization Machines with libFM. In ACM Transactions on Intelligent Systems and Technology 3, 3, Article 57, 22 pages.](https://analyticsconsultores.com.mx/wp-content/uploads/2019/03/Factorization-Machines-with-libFM-Steffen-Rendle-University-of-Konstanz2012-.pdf)
-
-    """
-
-    def __init__(
-        self,
-        n_factors=10,
-        weight_optimizer: optim.Optimizer = None,
-        latent_optimizer: optim.Optimizer = None,
-        loss: optim.losses.RegressionLoss = None,
-        sample_normalization=False,
-        l1_weight=0.0,
-        l2_weight=0.0,
-        l1_latent=0.0,
-        l2_latent=0.0,
-        intercept=0.0,
-        intercept_lr: typing.Union[optim.schedulers.Scheduler, float] = 0.01,
-        weight_initializer: optim.initializers.Initializer = None,
-        latent_initializer: optim.initializers.Initializer = None,
-        clip_gradient=1e12,
-        seed: int = None,
-    ):
-
-        super().__init__(
-            n_factors=n_factors,
-            weight_optimizer=weight_optimizer,
-            latent_optimizer=latent_optimizer,
-            loss=optim.losses.Squared() if loss is None else loss,
-            sample_normalization=sample_normalization,
-            l1_weight=l1_weight,
-            l2_weight=l2_weight,
-            l1_latent=l1_latent,
-            l2_latent=l2_latent,
-            intercept=intercept,
-            intercept_lr=intercept_lr,
-            weight_initializer=weight_initializer,
-            latent_initializer=latent_initializer,
-            clip_gradient=clip_gradient,
-            seed=seed,
-        )
-
-    def predict_one(self, x):
-        x = self._ohe_cat_features(x)
-        return self._raw_dot(x)
-
-
-class FMClassifier(FM, base.Classifier):
-    """Factorization Machine for binary classification.
-
-    The model equation is defined as:
-
-    $$\\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j}  + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle x_{j} x_{j'}$$
-
-    Where $\\mathbf{v}_j$ and $\\mathbf{v}_{j'}$ are $j$ and $j'$ latent vectors, respectively.
-
-    For more efficiency, this model automatically one-hot encodes strings features considering them
-    as categorical variables.
-
-    Parameters
-    ----------
-    n_factors
-        Dimensionality of the factorization or number of latent factors.
-    weight_optimizer
-        The sequential optimizer used for updating the feature weights. Note that the intercept is
-        handled separately.
-    latent_optimizer
-        The sequential optimizer used for updating the latent factors.
-    loss
-        The loss function to optimize for.
-    sample_normalization
-        Whether to divide each element of `x` by `x`'s L2-norm.
-    l1_weight
-        Amount of L1 regularization used to push weights towards 0.
-    l2_weight
-        Amount of L2 regularization used to push weights towards 0.
-    l1_latent
-        Amount of L1 regularization used to push latent weights towards 0.
-    l2_latent
-        Amount of L2 regularization used to push latent weights towards 0.
-    intercept
-        Initial intercept value.
-    intercept_lr
-        Learning rate scheduler used for updating the intercept. An instance of
-        `optim.schedulers.Constant` is used if a `float` is passed. No intercept will be used
-        if this is set to 0.
-    weight_initializer
-        Weights initialization scheme. Defaults to `optim.initializers.Zeros()`.
-    latent_initializer
-        Latent factors initialization scheme. Defaults to
-        `optim.initializers.Normal(mu=.0, sigma=.1, random_state=self.random_state)`.
-    clip_gradient
-        Clips the absolute value of each gradient value.
-    seed
-        Randomization seed used for reproducibility.
-
-    Attributes
-    ----------
-    weights
-        The current weights assigned to the features.
-    latents
-        The current latent weights assigned to the features.
-
-    Examples
-    --------
-
-    >>> from river import facto
-
-    >>> dataset = (
-    ...     ({'user': 'Alice', 'item': 'Superman'}, True),
-    ...     ({'user': 'Alice', 'item': 'Terminator'}, True),
-    ...     ({'user': 'Alice', 'item': 'Star Wars'}, True),
-    ...     ({'user': 'Alice', 'item': 'Notting Hill'}, False),
-    ...     ({'user': 'Alice', 'item': 'Harry Potter '}, True),
-    ...     ({'user': 'Bob', 'item': 'Superman'}, True),
-    ...     ({'user': 'Bob', 'item': 'Terminator'}, True),
-    ...     ({'user': 'Bob', 'item': 'Star Wars'}, True),
-    ...     ({'user': 'Bob', 'item': 'Notting Hill'}, False)
-    ... )
-
-    >>> model = facto.FMClassifier(
-    ...     n_factors=10,
-    ...     seed=42,
-    ... )
-
-    >>> for x, y in dataset:
-    ...     _ = model.learn_one(x, y)
-
-    >>> model.predict_one({'Bob': 1, 'Harry Potter': 1})
-    True
-
-    References
-    ----------
-    [^1]: [Rendle, S., 2010, December. Factorization machines. In 2010 IEEE International Conference on Data Mining (pp. 995-1000). IEEE.](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)
-    [^2]: [Rendle, S., 2012, May. Factorization Machines with libFM. In ACM Transactions on Intelligent Systems and Technology 3, 3, Article 57, 22 pages.](https://analyticsconsultores.com.mx/wp-content/uploads/2019/03/Factorization-Machines-with-libFM-Steffen-Rendle-University-of-Konstanz2012-.pdf)
-
-    """
-
-    def __init__(
-        self,
-        n_factors=10,
-        weight_optimizer: optim.Optimizer = None,
-        latent_optimizer: optim.Optimizer = None,
-        loss: optim.losses.BinaryLoss = None,
-        sample_normalization=False,
-        l1_weight=0.0,
-        l2_weight=0.0,
-        l1_latent=0.0,
-        l2_latent=0.0,
-        intercept=0.0,
-        intercept_lr: typing.Union[optim.schedulers.Scheduler, float] = 0.01,
-        weight_initializer: optim.initializers.Initializer = None,
-        latent_initializer: optim.initializers.Initializer = None,
-        clip_gradient=1e12,
-        seed: int = None,
-    ):
-
-        super().__init__(
-            n_factors=n_factors,
-            weight_optimizer=weight_optimizer,
-            latent_optimizer=latent_optimizer,
-            loss=optim.losses.Log() if loss is None else loss,
-            sample_normalization=sample_normalization,
-            l1_weight=l1_weight,
-            l2_weight=l2_weight,
-            l1_latent=l1_latent,
-            l2_latent=l2_latent,
-            intercept=intercept,
-            intercept_lr=intercept_lr,
-            weight_initializer=weight_initializer,
-            latent_initializer=latent_initializer,
-            clip_gradient=clip_gradient,
-            seed=seed,
-        )
-
-    def predict_proba_one(self, x):
-        x = self._ohe_cat_features(x)
-        p = utils.math.sigmoid(self._raw_dot(x))  # Convert logit to probability
-        return {False: 1.0 - p, True: p}
+import collections
+import functools
+import itertools
+import typing
+
+import numpy as np
+
+from river import base, optim, utils
+
+from .base import BaseFM
+
+__all__ = ["FFMClassifier", "FFMRegressor"]
+
+
+class FFM(BaseFM):
+    """Field-aware Factorization Machine base class."""
+
+    def __init__(
+        self,
+        n_factors,
+        weight_optimizer,
+        latent_optimizer,
+        loss,
+        sample_normalization,
+        l1_weight,
+        l2_weight,
+        l1_latent,
+        l2_latent,
+        intercept,
+        intercept_lr,
+        weight_initializer,
+        latent_initializer,
+        clip_gradient,
+        seed,
+    ):
+        super().__init__(
+            n_factors=n_factors,
+            weight_optimizer=weight_optimizer,
+            latent_optimizer=latent_optimizer,
+            loss=loss,
+            sample_normalization=sample_normalization,
+            l1_weight=l1_weight,
+            l2_weight=l2_weight,
+            l1_latent=l1_latent,
+            l2_latent=l2_latent,
+            intercept=intercept,
+            intercept_lr=intercept_lr,
+            weight_initializer=weight_initializer,
+            latent_initializer=latent_initializer,
+            clip_gradient=clip_gradient,
+            seed=seed,
+        )
+
+    def _init_latents(self):
+        random_latents = functools.partial(
+            self.latent_initializer, shape=self.n_factors
+        )
+        field_latents_dict = functools.partial(collections.defaultdict, random_latents)
+        return collections.defaultdict(field_latents_dict)
+
+    def _calculate_interactions(self, x):
+        """Calculates pairwise interactions."""
+        field = self._field
+        return sum(
+            x[j1]
+            * x[j2]
+            * np.dot(self.latents[j1][field(j2)], self.latents[j2][field(j1)])
+            for j1, j2 in itertools.combinations(x.keys(), 2)
+        )
+
+    def _calculate_weights_gradients(self, x, g_loss):
+
+        # For notational convenience
+        w, l1, l2, sign = self.weights, self.l1_weight, self.l2_weight, utils.math.sign
+
+        return {j: g_loss * xj + l1 * sign(w[j]) + l2 * w[j] for j, xj in x.items()}
+
+    def _update_latents(self, x, g_loss):
+
+        # For notational convenience
+        v, l1, l2 = self.latents, self.l1_latent, self.l2_latent
+        sign, field = utils.math.sign, self._field
+
+        # Calculate each latent factor gradient before updating any
+        latent_gradient = collections.defaultdict(
+            lambda: collections.defaultdict(lambda: collections.defaultdict(float))
+        )
+
+        for j1, j2 in itertools.combinations(x.keys(), 2):
+            xj1_xj2 = x[j1] * x[j2]
+            field_j1, field_j2 = field(j1), field(j2)
+
+            for f in range(self.n_factors):
+                latent_gradient[j1][field_j2][f] += xj1_xj2 * v[j2][field_j1][f]
+                latent_gradient[j2][field_j1][f] += xj1_xj2 * v[j1][field_j2][f]
+
+        # Finally update the latent weights
+        for j in x.keys():
+            for field in latent_gradient[j].keys():
+                self.latents[j][field] = self.latent_optimizer.step(
+                    w=v[j][field],
+                    g={
+                        f: g_loss * latent_gradient[j][field][f]
+                        + l1 * sign(v[j][field][f])
+                        + 2.0 * l2 * v[j][field][f]
+                        for f in range(self.n_factors)
+                    },
+                )
+
+
+class FFMRegressor(FFM, base.Regressor):
+    """Field-aware Factorization Machine for regression.
+
+    The model equation is defined by:
+
+    $$\\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j}  + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} \\langle \\mathbf{v}_{j, f_{j'}}, \\mathbf{v}_{j', f_j} \\rangle x_{j} x_{j'}$$
+
+    Where \\mathbf{v}_{j, f_{j'}} is the latent vector corresponding to $j$ feature for $f_{j'}$
+    field, and \\mathbf{v}_{j', f_j} is the latent vector corresponding to $j'$ feature for $f_j$
+    field.
+
+    For more efficiency, this model automatically one-hot encodes strings features considering them
+    as categorical variables. Field names are inferred from feature names by taking everything
+    before the first underscore: `feature_name.split('_')[0]`.
+
+    Parameters
+    ----------
+    n_factors
+        Dimensionality of the factorization or number of latent factors.
+    weight_optimizer
+        The sequential optimizer used for updating the feature weights. Note that the intercept is
+        handled separately.
+    latent_optimizer
+        The sequential optimizer used for updating the latent factors.
+    loss
+        The loss function to optimize for.
+    sample_normalization
+        Whether to divide each element of `x` by `x`'s L2-norm.
+    l1_weight
+        Amount of L1 regularization used to push weights towards 0.
+    l2_weight
+        Amount of L2 regularization used to push weights towards 0.
+    l1_latent
+        Amount of L1 regularization used to push latent weights towards 0.
+    l2_latent
+        Amount of L2 regularization used to push latent weights towards 0.
+    intercept
+        Initial intercept value.
+    intercept_lr
+        Learning rate scheduler used for updating the intercept. An instance of
+        `optim.schedulers.Constant` is used if a `float` is passed. No intercept will be used
+        if this is set to 0.
+    weight_initializer
+        Weights initialization scheme. Defaults to `optim.initializers.Zeros()`.
+    latent_initializer
+        Latent factors initialization scheme. Defaults to
+        `optim.initializers.Normal(mu=.0, sigma=.1, random_state=self.random_state)`.
+    clip_gradient
+        Clips the absolute value of each gradient value.
+    seed
+        Randomization seed used for reproducibility.
+
+    Attributes
+    ----------
+    weights
+        The current weights assigned to the features.
+    latents
+        The current latent weights assigned to the features.
+
+    Examples
+    --------
+
+    >>> from river import facto
+
+    >>> dataset = (
+    ...     ({'user': 'Alice', 'item': 'Superman', 'time': .12}, 8),
+    ...     ({'user': 'Alice', 'item': 'Terminator', 'time': .13}, 9),
+    ...     ({'user': 'Alice', 'item': 'Star Wars', 'time': .14}, 8),
+    ...     ({'user': 'Alice', 'item': 'Notting Hill', 'time': .15}, 2),
+    ...     ({'user': 'Alice', 'item': 'Harry Potter ', 'time': .16}, 5),
+    ...     ({'user': 'Bob', 'item': 'Superman', 'time': .13}, 8),
+    ...     ({'user': 'Bob', 'item': 'Terminator', 'time': .12}, 9),
+    ...     ({'user': 'Bob', 'item': 'Star Wars', 'time': .16}, 8),
+    ...     ({'user': 'Bob', 'item': 'Notting Hill', 'time': .10}, 2)
+    ... )
+
+    >>> model = facto.FFMRegressor(
+    ...     n_factors=10,
+    ...     intercept=5,
+    ...     seed=42,
+    ... )
+
+    >>> for x, y in dataset:
+    ...     model = model.learn_one(x, y)
+
+    >>> model.predict_one({'user': 'Bob', 'item': 'Harry Potter', 'time': .14})
+    5.319945
+
+    References
+    ----------
+    [^1]: [Juan, Y., Zhuang, Y., Chin, W.S. and Lin, C.J., 2016, September. Field-aware factorization machines for CTR prediction. In Proceedings of the 10th ACM Conference on Recommender Systems (pp. 43-50).](https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf)
+
+    """
+
+    def __init__(
+        self,
+        n_factors=10,
+        weight_optimizer: optim.Optimizer = None,
+        latent_optimizer: optim.Optimizer = None,
+        loss: optim.losses.RegressionLoss = None,
+        sample_normalization=False,
+        l1_weight=0.0,
+        l2_weight=0.0,
+        l1_latent=0.0,
+        l2_latent=0.0,
+        intercept=0.0,
+        intercept_lr: typing.Union[optim.schedulers.Scheduler, float] = 0.01,
+        weight_initializer: optim.initializers.Initializer = None,
+        latent_initializer: optim.initializers.Initializer = None,
+        clip_gradient=1e12,
+        seed: int = None,
+    ):
+
+        super().__init__(
+            n_factors=n_factors,
+            weight_optimizer=weight_optimizer,
+            latent_optimizer=latent_optimizer,
+            loss=optim.losses.Squared() if loss is None else loss,
+            sample_normalization=sample_normalization,
+            l1_weight=l1_weight,
+            l2_weight=l2_weight,
+            l1_latent=l1_latent,
+            l2_latent=l2_latent,
+            intercept=intercept,
+            intercept_lr=intercept_lr,
+            weight_initializer=weight_initializer,
+            latent_initializer=latent_initializer,
+            clip_gradient=clip_gradient,
+            seed=seed,
+        )
+
+    def predict_one(self, x):
+        x = self._ohe_cat_features(x)
+        return self._raw_dot(x)
+
+
+class FFMClassifier(FFM, base.Classifier):
+    """Field-aware Factorization Machine for binary classification.
+
+    The model equation is defined by:
+
+    $$\\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j}  + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} \\langle \\mathbf{v}_{j, f_{j'}}, \\mathbf{v}_{j', f_j} \\rangle x_{j} x_{j'}$$
+
+    Where \\mathbf{v}_{j, f_{j'}} is the latent vector corresponding to $j$ feature for $f_{j'}$
+    field, and \\mathbf{v}_{j', f_j} is the latent vector corresponding to $j'$ feature for $f_j$
+    field.
+
+    For more efficiency, this model automatically one-hot encodes strings features considering them
+    as categorical variables. Field names are inferred from feature names by taking everything
+    before the first underscore: `feature_name.split('_')[0]`.
+
+    Parameters
+    ----------
+    n_factors
+        Dimensionality of the factorization or number of latent factors.
+    weight_optimizer
+        The sequential optimizer used for updating the feature weights. Note that the intercept is
+        handled separately.
+    latent_optimizer
+        The sequential optimizer used for updating the latent factors.
+    loss
+        The loss function to optimize for.
+    sample_normalization
+        Whether to divide each element of `x` by `x`'s L2-norm.
+    l1_weight
+        Amount of L1 regularization used to push weights towards 0.
+    l2_weight
+        Amount of L2 regularization used to push weights towards 0.
+    l1_latent
+        Amount of L1 regularization used to push latent weights towards 0.
+    l2_latent
+        Amount of L2 regularization used to push latent weights towards 0.
+    intercept
+        Initial intercept value.
+    intercept_lr
+        Learning rate scheduler used for updating the intercept. An instance of
+        `optim.schedulers.Constant` is used if a `float` is passed. No intercept will be used
+        if this is set to 0.
+    weight_initializer
+        Weights initialization scheme. Defaults to `optim.initializers.Zeros()`.
+    latent_initializer
+        Latent factors initialization scheme. Defaults to
+        `optim.initializers.Normal(mu=.0, sigma=.1, random_state=self.random_state)`.
+    clip_gradient
+        Clips the absolute value of each gradient value.
+    seed
+        Randomization seed used for reproducibility.
+
+    Attributes
+    ----------
+    weights
+        The current weights assigned to the features.
+    latents
+        The current latent weights assigned to the features.
+
+    Examples
+    --------
+
+    >>> from river import facto
+
+    >>> dataset = (
+    ...     ({'user': 'Alice', 'item': 'Superman', 'time': .12}, True),
+    ...     ({'user': 'Alice', 'item': 'Terminator', 'time': .13}, True),
+    ...     ({'user': 'Alice', 'item': 'Star Wars', 'time': .14}, True),
+    ...     ({'user': 'Alice', 'item': 'Notting Hill', 'time': .15}, False),
+    ...     ({'user': 'Alice', 'item': 'Harry Potter ', 'time': .16}, True),
+    ...     ({'user': 'Bob', 'item': 'Superman', 'time': .13}, True),
+    ...     ({'user': 'Bob', 'item': 'Terminator', 'time': .12}, True),
+    ...     ({'user': 'Bob', 'item': 'Star Wars', 'time': .16}, True),
+    ...     ({'user': 'Bob', 'item': 'Notting Hill', 'time': .10}, False)
+    ... )
+
+    >>> model = facto.FFMClassifier(
+    ...     n_factors=10,
+    ...     intercept=.5,
+    ...     seed=42,
+    ... )
+
+    >>> for x, y in dataset:
+    ...     model = model.learn_one(x, y)
+
+    >>> model.predict_one({'user': 'Bob', 'item': 'Harry Potter', 'time': .14})
+    True
+
+    References
+    ----------
+    1. [Juan, Y., Zhuang, Y., Chin, W.S. and Lin, C.J., 2016, September. Field-aware factorization machines for CTR prediction. In Proceedings of the 10th ACM Conference on Recommender Systems (pp. 43-50).](https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf)
+
+    """
+
+    def __init__(
+        self,
+        n_factors=10,
+        weight_optimizer: optim.Optimizer = None,
+        latent_optimizer: optim.Optimizer = None,
+        loss: optim.losses.BinaryLoss = None,
+        sample_normalization=False,
+        l1_weight=0.0,
+        l2_weight=0.0,
+        l1_latent=0.0,
+        l2_latent=0.0,
+        intercept=0.0,
+        intercept_lr: typing.Union[optim.schedulers.Scheduler, float] = 0.01,
+        weight_initializer: optim.initializers.Initializer = None,
+        latent_initializer: optim.initializers.Initializer = None,
+        clip_gradient=1e12,
+        seed: int = None,
+    ):
+
+        super().__init__(
+            n_factors=n_factors,
+            weight_optimizer=weight_optimizer,
+            latent_optimizer=latent_optimizer,
+            loss=optim.losses.Log() if loss is None else loss,
+            sample_normalization=sample_normalization,
+            l1_weight=l1_weight,
+            l2_weight=l2_weight,
+            l1_latent=l1_latent,
+            l2_latent=l2_latent,
+            intercept=intercept,
+            intercept_lr=intercept_lr,
+            weight_initializer=weight_initializer,
+            latent_initializer=latent_initializer,
+            clip_gradient=clip_gradient,
+            seed=seed,
+        )
+
+    def predict_proba_one(self, x):
+        x = self._ohe_cat_features(x)
+        p = utils.math.sigmoid(self._raw_dot(x))  # Convert logit to probability
+        return {False: 1.0 - p, True: p}
```

### Comparing `river-0.8.0/river/facto/hofm.py` & `river-0.9.0/river/facto/hofm.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,403 +1,403 @@
-import collections
-import functools
-import itertools
-import typing
-
-from river import base, optim, utils
-
-from .base import BaseFM
-
-__all__ = ["HOFMClassifier", "HOFMRegressor"]
-
-
-class HOFM(BaseFM):
-    """Higher-Order Factorization Machine base class."""
-
-    def __init__(
-        self,
-        degree,
-        n_factors,
-        weight_optimizer,
-        latent_optimizer,
-        loss,
-        sample_normalization,
-        l1_weight,
-        l2_weight,
-        l1_latent,
-        l2_latent,
-        intercept,
-        intercept_lr,
-        weight_initializer,
-        latent_initializer,
-        clip_gradient,
-        seed,
-    ):
-        super().__init__(
-            n_factors=n_factors,
-            weight_optimizer=weight_optimizer,
-            latent_optimizer=latent_optimizer,
-            loss=loss,
-            sample_normalization=sample_normalization,
-            l1_weight=l1_weight,
-            l2_weight=l2_weight,
-            l1_latent=l1_latent,
-            l2_latent=l2_latent,
-            intercept=intercept,
-            intercept_lr=intercept_lr,
-            weight_initializer=weight_initializer,
-            latent_initializer=latent_initializer,
-            clip_gradient=clip_gradient,
-            seed=seed,
-        )
-        self.degree = degree
-
-    def _init_latents(self):
-        random_latents = functools.partial(
-            self.latent_initializer, shape=self.n_factors
-        )
-        order_latents_dict = functools.partial(collections.defaultdict, random_latents)
-        return collections.defaultdict(order_latents_dict)
-
-    def _calculate_interactions(self, x):
-        """Calculates greater than unary interactions."""
-        return sum(
-            self._calculate_interaction(x, d, combination)
-            for d in range(2, self.degree + 1)
-            for combination in itertools.combinations(x.keys(), d)
-        )
-
-    def _calculate_interaction(self, x, d, combination):
-        feature_product = functools.reduce(
-            lambda x, y: x * y, (x[j] for j in combination)
-        )
-        latent_scalar_product = sum(
-            functools.reduce(
-                lambda x, y: x * y, (self.latents[j][d][f] for j in combination)
-            )
-            for f in range(self.n_factors)
-        )
-        return feature_product * latent_scalar_product
-
-    def _calculate_weights_gradients(self, x, g_loss):
-
-        # For notational convenience
-        w, l1, l2, sign = self.weights, self.l1_weight, self.l2_weight, utils.math.sign
-
-        return {j: g_loss * xj + l1 * sign(w[j]) + l2 * w[j] for j, xj in x.items()}
-
-    def _update_latents(self, x, g_loss):
-
-        # For notational convenience
-        v, l1, l2, sign = self.latents, self.l1_latent, self.l2_latent, utils.math.sign
-
-        # Calculate each latent factor gradient before updating any
-        gradients = collections.defaultdict(
-            lambda: collections.defaultdict(lambda: collections.defaultdict(float))
-        )
-
-        for d in range(2, self.degree + 1):
-
-            for combination in itertools.combinations(x.keys(), d):
-                feature_product = functools.reduce(
-                    lambda x, y: x * y, (x[j] for j in combination)
-                )
-
-                for f in range(self.n_factors):
-                    latent_product = functools.reduce(
-                        lambda x, y: x * y, (v[j][d][f] for j in combination)
-                    )
-
-                    for j in combination:
-                        gradients[j][d][f] += (
-                            feature_product * latent_product / v[j][d][f]
-                        )
-
-        # Finally update the latent weights
-        for j in x.keys():
-            for d in range(2, self.degree + 1):
-                self.latents[j][d] = self.latent_optimizer.step(
-                    w=v[j][d],
-                    g={
-                        f: g_loss * gradients[j][d][f]
-                        + l1 * sign(v[j][d][f])
-                        + 2 * l2 * v[j][d][f]
-                        for f in range(self.n_factors)
-                    },
-                )
-
-
-class HOFMRegressor(HOFM, base.Regressor):
-    """Higher-Order Factorization Machine for regression.
-
-    The model equation is defined as:
-
-    $$\\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j}  + \\sum_{l=2}^{d} \\sum_{j_1=1}^{p} \\cdots \\sum_{j_l=j_{l-1}+1}^{p} \\left(\\prod_{j'=1}^{l} x_{j_{j'}} \\right) \\left(\\sum_{f=1}^{k_l} \\prod_{j'=1}^{l} v_{j_{j'}, f}^{(l)} \\right)$$
-
-    For more efficiency, this model automatically one-hot encodes strings features considering
-    them as categorical variables.
-
-    Parameters
-    ----------
-    degree
-        Polynomial degree or model order.
-    n_factors
-        Dimensionality of the factorization or number of latent factors.
-    weight_optimizer
-        The sequential optimizer used for updating the feature weights. Note thatthe intercept is
-        handled separately.
-    latent_optimizer
-        The sequential optimizer used for updating the latent factors.
-    int_weight_optimizer
-        The sequential optimizer used for updating the field pairs interaction weights.
-    loss
-        The loss function to optimize for.
-    sample_normalization
-        Whether to divide each element of `x` by `x`'s L2-norm.
-    l1_weight
-        Amount of L1 regularization used to push weights towards 0.
-    l2_weight
-        Amount of L2 regularization used to push weights towards 0.
-    l1_latent
-        Amount of L1 regularization used to push latent weights towards 0.
-    l2_latent
-        Amount of L2 regularization used to push latent weights towards 0.
-    intercept
-        Initial intercept value.
-    intercept_lr
-        Learning rate scheduler used for updating the intercept. An instance of
-        `optim.schedulers.Constant` is used if a `float` is passed. No intercept will be used
-        if this is set to 0.
-    weight_initializer
-        Weights initialization scheme. Defaults to `optim.initializers.Zeros()`.
-    latent_initializer
-        Latent factors initialization scheme. Defaults to
-        `optim.initializers.Normal(mu=.0, sigma=.1, random_state=self.random_state)`.
-    clip_gradient
-        Clips the absolute value of each gradient value.
-    seed
-        Randomization seed used for reproducibility.
-
-    Attributes
-    ----------
-    weights
-        The current weights assigned to the features.
-    latents
-        The current latent weights assigned to the features.
-
-    Examples
-    --------
-
-    >>> from river import facto
-
-    >>> dataset = (
-    ...     ({'user': 'Alice', 'item': 'Superman', 'time': .12}, 8),
-    ...     ({'user': 'Alice', 'item': 'Terminator', 'time': .13}, 9),
-    ...     ({'user': 'Alice', 'item': 'Star Wars', 'time': .14}, 8),
-    ...     ({'user': 'Alice', 'item': 'Notting Hill', 'time': .15}, 2),
-    ...     ({'user': 'Alice', 'item': 'Harry Potter ', 'time': .16}, 5),
-    ...     ({'user': 'Bob', 'item': 'Superman', 'time': .13}, 8),
-    ...     ({'user': 'Bob', 'item': 'Terminator', 'time': .12}, 9),
-    ...     ({'user': 'Bob', 'item': 'Star Wars', 'time': .16}, 8),
-    ...     ({'user': 'Bob', 'item': 'Notting Hill', 'time': .10}, 2)
-    ... )
-
-    >>> model = facto.HOFMRegressor(
-    ...     degree=3,
-    ...     n_factors=10,
-    ...     intercept=5,
-    ...     seed=42,
-    ... )
-
-    >>> for x, y in dataset:
-    ...     _ = model.learn_one(x, y)
-
-    >>> model.predict_one({'user': 'Bob', 'item': 'Harry Potter', 'time': .14})
-    5.311745
-
-    References
-    ----------
-    [^1]: [Rendle, S., 2010, December. Factorization machines. In 2010 IEEE International Conference on Data Mining (pp. 995-1000). IEEE.](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)
-
-    """
-
-    def __init__(
-        self,
-        degree=3,
-        n_factors=10,
-        weight_optimizer: optim.Optimizer = None,
-        latent_optimizer: optim.Optimizer = None,
-        loss: optim.losses.RegressionLoss = None,
-        sample_normalization=False,
-        l1_weight=0.0,
-        l2_weight=0.0,
-        l1_latent=0.0,
-        l2_latent=0.0,
-        intercept=0.0,
-        intercept_lr: typing.Union[optim.schedulers.Scheduler, float] = 0.01,
-        weight_initializer: optim.initializers.Initializer = None,
-        latent_initializer: optim.initializers.Initializer = None,
-        clip_gradient=1e12,
-        seed: int = None,
-    ):
-
-        super().__init__(
-            degree=degree,
-            n_factors=n_factors,
-            weight_optimizer=weight_optimizer,
-            latent_optimizer=latent_optimizer,
-            loss=optim.losses.Squared() if loss is None else loss,
-            sample_normalization=sample_normalization,
-            l1_weight=l1_weight,
-            l2_weight=l2_weight,
-            l1_latent=l1_latent,
-            l2_latent=l2_latent,
-            intercept=intercept,
-            intercept_lr=intercept_lr,
-            weight_initializer=weight_initializer,
-            latent_initializer=latent_initializer,
-            clip_gradient=clip_gradient,
-            seed=seed,
-        )
-
-    def predict_one(self, x):
-        x = self._ohe_cat_features(x)
-        return self._raw_dot(x)
-
-
-class HOFMClassifier(HOFM, base.Classifier):
-    """Higher-Order Factorization Machine for binary classification.
-
-    The model equation is defined as:
-
-    $$\\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j}  + \\sum_{l=2}^{d} \\sum_{j_1=1}^{p} \\cdots \\sum_{j_l=j_{l-1}+1}^{p} \\left(\\prod_{j'=1}^{l} x_{j_{j'}} \\right) \\left(\\sum_{f=1}^{k_l} \\prod_{j'=1}^{l} v_{j_{j'}, f}^{(l)} \\right)$$
-
-    For more efficiency, this model automatically one-hot encodes strings features considering
-    them as categorical variables.
-
-    Parameters
-    ----------
-    degree
-        Polynomial degree or model order.
-    n_factors
-        Dimensionality of the factorization or number of latent factors.
-    weight_optimizer
-        The sequential optimizer used for updating the feature weights. Note that the intercept is
-        handled separately.
-    latent_optimizer
-        The sequential optimizer used for updating the latent factors.
-    int_weight_optimizer
-        The sequential optimizer used for updating the field pairs interaction weights.
-    loss
-        The loss function to optimize for.
-    sample_normalization
-        Whether to divide each element of `x` by `x`'s L2-norm.
-    l1_weight
-        Amount of L1 regularization used to push weights towards 0.
-    l2_weight
-        Amount of L2 regularization used to push weights towards 0.
-    l1_latent
-        Amount of L1 regularization used to push latent weights towards 0.
-    l2_latent
-        Amount of L2 regularization used to push latent weights towards 0.
-    intercept
-        Initial intercept value.
-    intercept_lr
-        Learning rate scheduler used for updating the intercept. An instance of
-        `optim.schedulers.Constant` is used if a `float` is passed. No intercept will be used
-        if this is set to 0.
-    weight_initializer
-        Weights initialization scheme. Defaults to `optim.initializers.Zeros()`.
-    latent_initializer
-        Latent factors initialization scheme. Defaults to
-        `optim.initializers.Normal(mu=.0, sigma=.1, random_state=self.random_state)`.
-    clip_gradient
-        Clips the absolute value of each gradient value.
-    seed
-        Randomization seed used for reproducibility.
-
-    Attributes
-    ----------
-    weights
-        The current weights assigned to the features.
-    latents
-        The current latent weights assigned to the features.
-
-    Examples
-    --------
-
-    >>> from river import facto
-
-    >>> dataset = (
-    ...     ({'user': 'Alice', 'item': 'Superman', 'time': .12}, True),
-    ...     ({'user': 'Alice', 'item': 'Terminator', 'time': .13}, True),
-    ...     ({'user': 'Alice', 'item': 'Star Wars', 'time': .14}, True),
-    ...     ({'user': 'Alice', 'item': 'Notting Hill', 'time': .15}, False),
-    ...     ({'user': 'Alice', 'item': 'Harry Potter ', 'time': .16}, True),
-    ...     ({'user': 'Bob', 'item': 'Superman', 'time': .13}, True),
-    ...     ({'user': 'Bob', 'item': 'Terminator', 'time': .12}, True),
-    ...     ({'user': 'Bob', 'item': 'Star Wars', 'time': .16}, True),
-    ...     ({'user': 'Bob', 'item': 'Notting Hill', 'time': .10}, False)
-    ... )
-
-    >>> model = facto.HOFMClassifier(
-    ...     degree=3,
-    ...     n_factors=10,
-    ...     intercept=.5,
-    ...     seed=42,
-    ... )
-
-    >>> for x, y in dataset:
-    ...     _ = model.learn_one(x, y)
-
-    >>> model.predict_one({'user': 'Bob', 'item': 'Harry Potter', 'time': .14})
-    True
-
-    References
-    ----------
-    [^1]: [Rendle, S., 2010, December. Factorization machines. In 2010 IEEE International Conference on Data Mining (pp. 995-1000). IEEE.](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)
-
-    """
-
-    def __init__(
-        self,
-        degree=3,
-        n_factors=10,
-        weight_optimizer: optim.Optimizer = None,
-        latent_optimizer: optim.Optimizer = None,
-        loss: optim.losses.BinaryLoss = None,
-        sample_normalization=False,
-        l1_weight=0.0,
-        l2_weight=0.0,
-        l1_latent=0.0,
-        l2_latent=0.0,
-        intercept=0.0,
-        intercept_lr: typing.Union[optim.schedulers.Scheduler, float] = 0.01,
-        weight_initializer: optim.initializers.Initializer = None,
-        latent_initializer: optim.initializers.Initializer = None,
-        clip_gradient=1e12,
-        seed: int = None,
-    ):
-
-        super().__init__(
-            degree=degree,
-            n_factors=n_factors,
-            weight_optimizer=weight_optimizer,
-            latent_optimizer=latent_optimizer,
-            loss=optim.losses.Log() if loss is None else loss,
-            sample_normalization=sample_normalization,
-            l1_weight=l1_weight,
-            l2_weight=l2_weight,
-            l1_latent=l1_latent,
-            l2_latent=l2_latent,
-            intercept=intercept,
-            intercept_lr=intercept_lr,
-            weight_initializer=weight_initializer,
-            latent_initializer=latent_initializer,
-            clip_gradient=clip_gradient,
-            seed=seed,
-        )
-
-    def predict_proba_one(self, x):
-        x = self._ohe_cat_features(x)
-        p = utils.math.sigmoid(self._raw_dot(x))  # Convert logit to probability
-        return {False: 1.0 - p, True: p}
+import collections
+import functools
+import itertools
+import typing
+
+from river import base, optim, utils
+
+from .base import BaseFM
+
+__all__ = ["HOFMClassifier", "HOFMRegressor"]
+
+
+class HOFM(BaseFM):
+    """Higher-Order Factorization Machine base class."""
+
+    def __init__(
+        self,
+        degree,
+        n_factors,
+        weight_optimizer,
+        latent_optimizer,
+        loss,
+        sample_normalization,
+        l1_weight,
+        l2_weight,
+        l1_latent,
+        l2_latent,
+        intercept,
+        intercept_lr,
+        weight_initializer,
+        latent_initializer,
+        clip_gradient,
+        seed,
+    ):
+        super().__init__(
+            n_factors=n_factors,
+            weight_optimizer=weight_optimizer,
+            latent_optimizer=latent_optimizer,
+            loss=loss,
+            sample_normalization=sample_normalization,
+            l1_weight=l1_weight,
+            l2_weight=l2_weight,
+            l1_latent=l1_latent,
+            l2_latent=l2_latent,
+            intercept=intercept,
+            intercept_lr=intercept_lr,
+            weight_initializer=weight_initializer,
+            latent_initializer=latent_initializer,
+            clip_gradient=clip_gradient,
+            seed=seed,
+        )
+        self.degree = degree
+
+    def _init_latents(self):
+        random_latents = functools.partial(
+            self.latent_initializer, shape=self.n_factors
+        )
+        order_latents_dict = functools.partial(collections.defaultdict, random_latents)
+        return collections.defaultdict(order_latents_dict)
+
+    def _calculate_interactions(self, x):
+        """Calculates greater than unary interactions."""
+        return sum(
+            self._calculate_interaction(x, d, combination)
+            for d in range(2, self.degree + 1)
+            for combination in itertools.combinations(x.keys(), d)
+        )
+
+    def _calculate_interaction(self, x, d, combination):
+        feature_product = functools.reduce(
+            lambda x, y: x * y, (x[j] for j in combination)
+        )
+        latent_scalar_product = sum(
+            functools.reduce(
+                lambda x, y: x * y, (self.latents[j][d][f] for j in combination)
+            )
+            for f in range(self.n_factors)
+        )
+        return feature_product * latent_scalar_product
+
+    def _calculate_weights_gradients(self, x, g_loss):
+
+        # For notational convenience
+        w, l1, l2, sign = self.weights, self.l1_weight, self.l2_weight, utils.math.sign
+
+        return {j: g_loss * xj + l1 * sign(w[j]) + l2 * w[j] for j, xj in x.items()}
+
+    def _update_latents(self, x, g_loss):
+
+        # For notational convenience
+        v, l1, l2, sign = self.latents, self.l1_latent, self.l2_latent, utils.math.sign
+
+        # Calculate each latent factor gradient before updating any
+        gradients = collections.defaultdict(
+            lambda: collections.defaultdict(lambda: collections.defaultdict(float))
+        )
+
+        for d in range(2, self.degree + 1):
+
+            for combination in itertools.combinations(x.keys(), d):
+                feature_product = functools.reduce(
+                    lambda x, y: x * y, (x[j] for j in combination)
+                )
+
+                for f in range(self.n_factors):
+                    latent_product = functools.reduce(
+                        lambda x, y: x * y, (v[j][d][f] for j in combination)
+                    )
+
+                    for j in combination:
+                        gradients[j][d][f] += (
+                            feature_product * latent_product / v[j][d][f]
+                        )
+
+        # Finally update the latent weights
+        for j in x.keys():
+            for d in range(2, self.degree + 1):
+                self.latents[j][d] = self.latent_optimizer.step(
+                    w=v[j][d],
+                    g={
+                        f: g_loss * gradients[j][d][f]
+                        + l1 * sign(v[j][d][f])
+                        + 2 * l2 * v[j][d][f]
+                        for f in range(self.n_factors)
+                    },
+                )
+
+
+class HOFMRegressor(HOFM, base.Regressor):
+    """Higher-Order Factorization Machine for regression.
+
+    The model equation is defined as:
+
+    $$\\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j}  + \\sum_{l=2}^{d} \\sum_{j_1=1}^{p} \\cdots \\sum_{j_l=j_{l-1}+1}^{p} \\left(\\prod_{j'=1}^{l} x_{j_{j'}} \\right) \\left(\\sum_{f=1}^{k_l} \\prod_{j'=1}^{l} v_{j_{j'}, f}^{(l)} \\right)$$
+
+    For more efficiency, this model automatically one-hot encodes strings features considering
+    them as categorical variables.
+
+    Parameters
+    ----------
+    degree
+        Polynomial degree or model order.
+    n_factors
+        Dimensionality of the factorization or number of latent factors.
+    weight_optimizer
+        The sequential optimizer used for updating the feature weights. Note thatthe intercept is
+        handled separately.
+    latent_optimizer
+        The sequential optimizer used for updating the latent factors.
+    int_weight_optimizer
+        The sequential optimizer used for updating the field pairs interaction weights.
+    loss
+        The loss function to optimize for.
+    sample_normalization
+        Whether to divide each element of `x` by `x`'s L2-norm.
+    l1_weight
+        Amount of L1 regularization used to push weights towards 0.
+    l2_weight
+        Amount of L2 regularization used to push weights towards 0.
+    l1_latent
+        Amount of L1 regularization used to push latent weights towards 0.
+    l2_latent
+        Amount of L2 regularization used to push latent weights towards 0.
+    intercept
+        Initial intercept value.
+    intercept_lr
+        Learning rate scheduler used for updating the intercept. An instance of
+        `optim.schedulers.Constant` is used if a `float` is passed. No intercept will be used
+        if this is set to 0.
+    weight_initializer
+        Weights initialization scheme. Defaults to `optim.initializers.Zeros()`.
+    latent_initializer
+        Latent factors initialization scheme. Defaults to
+        `optim.initializers.Normal(mu=.0, sigma=.1, random_state=self.random_state)`.
+    clip_gradient
+        Clips the absolute value of each gradient value.
+    seed
+        Randomization seed used for reproducibility.
+
+    Attributes
+    ----------
+    weights
+        The current weights assigned to the features.
+    latents
+        The current latent weights assigned to the features.
+
+    Examples
+    --------
+
+    >>> from river import facto
+
+    >>> dataset = (
+    ...     ({'user': 'Alice', 'item': 'Superman', 'time': .12}, 8),
+    ...     ({'user': 'Alice', 'item': 'Terminator', 'time': .13}, 9),
+    ...     ({'user': 'Alice', 'item': 'Star Wars', 'time': .14}, 8),
+    ...     ({'user': 'Alice', 'item': 'Notting Hill', 'time': .15}, 2),
+    ...     ({'user': 'Alice', 'item': 'Harry Potter ', 'time': .16}, 5),
+    ...     ({'user': 'Bob', 'item': 'Superman', 'time': .13}, 8),
+    ...     ({'user': 'Bob', 'item': 'Terminator', 'time': .12}, 9),
+    ...     ({'user': 'Bob', 'item': 'Star Wars', 'time': .16}, 8),
+    ...     ({'user': 'Bob', 'item': 'Notting Hill', 'time': .10}, 2)
+    ... )
+
+    >>> model = facto.HOFMRegressor(
+    ...     degree=3,
+    ...     n_factors=10,
+    ...     intercept=5,
+    ...     seed=42,
+    ... )
+
+    >>> for x, y in dataset:
+    ...     _ = model.learn_one(x, y)
+
+    >>> model.predict_one({'user': 'Bob', 'item': 'Harry Potter', 'time': .14})
+    5.311745
+
+    References
+    ----------
+    [^1]: [Rendle, S., 2010, December. Factorization machines. In 2010 IEEE International Conference on Data Mining (pp. 995-1000). IEEE.](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)
+
+    """
+
+    def __init__(
+        self,
+        degree=3,
+        n_factors=10,
+        weight_optimizer: optim.Optimizer = None,
+        latent_optimizer: optim.Optimizer = None,
+        loss: optim.losses.RegressionLoss = None,
+        sample_normalization=False,
+        l1_weight=0.0,
+        l2_weight=0.0,
+        l1_latent=0.0,
+        l2_latent=0.0,
+        intercept=0.0,
+        intercept_lr: typing.Union[optim.schedulers.Scheduler, float] = 0.01,
+        weight_initializer: optim.initializers.Initializer = None,
+        latent_initializer: optim.initializers.Initializer = None,
+        clip_gradient=1e12,
+        seed: int = None,
+    ):
+
+        super().__init__(
+            degree=degree,
+            n_factors=n_factors,
+            weight_optimizer=weight_optimizer,
+            latent_optimizer=latent_optimizer,
+            loss=optim.losses.Squared() if loss is None else loss,
+            sample_normalization=sample_normalization,
+            l1_weight=l1_weight,
+            l2_weight=l2_weight,
+            l1_latent=l1_latent,
+            l2_latent=l2_latent,
+            intercept=intercept,
+            intercept_lr=intercept_lr,
+            weight_initializer=weight_initializer,
+            latent_initializer=latent_initializer,
+            clip_gradient=clip_gradient,
+            seed=seed,
+        )
+
+    def predict_one(self, x):
+        x = self._ohe_cat_features(x)
+        return self._raw_dot(x)
+
+
+class HOFMClassifier(HOFM, base.Classifier):
+    """Higher-Order Factorization Machine for binary classification.
+
+    The model equation is defined as:
+
+    $$\\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j}  + \\sum_{l=2}^{d} \\sum_{j_1=1}^{p} \\cdots \\sum_{j_l=j_{l-1}+1}^{p} \\left(\\prod_{j'=1}^{l} x_{j_{j'}} \\right) \\left(\\sum_{f=1}^{k_l} \\prod_{j'=1}^{l} v_{j_{j'}, f}^{(l)} \\right)$$
+
+    For more efficiency, this model automatically one-hot encodes strings features considering
+    them as categorical variables.
+
+    Parameters
+    ----------
+    degree
+        Polynomial degree or model order.
+    n_factors
+        Dimensionality of the factorization or number of latent factors.
+    weight_optimizer
+        The sequential optimizer used for updating the feature weights. Note that the intercept is
+        handled separately.
+    latent_optimizer
+        The sequential optimizer used for updating the latent factors.
+    int_weight_optimizer
+        The sequential optimizer used for updating the field pairs interaction weights.
+    loss
+        The loss function to optimize for.
+    sample_normalization
+        Whether to divide each element of `x` by `x`'s L2-norm.
+    l1_weight
+        Amount of L1 regularization used to push weights towards 0.
+    l2_weight
+        Amount of L2 regularization used to push weights towards 0.
+    l1_latent
+        Amount of L1 regularization used to push latent weights towards 0.
+    l2_latent
+        Amount of L2 regularization used to push latent weights towards 0.
+    intercept
+        Initial intercept value.
+    intercept_lr
+        Learning rate scheduler used for updating the intercept. An instance of
+        `optim.schedulers.Constant` is used if a `float` is passed. No intercept will be used
+        if this is set to 0.
+    weight_initializer
+        Weights initialization scheme. Defaults to `optim.initializers.Zeros()`.
+    latent_initializer
+        Latent factors initialization scheme. Defaults to
+        `optim.initializers.Normal(mu=.0, sigma=.1, random_state=self.random_state)`.
+    clip_gradient
+        Clips the absolute value of each gradient value.
+    seed
+        Randomization seed used for reproducibility.
+
+    Attributes
+    ----------
+    weights
+        The current weights assigned to the features.
+    latents
+        The current latent weights assigned to the features.
+
+    Examples
+    --------
+
+    >>> from river import facto
+
+    >>> dataset = (
+    ...     ({'user': 'Alice', 'item': 'Superman', 'time': .12}, True),
+    ...     ({'user': 'Alice', 'item': 'Terminator', 'time': .13}, True),
+    ...     ({'user': 'Alice', 'item': 'Star Wars', 'time': .14}, True),
+    ...     ({'user': 'Alice', 'item': 'Notting Hill', 'time': .15}, False),
+    ...     ({'user': 'Alice', 'item': 'Harry Potter ', 'time': .16}, True),
+    ...     ({'user': 'Bob', 'item': 'Superman', 'time': .13}, True),
+    ...     ({'user': 'Bob', 'item': 'Terminator', 'time': .12}, True),
+    ...     ({'user': 'Bob', 'item': 'Star Wars', 'time': .16}, True),
+    ...     ({'user': 'Bob', 'item': 'Notting Hill', 'time': .10}, False)
+    ... )
+
+    >>> model = facto.HOFMClassifier(
+    ...     degree=3,
+    ...     n_factors=10,
+    ...     intercept=.5,
+    ...     seed=42,
+    ... )
+
+    >>> for x, y in dataset:
+    ...     _ = model.learn_one(x, y)
+
+    >>> model.predict_one({'user': 'Bob', 'item': 'Harry Potter', 'time': .14})
+    True
+
+    References
+    ----------
+    [^1]: [Rendle, S., 2010, December. Factorization machines. In 2010 IEEE International Conference on Data Mining (pp. 995-1000). IEEE.](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)
+
+    """
+
+    def __init__(
+        self,
+        degree=3,
+        n_factors=10,
+        weight_optimizer: optim.Optimizer = None,
+        latent_optimizer: optim.Optimizer = None,
+        loss: optim.losses.BinaryLoss = None,
+        sample_normalization=False,
+        l1_weight=0.0,
+        l2_weight=0.0,
+        l1_latent=0.0,
+        l2_latent=0.0,
+        intercept=0.0,
+        intercept_lr: typing.Union[optim.schedulers.Scheduler, float] = 0.01,
+        weight_initializer: optim.initializers.Initializer = None,
+        latent_initializer: optim.initializers.Initializer = None,
+        clip_gradient=1e12,
+        seed: int = None,
+    ):
+
+        super().__init__(
+            degree=degree,
+            n_factors=n_factors,
+            weight_optimizer=weight_optimizer,
+            latent_optimizer=latent_optimizer,
+            loss=optim.losses.Log() if loss is None else loss,
+            sample_normalization=sample_normalization,
+            l1_weight=l1_weight,
+            l2_weight=l2_weight,
+            l1_latent=l1_latent,
+            l2_latent=l2_latent,
+            intercept=intercept,
+            intercept_lr=intercept_lr,
+            weight_initializer=weight_initializer,
+            latent_initializer=latent_initializer,
+            clip_gradient=clip_gradient,
+            seed=seed,
+        )
+
+    def predict_proba_one(self, x):
+        x = self._ohe_cat_features(x)
+        p = utils.math.sigmoid(self._raw_dot(x))  # Convert logit to probability
+        return {False: 1.0 - p, True: p}
```

### Comparing `river-0.8.0/river/feature_extraction/__init__.py` & `river-0.9.0/river/feature_extraction/__init__.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,21 +1,24 @@
-"""Feature extraction.
-
-This module can be used to extract information from raw features. This includes encoding
-categorical data as well as looking at interactions between existing features. This differs from
-the `processing` module in that the latter's purpose is rather to clean the data so that it may
-be processed by a particular machine learning algorithm.
-
-"""
-from .agg import Agg, TargetAgg
-from .kernel_approx import RBFSampler
-from .poly import PolynomialExtender
-from .vectorize import TFIDF, BagOfWords
-
-__all__ = [
-    "Agg",
-    "BagOfWords",
-    "PolynomialExtender",
-    "RBFSampler",
-    "TargetAgg",
-    "TFIDF",
-]
+"""Feature extraction.
+
+This module can be used to extract information from raw features. This includes encoding
+categorical data as well as looking at interactions between existing features. This differs from
+the `processing` module in that the latter's purpose is rather to clean the data so that it may
+be processed by a particular machine learning algorithm.
+
+"""
+from .agg import Agg, TargetAgg
+from .kernel_approx import RBFSampler
+from .lag import Lagger, TargetLagger
+from .poly import PolynomialExtender
+from .vectorize import TFIDF, BagOfWords
+
+__all__ = [
+    "Agg",
+    "BagOfWords",
+    "Lagger",
+    "PolynomialExtender",
+    "RBFSampler",
+    "TargetAgg",
+    "TargetLagger",
+    "TFIDF",
+]
```

### Comparing `river-0.8.0/river/feature_extraction/agg.py` & `river-0.9.0/river/feature_extraction/agg.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,273 +1,265 @@
-import collections
-import copy
-import functools
-import typing
-
-from river import base, stats
-
-__all__ = ["Agg", "TargetAgg"]
-
-
-class Agg(base.Transformer):
-    """Computes a streaming aggregate.
-
-    This transformer allows to compute an aggregate statistic, very much like the groupby method
-    from `pandas`, but on a streaming dataset. This makes use of the streaming statistics from the
-    `stats` module.
-
-    When `learn_one` is called, the running statistic `how` of group `by` is updated with the value
-    of `on`. Meanwhile, the output of `transform_one` is a single-element dictionary, where the key
-    is the name of the aggregate and the value is the current value of the statistic for the
-    relevant group. The key is automatically inferred from the parameters.
-
-    Note that you can use a `compose.TransformerUnion` to extract many aggregate statistics in a
-    concise manner.
-
-    Parameters
-    ----------
-    on
-        The feature on which to compute the aggregate statistic.
-    by
-        The feature by which to group the data.
-    how
-        The statistic to compute.
-
-    Attributes
-    ----------
-    groups : collections.defaultdict
-        Maps group keys to univariate statistics.
-    feature_name : str
-        The name of the feature used in the output.
-
-    Examples
-    --------
-
-    Consider the following dataset:
-
-    >>> X = [
-    ...     {'country': 'France', 'place': 'Taco Bell', 'revenue': 42},
-    ...     {'country': 'Sweden', 'place': 'Burger King', 'revenue': 16},
-    ...     {'country': 'France', 'place': 'Burger King', 'revenue': 24},
-    ...     {'country': 'Sweden', 'place': 'Taco Bell', 'revenue': 58},
-    ...     {'country': 'Sweden', 'place': 'Burger King', 'revenue': 20},
-    ...     {'country': 'France', 'place': 'Taco Bell', 'revenue': 50},
-    ...     {'country': 'France', 'place': 'Burger King', 'revenue': 10},
-    ...     {'country': 'Sweden', 'place': 'Taco Bell', 'revenue': 80}
-    ... ]
-
-    As an example, we can calculate the average (how) revenue (on) for each place (by):
-
-    >>> from river import feature_extraction as fx
-    >>> from river import stats
-
-    >>> agg = fx.Agg(
-    ...     on='revenue',
-    ...     by='place',
-    ...     how=stats.Mean()
-    ... )
-
-    >>> for x in X:
-    ...     agg = agg.learn_one(x)
-    ...     print(agg.transform_one(x))
-    {'revenue_mean_by_place': 42.0}
-    {'revenue_mean_by_place': 16.0}
-    {'revenue_mean_by_place': 20.0}
-    {'revenue_mean_by_place': 50.0}
-    {'revenue_mean_by_place': 20.0}
-    {'revenue_mean_by_place': 50.0}
-    {'revenue_mean_by_place': 17.5}
-    {'revenue_mean_by_place': 57.5}
-
-    You can compute an aggregate over multiple keys by passing a tuple to the `by` argument.
-    For instance, we can compute the maximum (how) revenue (on) per place as well as per
-    day (by):
-
-    >>> agg = fx.Agg(
-    ...     on='revenue',
-    ...     by=['place', 'country'],
-    ...     how=stats.Max()
-    ... )
-
-    >>> for x in X:
-    ...     agg = agg.learn_one(x)
-    ...     print(agg.transform_one(x))
-    {'revenue_max_by_place_and_country': 42}
-    {'revenue_max_by_place_and_country': 16}
-    {'revenue_max_by_place_and_country': 24}
-    {'revenue_max_by_place_and_country': 58}
-    {'revenue_max_by_place_and_country': 20}
-    {'revenue_max_by_place_and_country': 50}
-    {'revenue_max_by_place_and_country': 24}
-    {'revenue_max_by_place_and_country': 80}
-
-    You can use a `compose.TransformerUnion` in order to calculate multiple aggregates in one
-    go. The latter can be constructed by using the `+` operator:
-
-    >>> agg = (
-    ...     fx.Agg(on='revenue', by='place', how=stats.Mean()) +
-    ...     fx.Agg(on='revenue', by=['place', 'country'], how=stats.Max())
-    ... )
-
-    >>> import pprint
-    >>> for x in X:
-    ...     agg = agg.learn_one(x)
-    ...     pprint.pprint(agg.transform_one(x))
-    {'revenue_max_by_place_and_country': 42, 'revenue_mean_by_place': 42.0}
-    {'revenue_max_by_place_and_country': 16, 'revenue_mean_by_place': 16.0}
-    {'revenue_max_by_place_and_country': 24, 'revenue_mean_by_place': 20.0}
-    {'revenue_max_by_place_and_country': 58, 'revenue_mean_by_place': 50.0}
-    {'revenue_max_by_place_and_country': 20, 'revenue_mean_by_place': 20.0}
-    {'revenue_max_by_place_and_country': 50, 'revenue_mean_by_place': 50.0}
-    {'revenue_max_by_place_and_country': 24, 'revenue_mean_by_place': 17.5}
-    {'revenue_max_by_place_and_country': 80, 'revenue_mean_by_place': 57.5}
-
-    References
-    ----------
-    [^1]: [Streaming groupbys in pandas for big datasets](https://maxhalford.github.io/blog/pandas-streaming-groupby/)
-
-    """
-
-    def __init__(
-        self, on: str, by: typing.Union[str, typing.List[str]], how: stats.Univariate
-    ):
-        self.on = on
-        self.by = by if isinstance(by, list) else [by]
-        self.how = how
-        self.groups = collections.defaultdict(functools.partial(copy.deepcopy, how))
-        self.feature_name = f'{self.on}_{self.how.name}_by_{"_and_".join(self.by)}'
-
-    def _get_key(self, x):
-        return "_".join(str(x[k]) for k in self.by)
-
-    def learn_one(self, x):
-        self.groups[self._get_key(x)].update(x[self.on])
-        return self
-
-    def transform_one(self, x):
-        return {self.feature_name: self.groups[self._get_key(x)].get()}
-
-    def __str__(self):
-        return self.feature_name
-
-
-class TargetAgg(base.SupervisedTransformer):
-    """Computes a streaming aggregate of the target values.
-
-    This transformer is identical to `feature_extraction.Agg`, the only difference is that it
-    operates on the target rather than on a feature. At each step, the running statistic `how` of
-    target values in group `by` is updated with the target. It is therefore a supervised
-    transformer.
-
-    Parameters
-    ----------
-    by
-        The feature by which to group the target values.
-    how
-        The statistic to compute.
-    target_name
-        The target name which is used in the result.
-
-    Attributes
-    ----------
-    groups
-        Maps group keys to univariate statistics.
-    feature_name
-        The name of the feature in the output.
-
-    Examples
-    --------
-
-    Consider the following dataset, where the second value of each value is the target:
-
-    >>> dataset = [
-    ...     ({'country': 'France', 'place': 'Taco Bell'}, 42),
-    ...     ({'country': 'Sweden', 'place': 'Burger King'}, 16),
-    ...     ({'country': 'France', 'place': 'Burger King'}, 24),
-    ...     ({'country': 'Sweden', 'place': 'Taco Bell'}, 58),
-    ...     ({'country': 'Sweden', 'place': 'Burger King'}, 20),
-    ...     ({'country': 'France', 'place': 'Taco Bell'}, 50),
-    ...     ({'country': 'France', 'place': 'Burger King'}, 10),
-    ...     ({'country': 'Sweden', 'place': 'Taco Bell'}, 80)
-    ... ]
-
-    As an example, let's perform a target encoding of the `place` feature. Instead of simply
-    updating a running average, we use a `stats.BayesianMean` which allows us to incorporate
-    some prior knowledge. This makes subsequent models less prone to overfitting. Indeed, it
-    dampens the fact that too few samples might have been seen within a group.
-
-    >>> from river import feature_extraction
-    >>> from river import stats
-
-    >>> agg = feature_extraction.TargetAgg(
-    ...     by='place',
-    ...     how=stats.BayesianMean(
-    ...         prior=3,
-    ...         prior_weight=1
-    ...     )
-    ... )
-
-    >>> for x, y in dataset:
-    ...     print(agg.transform_one(x))
-    ...     agg = agg.learn_one(x, y)
-    {'target_bayes_mean_by_place': 3.0}
-    {'target_bayes_mean_by_place': 3.0}
-    {'target_bayes_mean_by_place': 9.5}
-    {'target_bayes_mean_by_place': 22.5}
-    {'target_bayes_mean_by_place': 14.333}
-    {'target_bayes_mean_by_place': 34.333}
-    {'target_bayes_mean_by_place': 15.75}
-    {'target_bayes_mean_by_place': 38.25}
-
-    Just like with `feature_extraction.Agg`, we can specify multiple features on which to
-    group the data:
-
-    >>> agg = feature_extraction.TargetAgg(
-    ...     by=['place', 'country'],
-    ...     how=stats.BayesianMean(
-    ...         prior=3,
-    ...         prior_weight=1
-    ...     )
-    ... )
-
-    >>> for x, y in dataset:
-    ...     print(agg.transform_one(x))
-    ...     agg = agg.learn_one(x, y)
-    {'target_bayes_mean_by_place_and_country': 3.0}
-    {'target_bayes_mean_by_place_and_country': 3.0}
-    {'target_bayes_mean_by_place_and_country': 3.0}
-    {'target_bayes_mean_by_place_and_country': 3.0}
-    {'target_bayes_mean_by_place_and_country': 9.5}
-    {'target_bayes_mean_by_place_and_country': 22.5}
-    {'target_bayes_mean_by_place_and_country': 13.5}
-    {'target_bayes_mean_by_place_and_country': 30.5}
-
-    References
-    ----------
-    1. [Streaming groupbys in pandas for big datasets](https://maxhalford.github.io/blog/streaming-groupbys-in-pandas-for-big-datasets/)
-
-    """
-
-    def __init__(
-        self,
-        by: typing.Union[str, typing.List[str]],
-        how: stats.Univariate,
-        target_name="target",
-    ):
-        self.by = by if isinstance(by, list) else [by]
-        self.how = how
-        self.target_name = target_name
-        self.groups = collections.defaultdict(functools.partial(copy.deepcopy, how))
-        self.feature_name = f'{target_name}_{how.name}_by_{"_and_".join(self.by)}'
-
-    def _get_key(self, x):
-        return "_".join(str(x[k]) for k in self.by)
-
-    def learn_one(self, x, y):
-        self.groups[self._get_key(x)].update(y)
-        return self
-
-    def transform_one(self, x):
-        return {self.feature_name: self.groups[self._get_key(x)].get()}
-
-    def __str__(self):
-        return self.feature_name
+import collections
+import copy
+import functools
+import typing
+
+from river import base, stats
+
+
+class Agg(base.Transformer):
+    """Computes a streaming aggregate.
+
+    This transformer allows to compute an aggregate statistic, very much like the groupby method
+    from `pandas`, but on a streaming dataset. This makes use of the streaming statistics from the
+    `stats` module.
+
+    When `learn_one` is called, the running statistic `how` of group `by` is updated with the value
+    of `on`. Meanwhile, the output of `transform_one` is a single-element dictionary, where the key
+    is the name of the aggregate and the value is the current value of the statistic for the
+    relevant group. The key is automatically inferred from the parameters.
+
+    Note that you can use a `compose.TransformerUnion` to extract many aggregate statistics in a
+    concise manner.
+
+    Parameters
+    ----------
+    on
+        The feature on which to compute the aggregate statistic.
+    by
+        The feature by which to group the data.
+    how
+        The statistic to compute.
+
+    Attributes
+    ----------
+    groups : collections.defaultdict
+        Maps group keys to univariate statistics.
+    feature_name : str
+        The name of the feature used in the output.
+
+    Examples
+    --------
+
+    Consider the following dataset:
+
+    >>> X = [
+    ...     {'country': 'France', 'place': 'Taco Bell', 'revenue': 42},
+    ...     {'country': 'Sweden', 'place': 'Burger King', 'revenue': 16},
+    ...     {'country': 'France', 'place': 'Burger King', 'revenue': 24},
+    ...     {'country': 'Sweden', 'place': 'Taco Bell', 'revenue': 58},
+    ...     {'country': 'Sweden', 'place': 'Burger King', 'revenue': 20},
+    ...     {'country': 'France', 'place': 'Taco Bell', 'revenue': 50},
+    ...     {'country': 'France', 'place': 'Burger King', 'revenue': 10},
+    ...     {'country': 'Sweden', 'place': 'Taco Bell', 'revenue': 80}
+    ... ]
+
+    As an example, we can calculate the average (how) revenue (on) for each place (by):
+
+    >>> from river import feature_extraction as fx
+    >>> from river import stats
+
+    >>> agg = fx.Agg(
+    ...     on='revenue',
+    ...     by='place',
+    ...     how=stats.Mean()
+    ... )
+
+    >>> for x in X:
+    ...     agg = agg.learn_one(x)
+    ...     print(agg.transform_one(x))
+    {'revenue_mean_by_place': 42.0}
+    {'revenue_mean_by_place': 16.0}
+    {'revenue_mean_by_place': 20.0}
+    {'revenue_mean_by_place': 50.0}
+    {'revenue_mean_by_place': 20.0}
+    {'revenue_mean_by_place': 50.0}
+    {'revenue_mean_by_place': 17.5}
+    {'revenue_mean_by_place': 57.5}
+
+    You can compute an aggregate over multiple keys by passing a tuple to the `by` argument.
+    For instance, we can compute the maximum (how) revenue (on) per place as well as per
+    day (by):
+
+    >>> agg = fx.Agg(
+    ...     on='revenue',
+    ...     by=['place', 'country'],
+    ...     how=stats.Max()
+    ... )
+
+    >>> for x in X:
+    ...     agg = agg.learn_one(x)
+    ...     print(agg.transform_one(x))
+    {'revenue_max_by_place_and_country': 42}
+    {'revenue_max_by_place_and_country': 16}
+    {'revenue_max_by_place_and_country': 24}
+    {'revenue_max_by_place_and_country': 58}
+    {'revenue_max_by_place_and_country': 20}
+    {'revenue_max_by_place_and_country': 50}
+    {'revenue_max_by_place_and_country': 24}
+    {'revenue_max_by_place_and_country': 80}
+
+    You can use a `compose.TransformerUnion` in order to calculate multiple aggregates in one
+    go. The latter can be constructed by using the `+` operator:
+
+    >>> agg = (
+    ...     fx.Agg(on='revenue', by='place', how=stats.Mean()) +
+    ...     fx.Agg(on='revenue', by=['place', 'country'], how=stats.Max())
+    ... )
+
+    >>> import pprint
+    >>> for x in X:
+    ...     agg = agg.learn_one(x)
+    ...     pprint.pprint(agg.transform_one(x))
+    {'revenue_max_by_place_and_country': 42, 'revenue_mean_by_place': 42.0}
+    {'revenue_max_by_place_and_country': 16, 'revenue_mean_by_place': 16.0}
+    {'revenue_max_by_place_and_country': 24, 'revenue_mean_by_place': 20.0}
+    {'revenue_max_by_place_and_country': 58, 'revenue_mean_by_place': 50.0}
+    {'revenue_max_by_place_and_country': 20, 'revenue_mean_by_place': 20.0}
+    {'revenue_max_by_place_and_country': 50, 'revenue_mean_by_place': 50.0}
+    {'revenue_max_by_place_and_country': 24, 'revenue_mean_by_place': 17.5}
+    {'revenue_max_by_place_and_country': 80, 'revenue_mean_by_place': 57.5}
+
+    References
+    ----------
+    [^1]: [Streaming groupbys in pandas for big datasets](https://maxhalford.github.io/blog/pandas-streaming-groupby/)
+
+    """
+
+    def __init__(
+        self, on: str, by: typing.Union[str, typing.List[str]], how: stats.Univariate
+    ):
+        self.on = on
+        self.by = by if isinstance(by, list) else [by]
+        self.how = how
+        self.groups = collections.defaultdict(functools.partial(copy.deepcopy, how))
+        self.feature_name = f'{self.on}_{self.how.name}_by_{"_and_".join(self.by)}'
+
+    def _make_key(self, x):
+        return tuple(x[k] for k in self.by)
+
+    def learn_one(self, x):
+        self.groups[self._make_key(x)].update(x[self.on])
+        return self
+
+    def transform_one(self, x):
+        return {self.feature_name: self.groups[self._make_key(x)].get()}
+
+    def __str__(self):
+        return self.feature_name
+
+
+class TargetAgg(base.SupervisedTransformer):
+    """Computes a streaming aggregate of the target values.
+
+    This transformer is identical to `feature_extraction.Agg`, the only difference is that it
+    operates on the target rather than on a feature. At each step, the running statistic `how` of
+    target values in group `by` is updated with the target. It is therefore a supervised
+    transformer.
+
+    Parameters
+    ----------
+    by
+        The feature by which to group the target values.
+    how
+        The statistic to compute.
+    target_name
+        The target name which is used in the result.
+
+    Examples
+    --------
+
+    Consider the following dataset, where the second value of each value is the target:
+
+    >>> dataset = [
+    ...     ({'country': 'France', 'place': 'Taco Bell'}, 42),
+    ...     ({'country': 'Sweden', 'place': 'Burger King'}, 16),
+    ...     ({'country': 'France', 'place': 'Burger King'}, 24),
+    ...     ({'country': 'Sweden', 'place': 'Taco Bell'}, 58),
+    ...     ({'country': 'Sweden', 'place': 'Burger King'}, 20),
+    ...     ({'country': 'France', 'place': 'Taco Bell'}, 50),
+    ...     ({'country': 'France', 'place': 'Burger King'}, 10),
+    ...     ({'country': 'Sweden', 'place': 'Taco Bell'}, 80)
+    ... ]
+
+    As an example, let's perform a target encoding of the `place` feature. Instead of simply
+    updating a running average, we use a `stats.BayesianMean` which allows us to incorporate
+    some prior knowledge. This makes subsequent models less prone to overfitting. Indeed, it
+    dampens the fact that too few samples might have been seen within a group.
+
+    >>> from river import feature_extraction
+    >>> from river import stats
+
+    >>> agg = feature_extraction.TargetAgg(
+    ...     by='place',
+    ...     how=stats.BayesianMean(
+    ...         prior=3,
+    ...         prior_weight=1
+    ...     )
+    ... )
+
+    >>> for x, y in dataset:
+    ...     print(agg.transform_one(x))
+    ...     agg = agg.learn_one(x, y)
+    {'y_bayes_mean_by_place': 3.0}
+    {'y_bayes_mean_by_place': 3.0}
+    {'y_bayes_mean_by_place': 9.5}
+    {'y_bayes_mean_by_place': 22.5}
+    {'y_bayes_mean_by_place': 14.333}
+    {'y_bayes_mean_by_place': 34.333}
+    {'y_bayes_mean_by_place': 15.75}
+    {'y_bayes_mean_by_place': 38.25}
+
+    Just like with `feature_extraction.Agg`, we can specify multiple features on which to
+    group the data:
+
+    >>> agg = feature_extraction.TargetAgg(
+    ...     by=['place', 'country'],
+    ...     how=stats.BayesianMean(
+    ...         prior=3,
+    ...         prior_weight=1
+    ...     )
+    ... )
+
+    >>> for x, y in dataset:
+    ...     print(agg.transform_one(x))
+    ...     agg = agg.learn_one(x, y)
+    {'y_bayes_mean_by_place_and_country': 3.0}
+    {'y_bayes_mean_by_place_and_country': 3.0}
+    {'y_bayes_mean_by_place_and_country': 3.0}
+    {'y_bayes_mean_by_place_and_country': 3.0}
+    {'y_bayes_mean_by_place_and_country': 9.5}
+    {'y_bayes_mean_by_place_and_country': 22.5}
+    {'y_bayes_mean_by_place_and_country': 13.5}
+    {'y_bayes_mean_by_place_and_country': 30.5}
+
+    References
+    ----------
+    1. [Streaming groupbys in pandas for big datasets](https://maxhalford.github.io/blog/streaming-groupbys-in-pandas-for-big-datasets/)
+
+    """
+
+    def __init__(
+        self,
+        by: typing.Union[str, typing.List[str]],
+        how: stats.Univariate,
+        target_name="y",
+    ):
+        self.by = by if isinstance(by, list) else [by]
+        self.how = how
+        self.target_name = target_name
+
+        self._feature_name = f'{target_name}_{how.name}_by_{"_and_".join(self.by)}'
+        self._groups = collections.defaultdict(functools.partial(copy.deepcopy, how))
+
+    def _make_key(self, x):
+        return tuple(x[k] for k in self.by)
+
+    def learn_one(self, x, y):
+        self._groups[self._make_key(x)].update(y)
+        return self
+
+    def transform_one(self, x):
+        return {self._feature_name: self._groups[self._make_key(x)].get()}
+
+    def __str__(self):
+        return self._feature_name
```

### Comparing `river-0.8.0/river/feature_extraction/vectorize.py` & `river-0.9.0/river/feature_extraction/vectorize.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,426 +1,426 @@
-import collections
-import functools
-import itertools
-import math
-import operator
-import re
-import typing
-import unicodedata
-
-import pandas as pd
-from scipy import sparse
-
-from river import base
-
-__all__ = ["BagOfWords", "TFIDF"]
-
-
-N_GRAM = typing.Union[str, typing.Tuple[str, ...]]  # unigram  # n-gram
-
-
-def strip_accents_unicode(s: str) -> str:
-    """Transform accentuated unicode symbols into their ASCII counterpart."""
-    try:
-        # If `s` is ASCII-compatible, then it does not contain any accented
-        # characters and we can avoid an expensive list comprehension
-        s.encode("ASCII", errors="strict")
-        return s
-    except UnicodeEncodeError:
-        normalized = unicodedata.normalize("NFKD", s)
-        return "".join([c for c in normalized if not unicodedata.combining(c)])
-
-
-def find_ngrams(tokens: typing.List[str], n: int) -> typing.Iterator[N_GRAM]:
-    """Generates n-grams from a list of tokens.
-
-    From http://www.locallyoptimal.com/blog/2013/01/20/elegant-n-gram-generation-in-python/.
-
-    Examples
-    --------
-
-    >>> tokens = ['a', 'b', 'c']
-
-    >>> list(find_ngrams(tokens, 0))
-    []
-
-    >>> list(find_ngrams(tokens, 1))
-    ['a', 'b', 'c']
-
-    >>> list(find_ngrams(tokens, 2))
-    [('a', 'b'), ('b', 'c')]
-
-    >>> list(find_ngrams(tokens, 3))
-    [('a', 'b', 'c')]
-
-    >>> list(find_ngrams(tokens, 4))
-    []
-
-    """
-    if n == 1:
-        return iter(tokens)
-    return zip(*[tokens[i:] for i in range(n)])
-
-
-def find_all_ngrams(
-    tokens: typing.List[str], ngram_range: range
-) -> typing.Iterator[N_GRAM]:
-    """Generates all n-grams in a given range.
-
-    Examples
-    --------
-
-    >>> tokens = ['a', 'b', 'c']
-
-    >>> list(find_all_ngrams(tokens, range(2)))
-    ['a', 'b', 'c']
-
-    >>> list(find_all_ngrams(tokens, range(1, 4)))
-    ['a', 'b', 'c', ('a', 'b'), ('b', 'c'), ('a', 'b', 'c')]
-
-    """
-    return itertools.chain(*(find_ngrams(tokens, n) for n in ngram_range))
-
-
-class VectorizerMixin:
-    """Contains common processing steps used by each vectorizer.
-
-    Parameters
-    ----------
-    on
-        The name of the feature that contains the text to vectorize. If `None`, then each
-        `learn_one` and `transform_one` should treat `x` as a `str` and not as a `dict`.
-    strip_accents
-        Whether or not to strip accent characters.
-    lowercase
-        Whether or not to convert all characters to lowercase.
-    preprocessor
-        Override the preprocessing step while preserving the tokenizing and n-grams generation
-        steps.
-    tokenizer
-        A function used to convert preprocessed text into a `dict` of tokens. A default tokenizer
-        is used if `None` is passed. Set to `False` to disable tokenization.
-    ngram_range
-        The lower and upper boundary of the range n-grams to be extracted. All values of n such
-        that `min_n <= n <= max_n` will be used. For example an `ngram_range` of `(1, 1)` means
-        only unigrams, `(1, 2)` means unigrams and bigrams, and `(2, 2)` means only bigrams.
-
-    Attributes
-    ----------
-    processing_steps : list
-        A list of preprocessing steps that are applied to each text.
-
-    """
-
-    def __init__(
-        self,
-        on: str = None,
-        strip_accents=True,
-        lowercase=True,
-        preprocessor: typing.Callable = None,
-        tokenizer: typing.Callable = None,
-        ngram_range=(1, 1),
-    ):
-        self.on = on
-        self.strip_accents = strip_accents
-        self.lowercase = lowercase
-        self.preprocessor = preprocessor
-        self.tokenizer = (
-            re.compile(r"(?u)\b\w\w+\b").findall if tokenizer is None else tokenizer
-        )
-        self.ngram_range = ngram_range
-
-        self.processing_steps = []
-
-        # Text extraction
-        if on is not None:
-            self.processing_steps.append(operator.itemgetter(on))
-
-        # Preprocessing
-        if preprocessor is not None:
-            self.processing_steps.append(preprocessor)
-        else:
-            if self.strip_accents:
-                self.processing_steps.append(strip_accents_unicode)
-            if self.lowercase:
-                self.processing_steps.append(str.lower)
-
-        # Tokenization
-        if self.tokenizer:
-            self.processing_steps.append(self.tokenizer)
-
-        # n-grams
-        if ngram_range[1] > 1:
-            self.processing_steps.append(
-                functools.partial(
-                    find_all_ngrams,
-                    ngram_range=range(ngram_range[0], ngram_range[1] + 1),
-                )
-            )
-
-    def process_text(self, x):
-        for step in self.processing_steps:
-            x = step(x)
-        return x
-
-    def _more_tags(self):
-        if self.on is None:
-            return {base.tags.TEXT_INPUT}
-        return {}
-
-
-class BagOfWords(base.Transformer, VectorizerMixin):
-    """Counts tokens in sentences.
-
-    This transformer can be used to counts tokens in a given piece of text. It takes care of
-    normalizing the text before tokenizing it. In mini-batch settings, this transformers
-    allows to convert a series of pandas of text into sparse dataframe.
-
-    Note that the parameters are identical to those of `feature_extraction.TFIDF`.
-
-    Parameters
-    ----------
-    on
-        The name of the feature that contains the text to vectorize. If `None`, then each
-        `learn_one` and `transform_one` will assume that each `x` that is provided is a `str`,
-        andnot a `dict`.
-    strip_accents
-        Whether or not to strip accent characters.
-    lowercase
-        Whether or not to convert all characters to lowercase.
-    preprocessor
-        Override the preprocessing step while preserving the tokenizing and n-grams generation
-        steps.
-    tokenizer
-        A function used to convert preprocessed text into a `dict` of tokens. By default, a regex
-        formula that works well in most cases is used.
-    ngram_range
-        The lower and upper boundary of the range n-grams to be extracted. All values of n such
-        that `min_n <= n <= max_n` will be used. For example an `ngram_range` of `(1, 1)` means
-        only unigrams, `(1, 2)` means unigrams and bigrams, and `(2, 2)` means only bigrams.
-
-    Examples
-    --------
-
-    By default, `BagOfWords` will take as input a sentence, preprocess it, tokenize the
-    preprocessed text, and then return a `collections.Counter` containing the number of
-    occurrences of each token.
-
-    >>> from river import feature_extraction as fx
-
-    >>> corpus = [
-    ...     'This is the first document.',
-    ...     'This document is the second document.',
-    ...     'And this is the third one.',
-    ...     'Is this the first document?',
-    ... ]
-
-    >>> bow = fx.BagOfWords()
-
-    >>> for sentence in corpus:
-    ...     print(bow.transform_one(sentence))
-    Counter({'this': 1, 'is': 1, 'the': 1, 'first': 1, 'document': 1})
-    Counter({'document': 2, 'this': 1, 'is': 1, 'the': 1, 'second': 1})
-    Counter({'and': 1, 'this': 1, 'is': 1, 'the': 1, 'third': 1, 'one': 1})
-    Counter({'is': 1, 'this': 1, 'the': 1, 'first': 1, 'document': 1})
-
-    Note that `learn_one` does not have to be called because `BagOfWords` is stateless. You can
-    call it but it won't do anything.
-
-    In the above example, a string is passed to `transform_one`. You can also indicate which
-    field to access if the string is stored in a dictionary:
-
-    >>> bow = fx.BagOfWords(on='sentence')
-
-    >>> for sentence in corpus:
-    ...     x = {'sentence': sentence}
-    ...     print(bow.transform_one(x))
-    Counter({'this': 1, 'is': 1, 'the': 1, 'first': 1, 'document': 1})
-    Counter({'document': 2, 'this': 1, 'is': 1, 'the': 1, 'second': 1})
-    Counter({'and': 1, 'this': 1, 'is': 1, 'the': 1, 'third': 1, 'one': 1})
-    Counter({'is': 1, 'this': 1, 'the': 1, 'first': 1, 'document': 1})
-
-    The `ngram_range` parameter can be used to extract n-grams (including unigrams):
-
-    >>> ngrammer = fx.BagOfWords(ngram_range=(1, 2))
-
-    >>> ngrams = ngrammer.transform_one('I love the smell of napalm in the morning')
-    >>> for ngram, count in ngrams.items():
-    ...     print(ngram, count)
-    love 1
-    the 2
-    smell 1
-    of 1
-    napalm 1
-    in 1
-    morning 1
-    ('love', 'the') 1
-    ('the', 'smell') 1
-    ('smell', 'of') 1
-    ('of', 'napalm') 1
-    ('napalm', 'in') 1
-    ('in', 'the') 1
-    ('the', 'morning') 1
-
-    `BagOfWord` allows to build a term-frequency pandas sparse dataframe with the `transform_many` method.
-
-    >>> import pandas as pd
-    >>> X = pd.Series(['Hello world', 'Hello River'], index = ['river', 'rocks'])
-    >>> bow = fx.BagOfWords()
-    >>> bow.transform_many(X=X)
-           hello  world  river
-    river      1      1      0
-    rocks      1      0      1
-
-    """
-
-    def transform_one(self, x):
-        return collections.Counter(self.process_text(x))
-
-    def transform_many(self, X: pd.Series) -> pd.DataFrame:
-        """Transform pandas series of string into term-frequency pandas sparse dataframe."""
-        indptr, indices, data = [0], [], []
-        index = {}
-
-        for d in X:
-            for t, f in collections.Counter(self.process_text(d)).items():
-                indices.append(index.setdefault(t, len(index)))
-                data.append(f)
-
-            indptr.append(len(data))
-
-        return pd.DataFrame.sparse.from_spmatrix(
-            sparse.csr_matrix((data, indices, indptr)),
-            index=X.index,
-            columns=index.keys(),
-        )
-
-    def learn_many(self, X):
-        return self
-
-
-class TFIDF(BagOfWords):
-    """Computes TF-IDF values from sentences.
-
-    The TF-IDF formula is the same one as scikit-learn. The only difference is the fact that the
-    document frequencies are determined online, whereas in a batch setting they can be determined
-    by performing an initial pass through the data.
-
-    Note that the parameters are identical to those of `feature_extraction.BagOfWords`.
-
-    Parameters
-    ----------
-    normalize
-        Whether or not the TF-IDF values by their L2 norm.
-    on
-        The name of the feature that contains the text to vectorize. If `None`, then the input is
-        treated as a document instead of a set of features.
-    strip_accents
-        Whether or not to strip accent characters.
-    lowercase
-        Whether or not to convert all characters to lowercase.
-    preprocessor
-        Override the preprocessing step while preserving the tokenizing and n-grams generation
-        steps.
-    tokenizer
-        A function used to convert preprocessed text into a `dict` of tokens. By default, a regex
-        formula that works well in most cases is used.
-    ngram_range
-        The lower and upper boundary of the range n-grams to be extracted. All values of n such
-        that `min_n <= n <= max_n` will be used. For example an `ngram_range` of `(1, 1)` means
-        only unigrams, `(1, 2)` means unigrams and bigrams, and `(2, 2)` means only bigrams. Only
-        works if `tokenizer` is not set to `False`.
-
-    Attributes
-    ----------
-    dfs : collections.defaultdict)
-        Document counts.
-    n : int
-        Number of scanned documents.
-
-    Examples
-    --------
-
-    >>> from river import feature_extraction
-
-    >>> tfidf = feature_extraction.TFIDF()
-
-    >>> corpus = [
-    ...     'This is the first document.',
-    ...     'This document is the second document.',
-    ...     'And this is the third one.',
-    ...     'Is this the first document?',
-    ... ]
-
-    >>> for sentence in corpus:
-    ...     tfidf = tfidf.learn_one(sentence)
-    ...     print(tfidf.transform_one(sentence))
-    {'this': 0.447, 'is': 0.447, 'the': 0.447, 'first': 0.447, 'document': 0.447}
-    {'this': 0.333, 'document': 0.667, 'is': 0.333, 'the': 0.333, 'second': 0.469}
-    {'and': 0.497, 'this': 0.293, 'is': 0.293, 'the': 0.293, 'third': 0.497, 'one': 0.497}
-    {'is': 0.384, 'this': 0.384, 'the': 0.384, 'first': 0.580, 'document': 0.469}
-
-    In the above example, a string is passed to `transform_one`. You can also indicate which
-    field to access if the string is stored in a dictionary:
-
-    >>> tfidf = feature_extraction.TFIDF(on='sentence')
-
-    >>> for sentence in corpus:
-    ...     x = {'sentence': sentence}
-    ...     tfidf = tfidf.learn_one(x)
-    ...     print(tfidf.transform_one(x))
-    {'this': 0.447, 'is': 0.447, 'the': 0.447, 'first': 0.447, 'document': 0.447}
-    {'this': 0.333, 'document': 0.667, 'is': 0.333, 'the': 0.333, 'second': 0.469}
-    {'and': 0.497, 'this': 0.293, 'is': 0.293, 'the': 0.293, 'third': 0.497, 'one': 0.497}
-    {'is': 0.384, 'this': 0.384, 'the': 0.384, 'first': 0.580, 'document': 0.469}
-
-    """
-
-    def __init__(
-        self,
-        normalize=True,
-        on: str = None,
-        strip_accents=True,
-        lowercase=True,
-        preprocessor: typing.Callable = None,
-        tokenizer: typing.Callable = None,
-        ngram_range=(1, 1),
-    ):
-        super().__init__(
-            on=on,
-            strip_accents=strip_accents,
-            lowercase=lowercase,
-            preprocessor=preprocessor,
-            tokenizer=tokenizer,
-            ngram_range=ngram_range,
-        )
-        self.normalize = normalize
-        self.dfs = collections.Counter()
-        self.n = 0
-
-    def learn_one(self, x):
-
-        # Update the document counts
-        terms = self.process_text(x)
-        self.dfs.update(set(terms))
-
-        # Increment the global document counter
-        self.n += 1
-
-        return self
-
-    def transform_one(self, x):
-
-        term_counts = super().transform_one(x)
-        n_terms = sum(term_counts.values())
-
-        tfidfs = {}
-
-        for term, count in term_counts.items():
-            tf = count / n_terms
-            idf = math.log(((1 + self.n) / (1 + self.dfs[term]))) + 1
-            tfidfs[term] = tf * idf
-
-        if self.normalize:
-            norm = math.sqrt(sum(tfidf ** 2 for tfidf in tfidfs.values()))
-            return {term: tfidf / norm for term, tfidf in tfidfs.items()}
-        return tfidfs
+import collections
+import functools
+import itertools
+import math
+import operator
+import re
+import typing
+import unicodedata
+
+import pandas as pd
+from scipy import sparse
+
+from river import base
+
+__all__ = ["BagOfWords", "TFIDF"]
+
+
+N_GRAM = typing.Union[str, typing.Tuple[str, ...]]  # unigram  # n-gram
+
+
+def strip_accents_unicode(s: str) -> str:
+    """Transform accentuated unicode symbols into their ASCII counterpart."""
+    try:
+        # If `s` is ASCII-compatible, then it does not contain any accented
+        # characters and we can avoid an expensive list comprehension
+        s.encode("ASCII", errors="strict")
+        return s
+    except UnicodeEncodeError:
+        normalized = unicodedata.normalize("NFKD", s)
+        return "".join([c for c in normalized if not unicodedata.combining(c)])
+
+
+def find_ngrams(tokens: typing.List[str], n: int) -> typing.Iterator[N_GRAM]:
+    """Generates n-grams from a list of tokens.
+
+    From http://www.locallyoptimal.com/blog/2013/01/20/elegant-n-gram-generation-in-python/.
+
+    Examples
+    --------
+
+    >>> tokens = ['a', 'b', 'c']
+
+    >>> list(find_ngrams(tokens, 0))
+    []
+
+    >>> list(find_ngrams(tokens, 1))
+    ['a', 'b', 'c']
+
+    >>> list(find_ngrams(tokens, 2))
+    [('a', 'b'), ('b', 'c')]
+
+    >>> list(find_ngrams(tokens, 3))
+    [('a', 'b', 'c')]
+
+    >>> list(find_ngrams(tokens, 4))
+    []
+
+    """
+    if n == 1:
+        return iter(tokens)
+    return zip(*[tokens[i:] for i in range(n)])
+
+
+def find_all_ngrams(
+    tokens: typing.List[str], ngram_range: range
+) -> typing.Iterator[N_GRAM]:
+    """Generates all n-grams in a given range.
+
+    Examples
+    --------
+
+    >>> tokens = ['a', 'b', 'c']
+
+    >>> list(find_all_ngrams(tokens, range(2)))
+    ['a', 'b', 'c']
+
+    >>> list(find_all_ngrams(tokens, range(1, 4)))
+    ['a', 'b', 'c', ('a', 'b'), ('b', 'c'), ('a', 'b', 'c')]
+
+    """
+    return itertools.chain(*(find_ngrams(tokens, n) for n in ngram_range))
+
+
+class VectorizerMixin:
+    """Contains common processing steps used by each vectorizer.
+
+    Parameters
+    ----------
+    on
+        The name of the feature that contains the text to vectorize. If `None`, then each
+        `learn_one` and `transform_one` should treat `x` as a `str` and not as a `dict`.
+    strip_accents
+        Whether or not to strip accent characters.
+    lowercase
+        Whether or not to convert all characters to lowercase.
+    preprocessor
+        Override the preprocessing step while preserving the tokenizing and n-grams generation
+        steps.
+    tokenizer
+        A function used to convert preprocessed text into a `dict` of tokens. A default tokenizer
+        is used if `None` is passed. Set to `False` to disable tokenization.
+    ngram_range
+        The lower and upper boundary of the range n-grams to be extracted. All values of n such
+        that `min_n <= n <= max_n` will be used. For example an `ngram_range` of `(1, 1)` means
+        only unigrams, `(1, 2)` means unigrams and bigrams, and `(2, 2)` means only bigrams.
+
+    Attributes
+    ----------
+    processing_steps : list
+        A list of preprocessing steps that are applied to each text.
+
+    """
+
+    def __init__(
+        self,
+        on: str = None,
+        strip_accents=True,
+        lowercase=True,
+        preprocessor: typing.Callable = None,
+        tokenizer: typing.Callable = None,
+        ngram_range=(1, 1),
+    ):
+        self.on = on
+        self.strip_accents = strip_accents
+        self.lowercase = lowercase
+        self.preprocessor = preprocessor
+        self.tokenizer = (
+            re.compile(r"(?u)\b\w\w+\b").findall if tokenizer is None else tokenizer
+        )
+        self.ngram_range = ngram_range
+
+        self.processing_steps = []
+
+        # Text extraction
+        if on is not None:
+            self.processing_steps.append(operator.itemgetter(on))
+
+        # Preprocessing
+        if preprocessor is not None:
+            self.processing_steps.append(preprocessor)
+        else:
+            if self.strip_accents:
+                self.processing_steps.append(strip_accents_unicode)
+            if self.lowercase:
+                self.processing_steps.append(str.lower)
+
+        # Tokenization
+        if self.tokenizer:
+            self.processing_steps.append(self.tokenizer)
+
+        # n-grams
+        if ngram_range[1] > 1:
+            self.processing_steps.append(
+                functools.partial(
+                    find_all_ngrams,
+                    ngram_range=range(ngram_range[0], ngram_range[1] + 1),
+                )
+            )
+
+    def process_text(self, x):
+        for step in self.processing_steps:
+            x = step(x)
+        return x
+
+    def _more_tags(self):
+        if self.on is None:
+            return {base.tags.TEXT_INPUT}
+        return {}
+
+
+class BagOfWords(base.Transformer, VectorizerMixin):
+    """Counts tokens in sentences.
+
+    This transformer can be used to counts tokens in a given piece of text. It takes care of
+    normalizing the text before tokenizing it. In mini-batch settings, this transformers
+    allows to convert a series of pandas of text into sparse dataframe.
+
+    Note that the parameters are identical to those of `feature_extraction.TFIDF`.
+
+    Parameters
+    ----------
+    on
+        The name of the feature that contains the text to vectorize. If `None`, then each
+        `learn_one` and `transform_one` will assume that each `x` that is provided is a `str`,
+        andnot a `dict`.
+    strip_accents
+        Whether or not to strip accent characters.
+    lowercase
+        Whether or not to convert all characters to lowercase.
+    preprocessor
+        Override the preprocessing step while preserving the tokenizing and n-grams generation
+        steps.
+    tokenizer
+        A function used to convert preprocessed text into a `dict` of tokens. By default, a regex
+        formula that works well in most cases is used.
+    ngram_range
+        The lower and upper boundary of the range n-grams to be extracted. All values of n such
+        that `min_n <= n <= max_n` will be used. For example an `ngram_range` of `(1, 1)` means
+        only unigrams, `(1, 2)` means unigrams and bigrams, and `(2, 2)` means only bigrams.
+
+    Examples
+    --------
+
+    By default, `BagOfWords` will take as input a sentence, preprocess it, tokenize the
+    preprocessed text, and then return a `collections.Counter` containing the number of
+    occurrences of each token.
+
+    >>> from river import feature_extraction as fx
+
+    >>> corpus = [
+    ...     'This is the first document.',
+    ...     'This document is the second document.',
+    ...     'And this is the third one.',
+    ...     'Is this the first document?',
+    ... ]
+
+    >>> bow = fx.BagOfWords()
+
+    >>> for sentence in corpus:
+    ...     print(bow.transform_one(sentence))
+    Counter({'this': 1, 'is': 1, 'the': 1, 'first': 1, 'document': 1})
+    Counter({'document': 2, 'this': 1, 'is': 1, 'the': 1, 'second': 1})
+    Counter({'and': 1, 'this': 1, 'is': 1, 'the': 1, 'third': 1, 'one': 1})
+    Counter({'is': 1, 'this': 1, 'the': 1, 'first': 1, 'document': 1})
+
+    Note that `learn_one` does not have to be called because `BagOfWords` is stateless. You can
+    call it but it won't do anything.
+
+    In the above example, a string is passed to `transform_one`. You can also indicate which
+    field to access if the string is stored in a dictionary:
+
+    >>> bow = fx.BagOfWords(on='sentence')
+
+    >>> for sentence in corpus:
+    ...     x = {'sentence': sentence}
+    ...     print(bow.transform_one(x))
+    Counter({'this': 1, 'is': 1, 'the': 1, 'first': 1, 'document': 1})
+    Counter({'document': 2, 'this': 1, 'is': 1, 'the': 1, 'second': 1})
+    Counter({'and': 1, 'this': 1, 'is': 1, 'the': 1, 'third': 1, 'one': 1})
+    Counter({'is': 1, 'this': 1, 'the': 1, 'first': 1, 'document': 1})
+
+    The `ngram_range` parameter can be used to extract n-grams (including unigrams):
+
+    >>> ngrammer = fx.BagOfWords(ngram_range=(1, 2))
+
+    >>> ngrams = ngrammer.transform_one('I love the smell of napalm in the morning')
+    >>> for ngram, count in ngrams.items():
+    ...     print(ngram, count)
+    love 1
+    the 2
+    smell 1
+    of 1
+    napalm 1
+    in 1
+    morning 1
+    ('love', 'the') 1
+    ('the', 'smell') 1
+    ('smell', 'of') 1
+    ('of', 'napalm') 1
+    ('napalm', 'in') 1
+    ('in', 'the') 1
+    ('the', 'morning') 1
+
+    `BagOfWord` allows to build a term-frequency pandas sparse dataframe with the `transform_many` method.
+
+    >>> import pandas as pd
+    >>> X = pd.Series(['Hello world', 'Hello River'], index = ['river', 'rocks'])
+    >>> bow = fx.BagOfWords()
+    >>> bow.transform_many(X=X)
+           hello  world  river
+    river      1      1      0
+    rocks      1      0      1
+
+    """
+
+    def transform_one(self, x):
+        return collections.Counter(self.process_text(x))
+
+    def transform_many(self, X: pd.Series) -> pd.DataFrame:
+        """Transform pandas series of string into term-frequency pandas sparse dataframe."""
+        indptr, indices, data = [0], [], []
+        index = {}
+
+        for d in X:
+            for t, f in collections.Counter(self.process_text(d)).items():
+                indices.append(index.setdefault(t, len(index)))
+                data.append(f)
+
+            indptr.append(len(data))
+
+        return pd.DataFrame.sparse.from_spmatrix(
+            sparse.csr_matrix((data, indices, indptr)),
+            index=X.index,
+            columns=index.keys(),
+        )
+
+    def learn_many(self, X):
+        return self
+
+
+class TFIDF(BagOfWords):
+    """Computes TF-IDF values from sentences.
+
+    The TF-IDF formula is the same one as scikit-learn. The only difference is the fact that the
+    document frequencies are determined online, whereas in a batch setting they can be determined
+    by performing an initial pass through the data.
+
+    Note that the parameters are identical to those of `feature_extraction.BagOfWords`.
+
+    Parameters
+    ----------
+    normalize
+        Whether or not the TF-IDF values by their L2 norm.
+    on
+        The name of the feature that contains the text to vectorize. If `None`, then the input is
+        treated as a document instead of a set of features.
+    strip_accents
+        Whether or not to strip accent characters.
+    lowercase
+        Whether or not to convert all characters to lowercase.
+    preprocessor
+        Override the preprocessing step while preserving the tokenizing and n-grams generation
+        steps.
+    tokenizer
+        A function used to convert preprocessed text into a `dict` of tokens. By default, a regex
+        formula that works well in most cases is used.
+    ngram_range
+        The lower and upper boundary of the range n-grams to be extracted. All values of n such
+        that `min_n <= n <= max_n` will be used. For example an `ngram_range` of `(1, 1)` means
+        only unigrams, `(1, 2)` means unigrams and bigrams, and `(2, 2)` means only bigrams. Only
+        works if `tokenizer` is not set to `False`.
+
+    Attributes
+    ----------
+    dfs : collections.defaultdict)
+        Document counts.
+    n : int
+        Number of scanned documents.
+
+    Examples
+    --------
+
+    >>> from river import feature_extraction
+
+    >>> tfidf = feature_extraction.TFIDF()
+
+    >>> corpus = [
+    ...     'This is the first document.',
+    ...     'This document is the second document.',
+    ...     'And this is the third one.',
+    ...     'Is this the first document?',
+    ... ]
+
+    >>> for sentence in corpus:
+    ...     tfidf = tfidf.learn_one(sentence)
+    ...     print(tfidf.transform_one(sentence))
+    {'this': 0.447, 'is': 0.447, 'the': 0.447, 'first': 0.447, 'document': 0.447}
+    {'this': 0.333, 'document': 0.667, 'is': 0.333, 'the': 0.333, 'second': 0.469}
+    {'and': 0.497, 'this': 0.293, 'is': 0.293, 'the': 0.293, 'third': 0.497, 'one': 0.497}
+    {'is': 0.384, 'this': 0.384, 'the': 0.384, 'first': 0.580, 'document': 0.469}
+
+    In the above example, a string is passed to `transform_one`. You can also indicate which
+    field to access if the string is stored in a dictionary:
+
+    >>> tfidf = feature_extraction.TFIDF(on='sentence')
+
+    >>> for sentence in corpus:
+    ...     x = {'sentence': sentence}
+    ...     tfidf = tfidf.learn_one(x)
+    ...     print(tfidf.transform_one(x))
+    {'this': 0.447, 'is': 0.447, 'the': 0.447, 'first': 0.447, 'document': 0.447}
+    {'this': 0.333, 'document': 0.667, 'is': 0.333, 'the': 0.333, 'second': 0.469}
+    {'and': 0.497, 'this': 0.293, 'is': 0.293, 'the': 0.293, 'third': 0.497, 'one': 0.497}
+    {'is': 0.384, 'this': 0.384, 'the': 0.384, 'first': 0.580, 'document': 0.469}
+
+    """
+
+    def __init__(
+        self,
+        normalize=True,
+        on: str = None,
+        strip_accents=True,
+        lowercase=True,
+        preprocessor: typing.Callable = None,
+        tokenizer: typing.Callable = None,
+        ngram_range=(1, 1),
+    ):
+        super().__init__(
+            on=on,
+            strip_accents=strip_accents,
+            lowercase=lowercase,
+            preprocessor=preprocessor,
+            tokenizer=tokenizer,
+            ngram_range=ngram_range,
+        )
+        self.normalize = normalize
+        self.dfs = collections.Counter()
+        self.n = 0
+
+    def learn_one(self, x):
+
+        # Update the document counts
+        terms = self.process_text(x)
+        self.dfs.update(set(terms))
+
+        # Increment the global document counter
+        self.n += 1
+
+        return self
+
+    def transform_one(self, x):
+
+        term_counts = super().transform_one(x)
+        n_terms = sum(term_counts.values())
+
+        tfidfs = {}
+
+        for term, count in term_counts.items():
+            tf = count / n_terms
+            idf = math.log(((1 + self.n) / (1 + self.dfs[term]))) + 1
+            tfidfs[term] = tf * idf
+
+        if self.normalize:
+            norm = math.sqrt(sum(tfidf ** 2 for tfidf in tfidfs.values()))
+            return {term: tfidf / norm for term, tfidf in tfidfs.items()}
+        return tfidfs
```

### Comparing `river-0.8.0/river/feature_selection/k_best.py` & `river-0.9.0/river/feature_selection/k_best.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,92 +1,92 @@
-import collections
-import copy
-import functools
-
-from river import base, stats
-
-
-class SelectKBest(base.SupervisedTransformer):
-    """Removes all but the $k$ highest scoring features.
-
-    Parameters
-    ----------
-    similarity
-    k
-        The number of features to keep.
-
-    Attributes
-    ----------
-    similarities : dict
-        The similarity instances used for each feature.
-    leaderboard : dict
-        The actual similarity measures.
-
-    Examples
-    --------
-
-    >>> from pprint import pprint
-    >>> from river import feature_selection
-    >>> from river import stats
-    >>> from river import stream
-    >>> from sklearn import datasets
-
-    >>> X, y = datasets.make_regression(
-    ...     n_samples=100,
-    ...     n_features=10,
-    ...     n_informative=2,
-    ...     random_state=42
-    ... )
-
-    >>> selector = feature_selection.SelectKBest(
-    ...     similarity=stats.PearsonCorr(),
-    ...     k=2
-    ... )
-
-    >>> for xi, yi, in stream.iter_array(X, y):
-    ...     selector = selector.learn_one(xi, yi)
-
-    >>> pprint(selector.leaderboard)
-    Counter({9: 0.7898,
-            7: 0.5444,
-            8: 0.1062,
-            2: 0.0638,
-            4: 0.0538,
-            5: 0.0271,
-            1: -0.0312,
-            6: -0.0657,
-            3: -0.1501,
-            0: -0.1895})
-
-    >>> selector.transform_one(xi)
-    {7: -1.2795, 9: -1.8408}
-
-    """
-
-    def __init__(self, similarity: stats.Bivariate, k=10):
-        self.k = k
-        self.similarity = similarity
-        self.similarities = collections.defaultdict(
-            functools.partial(copy.deepcopy, similarity)
-        )
-        self.leaderboard = collections.Counter()
-
-    @classmethod
-    def _unit_test_params(cls):
-        return {"similarity": stats.PearsonCorr()}
-
-    def learn_one(self, x, y):
-
-        for i, xi in x.items():
-            self.leaderboard[i] = self.similarities[i].update(xi, y).get()
-
-        return self
-
-    def transform_one(self, x):
-
-        best_features = set(pair[0] for pair in self.leaderboard.most_common(self.k))
-
-        if self.leaderboard:
-
-            return {i: xi for i, xi in x.items() if i in best_features}
-
-        return copy.deepcopy(x)
+import collections
+import copy
+import functools
+
+from river import base, stats
+
+
+class SelectKBest(base.SupervisedTransformer):
+    """Removes all but the $k$ highest scoring features.
+
+    Parameters
+    ----------
+    similarity
+    k
+        The number of features to keep.
+
+    Attributes
+    ----------
+    similarities : dict
+        The similarity instances used for each feature.
+    leaderboard : dict
+        The actual similarity measures.
+
+    Examples
+    --------
+
+    >>> from pprint import pprint
+    >>> from river import feature_selection
+    >>> from river import stats
+    >>> from river import stream
+    >>> from sklearn import datasets
+
+    >>> X, y = datasets.make_regression(
+    ...     n_samples=100,
+    ...     n_features=10,
+    ...     n_informative=2,
+    ...     random_state=42
+    ... )
+
+    >>> selector = feature_selection.SelectKBest(
+    ...     similarity=stats.PearsonCorr(),
+    ...     k=2
+    ... )
+
+    >>> for xi, yi, in stream.iter_array(X, y):
+    ...     selector = selector.learn_one(xi, yi)
+
+    >>> pprint(selector.leaderboard)
+    Counter({9: 0.7898,
+            7: 0.5444,
+            8: 0.1062,
+            2: 0.0638,
+            4: 0.0538,
+            5: 0.0271,
+            1: -0.0312,
+            6: -0.0657,
+            3: -0.1501,
+            0: -0.1895})
+
+    >>> selector.transform_one(xi)
+    {7: -1.2795, 9: -1.8408}
+
+    """
+
+    def __init__(self, similarity: stats.Bivariate, k=10):
+        self.k = k
+        self.similarity = similarity
+        self.similarities = collections.defaultdict(
+            functools.partial(copy.deepcopy, similarity)
+        )
+        self.leaderboard = collections.Counter()
+
+    @classmethod
+    def _unit_test_params(cls):
+        yield {"similarity": stats.PearsonCorr()}
+
+    def learn_one(self, x, y):
+
+        for i, xi in x.items():
+            self.leaderboard[i] = self.similarities[i].update(xi, y).get()
+
+        return self
+
+    def transform_one(self, x):
+
+        best_features = set(pair[0] for pair in self.leaderboard.most_common(self.k))
+
+        if self.leaderboard:
+
+            return {i: xi for i, xi in x.items() if i in best_features}
+
+        return copy.deepcopy(x)
```

### Comparing `river-0.8.0/river/feature_selection/random.py` & `river-0.9.0/river/feature_selection/random.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,70 +1,70 @@
-import random
-
-from river import base
-
-__all__ = ["PoissonInclusion"]
-
-
-class PoissonInclusion(base.Transformer):
-    """Randomly selects features with an inclusion trial.
-
-    When a new feature is encountered, it is selected with probability `p`. The number of times a
-    feature needs to beseen before it is added to the model follows a geometric distribution with
-    expected value `1 / p`. This feature selection method is meant to be used when you have a
-    very large amount of sparse features.
-
-    Parameters
-    ----------
-    p
-        Probability of including a feature the first time it is encountered.
-    seed
-        Random seed value used for reproducibility.
-
-    Examples
-    --------
-
-    >>> from river import datasets
-    >>> from river import feature_selection
-    >>> from river import stream
-
-    >>> selector = feature_selection.PoissonInclusion(p=0.1, seed=42)
-
-    >>> dataset = iter(datasets.TrumpApproval())
-
-    >>> feature_names = next(dataset)[0].keys()
-    >>> n = 0
-
-    >>> while True:
-    ...     x, y = next(dataset)
-    ...     xt = selector.transform_one(x)
-    ...     if xt.keys() == feature_names:
-    ...         break
-    ...     n += 1
-
-    >>> n
-    12
-
-    References
-    ----------
-    [^1]: [McMahan, H.B., Holt, G., Sculley, D., Young, M., Ebner, D., Grady, J., Nie, L., Phillips, T., Davydov, E., Golovin, D. and Chikkerur, S., 2013, August. Ad click prediction: a view from the trenches. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 1222-1230)](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41159.pdf)
-
-    """
-
-    def __init__(self, p: float, seed: int = None):
-        self.p = p
-        self.seed = seed
-        self.rng = random.Random(seed)
-        self.included = set()
-
-    def transform_one(self, x):
-
-        xt = {}
-
-        for i, xi in x.items():
-            if i in self.included:
-                xt[i] = xi
-            elif self.rng.random() < self.p:
-                self.included.add(i)
-                xt[i] = xi
-
-        return xt
+import random
+
+from river import base
+
+__all__ = ["PoissonInclusion"]
+
+
+class PoissonInclusion(base.Transformer):
+    """Randomly selects features with an inclusion trial.
+
+    When a new feature is encountered, it is selected with probability `p`. The number of times a
+    feature needs to beseen before it is added to the model follows a geometric distribution with
+    expected value `1 / p`. This feature selection method is meant to be used when you have a
+    very large amount of sparse features.
+
+    Parameters
+    ----------
+    p
+        Probability of including a feature the first time it is encountered.
+    seed
+        Random seed value used for reproducibility.
+
+    Examples
+    --------
+
+    >>> from river import datasets
+    >>> from river import feature_selection
+    >>> from river import stream
+
+    >>> selector = feature_selection.PoissonInclusion(p=0.1, seed=42)
+
+    >>> dataset = iter(datasets.TrumpApproval())
+
+    >>> feature_names = next(dataset)[0].keys()
+    >>> n = 0
+
+    >>> while True:
+    ...     x, y = next(dataset)
+    ...     xt = selector.transform_one(x)
+    ...     if xt.keys() == feature_names:
+    ...         break
+    ...     n += 1
+
+    >>> n
+    12
+
+    References
+    ----------
+    [^1]: [McMahan, H.B., Holt, G., Sculley, D., Young, M., Ebner, D., Grady, J., Nie, L., Phillips, T., Davydov, E., Golovin, D. and Chikkerur, S., 2013, August. Ad click prediction: a view from the trenches. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 1222-1230)](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41159.pdf)
+
+    """
+
+    def __init__(self, p: float, seed: int = None):
+        self.p = p
+        self.seed = seed
+        self.rng = random.Random(seed)
+        self.included = set()
+
+    def transform_one(self, x):
+
+        xt = {}
+
+        for i, xi in x.items():
+            if i in self.included:
+                xt[i] = xi
+            elif self.rng.random() < self.p:
+                self.included.add(i)
+                xt[i] = xi
+
+        return xt
```

### Comparing `river-0.8.0/river/feature_selection/variance.py` & `river-0.9.0/river/feature_selection/variance.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,63 +1,63 @@
-import collections
-
-from river import base, stats
-
-
-class VarianceThreshold(base.Transformer):
-    """Removes low-variance features.
-
-    Parameters
-    ----------
-    threshold
-        Only features with a variance above the threshold will be kept.
-    min_samples
-        The minimum number of samples required to perform selection.
-
-    Attributes
-    ----------
-    variances : dict
-        The variance of each feature.
-
-    Examples
-    --------
-
-    >>> from river import feature_selection
-    >>> from river import stream
-
-    >>> X = [
-    ...     [0, 2, 0, 3],
-    ...     [0, 1, 4, 3],
-    ...     [0, 1, 1, 3]
-    ... ]
-
-    >>> selector = feature_selection.VarianceThreshold()
-
-    >>> for x, _ in stream.iter_array(X):
-    ...     print(selector.learn_one(x).transform_one(x))
-    {0: 0, 1: 2, 2: 0, 3: 3}
-    {1: 1, 2: 4}
-    {1: 1, 2: 1}
-
-    """
-
-    def __init__(self, threshold=0, min_samples=2):
-        self.threshold = threshold
-        self.min_samples = min_samples
-        self.variances = collections.defaultdict(stats.Var)
-
-    def learn_one(self, x):
-
-        for i, xi in x.items():
-            self.variances[i].update(xi)
-
-        return self
-
-    def check_feature(self, feature):
-        if feature not in self.variances:
-            return True
-        if self.variances[feature].mean.n < self.min_samples:
-            return True
-        return self.variances[feature].get() > self.threshold
-
-    def transform_one(self, x):
-        return {i: xi for i, xi in x.items() if self.check_feature(i)}
+import collections
+
+from river import base, stats
+
+
+class VarianceThreshold(base.Transformer):
+    """Removes low-variance features.
+
+    Parameters
+    ----------
+    threshold
+        Only features with a variance above the threshold will be kept.
+    min_samples
+        The minimum number of samples required to perform selection.
+
+    Attributes
+    ----------
+    variances : dict
+        The variance of each feature.
+
+    Examples
+    --------
+
+    >>> from river import feature_selection
+    >>> from river import stream
+
+    >>> X = [
+    ...     [0, 2, 0, 3],
+    ...     [0, 1, 4, 3],
+    ...     [0, 1, 1, 3]
+    ... ]
+
+    >>> selector = feature_selection.VarianceThreshold()
+
+    >>> for x, _ in stream.iter_array(X):
+    ...     print(selector.learn_one(x).transform_one(x))
+    {0: 0, 1: 2, 2: 0, 3: 3}
+    {1: 1, 2: 4}
+    {1: 1, 2: 1}
+
+    """
+
+    def __init__(self, threshold=0, min_samples=2):
+        self.threshold = threshold
+        self.min_samples = min_samples
+        self.variances = collections.defaultdict(stats.Var)
+
+    def learn_one(self, x):
+
+        for i, xi in x.items():
+            self.variances[i].update(xi)
+
+        return self
+
+    def check_feature(self, feature):
+        if feature not in self.variances:
+            return True
+        if self.variances[feature].mean.n < self.min_samples:
+            return True
+        return self.variances[feature].get() > self.threshold
+
+    def transform_one(self, x):
+        return {i: xi for i, xi in x.items() if self.check_feature(i)}
```

### Comparing `river-0.8.0/river/imblearn/random.py` & `river-0.9.0/river/imblearn/random.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,262 +1,262 @@
-import collections
-
-import numpy as np
-
-from river import base
-
-
-class ClassificationSampler(base.WrapperMixin, base.Classifier):
-    def __init__(self, classifier, seed=None):
-        self.classifier = classifier
-        self.seed = seed
-        self._rng = np.random.RandomState(seed)
-
-    @property
-    def _wrapped_model(self):
-        return self.classifier
-
-    def predict_proba_one(self, x):
-        return self.classifier.predict_proba_one(x)
-
-    def predict_one(self, x):
-        return self.classifier.predict_one(x)
-
-
-class RandomUnderSampler(ClassificationSampler):
-    """Random under-sampling.
-
-    This is a wrapper for classifiers. It will train the provided classifier by under-sampling the
-    stream of given observations so that the class distribution seen by the classifier follows
-    a given desired distribution. The implementation is a discrete version of rejection sampling.
-
-    See [Working with imbalanced data](/user-guide/imbalanced-learning) for example usage.
-
-    Parameters
-    ----------
-    classifier
-    desired_dist
-        The desired class distribution. The keys are the classes whilst the values are the desired
-        class percentages. The values must sum up to 1.
-    seed
-        Random seed for reproducibility.
-
-    Examples
-    --------
-
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import imblearn
-    >>> from river import linear_model
-    >>> from river import metrics
-    >>> from river import preprocessing
-
-    >>> model = imblearn.RandomUnderSampler(
-    ...     (
-    ...         preprocessing.StandardScaler() |
-    ...         linear_model.LogisticRegression()
-    ...     ),
-    ...     desired_dist={False: 0.4, True: 0.6},
-    ...     seed=42
-    ... )
-
-    >>> dataset = datasets.CreditCard().take(3000)
-
-    >>> metric = metrics.LogLoss()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    LogLoss: 0.07292
-
-    References
-    ----------
-    [^1]: [Under-sampling a dataset with desired ratios](https://maxhalford.github.io/blog/undersampling-ratios/)
-    [^2]: [Wikipedia article on rejection sampling](https://www.wikiwand.com/en/Rejection_sampling)
-
-    """
-
-    def __init__(
-        self, classifier: base.Classifier, desired_dist: dict, seed: int = None
-    ):
-        super().__init__(classifier=classifier, seed=seed)
-        self.desired_dist = desired_dist
-        self._actual_dist = collections.Counter()
-        self._pivot = None
-
-    def learn_one(self, x, y):
-
-        self._actual_dist[y] += 1
-        f = self.desired_dist
-        g = self._actual_dist
-
-        # Check if the pivot needs to be changed
-        if y != self._pivot:
-            self._pivot = max(g.keys(), key=lambda y: f[y] / g[y])
-        else:
-            self.classifier.learn_one(x, y)
-            return self
-
-        # Determine the sampling ratio if the class is not the pivot
-        M = f[self._pivot] / g[self._pivot]  # Likelihood ratio
-        ratio = f[y] / (M * g[y])
-
-        if ratio < 1 and self._rng.random() < ratio:
-            self.classifier.learn_one(x, y)
-
-        return self
-
-
-class RandomOverSampler(ClassificationSampler):
-    """Random over-sampling.
-
-    This is a wrapper for classifiers. It will train the provided classifier by over-sampling the
-    stream of given observations so that the class distribution seen by the classifier follows
-    a given desired distribution. The implementation is a discrete version of reverse rejection
-    sampling.
-
-    See [Working with imbalanced data](/user-guide/imbalanced-learning) for example usage.
-
-    Parameters
-    ----------
-    classifier
-    desired_dist
-        The desired class distribution. The keys are the classes whilst the values are the desired
-        class percentages. The values must sum up to 1.
-    seed
-        Random seed for reproducibility.
-
-    Examples
-    --------
-
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import imblearn
-    >>> from river import linear_model
-    >>> from river import metrics
-    >>> from river import preprocessing
-
-    >>> model = imblearn.RandomOverSampler(
-    ...     (
-    ...         preprocessing.StandardScaler() |
-    ...         linear_model.LogisticRegression()
-    ...     ),
-    ...     desired_dist={False: 0.4, True: 0.6},
-    ...     seed=42
-    ... )
-
-    >>> dataset = datasets.CreditCard().take(3000)
-
-    >>> metric = metrics.LogLoss()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    LogLoss: 0.05421
-
-    """
-
-    def __init__(
-        self, classifier: base.Classifier, desired_dist: dict, seed: int = None
-    ):
-        super().__init__(classifier=classifier, seed=seed)
-        self.desired_dist = desired_dist
-        self._actual_dist = collections.Counter()
-        self._pivot = None
-
-    def learn_one(self, x, y):
-
-        self._actual_dist[y] += 1
-        f = self.desired_dist
-        g = self._actual_dist
-
-        # Check if the pivot needs to be changed
-        if y != self._pivot:
-            self._pivot = max(g.keys(), key=lambda y: g[y] / f[y])
-        else:
-            self.classifier.learn_one(x, y)
-            return self
-
-        M = g[self._pivot] / f[self._pivot]
-        rate = M * f[y] / g[y]
-
-        for _ in range(self._rng.poisson(rate)):
-            self.classifier.learn_one(x, y)
-
-        return self
-
-
-class RandomSampler(ClassificationSampler):
-    """Random sampling by mixing under-sampling and over-sampling.
-
-    This is a wrapper for classifiers. It will train the provided classifier by both under-sampling
-    and over-sampling the stream of given observations so that the class distribution seen by the
-    classifier follows a given desired distribution.
-
-    See [Working with imbalanced data](/user-guide/imbalanced-learning) for example usage.
-
-    Parameters
-    ----------
-    classifier
-    desired_dist
-        The desired class distribution. The keys are the classes whilst the values are the desired
-        class percentages. The values must sum up to 1. If set to `None`, then the observations
-        will be sampled uniformly at random, which is stricly equivalent to using
-        `ensemble.BaggingClassifier`.
-    sampling_rate
-        The desired ratio of data to sample.
-    seed
-        Random seed for reproducibility.
-
-    Examples
-    --------
-
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import imblearn
-    >>> from river import linear_model
-    >>> from river import metrics
-    >>> from river import preprocessing
-
-    >>> model = imblearn.RandomSampler(
-    ...     (
-    ...         preprocessing.StandardScaler() |
-    ...         linear_model.LogisticRegression()
-    ...     ),
-    ...     desired_dist={False: 0.4, True: 0.6},
-    ...     sampling_rate=0.8,
-    ...     seed=42
-    ... )
-
-    >>> dataset = datasets.CreditCard().take(3000)
-
-    >>> metric = metrics.LogLoss()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    LogLoss: 0.130906
-
-    """
-
-    def __init__(
-        self,
-        classifier: base.Classifier,
-        desired_dist: dict,
-        sampling_rate=1.0,
-        seed: int = None,
-    ):
-        super().__init__(classifier=classifier, seed=seed)
-        self.sampling_rate = sampling_rate
-        self._actual_dist = collections.Counter()
-        if desired_dist is None:
-            desired_dist = self._actual_dist
-        self.desired_dist = desired_dist
-        self._n = 0
-
-    def learn_one(self, x, y):
-
-        self._actual_dist[y] += 1
-        self._n += 1
-        f = self.desired_dist
-        g = self._actual_dist
-
-        rate = self.sampling_rate * f[y] / (g[y] / self._n)
-
-        for _ in range(self._rng.poisson(rate)):
-            self.classifier.learn_one(x, y)
-
-        return self
+import collections
+
+import numpy as np
+
+from river import base
+
+
+class ClassificationSampler(base.Wrapper, base.Classifier):
+    def __init__(self, classifier, seed=None):
+        self.classifier = classifier
+        self.seed = seed
+        self._rng = np.random.RandomState(seed)
+
+    @property
+    def _wrapped_model(self):
+        return self.classifier
+
+    def predict_proba_one(self, x):
+        return self.classifier.predict_proba_one(x)
+
+    def predict_one(self, x):
+        return self.classifier.predict_one(x)
+
+
+class RandomUnderSampler(ClassificationSampler):
+    """Random under-sampling.
+
+    This is a wrapper for classifiers. It will train the provided classifier by under-sampling the
+    stream of given observations so that the class distribution seen by the classifier follows
+    a given desired distribution. The implementation is a discrete version of rejection sampling.
+
+    See [Working with imbalanced data](/user-guide/imbalanced-learning) for example usage.
+
+    Parameters
+    ----------
+    classifier
+    desired_dist
+        The desired class distribution. The keys are the classes whilst the values are the desired
+        class percentages. The values must sum up to 1.
+    seed
+        Random seed for reproducibility.
+
+    Examples
+    --------
+
+    >>> from river import datasets
+    >>> from river import evaluate
+    >>> from river import imblearn
+    >>> from river import linear_model
+    >>> from river import metrics
+    >>> from river import preprocessing
+
+    >>> model = imblearn.RandomUnderSampler(
+    ...     (
+    ...         preprocessing.StandardScaler() |
+    ...         linear_model.LogisticRegression()
+    ...     ),
+    ...     desired_dist={False: 0.4, True: 0.6},
+    ...     seed=42
+    ... )
+
+    >>> dataset = datasets.CreditCard().take(3000)
+
+    >>> metric = metrics.LogLoss()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    LogLoss: 0.07292
+
+    References
+    ----------
+    [^1]: [Under-sampling a dataset with desired ratios](https://maxhalford.github.io/blog/undersampling-ratios/)
+    [^2]: [Wikipedia article on rejection sampling](https://www.wikiwand.com/en/Rejection_sampling)
+
+    """
+
+    def __init__(
+        self, classifier: base.Classifier, desired_dist: dict, seed: int = None
+    ):
+        super().__init__(classifier=classifier, seed=seed)
+        self.desired_dist = desired_dist
+        self._actual_dist = collections.Counter()
+        self._pivot = None
+
+    def learn_one(self, x, y):
+
+        self._actual_dist[y] += 1
+        f = self.desired_dist
+        g = self._actual_dist
+
+        # Check if the pivot needs to be changed
+        if y != self._pivot:
+            self._pivot = max(g.keys(), key=lambda y: f[y] / g[y])
+        else:
+            self.classifier.learn_one(x, y)
+            return self
+
+        # Determine the sampling ratio if the class is not the pivot
+        M = f[self._pivot] / g[self._pivot]  # Likelihood ratio
+        ratio = f[y] / (M * g[y])
+
+        if ratio < 1 and self._rng.random() < ratio:
+            self.classifier.learn_one(x, y)
+
+        return self
+
+
+class RandomOverSampler(ClassificationSampler):
+    """Random over-sampling.
+
+    This is a wrapper for classifiers. It will train the provided classifier by over-sampling the
+    stream of given observations so that the class distribution seen by the classifier follows
+    a given desired distribution. The implementation is a discrete version of reverse rejection
+    sampling.
+
+    See [Working with imbalanced data](/user-guide/imbalanced-learning) for example usage.
+
+    Parameters
+    ----------
+    classifier
+    desired_dist
+        The desired class distribution. The keys are the classes whilst the values are the desired
+        class percentages. The values must sum up to 1.
+    seed
+        Random seed for reproducibility.
+
+    Examples
+    --------
+
+    >>> from river import datasets
+    >>> from river import evaluate
+    >>> from river import imblearn
+    >>> from river import linear_model
+    >>> from river import metrics
+    >>> from river import preprocessing
+
+    >>> model = imblearn.RandomOverSampler(
+    ...     (
+    ...         preprocessing.StandardScaler() |
+    ...         linear_model.LogisticRegression()
+    ...     ),
+    ...     desired_dist={False: 0.4, True: 0.6},
+    ...     seed=42
+    ... )
+
+    >>> dataset = datasets.CreditCard().take(3000)
+
+    >>> metric = metrics.LogLoss()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    LogLoss: 0.05421
+
+    """
+
+    def __init__(
+        self, classifier: base.Classifier, desired_dist: dict, seed: int = None
+    ):
+        super().__init__(classifier=classifier, seed=seed)
+        self.desired_dist = desired_dist
+        self._actual_dist = collections.Counter()
+        self._pivot = None
+
+    def learn_one(self, x, y):
+
+        self._actual_dist[y] += 1
+        f = self.desired_dist
+        g = self._actual_dist
+
+        # Check if the pivot needs to be changed
+        if y != self._pivot:
+            self._pivot = max(g.keys(), key=lambda y: g[y] / f[y])
+        else:
+            self.classifier.learn_one(x, y)
+            return self
+
+        M = g[self._pivot] / f[self._pivot]
+        rate = M * f[y] / g[y]
+
+        for _ in range(self._rng.poisson(rate)):
+            self.classifier.learn_one(x, y)
+
+        return self
+
+
+class RandomSampler(ClassificationSampler):
+    """Random sampling by mixing under-sampling and over-sampling.
+
+    This is a wrapper for classifiers. It will train the provided classifier by both under-sampling
+    and over-sampling the stream of given observations so that the class distribution seen by the
+    classifier follows a given desired distribution.
+
+    See [Working with imbalanced data](/user-guide/imbalanced-learning) for example usage.
+
+    Parameters
+    ----------
+    classifier
+    desired_dist
+        The desired class distribution. The keys are the classes whilst the values are the desired
+        class percentages. The values must sum up to 1. If set to `None`, then the observations
+        will be sampled uniformly at random, which is stricly equivalent to using
+        `ensemble.BaggingClassifier`.
+    sampling_rate
+        The desired ratio of data to sample.
+    seed
+        Random seed for reproducibility.
+
+    Examples
+    --------
+
+    >>> from river import datasets
+    >>> from river import evaluate
+    >>> from river import imblearn
+    >>> from river import linear_model
+    >>> from river import metrics
+    >>> from river import preprocessing
+
+    >>> model = imblearn.RandomSampler(
+    ...     (
+    ...         preprocessing.StandardScaler() |
+    ...         linear_model.LogisticRegression()
+    ...     ),
+    ...     desired_dist={False: 0.4, True: 0.6},
+    ...     sampling_rate=0.8,
+    ...     seed=42
+    ... )
+
+    >>> dataset = datasets.CreditCard().take(3000)
+
+    >>> metric = metrics.LogLoss()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    LogLoss: 0.130906
+
+    """
+
+    def __init__(
+        self,
+        classifier: base.Classifier,
+        desired_dist: dict,
+        sampling_rate=1.0,
+        seed: int = None,
+    ):
+        super().__init__(classifier=classifier, seed=seed)
+        self.sampling_rate = sampling_rate
+        self._actual_dist = collections.Counter()
+        if desired_dist is None:
+            desired_dist = self._actual_dist
+        self.desired_dist = desired_dist
+        self._n = 0
+
+    def learn_one(self, x, y):
+
+        self._actual_dist[y] += 1
+        self._n += 1
+        f = self.desired_dist
+        g = self._actual_dist
+
+        rate = self.sampling_rate * f[y] / (g[y] / self._n)
+
+        for _ in range(self._rng.poisson(rate)):
+            self.classifier.learn_one(x, y)
+
+        return self
```

### Comparing `river-0.8.0/river/linear_model/alma.py` & `river-0.9.0/river/linear_model/alma.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,89 +1,89 @@
-import collections
-import math
-
-from river import base, utils
-
-__all__ = ["ALMAClassifier"]
-
-
-class ALMAClassifier(base.Classifier):
-    """Approximate Large Margin Algorithm (ALMA).
-
-    Parameters
-    ----------
-    p
-    alpha
-    B
-    C
-
-    Attributes
-    ----------
-    w : collections.defaultdict
-        The current weights.
-    k : int
-        The number of instances seen during training.
-
-    Examples
-    --------
-
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import linear_model
-    >>> from river import metrics
-    >>> from river import preprocessing
-
-    >>> dataset = datasets.Phishing()
-
-    >>> model = (
-    ...     preprocessing.StandardScaler() |
-    ...     linear_model.ALMAClassifier()
-    ... )
-
-    >>> metric = metrics.Accuracy()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    Accuracy: 82.64%
-
-    References
-    ----------
-    [^1]: [Gentile, Claudio. "A new approximate maximal margin classification algorithm." Journal of Machine Learning Research 2.Dec (2001): 213-242](http://www.jmlr.org/papers/volume2/gentile01a/gentile01a.pdf)
-
-    """
-
-    def __init__(self, p=2, alpha=0.9, B=1 / 0.9, C=2 ** 0.5):
-        self.p = p
-        self.alpha = alpha
-        self.B = B
-        self.C = C
-        self.w = collections.defaultdict(float)
-        self.k = 1
-
-    def _raw_dot(self, x):
-        return utils.math.dot(x, self.w)
-
-    def predict_proba_one(self, x):
-        yp = utils.math.sigmoid(self._raw_dot(x))
-        return {False: 1 - yp, True: yp}
-
-    def learn_one(self, x, y):
-
-        # Convert 0 to -1
-        y = int(y or -1)
-
-        gamma = self.B * math.sqrt(self.p - 1) / math.sqrt(self.k)
-
-        if y * self._raw_dot(x) < (1 - self.alpha) * gamma:
-
-            eta = self.C / (math.sqrt(self.p - 1) * math.sqrt(self.k))
-
-            for i, xi in x.items():
-                self.w[i] += eta * y * xi
-
-            norm = utils.math.norm(self.w, order=self.p)
-
-            for i in x:
-                self.w[i] /= max(1, norm)
-
-            self.k += 1
-
-        return self
+import collections
+import math
+
+from river import base, utils
+
+__all__ = ["ALMAClassifier"]
+
+
+class ALMAClassifier(base.Classifier):
+    """Approximate Large Margin Algorithm (ALMA).
+
+    Parameters
+    ----------
+    p
+    alpha
+    B
+    C
+
+    Attributes
+    ----------
+    w : collections.defaultdict
+        The current weights.
+    k : int
+        The number of instances seen during training.
+
+    Examples
+    --------
+
+    >>> from river import datasets
+    >>> from river import evaluate
+    >>> from river import linear_model
+    >>> from river import metrics
+    >>> from river import preprocessing
+
+    >>> dataset = datasets.Phishing()
+
+    >>> model = (
+    ...     preprocessing.StandardScaler() |
+    ...     linear_model.ALMAClassifier()
+    ... )
+
+    >>> metric = metrics.Accuracy()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    Accuracy: 82.64%
+
+    References
+    ----------
+    [^1]: [Gentile, Claudio. "A new approximate maximal margin classification algorithm." Journal of Machine Learning Research 2.Dec (2001): 213-242](http://www.jmlr.org/papers/volume2/gentile01a/gentile01a.pdf)
+
+    """
+
+    def __init__(self, p=2, alpha=0.9, B=1 / 0.9, C=2 ** 0.5):
+        self.p = p
+        self.alpha = alpha
+        self.B = B
+        self.C = C
+        self.w = collections.defaultdict(float)
+        self.k = 1
+
+    def _raw_dot(self, x):
+        return utils.math.dot(x, self.w)
+
+    def predict_proba_one(self, x):
+        yp = utils.math.sigmoid(self._raw_dot(x))
+        return {False: 1 - yp, True: yp}
+
+    def learn_one(self, x, y):
+
+        # Convert 0 to -1
+        y = int(y or -1)
+
+        gamma = self.B * math.sqrt(self.p - 1) / math.sqrt(self.k)
+
+        if y * self._raw_dot(x) < (1 - self.alpha) * gamma:
+
+            eta = self.C / (math.sqrt(self.p - 1) * math.sqrt(self.k))
+
+            for i, xi in x.items():
+                self.w[i] += eta * y * xi
+
+            norm = utils.math.norm(self.w, order=self.p)
+
+            for i in x:
+                self.w[i] /= max(1, norm)
+
+            self.k += 1
+
+        return self
```

### Comparing `river-0.8.0/river/linear_model/glm.py` & `river-0.9.0/river/multioutput/chain.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,445 +1,453 @@
-import contextlib
-import numbers
-import typing
-
-import numpy as np
-import pandas as pd
-
-from river import base, optim, utils
-
-__all__ = ["LinearRegression", "LogisticRegression", "Perceptron"]
-
-
-class GLM:
-    """Generalized Linear Model.
-
-    This serves as a base class for linear and logistic regression.
-
-    """
-
-    def __init__(
-        self,
-        optimizer,
-        loss,
-        l2,
-        intercept_init,
-        intercept_lr,
-        clip_gradient,
-        initializer,
-    ):
-        self.optimizer = optimizer
-        self.loss = loss
-        self.l2 = l2
-        self.intercept_init = intercept_init
-        self.intercept = intercept_init
-        self.intercept_lr = (
-            optim.schedulers.Constant(intercept_lr)
-            if isinstance(intercept_lr, numbers.Number)
-            else intercept_lr
-        )
-        self.clip_gradient = clip_gradient
-        self.initializer = initializer
-        self._weights = utils.VectorDict(None)
-
-        # The predict_many functions are going to return pandas.Series. We can name the series with
-        # the name given to the y series seen during the last learn_many call.
-        self._y_name = None
-
-    @property
-    def weights(self):
-        return self._weights.to_dict()
-
-    @contextlib.contextmanager
-    def _learn_mode(self, mask=None):
-        weights = self._weights
-        try:
-            # enable the initializer and set a mask
-            self._weights = utils.VectorDict(weights, self.initializer, mask)
-            yield
-        finally:
-            self._weights = weights
-
-    def _fit(self, x, y, w, get_grad):
-
-        # Some optimizers need to do something before a prediction is made
-        self.optimizer.look_ahead(w=self._weights)
-
-        # Calculate the gradient
-        gradient, loss_gradient = get_grad(x, y, w)
-
-        # Update the intercept
-        self.intercept -= (
-            self.intercept_lr.get(self.optimizer.n_iterations) * loss_gradient
-        )
-
-        # Update the weights
-        self.optimizer.step(w=self._weights, g=gradient)
-
-        return self
-
-    # Single instance methods
-
-    def _raw_dot_one(self, x: dict) -> float:
-        return self._weights @ utils.VectorDict(x) + self.intercept
-
-    def _eval_gradient_one(self, x: dict, y: float, w: float) -> (dict, float):
-
-        loss_gradient = self.loss.gradient(y_true=y, y_pred=self._raw_dot_one(x))
-        loss_gradient *= w
-        loss_gradient = float(
-            utils.math.clamp(loss_gradient, -self.clip_gradient, self.clip_gradient)
-        )
-
-        return (
-            loss_gradient * utils.VectorDict(x) + 2.0 * self.l2 * self._weights,
-            loss_gradient,
-        )
-
-    def learn_one(self, x, y, w=1.0):
-        with self._learn_mode(x):
-            return self._fit(x, y, w, get_grad=self._eval_gradient_one)
-
-    # Mini-batch methods
-
-    def _raw_dot_many(self, X: pd.DataFrame) -> np.ndarray:
-        return X.values @ self._weights.to_numpy(X.columns) + self.intercept
-
-    def _eval_gradient_many(
-        self, X: pd.DataFrame, y: pd.Series, w: typing.Union[float, pd.Series]
-    ) -> (dict, float):
-
-        loss_gradient = self.loss.gradient(
-            y_true=y.values, y_pred=self._raw_dot_many(X)
-        )
-        loss_gradient *= w
-        loss_gradient = np.clip(loss_gradient, -self.clip_gradient, self.clip_gradient)
-
-        # At this point we have a feature matrix X of shape (n, p). The loss gradient is a vector
-        # of length p. We want to multiply each of X's rows by the corresponding value in the loss
-        # gradient. When this is all done, we collapse X by computing the average of each column,
-        # thereby obtaining the mean gradient of the batch. From thereon, the code reduces to the
-        # single instance case.
-        gradient = np.einsum("ij,i->ij", X.values, loss_gradient).mean(axis=0)
-
-        return dict(zip(X.columns, gradient)), loss_gradient.mean()
-
-    def learn_many(
-        self, X: pd.DataFrame, y: pd.Series, w: typing.Union[float, pd.Series] = 1
-    ):
-        self._y_name = y.name
-        with self._learn_mode(set(X)):
-            return self._fit(X, y, w, get_grad=self._eval_gradient_many)
-
-
-class LinearRegression(GLM, base.MiniBatchRegressor):
-    """Linear regression.
-
-    This estimator supports learning with mini-batches. On top of the single instance methods, it
-    provides the following methods: `learn_many`, `predict_many`, `predict_proba_many`. Each method
-    takes as input a `pandas.DataFrame` where each column represents a feature.
-
-    It is generally a good idea to scale the data beforehand in order for the optimizer to
-    converge. You can do this online with a `preprocessing.StandardScaler`.
-
-    Parameters
-    ----------
-    optimizer
-        The sequential optimizer used for updating the weights. Note that the intercept updates are
-        handled separately.
-    loss
-        The loss function to optimize for.
-    l2
-        Amount of L2 regularization used to push weights towards 0.
-    intercept_init
-        Initial intercept value.
-    intercept_lr
-        Learning rate scheduler used for updating the intercept. A `optim.schedulers.Constant` is
-        used if a `float` is provided. The intercept is not updated when this is set to 0.
-    clip_gradient
-        Clips the absolute value of each gradient value.
-    initializer
-        Weights initialization scheme.
-
-    Attributes
-    ----------
-    weights : dict
-        The current weights.
-
-    Examples
-    --------
-
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import linear_model
-    >>> from river import metrics
-    >>> from river import preprocessing
-
-    >>> dataset = datasets.TrumpApproval()
-
-    >>> model = (
-    ...     preprocessing.StandardScaler() |
-    ...     linear_model.LinearRegression(intercept_lr=.1)
-    ... )
-    >>> metric = metrics.MAE()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    MAE: 0.555971
-
-    >>> model['LinearRegression'].intercept
-    35.617670
-
-    You can call the `debug_one` method to break down a prediction. This works even if the
-    linear regression is part of a pipeline.
-
-    >>> x, y = next(iter(dataset))
-    >>> report = model.debug_one(x)
-    >>> print(report)
-    0. Input
-    --------
-    gallup: 43.84321 (float)
-    ipsos: 46.19925 (float)
-    morning_consult: 48.31875 (float)
-    ordinal_date: 736389 (int)
-    rasmussen: 44.10469 (float)
-    you_gov: 43.63691 (float)
-    <BLANKLINE>
-    1. StandardScaler
-    -----------------
-    gallup: 1.18810 (float)
-    ipsos: 2.10348 (float)
-    morning_consult: 2.73545 (float)
-    ordinal_date: -1.73032 (float)
-    rasmussen: 1.26872 (float)
-    you_gov: 1.48391 (float)
-    <BLANKLINE>
-    2. LinearRegression
-    -------------------
-    Name              Value      Weight      Contribution
-          Intercept    1.00000    35.61767       35.61767
-              ipsos    2.10348     0.62689        1.31866
-    morning_consult    2.73545     0.24180        0.66144
-             gallup    1.18810     0.43568        0.51764
-          rasmussen    1.26872     0.28118        0.35674
-            you_gov    1.48391     0.03123        0.04634
-       ordinal_date   -1.73032     3.45162       -5.97242
-    <BLANKLINE>
-    Prediction: 32.54607
-
-    """
-
-    def __init__(
-        self,
-        optimizer: optim.Optimizer = None,
-        loss: optim.losses.RegressionLoss = None,
-        l2=0.0,
-        intercept_init=0.0,
-        intercept_lr: typing.Union[optim.schedulers.Scheduler, float] = 0.01,
-        clip_gradient=1e12,
-        initializer: optim.initializers.Initializer = None,
-    ):
-        super().__init__(
-            optimizer=optim.SGD(0.01) if optimizer is None else optimizer,
-            loss=optim.losses.Squared() if loss is None else loss,
-            intercept_init=intercept_init,
-            intercept_lr=intercept_lr,
-            l2=l2,
-            clip_gradient=clip_gradient,
-            initializer=initializer if initializer else optim.initializers.Zeros(),
-        )
-
-    def predict_one(self, x):
-        return self.loss.mean_func(self._raw_dot_one(x))
-
-    def predict_many(self, X):
-        return pd.Series(
-            self.loss.mean_func(self._raw_dot_many(X)),
-            index=X.index,
-            name=self._y_name,
-            copy=False,
-        )
-
-    def debug_one(self, x: dict, decimals=5) -> str:
-        """Debugs the output of the linear regression.
-
-        Parameters
-        ----------
-        x
-            A dictionary of features.
-        decimals
-            The number of decimals use for printing each numeric value.
-
-        Returns
-        -------
-        A table which explains the output.
-
-        """
-
-        def fmt_float(x):
-            return "{: ,.{prec}f}".format(x, prec=decimals)
-
-        names = list(map(str, x.keys())) + ["Intercept"]
-        values = list(map(fmt_float, list(x.values()) + [1]))
-        weights = list(
-            map(fmt_float, [self._weights.get(i, 0) for i in x] + [self.intercept])
-        )
-        contributions = [xi * self._weights.get(i, 0) for i, xi in x.items()] + [
-            self.intercept
-        ]
-        order = reversed(np.argsort(contributions))
-        contributions = list(map(fmt_float, contributions))
-
-        table = utils.pretty.print_table(
-            headers=["Name", "Value", "Weight", "Contribution"],
-            columns=[names, values, weights, contributions],
-            order=order,
-        )
-
-        return table
-
-
-class LogisticRegression(GLM, base.MiniBatchClassifier):
-    """Logistic regression.
-
-    This estimator supports learning with mini-batches. On top of the single instance methods, it
-    provides the following methods: `learn_many`, `predict_many`, `predict_proba_many`. Each method
-    takes as input a `pandas.DataFrame` where each column represents a feature.
-
-    It is generally a good idea to scale the data beforehand in order for the optimizer to
-    converge. You can do this online with a `preprocessing.StandardScaler`.
-
-    Parameters
-    ----------
-    optimizer
-        The sequential optimizer used for updating the weights. Note that the intercept is handled
-        separately.
-    loss
-        The loss function to optimize for. Defaults to `optim.losses.Log`.
-    l2
-        Amount of L2 regularization used to push weights towards 0.
-    intercept_init
-        Initial intercept value.
-    intercept_lr
-        Learning rate scheduler used for updating the intercept. A `optim.schedulers.Constant` is
-        used if a `float` is provided. The intercept is not updated when this is set to 0.
-    clip_gradient
-        Clips the absolute value of each gradient value.
-    initializer
-        Weights initialization scheme.
-
-    Attributes
-    ----------
-    weights
-        The current weights.
-
-    Examples
-    --------
-
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import linear_model
-    >>> from river import metrics
-    >>> from river import optim
-    >>> from river import preprocessing
-
-    >>> dataset = datasets.Phishing()
-
-    >>> model = (
-    ...     preprocessing.StandardScaler() |
-    ...     linear_model.LogisticRegression(optimizer=optim.SGD(.1))
-    ... )
-
-    >>> metric = metrics.Accuracy()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    Accuracy: 88.96%
-
-    """
-
-    def __init__(
-        self,
-        optimizer: optim.Optimizer = None,
-        loss: optim.losses.BinaryLoss = None,
-        l2=0.0,
-        intercept_init=0.0,
-        intercept_lr: typing.Union[float, optim.schedulers.Scheduler] = 0.01,
-        clip_gradient=1e12,
-        initializer: optim.initializers.Initializer = None,
-    ):
-
-        super().__init__(
-            optimizer=optim.SGD(0.01) if optimizer is None else optimizer,
-            loss=optim.losses.Log() if loss is None else loss,
-            intercept_init=intercept_init,
-            intercept_lr=intercept_lr,
-            l2=l2,
-            clip_gradient=clip_gradient,
-            initializer=initializer if initializer else optim.initializers.Zeros(),
-        )
-
-    def predict_proba_one(self, x):
-        p = self.loss.mean_func(self._raw_dot_one(x))  # Convert logit to probability
-        return {False: 1.0 - p, True: p}
-
-    def predict_proba_many(self, X: pd.DataFrame) -> pd.DataFrame:
-        p = self.loss.mean_func(
-            self._raw_dot_many(X)
-        )  # Convert logits to probabilities
-        return pd.DataFrame({False: 1.0 - p, True: p}, index=X.index, copy=False)
-
-
-class Perceptron(LogisticRegression):
-    """Perceptron classifier.
-
-    In this implementation, the Perceptron is viewed as a special case of the logistic regression.
-    The loss function that is used is the Hinge loss with a threshold set to 0, whilst the learning
-    rate of the stochastic gradient descent procedure is set to 1 for both the weights and the
-    intercept.
-
-    Parameters
-    ----------
-    l2
-        Amount of L2 regularization used to push weights towards 0.
-    clip_gradient
-        Clips the absolute value of each gradient value.
-    initializer
-        Weights initialization scheme.
-
-    Attributes
-    ----------
-    weights
-        The current weights.
-
-    Examples
-    --------
-
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import linear_model as lm
-    >>> from river import metrics
-    >>> from river import preprocessing as pp
-
-    >>> dataset = datasets.Phishing()
-
-    >>> model = pp.StandardScaler() | lm.Perceptron()
-
-    >>> metric = metrics.Accuracy()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    Accuracy: 85.84%
-
-    """
-
-    def __init__(
-        self,
-        l2=0.0,
-        clip_gradient=1e12,
-        initializer: optim.initializers.Initializer = None,
-    ):
-        super().__init__(
-            optimizer=optim.SGD(1),
-            intercept_lr=1,
-            loss=optim.losses.Hinge(threshold=0.0),
-            l2=l2,
-            clip_gradient=clip_gradient,
-            initializer=initializer,
-        )
+import collections
+import copy
+
+from river import base, linear_model
+from river.utils.math import prod
+from river.utils.skmultiflow_utils import check_random_state
+
+__all__ = [
+    "ClassifierChain",
+    "RegressorChain",
+    "ProbabilisticClassifierChain",
+    "MonteCarloClassifierChain",
+]
+
+
+class BaseChain(base.Wrapper, collections.UserDict):
+    def __init__(self, model, order: list = None):
+        super().__init__()
+        self.model = model
+        self.order = order or []
+
+    @property
+    def _wrapped_model(self):
+        return self.model
+
+    def __getitem__(self, key):
+        try:
+            return collections.UserDict.__getitem__(self, key)
+        except KeyError:
+            collections.UserDict.__setitem__(self, key, copy.deepcopy(self.model))
+            return self[key]
+
+
+class ClassifierChain(BaseChain, base.Classifier, base.MultiOutputMixin):
+    """A multi-output model that arranges classifiers into a chain.
+
+    This will create one model per output. The prediction of the first output will be used as a
+    feature in the second model. The prediction for the second output will be used as a feature
+    for the third model, etc. This "chain model" is therefore capable of capturing dependencies
+    between outputs.
+
+    Parameters
+    ----------
+    model
+    order
+        A list with the targets order in which to construct the chain. If `None` then the order
+        will be inferred from the order of the keys in the target.
+
+
+    Examples
+    --------
+
+    >>> from river import feature_selection
+    >>> from river import linear_model
+    >>> from river import metrics
+    >>> from river import multioutput
+    >>> from river import preprocessing
+    >>> from river import stream
+    >>> from sklearn import datasets
+
+    >>> dataset = stream.iter_sklearn_dataset(
+    ...     dataset=datasets.fetch_openml('yeast', version=4, as_frame=False),
+    ...     shuffle=True,
+    ...     seed=42
+    ... )
+
+    >>> model = feature_selection.VarianceThreshold(threshold=0.01)
+    >>> model |= preprocessing.StandardScaler()
+    >>> model |= multioutput.ClassifierChain(
+    ...     model=linear_model.LogisticRegression(),
+    ...     order=list(range(14))
+    ... )
+
+    >>> metric = metrics.Jaccard()
+
+    >>> for x, y in dataset:
+    ...     # Convert y values to booleans
+    ...     y = {i: yi == 'TRUE' for i, yi in y.items()}
+    ...     y_pred = model.predict_one(x)
+    ...     metric = metric.update(y, y_pred)
+    ...     model = model.learn_one(x, y)
+
+    >>> metric
+    Jaccard: 0.451524
+
+    References
+    ----------
+    [^1]: [Multi-Output Chain Models and their Application in Data Streams](https://jmread.github.io/talks/2019_03_08-Imperial_Stats_Seminar.pdf)
+
+    """
+
+    def __init__(self, model: base.Classifier, order: list = None):
+        super().__init__(model, order)
+
+    @classmethod
+    def _unit_test_params(cls):
+        yield {"model": linear_model.LogisticRegression()}
+
+    @property
+    def _multiclass(self):
+        return self.model._multiclass
+
+    def learn_one(self, x, y):
+
+        x = copy.copy(x)
+        n_seen = 0
+
+        for o in self.order:
+            clf = self[o]
+
+            # Make predictions before the model is updated to avoid leakage
+            y_pred = clf.predict_proba_one(x)
+
+            # We handle the case where an output has been seen in the past but is missing now
+            try:
+                y_o = y[o]
+                n_seen += 1
+                clf.learn_one(x, y_o)
+            except KeyError:
+                pass
+
+            # The predictions are stored as features for the next label
+            if clf._multiclass:
+                for label, proba in y_pred.items():
+                    x[f"{o}_{label}"] = proba
+            else:
+                x[o] = y_pred[True]
+
+        # Now we check if there are any new outputs
+        n_unseen = len(y) - n_seen
+        if n_unseen:
+            for o in y:
+                if o not in self.order:
+                    self.order.append(o)
+
+        return self
+
+    def predict_proba_one(self, x):
+
+        x = copy.copy(x)
+        y_pred = {}
+
+        for o in self.order:
+            clf = self[o]
+
+            y_pred[o] = clf.predict_proba_one(x)
+
+            # The predictions are stored as features for the next label
+            if clf._multiclass:
+                for label, proba in y_pred.items():
+                    x[f"{o}_{label}"] = proba
+            else:
+                x[o] = y_pred[o][True]
+
+        return y_pred
+
+    def predict_one(self, x):
+        y_pred = self.predict_proba_one(x)
+        return {c: max(y_pred[c], key=y_pred[c].get) for c in y_pred}
+
+
+class RegressorChain(BaseChain, base.Regressor, base.MultiOutputMixin):
+    """A multi-output model that arranges regressor into a chain.
+
+    This will create one model per output. The prediction of the first output will be used as a
+    feature in the second output. The prediction for the second output will be used as a feature
+    for the third, etc. This "chain model" is therefore capable of capturing dependencies between
+    outputs.
+
+    Parameters
+    ----------
+    model
+    order
+        A list with the targets order in which to construct the chain. If `None` then the order
+        will be inferred from the order of the keys in the target.
+
+    Examples
+    --------
+
+    >>> from river import evaluate
+    >>> from river import linear_model
+    >>> from river import metrics
+    >>> from river import multioutput
+    >>> from river import preprocessing
+    >>> from river import stream
+    >>> from sklearn import datasets
+
+    >>> dataset = stream.iter_sklearn_dataset(
+    ...     dataset=datasets.load_linnerud(),
+    ...     shuffle=True,
+    ...     seed=42
+    ... )
+
+    >>> model = multioutput.RegressorChain(
+    ...     model=(
+    ...         preprocessing.StandardScaler() |
+    ...         linear_model.LinearRegression(intercept_lr=0.3)
+    ...     ),
+    ...     order=[0, 1, 2]
+    ... )
+
+    >>> metric = metrics.RegressionMultiOutput(metrics.MAE())
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    MAE: 12.649592
+
+    """
+
+    def __init__(self, model: base.Regressor, order: list = None):
+        super().__init__(model, order)
+
+    @classmethod
+    def _unit_test_params(cls):
+        yield {"model": linear_model.LinearRegression()}
+
+    def learn_one(self, x, y):
+
+        x = copy.copy(x)
+        n_seen = 0
+
+        for o in self.order:
+            reg = self[o]
+
+            # Make predictions before the model is updated to avoid leakage
+            y_pred = reg.predict_one(x)
+
+            # We handle the case where an output has been seen in the past but is missing now
+            try:
+                y_o = y[o]
+                n_seen += 1
+                reg.learn_one(x, y_o)
+            except KeyError:
+                pass
+
+            # The predictions are stored as features for the next label
+            x[o] = y_pred
+
+        # Now we check if there are any new outputs
+        n_unseen = len(y) - n_seen
+        if n_unseen:
+            for o in y:
+                if o not in self.order:
+                    self.order.append(o)
+
+        return self
+
+    def predict_one(self, x):
+
+        x = copy.copy(x)
+        y_pred = {}
+
+        if not isinstance(self.order, list):
+            return y_pred
+
+        for o, clf in self.items():
+            y_pred[o] = clf.predict_one(x)
+            x[o] = y_pred[o]
+
+        return y_pred
+
+
+class ProbabilisticClassifierChain(ClassifierChain):
+    r"""Probabilistic Classifier Chains.
+
+    The Probabilistic Classifier Chains (PCC) [^1] is a Bayes-optimal method
+    based on the Classifier Chains (CC).
+
+    Consider the concept of chaining classifiers as searching a path in a
+    binary tree whose leaf nodes are associated with a label $y \in Y$. While
+    CC searches only a single path in the aforementioned binary tree, PCC looks
+    at each of the $2^l$ paths, where $l$ is the number of labels. This limits
+    the applicability of the method to data sets with a small to moderate
+    number of labels. The authors recommend no more than about 15 labels for
+    real-world applications.
+
+    Parameters
+    ----------
+    model
+
+    Examples
+    --------
+    >>> from river import feature_selection
+    >>> from river import linear_model
+    >>> from river import metrics
+    >>> from river import multioutput
+    >>> from river import preprocessing
+    >>> from river import synth
+
+    >>> dataset = synth.Logical(seed=42, n_tiles=100)
+
+    >>> model = multioutput.ProbabilisticClassifierChain(
+    ...     model=linear_model.LogisticRegression()
+    ... )
+
+    >>> metric = metrics.Jaccard()
+
+    >>> for x, y in dataset:
+    ...    y_pred = model.predict_one(x)
+    ...    metric = metric.update(y, y_pred)
+    ...    model = model.learn_one(x, y)
+
+    >>> metric
+    Jaccard: 0.571429
+
+    References
+    ----------
+
+    [^1]: Cheng, W., Hüllermeier, E., & Dembczynski, K. J. (2010).
+          Bayes optimal multilabel classification via probabilistic classifier
+          chains. In Proceedings of the 27th international conference on
+          machine learning (ICML-10) (pp. 279-286).
+
+    """
+
+    def __init__(self, model: base.Classifier):
+        super().__init__(model)
+
+    def predict_one(self, x):
+        y_pred = {}
+
+        if not isinstance(self.order, list):
+            return y_pred
+
+        max_payoff = 0.0
+        n_labels = len(self.order)
+        # for each and every possible label combination
+        for label in range(2 ** n_labels):
+            # put together a binary label vector
+            y_gen = {
+                i: int(v)
+                for i, v in zip(self.order, list(bin(label)[2:].zfill(n_labels)))
+            }
+            # ... and gauge a probability for it (given x)
+            payoff = self._payoff(x=x, y=y_gen)
+            # if it performs well, keep it, and record the max
+            if payoff > max_payoff:
+                y_pred = copy.copy(y_gen)
+                max_payoff = payoff
+        return y_pred
+
+    def _payoff(self, x, y):
+        # Calculate payoff for predicting y | x, under the chains model.
+        p = {}
+
+        x = copy.copy(x)
+
+        for label in self.order:
+            clf = self[label]
+
+            y_pred = clf.predict_proba_one(x)
+            # Extend features
+            x[label] = y[label]
+            p[label] = y_pred[y[label]]
+
+        return prod(p.values())
+
+
+class MonteCarloClassifierChain(ProbabilisticClassifierChain):
+    """Monte Carlo Sampling Classifier Chains.
+
+    Probabilistic Classifier Chains using Monte Carlo sampling, as
+    described in [^1].
+
+    m samples are taken from the posterior distribution. Therefore we
+    need a probabilistic interpretation of the output, and thus, this is a
+    particular variety of ProbabilisticClassifierChain.
+
+    Parameters
+    ----------
+    model
+    m
+        Number of samples to take from the posterior distribution.
+    seed
+        Random number generator seed for reproducibility.
+
+    Examples
+    --------
+    >>> from river import feature_selection
+    >>> from river import linear_model
+    >>> from river import metrics
+    >>> from river import multioutput
+    >>> from river import preprocessing
+    >>> from river import synth
+
+    >>> dataset = synth.Logical(seed=42, n_tiles=100)
+
+    >>> model = multioutput.MonteCarloClassifierChain(
+    ...     model=linear_model.LogisticRegression(),
+    ...     m=10,
+    ...     seed=42
+    ... )
+
+    >>> metric = metrics.Jaccard()
+
+    >>> for x, y in dataset:
+    ...    y_pred = model.predict_one(x)
+    ...    metric = metric.update(y, y_pred)
+    ...    model = model.learn_one(x, y)
+
+    >>> metric
+    Jaccard: 0.568087
+
+    References
+    ----------
+    [^1]: Read, J., Martino, L., & Luengo, D. (2014). Efficient monte carlo
+          methods for multi-dimensional learning with classifier chains.
+          Pattern Recognition, 47(3), 1535-1546.
+
+    """
+
+    def __init__(self, model: base.Classifier, m: int = 10, seed: int = None):
+        ClassifierChain.__init__(self, model=model, order=None)
+        self.seed = seed
+        self._rng = check_random_state(seed)
+        self.m = m
+
+    def _sample(self, x):
+        # Sample y ~ P(y|x)
+        p = {}
+        y = {}
+        x = copy.copy(x)
+
+        for label in self.order:
+            clf = self[label]
+
+            y_pred = clf.predict_proba_one(x)
+            y_val = self._rng.choice(2, 1, p=[v for v in y_pred.values()])[0]
+            # Extend features
+            x[label] = y_val
+            y[label] = y_val
+            p[label] = y_pred[y_val]
+
+        return y, p
+
+    def predict_one(self, x):
+        y_pred = {}
+
+        if not isinstance(self.order, list):
+            return y_pred
+
+        y_pred = ClassifierChain.predict_one(self, x)
+        max_payoff = self._payoff(x=x, y=y_pred)
+        # for M times
+        for m in range(self.m):
+            y_, p_ = self._sample(
+                x
+            )  # N.B. in fact, the calculation p_ is done again in P.
+            payoff = self._payoff(x=x, y=y_)
+            # if it performs well, keep it, and record the max
+            if payoff > max_payoff:
+                y_pred = copy.copy(y_)
+                max_payoff = payoff
+        return y_pred
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `river-0.8.0/river/linear_model/softmax.py` & `river-0.9.0/river/linear_model/softmax.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,108 +1,108 @@
-import collections
-import copy
-import functools
-
-from river import base, optim, utils
-
-__all__ = ["SoftmaxRegression"]
-
-
-class SoftmaxRegression(base.Classifier):
-    """Softmax regression is a generalization of logistic regression to multiple classes.
-
-    Softmax regression is also known as "multinomial logistic regression". There are a set weights
-    for each class, hence the `weights` attribute is a nested `collections.defaultdict`. The main
-    advantage of using this instead of a one-vs-all logistic regression is that the probabilities
-    will be calibrated. Moreover softmax regression is more robust to outliers.
-
-    Parameters
-    ----------
-    optimizer
-        The sequential optimizer used to tune the weights.
-    loss
-        The loss function to optimize for.
-    l2
-        Amount of L2 regularization used to push weights towards 0.
-
-    Attributes
-    ----------
-    weights : collections.defaultdict
-
-    Examples
-    --------
-
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import linear_model
-    >>> from river import metrics
-    >>> from river import optim
-    >>> from river import preprocessing
-
-    >>> dataset = datasets.ImageSegments()
-
-    >>> model = preprocessing.StandardScaler()
-    >>> model |= linear_model.SoftmaxRegression()
-
-    >>> metric = metrics.MacroF1()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    MacroF1: 0.818765
-
-    References
-    ----------
-    [^1]: [Course on classification stochastic gradient descent](https://www.inf.ed.ac.uk/teaching/courses/mlp/2016/mlp02-sln.pdf)
-    [^2]: [Binary vs. Multi-Class Logistic Regression](https://chrisyeh96.github.io/2018/06/11/logistic-regression.html)
-
-    """
-
-    def __init__(
-        self,
-        optimizer: optim.Optimizer = None,
-        loss: optim.losses.MultiClassLoss = None,
-        l2=0,
-    ):
-        if optimizer is None:
-            optimizer = optim.SGD(0.01)
-        new_optimizer = functools.partial(copy.deepcopy, optimizer)
-        self.optimizer = optimizer
-        self.optimizers = collections.defaultdict(new_optimizer)  # type: ignore
-        self.loss = optim.losses.CrossEntropy() if loss is None else loss
-        self.l2 = l2
-        self.weights = collections.defaultdict(
-            functools.partial(collections.defaultdict, float)
-        )  # type: ignore
-
-    @property
-    def _multiclass(self):
-        return True
-
-    def learn_one(self, x, y):
-
-        # Some optimizers need to do something before a prediction is made
-        for label, weights in self.weights.items():
-            self.optimizers[label].look_ahead(w=weights)
-
-        # Make a prediction for the given features
-        y_pred = self.predict_proba_one(x)
-
-        # Compute the gradient of the loss w.r.t. each label
-        loss_gradients = self.loss.gradient(y_true=y, y_pred=y_pred)
-
-        for label, loss in loss_gradients.items():
-
-            # Compute the gradient w.r.t. each feature
-            weights = self.weights[label]
-            gradient = {
-                i: xi * loss + self.l2 * weights.get(i, 0) for i, xi in x.items()
-            }
-            self.weights[label] = self.optimizers[label].step(w=weights, g=gradient)
-
-        return self
-
-    def predict_proba_one(self, x):
-        return utils.math.softmax(
-            {
-                label: utils.math.dot(weights, x)
-                for label, weights in self.weights.items()
-            }
-        )
+import collections
+import copy
+import functools
+
+from river import base, optim, utils
+
+__all__ = ["SoftmaxRegression"]
+
+
+class SoftmaxRegression(base.Classifier):
+    """Softmax regression is a generalization of logistic regression to multiple classes.
+
+    Softmax regression is also known as "multinomial logistic regression". There are a set weights
+    for each class, hence the `weights` attribute is a nested `collections.defaultdict`. The main
+    advantage of using this instead of a one-vs-all logistic regression is that the probabilities
+    will be calibrated. Moreover softmax regression is more robust to outliers.
+
+    Parameters
+    ----------
+    optimizer
+        The sequential optimizer used to tune the weights.
+    loss
+        The loss function to optimize for.
+    l2
+        Amount of L2 regularization used to push weights towards 0.
+
+    Attributes
+    ----------
+    weights : collections.defaultdict
+
+    Examples
+    --------
+
+    >>> from river import datasets
+    >>> from river import evaluate
+    >>> from river import linear_model
+    >>> from river import metrics
+    >>> from river import optim
+    >>> from river import preprocessing
+
+    >>> dataset = datasets.ImageSegments()
+
+    >>> model = preprocessing.StandardScaler()
+    >>> model |= linear_model.SoftmaxRegression()
+
+    >>> metric = metrics.MacroF1()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    MacroF1: 0.818765
+
+    References
+    ----------
+    [^1]: [Course on classification stochastic gradient descent](https://www.inf.ed.ac.uk/teaching/courses/mlp/2016/mlp02-sln.pdf)
+    [^2]: [Binary vs. Multi-Class Logistic Regression](https://chrisyeh96.github.io/2018/06/11/logistic-regression.html)
+
+    """
+
+    def __init__(
+        self,
+        optimizer: optim.Optimizer = None,
+        loss: optim.losses.MultiClassLoss = None,
+        l2=0,
+    ):
+        if optimizer is None:
+            optimizer = optim.SGD(0.01)
+        new_optimizer = functools.partial(copy.deepcopy, optimizer)
+        self.optimizer = optimizer
+        self.optimizers = collections.defaultdict(new_optimizer)  # type: ignore
+        self.loss = optim.losses.CrossEntropy() if loss is None else loss
+        self.l2 = l2
+        self.weights = collections.defaultdict(
+            functools.partial(collections.defaultdict, float)
+        )  # type: ignore
+
+    @property
+    def _multiclass(self):
+        return True
+
+    def learn_one(self, x, y):
+
+        # Some optimizers need to do something before a prediction is made
+        for label, weights in self.weights.items():
+            self.optimizers[label].look_ahead(w=weights)
+
+        # Make a prediction for the given features
+        y_pred = self.predict_proba_one(x)
+
+        # Compute the gradient of the loss w.r.t. each label
+        loss_gradients = self.loss.gradient(y_true=y, y_pred=y_pred)
+
+        for label, loss in loss_gradients.items():
+
+            # Compute the gradient w.r.t. each feature
+            weights = self.weights[label]
+            gradient = {
+                i: xi * loss + self.l2 * weights.get(i, 0) for i, xi in x.items()
+            }
+            self.weights[label] = self.optimizers[label].step(w=weights, g=gradient)
+
+        return self
+
+    def predict_proba_one(self, x):
+        return utils.math.softmax(
+            {
+                label: utils.math.dot(weights, x)
+                for label, weights in self.weights.items()
+            }
+        )
```

### Comparing `river-0.8.0/river/meta/pred_clipper.py` & `river-0.9.0/river/preprocessing/pred_clipper.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,66 +1,73 @@
-from river import base, utils
-
-__all__ = ["PredClipper"]
-
-
-class PredClipper(base.Regressor, base.WrapperMixin):
-    """Clips the target after predicting.
-
-    Parameters
-    ----------
-    regressor
-        Regressor model for which to clip the predictions.
-    y_min
-        minimum value.
-    y_max
-        maximum value.
-
-    Examples
-    --------
-
-    >>> from river import linear_model
-    >>> from river import meta
-
-    >>> dataset = (
-    ...     ({'a': 2, 'b': 4}, 80),
-    ...     ({'a': 3, 'b': 5}, 100),
-    ...     ({'a': 4, 'b': 6}, 120)
-    ... )
-
-    >>> model = meta.PredClipper(
-    ...     regressor=linear_model.LinearRegression(),
-    ...     y_min=0,
-    ...     y_max=200
-    ... )
-
-    >>> for x, y in dataset:
-    ...     _ = model.learn_one(x, y)
-
-    >>> model.predict_one({'a': -100, 'b': -200})
-    0
-
-    >>> model.predict_one({'a': 50, 'b': 60})
-    200
-
-    """
-
-    def __init__(self, regressor: base.Regressor, y_min: float, y_max: float):
-        self.regressor = regressor
-        self.y_min = y_min
-        self.y_max = y_max
-
-    @property
-    def _wrapped_model(self):
-        return self.regressor
-
-    @property
-    def _labelloc(self):
-        return "b"  # for bottom
-
-    def learn_one(self, x, y):
-        self.regressor.learn_one(x, y)
-        return self
-
-    def predict_one(self, x):
-        y_pred = self.regressor.predict_one(x)
-        return utils.math.clamp(y_pred, self.y_min, self.y_max)
+from river import base, utils
+
+__all__ = ["PredClipper"]
+
+
+class PredClipper(base.Regressor, base.Wrapper):
+    """Clips the target after predicting.
+
+    Parameters
+    ----------
+    regressor
+        Regressor model for which to clip the predictions.
+    y_min
+        minimum value.
+    y_max
+        maximum value.
+
+    Examples
+    --------
+    >>> from river import linear_model
+    >>> from river import preprocessing
+
+    >>> dataset = (
+    ...     ({'a': 2, 'b': 4}, 80),
+    ...     ({'a': 3, 'b': 5}, 100),
+    ...     ({'a': 4, 'b': 6}, 120)
+    ... )
+
+    >>> model = preprocessing.PredClipper(
+    ...     regressor=linear_model.LinearRegression(),
+    ...     y_min=0,
+    ...     y_max=200
+    ... )
+
+    >>> for x, y in dataset:
+    ...     _ = model.learn_one(x, y)
+
+    >>> model.predict_one({'a': -100, 'b': -200})
+    0
+
+    >>> model.predict_one({'a': 50, 'b': 60})
+    200
+
+    """
+
+    def __init__(self, regressor: base.Regressor, y_min: float, y_max: float):
+        self.regressor = regressor
+        self.y_min = y_min
+        self.y_max = y_max
+
+    @property
+    def _wrapped_model(self):
+        return self.regressor
+
+    def learn_one(self, x, y):
+        self.regressor.learn_one(x, y)
+        return self
+
+    def predict_one(self, x):
+        y_pred = self.regressor.predict_one(x)
+        return utils.math.clamp(y_pred, self.y_min, self.y_max)
+
+    @classmethod
+    def _unit_test_params(cls):
+        import math
+
+        from river import linear_model
+
+        yield {
+            "regressor": linear_model.LinearRegression(),
+            "y_min": -math.inf,
+            "y_max": math.inf,
+        }
```

### Comparing `river-0.8.0/river/metrics/__init__.py` & `river-0.9.0/river/metrics/__init__.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,162 +1,162 @@
-"""Evaluation metrics.
-
-All the metrics are updated one sample at a time. This way we can track performance of
-predictive methods over time.
-
-"""
-
-from . import cluster
-from ._performance_evaluator import _ClassificationReport  # noqa: F401
-from ._performance_evaluator import _ClusteringReport  # noqa: F401
-from ._performance_evaluator import _MLClassificationReport  # noqa: F401
-from ._performance_evaluator import _MTRegressionReport  # noqa: F401
-from ._performance_evaluator import _RegressionReport  # noqa: F401
-from ._performance_evaluator import _RollingClassificationReport  # noqa: F401
-from ._performance_evaluator import _RollingClusteringReport  # noqa: F401
-from ._performance_evaluator import _RollingMLClassificationReport  # noqa: F401
-from ._performance_evaluator import _RollingMTRegressionReport  # noqa: F401
-from ._performance_evaluator import _RollingRegressionReport  # noqa: F401
-from .accuracy import Accuracy
-from .balanced_accuracy import BalancedAccuracy
-from .base import (
-    BinaryMetric,
-    ClassificationMetric,
-    Metric,
-    Metrics,
-    MultiClassMetric,
-    MultiOutputClassificationMetric,
-    MultiOutputRegressionMetric,
-    RegressionMetric,
-    WrapperMetric,
-)
-from .confusion import ConfusionMatrix, MultiLabelConfusionMatrix
-from .cross_entropy import CrossEntropy
-from .exact_match import ExactMatch
-from .expected_mutual_info import expected_mutual_info
-from .fbeta import (
-    F1,
-    ExampleF1,
-    ExampleFBeta,
-    FBeta,
-    MacroF1,
-    MacroFBeta,
-    MicroF1,
-    MicroFBeta,
-    MultiFBeta,
-    WeightedF1,
-    WeightedFBeta,
-)
-from .fowlkes_mallows import FowlkesMallows
-from .geometric_mean import GeometricMean
-from .hamming import Hamming, HammingLoss
-from .jaccard import Jaccard, SorensenDice
-from .kappa import CohenKappa, KappaM, KappaT
-from .log_loss import LogLoss
-from .mae import MAE
-from .matthews_corrcoef import MatthewsCorrCoef
-from .mcc import MCC
-from .mse import MSE, RMSE, RMSLE
-from .multioutput import RegressionMultiOutput
-from .mutual_info import AdjustedMutualInfo, MutualInfo, NormalizedMutualInfo
-from .pair_confusion import PairConfusionMatrix
-from .precision import (
-    ExamplePrecision,
-    MacroPrecision,
-    MicroPrecision,
-    Precision,
-    WeightedPrecision,
-)
-from .prevalence_threshold import PrevalenceThreshold
-from .purity import Purity
-from .q0 import Q0, Q2
-from .r2 import R2
-from .rand import AdjustedRand, Rand
-from .recall import ExampleRecall, MacroRecall, MicroRecall, Recall, WeightedRecall
-from .report import ClassificationReport
-from .roc_auc import ROCAUC
-from .rolling import Rolling
-from .smape import SMAPE
-from .time_rolling import TimeRolling
-from .variation_info import VariationInfo
-from .vbeta import Completeness, Homogeneity, VBeta
-
-__all__ = [
-    "Accuracy",
-    "AdjustedMutualInfo",
-    "AdjustedRand",
-    "BalancedAccuracy",
-    "BinaryMetric",
-    "Completeness",
-    "ClassificationMetric",
-    "ClassificationReport",
-    "ClusteringReport",
-    "cluster",
-    "CohenKappa",
-    "ConfusionMatrix",
-    "CrossEntropy",
-    "ExactMatch",
-    "ExamplePrecision",
-    "ExampleRecall",
-    "ExampleF1",
-    "ExampleFBeta",
-    "expected_mutual_info",
-    "F1",
-    "FBeta",
-    "FowlkesMallows",
-    "GeometricMean",
-    "Hamming",
-    "HammingLoss",
-    "Homogeneity",
-    "Jaccard",
-    "KappaM",
-    "KappaT",
-    "LogLoss",
-    "MAE",
-    "MacroF1",
-    "MacroFBeta",
-    "MacroPrecision",
-    "MacroRecall",
-    "MatthewsCorrCoef",
-    "MCC",
-    "Metric",
-    "Metrics",
-    "MicroF1",
-    "MicroFBeta",
-    "MicroPrecision",
-    "MicroRecall",
-    "MultiClassMetric",
-    "MultiFBeta",
-    "MultiLabelConfusionMatrix",
-    "MultiOutputClassificationMetric",
-    "MultiOutputRegressionMetric",
-    "MSE",
-    "MutualInfo",
-    "NormalizedMutualInfo",
-    "PairConfusionMatrix",
-    "Precision",
-    "Purity",
-    "Q0",
-    "Q2",
-    "Rand",
-    "Recall",
-    "RegressionMetric",
-    "RegressionMultiOutput",
-    "RMSE",
-    "RMSLE",
-    "ROCAUC",
-    "Rolling",
-    "RollingClusteringReport",
-    "R2",
-    "Precision",
-    "PrevalenceThreshold",
-    "SMAPE",
-    "SorensenDice",
-    "TimeRolling",
-    "VariationInfo",
-    "VBeta",
-    "WeightedF1",
-    "WeightedFBeta",
-    "WeightedPrecision",
-    "WeightedRecall",
-    "WrapperMetric",
-]
+"""Evaluation metrics.
+
+All the metrics are updated one sample at a time. This way we can track performance of
+predictive methods over time.
+
+"""
+
+from . import cluster
+from ._performance_evaluator import _ClassificationReport  # noqa: F401
+from ._performance_evaluator import _ClusteringReport  # noqa: F401
+from ._performance_evaluator import _MLClassificationReport  # noqa: F401
+from ._performance_evaluator import _MTRegressionReport  # noqa: F401
+from ._performance_evaluator import _RegressionReport  # noqa: F401
+from ._performance_evaluator import _RollingClassificationReport  # noqa: F401
+from ._performance_evaluator import _RollingClusteringReport  # noqa: F401
+from ._performance_evaluator import _RollingMLClassificationReport  # noqa: F401
+from ._performance_evaluator import _RollingMTRegressionReport  # noqa: F401
+from ._performance_evaluator import _RollingRegressionReport  # noqa: F401
+from .accuracy import Accuracy
+from .balanced_accuracy import BalancedAccuracy
+from .base import (
+    BinaryMetric,
+    ClassificationMetric,
+    Metric,
+    Metrics,
+    MultiClassMetric,
+    MultiOutputClassificationMetric,
+    MultiOutputRegressionMetric,
+    RegressionMetric,
+    WrapperMetric,
+)
+from .confusion import ConfusionMatrix, MultiLabelConfusionMatrix
+from .cross_entropy import CrossEntropy
+from .exact_match import ExactMatch
+from .expected_mutual_info import expected_mutual_info
+from .fbeta import (
+    F1,
+    ExampleF1,
+    ExampleFBeta,
+    FBeta,
+    MacroF1,
+    MacroFBeta,
+    MicroF1,
+    MicroFBeta,
+    MultiFBeta,
+    WeightedF1,
+    WeightedFBeta,
+)
+from .fowlkes_mallows import FowlkesMallows
+from .geometric_mean import GeometricMean
+from .hamming import Hamming, HammingLoss
+from .jaccard import Jaccard, SorensenDice
+from .kappa import CohenKappa, KappaM, KappaT
+from .log_loss import LogLoss
+from .mae import MAE
+from .matthews_corrcoef import MatthewsCorrCoef
+from .mcc import MCC
+from .mse import MSE, RMSE, RMSLE
+from .multioutput import RegressionMultiOutput
+from .mutual_info import AdjustedMutualInfo, MutualInfo, NormalizedMutualInfo
+from .pair_confusion import PairConfusionMatrix
+from .precision import (
+    ExamplePrecision,
+    MacroPrecision,
+    MicroPrecision,
+    Precision,
+    WeightedPrecision,
+)
+from .prevalence_threshold import PrevalenceThreshold
+from .purity import Purity
+from .q0 import Q0, Q2
+from .r2 import R2
+from .rand import AdjustedRand, Rand
+from .recall import ExampleRecall, MacroRecall, MicroRecall, Recall, WeightedRecall
+from .report import ClassificationReport
+from .roc_auc import ROCAUC
+from .rolling import Rolling
+from .smape import SMAPE
+from .time_rolling import TimeRolling
+from .variation_info import VariationInfo
+from .vbeta import Completeness, Homogeneity, VBeta
+
+__all__ = [
+    "Accuracy",
+    "AdjustedMutualInfo",
+    "AdjustedRand",
+    "BalancedAccuracy",
+    "BinaryMetric",
+    "Completeness",
+    "ClassificationMetric",
+    "ClassificationReport",
+    "ClusteringReport",
+    "cluster",
+    "CohenKappa",
+    "ConfusionMatrix",
+    "CrossEntropy",
+    "ExactMatch",
+    "ExamplePrecision",
+    "ExampleRecall",
+    "ExampleF1",
+    "ExampleFBeta",
+    "expected_mutual_info",
+    "F1",
+    "FBeta",
+    "FowlkesMallows",
+    "GeometricMean",
+    "Hamming",
+    "HammingLoss",
+    "Homogeneity",
+    "Jaccard",
+    "KappaM",
+    "KappaT",
+    "LogLoss",
+    "MAE",
+    "MacroF1",
+    "MacroFBeta",
+    "MacroPrecision",
+    "MacroRecall",
+    "MatthewsCorrCoef",
+    "MCC",
+    "Metric",
+    "Metrics",
+    "MicroF1",
+    "MicroFBeta",
+    "MicroPrecision",
+    "MicroRecall",
+    "MultiClassMetric",
+    "MultiFBeta",
+    "MultiLabelConfusionMatrix",
+    "MultiOutputClassificationMetric",
+    "MultiOutputRegressionMetric",
+    "MSE",
+    "MutualInfo",
+    "NormalizedMutualInfo",
+    "PairConfusionMatrix",
+    "Precision",
+    "Purity",
+    "Q0",
+    "Q2",
+    "Rand",
+    "Recall",
+    "RegressionMetric",
+    "RegressionMultiOutput",
+    "RMSE",
+    "RMSLE",
+    "ROCAUC",
+    "Rolling",
+    "RollingClusteringReport",
+    "R2",
+    "Precision",
+    "PrevalenceThreshold",
+    "SMAPE",
+    "SorensenDice",
+    "TimeRolling",
+    "VariationInfo",
+    "VBeta",
+    "WeightedF1",
+    "WeightedFBeta",
+    "WeightedPrecision",
+    "WeightedRecall",
+    "WrapperMetric",
+]
```

### Comparing `river-0.8.0/river/metrics/balanced_accuracy.py` & `river-0.9.0/river/metrics/balanced_accuracy.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,60 +1,60 @@
-from . import base
-
-__all__ = ["BalancedAccuracy"]
-
-
-class BalancedAccuracy(base.MultiClassMetric):
-    """Balanced accuracy.
-
-    Balanced accuracy is the average of recall obtained on each class. It is used to
-    deal with imbalanced datasets in binary and multi-class classification problems.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-
-    Examples
-    --------
-
-        >>> from river import metrics
-        >>> y_true = [True, False, True, True, False, True]
-        >>> y_pred = [True, False, True, True, True, False]
-
-        >>> metric = metrics.BalancedAccuracy()
-        >>> for yt, yp in zip(y_true, y_pred):
-        ...     metric = metric.update(yt, yp)
-
-        >>> metric
-        BalancedAccuracy: 62.50%
-
-        >>> y_true = [0, 1, 0, 0, 1, 0]
-        >>> y_pred = [0, 1, 0, 0, 0, 1]
-        >>> metric = metrics.BalancedAccuracy()
-        >>> for yt, yp in zip(y_true, y_pred):
-        ...     metric = metric.update(yt, yp)
-
-        >>> metric
-        BalancedAccuracy: 62.50%
-
-    """
-
-    _fmt = ".2%"  # will output a percentage, e.g. 0.625 will become "62,5%"
-
-    def get(self):
-        total = 0
-        for c in self.cm.classes:
-            try:
-                total += self.cm[c][c] / self.cm.sum_row[c]
-            except ZeroDivisionError:
-                continue
-        try:
-            n_classes = len(self.cm.classes)
-            score = total / n_classes
-
-            return score
-
-        except ZeroDivisionError:
-            return 0.0
+from . import base
+
+__all__ = ["BalancedAccuracy"]
+
+
+class BalancedAccuracy(base.MultiClassMetric):
+    """Balanced accuracy.
+
+    Balanced accuracy is the average of recall obtained on each class. It is used to
+    deal with imbalanced datasets in binary and multi-class classification problems.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+
+    Examples
+    --------
+
+        >>> from river import metrics
+        >>> y_true = [True, False, True, True, False, True]
+        >>> y_pred = [True, False, True, True, True, False]
+
+        >>> metric = metrics.BalancedAccuracy()
+        >>> for yt, yp in zip(y_true, y_pred):
+        ...     metric = metric.update(yt, yp)
+
+        >>> metric
+        BalancedAccuracy: 62.50%
+
+        >>> y_true = [0, 1, 0, 0, 1, 0]
+        >>> y_pred = [0, 1, 0, 0, 0, 1]
+        >>> metric = metrics.BalancedAccuracy()
+        >>> for yt, yp in zip(y_true, y_pred):
+        ...     metric = metric.update(yt, yp)
+
+        >>> metric
+        BalancedAccuracy: 62.50%
+
+    """
+
+    _fmt = ".2%"  # will output a percentage, e.g. 0.625 will become "62,5%"
+
+    def get(self):
+        total = 0
+        for c in self.cm.classes:
+            try:
+                total += self.cm[c][c] / self.cm.sum_row[c]
+            except ZeroDivisionError:
+                continue
+        try:
+            n_classes = len(self.cm.classes)
+            score = total / n_classes
+
+            return score
+
+        except ZeroDivisionError:
+            return 0.0
```

### Comparing `river-0.8.0/river/metrics/base.py` & `river-0.9.0/river/metrics/base.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,427 +1,434 @@
-import abc
-import collections
-import numbers
-import typing
-
-from river import base, stats, utils
-
-from . import confusion
-
-__all__ = [
-    "BinaryMetric",
-    "ClassificationMetric",
-    "Metric",
-    "Metrics",
-    "MultiClassMetric",
-    "MultiOutputClassificationMetric",
-    "MultiOutputRegressionMetric",
-    "RegressionMetric",
-    "WrapperMetric",
-]
-
-
-class Metric(base.Base, abc.ABC):
-    """Mother class for all metrics."""
-
-    # Define the format specification used for string representation.
-    _fmt = ",.6f"  # Use commas to separate big numbers and show 6 decimals
-
-    @abc.abstractmethod
-    def update(self, y_true, y_pred, sample_weight) -> "Metric":
-        """Update the metric."""
-
-    @abc.abstractmethod
-    def revert(self, y_true, y_pred, sample_weight) -> "Metric":
-        """Revert the metric."""
-
-    @abc.abstractmethod
-    def get(self) -> float:
-        """Return the current value of the metric."""
-
-    @abc.abstractproperty
-    def bigger_is_better(self) -> bool:
-        """Indicate if a high value is better than a low one or not."""
-
-    @abc.abstractmethod
-    def works_with(self, model: base.Estimator) -> bool:
-        """Indicates whether or not a metric can work with a given model."""
-
-    @property
-    def works_with_weights(self) -> bool:
-        """Indicate whether the model takes into consideration the effect of sample weights"""
-        return True
-
-    def __repr__(self):
-        """Return the class name along with the current value of the metric."""
-        return f"{self.__class__.__name__}: {self.get():{self._fmt}}".rstrip("0")
-
-    def __str__(self):
-        return repr(self)
-
-
-class ClassificationMetric(Metric):
-    """Mother class for all classification metrics.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-
-    """
-
-    def __init__(self, cm: confusion.ConfusionMatrix = None):
-        if cm is None:
-            cm = confusion.ConfusionMatrix()
-        self.cm = cm
-
-    def update(self, y_true, y_pred, sample_weight=1.0):
-        self.cm.update(
-            y_true,
-            y_pred,
-            sample_weight=sample_weight if self.works_with_weights else 1.0,
-        )
-        return self
-
-    def revert(self, y_true, y_pred, sample_weight=1.0, correction=None):
-        self.cm.revert(
-            y_true,
-            y_pred,
-            sample_weight=sample_weight if self.works_with_weights else 1.0,
-            correction=correction,
-        )
-        return self
-
-    @property
-    def bigger_is_better(self):
-        return True
-
-    def works_with(self, model) -> bool:
-        return utils.inspect.isclassifier(model)
-
-    @property
-    def requires_labels(self):
-        """Indicates if labels are required, rather than probabilities."""
-        return True
-
-    @staticmethod
-    def _clamp_proba(p):
-        """Clamp a number in between the (0, 1) interval."""
-        return utils.math.clamp(p, minimum=1e-15, maximum=1 - 1e-15)
-
-    def __add__(self, other) -> "Metrics":
-        if not isinstance(other, ClassificationMetric):
-            raise ValueError(
-                f"{self.__class__.__name__} and {other.__class__.__name__} metrics "
-                "are not compatible"
-            )
-        return Metrics([self, other])
-
-    @property
-    def sample_correction(self):
-        return self.cm.sample_correction
-
-
-class BinaryMetric(ClassificationMetric):
-    """Mother class for all binary classification metrics.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-    pos_val
-        Value to treat as "positive".
-
-    """
-
-    def __init__(self, cm=None, pos_val=True):
-        super().__init__(cm)
-        self.pos_val = pos_val
-
-    def update(
-        self,
-        y_true: bool,
-        y_pred: typing.Union[bool, float, typing.Dict[bool, float]],
-        sample_weight=1.0,
-    ) -> "BinaryMetric":
-        if self.requires_labels:
-            y_pred = y_pred == self.pos_val
-        return super().update(y_true == self.pos_val, y_pred, sample_weight)
-
-    def revert(
-        self,
-        y_true: bool,
-        y_pred: typing.Union[bool, float, typing.Dict[bool, float]],
-        sample_weight=1.0,
-        correction=None,
-    ) -> "BinaryMetric":
-        if self.requires_labels:
-            y_pred = y_pred == self.pos_val
-        return super().revert(y_true == self.pos_val, y_pred, sample_weight, correction)
-
-
-class MultiClassMetric(ClassificationMetric):
-    """Mother class for all multi-class classification metrics.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-
-    """
-
-    def works_with(self, model) -> bool:
-        return utils.inspect.isclassifier(model) or utils.inspect.isclusterer(model)
-
-
-class RegressionMetric(Metric):
-    """Mother class for all regression metrics."""
-
-    @abc.abstractmethod
-    def update(
-        self,
-        y_true: numbers.Number,
-        y_pred: numbers.Number,
-        sample_weight: numbers.Number,
-    ) -> "RegressionMetric":
-        """Update the metric."""
-
-    @abc.abstractmethod
-    def revert(
-        self,
-        y_true: numbers.Number,
-        y_pred: numbers.Number,
-        sample_weight: numbers.Number,
-    ) -> "RegressionMetric":
-        """Revert the metric."""
-
-    @property
-    def bigger_is_better(self):
-        return False
-
-    def works_with(self, model) -> bool:
-        return utils.inspect.isregressor(model)
-
-    def __add__(self, other) -> "Metrics":
-        if not isinstance(other, RegressionMetric):
-            raise ValueError(
-                f"{self.__class__.__name__} and {other.__class__.__name__} metrics "
-                "are not compatible"
-            )
-        return Metrics([self, other])
-
-
-class MultiOutputClassificationMetric(Metric):
-    """Mother class for all multi-output classification metrics.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-
-    """
-
-    def __init__(self, cm: confusion.MultiLabelConfusionMatrix = None):
-        if cm is None:
-            cm = confusion.MultiLabelConfusionMatrix()
-        self.cm = cm
-
-    def update(
-        self,
-        y_true: typing.Dict[typing.Union[str, int], base.typing.ClfTarget],
-        y_pred: typing.Union[
-            typing.Dict[typing.Union[str, int], base.typing.ClfTarget],
-            typing.Dict[
-                typing.Union[str, int], typing.Dict[base.typing.ClfTarget, float]
-            ],
-        ],
-        sample_weight: numbers.Number = 1.0,
-    ) -> "MultiOutputClassificationMetric":
-        """Update the metric."""
-        self.cm.update(y_true, y_pred, sample_weight)
-        return self
-
-    def revert(
-        self,
-        y_true: typing.Dict[typing.Union[str, int], base.typing.ClfTarget],
-        y_pred: typing.Union[
-            typing.Dict[typing.Union[str, int], base.typing.ClfTarget],
-            typing.Dict[
-                typing.Union[str, int], typing.Dict[base.typing.ClfTarget, float]
-            ],
-        ],
-        sample_weight: numbers.Number = 1.0,
-        correction=None,
-    ) -> "MultiOutputClassificationMetric":
-        """Revert the metric."""
-        self.cm.revert(y_true, y_pred, sample_weight, correction)
-        return self
-
-    def works_with(self, model) -> bool:
-        return utils.inspect.ismoclassifier(model)
-
-    @property
-    def sample_correction(self):
-        return self.cm.sample_correction
-
-
-class MultiOutputRegressionMetric(Metric):
-    """Mother class for all multi-output regression metrics."""
-
-    def update(
-        self,
-        y_true: typing.Dict[typing.Union[str, int], typing.Union[float, int]],
-        y_pred: typing.Dict[typing.Union[str, int], typing.Union[float, int]],
-        sample_weight: numbers.Number,
-    ) -> "MultiOutputRegressionMetric":
-        """Update the metric."""
-
-    def revert(
-        self,
-        y_true: typing.Dict[typing.Union[str, int], typing.Union[float, int]],
-        y_pred: typing.Dict[typing.Union[str, int], typing.Union[float, int]],
-        sample_weight: numbers.Number,
-    ) -> "MultiOutputRegressionMetric":
-        """Revert the metric."""
-
-    def works_with(self, model) -> bool:
-        return utils.inspect.ismoregressor(model)
-
-
-class Metrics(Metric, collections.UserList):
-    """A container class for handling multiple metrics at once.
-
-    Parameters
-    ----------
-    metrics
-    str_sep
-
-    """
-
-    def __init__(self, metrics, str_sep=", "):
-        super().__init__(metrics)
-        self.str_sep = str_sep
-
-    def update(self, y_true, y_pred, sample_weight=1.0):
-
-        # If the metrics are classification metrics, then we have to handle the case where some
-        # of the metrics require labels, whilst others need to be fed probabilities
-        if hasattr(self, "requires_labels") and not self.requires_labels:
-            for m in self:
-                if m.requires_labels:
-                    m.update(y_true, max(y_pred, key=y_pred.get))
-                else:
-                    m.update(y_true, y_pred)
-            return self
-
-        for m in self:
-            m.update(y_true, y_pred)
-        return self
-
-    def revert(self, y_true, y_pred, sample_weight=1.0):
-
-        # If the metrics are classification metrics, then we have to handle the case where some
-        # of the metrics require labels, whilst others need to be fed probabilities
-        if hasattr(self, "requires_labels") and not self.requires_labels:
-            for m in self:
-                if m.requires_labels:
-                    m.revert(y_true, max(y_pred, key=y_pred.get), sample_weight)
-                else:
-                    m.revert(y_true, y_pred, sample_weight)
-            return self
-
-        for m in self:
-            m.revert(y_true, y_pred, sample_weight)
-        return self
-
-    def get(self):
-        return [m.get() for m in self]
-
-    def works_with(self, model) -> bool:
-        return all(m.works_with(model) for m in self)
-
-    @property
-    def bigger_is_better(self):
-        raise NotImplementedError
-
-    @property
-    def works_with_weights(self):
-        return all(m.works_with_weights for m in self)
-
-    @property
-    def requires_labels(self):
-        return all(m.requires_labels for m in self)
-
-    def __repr__(self):
-        return self.str_sep.join((str(m) for m in self))
-
-    def __add__(self, other):
-        try:
-            other + self[0]  # Will raise a ValueError if incompatible
-        except IndexError:
-            pass
-        self.append(other)
-        return self
-
-
-class WrapperMetric(Metric):
-    @property
-    def _fmt(self):
-        return self.metric._fmt
-
-    @abc.abstractproperty
-    def metric(self):
-        """Gives access to the wrapped metric."""
-
-    def get(self):
-        return self.metric.get()
-
-    @property
-    def bigger_is_better(self):
-        return self.metric.bigger_is_better
-
-    def works_with(self, model):
-        return self.metric.works_with(model)
-
-    @property
-    def requires_labels(self):
-        return self.metric.requires_labels
-
-    @property
-    def __metaclass__(self):
-        return self.metric.__class__
-
-    def __repr__(self):
-        return str(self.metric)
-
-
-class MeanMetric(abc.ABC):
-    """Many metrics are just running averages. This is a utility class that avoids repeating
-    tedious stuff throughout the module for such metrics.
-
-    """
-
-    def __init__(self):
-        self._mean = stats.Mean()
-
-    @abc.abstractmethod
-    def _eval(self, y_true, y_pred):
-        pass
-
-    def update(self, y_true, y_pred, sample_weight=1.0):
-        self._mean.update(x=self._eval(y_true, y_pred), w=sample_weight)
-        return self
-
-    def revert(self, y_true, y_pred, sample_weight=1.0):
-        self._mean.revert(x=self._eval(y_true, y_pred), w=sample_weight)
-        return self
-
-    def get(self):
-        return self._mean.get()
+import abc
+import collections
+import numbers
+import operator
+import typing
+
+from river import base, stats, utils
+
+from . import confusion
+
+__all__ = [
+    "BinaryMetric",
+    "ClassificationMetric",
+    "Metric",
+    "Metrics",
+    "MultiClassMetric",
+    "MultiOutputClassificationMetric",
+    "MultiOutputRegressionMetric",
+    "RegressionMetric",
+    "WrapperMetric",
+]
+
+
+class Metric(base.Base, abc.ABC):
+    """Mother class for all metrics."""
+
+    # Define the format specification used for string representation.
+    _fmt = ",.6f"  # Use commas to separate big numbers and show 6 decimals
+
+    @abc.abstractmethod
+    def update(self, y_true, y_pred, sample_weight) -> "Metric":
+        """Update the metric."""
+
+    @abc.abstractmethod
+    def revert(self, y_true, y_pred, sample_weight) -> "Metric":
+        """Revert the metric."""
+
+    @abc.abstractmethod
+    def get(self) -> float:
+        """Return the current value of the metric."""
+
+    @property
+    @abc.abstractmethod
+    def bigger_is_better(self) -> bool:
+        """Indicate if a high value is better than a low one or not."""
+
+    @abc.abstractmethod
+    def works_with(self, model: base.Estimator) -> bool:
+        """Indicates whether or not a metric can work with a given model."""
+
+    @property
+    def works_with_weights(self) -> bool:
+        """Indicate whether the model takes into consideration the effect of sample weights"""
+        return True
+
+    def is_better_than(self, other) -> bool:
+        op = operator.gt if self.bigger_is_better else operator.lt
+        return op(self.get(), other.get())
+
+    def __repr__(self):
+        """Return the class name along with the current value of the metric."""
+        return f"{self.__class__.__name__}: {self.get():{self._fmt}}".rstrip("0")
+
+    def __str__(self):
+        return repr(self)
+
+
+class ClassificationMetric(Metric):
+    """Mother class for all classification metrics.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+
+    """
+
+    def __init__(self, cm: confusion.ConfusionMatrix = None):
+        if cm is None:
+            cm = confusion.ConfusionMatrix()
+        self.cm = cm
+
+    def update(self, y_true, y_pred, sample_weight=1.0):
+        self.cm.update(
+            y_true,
+            y_pred,
+            sample_weight=sample_weight if self.works_with_weights else 1.0,
+        )
+        return self
+
+    def revert(self, y_true, y_pred, sample_weight=1.0, correction=None):
+        self.cm.revert(
+            y_true,
+            y_pred,
+            sample_weight=sample_weight if self.works_with_weights else 1.0,
+            correction=correction,
+        )
+        return self
+
+    @property
+    def bigger_is_better(self):
+        return True
+
+    def works_with(self, model) -> bool:
+        return utils.inspect.isclassifier(model)
+
+    @property
+    def requires_labels(self):
+        """Indicates if labels are required, rather than probabilities."""
+        return True
+
+    @staticmethod
+    def _clamp_proba(p):
+        """Clamp a number in between the (0, 1) interval."""
+        return utils.math.clamp(p, minimum=1e-15, maximum=1 - 1e-15)
+
+    def __add__(self, other) -> "Metrics":
+        if not isinstance(other, ClassificationMetric):
+            raise ValueError(
+                f"{self.__class__.__name__} and {other.__class__.__name__} metrics "
+                "are not compatible"
+            )
+        return Metrics([self, other])
+
+    @property
+    def sample_correction(self):
+        return self.cm.sample_correction
+
+
+class BinaryMetric(ClassificationMetric):
+    """Mother class for all binary classification metrics.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+    pos_val
+        Value to treat as "positive".
+
+    """
+
+    def __init__(self, cm=None, pos_val=True):
+        super().__init__(cm)
+        self.pos_val = pos_val
+
+    def update(
+        self,
+        y_true: bool,
+        y_pred: typing.Union[bool, float, typing.Dict[bool, float]],
+        sample_weight=1.0,
+    ) -> "BinaryMetric":
+        if self.requires_labels:
+            y_pred = y_pred == self.pos_val
+        return super().update(y_true == self.pos_val, y_pred, sample_weight)
+
+    def revert(
+        self,
+        y_true: bool,
+        y_pred: typing.Union[bool, float, typing.Dict[bool, float]],
+        sample_weight=1.0,
+        correction=None,
+    ) -> "BinaryMetric":
+        if self.requires_labels:
+            y_pred = y_pred == self.pos_val
+        return super().revert(y_true == self.pos_val, y_pred, sample_weight, correction)
+
+
+class MultiClassMetric(ClassificationMetric):
+    """Mother class for all multi-class classification metrics.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+
+    """
+
+    def works_with(self, model) -> bool:
+        return utils.inspect.isclassifier(model) or utils.inspect.isclusterer(model)
+
+
+class RegressionMetric(Metric):
+    """Mother class for all regression metrics."""
+
+    @abc.abstractmethod
+    def update(
+        self,
+        y_true: numbers.Number,
+        y_pred: numbers.Number,
+        sample_weight: numbers.Number,
+    ) -> "RegressionMetric":
+        """Update the metric."""
+
+    @abc.abstractmethod
+    def revert(
+        self,
+        y_true: numbers.Number,
+        y_pred: numbers.Number,
+        sample_weight: numbers.Number,
+    ) -> "RegressionMetric":
+        """Revert the metric."""
+
+    @property
+    def bigger_is_better(self):
+        return False
+
+    def works_with(self, model) -> bool:
+        return utils.inspect.isregressor(model)
+
+    def __add__(self, other) -> "Metrics":
+        if not isinstance(other, RegressionMetric):
+            raise ValueError(
+                f"{self.__class__.__name__} and {other.__class__.__name__} metrics "
+                "are not compatible"
+            )
+        return Metrics([self, other])
+
+
+class MultiOutputClassificationMetric(Metric):
+    """Mother class for all multi-output classification metrics.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+
+    """
+
+    def __init__(self, cm: confusion.MultiLabelConfusionMatrix = None):
+        if cm is None:
+            cm = confusion.MultiLabelConfusionMatrix()
+        self.cm = cm
+
+    def update(
+        self,
+        y_true: typing.Dict[typing.Union[str, int], base.typing.ClfTarget],
+        y_pred: typing.Union[
+            typing.Dict[typing.Union[str, int], base.typing.ClfTarget],
+            typing.Dict[
+                typing.Union[str, int], typing.Dict[base.typing.ClfTarget, float]
+            ],
+        ],
+        sample_weight: numbers.Number = 1.0,
+    ) -> "MultiOutputClassificationMetric":
+        """Update the metric."""
+        self.cm.update(y_true, y_pred, sample_weight)
+        return self
+
+    def revert(
+        self,
+        y_true: typing.Dict[typing.Union[str, int], base.typing.ClfTarget],
+        y_pred: typing.Union[
+            typing.Dict[typing.Union[str, int], base.typing.ClfTarget],
+            typing.Dict[
+                typing.Union[str, int], typing.Dict[base.typing.ClfTarget, float]
+            ],
+        ],
+        sample_weight: numbers.Number = 1.0,
+        correction=None,
+    ) -> "MultiOutputClassificationMetric":
+        """Revert the metric."""
+        self.cm.revert(y_true, y_pred, sample_weight, correction)
+        return self
+
+    def works_with(self, model) -> bool:
+        return utils.inspect.ismoclassifier(model)
+
+    @property
+    def sample_correction(self):
+        return self.cm.sample_correction
+
+
+class MultiOutputRegressionMetric(Metric):
+    """Mother class for all multi-output regression metrics."""
+
+    def update(
+        self,
+        y_true: typing.Dict[typing.Union[str, int], typing.Union[float, int]],
+        y_pred: typing.Dict[typing.Union[str, int], typing.Union[float, int]],
+        sample_weight: numbers.Number,
+    ) -> "MultiOutputRegressionMetric":
+        """Update the metric."""
+
+    def revert(
+        self,
+        y_true: typing.Dict[typing.Union[str, int], typing.Union[float, int]],
+        y_pred: typing.Dict[typing.Union[str, int], typing.Union[float, int]],
+        sample_weight: numbers.Number,
+    ) -> "MultiOutputRegressionMetric":
+        """Revert the metric."""
+
+    def works_with(self, model) -> bool:
+        return utils.inspect.ismoregressor(model)
+
+
+class Metrics(Metric, collections.UserList):
+    """A container class for handling multiple metrics at once.
+
+    Parameters
+    ----------
+    metrics
+    str_sep
+
+    """
+
+    def __init__(self, metrics, str_sep=", "):
+        super().__init__(metrics)
+        self.str_sep = str_sep
+
+    def update(self, y_true, y_pred, sample_weight=1.0):
+
+        # If the metrics are classification metrics, then we have to handle the case where some
+        # of the metrics require labels, whilst others need to be fed probabilities
+        if hasattr(self, "requires_labels") and not self.requires_labels:
+            for m in self:
+                if m.requires_labels:
+                    m.update(y_true, max(y_pred, key=y_pred.get))
+                else:
+                    m.update(y_true, y_pred)
+            return self
+
+        for m in self:
+            m.update(y_true, y_pred)
+        return self
+
+    def revert(self, y_true, y_pred, sample_weight=1.0):
+
+        # If the metrics are classification metrics, then we have to handle the case where some
+        # of the metrics require labels, whilst others need to be fed probabilities
+        if hasattr(self, "requires_labels") and not self.requires_labels:
+            for m in self:
+                if m.requires_labels:
+                    m.revert(y_true, max(y_pred, key=y_pred.get), sample_weight)
+                else:
+                    m.revert(y_true, y_pred, sample_weight)
+            return self
+
+        for m in self:
+            m.revert(y_true, y_pred, sample_weight)
+        return self
+
+    def get(self):
+        return [m.get() for m in self]
+
+    def works_with(self, model) -> bool:
+        return all(m.works_with(model) for m in self)
+
+    @property
+    def bigger_is_better(self):
+        raise NotImplementedError
+
+    @property
+    def works_with_weights(self):
+        return all(m.works_with_weights for m in self)
+
+    @property
+    def requires_labels(self):
+        return all(m.requires_labels for m in self)
+
+    def __repr__(self):
+        return self.str_sep.join((str(m) for m in self))
+
+    def __add__(self, other):
+        try:
+            other + self[0]  # Will raise a ValueError if incompatible
+        except IndexError:
+            pass
+        self.append(other)
+        return self
+
+
+class WrapperMetric(Metric):
+    @property
+    def _fmt(self):
+        return self.metric._fmt
+
+    @property
+    @abc.abstractmethod
+    def metric(self):
+        """Gives access to the wrapped metric."""
+
+    def get(self):
+        return self.metric.get()
+
+    @property
+    def bigger_is_better(self):
+        return self.metric.bigger_is_better
+
+    def works_with(self, model):
+        return self.metric.works_with(model)
+
+    @property
+    def requires_labels(self):
+        return self.metric.requires_labels
+
+    @property
+    def __metaclass__(self):
+        return self.metric.__class__
+
+    def __repr__(self):
+        return str(self.metric)
+
+
+class MeanMetric(abc.ABC):
+    """Many metrics are just running averages. This is a utility class that avoids repeating
+    tedious stuff throughout the module for such metrics.
+
+    """
+
+    def __init__(self):
+        self._mean = stats.Mean()
+
+    @abc.abstractmethod
+    def _eval(self, y_true, y_pred):
+        pass
+
+    def update(self, y_true, y_pred, sample_weight=1.0):
+        self._mean.update(x=self._eval(y_true, y_pred), w=sample_weight)
+        return self
+
+    def revert(self, y_true, y_pred, sample_weight=1.0):
+        self._mean.revert(x=self._eval(y_true, y_pred), w=sample_weight)
+        return self
+
+    def get(self):
+        return self._mean.get()
```

### Comparing `river-0.8.0/river/metrics/cluster/base.py` & `river-0.9.0/river/metrics/cluster/base.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,85 +1,85 @@
-import abc
-import numbers
-import typing
-
-from river import base, stats, utils
-from river.base.typing import FeatureName
-
-__all__ = ["InternalMetric"]
-
-
-class InternalMetric(abc.ABC):
-    """
-    Mother class of all internal clustering metrics.
-    """
-
-    # Define the format specification used for string representation.
-    _fmt = ",.6f"  # Use commas to separate big numbers and show 6 decimals
-
-    @abc.abstractmethod
-    def update(self, x, y_pred, centers, sample_weight=1.0) -> "InternalMetric":
-        """Update the metric."""
-
-    @abc.abstractmethod
-    def revert(self, x, y_pred, centers, sample_weight=1.0) -> "InternalMetric":
-        """Revert the metric."""
-
-    @abc.abstractmethod
-    def get(self) -> float:
-        """Return the current value of the metric."""
-
-    @property
-    @abc.abstractmethod
-    def bigger_is_better(self) -> bool:
-        """Indicates if a high value is better than a low one or not."""
-
-    def works_with(self, model: base.Estimator) -> bool:
-        """Indicates whether or not a metric can work with a given model."""
-        return utils.inspect.isclusterer(model)
-
-    def __repr__(self):
-        """Returns the class name along with the current value of the metric."""
-        return f"{self.__class__.__name__}: {self.get():{self._fmt}}".rstrip("0")
-
-
-class MeanInternalMetric(InternalMetric):
-    """Many metrics are just running averages. This is a utility class that avoids repeating
-    tedious stuff throughout the module for such metrics.
-
-    """
-
-    def __init__(self):
-        self._mean = stats.Mean()
-
-    @abc.abstractmethod
-    def _eval(
-        self,
-        x: typing.Dict[FeatureName, numbers.Number],
-        y_pred: numbers.Number,
-        centers,
-        sample_weight=1.0,
-    ):
-        pass
-
-    def update(
-        self,
-        x: typing.Dict[FeatureName, numbers.Number],
-        y_pred: numbers.Number,
-        centers,
-        sample_weight=1.0,
-    ):
-        self._mean.update(x=self._eval(x, y_pred, centers), w=sample_weight)
-        return self
-
-    def revert(
-        self,
-        x: typing.Dict[FeatureName, numbers.Number],
-        y_pred: numbers.Number,
-        centers,
-        sample_weight=1.0,
-    ):
-        self._mean.revert(x=self._eval(x, y_pred, centers), w=sample_weight)
-        return self
-
-    def get(self):
-        return self._mean.get()
+import abc
+import numbers
+import typing
+
+from river import base, stats, utils
+from river.base.typing import FeatureName
+
+__all__ = ["InternalMetric"]
+
+
+class InternalMetric(abc.ABC):
+    """
+    Mother class of all internal clustering metrics.
+    """
+
+    # Define the format specification used for string representation.
+    _fmt = ",.6f"  # Use commas to separate big numbers and show 6 decimals
+
+    @abc.abstractmethod
+    def update(self, x, y_pred, centers, sample_weight=1.0) -> "InternalMetric":
+        """Update the metric."""
+
+    @abc.abstractmethod
+    def revert(self, x, y_pred, centers, sample_weight=1.0) -> "InternalMetric":
+        """Revert the metric."""
+
+    @abc.abstractmethod
+    def get(self) -> float:
+        """Return the current value of the metric."""
+
+    @property
+    @abc.abstractmethod
+    def bigger_is_better(self) -> bool:
+        """Indicates if a high value is better than a low one or not."""
+
+    def works_with(self, model: base.Estimator) -> bool:
+        """Indicates whether or not a metric can work with a given model."""
+        return utils.inspect.isclusterer(model)
+
+    def __repr__(self):
+        """Returns the class name along with the current value of the metric."""
+        return f"{self.__class__.__name__}: {self.get():{self._fmt}}".rstrip("0")
+
+
+class MeanInternalMetric(InternalMetric):
+    """Many metrics are just running averages. This is a utility class that avoids repeating
+    tedious stuff throughout the module for such metrics.
+
+    """
+
+    def __init__(self):
+        self._mean = stats.Mean()
+
+    @abc.abstractmethod
+    def _eval(
+        self,
+        x: typing.Dict[FeatureName, numbers.Number],
+        y_pred: numbers.Number,
+        centers,
+        sample_weight=1.0,
+    ):
+        pass
+
+    def update(
+        self,
+        x: typing.Dict[FeatureName, numbers.Number],
+        y_pred: numbers.Number,
+        centers,
+        sample_weight=1.0,
+    ):
+        self._mean.update(x=self._eval(x, y_pred, centers), w=sample_weight)
+        return self
+
+    def revert(
+        self,
+        x: typing.Dict[FeatureName, numbers.Number],
+        y_pred: numbers.Number,
+        centers,
+        sample_weight=1.0,
+    ):
+        self._mean.revert(x=self._eval(x, y_pred, centers), w=sample_weight)
+        return self
+
+    def get(self):
+        return self._mean.get()
```

### Comparing `river-0.8.0/river/metrics/cluster/bic.py` & `river-0.9.0/river/metrics/cluster/bic.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,154 +1,154 @@
-import math
-
-from river import metrics
-
-from . import base
-
-
-class BIC(base.InternalMetric):
-    r"""Bayesian Information Criterion (BIC).
-
-    In statistics, the Bayesian Information Criterion (BIC) [^1], or Schwarz Information
-    Criterion (SIC), is a criterion for model selection among a finite set of models;
-    the model with the highest BIC is preferred. It is based, in part, on the likelihood
-    function and is closely related to the Akaike Information Criterion (AIC).
-
-    Let
-
-    * k being the number of clusters,
-
-    * $n_i$ being the number of points within each cluster, $n_1 + n_2 + ... + n_k = n$,
-
-    * $d$ being the dimension of the clustering problem.
-
-    Then, the variance of the clustering solution will be calculated as
-
-    $$
-    \hat{\sigma}^2 = \frac{1}{(n - m) \times d} \sum_{i = 1}^n \lVert x_i - c_j \rVert^2.
-    $$
-
-    The maximum likelihood function, used in the BIC version of `River`, would be
-
-    $$
-    \hat{l}(D) = \sum_{i = 1}^k n_i \log(n_i) - n \log n - \frac{n_i \times d}{2} \times \log(2 \pi \hat{\sigma}^2) - \frac{(n_i - 1) \times d}{2},
-    $$
-
-    and the BIC will then be calculated as
-
-    $$
-    BIC = \hat{l}(D) - 0.5 \times k \times log(n) \times (d+1).
-    $$
-
-    Using the previously mentioned maximum likelihood function, the higher the BIC value, the
-    better the clustering solution is. Moreover, the BIC calculated will always be less than 0 [^2].
-
-
-    Examples
-    --------
-
-    >>> from river import cluster
-    >>> from river import stream
-    >>> from river import metrics
-
-    >>> X = [
-    ...     [1, 2],
-    ...     [1, 4],
-    ...     [1, 0],
-    ...     [4, 2],
-    ...     [4, 4],
-    ...     [4, 0],
-    ...     [-2, 2],
-    ...     [-2, 4],
-    ...     [-2, 0]
-    ... ]
-
-    >>> k_means = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)
-    >>> metric = metrics.cluster.BIC()
-
-    >>> for x, _ in stream.iter_array(X):
-    ...     k_means = k_means.learn_one(x)
-    ...     y_pred = k_means.predict_one(x)
-    ...     metric = metric.update(x, y_pred, k_means.centers)
-
-    >>> metric
-    BIC: -30.060416
-
-    References
-    ----------
-    [^1]: Wikipedia contributors. (2020, December 14). Bayesian information criterion.
-          In Wikipedia, The Free Encyclopedia,
-          from https://en.wikipedia.org/w/index.php?title=Bayesian_information_criterion&oldid=994127616
-    [^2]: BIC Notes, https://github.com/bobhancock/goxmeans/blob/master/doc/BIC_notes.pdf
-
-    """
-
-    def __init__(self):
-        super().__init__()
-        self._ssw = metrics.cluster.SSW()
-        self._n_points_by_clusters = {}
-        self._n_clusters = 0
-        self._dim = 0
-        self._initialized = False
-
-    def update(self, x, y_pred, centers, sample_weight=1.0):
-
-        if not self._initialized:
-            self._dim = len(x)
-
-        self._ssw.update(x, y_pred, centers, sample_weight)
-
-        try:
-            self._n_points_by_clusters[y_pred] += 1
-        except KeyError:
-            self._n_points_by_clusters[y_pred] = 1
-
-        self._n_clusters = len(centers)
-
-        return self
-
-    def revert(self, x, y_pred, centers, sample_weight=1.0):
-
-        self._ssw.revert(x, y_pred, centers, sample_weight)
-
-        self._n_points_by_clusters[y_pred] -= 1
-
-        self._n_clusters = len(centers)
-
-        return self
-
-    def get(self):
-
-        BIC = 0
-
-        total_points = sum(self._n_points_by_clusters.values())
-
-        try:
-            variance = (
-                1 / (total_points - self._n_clusters) / self._dim * self._ssw.get()
-            )
-        except ZeroDivisionError:
-            return -math.inf
-
-        const_term = 0.5 * self._n_clusters * math.log(total_points) * (self._dim + 1)
-
-        for i in self._n_points_by_clusters:
-            try:
-                BIC += (
-                    self._n_points_by_clusters[i]
-                    * math.log(self._n_points_by_clusters[i])
-                    - self._n_points_by_clusters[i] * math.log(total_points)
-                    - (self._n_points_by_clusters[i] * self._dim)
-                    / 2
-                    * math.log(2 * math.pi * variance)
-                    - (self._n_points_by_clusters[i] - 1) * self._dim / 2
-                )
-            except ValueError:
-                continue
-
-        BIC -= const_term
-
-        return BIC
-
-    @property
-    def bigger_is_better(self):
-        return True
+import math
+
+from river import metrics
+
+from . import base
+
+
+class BIC(base.InternalMetric):
+    r"""Bayesian Information Criterion (BIC).
+
+    In statistics, the Bayesian Information Criterion (BIC) [^1], or Schwarz Information
+    Criterion (SIC), is a criterion for model selection among a finite set of models;
+    the model with the highest BIC is preferred. It is based, in part, on the likelihood
+    function and is closely related to the Akaike Information Criterion (AIC).
+
+    Let
+
+    * k being the number of clusters,
+
+    * $n_i$ being the number of points within each cluster, $n_1 + n_2 + ... + n_k = n$,
+
+    * $d$ being the dimension of the clustering problem.
+
+    Then, the variance of the clustering solution will be calculated as
+
+    $$
+    \hat{\sigma}^2 = \frac{1}{(n - m) \times d} \sum_{i = 1}^n \lVert x_i - c_j \rVert^2.
+    $$
+
+    The maximum likelihood function, used in the BIC version of `River`, would be
+
+    $$
+    \hat{l}(D) = \sum_{i = 1}^k n_i \log(n_i) - n \log n - \frac{n_i \times d}{2} \times \log(2 \pi \hat{\sigma}^2) - \frac{(n_i - 1) \times d}{2},
+    $$
+
+    and the BIC will then be calculated as
+
+    $$
+    BIC = \hat{l}(D) - 0.5 \times k \times log(n) \times (d+1).
+    $$
+
+    Using the previously mentioned maximum likelihood function, the higher the BIC value, the
+    better the clustering solution is. Moreover, the BIC calculated will always be less than 0 [^2].
+
+
+    Examples
+    --------
+
+    >>> from river import cluster
+    >>> from river import stream
+    >>> from river import metrics
+
+    >>> X = [
+    ...     [1, 2],
+    ...     [1, 4],
+    ...     [1, 0],
+    ...     [4, 2],
+    ...     [4, 4],
+    ...     [4, 0],
+    ...     [-2, 2],
+    ...     [-2, 4],
+    ...     [-2, 0]
+    ... ]
+
+    >>> k_means = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)
+    >>> metric = metrics.cluster.BIC()
+
+    >>> for x, _ in stream.iter_array(X):
+    ...     k_means = k_means.learn_one(x)
+    ...     y_pred = k_means.predict_one(x)
+    ...     metric = metric.update(x, y_pred, k_means.centers)
+
+    >>> metric
+    BIC: -30.060416
+
+    References
+    ----------
+    [^1]: Wikipedia contributors. (2020, December 14). Bayesian information criterion.
+          In Wikipedia, The Free Encyclopedia,
+          from https://en.wikipedia.org/w/index.php?title=Bayesian_information_criterion&oldid=994127616
+    [^2]: BIC Notes, https://github.com/bobhancock/goxmeans/blob/master/doc/BIC_notes.pdf
+
+    """
+
+    def __init__(self):
+        super().__init__()
+        self._ssw = metrics.cluster.SSW()
+        self._n_points_by_clusters = {}
+        self._n_clusters = 0
+        self._dim = 0
+        self._initialized = False
+
+    def update(self, x, y_pred, centers, sample_weight=1.0):
+
+        if not self._initialized:
+            self._dim = len(x)
+
+        self._ssw.update(x, y_pred, centers, sample_weight)
+
+        try:
+            self._n_points_by_clusters[y_pred] += 1
+        except KeyError:
+            self._n_points_by_clusters[y_pred] = 1
+
+        self._n_clusters = len(centers)
+
+        return self
+
+    def revert(self, x, y_pred, centers, sample_weight=1.0):
+
+        self._ssw.revert(x, y_pred, centers, sample_weight)
+
+        self._n_points_by_clusters[y_pred] -= 1
+
+        self._n_clusters = len(centers)
+
+        return self
+
+    def get(self):
+
+        BIC = 0
+
+        total_points = sum(self._n_points_by_clusters.values())
+
+        try:
+            variance = (
+                1 / (total_points - self._n_clusters) / self._dim * self._ssw.get()
+            )
+        except ZeroDivisionError:
+            return -math.inf
+
+        const_term = 0.5 * self._n_clusters * math.log(total_points) * (self._dim + 1)
+
+        for i in self._n_points_by_clusters:
+            try:
+                BIC += (
+                    self._n_points_by_clusters[i]
+                    * math.log(self._n_points_by_clusters[i])
+                    - self._n_points_by_clusters[i] * math.log(total_points)
+                    - (self._n_points_by_clusters[i] * self._dim)
+                    / 2
+                    * math.log(2 * math.pi * variance)
+                    - (self._n_points_by_clusters[i] - 1) * self._dim / 2
+                )
+            except ValueError:
+                continue
+
+        BIC -= const_term
+
+        return BIC
+
+    @property
+    def bigger_is_better(self):
+        return True
```

### Comparing `river-0.8.0/river/metrics/cluster/daviesbouldin.py` & `river-0.9.0/river/metrics/cluster/daviesbouldin.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,107 +1,107 @@
-import math
-
-from river import utils
-
-from . import base
-
-
-class DaviesBouldin(base.InternalMetric):
-    """Davies-Bouldin index (DB).
-
-    The Davies-Bouldin index (DB) [^1] is an old but still widely used inernal validaion measure.
-    DB uses intra-cluster variance and inter-cluster center distance to find the worst partner
-    cluster, i.e., the closest most scattered one for each cluster. Thus, minimizing DB gives
-    us the optimal number of clusters.
-
-    Examples
-    --------
-
-    >>> from river import cluster
-    >>> from river import stream
-    >>> from river import metrics
-
-    >>> X = [
-    ...     [1, 2],
-    ...     [1, 4],
-    ...     [1, 0],
-    ...     [4, 2],
-    ...     [4, 4],
-    ...     [4, 0],
-    ...     [-2, 2],
-    ...     [-2, 4],
-    ...     [-2, 0]
-    ... ]
-
-    >>> k_means = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)
-    >>> metric = metrics.cluster.DaviesBouldin()
-
-    >>> for x, _ in stream.iter_array(X):
-    ...     k_means = k_means.learn_one(x)
-    ...     y_pred = k_means.predict_one(x)
-    ...     metric = metric.update(x, y_pred, k_means.centers)
-
-    >>> metric
-    DaviesBouldin: 0.22583
-
-    References
-    ----------
-    [^1]: David L., D., Don, B. (1979). A Cluster Separation Measure. In: IEEE
-          Transactions on Pattern Analysis and Machine Intelligence (PAMI) 1(2), 224 - 227.
-          DOI: 10.1109/TPAMI.1979.4766909
-
-    """
-
-    def __init__(self):
-        super().__init__()
-        self._inter_cluster_distances = {}
-        self._n_points_by_clusters = {}
-        self._total_points = 0
-        self._centers = {}
-
-    def update(self, x, y_pred, centers, sample_weight=1.0):
-
-        distance = math.sqrt(utils.math.minkowski_distance(centers[y_pred], x, 2))
-
-        if y_pred not in self._inter_cluster_distances:
-            self._inter_cluster_distances[y_pred] = distance
-            self._n_points_by_clusters[y_pred] = 1
-        else:
-            self._inter_cluster_distances[y_pred] += distance
-            self._n_points_by_clusters[y_pred] += 1
-
-        self._centers = centers
-
-        return self
-
-    def revert(self, x, y_pred, centers, sample_weight=1.0):
-
-        distance = math.sqrt(utils.math.minkowski_distance(centers[y_pred], x, 2))
-
-        self._inter_cluster_distances[y_pred] -= distance
-        self._n_points_by_clusters[y_pred] -= 1
-        self._centers = centers
-
-        return self
-
-    def get(self):
-        max_partner_clusters_index = -math.inf
-        n_clusters = len(self._inter_cluster_distances)
-        for i in range(n_clusters):
-            for j in range(i + 1, n_clusters):
-                distance_ij = math.sqrt(
-                    utils.math.minkowski_distance(self._centers[i], self._centers[j], 2)
-                )
-                ij_partner_cluster_index = (
-                    self._inter_cluster_distances[i] / self._n_points_by_clusters[i]
-                    + self._inter_cluster_distances[j] / self._n_points_by_clusters[j]
-                ) / distance_ij
-                if ij_partner_cluster_index > max_partner_clusters_index:
-                    max_partner_clusters_index = ij_partner_cluster_index
-        try:
-            return max_partner_clusters_index / n_clusters
-        except ZeroDivisionError:
-            return math.inf
-
-    @property
-    def bigger_is_better(self):
-        return False
+import math
+
+from river import utils
+
+from . import base
+
+
+class DaviesBouldin(base.InternalMetric):
+    """Davies-Bouldin index (DB).
+
+    The Davies-Bouldin index (DB) [^1] is an old but still widely used inernal validaion measure.
+    DB uses intra-cluster variance and inter-cluster center distance to find the worst partner
+    cluster, i.e., the closest most scattered one for each cluster. Thus, minimizing DB gives
+    us the optimal number of clusters.
+
+    Examples
+    --------
+
+    >>> from river import cluster
+    >>> from river import stream
+    >>> from river import metrics
+
+    >>> X = [
+    ...     [1, 2],
+    ...     [1, 4],
+    ...     [1, 0],
+    ...     [4, 2],
+    ...     [4, 4],
+    ...     [4, 0],
+    ...     [-2, 2],
+    ...     [-2, 4],
+    ...     [-2, 0]
+    ... ]
+
+    >>> k_means = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)
+    >>> metric = metrics.cluster.DaviesBouldin()
+
+    >>> for x, _ in stream.iter_array(X):
+    ...     k_means = k_means.learn_one(x)
+    ...     y_pred = k_means.predict_one(x)
+    ...     metric = metric.update(x, y_pred, k_means.centers)
+
+    >>> metric
+    DaviesBouldin: 0.22583
+
+    References
+    ----------
+    [^1]: David L., D., Don, B. (1979). A Cluster Separation Measure. In: IEEE
+          Transactions on Pattern Analysis and Machine Intelligence (PAMI) 1(2), 224 - 227.
+          DOI: 10.1109/TPAMI.1979.4766909
+
+    """
+
+    def __init__(self):
+        super().__init__()
+        self._inter_cluster_distances = {}
+        self._n_points_by_clusters = {}
+        self._total_points = 0
+        self._centers = {}
+
+    def update(self, x, y_pred, centers, sample_weight=1.0):
+
+        distance = math.sqrt(utils.math.minkowski_distance(centers[y_pred], x, 2))
+
+        if y_pred not in self._inter_cluster_distances:
+            self._inter_cluster_distances[y_pred] = distance
+            self._n_points_by_clusters[y_pred] = 1
+        else:
+            self._inter_cluster_distances[y_pred] += distance
+            self._n_points_by_clusters[y_pred] += 1
+
+        self._centers = centers
+
+        return self
+
+    def revert(self, x, y_pred, centers, sample_weight=1.0):
+
+        distance = math.sqrt(utils.math.minkowski_distance(centers[y_pred], x, 2))
+
+        self._inter_cluster_distances[y_pred] -= distance
+        self._n_points_by_clusters[y_pred] -= 1
+        self._centers = centers
+
+        return self
+
+    def get(self):
+        max_partner_clusters_index = -math.inf
+        n_clusters = len(self._inter_cluster_distances)
+        for i in range(n_clusters):
+            for j in range(i + 1, n_clusters):
+                distance_ij = math.sqrt(
+                    utils.math.minkowski_distance(self._centers[i], self._centers[j], 2)
+                )
+                ij_partner_cluster_index = (
+                    self._inter_cluster_distances[i] / self._n_points_by_clusters[i]
+                    + self._inter_cluster_distances[j] / self._n_points_by_clusters[j]
+                ) / distance_ij
+                if ij_partner_cluster_index > max_partner_clusters_index:
+                    max_partner_clusters_index = ij_partner_cluster_index
+        try:
+            return max_partner_clusters_index / n_clusters
+        except ZeroDivisionError:
+            return math.inf
+
+    @property
+    def bigger_is_better(self):
+        return False
```

### Comparing `river-0.8.0/river/metrics/cluster/generalized_dunn.py` & `river-0.9.0/river/metrics/cluster/generalized_dunn.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,261 +1,261 @@
-import math
-
-from river import stats, utils
-
-from . import base
-
-__all__ = ["GD43", "GD53"]
-
-
-class GD43(base.InternalMetric):
-    r"""Generalized Dunn's index 43 (GD43).
-
-    The Generalized Dunn's indices comprise a set of 17 variants of the original
-    Dunn's index devised to address sensitivity to noise in the latter. The formula
-    of this index is given by:
-
-    $$
-    GD_{rs} = \frac{\min_{i \new q} [\delta_r (\omega_i, \omega_j)]}{\max_k [\Delta_s (\omega_k)]},
-    $$
-
-    where $\delta_r(.)$ is a measure of separation, and $\Delta_s(.)$ is a measure of compactness,
-    the parameters $r$ and $s$ index the measures' formulations. In particular, when employing
-    Euclidean distance, GD43 is formulated using:
-
-    $$
-    \delta_4 (\omega_i, \omega_j) = \lVert v_i - v_j \rVert_2,
-    $$
-
-    and
-
-    $$
-    \Delta_3 (\omega_k) = \frac{2 \times CP_1^2 (v_k, \omega_k)}{n_k}.
-    $$
-
-    Examples
-    --------
-
-    >>> from river import cluster
-    >>> from river import stream
-    >>> from river import metrics
-
-    >>> X = [
-    ...     [1, 2],
-    ...     [1, 4],
-    ...     [1, 0],
-    ...     [4, 2],
-    ...     [4, 4],
-    ...     [4, 0],
-    ...     [-2, 2],
-    ...     [-2, 4],
-    ...     [-2, 0]
-    ... ]
-
-    >>> k_means = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)
-    >>> metric = metrics.cluster.GD43()
-
-    >>> for x, _ in stream.iter_array(X):
-    ...     k_means = k_means.learn_one(x)
-    ...     y_pred = k_means.predict_one(x)
-    ...     metric = metric.update(x, y_pred, k_means.centers)
-
-    >>> metric
-    GD43: 0.731369
-
-    References
-    ----------
-    [^1]:  J. Bezdek and N. Pal, "Some new indexes of cluster validity,"
-           IEEE Trans. Syst., Man, Cybern. B, vol. 28, no. 3, pp. 301–315, Jun. 1998.
-
-    """
-
-    def __init__(self):
-        super().__init__()
-        self._minimum_separation = 0
-        self._avg_cp_by_clusters = {}
-
-    @staticmethod
-    def _find_minimum_separation(centers):
-        minimum_separation = math.inf
-        n_centers = max(centers) + 1
-        for i in range(n_centers):
-            for j in range(i + 1, n_centers):
-                separation_ij = math.sqrt(
-                    utils.math.minkowski_distance(centers[i], centers[j], 2)
-                )
-                if separation_ij < minimum_separation:
-                    minimum_separation = separation_ij
-        return minimum_separation
-
-    def update(self, x, y_pred, centers, sample_weight=1.0):
-
-        self._minimum_separation = self._find_minimum_separation(centers)
-
-        distance = math.sqrt(utils.math.minkowski_distance(centers[y_pred], x, 2))
-
-        if y_pred in self._avg_cp_by_clusters:
-            self._avg_cp_by_clusters[y_pred].update(distance, w=sample_weight)
-        else:
-            self._avg_cp_by_clusters[y_pred] = stats.Mean()
-            self._avg_cp_by_clusters[y_pred].update(distance, w=sample_weight)
-
-        return self
-
-    def revert(self, x, y_pred, centers, sample_weight=1.0):
-
-        self._minimum_separation = self._find_minimum_separation(centers)
-
-        distance = math.sqrt(utils.math.minkowski_distance(centers[y_pred], x, 2))
-
-        self._avg_cp_by_clusters[y_pred].update(distance, w=-sample_weight)
-
-        return self
-
-    def get(self):
-        avg_cp_by_clusters = {
-            i: self._avg_cp_by_clusters[i].get() for i in self._avg_cp_by_clusters
-        }
-
-        try:
-            return self._minimum_separation / (2 * max(avg_cp_by_clusters.values()))
-        except ZeroDivisionError:
-            return -math.inf
-
-    @property
-    def bigger_is_better(self):
-        return True
-
-
-class GD53(base.InternalMetric):
-    r"""Generalized Dunn's index 53 (GD53).
-
-    The Generalized Dunn's indices comprise a set of 17 variants of the original
-    Dunn's index devised to address sensitivity to noise in the latter. The formula
-    of this index is given by:
-
-    $$
-    GD_{rs} = \frac{\min_{i \new q} [\delta_r (\omega_i, \omega_j)]}{\max_k [\Delta_s (\omega_k)]},
-    $$
-
-    where $\delta_r(.)$ is a measure of separation, and $\Delta_s(.)$ is a measure of compactness,
-    the parameters $r$ and $s$ index the measures' formulations. In particular, when employing
-    Euclidean distance, GD43 is formulated using:
-
-    $$
-    \delta_5 (\omega_i, \omega_j) = \frac{CP_1^2 (v_i, \omega_i) + CP_1^2 (v_j, \omega_j)}{n_i + n_j},
-    $$
-
-    and
-
-    $$
-    \Delta_3 (\omega_k) = \frac{2 \times CP_1^2 (v_k, \omega_k)}{n_k}.
-    $$
-
-    Examples
-    --------
-
-    >>> from river import cluster
-    >>> from river import stream
-    >>> from river import metrics
-
-    >>> X = [
-    ...     [1, 2],
-    ...     [1, 4],
-    ...     [1, 0],
-    ...     [4, 2],
-    ...     [4, 4],
-    ...     [4, 0],
-    ...     [-2, 2],
-    ...     [-2, 4],
-    ...     [-2, 0]
-    ... ]
-
-    >>> k_means = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)
-    >>> metric = metrics.cluster.GD53()
-
-    >>> for x, _ in stream.iter_array(X):
-    ...     k_means = k_means.learn_one(x)
-    ...     y_pred = k_means.predict_one(x)
-    ...     metric = metric.update(x, y_pred, k_means.centers)
-
-    >>> metric
-    GD53: 0.158377
-
-    References
-    ----------
-    [^1]:  J. Bezdek and N. Pal, "Some new indexes of cluster validity,"
-           IEEE Trans. Syst., Man, Cybern. B, vol. 28, no. 3, pp. 301–315, Jun. 1998.
-
-    """
-
-    def __init__(self):
-        super().__init__()
-        self._minimum_separation = 0
-        self._cp_by_clusters = {}
-        self._n_points_by_clusters = {}
-        self._n_clusters = 0
-
-    @staticmethod
-    def _find_minimum_separation(centers):
-        minimum_separation = math.inf
-        n_centers = max(centers) + 1
-        for i in range(n_centers):
-            for j in range(i + 1, n_centers):
-                separation_ij = utils.math.minkowski_distance(centers[i], centers[j], 2)
-                if separation_ij < minimum_separation:
-                    minimum_separation = separation_ij
-        return minimum_separation
-
-    def update(self, x, y_pred, centers, sample_weight=1.0):
-
-        self._minimum_separation = self._find_minimum_separation(centers)
-
-        distance = math.sqrt(utils.math.minkowski_distance(centers[y_pred], x, 2))
-
-        try:
-            self._cp_by_clusters[y_pred] += distance
-            self._n_points_by_clusters[y_pred] += 1
-        except KeyError:
-            self._cp_by_clusters[y_pred] = distance
-            self._n_points_by_clusters[y_pred] = 1
-
-        self._n_clusters = len(centers)
-
-        return self
-
-    def revert(self, x, y_pred, centers, sample_weight=1.0):
-
-        self._minimum_separation = self._find_minimum_separation(centers)
-
-        distance = math.sqrt(utils.math.minkowski_distance(centers[y_pred], x, 2))
-
-        self._cp_by_clusters[y_pred] -= distance
-        self._n_points_by_clusters[y_pred] -= 1
-
-        self._n_clusters = len(centers)
-
-        return self
-
-    def get(self):
-
-        min_delta_5 = math.inf
-        for i in range(self._n_clusters):
-            for j in range(i + 1, self._n_clusters):
-                try:
-                    delta_5 = (self._cp_by_clusters[i] + self._cp_by_clusters[j]) / (
-                        self._n_points_by_clusters[i] + self._n_points_by_clusters[j]
-                    )
-                except KeyError:
-                    continue
-
-                if delta_5 < min_delta_5:
-                    min_delta_5 = delta_5
-
-        try:
-            return min_delta_5 / self._minimum_separation
-        except ZeroDivisionError:
-            return -math.inf
-
-    @property
-    def bigger_is_better(self):
-        return True
+import math
+
+from river import stats, utils
+
+from . import base
+
+__all__ = ["GD43", "GD53"]
+
+
+class GD43(base.InternalMetric):
+    r"""Generalized Dunn's index 43 (GD43).
+
+    The Generalized Dunn's indices comprise a set of 17 variants of the original
+    Dunn's index devised to address sensitivity to noise in the latter. The formula
+    of this index is given by:
+
+    $$
+    GD_{rs} = \frac{\min_{i \new q} [\delta_r (\omega_i, \omega_j)]}{\max_k [\Delta_s (\omega_k)]},
+    $$
+
+    where $\delta_r(.)$ is a measure of separation, and $\Delta_s(.)$ is a measure of compactness,
+    the parameters $r$ and $s$ index the measures' formulations. In particular, when employing
+    Euclidean distance, GD43 is formulated using:
+
+    $$
+    \delta_4 (\omega_i, \omega_j) = \lVert v_i - v_j \rVert_2,
+    $$
+
+    and
+
+    $$
+    \Delta_3 (\omega_k) = \frac{2 \times CP_1^2 (v_k, \omega_k)}{n_k}.
+    $$
+
+    Examples
+    --------
+
+    >>> from river import cluster
+    >>> from river import stream
+    >>> from river import metrics
+
+    >>> X = [
+    ...     [1, 2],
+    ...     [1, 4],
+    ...     [1, 0],
+    ...     [4, 2],
+    ...     [4, 4],
+    ...     [4, 0],
+    ...     [-2, 2],
+    ...     [-2, 4],
+    ...     [-2, 0]
+    ... ]
+
+    >>> k_means = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)
+    >>> metric = metrics.cluster.GD43()
+
+    >>> for x, _ in stream.iter_array(X):
+    ...     k_means = k_means.learn_one(x)
+    ...     y_pred = k_means.predict_one(x)
+    ...     metric = metric.update(x, y_pred, k_means.centers)
+
+    >>> metric
+    GD43: 0.731369
+
+    References
+    ----------
+    [^1]:  J. Bezdek and N. Pal, "Some new indexes of cluster validity,"
+           IEEE Trans. Syst., Man, Cybern. B, vol. 28, no. 3, pp. 301–315, Jun. 1998.
+
+    """
+
+    def __init__(self):
+        super().__init__()
+        self._minimum_separation = 0
+        self._avg_cp_by_clusters = {}
+
+    @staticmethod
+    def _find_minimum_separation(centers):
+        minimum_separation = math.inf
+        n_centers = max(centers) + 1
+        for i in range(n_centers):
+            for j in range(i + 1, n_centers):
+                separation_ij = math.sqrt(
+                    utils.math.minkowski_distance(centers[i], centers[j], 2)
+                )
+                if separation_ij < minimum_separation:
+                    minimum_separation = separation_ij
+        return minimum_separation
+
+    def update(self, x, y_pred, centers, sample_weight=1.0):
+
+        self._minimum_separation = self._find_minimum_separation(centers)
+
+        distance = math.sqrt(utils.math.minkowski_distance(centers[y_pred], x, 2))
+
+        if y_pred in self._avg_cp_by_clusters:
+            self._avg_cp_by_clusters[y_pred].update(distance, w=sample_weight)
+        else:
+            self._avg_cp_by_clusters[y_pred] = stats.Mean()
+            self._avg_cp_by_clusters[y_pred].update(distance, w=sample_weight)
+
+        return self
+
+    def revert(self, x, y_pred, centers, sample_weight=1.0):
+
+        self._minimum_separation = self._find_minimum_separation(centers)
+
+        distance = math.sqrt(utils.math.minkowski_distance(centers[y_pred], x, 2))
+
+        self._avg_cp_by_clusters[y_pred].update(distance, w=-sample_weight)
+
+        return self
+
+    def get(self):
+        avg_cp_by_clusters = {
+            i: self._avg_cp_by_clusters[i].get() for i in self._avg_cp_by_clusters
+        }
+
+        try:
+            return self._minimum_separation / (2 * max(avg_cp_by_clusters.values()))
+        except ZeroDivisionError:
+            return -math.inf
+
+    @property
+    def bigger_is_better(self):
+        return True
+
+
+class GD53(base.InternalMetric):
+    r"""Generalized Dunn's index 53 (GD53).
+
+    The Generalized Dunn's indices comprise a set of 17 variants of the original
+    Dunn's index devised to address sensitivity to noise in the latter. The formula
+    of this index is given by:
+
+    $$
+    GD_{rs} = \frac{\min_{i \new q} [\delta_r (\omega_i, \omega_j)]}{\max_k [\Delta_s (\omega_k)]},
+    $$
+
+    where $\delta_r(.)$ is a measure of separation, and $\Delta_s(.)$ is a measure of compactness,
+    the parameters $r$ and $s$ index the measures' formulations. In particular, when employing
+    Euclidean distance, GD43 is formulated using:
+
+    $$
+    \delta_5 (\omega_i, \omega_j) = \frac{CP_1^2 (v_i, \omega_i) + CP_1^2 (v_j, \omega_j)}{n_i + n_j},
+    $$
+
+    and
+
+    $$
+    \Delta_3 (\omega_k) = \frac{2 \times CP_1^2 (v_k, \omega_k)}{n_k}.
+    $$
+
+    Examples
+    --------
+
+    >>> from river import cluster
+    >>> from river import stream
+    >>> from river import metrics
+
+    >>> X = [
+    ...     [1, 2],
+    ...     [1, 4],
+    ...     [1, 0],
+    ...     [4, 2],
+    ...     [4, 4],
+    ...     [4, 0],
+    ...     [-2, 2],
+    ...     [-2, 4],
+    ...     [-2, 0]
+    ... ]
+
+    >>> k_means = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)
+    >>> metric = metrics.cluster.GD53()
+
+    >>> for x, _ in stream.iter_array(X):
+    ...     k_means = k_means.learn_one(x)
+    ...     y_pred = k_means.predict_one(x)
+    ...     metric = metric.update(x, y_pred, k_means.centers)
+
+    >>> metric
+    GD53: 0.158377
+
+    References
+    ----------
+    [^1]:  J. Bezdek and N. Pal, "Some new indexes of cluster validity,"
+           IEEE Trans. Syst., Man, Cybern. B, vol. 28, no. 3, pp. 301–315, Jun. 1998.
+
+    """
+
+    def __init__(self):
+        super().__init__()
+        self._minimum_separation = 0
+        self._cp_by_clusters = {}
+        self._n_points_by_clusters = {}
+        self._n_clusters = 0
+
+    @staticmethod
+    def _find_minimum_separation(centers):
+        minimum_separation = math.inf
+        n_centers = max(centers) + 1
+        for i in range(n_centers):
+            for j in range(i + 1, n_centers):
+                separation_ij = utils.math.minkowski_distance(centers[i], centers[j], 2)
+                if separation_ij < minimum_separation:
+                    minimum_separation = separation_ij
+        return minimum_separation
+
+    def update(self, x, y_pred, centers, sample_weight=1.0):
+
+        self._minimum_separation = self._find_minimum_separation(centers)
+
+        distance = math.sqrt(utils.math.minkowski_distance(centers[y_pred], x, 2))
+
+        try:
+            self._cp_by_clusters[y_pred] += distance
+            self._n_points_by_clusters[y_pred] += 1
+        except KeyError:
+            self._cp_by_clusters[y_pred] = distance
+            self._n_points_by_clusters[y_pred] = 1
+
+        self._n_clusters = len(centers)
+
+        return self
+
+    def revert(self, x, y_pred, centers, sample_weight=1.0):
+
+        self._minimum_separation = self._find_minimum_separation(centers)
+
+        distance = math.sqrt(utils.math.minkowski_distance(centers[y_pred], x, 2))
+
+        self._cp_by_clusters[y_pred] -= distance
+        self._n_points_by_clusters[y_pred] -= 1
+
+        self._n_clusters = len(centers)
+
+        return self
+
+    def get(self):
+
+        min_delta_5 = math.inf
+        for i in range(self._n_clusters):
+            for j in range(i + 1, self._n_clusters):
+                try:
+                    delta_5 = (self._cp_by_clusters[i] + self._cp_by_clusters[j]) / (
+                        self._n_points_by_clusters[i] + self._n_points_by_clusters[j]
+                    )
+                except KeyError:
+                    continue
+
+                if delta_5 < min_delta_5:
+                    min_delta_5 = delta_5
+
+        try:
+            return min_delta_5 / self._minimum_separation
+        except ZeroDivisionError:
+            return -math.inf
+
+    @property
+    def bigger_is_better(self):
+        return True
```

### Comparing `river-0.8.0/river/metrics/cluster/i_index.py` & `river-0.9.0/river/metrics/cluster/r2.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,139 +1,112 @@
-import math
-
-from river import stats, utils
-
-from . import base
-
-
-class IIndex(base.InternalMetric):
-    """I-Index (I).
-
-    I-Index (I) [^1] adopts the maximum distance between cluster centers. It also shares the type of
-    formulation numerator-separation/denominator-compactness. For compactness, the distance from
-    a data point to its cluster center is also used like CH.
-
-    Examples
-    --------
-
-    >>> from river import cluster
-    >>> from river import stream
-    >>> from river import metrics
-
-    >>> X = [
-    ...     [1, 2],
-    ...     [1, 4],
-    ...     [1, 0],
-    ...     [4, 2],
-    ...     [4, 4],
-    ...     [4, 0],
-    ...     [-2, 2],
-    ...     [-2, 4],
-    ...     [-2, 0]
-    ... ]
-
-    >>> k_means = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)
-    >>> metric = metrics.cluster.IIndex()
-
-    >>> for x, _ in stream.iter_array(X):
-    ...     k_means = k_means.learn_one(x)
-    ...     y_pred = k_means.predict_one(x)
-    ...     metric = metric.update(x, y_pred, k_means.centers)
-
-    >>> metric
-    IIndex: 6.836566
-
-    References
-    --------
-
-    [^1]: Maulik, U., Bandyopadhyay, S. (2002). Performance evaluation of some clustering algorithms and
-          validity indices. In: IEEE Transactions on Pattern Analysis and Machine Intelligence 24(12)
-          1650 - 1654. DOI: 10.1109/TPAMI.2002.1114856
-
-    """
-
-    def __init__(self):
-        super().__init__()
-        self._center_all_points = {}
-        self._ssq_points_cluster_centers = 0
-        self._ssq_points_center = 0
-        self._furthest_cluster_distance = 0
-        self._n_clusters = 0
-        self._dim = 0
-        self.sample_correction = {}
-        self._initialized = False
-
-    @staticmethod
-    def _find_furthest_cluster_distance(centers):
-        n_centers = len(centers)
-        max_distance = -math.inf
-        for i in range(n_centers):
-            for j in range(i + 1, n_centers):
-                distance_ij = math.sqrt(
-                    utils.math.minkowski_distance(centers[i], centers[j], 2)
-                )
-                if distance_ij > max_distance:
-                    max_distance = distance_ij
-        return max_distance
-
-    def update(self, x, y_pred, centers, sample_weight=1.0):
-
-        self._furthest_cluster_distance = self._find_furthest_cluster_distance(centers)
-
-        if not self._initialized:
-            self._center_all_points = {i: stats.Mean() for i in x}
-            self._dim = len(x)
-            self._initialized = True
-
-        for i in self._center_all_points:
-            self._center_all_points[i].update(x[i], w=sample_weight)
-        center_all_points = {
-            i: self._center_all_points[i].get() for i in self._center_all_points
-        }
-
-        distance_point_cluster_center = math.sqrt(
-            utils.math.minkowski_distance(centers[y_pred], x, 2)
-        )
-        distance_point_center = math.sqrt(
-            utils.math.minkowski_distance(center_all_points, x, 2)
-        )
-        self._ssq_points_cluster_centers += distance_point_cluster_center
-        self._ssq_points_center += distance_point_center
-        self._n_clusters = len(centers)
-
-        # To trace back
-        self.sample_correction = {
-            "distance_point_cluster_center": distance_point_cluster_center,
-            "distance_point_center": distance_point_center,
-        }
-
-        return self
-
-    def revert(self, x, y_pred, centers, sample_weight=1.0, correction=None):
-
-        self._furthest_cluster_distance = self._find_furthest_cluster_distance(centers)
-
-        for i in self._center_all_points:
-            self._center_all_points[i].update(x[i], w=-sample_weight)
-
-        self._ssq_points_cluster_centers -= correction["distance_point_cluster_center"]
-        self._ssq_points_center -= correction["distance_point_center"]
-        self._n_clusters = len(centers)
-        self._dim = len(x)
-
-        return self
-
-    def get(self):
-        try:
-            return (
-                1
-                / self._n_clusters
-                * self._ssq_points_center
-                / self._ssq_points_cluster_centers
-                * self._furthest_cluster_distance
-            ) ** self._dim
-        except ZeroDivisionError:
-            return -math.inf
-
-    @property
-    def bigger_is_better(self):
-        return True
+import math
+
+from river import stats, utils
+
+from . import base
+
+
+class R2(base.InternalMetric):
+    """R-Squared
+
+    R-Squared (RS) [^1] is the complement of the ratio of sum of squared distances between objects
+    in different clusters to the total sum of squares. It is an intuitive and simple formulation
+    of measuring the differences between clusters.
+
+    The maximum value of R-Squared is 1, which means that the higher the index, the better
+    the clustering results.
+
+    Examples
+    --------
+
+    >>> from river import cluster
+    >>> from river import stream
+    >>> from river import metrics
+
+    >>> X = [
+    ...     [1, 2],
+    ...     [1, 4],
+    ...     [1, 0],
+    ...     [4, 2],
+    ...     [4, 4],
+    ...     [4, 0],
+    ...     [-2, 2],
+    ...     [-2, 4],
+    ...     [-2, 0]
+    ... ]
+
+    >>> k_means = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)
+    >>> metric = metrics.cluster.R2()
+
+    >>> for x, _ in stream.iter_array(X):
+    ...     k_means = k_means.learn_one(x)
+    ...     y_pred = k_means.predict_one(x)
+    ...     metric = metric.update(x, y_pred, k_means.centers)
+
+    >>> metric
+    R2: 0.509203
+
+    References
+    ----------
+    [^1]: Halkidi, M., Vazirgiannis, M., & Batistakis, Y. (2000). Quality Scheme Assessment in the
+          Clustering Process. Principles Of Data Mining And Knowledge Discovery, 265-276.
+          DOI: 10.1007/3-540-45372-5_26
+
+    """
+
+    def __init__(self):
+        super().__init__()
+        self._center_all_points = {}
+        self._ssq_point_center = 0
+        self._ssq_point_cluster_centers = 0
+        self._cluster_variance = {}
+        self._centers = {}
+        self._initialized = False
+
+    def update(self, x, y_pred, centers, sample_weight=1.0):
+
+        if not self._initialized:
+            self._center_all_points = {i: stats.Mean() for i in x}
+            self._initialized = True
+        for i in self._center_all_points:
+            self._center_all_points[i].update(x[i], w=sample_weight)
+        center_all_points = {
+            i: self._center_all_points[i].get() for i in self._center_all_points
+        }
+
+        squared_distance_center = utils.math.minkowski_distance(x, center_all_points, 2)
+        squared_distance_cluster_center = utils.math.minkowski_distance(
+            x, centers[y_pred], 2
+        )
+
+        self._ssq_point_center += squared_distance_center
+        self._ssq_point_cluster_centers += squared_distance_cluster_center
+
+        return self
+
+    def revert(self, x, y_pred, centers, sample_weight=1.0):
+
+        for i in self._center_all_points:
+            self._center_all_points[i].update(x[i], w=-sample_weight)
+        center_all_points = {
+            i: self._center_all_points[i].get() for i in self._center_all_points
+        }
+
+        squared_distance_center = utils.math.minkowski_distance(x, center_all_points, 2)
+        squared_distance_cluster_center = utils.math.minkowski_distance(
+            x, centers[y_pred], 2
+        )
+
+        self._ssq_point_center -= squared_distance_center
+        self._ssq_point_cluster_centers -= squared_distance_cluster_center
+
+        return self
+
+    def get(self):
+        try:
+            return 1 - self._ssq_point_cluster_centers / self._ssq_point_center
+        except ZeroDivisionError:
+            return -math.inf
+
+    @property
+    def bigger_is_better(self):
+        return True
```

### Comparing `river-0.8.0/river/metrics/cluster/ps.py` & `river-0.9.0/river/metrics/cluster/ps.py`

 * *Ordering differences only*

 * *Files 25% similar despite different names*

```diff
@@ -1,159 +1,159 @@
-import math
-
-from river import stats, utils
-
-from . import base
-
-
-class PS(base.InternalMetric):
-    r"""Partition Separation (PS).
-
-    The PS index [^1] was originally developed for fuzzy clustering. This index
-    only comprises a measure of separation between prototypes. Although classified
-    as a batch clustering validity index (CVI), it can be readily used to evaluate
-    the partitions idenified by unsupervised incremental learners tha model clusters
-    using cenroids.
-
-    Larger values of PS indicate better clustering solutions.
-
-    The PS value is given by
-
-    $$
-    PS = \sum_{i=1}^k PS_i,
-    $$
-
-    where
-
-    $$
-    PS_i = \frac{n_i}{\max_j n_j} - exp \left[ - \frac{\min{i \neq j} (\lVert v_i - v_j \rVert_2^2)}{\beta_T} \right],
-    $$
-
-    $$
-    \beta_T = \frac{1}{k} \sum_{l=1}^k \lVert v_l - \bar{v} \rVert_2 ^2,
-    $$
-
-    and
-
-    $$
-    \bar{v} = \frac{1}{k} \sum_{l=1}^k v_l.
-    $$
-
-    Examples
-    --------
-
-    >>> from river import cluster
-    >>> from river import stream
-    >>> from river import metrics
-
-    >>> X = [
-    ...     [1, 2],
-    ...     [1, 4],
-    ...     [1, 0],
-    ...     [4, 2],
-    ...     [4, 4],
-    ...     [4, 0],
-    ...     [-2, 2],
-    ...     [-2, 4],
-    ...     [-2, 0]
-    ... ]
-
-    >>> k_means = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)
-    >>> metric = metrics.cluster.PS()
-
-    >>> for x, _ in stream.iter_array(X):
-    ...     k_means = k_means.learn_one(x)
-    ...     y_pred = k_means.predict_one(x)
-    ...     metric = metric.update(x, y_pred, k_means.centers)
-
-    >>> metric
-    PS: 1.336026
-
-    References
-    ----------
-    [^1]: E. Lughofer, "Extensions of vector quantization for incremental clustering,"
-          Pattern Recognit., vol. 41, no. 3, pp. 995–1011, Mar. 2008.
-
-    """
-
-    def __init__(self):
-        super().__init__()
-        self._minimum_separation = 0
-        self._center_centers = {}
-        self._n_points_by_cluster = {}
-        self._beta_t = 0
-        self._n_clusters = 0
-
-    @staticmethod
-    def _find_minimum_separation(centers):
-        minimum_separation = math.inf
-        n_centers = max(centers) + 1
-        for i in range(n_centers):
-            for j in range(i + 1, n_centers):
-                separation_ij = utils.math.minkowski_distance(centers[i], centers[j], 2)
-                if separation_ij < minimum_separation:
-                    minimum_separation = separation_ij
-        return minimum_separation
-
-    def update(self, x, y_pred, centers, sample_weight=1.0):
-
-        self._minimum_separation = self._find_minimum_separation(centers)
-
-        self._center_centers = {i: stats.Mean() for i in x}
-
-        for i in self._center_centers:
-            for j in centers:
-                self._center_centers[i].update(centers[j][i], w=sample_weight)
-
-        center_centers = {
-            i: self._center_centers[i].get() for i in self._center_centers
-        }
-        beta_t = stats.Mean()
-        for i in centers:
-            beta_t.update(utils.math.minkowski_distance(centers[i], center_centers, 2))
-        self._beta_t = beta_t.get()
-
-        try:
-            self._n_points_by_cluster[y_pred] += 1
-        except KeyError:
-            self._n_points_by_cluster[y_pred] = 1
-
-        self._n_clusters = len(centers)
-
-        return self
-
-    def revert(self, x, y_pred, centers, sample_weight=1.0):
-
-        self._minimum_separation = self._find_minimum_separation(centers)
-
-        self._center_centers = {i: stats.Mean() for i in x}
-
-        for i in self._center_centers:
-            for j in centers:
-                self._center_centers[i].update(centers[j][i], w=sample_weight)
-
-        center_centers = {
-            i: self._center_centers[i].get() for i in self._center_centers
-        }
-        beta_t = stats.Mean()
-        for i in centers:
-            beta_t.update(utils.math.minkowski_distance(centers[i], center_centers, 2))
-        self._beta_t = beta_t.get()
-
-        self._n_points_by_cluster[y_pred] -= 1
-
-        self._n_clusters = len(centers)
-
-        return self
-
-    def get(self):
-
-        try:
-            return sum(self._n_points_by_cluster.values()) / max(
-                self._n_points_by_cluster.values()
-            ) - self._n_clusters * math.exp(-self._minimum_separation / self._beta_t)
-        except (ZeroDivisionError, ValueError):
-            return -math.inf
-
-    @property
-    def bigger_is_better(self):
-        return True
+import math
+
+from river import stats, utils
+
+from . import base
+
+
+class PS(base.InternalMetric):
+    r"""Partition Separation (PS).
+
+    The PS index [^1] was originally developed for fuzzy clustering. This index
+    only comprises a measure of separation between prototypes. Although classified
+    as a batch clustering validity index (CVI), it can be readily used to evaluate
+    the partitions idenified by unsupervised incremental learners tha model clusters
+    using cenroids.
+
+    Larger values of PS indicate better clustering solutions.
+
+    The PS value is given by
+
+    $$
+    PS = \sum_{i=1}^k PS_i,
+    $$
+
+    where
+
+    $$
+    PS_i = \frac{n_i}{\max_j n_j} - exp \left[ - \frac{\min{i \neq j} (\lVert v_i - v_j \rVert_2^2)}{\beta_T} \right],
+    $$
+
+    $$
+    \beta_T = \frac{1}{k} \sum_{l=1}^k \lVert v_l - \bar{v} \rVert_2 ^2,
+    $$
+
+    and
+
+    $$
+    \bar{v} = \frac{1}{k} \sum_{l=1}^k v_l.
+    $$
+
+    Examples
+    --------
+
+    >>> from river import cluster
+    >>> from river import stream
+    >>> from river import metrics
+
+    >>> X = [
+    ...     [1, 2],
+    ...     [1, 4],
+    ...     [1, 0],
+    ...     [4, 2],
+    ...     [4, 4],
+    ...     [4, 0],
+    ...     [-2, 2],
+    ...     [-2, 4],
+    ...     [-2, 0]
+    ... ]
+
+    >>> k_means = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)
+    >>> metric = metrics.cluster.PS()
+
+    >>> for x, _ in stream.iter_array(X):
+    ...     k_means = k_means.learn_one(x)
+    ...     y_pred = k_means.predict_one(x)
+    ...     metric = metric.update(x, y_pred, k_means.centers)
+
+    >>> metric
+    PS: 1.336026
+
+    References
+    ----------
+    [^1]: E. Lughofer, "Extensions of vector quantization for incremental clustering,"
+          Pattern Recognit., vol. 41, no. 3, pp. 995–1011, Mar. 2008.
+
+    """
+
+    def __init__(self):
+        super().__init__()
+        self._minimum_separation = 0
+        self._center_centers = {}
+        self._n_points_by_cluster = {}
+        self._beta_t = 0
+        self._n_clusters = 0
+
+    @staticmethod
+    def _find_minimum_separation(centers):
+        minimum_separation = math.inf
+        n_centers = max(centers) + 1
+        for i in range(n_centers):
+            for j in range(i + 1, n_centers):
+                separation_ij = utils.math.minkowski_distance(centers[i], centers[j], 2)
+                if separation_ij < minimum_separation:
+                    minimum_separation = separation_ij
+        return minimum_separation
+
+    def update(self, x, y_pred, centers, sample_weight=1.0):
+
+        self._minimum_separation = self._find_minimum_separation(centers)
+
+        self._center_centers = {i: stats.Mean() for i in x}
+
+        for i in self._center_centers:
+            for j in centers:
+                self._center_centers[i].update(centers[j][i], w=sample_weight)
+
+        center_centers = {
+            i: self._center_centers[i].get() for i in self._center_centers
+        }
+        beta_t = stats.Mean()
+        for i in centers:
+            beta_t.update(utils.math.minkowski_distance(centers[i], center_centers, 2))
+        self._beta_t = beta_t.get()
+
+        try:
+            self._n_points_by_cluster[y_pred] += 1
+        except KeyError:
+            self._n_points_by_cluster[y_pred] = 1
+
+        self._n_clusters = len(centers)
+
+        return self
+
+    def revert(self, x, y_pred, centers, sample_weight=1.0):
+
+        self._minimum_separation = self._find_minimum_separation(centers)
+
+        self._center_centers = {i: stats.Mean() for i in x}
+
+        for i in self._center_centers:
+            for j in centers:
+                self._center_centers[i].update(centers[j][i], w=sample_weight)
+
+        center_centers = {
+            i: self._center_centers[i].get() for i in self._center_centers
+        }
+        beta_t = stats.Mean()
+        for i in centers:
+            beta_t.update(utils.math.minkowski_distance(centers[i], center_centers, 2))
+        self._beta_t = beta_t.get()
+
+        self._n_points_by_cluster[y_pred] -= 1
+
+        self._n_clusters = len(centers)
+
+        return self
+
+    def get(self):
+
+        try:
+            return sum(self._n_points_by_cluster.values()) / max(
+                self._n_points_by_cluster.values()
+            ) - self._n_clusters * math.exp(-self._minimum_separation / self._beta_t)
+        except (ZeroDivisionError, ValueError):
+            return -math.inf
+
+    @property
+    def bigger_is_better(self):
+        return True
```

### Comparing `river-0.8.0/river/metrics/cluster/r2.py` & `river-0.9.0/river/metrics/cluster/xiebeni.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,112 +1,103 @@
-import math
-
-from river import stats, utils
-
-from . import base
-
-
-class R2(base.InternalMetric):
-    """R-Squared
-
-    R-Squared (RS) [^1] is the complement of the ratio of sum of squared distances between objects
-    in different clusters to the total sum of squares. It is an intuitive and simple formulation
-    of measuring the differences between clusters.
-
-    The maximum value of R-Squared is 1, which means that the higher the index, the better
-    the clustering results.
-
-    Examples
-    --------
-
-    >>> from river import cluster
-    >>> from river import stream
-    >>> from river import metrics
-
-    >>> X = [
-    ...     [1, 2],
-    ...     [1, 4],
-    ...     [1, 0],
-    ...     [4, 2],
-    ...     [4, 4],
-    ...     [4, 0],
-    ...     [-2, 2],
-    ...     [-2, 4],
-    ...     [-2, 0]
-    ... ]
-
-    >>> k_means = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)
-    >>> metric = metrics.cluster.R2()
-
-    >>> for x, _ in stream.iter_array(X):
-    ...     k_means = k_means.learn_one(x)
-    ...     y_pred = k_means.predict_one(x)
-    ...     metric = metric.update(x, y_pred, k_means.centers)
-
-    >>> metric
-    R2: 0.509203
-
-    References
-    ----------
-    [^1]: Halkidi, M., Vazirgiannis, M., & Batistakis, Y. (2000). Quality Scheme Assessment in the
-          Clustering Process. Principles Of Data Mining And Knowledge Discovery, 265-276.
-          DOI: 10.1007/3-540-45372-5_26
-
-    """
-
-    def __init__(self):
-        super().__init__()
-        self._center_all_points = {}
-        self._ssq_point_center = 0
-        self._ssq_point_cluster_centers = 0
-        self._cluster_variance = {}
-        self._centers = {}
-        self._initialized = False
-
-    def update(self, x, y_pred, centers, sample_weight=1.0):
-
-        if not self._initialized:
-            self._center_all_points = {i: stats.Mean() for i in x}
-            self._initialized = True
-        for i in self._center_all_points:
-            self._center_all_points[i].update(x[i], w=sample_weight)
-        center_all_points = {
-            i: self._center_all_points[i].get() for i in self._center_all_points
-        }
-
-        squared_distance_center = utils.math.minkowski_distance(x, center_all_points, 2)
-        squared_distance_cluster_center = utils.math.minkowski_distance(
-            x, centers[y_pred], 2
-        )
-
-        self._ssq_point_center += squared_distance_center
-        self._ssq_point_cluster_centers += squared_distance_cluster_center
-
-        return self
-
-    def revert(self, x, y_pred, centers, sample_weight=1.0):
-
-        for i in self._center_all_points:
-            self._center_all_points[i].update(x[i], w=-sample_weight)
-        center_all_points = {
-            i: self._center_all_points[i].get() for i in self._center_all_points
-        }
-
-        squared_distance_center = utils.math.minkowski_distance(x, center_all_points, 2)
-        squared_distance_cluster_center = utils.math.minkowski_distance(
-            x, centers[y_pred], 2
-        )
-
-        self._ssq_point_center -= squared_distance_center
-        self._ssq_point_cluster_centers -= squared_distance_cluster_center
-
-        return self
-
-    def get(self):
-        try:
-            return 1 - self._ssq_point_cluster_centers / self._ssq_point_center
-        except ZeroDivisionError:
-            return -math.inf
-
-    @property
-    def bigger_is_better(self):
-        return True
+import math
+
+from river import utils
+
+from . import base
+
+
+class XieBeni(base.InternalMetric):
+    """Xie-Beni index (XB).
+
+    The Xie-Beni index [^1] has the form of (Compactness)/(Separation), which defines the
+    inter-cluster separation as the minimum squared distance between cluster centers,
+    and the intra-cluster compactness as the mean squared distance between each data
+    object and its cluster centers. The smaller the value of XB, the better the
+    clustering quality.
+
+    Examples
+    --------
+
+    >>> from river import cluster
+    >>> from river import stream
+    >>> from river import metrics
+
+    >>> X = [
+    ...     [1, 2],
+    ...     [1, 4],
+    ...     [1, 0],
+    ...     [4, 2],
+    ...     [4, 4],
+    ...     [4, 0],
+    ...     [-2, 2],
+    ...     [-2, 4],
+    ...     [-2, 0]
+    ... ]
+
+    >>> k_means = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)
+    >>> metric = metrics.cluster.XieBeni()
+
+    >>> for x, _ in stream.iter_array(X):
+    ...     k_means = k_means.learn_one(x)
+    ...     y_pred = k_means.predict_one(x)
+    ...     metric = metric.update(x, y_pred, k_means.centers)
+
+    >>> metric
+    XieBeni: 0.397043
+
+    References
+    ----------
+
+    [^1]: X. L. Xie, G. Beni (1991). A validity measure for fuzzy clustering. In: IEEE
+          Transactions on Pattern Analysis and Machine Intelligence 13(8), 841 - 847.
+          DOI: 10.1109/34.85677
+
+    """
+
+    def __init__(self):
+        super().__init__()
+        self._ssw = 0
+        self._minimum_separation = 0
+        self._total_points = 0
+
+    @staticmethod
+    def _find_minimum_separation(centers):
+        minimum_separation = math.inf
+        n_centers = max(centers) + 1
+        for i in range(n_centers):
+            for j in range(i + 1, n_centers):
+                separation_ij = utils.math.minkowski_distance(centers[i], centers[j], 2)
+                if separation_ij < minimum_separation:
+                    minimum_separation = separation_ij
+        return minimum_separation
+
+    def update(self, x, y_pred, centers, sample_weight=1.0):
+
+        squared_distance = utils.math.minkowski_distance(centers[y_pred], x, 2)
+        minimum_separation = self._find_minimum_separation(centers)
+
+        self._ssw += squared_distance
+        self._total_points += 1
+        self._minimum_separation = minimum_separation
+
+        return self
+
+    def revert(self, x, y_pred, centers, sample_weight=1.0):
+
+        squared_distance = utils.math.minkowski_distance(centers[y_pred], x, 2)
+        minimum_separation = self._find_minimum_separation(centers)
+
+        self._ssw -= squared_distance
+        self._total_points -= 1
+        self._minimum_separation -= minimum_separation
+
+        return self
+
+    def get(self):
+        try:
+            return self._ssw / (self._total_points * self._minimum_separation)
+        except ZeroDivisionError:
+            return math.inf
+
+    @property
+    def bigger_is_better(self):
+        return False
```

### Comparing `river-0.8.0/river/metrics/cluster/rmsstd.py` & `river-0.9.0/river/metrics/cluster/rmsstd.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,140 +1,140 @@
-import math
-
-from river import utils
-
-from . import base
-
-__all__ = ["MSSTD", "RMSSTD"]
-
-
-class MSSTD(base.InternalMetric):
-    """Mean Squared Standard Deviation.
-
-    This is the pooled sample variance of all the attributes, which measures
-    only the compactness of found clusters.
-
-    Examples
-    --------
-
-    >>> from river import cluster
-    >>> from river import stream
-    >>> from river import metrics
-
-    >>> X = [
-    ...     [1, 2],
-    ...     [1, 4],
-    ...     [1, 0],
-    ...     [4, 2],
-    ...     [4, 4],
-    ...     [4, 0],
-    ...     [-2, 2],
-    ...     [-2, 4],
-    ...     [-2, 0]
-    ... ]
-
-    >>> k_means = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)
-    >>> metric = metrics.cluster.MSSTD()
-
-    >>> for x, _ in stream.iter_array(X):
-    ...     k_means = k_means.learn_one(x)
-    ...     y_pred = k_means.predict_one(x)
-    ...     metric = metric.update(x, y_pred, k_means.centers)
-
-    >>> metric
-    MSSTD: 2.635708
-
-    References
-    ----------
-    [^1]: Halkidi, M., Batistakis, Y. and Vazirgiannis, M. (2001). On Clustering Validation Techniques.
-          Journal of Intelligent Information Systems, 17, 107 - 145.
-          DOI: 10.1023/a:1012801612483.
-
-    """
-
-    def __init__(self):
-        super().__init__()
-        self._ssq = 0
-        self._total_points = 0
-        self._total_clusters = 0
-        self._dim = 0
-
-    def update(self, x, y_pred, centers, sample_weight=1.0):
-
-        squared_distance = utils.math.minkowski_distance(centers[y_pred], x, 2)
-        n_added_centers = len(centers) - self._total_clusters
-
-        self._ssq += squared_distance
-        self._total_points += 1
-        self._total_clusters += n_added_centers
-        self._dim = len(x)
-
-        return self
-
-    def revert(self, x, y_pred, centers, sample_weight=1.0):
-
-        squared_distance = utils.math.minkowski_distance(centers[y_pred], x, 2)
-        n_added_centers = len(centers) - self._total_clusters
-
-        self._ssq -= squared_distance
-        self._total_clusters -= n_added_centers
-        self._total_points -= 1
-
-        return self
-
-    def get(self):
-        try:
-            return self._ssq / (self._dim * (self._total_points - self._total_clusters))
-        except ZeroDivisionError:
-            return math.inf
-
-    @property
-    def bigger_is_better(self):
-        return False
-
-
-class RMSSTD(MSSTD):
-    """Root Mean Squared Standard Deviation.
-
-    This is the square root of the pooled sample variance of all the attributes, which
-    measures only the compactness of found clusters.
-
-    Examples
-    --------
-
-    >>> from river import cluster
-    >>> from river import stream
-    >>> from river import metrics
-
-    >>> X = [
-    ...     [1, 2],
-    ...     [1, 4],
-    ...     [1, 0],
-    ...     [4, 2],
-    ...     [4, 4],
-    ...     [4, 0],
-    ...     [-2, 2],
-    ...     [-2, 4],
-    ...     [-2, 0]
-    ... ]
-
-    >>> k_means = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)
-    >>> metric = metrics.cluster.RMSSTD()
-
-    >>> for x, _ in stream.iter_array(X):
-    ...     k_means = k_means.learn_one(x)
-    ...     y_pred = k_means.predict_one(x)
-    ...     metric = metric.update(x, y_pred, k_means.centers)
-
-    >>> metric
-    RMSSTD: 1.623486
-
-    References
-    ----------
-    [^1]: Halkidi, M., Batistakis, Y. and Vazirgiannis, M. (2001). On Clustering Validation Techniques.
-          Journal of Intelligent Information Systems, 17, 107 - 145.
-          DOI: 10.1023/a:1012801612483.
-
-    """
-
-    def get(self):
-        return super().get() ** 0.5
+import math
+
+from river import utils
+
+from . import base
+
+__all__ = ["MSSTD", "RMSSTD"]
+
+
+class MSSTD(base.InternalMetric):
+    """Mean Squared Standard Deviation.
+
+    This is the pooled sample variance of all the attributes, which measures
+    only the compactness of found clusters.
+
+    Examples
+    --------
+
+    >>> from river import cluster
+    >>> from river import stream
+    >>> from river import metrics
+
+    >>> X = [
+    ...     [1, 2],
+    ...     [1, 4],
+    ...     [1, 0],
+    ...     [4, 2],
+    ...     [4, 4],
+    ...     [4, 0],
+    ...     [-2, 2],
+    ...     [-2, 4],
+    ...     [-2, 0]
+    ... ]
+
+    >>> k_means = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)
+    >>> metric = metrics.cluster.MSSTD()
+
+    >>> for x, _ in stream.iter_array(X):
+    ...     k_means = k_means.learn_one(x)
+    ...     y_pred = k_means.predict_one(x)
+    ...     metric = metric.update(x, y_pred, k_means.centers)
+
+    >>> metric
+    MSSTD: 2.635708
+
+    References
+    ----------
+    [^1]: Halkidi, M., Batistakis, Y. and Vazirgiannis, M. (2001). On Clustering Validation Techniques.
+          Journal of Intelligent Information Systems, 17, 107 - 145.
+          DOI: 10.1023/a:1012801612483.
+
+    """
+
+    def __init__(self):
+        super().__init__()
+        self._ssq = 0
+        self._total_points = 0
+        self._total_clusters = 0
+        self._dim = 0
+
+    def update(self, x, y_pred, centers, sample_weight=1.0):
+
+        squared_distance = utils.math.minkowski_distance(centers[y_pred], x, 2)
+        n_added_centers = len(centers) - self._total_clusters
+
+        self._ssq += squared_distance
+        self._total_points += 1
+        self._total_clusters += n_added_centers
+        self._dim = len(x)
+
+        return self
+
+    def revert(self, x, y_pred, centers, sample_weight=1.0):
+
+        squared_distance = utils.math.minkowski_distance(centers[y_pred], x, 2)
+        n_added_centers = len(centers) - self._total_clusters
+
+        self._ssq -= squared_distance
+        self._total_clusters -= n_added_centers
+        self._total_points -= 1
+
+        return self
+
+    def get(self):
+        try:
+            return self._ssq / (self._dim * (self._total_points - self._total_clusters))
+        except ZeroDivisionError:
+            return math.inf
+
+    @property
+    def bigger_is_better(self):
+        return False
+
+
+class RMSSTD(MSSTD):
+    """Root Mean Squared Standard Deviation.
+
+    This is the square root of the pooled sample variance of all the attributes, which
+    measures only the compactness of found clusters.
+
+    Examples
+    --------
+
+    >>> from river import cluster
+    >>> from river import stream
+    >>> from river import metrics
+
+    >>> X = [
+    ...     [1, 2],
+    ...     [1, 4],
+    ...     [1, 0],
+    ...     [4, 2],
+    ...     [4, 4],
+    ...     [4, 0],
+    ...     [-2, 2],
+    ...     [-2, 4],
+    ...     [-2, 0]
+    ... ]
+
+    >>> k_means = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)
+    >>> metric = metrics.cluster.RMSSTD()
+
+    >>> for x, _ in stream.iter_array(X):
+    ...     k_means = k_means.learn_one(x)
+    ...     y_pred = k_means.predict_one(x)
+    ...     metric = metric.update(x, y_pred, k_means.centers)
+
+    >>> metric
+    RMSSTD: 1.623486
+
+    References
+    ----------
+    [^1]: Halkidi, M., Batistakis, Y. and Vazirgiannis, M. (2001). On Clustering Validation Techniques.
+          Journal of Intelligent Information Systems, 17, 107 - 145.
+          DOI: 10.1023/a:1012801612483.
+
+    """
+
+    def get(self):
+        return super().get() ** 0.5
```

### Comparing `river-0.8.0/river/metrics/cluster/sd_validation.py` & `river-0.9.0/river/metrics/cluster/sd_validation.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,154 +1,154 @@
-import math
-
-from river import stats, utils
-
-from . import base
-
-
-class SD(base.InternalMetric):
-    """The SD validity index (SD).
-
-    The SD validity index (SD) [^1] is a more recent clustering validation measure. It is composed of
-    two terms:
-
-    * Scat(NC) stands for the scattering within clusters,
-
-    * Dis(NC) stands for the dispersion between clusters.
-
-    Like DB and SB, SD measures the compactness with variance of clustered objects and separation
-    with distance between cluster centers, but uses them in a different way. The smaller the value
-    of SD, the better.
-
-    In the original formula for SD validation index, the ratio between the maximum and the actual
-    number of clusters is taken into account. However, due to the fact that metrics are updated in
-    an incremental fashion, this ratio will be automatically set to default as 1.
-
-    Examples
-    --------
-
-    >>> from river import cluster
-    >>> from river import stream
-    >>> from river import metrics
-
-    >>> X = [
-    ...     [1, 2],
-    ...     [1, 4],
-    ...     [1, 0],
-    ...     [4, 2],
-    ...     [4, 4],
-    ...     [4, 0],
-    ...     [-2, 2],
-    ...     [-2, 4],
-    ...     [-2, 0]
-    ... ]
-
-    >>> k_means = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)
-    >>> metric = metrics.cluster.SD()
-
-    >>> for x, _ in stream.iter_array(X):
-    ...     k_means = k_means.learn_one(x)
-    ...     y_pred = k_means.predict_one(x)
-    ...     metric = metric.update(x, y_pred, k_means.centers)
-
-    >>> metric
-    SD: 2.339016
-
-    References
-    ----------
-    [^1]: Halkidi, M., Vazirgiannis, M., & Batistakis, Y. (2000). Quality Scheme Assessment in the
-          Clustering Process. Principles Of Data Mining And Knowledge Discovery, 265-276.
-          DOI: 10.1007/3-540-45372-5_26
-
-    """
-
-    def __init__(self):
-        super().__init__()
-        self._center_all_points = {}
-        self._overall_variance = {}
-        self._cluster_variance = {}
-        self._centers = {}
-        self._initialized = False
-
-    @staticmethod
-    def _calculate_dispersion_nc(centers):
-        min_distance_clusters = math.inf
-        max_distance_clusters = -math.inf
-        sum_inverse_distances = 0
-
-        n_clusters = len(centers)
-
-        for i in range(n_clusters):
-            for j in range(i + 1, n_clusters):
-                distance_ij = math.sqrt(
-                    utils.math.minkowski_distance(centers[i], centers[j], 2)
-                )
-                if distance_ij > max_distance_clusters:
-                    max_distance_clusters = distance_ij
-                if distance_ij < min_distance_clusters:
-                    min_distance_clusters = distance_ij
-                sum_inverse_distances += 1 / distance_ij
-
-        try:
-            return (
-                max_distance_clusters / min_distance_clusters
-            ) * sum_inverse_distances
-        except ZeroDivisionError:
-            return math.inf
-
-    @staticmethod
-    def _norm(x):
-        origin = {i: 0 for i in x}
-        return math.sqrt(utils.math.minkowski_distance(x, origin, 2))
-
-    def update(self, x, y_pred, centers, sample_weight=1.0):
-
-        if not self._initialized:
-            self._overall_variance = {i: stats.Var() for i in x}
-            self._initialized = True
-
-        if y_pred not in self._cluster_variance:
-            self._cluster_variance[y_pred] = {i: stats.Var() for i in x}
-
-        for i in x:
-            self._cluster_variance[y_pred][i].update(x[i], w=sample_weight)
-            self._overall_variance[i].update(x[i], w=sample_weight)
-
-        self._centers = centers
-
-        return self
-
-    def revert(self, x, y_pred, centers, sample_weight=1.0):
-
-        for i in x:
-            self._overall_variance[i].update(x[i], w=-sample_weight)
-            self._cluster_variance[y_pred][i].update(x[i], w=-sample_weight)
-
-        self._centers = centers
-
-        return self
-
-    def get(self):
-
-        dispersion_nc = self._calculate_dispersion_nc(self._centers)
-
-        overall_variance = {
-            i: self._overall_variance[i].get() for i in self._overall_variance
-        }
-        cluster_variance = {}
-        for i in self._cluster_variance:
-            cluster_variance[i] = {
-                j: self._cluster_variance[i][j].get() for j in self._cluster_variance[i]
-            }
-
-        scat_nc = 0
-        for i in cluster_variance:
-            scat_nc += self._norm(cluster_variance[i]) / self._norm(overall_variance)
-
-        try:
-            return scat_nc + dispersion_nc
-        except ZeroDivisionError:
-            return math.inf
-
-    @property
-    def bigger_is_better(self):
-        return False
+import math
+
+from river import stats, utils
+
+from . import base
+
+
+class SD(base.InternalMetric):
+    """The SD validity index (SD).
+
+    The SD validity index (SD) [^1] is a more recent clustering validation measure. It is composed of
+    two terms:
+
+    * Scat(NC) stands for the scattering within clusters,
+
+    * Dis(NC) stands for the dispersion between clusters.
+
+    Like DB and SB, SD measures the compactness with variance of clustered objects and separation
+    with distance between cluster centers, but uses them in a different way. The smaller the value
+    of SD, the better.
+
+    In the original formula for SD validation index, the ratio between the maximum and the actual
+    number of clusters is taken into account. However, due to the fact that metrics are updated in
+    an incremental fashion, this ratio will be automatically set to default as 1.
+
+    Examples
+    --------
+
+    >>> from river import cluster
+    >>> from river import stream
+    >>> from river import metrics
+
+    >>> X = [
+    ...     [1, 2],
+    ...     [1, 4],
+    ...     [1, 0],
+    ...     [4, 2],
+    ...     [4, 4],
+    ...     [4, 0],
+    ...     [-2, 2],
+    ...     [-2, 4],
+    ...     [-2, 0]
+    ... ]
+
+    >>> k_means = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)
+    >>> metric = metrics.cluster.SD()
+
+    >>> for x, _ in stream.iter_array(X):
+    ...     k_means = k_means.learn_one(x)
+    ...     y_pred = k_means.predict_one(x)
+    ...     metric = metric.update(x, y_pred, k_means.centers)
+
+    >>> metric
+    SD: 2.339016
+
+    References
+    ----------
+    [^1]: Halkidi, M., Vazirgiannis, M., & Batistakis, Y. (2000). Quality Scheme Assessment in the
+          Clustering Process. Principles Of Data Mining And Knowledge Discovery, 265-276.
+          DOI: 10.1007/3-540-45372-5_26
+
+    """
+
+    def __init__(self):
+        super().__init__()
+        self._center_all_points = {}
+        self._overall_variance = {}
+        self._cluster_variance = {}
+        self._centers = {}
+        self._initialized = False
+
+    @staticmethod
+    def _calculate_dispersion_nc(centers):
+        min_distance_clusters = math.inf
+        max_distance_clusters = -math.inf
+        sum_inverse_distances = 0
+
+        n_clusters = len(centers)
+
+        for i in range(n_clusters):
+            for j in range(i + 1, n_clusters):
+                distance_ij = math.sqrt(
+                    utils.math.minkowski_distance(centers[i], centers[j], 2)
+                )
+                if distance_ij > max_distance_clusters:
+                    max_distance_clusters = distance_ij
+                if distance_ij < min_distance_clusters:
+                    min_distance_clusters = distance_ij
+                sum_inverse_distances += 1 / distance_ij
+
+        try:
+            return (
+                max_distance_clusters / min_distance_clusters
+            ) * sum_inverse_distances
+        except ZeroDivisionError:
+            return math.inf
+
+    @staticmethod
+    def _norm(x):
+        origin = {i: 0 for i in x}
+        return math.sqrt(utils.math.minkowski_distance(x, origin, 2))
+
+    def update(self, x, y_pred, centers, sample_weight=1.0):
+
+        if not self._initialized:
+            self._overall_variance = {i: stats.Var() for i in x}
+            self._initialized = True
+
+        if y_pred not in self._cluster_variance:
+            self._cluster_variance[y_pred] = {i: stats.Var() for i in x}
+
+        for i in x:
+            self._cluster_variance[y_pred][i].update(x[i], w=sample_weight)
+            self._overall_variance[i].update(x[i], w=sample_weight)
+
+        self._centers = centers
+
+        return self
+
+    def revert(self, x, y_pred, centers, sample_weight=1.0):
+
+        for i in x:
+            self._overall_variance[i].update(x[i], w=-sample_weight)
+            self._cluster_variance[y_pred][i].update(x[i], w=-sample_weight)
+
+        self._centers = centers
+
+        return self
+
+    def get(self):
+
+        dispersion_nc = self._calculate_dispersion_nc(self._centers)
+
+        overall_variance = {
+            i: self._overall_variance[i].get() for i in self._overall_variance
+        }
+        cluster_variance = {}
+        for i in self._cluster_variance:
+            cluster_variance[i] = {
+                j: self._cluster_variance[i][j].get() for j in self._cluster_variance[i]
+            }
+
+        scat_nc = 0
+        for i in cluster_variance:
+            scat_nc += self._norm(cluster_variance[i]) / self._norm(overall_variance)
+
+        try:
+            return scat_nc + dispersion_nc
+        except ZeroDivisionError:
+            return math.inf
+
+    @property
+    def bigger_is_better(self):
+        return False
```

### Comparing `river-0.8.0/river/metrics/cluster/silhouette.py` & `river-0.9.0/river/metrics/cluster/silhouette.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,115 +1,115 @@
-import math
-
-from river import utils
-
-from . import base
-
-
-class Silhouette(base.InternalMetric):
-    """
-    Silhouette coefficient [^1], roughly speaking, is the ratio between cohesion and the average distances
-    from the points to their second-closest centroid. It rewards the clustering algorithm where
-    points are very close to their assigned centroids and far from any other centroids,
-    that is, clustering results with good cohesion and good separation.
-
-    It rewards clusterings where points are very close to their assigned centroids and far from any other
-    centroids, that is clusterings with good cohesion and good separation. [^2]
-
-    The definition of Silhouette coefficient for online clustering evaluation is different from that of
-    batch learning. It does not store information and calculate pairwise distances between all points at the
-    same time, since the practice is too expensive for an incremental metric.
-
-    Examples
-    --------
-
-    >>> from river import cluster
-    >>> from river import stream
-    >>> from river import metrics
-
-    >>> X = [
-    ...     [1, 2],
-    ...     [1, 4],
-    ...     [1, 0],
-    ...     [4, 2],
-    ...     [4, 4],
-    ...     [4, 0],
-    ...     [-2, 2],
-    ...     [-2, 4],
-    ...     [-2, 0]
-    ... ]
-
-    >>> k_means = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)
-    >>> metric = metrics.cluster.Silhouette()
-
-    >>> for x, _ in stream.iter_array(X):
-    ...     k_means = k_means.learn_one(x)
-    ...     y_pred = k_means.predict_one(x)
-    ...     metric = metric.update(x, y_pred, k_means.centers)
-
-    >>> metric
-    Silhouette: 0.453723
-
-    References
-    ----------
-
-    [^1]: Rousseeuw, P. (1987). Silhouettes: a graphical aid to the intepretation and validation
-          of cluster analysis 20, 53 - 65. DOI: 10.1016/0377-0427(87)90125-7
-
-    [^2]: Bifet, A. et al. (2018). "Machine Learning for Data Streams".
-          DOI: 10.7551/mitpress/10654.001.0001.
-
-    """
-
-    def __init__(self):
-        super().__init__()
-        self._sum_distance_closest_centroid = 0
-        self._sum_distance_second_closest_centroid = 0
-
-    @staticmethod
-    def _find_distance_second_closest_center(centers, x):
-        distances = {
-            i: math.sqrt(utils.math.minkowski_distance(centers[i], x, 2))
-            for i in centers
-        }
-        return sorted(distances.values())[-2]
-
-    def update(self, x, y_pred, centers, sample_weight=1.0):
-
-        distance_closest_centroid = math.sqrt(
-            utils.math.minkowski_distance(centers[y_pred], x, 2)
-        )
-        self._sum_distance_closest_centroid += distance_closest_centroid
-
-        distance_second_closest_centroid = self._find_distance_second_closest_center(
-            centers, x
-        )
-        self._sum_distance_second_closest_centroid += distance_second_closest_centroid
-
-        return self
-
-    def revert(self, x, y_pred, centers, sample_weight=1.0):
-
-        distance_closest_centroid = math.sqrt(
-            utils.math.minkowski_distance(centers[y_pred], x, 2)
-        )
-        self._sum_distance_closest_centroid -= distance_closest_centroid
-
-        distance_second_closest_centroid = self._find_distance_second_closest_center(
-            centers, x
-        )
-        self._sum_distance_second_closest_centroid -= distance_second_closest_centroid
-
-        return self
-
-    def get(self):
-        try:
-            return (
-                self._sum_distance_closest_centroid
-                / self._sum_distance_second_closest_centroid
-            )
-        except ZeroDivisionError:
-            return math.inf
-
-    @property
-    def bigger_is_better(self):
-        return False
+import math
+
+from river import utils
+
+from . import base
+
+
+class Silhouette(base.InternalMetric):
+    """
+    Silhouette coefficient [^1], roughly speaking, is the ratio between cohesion and the average distances
+    from the points to their second-closest centroid. It rewards the clustering algorithm where
+    points are very close to their assigned centroids and far from any other centroids,
+    that is, clustering results with good cohesion and good separation.
+
+    It rewards clusterings where points are very close to their assigned centroids and far from any other
+    centroids, that is clusterings with good cohesion and good separation. [^2]
+
+    The definition of Silhouette coefficient for online clustering evaluation is different from that of
+    batch learning. It does not store information and calculate pairwise distances between all points at the
+    same time, since the practice is too expensive for an incremental metric.
+
+    Examples
+    --------
+
+    >>> from river import cluster
+    >>> from river import stream
+    >>> from river import metrics
+
+    >>> X = [
+    ...     [1, 2],
+    ...     [1, 4],
+    ...     [1, 0],
+    ...     [4, 2],
+    ...     [4, 4],
+    ...     [4, 0],
+    ...     [-2, 2],
+    ...     [-2, 4],
+    ...     [-2, 0]
+    ... ]
+
+    >>> k_means = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)
+    >>> metric = metrics.cluster.Silhouette()
+
+    >>> for x, _ in stream.iter_array(X):
+    ...     k_means = k_means.learn_one(x)
+    ...     y_pred = k_means.predict_one(x)
+    ...     metric = metric.update(x, y_pred, k_means.centers)
+
+    >>> metric
+    Silhouette: 0.453723
+
+    References
+    ----------
+
+    [^1]: Rousseeuw, P. (1987). Silhouettes: a graphical aid to the intepretation and validation
+          of cluster analysis 20, 53 - 65. DOI: 10.1016/0377-0427(87)90125-7
+
+    [^2]: Bifet, A. et al. (2018). "Machine Learning for Data Streams".
+          DOI: 10.7551/mitpress/10654.001.0001.
+
+    """
+
+    def __init__(self):
+        super().__init__()
+        self._sum_distance_closest_centroid = 0
+        self._sum_distance_second_closest_centroid = 0
+
+    @staticmethod
+    def _find_distance_second_closest_center(centers, x):
+        distances = {
+            i: math.sqrt(utils.math.minkowski_distance(centers[i], x, 2))
+            for i in centers
+        }
+        return sorted(distances.values())[-2]
+
+    def update(self, x, y_pred, centers, sample_weight=1.0):
+
+        distance_closest_centroid = math.sqrt(
+            utils.math.minkowski_distance(centers[y_pred], x, 2)
+        )
+        self._sum_distance_closest_centroid += distance_closest_centroid
+
+        distance_second_closest_centroid = self._find_distance_second_closest_center(
+            centers, x
+        )
+        self._sum_distance_second_closest_centroid += distance_second_closest_centroid
+
+        return self
+
+    def revert(self, x, y_pred, centers, sample_weight=1.0):
+
+        distance_closest_centroid = math.sqrt(
+            utils.math.minkowski_distance(centers[y_pred], x, 2)
+        )
+        self._sum_distance_closest_centroid -= distance_closest_centroid
+
+        distance_second_closest_centroid = self._find_distance_second_closest_center(
+            centers, x
+        )
+        self._sum_distance_second_closest_centroid -= distance_second_closest_centroid
+
+        return self
+
+    def get(self):
+        try:
+            return (
+                self._sum_distance_closest_centroid
+                / self._sum_distance_second_closest_centroid
+            )
+        except ZeroDivisionError:
+            return math.inf
+
+    @property
+    def bigger_is_better(self):
+        return False
```

### Comparing `river-0.8.0/river/metrics/cluster/ssb.py` & `river-0.9.0/river/metrics/cluster/ssb.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,121 +1,121 @@
-from river import stats, utils
-
-from . import base
-
-
-class SSB(base.InternalMetric):
-    """Sum-of-Squares Between Clusters (SSB).
-
-    The Sum-of-Squares Between Clusters is the weighted mean of the squares of distances
-    between cluster centers to the mean value of the whole dataset.
-
-    Examples
-    --------
-
-    >>> from river import cluster
-    >>> from river import stream
-    >>> from river import metrics
-
-    >>> X = [
-    ...     [1, 2],
-    ...     [1, 4],
-    ...     [1, 0],
-    ...     [4, 2],
-    ...     [4, 4],
-    ...     [4, 0],
-    ...     [-2, 2],
-    ...     [-2, 4],
-    ...     [-2, 0]
-    ... ]
-
-    >>> k_means = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)
-    >>> metric = metrics.cluster.SSB()
-
-    >>> for x, _ in stream.iter_array(X):
-    ...     k_means = k_means.learn_one(x)
-    ...     y_pred = k_means.predict_one(x)
-    ...     metric = metric.update(x, y_pred, k_means.centers)
-
-    >>> metric
-    SSB: 8.109389
-
-    References
-    ----------
-    [^1]: Q. Zhao, M. Xu, and P. Franti, "Sum-of-squares based cluster validity index
-          and significance analysis," in Adaptive and Natural Computing Algorithms,
-          M. Kolehmainen, P. Toivanen, and B. Beliczynski, Eds.
-          Berlin, Germany: Springer, 2009, pp. 313–322.
-
-    """
-
-    def __init__(self):
-        super().__init__()
-        self._center_all_points = {}
-        self._n_points = 0
-        self._n_points_by_clusters = {}
-        self._squared_distances = {}
-        self._initialized = False
-
-    def update(self, x, y_pred, centers, sample_weight=1.0):
-
-        if not self._initialized:
-            self._center_all_points = {i: stats.Mean() for i in x}
-            self._initialized = True
-
-        for i in self._center_all_points:
-            self._center_all_points[i].update(x[i], w=sample_weight)
-        center_all_points = {
-            i: self._center_all_points[i].get() for i in self._center_all_points
-        }
-
-        self._n_points += 1
-
-        try:
-            self._n_points_by_clusters[y_pred] += 1
-        except KeyError:
-            self._n_points_by_clusters[y_pred] = 1
-
-        for i in centers:
-            self._squared_distances[i] = utils.math.minkowski_distance(
-                centers[i], center_all_points, 2
-            )
-
-        return self
-
-    def revert(self, x, y_pred, centers, sample_weight=1.0):
-
-        for i in self._center_all_points:
-            self._center_all_points[i].update(x[i], w=-sample_weight)
-        center_all_points = {
-            i: self._center_all_points[i].get() for i in self._center_all_points
-        }
-
-        self._n_points -= 1
-
-        self._n_points_by_clusters[y_pred] -= 1
-
-        for i in centers:
-            self._squared_distances[i] = utils.math.minkowski_distance(
-                centers[i], center_all_points, 2
-            )
-
-        return self
-
-    def get(self):
-        ssb = 0
-        for i in self._n_points_by_clusters:
-            try:
-                ssb += (
-                    1
-                    / self._n_points
-                    * self._n_points_by_clusters[i]
-                    * self._squared_distances[i]
-                )
-            except ZeroDivisionError:
-                ssb += 0
-
-        return ssb
-
-    @property
-    def bigger_is_better(self):
-        return True
+from river import stats, utils
+
+from . import base
+
+
+class SSB(base.InternalMetric):
+    """Sum-of-Squares Between Clusters (SSB).
+
+    The Sum-of-Squares Between Clusters is the weighted mean of the squares of distances
+    between cluster centers to the mean value of the whole dataset.
+
+    Examples
+    --------
+
+    >>> from river import cluster
+    >>> from river import stream
+    >>> from river import metrics
+
+    >>> X = [
+    ...     [1, 2],
+    ...     [1, 4],
+    ...     [1, 0],
+    ...     [4, 2],
+    ...     [4, 4],
+    ...     [4, 0],
+    ...     [-2, 2],
+    ...     [-2, 4],
+    ...     [-2, 0]
+    ... ]
+
+    >>> k_means = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)
+    >>> metric = metrics.cluster.SSB()
+
+    >>> for x, _ in stream.iter_array(X):
+    ...     k_means = k_means.learn_one(x)
+    ...     y_pred = k_means.predict_one(x)
+    ...     metric = metric.update(x, y_pred, k_means.centers)
+
+    >>> metric
+    SSB: 8.109389
+
+    References
+    ----------
+    [^1]: Q. Zhao, M. Xu, and P. Franti, "Sum-of-squares based cluster validity index
+          and significance analysis," in Adaptive and Natural Computing Algorithms,
+          M. Kolehmainen, P. Toivanen, and B. Beliczynski, Eds.
+          Berlin, Germany: Springer, 2009, pp. 313–322.
+
+    """
+
+    def __init__(self):
+        super().__init__()
+        self._center_all_points = {}
+        self._n_points = 0
+        self._n_points_by_clusters = {}
+        self._squared_distances = {}
+        self._initialized = False
+
+    def update(self, x, y_pred, centers, sample_weight=1.0):
+
+        if not self._initialized:
+            self._center_all_points = {i: stats.Mean() for i in x}
+            self._initialized = True
+
+        for i in self._center_all_points:
+            self._center_all_points[i].update(x[i], w=sample_weight)
+        center_all_points = {
+            i: self._center_all_points[i].get() for i in self._center_all_points
+        }
+
+        self._n_points += 1
+
+        try:
+            self._n_points_by_clusters[y_pred] += 1
+        except KeyError:
+            self._n_points_by_clusters[y_pred] = 1
+
+        for i in centers:
+            self._squared_distances[i] = utils.math.minkowski_distance(
+                centers[i], center_all_points, 2
+            )
+
+        return self
+
+    def revert(self, x, y_pred, centers, sample_weight=1.0):
+
+        for i in self._center_all_points:
+            self._center_all_points[i].update(x[i], w=-sample_weight)
+        center_all_points = {
+            i: self._center_all_points[i].get() for i in self._center_all_points
+        }
+
+        self._n_points -= 1
+
+        self._n_points_by_clusters[y_pred] -= 1
+
+        for i in centers:
+            self._squared_distances[i] = utils.math.minkowski_distance(
+                centers[i], center_all_points, 2
+            )
+
+        return self
+
+    def get(self):
+        ssb = 0
+        for i in self._n_points_by_clusters:
+            try:
+                ssb += (
+                    1
+                    / self._n_points
+                    * self._n_points_by_clusters[i]
+                    * self._squared_distances[i]
+                )
+            except ZeroDivisionError:
+                ssb += 0
+
+        return ssb
+
+    @property
+    def bigger_is_better(self):
+        return True
```

### Comparing `river-0.8.0/river/metrics/cluster/ssq_based.py` & `river-0.9.0/river/metrics/cluster/ssq_based.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,271 +1,271 @@
-import math
-
-from river import metrics
-
-from . import base
-
-__all__ = ["CalinskiHarabasz", "Hartigan", "WB"]
-
-
-class CalinskiHarabasz(base.InternalMetric):
-    """Calinski-Harabasz index (CH).
-
-    The Calinski-Harabasz index (CH) index measures the criteria simultaneously
-    with the help of average between and within cluster sum of squares.
-
-        * The **numerator** reflects the degree of separation in the way of how much centers are spread.
-
-        * The **denominator** corresponds to compactness, to reflect how close the in-cluster objects
-    are gathered around the cluster center.
-
-    Examples
-    --------
-
-    >>> from river import cluster
-    >>> from river import stream
-    >>> from river import metrics
-
-    >>> X = [
-    ...     [1, 2],
-    ...     [1, 4],
-    ...     [1, 0],
-    ...     [4, 2],
-    ...     [4, 4],
-    ...     [4, 0],
-    ...     [-2, 2],
-    ...     [-2, 4],
-    ...     [-2, 0]
-    ... ]
-
-    >>> k_means = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)
-    >>> metric = metrics.cluster.CalinskiHarabasz()
-
-    >>> for x, _ in stream.iter_array(X):
-    ...     k_means = k_means.learn_one(x)
-    ...     y_pred = k_means.predict_one(x)
-    ...     metric = metric.update(x, y_pred, k_means.centers)
-
-    >>> metric
-    CalinskiHarabasz: 6.922666
-
-    References
-    ----------
-    [^1]: Calinski, T., Harabasz, J.-A. (1974). A Dendrite Method for Cluster Analysis.
-          Communications in Statistics 3(1), 1 - 27. DOI: 10.1080/03610927408827101
-
-    """
-
-    def __init__(self):
-        super().__init__()
-        self._ssb = metrics.cluster.SSB()
-        self._ssw = metrics.cluster.SSW()
-        self._n_clusters = 0
-        self._n_points = 0
-
-    def update(self, x, y_pred, centers, sample_weight=1.0):
-
-        self._ssb.update(x, y_pred, centers, sample_weight)
-
-        self._ssw.update(x, y_pred, centers, sample_weight)
-
-        self._n_clusters = len(centers)
-
-        self._n_points += 1
-
-        return self
-
-    def revert(self, x, y_pred, centers, sample_weight=1.0):
-
-        self._ssb.revert(x, y_pred, centers, sample_weight)
-
-        self._ssw.revert(x, y_pred, centers, sample_weight)
-
-        self._n_clusters = len(centers)
-
-        self._n_points -= 1
-
-        return self
-
-    def get(self):
-        try:
-            return (self._ssb.get() / (self._n_clusters - 1)) / (
-                self._ssw.get() / (self._n_points - self._n_clusters)
-            )
-        except ZeroDivisionError:
-            return -math.inf
-
-    @property
-    def bigger_is_better(self):
-        return True
-
-
-class Hartigan(base.InternalMetric):
-    """Hartigan Index (H - Index)
-
-    Hartigan Index (H - Index) [^1] is a sum-of-square based index [^2], which is
-    equal to the negative log of the division of SSW (Sum-of-Squares Within Clusters)
-    by SSB (Sum-of-Squares Between Clusters).
-
-    The higher the Hartigan index, the higher the clustering quality is.
-
-    Examples
-    --------
-
-    >>> from river import cluster
-    >>> from river import stream
-    >>> from river import metrics
-
-    >>> X = [
-    ...     [1, 2],
-    ...     [1, 4],
-    ...     [1, 0],
-    ...     [4, 2],
-    ...     [4, 4],
-    ...     [4, 0],
-    ...     [-2, 2],
-    ...     [-2, 4],
-    ...     [-2, 0]
-    ... ]
-
-    >>> k_means = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)
-    >>> metric = metrics.cluster.Hartigan()
-
-    >>> for x, _ in stream.iter_array(X):
-    ...     k_means = k_means.learn_one(x)
-    ...     y_pred = k_means.predict_one(x)
-    ...     metric = metric.update(x, y_pred, k_means.centers)
-
-    >>> metric
-    Hartigan: 0.836189
-
-    References
-    ----------
-    [^1]: Hartigan JA (1975). Clustering Algorithms. John Wiley & Sons, Inc.,
-          New York, NY, USA. ISBN 047135645X.
-
-    [^2]: Q. Zhao, M. Xu, and P. Franti, "Sum-of-squares based cluster validity index
-          and significance analysis," in Adaptive and Natural Computing Algorithms,
-          M. Kolehmainen, P. Toivanen, and B. Beliczynski, Eds.
-          Berlin, Germany: Springer, 2009, pp. 313–322.
-
-    """
-
-    def __init__(self):
-        super().__init__()
-        self._ssb = metrics.cluster.SSB()
-        self._ssw = metrics.cluster.SSW()
-
-    def update(self, x, y_pred, centers, sample_weight=1.0):
-
-        self._ssb.update(x, y_pred, centers, sample_weight)
-
-        self._ssw.update(x, y_pred, centers, sample_weight)
-
-        return self
-
-    def revert(self, x, y_pred, centers, sample_weight=1.0):
-
-        self._ssb.revert(x, y_pred, centers, sample_weight)
-
-        self._ssw.revert(x, y_pred, centers, sample_weight)
-
-        return self
-
-    def get(self):
-
-        try:
-            return -math.log(self._ssw.get() / self._ssb.get())
-        except ZeroDivisionError:
-            return -math.inf
-
-    @property
-    def bigger_is_better(self):
-        return True
-
-
-class WB(base.InternalMetric):
-    """WB Index
-
-    WB Index is a simple sum-of-square method, calculated by dividing the within
-    cluster sum-of-squares by the between cluster sum-of-squares. Its effect is emphasized
-    by multiplying the number of clusters. The advantages of the proposed method are
-    that one can determine the number of clusters by minimizing the WB value, without
-    relying on any knee point detection, and this metric is straightforward to implement.
-
-    The lower the WB index, the higher the clustering quality is.
-
-    Examples
-    --------
-
-    >>> from river import cluster
-    >>> from river import stream
-    >>> from river import metrics
-
-    >>> X = [
-    ...     [1, 2],
-    ...     [1, 4],
-    ...     [1, 0],
-    ...     [4, 2],
-    ...     [4, 4],
-    ...     [4, 0],
-    ...     [-2, 2],
-    ...     [-2, 4],
-    ...     [-2, 0]
-    ... ]
-
-    >>> k_means = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)
-    >>> metric = metrics.cluster.WB()
-
-    >>> for x, _ in stream.iter_array(X):
-    ...     k_means = k_means.learn_one(x)
-    ...     y_pred = k_means.predict_one(x)
-    ...     metric = metric.update(x, y_pred, k_means.centers)
-
-    >>> metric
-    WB: 1.300077
-
-    References
-    ----------
-    [^1]: Q. Zhao, M. Xu, and P. Franti, "Sum-of-squares based cluster validity index
-          and significance analysis," in Adaptive and Natural Computing Algorithms,
-          M. Kolehmainen, P. Toivanen, and B. Beliczynski, Eds.
-          Berlin, Germany: Springer, 2009, pp. 313–322.
-
-    """
-
-    def __init__(self):
-        super().__init__()
-        self._ssb = metrics.cluster.SSB()
-        self._ssw = metrics.cluster.SSW()
-        self._n_clusters = 0
-
-    def update(self, x, y_pred, centers, sample_weight=1.0):
-
-        self._ssb.update(x, y_pred, centers, sample_weight)
-
-        self._ssw.update(x, y_pred, centers, sample_weight)
-
-        self._n_clusters = len(centers)
-
-        return self
-
-    def revert(self, x, y_pred, centers, sample_weight=1.0):
-
-        self._ssb.revert(x, y_pred, centers, sample_weight)
-
-        self._ssw.revert(x, y_pred, centers, sample_weight)
-
-        self._n_clusters = len(centers)
-
-        return self
-
-    def get(self):
-
-        try:
-            return self._n_clusters * self._ssw.get() / self._ssb.get()
-        except ZeroDivisionError:
-            return math.inf
-
-    @property
-    def bigger_is_better(self):
-        return False
+import math
+
+from river import metrics
+
+from . import base
+
+__all__ = ["CalinskiHarabasz", "Hartigan", "WB"]
+
+
+class CalinskiHarabasz(base.InternalMetric):
+    """Calinski-Harabasz index (CH).
+
+    The Calinski-Harabasz index (CH) index measures the criteria simultaneously
+    with the help of average between and within cluster sum of squares.
+
+        * The **numerator** reflects the degree of separation in the way of how much centers are spread.
+
+        * The **denominator** corresponds to compactness, to reflect how close the in-cluster objects
+    are gathered around the cluster center.
+
+    Examples
+    --------
+
+    >>> from river import cluster
+    >>> from river import stream
+    >>> from river import metrics
+
+    >>> X = [
+    ...     [1, 2],
+    ...     [1, 4],
+    ...     [1, 0],
+    ...     [4, 2],
+    ...     [4, 4],
+    ...     [4, 0],
+    ...     [-2, 2],
+    ...     [-2, 4],
+    ...     [-2, 0]
+    ... ]
+
+    >>> k_means = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)
+    >>> metric = metrics.cluster.CalinskiHarabasz()
+
+    >>> for x, _ in stream.iter_array(X):
+    ...     k_means = k_means.learn_one(x)
+    ...     y_pred = k_means.predict_one(x)
+    ...     metric = metric.update(x, y_pred, k_means.centers)
+
+    >>> metric
+    CalinskiHarabasz: 6.922666
+
+    References
+    ----------
+    [^1]: Calinski, T., Harabasz, J.-A. (1974). A Dendrite Method for Cluster Analysis.
+          Communications in Statistics 3(1), 1 - 27. DOI: 10.1080/03610927408827101
+
+    """
+
+    def __init__(self):
+        super().__init__()
+        self._ssb = metrics.cluster.SSB()
+        self._ssw = metrics.cluster.SSW()
+        self._n_clusters = 0
+        self._n_points = 0
+
+    def update(self, x, y_pred, centers, sample_weight=1.0):
+
+        self._ssb.update(x, y_pred, centers, sample_weight)
+
+        self._ssw.update(x, y_pred, centers, sample_weight)
+
+        self._n_clusters = len(centers)
+
+        self._n_points += 1
+
+        return self
+
+    def revert(self, x, y_pred, centers, sample_weight=1.0):
+
+        self._ssb.revert(x, y_pred, centers, sample_weight)
+
+        self._ssw.revert(x, y_pred, centers, sample_weight)
+
+        self._n_clusters = len(centers)
+
+        self._n_points -= 1
+
+        return self
+
+    def get(self):
+        try:
+            return (self._ssb.get() / (self._n_clusters - 1)) / (
+                self._ssw.get() / (self._n_points - self._n_clusters)
+            )
+        except ZeroDivisionError:
+            return -math.inf
+
+    @property
+    def bigger_is_better(self):
+        return True
+
+
+class Hartigan(base.InternalMetric):
+    """Hartigan Index (H - Index)
+
+    Hartigan Index (H - Index) [^1] is a sum-of-square based index [^2], which is
+    equal to the negative log of the division of SSW (Sum-of-Squares Within Clusters)
+    by SSB (Sum-of-Squares Between Clusters).
+
+    The higher the Hartigan index, the higher the clustering quality is.
+
+    Examples
+    --------
+
+    >>> from river import cluster
+    >>> from river import stream
+    >>> from river import metrics
+
+    >>> X = [
+    ...     [1, 2],
+    ...     [1, 4],
+    ...     [1, 0],
+    ...     [4, 2],
+    ...     [4, 4],
+    ...     [4, 0],
+    ...     [-2, 2],
+    ...     [-2, 4],
+    ...     [-2, 0]
+    ... ]
+
+    >>> k_means = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)
+    >>> metric = metrics.cluster.Hartigan()
+
+    >>> for x, _ in stream.iter_array(X):
+    ...     k_means = k_means.learn_one(x)
+    ...     y_pred = k_means.predict_one(x)
+    ...     metric = metric.update(x, y_pred, k_means.centers)
+
+    >>> metric
+    Hartigan: 0.836189
+
+    References
+    ----------
+    [^1]: Hartigan JA (1975). Clustering Algorithms. John Wiley & Sons, Inc.,
+          New York, NY, USA. ISBN 047135645X.
+
+    [^2]: Q. Zhao, M. Xu, and P. Franti, "Sum-of-squares based cluster validity index
+          and significance analysis," in Adaptive and Natural Computing Algorithms,
+          M. Kolehmainen, P. Toivanen, and B. Beliczynski, Eds.
+          Berlin, Germany: Springer, 2009, pp. 313–322.
+
+    """
+
+    def __init__(self):
+        super().__init__()
+        self._ssb = metrics.cluster.SSB()
+        self._ssw = metrics.cluster.SSW()
+
+    def update(self, x, y_pred, centers, sample_weight=1.0):
+
+        self._ssb.update(x, y_pred, centers, sample_weight)
+
+        self._ssw.update(x, y_pred, centers, sample_weight)
+
+        return self
+
+    def revert(self, x, y_pred, centers, sample_weight=1.0):
+
+        self._ssb.revert(x, y_pred, centers, sample_weight)
+
+        self._ssw.revert(x, y_pred, centers, sample_weight)
+
+        return self
+
+    def get(self):
+
+        try:
+            return -math.log(self._ssw.get() / self._ssb.get())
+        except ZeroDivisionError:
+            return -math.inf
+
+    @property
+    def bigger_is_better(self):
+        return True
+
+
+class WB(base.InternalMetric):
+    """WB Index
+
+    WB Index is a simple sum-of-square method, calculated by dividing the within
+    cluster sum-of-squares by the between cluster sum-of-squares. Its effect is emphasized
+    by multiplying the number of clusters. The advantages of the proposed method are
+    that one can determine the number of clusters by minimizing the WB value, without
+    relying on any knee point detection, and this metric is straightforward to implement.
+
+    The lower the WB index, the higher the clustering quality is.
+
+    Examples
+    --------
+
+    >>> from river import cluster
+    >>> from river import stream
+    >>> from river import metrics
+
+    >>> X = [
+    ...     [1, 2],
+    ...     [1, 4],
+    ...     [1, 0],
+    ...     [4, 2],
+    ...     [4, 4],
+    ...     [4, 0],
+    ...     [-2, 2],
+    ...     [-2, 4],
+    ...     [-2, 0]
+    ... ]
+
+    >>> k_means = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)
+    >>> metric = metrics.cluster.WB()
+
+    >>> for x, _ in stream.iter_array(X):
+    ...     k_means = k_means.learn_one(x)
+    ...     y_pred = k_means.predict_one(x)
+    ...     metric = metric.update(x, y_pred, k_means.centers)
+
+    >>> metric
+    WB: 1.300077
+
+    References
+    ----------
+    [^1]: Q. Zhao, M. Xu, and P. Franti, "Sum-of-squares based cluster validity index
+          and significance analysis," in Adaptive and Natural Computing Algorithms,
+          M. Kolehmainen, P. Toivanen, and B. Beliczynski, Eds.
+          Berlin, Germany: Springer, 2009, pp. 313–322.
+
+    """
+
+    def __init__(self):
+        super().__init__()
+        self._ssb = metrics.cluster.SSB()
+        self._ssw = metrics.cluster.SSW()
+        self._n_clusters = 0
+
+    def update(self, x, y_pred, centers, sample_weight=1.0):
+
+        self._ssb.update(x, y_pred, centers, sample_weight)
+
+        self._ssw.update(x, y_pred, centers, sample_weight)
+
+        self._n_clusters = len(centers)
+
+        return self
+
+    def revert(self, x, y_pred, centers, sample_weight=1.0):
+
+        self._ssb.revert(x, y_pred, centers, sample_weight)
+
+        self._ssw.revert(x, y_pred, centers, sample_weight)
+
+        self._n_clusters = len(centers)
+
+        return self
+
+    def get(self):
+
+        try:
+            return self._n_clusters * self._ssw.get() / self._ssb.get()
+        except ZeroDivisionError:
+            return math.inf
+
+    @property
+    def bigger_is_better(self):
+        return False
```

### Comparing `river-0.8.0/river/metrics/confusion.c` & `river-0.9.0/river/metrics/confusion.c`

 * *Files 2% similar despite different names*

```diff
@@ -6,29 +6,26 @@
         "define_macros": [
             [
                 "NPY_NO_DEPRECATED_API",
                 "NPY_1_7_API_VERSION"
             ]
         ],
         "depends": [
-            "/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/core/include/numpy/arrayobject.h",
-            "/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/core/include/numpy/arrayscalars.h",
-            "/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/core/include/numpy/ndarrayobject.h",
-            "/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/core/include/numpy/ndarraytypes.h",
-            "/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/core/include/numpy/ufuncobject.h"
+            "C:\\cibw\\python\\python.3.9.9\\tools\\lib\\site-packages\\numpy\\core\\include\\numpy\\arrayobject.h",
+            "C:\\cibw\\python\\python.3.9.9\\tools\\lib\\site-packages\\numpy\\core\\include\\numpy\\arrayscalars.h",
+            "C:\\cibw\\python\\python.3.9.9\\tools\\lib\\site-packages\\numpy\\core\\include\\numpy\\ndarrayobject.h",
+            "C:\\cibw\\python\\python.3.9.9\\tools\\lib\\site-packages\\numpy\\core\\include\\numpy\\ndarraytypes.h",
+            "C:\\cibw\\python\\python.3.9.9\\tools\\lib\\site-packages\\numpy\\core\\include\\numpy\\ufuncobject.h"
         ],
         "include_dirs": [
-            "/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/core/include"
-        ],
-        "libraries": [
-            "m"
+            "C:\\cibw\\python\\python.3.9.9\\tools\\lib\\site-packages\\numpy\\core\\include"
         ],
         "name": "river.metrics.confusion",
         "sources": [
-            "river/metrics/confusion.pyx"
+            "river\\metrics\\confusion.pyx"
         ]
     },
     "module_name": "river.metrics.confusion"
 }
 END: Cython Metadata */
 
 #ifndef PY_SSIZE_T_CLEAN
@@ -882,202 +879,202 @@
 #if CYTHON_CCOMPLEX && !defined(__cplusplus) && defined(__sun__) && defined(__GNUC__)
   #undef _Complex_I
   #define _Complex_I 1.0fj
 #endif
 
 
 static const char *__pyx_f[] = {
-  "river/metrics/confusion.pyx",
-  "river/metrics/confusion.pxd",
+  "river\\metrics\\confusion.pyx",
+  "river\\metrics\\confusion.pxd",
   "stringsource",
   "__init__.pxd",
   "type.pxd",
 };
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":690
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":690
  * # in Cython to enable them only on the right systems.
  * 
  * ctypedef npy_int8       int8_t             # <<<<<<<<<<<<<<
  * ctypedef npy_int16      int16_t
  * ctypedef npy_int32      int32_t
  */
 typedef npy_int8 __pyx_t_5numpy_int8_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":691
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":691
  * 
  * ctypedef npy_int8       int8_t
  * ctypedef npy_int16      int16_t             # <<<<<<<<<<<<<<
  * ctypedef npy_int32      int32_t
  * ctypedef npy_int64      int64_t
  */
 typedef npy_int16 __pyx_t_5numpy_int16_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":692
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":692
  * ctypedef npy_int8       int8_t
  * ctypedef npy_int16      int16_t
  * ctypedef npy_int32      int32_t             # <<<<<<<<<<<<<<
  * ctypedef npy_int64      int64_t
  * #ctypedef npy_int96      int96_t
  */
 typedef npy_int32 __pyx_t_5numpy_int32_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":693
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":693
  * ctypedef npy_int16      int16_t
  * ctypedef npy_int32      int32_t
  * ctypedef npy_int64      int64_t             # <<<<<<<<<<<<<<
  * #ctypedef npy_int96      int96_t
  * #ctypedef npy_int128     int128_t
  */
 typedef npy_int64 __pyx_t_5numpy_int64_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":697
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":697
  * #ctypedef npy_int128     int128_t
  * 
  * ctypedef npy_uint8      uint8_t             # <<<<<<<<<<<<<<
  * ctypedef npy_uint16     uint16_t
  * ctypedef npy_uint32     uint32_t
  */
 typedef npy_uint8 __pyx_t_5numpy_uint8_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":698
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":698
  * 
  * ctypedef npy_uint8      uint8_t
  * ctypedef npy_uint16     uint16_t             # <<<<<<<<<<<<<<
  * ctypedef npy_uint32     uint32_t
  * ctypedef npy_uint64     uint64_t
  */
 typedef npy_uint16 __pyx_t_5numpy_uint16_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":699
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":699
  * ctypedef npy_uint8      uint8_t
  * ctypedef npy_uint16     uint16_t
  * ctypedef npy_uint32     uint32_t             # <<<<<<<<<<<<<<
  * ctypedef npy_uint64     uint64_t
  * #ctypedef npy_uint96     uint96_t
  */
 typedef npy_uint32 __pyx_t_5numpy_uint32_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":700
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":700
  * ctypedef npy_uint16     uint16_t
  * ctypedef npy_uint32     uint32_t
  * ctypedef npy_uint64     uint64_t             # <<<<<<<<<<<<<<
  * #ctypedef npy_uint96     uint96_t
  * #ctypedef npy_uint128    uint128_t
  */
 typedef npy_uint64 __pyx_t_5numpy_uint64_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":704
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":704
  * #ctypedef npy_uint128    uint128_t
  * 
  * ctypedef npy_float32    float32_t             # <<<<<<<<<<<<<<
  * ctypedef npy_float64    float64_t
  * #ctypedef npy_float80    float80_t
  */
 typedef npy_float32 __pyx_t_5numpy_float32_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":705
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":705
  * 
  * ctypedef npy_float32    float32_t
  * ctypedef npy_float64    float64_t             # <<<<<<<<<<<<<<
  * #ctypedef npy_float80    float80_t
  * #ctypedef npy_float128   float128_t
  */
 typedef npy_float64 __pyx_t_5numpy_float64_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":714
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":714
  * # The int types are mapped a bit surprising --
  * # numpy.int corresponds to 'l' and numpy.long to 'q'
  * ctypedef npy_long       int_t             # <<<<<<<<<<<<<<
  * ctypedef npy_longlong   long_t
  * ctypedef npy_longlong   longlong_t
  */
 typedef npy_long __pyx_t_5numpy_int_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":715
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":715
  * # numpy.int corresponds to 'l' and numpy.long to 'q'
  * ctypedef npy_long       int_t
  * ctypedef npy_longlong   long_t             # <<<<<<<<<<<<<<
  * ctypedef npy_longlong   longlong_t
  * 
  */
 typedef npy_longlong __pyx_t_5numpy_long_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":716
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":716
  * ctypedef npy_long       int_t
  * ctypedef npy_longlong   long_t
  * ctypedef npy_longlong   longlong_t             # <<<<<<<<<<<<<<
  * 
  * ctypedef npy_ulong      uint_t
  */
 typedef npy_longlong __pyx_t_5numpy_longlong_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":718
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":718
  * ctypedef npy_longlong   longlong_t
  * 
  * ctypedef npy_ulong      uint_t             # <<<<<<<<<<<<<<
  * ctypedef npy_ulonglong  ulong_t
  * ctypedef npy_ulonglong  ulonglong_t
  */
 typedef npy_ulong __pyx_t_5numpy_uint_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":719
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":719
  * 
  * ctypedef npy_ulong      uint_t
  * ctypedef npy_ulonglong  ulong_t             # <<<<<<<<<<<<<<
  * ctypedef npy_ulonglong  ulonglong_t
  * 
  */
 typedef npy_ulonglong __pyx_t_5numpy_ulong_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":720
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":720
  * ctypedef npy_ulong      uint_t
  * ctypedef npy_ulonglong  ulong_t
  * ctypedef npy_ulonglong  ulonglong_t             # <<<<<<<<<<<<<<
  * 
  * ctypedef npy_intp       intp_t
  */
 typedef npy_ulonglong __pyx_t_5numpy_ulonglong_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":722
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":722
  * ctypedef npy_ulonglong  ulonglong_t
  * 
  * ctypedef npy_intp       intp_t             # <<<<<<<<<<<<<<
  * ctypedef npy_uintp      uintp_t
  * 
  */
 typedef npy_intp __pyx_t_5numpy_intp_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":723
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":723
  * 
  * ctypedef npy_intp       intp_t
  * ctypedef npy_uintp      uintp_t             # <<<<<<<<<<<<<<
  * 
  * ctypedef npy_double     float_t
  */
 typedef npy_uintp __pyx_t_5numpy_uintp_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":725
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":725
  * ctypedef npy_uintp      uintp_t
  * 
  * ctypedef npy_double     float_t             # <<<<<<<<<<<<<<
  * ctypedef npy_double     double_t
  * ctypedef npy_longdouble longdouble_t
  */
 typedef npy_double __pyx_t_5numpy_float_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":726
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":726
  * 
  * ctypedef npy_double     float_t
  * ctypedef npy_double     double_t             # <<<<<<<<<<<<<<
  * ctypedef npy_longdouble longdouble_t
  * 
  */
 typedef npy_double __pyx_t_5numpy_double_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":727
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":727
  * ctypedef npy_double     float_t
  * ctypedef npy_double     double_t
  * ctypedef npy_longdouble longdouble_t             # <<<<<<<<<<<<<<
  * 
  * ctypedef npy_cfloat      cfloat_t
  */
 typedef npy_longdouble __pyx_t_5numpy_longdouble_t;
@@ -1116,42 +1113,42 @@
 struct __pyx_obj_5river_7metrics_9confusion___pyx_scope_struct_4_genexpr;
 struct __pyx_obj_5river_7metrics_9confusion___pyx_scope_struct_5_genexpr;
 struct __pyx_obj_5river_7metrics_9confusion___pyx_scope_struct_6_genexpr;
 struct __pyx_obj_5river_7metrics_9confusion___pyx_scope_struct_7_genexpr;
 struct __pyx_obj_5river_7metrics_9confusion___pyx_scope_struct_8___repr__;
 struct __pyx_obj_5river_7metrics_9confusion___pyx_scope_struct_9_genexpr;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":729
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":729
  * ctypedef npy_longdouble longdouble_t
  * 
  * ctypedef npy_cfloat      cfloat_t             # <<<<<<<<<<<<<<
  * ctypedef npy_cdouble     cdouble_t
  * ctypedef npy_clongdouble clongdouble_t
  */
 typedef npy_cfloat __pyx_t_5numpy_cfloat_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":730
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":730
  * 
  * ctypedef npy_cfloat      cfloat_t
  * ctypedef npy_cdouble     cdouble_t             # <<<<<<<<<<<<<<
  * ctypedef npy_clongdouble clongdouble_t
  * 
  */
 typedef npy_cdouble __pyx_t_5numpy_cdouble_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":731
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":731
  * ctypedef npy_cfloat      cfloat_t
  * ctypedef npy_cdouble     cdouble_t
  * ctypedef npy_clongdouble clongdouble_t             # <<<<<<<<<<<<<<
  * 
  * ctypedef npy_cdouble     complex_t
  */
 typedef npy_clongdouble __pyx_t_5numpy_clongdouble_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":733
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":733
  * ctypedef npy_clongdouble clongdouble_t
  * 
  * ctypedef npy_cdouble     complex_t             # <<<<<<<<<<<<<<
  * 
  * cdef inline object PyArray_MultiIterNew1(a):
  */
 typedef npy_cdouble __pyx_t_5numpy_complex_t;
@@ -1323,28 +1320,28 @@
   PyObject_HEAD
   struct __pyx_obj_5river_7metrics_9confusion___pyx_scope_struct_3___repr__ *__pyx_outer_scope;
   PyObject *__pyx_8genexpr6__pyx_v_y_pred;
   PyObject *__pyx_v_y_true;
 };
 
 
-/* "river/metrics/confusion.pyx":352
+/* "river/metrics/confusion.pyx":354
  *         return self.data.__str__()
  * 
  *     def __repr__(self):             # <<<<<<<<<<<<<<
  *         # The labels are sorted alphabetically for reproducibility reasons
  *         labels = sorted(self.labels)
  */
 struct __pyx_obj_5river_7metrics_9confusion___pyx_scope_struct_8___repr__ {
   PyObject_HEAD
   PyObject *__pyx_v_labels;
 };
 
 
-/* "river/metrics/confusion.pyx":360
+/* "river/metrics/confusion.pyx":362
  * 
  *         # Determine the required width of each column in the table
  *         largest_label_len = max(len(str(label)) for label in labels)             # <<<<<<<<<<<<<<
  *         largest_value_len = len(str(self.data[:].max()))
  *         width = max(5, largest_label_len, largest_value_len) + 2   # Min value is 5=len('label')
  */
 struct __pyx_obj_5river_7metrics_9confusion___pyx_scope_struct_9_genexpr {
@@ -2360,15 +2357,15 @@
 static const char __pyx_k_cline_in_traceback[] = "cline_in_traceback";
 static const char __pyx_k_ConfusionMatrix_reset[] = "ConfusionMatrix.reset";
 static const char __pyx_k_repr___locals_genexpr[] = "__repr__.<locals>.genexpr";
 static const char __pyx_k_ConfusionMatrix_revert[] = "ConfusionMatrix.revert";
 static const char __pyx_k_ConfusionMatrix_update[] = "ConfusionMatrix.update";
 static const char __pyx_k_river_metrics_confusion[] = "river.metrics.confusion";
 static const char __pyx_k_MultiLabelConfusionMatrix[] = "MultiLabelConfusionMatrix";
-static const char __pyx_k_river_metrics_confusion_pyx[] = "river/metrics/confusion.pyx";
+static const char __pyx_k_river_metrics_confusion_pyx[] = "river\\metrics\\confusion.pyx";
 static const char __pyx_k_pyx_unpickle_ConfusionMatrix[] = "__pyx_unpickle_ConfusionMatrix";
 static const char __pyx_k_width_width_width_width_width[] = "{:>{width}}{:>{width}}{:>{width}}{:>{width}}{:>{width}}";
 static const char __pyx_k_ConfusionMatrix__update_matrix[] = "ConfusionMatrix._update_matrix";
 static const char __pyx_k_ConfusionMatrix_true_negatives[] = "ConfusionMatrix.true_negatives";
 static const char __pyx_k_ConfusionMatrix_true_positives[] = "ConfusionMatrix.true_positives";
 static const char __pyx_k_pyx_unpickle_MultiLabelConfusi[] = "__pyx_unpickle_MultiLabelConfusionMatrix";
 static const char __pyx_k_repr___locals_genexpr_locals_g[] = "__repr__.<locals>.genexpr.<locals>.genexpr";
@@ -8149,33 +8146,43 @@
   __pyx_v_self->last_y_true = __pyx_v_y_true;
 
   /* "river/metrics/confusion.pyx":295
  *         # Keep track of last entry
  *         self.last_y_true = y_true
  *         self.last_y_pred = y_pred             # <<<<<<<<<<<<<<
  * 
- * 
+ *         return self
  */
   __Pyx_INCREF(__pyx_v_y_pred);
   __Pyx_GIVEREF(__pyx_v_y_pred);
   __Pyx_GOTREF(__pyx_v_self->last_y_pred);
   __Pyx_DECREF(__pyx_v_self->last_y_pred);
   __pyx_v_self->last_y_pred = __pyx_v_y_pred;
 
+  /* "river/metrics/confusion.pyx":297
+ *         self.last_y_pred = y_pred
+ * 
+ *         return self             # <<<<<<<<<<<<<<
+ * 
+ * 
+ */
+  __Pyx_XDECREF(__pyx_r);
+  __Pyx_INCREF(((PyObject *)__pyx_v_self));
+  __pyx_r = ((PyObject *)__pyx_v_self);
+  goto __pyx_L0;
+
   /* "river/metrics/confusion.pyx":247
  *         self.recall_sum = 0.                        # Recall sum
  * 
  *     def update(self, y_true, y_pred, sample_weight=1.0):             # <<<<<<<<<<<<<<
  * 
  *         cdef int is_equal = 1
  */
 
   /* function exit code */
-  __pyx_r = Py_None; __Pyx_INCREF(Py_None);
-  goto __pyx_L0;
   __pyx_L1_error:;
   __Pyx_XDECREF(__pyx_t_1);
   __Pyx_XDECREF(__pyx_t_7);
   __Pyx_XDECREF(__pyx_t_9);
   __Pyx_XDECREF(__pyx_t_10);
   __Pyx_XDECREF(__pyx_t_11);
   __Pyx_XDECREF(__pyx_t_12);
@@ -8184,15 +8191,15 @@
   __pyx_L0:;
   __Pyx_XDECREF(__pyx_v_label);
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "river/metrics/confusion.pyx":298
+/* "river/metrics/confusion.pyx":300
  * 
  * 
  *     def revert(self, y_true, y_pred, sample_weight=1., correction=None):             # <<<<<<<<<<<<<<
  *         self.n_samples -= 1
  *         # Revert is equal to subtracting so we pass the negative sample_weight
  */
 
@@ -8236,15 +8243,15 @@
         case  0:
         if (likely((values[0] = __Pyx_PyDict_GetItemStr(__pyx_kwds, __pyx_n_s_y_true)) != 0)) kw_args--;
         else goto __pyx_L5_argtuple_error;
         CYTHON_FALLTHROUGH;
         case  1:
         if (likely((values[1] = __Pyx_PyDict_GetItemStr(__pyx_kwds, __pyx_n_s_y_pred)) != 0)) kw_args--;
         else {
-          __Pyx_RaiseArgtupleInvalid("revert", 0, 2, 4, 1); __PYX_ERR(0, 298, __pyx_L3_error)
+          __Pyx_RaiseArgtupleInvalid("revert", 0, 2, 4, 1); __PYX_ERR(0, 300, __pyx_L3_error)
         }
         CYTHON_FALLTHROUGH;
         case  2:
         if (kw_args > 0) {
           PyObject* value = __Pyx_PyDict_GetItemStr(__pyx_kwds, __pyx_n_s_sample_weight);
           if (value) { values[2] = value; kw_args--; }
         }
@@ -8252,15 +8259,15 @@
         case  3:
         if (kw_args > 0) {
           PyObject* value = __Pyx_PyDict_GetItemStr(__pyx_kwds, __pyx_n_s_correction);
           if (value) { values[3] = value; kw_args--; }
         }
       }
       if (unlikely(kw_args > 0)) {
-        if (unlikely(__Pyx_ParseOptionalKeywords(__pyx_kwds, __pyx_pyargnames, 0, values, pos_args, "revert") < 0)) __PYX_ERR(0, 298, __pyx_L3_error)
+        if (unlikely(__Pyx_ParseOptionalKeywords(__pyx_kwds, __pyx_pyargnames, 0, values, pos_args, "revert") < 0)) __PYX_ERR(0, 300, __pyx_L3_error)
       }
     } else {
       switch (PyTuple_GET_SIZE(__pyx_args)) {
         case  4: values[3] = PyTuple_GET_ITEM(__pyx_args, 3);
         CYTHON_FALLTHROUGH;
         case  3: values[2] = PyTuple_GET_ITEM(__pyx_args, 2);
         CYTHON_FALLTHROUGH;
@@ -8273,15 +8280,15 @@
     __pyx_v_y_true = values[0];
     __pyx_v_y_pred = values[1];
     __pyx_v_sample_weight = values[2];
     __pyx_v_correction = values[3];
   }
   goto __pyx_L4_argument_unpacking_done;
   __pyx_L5_argtuple_error:;
-  __Pyx_RaiseArgtupleInvalid("revert", 0, 2, 4, PyTuple_GET_SIZE(__pyx_args)); __PYX_ERR(0, 298, __pyx_L3_error)
+  __Pyx_RaiseArgtupleInvalid("revert", 0, 2, 4, PyTuple_GET_SIZE(__pyx_args)); __PYX_ERR(0, 300, __pyx_L3_error)
   __pyx_L3_error:;
   __Pyx_AddTraceback("river.metrics.confusion.MultiLabelConfusionMatrix.revert", __pyx_clineno, __pyx_lineno, __pyx_filename);
   __Pyx_RefNannyFinishContext();
   return NULL;
   __pyx_L4_argument_unpacking_done:;
   __pyx_r = __pyx_pf_5river_7metrics_9confusion_25MultiLabelConfusionMatrix_4revert(((struct __pyx_obj_5river_7metrics_9confusion_MultiLabelConfusionMatrix *)__pyx_v_self), __pyx_v_y_true, __pyx_v_y_pred, __pyx_v_sample_weight, __pyx_v_correction);
 
@@ -8307,187 +8314,187 @@
   PyObject *__pyx_t_10 = NULL;
   double __pyx_t_11;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("revert", 0);
 
-  /* "river/metrics/confusion.pyx":299
+  /* "river/metrics/confusion.pyx":301
  * 
  *     def revert(self, y_true, y_pred, sample_weight=1., correction=None):
  *         self.n_samples -= 1             # <<<<<<<<<<<<<<
  *         # Revert is equal to subtracting so we pass the negative sample_weight
  *         for label in y_true.keys():
  */
   __pyx_v_self->n_samples = (__pyx_v_self->n_samples - 1);
 
-  /* "river/metrics/confusion.pyx":301
+  /* "river/metrics/confusion.pyx":303
  *         self.n_samples -= 1
  *         # Revert is equal to subtracting so we pass the negative sample_weight
  *         for label in y_true.keys():             # <<<<<<<<<<<<<<
  *             label_idx = self._map_label(label, add_label=True)
  *             self.data[label_idx, y_true[label], y_pred[label]] += -sample_weight
  */
   __pyx_t_2 = 0;
   if (unlikely(__pyx_v_y_true == Py_None)) {
     PyErr_Format(PyExc_AttributeError, "'NoneType' object has no attribute '%.30s'", "keys");
-    __PYX_ERR(0, 301, __pyx_L1_error)
+    __PYX_ERR(0, 303, __pyx_L1_error)
   }
-  __pyx_t_5 = __Pyx_dict_iterator(__pyx_v_y_true, 0, __pyx_n_s_keys, (&__pyx_t_3), (&__pyx_t_4)); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 301, __pyx_L1_error)
+  __pyx_t_5 = __Pyx_dict_iterator(__pyx_v_y_true, 0, __pyx_n_s_keys, (&__pyx_t_3), (&__pyx_t_4)); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 303, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_5);
   __Pyx_XDECREF(__pyx_t_1);
   __pyx_t_1 = __pyx_t_5;
   __pyx_t_5 = 0;
   while (1) {
     __pyx_t_6 = __Pyx_dict_iter_next(__pyx_t_1, __pyx_t_3, &__pyx_t_2, &__pyx_t_5, NULL, NULL, __pyx_t_4);
     if (unlikely(__pyx_t_6 == 0)) break;
-    if (unlikely(__pyx_t_6 == -1)) __PYX_ERR(0, 301, __pyx_L1_error)
+    if (unlikely(__pyx_t_6 == -1)) __PYX_ERR(0, 303, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_5);
     __Pyx_XDECREF_SET(__pyx_v_label, __pyx_t_5);
     __pyx_t_5 = 0;
 
-    /* "river/metrics/confusion.pyx":302
+    /* "river/metrics/confusion.pyx":304
  *         # Revert is equal to subtracting so we pass the negative sample_weight
  *         for label in y_true.keys():
  *             label_idx = self._map_label(label, add_label=True)             # <<<<<<<<<<<<<<
  *             self.data[label_idx, y_true[label], y_pred[label]] += -sample_weight
  * 
  */
     __pyx_v_label_idx = ((struct __pyx_vtabstruct_5river_7metrics_9confusion_MultiLabelConfusionMatrix *)__pyx_v_self->__pyx_vtab)->_map_label(__pyx_v_self, __pyx_v_label, 1);
 
-    /* "river/metrics/confusion.pyx":303
+    /* "river/metrics/confusion.pyx":305
  *         for label in y_true.keys():
  *             label_idx = self._map_label(label, add_label=True)
  *             self.data[label_idx, y_true[label], y_pred[label]] += -sample_weight             # <<<<<<<<<<<<<<
  * 
  *         # Update auxiliary variables
  */
     __Pyx_INCREF(__pyx_v_self->data);
     __pyx_t_5 = __pyx_v_self->data;
-    __pyx_t_7 = __Pyx_PyInt_From_int(__pyx_v_label_idx); if (unlikely(!__pyx_t_7)) __PYX_ERR(0, 303, __pyx_L1_error)
+    __pyx_t_7 = __Pyx_PyInt_From_int(__pyx_v_label_idx); if (unlikely(!__pyx_t_7)) __PYX_ERR(0, 305, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_7);
-    __pyx_t_8 = __Pyx_PyObject_GetItem(__pyx_v_y_true, __pyx_v_label); if (unlikely(!__pyx_t_8)) __PYX_ERR(0, 303, __pyx_L1_error)
+    __pyx_t_8 = __Pyx_PyObject_GetItem(__pyx_v_y_true, __pyx_v_label); if (unlikely(!__pyx_t_8)) __PYX_ERR(0, 305, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_8);
-    __pyx_t_9 = __Pyx_PyObject_GetItem(__pyx_v_y_pred, __pyx_v_label); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 303, __pyx_L1_error)
+    __pyx_t_9 = __Pyx_PyObject_GetItem(__pyx_v_y_pred, __pyx_v_label); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 305, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_9);
-    __pyx_t_10 = PyTuple_New(3); if (unlikely(!__pyx_t_10)) __PYX_ERR(0, 303, __pyx_L1_error)
+    __pyx_t_10 = PyTuple_New(3); if (unlikely(!__pyx_t_10)) __PYX_ERR(0, 305, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_10);
     __Pyx_GIVEREF(__pyx_t_7);
     PyTuple_SET_ITEM(__pyx_t_10, 0, __pyx_t_7);
     __Pyx_GIVEREF(__pyx_t_8);
     PyTuple_SET_ITEM(__pyx_t_10, 1, __pyx_t_8);
     __Pyx_GIVEREF(__pyx_t_9);
     PyTuple_SET_ITEM(__pyx_t_10, 2, __pyx_t_9);
     __pyx_t_7 = 0;
     __pyx_t_8 = 0;
     __pyx_t_9 = 0;
-    __pyx_t_9 = __Pyx_PyObject_GetItem(__pyx_t_5, __pyx_t_10); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 303, __pyx_L1_error)
+    __pyx_t_9 = __Pyx_PyObject_GetItem(__pyx_t_5, __pyx_t_10); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 305, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_9);
-    __pyx_t_8 = PyNumber_Negative(__pyx_v_sample_weight); if (unlikely(!__pyx_t_8)) __PYX_ERR(0, 303, __pyx_L1_error)
+    __pyx_t_8 = PyNumber_Negative(__pyx_v_sample_weight); if (unlikely(!__pyx_t_8)) __PYX_ERR(0, 305, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_8);
-    __pyx_t_7 = PyNumber_InPlaceAdd(__pyx_t_9, __pyx_t_8); if (unlikely(!__pyx_t_7)) __PYX_ERR(0, 303, __pyx_L1_error)
+    __pyx_t_7 = PyNumber_InPlaceAdd(__pyx_t_9, __pyx_t_8); if (unlikely(!__pyx_t_7)) __PYX_ERR(0, 305, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_7);
     __Pyx_DECREF(__pyx_t_9); __pyx_t_9 = 0;
     __Pyx_DECREF(__pyx_t_8); __pyx_t_8 = 0;
-    if (unlikely(PyObject_SetItem(__pyx_t_5, __pyx_t_10, __pyx_t_7) < 0)) __PYX_ERR(0, 303, __pyx_L1_error)
+    if (unlikely(PyObject_SetItem(__pyx_t_5, __pyx_t_10, __pyx_t_7) < 0)) __PYX_ERR(0, 305, __pyx_L1_error)
     __Pyx_DECREF(__pyx_t_7); __pyx_t_7 = 0;
     __Pyx_DECREF(__pyx_t_10); __pyx_t_10 = 0;
     __Pyx_DECREF(__pyx_t_5); __pyx_t_5 = 0;
   }
   __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;
 
-  /* "river/metrics/confusion.pyx":306
+  /* "river/metrics/confusion.pyx":308
  * 
  *         # Update auxiliary variables
  *         self.exact_match_cnt -= correction['IS_EQUAL']             # <<<<<<<<<<<<<<
  *         self.precision_sum -= correction['P_SUM']
  *         self.recall_sum -= correction['R_SUM']
  */
-  __pyx_t_1 = __Pyx_PyInt_From_int(__pyx_v_self->exact_match_cnt); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 306, __pyx_L1_error)
+  __pyx_t_1 = __Pyx_PyInt_From_int(__pyx_v_self->exact_match_cnt); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 308, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_1);
-  __pyx_t_5 = __Pyx_PyObject_Dict_GetItem(__pyx_v_correction, __pyx_n_u_IS_EQUAL); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 306, __pyx_L1_error)
+  __pyx_t_5 = __Pyx_PyObject_Dict_GetItem(__pyx_v_correction, __pyx_n_u_IS_EQUAL); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 308, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_5);
-  __pyx_t_10 = PyNumber_InPlaceSubtract(__pyx_t_1, __pyx_t_5); if (unlikely(!__pyx_t_10)) __PYX_ERR(0, 306, __pyx_L1_error)
+  __pyx_t_10 = PyNumber_InPlaceSubtract(__pyx_t_1, __pyx_t_5); if (unlikely(!__pyx_t_10)) __PYX_ERR(0, 308, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_10);
   __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;
   __Pyx_DECREF(__pyx_t_5); __pyx_t_5 = 0;
-  __pyx_t_4 = __Pyx_PyInt_As_int(__pyx_t_10); if (unlikely((__pyx_t_4 == (int)-1) && PyErr_Occurred())) __PYX_ERR(0, 306, __pyx_L1_error)
+  __pyx_t_4 = __Pyx_PyInt_As_int(__pyx_t_10); if (unlikely((__pyx_t_4 == (int)-1) && PyErr_Occurred())) __PYX_ERR(0, 308, __pyx_L1_error)
   __Pyx_DECREF(__pyx_t_10); __pyx_t_10 = 0;
   __pyx_v_self->exact_match_cnt = __pyx_t_4;
 
-  /* "river/metrics/confusion.pyx":307
+  /* "river/metrics/confusion.pyx":309
  *         # Update auxiliary variables
  *         self.exact_match_cnt -= correction['IS_EQUAL']
  *         self.precision_sum -= correction['P_SUM']             # <<<<<<<<<<<<<<
  *         self.recall_sum -= correction['R_SUM']
  *         self.jaccard_sum -= correction['J_SUM']
  */
-  __pyx_t_10 = PyFloat_FromDouble(__pyx_v_self->precision_sum); if (unlikely(!__pyx_t_10)) __PYX_ERR(0, 307, __pyx_L1_error)
+  __pyx_t_10 = PyFloat_FromDouble(__pyx_v_self->precision_sum); if (unlikely(!__pyx_t_10)) __PYX_ERR(0, 309, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_10);
-  __pyx_t_5 = __Pyx_PyObject_Dict_GetItem(__pyx_v_correction, __pyx_n_u_P_SUM); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 307, __pyx_L1_error)
+  __pyx_t_5 = __Pyx_PyObject_Dict_GetItem(__pyx_v_correction, __pyx_n_u_P_SUM); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 309, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_5);
-  __pyx_t_1 = PyNumber_InPlaceSubtract(__pyx_t_10, __pyx_t_5); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 307, __pyx_L1_error)
+  __pyx_t_1 = PyNumber_InPlaceSubtract(__pyx_t_10, __pyx_t_5); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 309, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_1);
   __Pyx_DECREF(__pyx_t_10); __pyx_t_10 = 0;
   __Pyx_DECREF(__pyx_t_5); __pyx_t_5 = 0;
-  __pyx_t_11 = __pyx_PyFloat_AsDouble(__pyx_t_1); if (unlikely((__pyx_t_11 == (double)-1) && PyErr_Occurred())) __PYX_ERR(0, 307, __pyx_L1_error)
+  __pyx_t_11 = __pyx_PyFloat_AsDouble(__pyx_t_1); if (unlikely((__pyx_t_11 == (double)-1) && PyErr_Occurred())) __PYX_ERR(0, 309, __pyx_L1_error)
   __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;
   __pyx_v_self->precision_sum = __pyx_t_11;
 
-  /* "river/metrics/confusion.pyx":308
+  /* "river/metrics/confusion.pyx":310
  *         self.exact_match_cnt -= correction['IS_EQUAL']
  *         self.precision_sum -= correction['P_SUM']
  *         self.recall_sum -= correction['R_SUM']             # <<<<<<<<<<<<<<
  *         self.jaccard_sum -= correction['J_SUM']
  * 
  */
-  __pyx_t_1 = PyFloat_FromDouble(__pyx_v_self->recall_sum); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 308, __pyx_L1_error)
+  __pyx_t_1 = PyFloat_FromDouble(__pyx_v_self->recall_sum); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 310, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_1);
-  __pyx_t_5 = __Pyx_PyObject_Dict_GetItem(__pyx_v_correction, __pyx_n_u_R_SUM); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 308, __pyx_L1_error)
+  __pyx_t_5 = __Pyx_PyObject_Dict_GetItem(__pyx_v_correction, __pyx_n_u_R_SUM); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 310, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_5);
-  __pyx_t_10 = PyNumber_InPlaceSubtract(__pyx_t_1, __pyx_t_5); if (unlikely(!__pyx_t_10)) __PYX_ERR(0, 308, __pyx_L1_error)
+  __pyx_t_10 = PyNumber_InPlaceSubtract(__pyx_t_1, __pyx_t_5); if (unlikely(!__pyx_t_10)) __PYX_ERR(0, 310, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_10);
   __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;
   __Pyx_DECREF(__pyx_t_5); __pyx_t_5 = 0;
-  __pyx_t_11 = __pyx_PyFloat_AsDouble(__pyx_t_10); if (unlikely((__pyx_t_11 == (double)-1) && PyErr_Occurred())) __PYX_ERR(0, 308, __pyx_L1_error)
+  __pyx_t_11 = __pyx_PyFloat_AsDouble(__pyx_t_10); if (unlikely((__pyx_t_11 == (double)-1) && PyErr_Occurred())) __PYX_ERR(0, 310, __pyx_L1_error)
   __Pyx_DECREF(__pyx_t_10); __pyx_t_10 = 0;
   __pyx_v_self->recall_sum = __pyx_t_11;
 
-  /* "river/metrics/confusion.pyx":309
+  /* "river/metrics/confusion.pyx":311
  *         self.precision_sum -= correction['P_SUM']
  *         self.recall_sum -= correction['R_SUM']
  *         self.jaccard_sum -= correction['J_SUM']             # <<<<<<<<<<<<<<
  * 
  *         return self
  */
-  __pyx_t_10 = PyFloat_FromDouble(__pyx_v_self->jaccard_sum); if (unlikely(!__pyx_t_10)) __PYX_ERR(0, 309, __pyx_L1_error)
+  __pyx_t_10 = PyFloat_FromDouble(__pyx_v_self->jaccard_sum); if (unlikely(!__pyx_t_10)) __PYX_ERR(0, 311, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_10);
-  __pyx_t_5 = __Pyx_PyObject_Dict_GetItem(__pyx_v_correction, __pyx_n_u_J_SUM); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 309, __pyx_L1_error)
+  __pyx_t_5 = __Pyx_PyObject_Dict_GetItem(__pyx_v_correction, __pyx_n_u_J_SUM); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 311, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_5);
-  __pyx_t_1 = PyNumber_InPlaceSubtract(__pyx_t_10, __pyx_t_5); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 309, __pyx_L1_error)
+  __pyx_t_1 = PyNumber_InPlaceSubtract(__pyx_t_10, __pyx_t_5); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 311, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_1);
   __Pyx_DECREF(__pyx_t_10); __pyx_t_10 = 0;
   __Pyx_DECREF(__pyx_t_5); __pyx_t_5 = 0;
-  __pyx_t_11 = __pyx_PyFloat_AsDouble(__pyx_t_1); if (unlikely((__pyx_t_11 == (double)-1) && PyErr_Occurred())) __PYX_ERR(0, 309, __pyx_L1_error)
+  __pyx_t_11 = __pyx_PyFloat_AsDouble(__pyx_t_1); if (unlikely((__pyx_t_11 == (double)-1) && PyErr_Occurred())) __PYX_ERR(0, 311, __pyx_L1_error)
   __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;
   __pyx_v_self->jaccard_sum = __pyx_t_11;
 
-  /* "river/metrics/confusion.pyx":311
+  /* "river/metrics/confusion.pyx":313
  *         self.jaccard_sum -= correction['J_SUM']
  * 
  *         return self             # <<<<<<<<<<<<<<
  * 
  *     def __getitem__(self, label):
  */
   __Pyx_XDECREF(__pyx_r);
   __Pyx_INCREF(((PyObject *)__pyx_v_self));
   __pyx_r = ((PyObject *)__pyx_v_self);
   goto __pyx_L0;
 
-  /* "river/metrics/confusion.pyx":298
+  /* "river/metrics/confusion.pyx":300
  * 
  * 
  *     def revert(self, y_true, y_pred, sample_weight=1., correction=None):             # <<<<<<<<<<<<<<
  *         self.n_samples -= 1
  *         # Revert is equal to subtracting so we pass the negative sample_weight
  */
 
@@ -8504,15 +8511,15 @@
   __pyx_L0:;
   __Pyx_XDECREF(__pyx_v_label);
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "river/metrics/confusion.pyx":313
+/* "river/metrics/confusion.pyx":315
  *         return self
  * 
  *     def __getitem__(self, label):             # <<<<<<<<<<<<<<
  *         if label in self.labels:
  *             label_idx = self._map_label(label, add_label=False)
  */
 
@@ -8538,81 +8545,81 @@
   PyObject *__pyx_t_3 = NULL;
   PyObject *__pyx_t_4 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("__getitem__", 0);
 
-  /* "river/metrics/confusion.pyx":314
+  /* "river/metrics/confusion.pyx":316
  * 
  *     def __getitem__(self, label):
  *         if label in self.labels:             # <<<<<<<<<<<<<<
  *             label_idx = self._map_label(label, add_label=False)
  *             return self.data[label_idx]
  */
   if (unlikely(__pyx_v_self->labels == Py_None)) {
     PyErr_SetString(PyExc_TypeError, "'NoneType' object is not iterable");
-    __PYX_ERR(0, 314, __pyx_L1_error)
+    __PYX_ERR(0, 316, __pyx_L1_error)
   }
-  __pyx_t_1 = (__Pyx_PySet_ContainsTF(__pyx_v_label, __pyx_v_self->labels, Py_EQ)); if (unlikely(__pyx_t_1 < 0)) __PYX_ERR(0, 314, __pyx_L1_error)
+  __pyx_t_1 = (__Pyx_PySet_ContainsTF(__pyx_v_label, __pyx_v_self->labels, Py_EQ)); if (unlikely(__pyx_t_1 < 0)) __PYX_ERR(0, 316, __pyx_L1_error)
   __pyx_t_2 = (__pyx_t_1 != 0);
   if (__pyx_t_2) {
 
-    /* "river/metrics/confusion.pyx":315
+    /* "river/metrics/confusion.pyx":317
  *     def __getitem__(self, label):
  *         if label in self.labels:
  *             label_idx = self._map_label(label, add_label=False)             # <<<<<<<<<<<<<<
  *             return self.data[label_idx]
  *         raise KeyError(f'Unknown label: {label}')
  */
     __pyx_v_label_idx = ((struct __pyx_vtabstruct_5river_7metrics_9confusion_MultiLabelConfusionMatrix *)__pyx_v_self->__pyx_vtab)->_map_label(__pyx_v_self, __pyx_v_label, 0);
 
-    /* "river/metrics/confusion.pyx":316
+    /* "river/metrics/confusion.pyx":318
  *         if label in self.labels:
  *             label_idx = self._map_label(label, add_label=False)
  *             return self.data[label_idx]             # <<<<<<<<<<<<<<
  *         raise KeyError(f'Unknown label: {label}')
  * 
  */
     __Pyx_XDECREF(__pyx_r);
-    __pyx_t_3 = __Pyx_GetItemInt(__pyx_v_self->data, __pyx_v_label_idx, int, 1, __Pyx_PyInt_From_int, 0, 1, 1); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 316, __pyx_L1_error)
+    __pyx_t_3 = __Pyx_GetItemInt(__pyx_v_self->data, __pyx_v_label_idx, int, 1, __Pyx_PyInt_From_int, 0, 1, 1); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 318, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_3);
     __pyx_r = __pyx_t_3;
     __pyx_t_3 = 0;
     goto __pyx_L0;
 
-    /* "river/metrics/confusion.pyx":314
+    /* "river/metrics/confusion.pyx":316
  * 
  *     def __getitem__(self, label):
  *         if label in self.labels:             # <<<<<<<<<<<<<<
  *             label_idx = self._map_label(label, add_label=False)
  *             return self.data[label_idx]
  */
   }
 
-  /* "river/metrics/confusion.pyx":317
+  /* "river/metrics/confusion.pyx":319
  *             label_idx = self._map_label(label, add_label=False)
  *             return self.data[label_idx]
  *         raise KeyError(f'Unknown label: {label}')             # <<<<<<<<<<<<<<
  * 
  *     cdef int _map_label(self, label, bint add_label):
  */
-  __pyx_t_3 = __Pyx_PyObject_FormatSimple(__pyx_v_label, __pyx_empty_unicode); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 317, __pyx_L1_error)
+  __pyx_t_3 = __Pyx_PyObject_FormatSimple(__pyx_v_label, __pyx_empty_unicode); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 319, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_3);
-  __pyx_t_4 = __Pyx_PyUnicode_Concat(__pyx_kp_u_Unknown_label, __pyx_t_3); if (unlikely(!__pyx_t_4)) __PYX_ERR(0, 317, __pyx_L1_error)
+  __pyx_t_4 = __Pyx_PyUnicode_Concat(__pyx_kp_u_Unknown_label, __pyx_t_3); if (unlikely(!__pyx_t_4)) __PYX_ERR(0, 319, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_4);
   __Pyx_DECREF(__pyx_t_3); __pyx_t_3 = 0;
-  __pyx_t_3 = __Pyx_PyObject_CallOneArg(__pyx_builtin_KeyError, __pyx_t_4); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 317, __pyx_L1_error)
+  __pyx_t_3 = __Pyx_PyObject_CallOneArg(__pyx_builtin_KeyError, __pyx_t_4); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 319, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_3);
   __Pyx_DECREF(__pyx_t_4); __pyx_t_4 = 0;
   __Pyx_Raise(__pyx_t_3, 0, 0, 0);
   __Pyx_DECREF(__pyx_t_3); __pyx_t_3 = 0;
-  __PYX_ERR(0, 317, __pyx_L1_error)
+  __PYX_ERR(0, 319, __pyx_L1_error)
 
-  /* "river/metrics/confusion.pyx":313
+  /* "river/metrics/confusion.pyx":315
  *         return self
  * 
  *     def __getitem__(self, label):             # <<<<<<<<<<<<<<
  *         if label in self.labels:
  *             label_idx = self._map_label(label, add_label=False)
  */
 
@@ -8624,15 +8631,15 @@
   __pyx_r = NULL;
   __pyx_L0:;
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "river/metrics/confusion.pyx":319
+/* "river/metrics/confusion.pyx":321
  *         raise KeyError(f'Unknown label: {label}')
  * 
  *     cdef int _map_label(self, label, bint add_label):             # <<<<<<<<<<<<<<
  *         try:
  *             label_key = self._label_dict[label]
  */
 
@@ -8651,15 +8658,15 @@
   PyObject *__pyx_t_9 = NULL;
   PyObject *__pyx_t_10 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("_map_label", 0);
 
-  /* "river/metrics/confusion.pyx":320
+  /* "river/metrics/confusion.pyx":322
  * 
  *     cdef int _map_label(self, label, bint add_label):
  *         try:             # <<<<<<<<<<<<<<
  *             label_key = self._label_dict[label]
  *         except KeyError:
  */
   {
@@ -8667,145 +8674,145 @@
     __Pyx_PyThreadState_assign
     __Pyx_ExceptionSave(&__pyx_t_1, &__pyx_t_2, &__pyx_t_3);
     __Pyx_XGOTREF(__pyx_t_1);
     __Pyx_XGOTREF(__pyx_t_2);
     __Pyx_XGOTREF(__pyx_t_3);
     /*try:*/ {
 
-      /* "river/metrics/confusion.pyx":321
+      /* "river/metrics/confusion.pyx":323
  *     cdef int _map_label(self, label, bint add_label):
  *         try:
  *             label_key = self._label_dict[label]             # <<<<<<<<<<<<<<
  *         except KeyError:
  *             if add_label:
  */
       if (unlikely(__pyx_v_self->_label_dict == Py_None)) {
         PyErr_SetString(PyExc_TypeError, "'NoneType' object is not subscriptable");
-        __PYX_ERR(0, 321, __pyx_L3_error)
+        __PYX_ERR(0, 323, __pyx_L3_error)
       }
-      __pyx_t_4 = __Pyx_PyDict_GetItem(__pyx_v_self->_label_dict, __pyx_v_label); if (unlikely(!__pyx_t_4)) __PYX_ERR(0, 321, __pyx_L3_error)
+      __pyx_t_4 = __Pyx_PyDict_GetItem(__pyx_v_self->_label_dict, __pyx_v_label); if (unlikely(!__pyx_t_4)) __PYX_ERR(0, 323, __pyx_L3_error)
       __Pyx_GOTREF(__pyx_t_4);
       __pyx_v_label_key = __pyx_t_4;
       __pyx_t_4 = 0;
 
-      /* "river/metrics/confusion.pyx":320
+      /* "river/metrics/confusion.pyx":322
  * 
  *     cdef int _map_label(self, label, bint add_label):
  *         try:             # <<<<<<<<<<<<<<
  *             label_key = self._label_dict[label]
  *         except KeyError:
  */
     }
     __Pyx_XDECREF(__pyx_t_1); __pyx_t_1 = 0;
     __Pyx_XDECREF(__pyx_t_2); __pyx_t_2 = 0;
     __Pyx_XDECREF(__pyx_t_3); __pyx_t_3 = 0;
     goto __pyx_L8_try_end;
     __pyx_L3_error:;
     __Pyx_XDECREF(__pyx_t_4); __pyx_t_4 = 0;
 
-    /* "river/metrics/confusion.pyx":322
+    /* "river/metrics/confusion.pyx":324
  *         try:
  *             label_key = self._label_dict[label]
  *         except KeyError:             # <<<<<<<<<<<<<<
  *             if add_label:
  *                 self._add_label(label)
  */
     __pyx_t_5 = __Pyx_PyErr_ExceptionMatches(__pyx_builtin_KeyError);
     if (__pyx_t_5) {
       __Pyx_AddTraceback("river.metrics.confusion.MultiLabelConfusionMatrix._map_label", __pyx_clineno, __pyx_lineno, __pyx_filename);
-      if (__Pyx_GetException(&__pyx_t_4, &__pyx_t_6, &__pyx_t_7) < 0) __PYX_ERR(0, 322, __pyx_L5_except_error)
+      if (__Pyx_GetException(&__pyx_t_4, &__pyx_t_6, &__pyx_t_7) < 0) __PYX_ERR(0, 324, __pyx_L5_except_error)
       __Pyx_GOTREF(__pyx_t_4);
       __Pyx_GOTREF(__pyx_t_6);
       __Pyx_GOTREF(__pyx_t_7);
 
-      /* "river/metrics/confusion.pyx":323
+      /* "river/metrics/confusion.pyx":325
  *             label_key = self._label_dict[label]
  *         except KeyError:
  *             if add_label:             # <<<<<<<<<<<<<<
  *                 self._add_label(label)
  *                 label_key = self._label_dict[label]
  */
       __pyx_t_8 = (__pyx_v_add_label != 0);
       if (likely(__pyx_t_8)) {
 
-        /* "river/metrics/confusion.pyx":324
+        /* "river/metrics/confusion.pyx":326
  *         except KeyError:
  *             if add_label:
  *                 self._add_label(label)             # <<<<<<<<<<<<<<
  *                 label_key = self._label_dict[label]
  *             else:
  */
         ((struct __pyx_vtabstruct_5river_7metrics_9confusion_MultiLabelConfusionMatrix *)__pyx_v_self->__pyx_vtab)->_add_label(__pyx_v_self, __pyx_v_label);
 
-        /* "river/metrics/confusion.pyx":325
+        /* "river/metrics/confusion.pyx":327
  *             if add_label:
  *                 self._add_label(label)
  *                 label_key = self._label_dict[label]             # <<<<<<<<<<<<<<
  *             else:
  *                 label_key = -1
  */
         if (unlikely(__pyx_v_self->_label_dict == Py_None)) {
           PyErr_SetString(PyExc_TypeError, "'NoneType' object is not subscriptable");
-          __PYX_ERR(0, 325, __pyx_L5_except_error)
+          __PYX_ERR(0, 327, __pyx_L5_except_error)
         }
-        __pyx_t_9 = __Pyx_PyDict_GetItem(__pyx_v_self->_label_dict, __pyx_v_label); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 325, __pyx_L5_except_error)
+        __pyx_t_9 = __Pyx_PyDict_GetItem(__pyx_v_self->_label_dict, __pyx_v_label); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 327, __pyx_L5_except_error)
         __Pyx_GOTREF(__pyx_t_9);
         __Pyx_XDECREF_SET(__pyx_v_label_key, __pyx_t_9);
         __pyx_t_9 = 0;
 
-        /* "river/metrics/confusion.pyx":323
+        /* "river/metrics/confusion.pyx":325
  *             label_key = self._label_dict[label]
  *         except KeyError:
  *             if add_label:             # <<<<<<<<<<<<<<
  *                 self._add_label(label)
  *                 label_key = self._label_dict[label]
  */
         goto __pyx_L11;
       }
 
-      /* "river/metrics/confusion.pyx":327
+      /* "river/metrics/confusion.pyx":329
  *                 label_key = self._label_dict[label]
  *             else:
  *                 label_key = -1             # <<<<<<<<<<<<<<
  *                 raise KeyError(f'Unknown label: {label}')
  *         return label_key
  */
       /*else*/ {
         __Pyx_INCREF(__pyx_int_neg_1);
         __Pyx_XDECREF_SET(__pyx_v_label_key, __pyx_int_neg_1);
 
-        /* "river/metrics/confusion.pyx":328
+        /* "river/metrics/confusion.pyx":330
  *             else:
  *                 label_key = -1
  *                 raise KeyError(f'Unknown label: {label}')             # <<<<<<<<<<<<<<
  *         return label_key
  * 
  */
-        __pyx_t_9 = __Pyx_PyObject_FormatSimple(__pyx_v_label, __pyx_empty_unicode); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 328, __pyx_L5_except_error)
+        __pyx_t_9 = __Pyx_PyObject_FormatSimple(__pyx_v_label, __pyx_empty_unicode); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 330, __pyx_L5_except_error)
         __Pyx_GOTREF(__pyx_t_9);
-        __pyx_t_10 = __Pyx_PyUnicode_Concat(__pyx_kp_u_Unknown_label, __pyx_t_9); if (unlikely(!__pyx_t_10)) __PYX_ERR(0, 328, __pyx_L5_except_error)
+        __pyx_t_10 = __Pyx_PyUnicode_Concat(__pyx_kp_u_Unknown_label, __pyx_t_9); if (unlikely(!__pyx_t_10)) __PYX_ERR(0, 330, __pyx_L5_except_error)
         __Pyx_GOTREF(__pyx_t_10);
         __Pyx_DECREF(__pyx_t_9); __pyx_t_9 = 0;
-        __pyx_t_9 = __Pyx_PyObject_CallOneArg(__pyx_builtin_KeyError, __pyx_t_10); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 328, __pyx_L5_except_error)
+        __pyx_t_9 = __Pyx_PyObject_CallOneArg(__pyx_builtin_KeyError, __pyx_t_10); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 330, __pyx_L5_except_error)
         __Pyx_GOTREF(__pyx_t_9);
         __Pyx_DECREF(__pyx_t_10); __pyx_t_10 = 0;
         __Pyx_Raise(__pyx_t_9, 0, 0, 0);
         __Pyx_DECREF(__pyx_t_9); __pyx_t_9 = 0;
-        __PYX_ERR(0, 328, __pyx_L5_except_error)
+        __PYX_ERR(0, 330, __pyx_L5_except_error)
       }
       __pyx_L11:;
       __Pyx_XDECREF(__pyx_t_4); __pyx_t_4 = 0;
       __Pyx_XDECREF(__pyx_t_6); __pyx_t_6 = 0;
       __Pyx_XDECREF(__pyx_t_7); __pyx_t_7 = 0;
       goto __pyx_L4_exception_handled;
     }
     goto __pyx_L5_except_error;
     __pyx_L5_except_error:;
 
-    /* "river/metrics/confusion.pyx":320
+    /* "river/metrics/confusion.pyx":322
  * 
  *     cdef int _map_label(self, label, bint add_label):
  *         try:             # <<<<<<<<<<<<<<
  *             label_key = self._label_dict[label]
  *         except KeyError:
  */
     __Pyx_XGIVEREF(__pyx_t_1);
@@ -8817,26 +8824,26 @@
     __Pyx_XGIVEREF(__pyx_t_1);
     __Pyx_XGIVEREF(__pyx_t_2);
     __Pyx_XGIVEREF(__pyx_t_3);
     __Pyx_ExceptionReset(__pyx_t_1, __pyx_t_2, __pyx_t_3);
     __pyx_L8_try_end:;
   }
 
-  /* "river/metrics/confusion.pyx":329
+  /* "river/metrics/confusion.pyx":331
  *                 label_key = -1
  *                 raise KeyError(f'Unknown label: {label}')
  *         return label_key             # <<<<<<<<<<<<<<
  * 
  *     cdef void _add_label(self, label):
  */
-  __pyx_t_5 = __Pyx_PyInt_As_int(__pyx_v_label_key); if (unlikely((__pyx_t_5 == (int)-1) && PyErr_Occurred())) __PYX_ERR(0, 329, __pyx_L1_error)
+  __pyx_t_5 = __Pyx_PyInt_As_int(__pyx_v_label_key); if (unlikely((__pyx_t_5 == (int)-1) && PyErr_Occurred())) __PYX_ERR(0, 331, __pyx_L1_error)
   __pyx_r = __pyx_t_5;
   goto __pyx_L0;
 
-  /* "river/metrics/confusion.pyx":319
+  /* "river/metrics/confusion.pyx":321
  *         raise KeyError(f'Unknown label: {label}')
  * 
  *     cdef int _map_label(self, label, bint add_label):             # <<<<<<<<<<<<<<
  *         try:
  *             label_key = self._label_dict[label]
  */
 
@@ -8851,15 +8858,15 @@
   __pyx_r = 0;
   __pyx_L0:;
   __Pyx_XDECREF(__pyx_v_label_key);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "river/metrics/confusion.pyx":331
+/* "river/metrics/confusion.pyx":333
  *         return label_key
  * 
  *     cdef void _add_label(self, label):             # <<<<<<<<<<<<<<
  *         self._label_dict[label] = self._label_idx_cnt
  *         if self._label_idx_cnt > self.data.shape[0] - 1:
  */
 
@@ -8872,112 +8879,112 @@
   int __pyx_t_5;
   Py_ssize_t __pyx_t_6;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("_add_label", 0);
 
-  /* "river/metrics/confusion.pyx":332
+  /* "river/metrics/confusion.pyx":334
  * 
  *     cdef void _add_label(self, label):
  *         self._label_dict[label] = self._label_idx_cnt             # <<<<<<<<<<<<<<
  *         if self._label_idx_cnt > self.data.shape[0] - 1:
  *             self._reshape()
  */
-  __pyx_t_1 = __Pyx_PyInt_From_int(__pyx_v_self->_label_idx_cnt); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 332, __pyx_L1_error)
+  __pyx_t_1 = __Pyx_PyInt_From_int(__pyx_v_self->_label_idx_cnt); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 334, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_1);
   if (unlikely(__pyx_v_self->_label_dict == Py_None)) {
     PyErr_SetString(PyExc_TypeError, "'NoneType' object is not subscriptable");
-    __PYX_ERR(0, 332, __pyx_L1_error)
+    __PYX_ERR(0, 334, __pyx_L1_error)
   }
-  if (unlikely(PyDict_SetItem(__pyx_v_self->_label_dict, __pyx_v_label, __pyx_t_1) < 0)) __PYX_ERR(0, 332, __pyx_L1_error)
+  if (unlikely(PyDict_SetItem(__pyx_v_self->_label_dict, __pyx_v_label, __pyx_t_1) < 0)) __PYX_ERR(0, 334, __pyx_L1_error)
   __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;
 
-  /* "river/metrics/confusion.pyx":333
+  /* "river/metrics/confusion.pyx":335
  *     cdef void _add_label(self, label):
  *         self._label_dict[label] = self._label_idx_cnt
  *         if self._label_idx_cnt > self.data.shape[0] - 1:             # <<<<<<<<<<<<<<
  *             self._reshape()
  *         self._label_idx_cnt += 1
  */
-  __pyx_t_1 = __Pyx_PyInt_From_int(__pyx_v_self->_label_idx_cnt); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 333, __pyx_L1_error)
+  __pyx_t_1 = __Pyx_PyInt_From_int(__pyx_v_self->_label_idx_cnt); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 335, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_1);
-  __pyx_t_2 = __Pyx_PyObject_GetAttrStr(__pyx_v_self->data, __pyx_n_s_shape); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 333, __pyx_L1_error)
+  __pyx_t_2 = __Pyx_PyObject_GetAttrStr(__pyx_v_self->data, __pyx_n_s_shape); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 335, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_2);
-  __pyx_t_3 = __Pyx_GetItemInt(__pyx_t_2, 0, long, 1, __Pyx_PyInt_From_long, 0, 0, 1); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 333, __pyx_L1_error)
+  __pyx_t_3 = __Pyx_GetItemInt(__pyx_t_2, 0, long, 1, __Pyx_PyInt_From_long, 0, 0, 1); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 335, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_3);
   __Pyx_DECREF(__pyx_t_2); __pyx_t_2 = 0;
-  __pyx_t_2 = __Pyx_PyInt_SubtractObjC(__pyx_t_3, __pyx_int_1, 1, 0, 0); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 333, __pyx_L1_error)
+  __pyx_t_2 = __Pyx_PyInt_SubtractObjC(__pyx_t_3, __pyx_int_1, 1, 0, 0); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 335, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_2);
   __Pyx_DECREF(__pyx_t_3); __pyx_t_3 = 0;
-  __pyx_t_3 = PyObject_RichCompare(__pyx_t_1, __pyx_t_2, Py_GT); __Pyx_XGOTREF(__pyx_t_3); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 333, __pyx_L1_error)
+  __pyx_t_3 = PyObject_RichCompare(__pyx_t_1, __pyx_t_2, Py_GT); __Pyx_XGOTREF(__pyx_t_3); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 335, __pyx_L1_error)
   __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;
   __Pyx_DECREF(__pyx_t_2); __pyx_t_2 = 0;
-  __pyx_t_4 = __Pyx_PyObject_IsTrue(__pyx_t_3); if (unlikely(__pyx_t_4 < 0)) __PYX_ERR(0, 333, __pyx_L1_error)
+  __pyx_t_4 = __Pyx_PyObject_IsTrue(__pyx_t_3); if (unlikely(__pyx_t_4 < 0)) __PYX_ERR(0, 335, __pyx_L1_error)
   __Pyx_DECREF(__pyx_t_3); __pyx_t_3 = 0;
   if (__pyx_t_4) {
 
-    /* "river/metrics/confusion.pyx":334
+    /* "river/metrics/confusion.pyx":336
  *         self._label_dict[label] = self._label_idx_cnt
  *         if self._label_idx_cnt > self.data.shape[0] - 1:
  *             self._reshape()             # <<<<<<<<<<<<<<
  *         self._label_idx_cnt += 1
  *         self.labels.add(label)
  */
     ((struct __pyx_vtabstruct_5river_7metrics_9confusion_MultiLabelConfusionMatrix *)__pyx_v_self->__pyx_vtab)->_reshape(__pyx_v_self);
 
-    /* "river/metrics/confusion.pyx":333
+    /* "river/metrics/confusion.pyx":335
  *     cdef void _add_label(self, label):
  *         self._label_dict[label] = self._label_idx_cnt
  *         if self._label_idx_cnt > self.data.shape[0] - 1:             # <<<<<<<<<<<<<<
  *             self._reshape()
  *         self._label_idx_cnt += 1
  */
   }
 
-  /* "river/metrics/confusion.pyx":335
+  /* "river/metrics/confusion.pyx":337
  *         if self._label_idx_cnt > self.data.shape[0] - 1:
  *             self._reshape()
  *         self._label_idx_cnt += 1             # <<<<<<<<<<<<<<
  *         self.labels.add(label)
  *         self.n_labels = len(self.labels)
  */
   __pyx_v_self->_label_idx_cnt = (__pyx_v_self->_label_idx_cnt + 1);
 
-  /* "river/metrics/confusion.pyx":336
+  /* "river/metrics/confusion.pyx":338
  *             self._reshape()
  *         self._label_idx_cnt += 1
  *         self.labels.add(label)             # <<<<<<<<<<<<<<
  *         self.n_labels = len(self.labels)
  * 
  */
   if (unlikely(__pyx_v_self->labels == Py_None)) {
     PyErr_Format(PyExc_AttributeError, "'NoneType' object has no attribute '%.30s'", "add");
-    __PYX_ERR(0, 336, __pyx_L1_error)
+    __PYX_ERR(0, 338, __pyx_L1_error)
   }
-  __pyx_t_5 = PySet_Add(__pyx_v_self->labels, __pyx_v_label); if (unlikely(__pyx_t_5 == ((int)-1))) __PYX_ERR(0, 336, __pyx_L1_error)
+  __pyx_t_5 = PySet_Add(__pyx_v_self->labels, __pyx_v_label); if (unlikely(__pyx_t_5 == ((int)-1))) __PYX_ERR(0, 338, __pyx_L1_error)
 
-  /* "river/metrics/confusion.pyx":337
+  /* "river/metrics/confusion.pyx":339
  *         self._label_idx_cnt += 1
  *         self.labels.add(label)
  *         self.n_labels = len(self.labels)             # <<<<<<<<<<<<<<
  * 
  *     cdef void _reshape(self):
  */
   __pyx_t_3 = __pyx_v_self->labels;
   __Pyx_INCREF(__pyx_t_3);
   if (unlikely(__pyx_t_3 == Py_None)) {
     PyErr_SetString(PyExc_TypeError, "object of type 'NoneType' has no len()");
-    __PYX_ERR(0, 337, __pyx_L1_error)
+    __PYX_ERR(0, 339, __pyx_L1_error)
   }
-  __pyx_t_6 = PySet_GET_SIZE(__pyx_t_3); if (unlikely(__pyx_t_6 == ((Py_ssize_t)-1))) __PYX_ERR(0, 337, __pyx_L1_error)
+  __pyx_t_6 = PySet_GET_SIZE(__pyx_t_3); if (unlikely(__pyx_t_6 == ((Py_ssize_t)-1))) __PYX_ERR(0, 339, __pyx_L1_error)
   __Pyx_DECREF(__pyx_t_3); __pyx_t_3 = 0;
   __pyx_v_self->n_labels = __pyx_t_6;
 
-  /* "river/metrics/confusion.pyx":331
+  /* "river/metrics/confusion.pyx":333
  *         return label_key
  * 
  *     cdef void _add_label(self, label):             # <<<<<<<<<<<<<<
  *         self._label_dict[label] = self._label_idx_cnt
  *         if self._label_idx_cnt > self.data.shape[0] - 1:
  */
 
@@ -8988,15 +8995,15 @@
   __Pyx_XDECREF(__pyx_t_2);
   __Pyx_XDECREF(__pyx_t_3);
   __Pyx_WriteUnraisable("river.metrics.confusion.MultiLabelConfusionMatrix._add_label", __pyx_clineno, __pyx_lineno, __pyx_filename, 1, 0);
   __pyx_L0:;
   __Pyx_RefNannyFinishContext();
 }
 
-/* "river/metrics/confusion.pyx":339
+/* "river/metrics/confusion.pyx":341
  *         self.n_labels = len(self.labels)
  * 
  *     cdef void _reshape(self):             # <<<<<<<<<<<<<<
  *         self.data = np.vstack((self.data, np.zeros((1, 2, 2))))
  * 
  */
 
@@ -9008,47 +9015,47 @@
   PyObject *__pyx_t_4 = NULL;
   PyObject *__pyx_t_5 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("_reshape", 0);
 
-  /* "river/metrics/confusion.pyx":340
+  /* "river/metrics/confusion.pyx":342
  * 
  *     cdef void _reshape(self):
  *         self.data = np.vstack((self.data, np.zeros((1, 2, 2))))             # <<<<<<<<<<<<<<
  * 
  *     @property
  */
-  __Pyx_GetModuleGlobalName(__pyx_t_2, __pyx_n_s_np); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 340, __pyx_L1_error)
+  __Pyx_GetModuleGlobalName(__pyx_t_2, __pyx_n_s_np); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 342, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_2);
-  __pyx_t_3 = __Pyx_PyObject_GetAttrStr(__pyx_t_2, __pyx_n_s_vstack); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 340, __pyx_L1_error)
+  __pyx_t_3 = __Pyx_PyObject_GetAttrStr(__pyx_t_2, __pyx_n_s_vstack); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 342, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_3);
   __Pyx_DECREF(__pyx_t_2); __pyx_t_2 = 0;
-  __Pyx_GetModuleGlobalName(__pyx_t_4, __pyx_n_s_np); if (unlikely(!__pyx_t_4)) __PYX_ERR(0, 340, __pyx_L1_error)
+  __Pyx_GetModuleGlobalName(__pyx_t_4, __pyx_n_s_np); if (unlikely(!__pyx_t_4)) __PYX_ERR(0, 342, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_4);
-  __pyx_t_5 = __Pyx_PyObject_GetAttrStr(__pyx_t_4, __pyx_n_s_zeros); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 340, __pyx_L1_error)
+  __pyx_t_5 = __Pyx_PyObject_GetAttrStr(__pyx_t_4, __pyx_n_s_zeros); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 342, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_5);
   __Pyx_DECREF(__pyx_t_4); __pyx_t_4 = 0;
   __pyx_t_4 = NULL;
   if (CYTHON_UNPACK_METHODS && unlikely(PyMethod_Check(__pyx_t_5))) {
     __pyx_t_4 = PyMethod_GET_SELF(__pyx_t_5);
     if (likely(__pyx_t_4)) {
       PyObject* function = PyMethod_GET_FUNCTION(__pyx_t_5);
       __Pyx_INCREF(__pyx_t_4);
       __Pyx_INCREF(function);
       __Pyx_DECREF_SET(__pyx_t_5, function);
     }
   }
   __pyx_t_2 = (__pyx_t_4) ? __Pyx_PyObject_Call2Args(__pyx_t_5, __pyx_t_4, __pyx_tuple__5) : __Pyx_PyObject_CallOneArg(__pyx_t_5, __pyx_tuple__5);
   __Pyx_XDECREF(__pyx_t_4); __pyx_t_4 = 0;
-  if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 340, __pyx_L1_error)
+  if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 342, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_2);
   __Pyx_DECREF(__pyx_t_5); __pyx_t_5 = 0;
-  __pyx_t_5 = PyTuple_New(2); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 340, __pyx_L1_error)
+  __pyx_t_5 = PyTuple_New(2); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 342, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_5);
   __Pyx_INCREF(__pyx_v_self->data);
   __Pyx_GIVEREF(__pyx_v_self->data);
   PyTuple_SET_ITEM(__pyx_t_5, 0, __pyx_v_self->data);
   __Pyx_GIVEREF(__pyx_t_2);
   PyTuple_SET_ITEM(__pyx_t_5, 1, __pyx_t_2);
   __pyx_t_2 = 0;
@@ -9061,24 +9068,24 @@
       __Pyx_INCREF(function);
       __Pyx_DECREF_SET(__pyx_t_3, function);
     }
   }
   __pyx_t_1 = (__pyx_t_2) ? __Pyx_PyObject_Call2Args(__pyx_t_3, __pyx_t_2, __pyx_t_5) : __Pyx_PyObject_CallOneArg(__pyx_t_3, __pyx_t_5);
   __Pyx_XDECREF(__pyx_t_2); __pyx_t_2 = 0;
   __Pyx_DECREF(__pyx_t_5); __pyx_t_5 = 0;
-  if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 340, __pyx_L1_error)
+  if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 342, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_1);
   __Pyx_DECREF(__pyx_t_3); __pyx_t_3 = 0;
   __Pyx_GIVEREF(__pyx_t_1);
   __Pyx_GOTREF(__pyx_v_self->data);
   __Pyx_DECREF(__pyx_v_self->data);
   __pyx_v_self->data = __pyx_t_1;
   __pyx_t_1 = 0;
 
-  /* "river/metrics/confusion.pyx":339
+  /* "river/metrics/confusion.pyx":341
  *         self.n_labels = len(self.labels)
  * 
  *     cdef void _reshape(self):             # <<<<<<<<<<<<<<
  *         self.data = np.vstack((self.data, np.zeros((1, 2, 2))))
  * 
  */
 
@@ -9091,15 +9098,15 @@
   __Pyx_XDECREF(__pyx_t_4);
   __Pyx_XDECREF(__pyx_t_5);
   __Pyx_WriteUnraisable("river.metrics.confusion.MultiLabelConfusionMatrix._reshape", __pyx_clineno, __pyx_lineno, __pyx_filename, 1, 0);
   __pyx_L0:;
   __Pyx_RefNannyFinishContext();
 }
 
-/* "river/metrics/confusion.pyx":343
+/* "river/metrics/confusion.pyx":345
  * 
  *     @property
  *     def shape(self):             # <<<<<<<<<<<<<<
  *         return self.data.shape
  * 
  */
 
@@ -9121,29 +9128,29 @@
   __Pyx_RefNannyDeclarations
   PyObject *__pyx_t_1 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("__get__", 0);
 
-  /* "river/metrics/confusion.pyx":344
+  /* "river/metrics/confusion.pyx":346
  *     @property
  *     def shape(self):
  *         return self.data.shape             # <<<<<<<<<<<<<<
  * 
  *     def reset(self):
  */
   __Pyx_XDECREF(__pyx_r);
-  __pyx_t_1 = __Pyx_PyObject_GetAttrStr(__pyx_v_self->data, __pyx_n_s_shape); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 344, __pyx_L1_error)
+  __pyx_t_1 = __Pyx_PyObject_GetAttrStr(__pyx_v_self->data, __pyx_n_s_shape); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 346, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_1);
   __pyx_r = __pyx_t_1;
   __pyx_t_1 = 0;
   goto __pyx_L0;
 
-  /* "river/metrics/confusion.pyx":343
+  /* "river/metrics/confusion.pyx":345
  * 
  *     @property
  *     def shape(self):             # <<<<<<<<<<<<<<
  *         return self.data.shape
  * 
  */
 
@@ -9154,15 +9161,15 @@
   __pyx_r = NULL;
   __pyx_L0:;
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "river/metrics/confusion.pyx":346
+/* "river/metrics/confusion.pyx":348
  *         return self.data.shape
  * 
  *     def reset(self):             # <<<<<<<<<<<<<<
  *         self.__init__(labels=self._init_labels)
  * 
  */
 
@@ -9188,33 +9195,33 @@
   PyObject *__pyx_t_2 = NULL;
   PyObject *__pyx_t_3 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("reset", 0);
 
-  /* "river/metrics/confusion.pyx":347
+  /* "river/metrics/confusion.pyx":349
  * 
  *     def reset(self):
  *         self.__init__(labels=self._init_labels)             # <<<<<<<<<<<<<<
  * 
  *     def __str__(self):
  */
-  __pyx_t_1 = __Pyx_PyObject_GetAttrStr(((PyObject *)__pyx_v_self), __pyx_n_s_init); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 347, __pyx_L1_error)
+  __pyx_t_1 = __Pyx_PyObject_GetAttrStr(((PyObject *)__pyx_v_self), __pyx_n_s_init); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 349, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_1);
-  __pyx_t_2 = __Pyx_PyDict_NewPresized(1); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 347, __pyx_L1_error)
+  __pyx_t_2 = __Pyx_PyDict_NewPresized(1); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 349, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_2);
-  if (PyDict_SetItem(__pyx_t_2, __pyx_n_s_labels, __pyx_v_self->_init_labels) < 0) __PYX_ERR(0, 347, __pyx_L1_error)
-  __pyx_t_3 = __Pyx_PyObject_Call(__pyx_t_1, __pyx_empty_tuple, __pyx_t_2); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 347, __pyx_L1_error)
+  if (PyDict_SetItem(__pyx_t_2, __pyx_n_s_labels, __pyx_v_self->_init_labels) < 0) __PYX_ERR(0, 349, __pyx_L1_error)
+  __pyx_t_3 = __Pyx_PyObject_Call(__pyx_t_1, __pyx_empty_tuple, __pyx_t_2); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 349, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_3);
   __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;
   __Pyx_DECREF(__pyx_t_2); __pyx_t_2 = 0;
   __Pyx_DECREF(__pyx_t_3); __pyx_t_3 = 0;
 
-  /* "river/metrics/confusion.pyx":346
+  /* "river/metrics/confusion.pyx":348
  *         return self.data.shape
  * 
  *     def reset(self):             # <<<<<<<<<<<<<<
  *         self.__init__(labels=self._init_labels)
  * 
  */
 
@@ -9229,15 +9236,15 @@
   __pyx_r = NULL;
   __pyx_L0:;
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "river/metrics/confusion.pyx":349
+/* "river/metrics/confusion.pyx":351
  *         self.__init__(labels=self._init_labels)
  * 
  *     def __str__(self):             # <<<<<<<<<<<<<<
  *         return self.data.__str__()
  * 
  */
 
@@ -9261,44 +9268,44 @@
   PyObject *__pyx_t_2 = NULL;
   PyObject *__pyx_t_3 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("__str__", 0);
 
-  /* "river/metrics/confusion.pyx":350
+  /* "river/metrics/confusion.pyx":352
  * 
  *     def __str__(self):
  *         return self.data.__str__()             # <<<<<<<<<<<<<<
  * 
  *     def __repr__(self):
  */
   __Pyx_XDECREF(__pyx_r);
-  __pyx_t_2 = __Pyx_PyObject_GetAttrStr(__pyx_v_self->data, __pyx_n_s_str); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 350, __pyx_L1_error)
+  __pyx_t_2 = __Pyx_PyObject_GetAttrStr(__pyx_v_self->data, __pyx_n_s_str); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 352, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_2);
   __pyx_t_3 = NULL;
   if (CYTHON_UNPACK_METHODS && likely(PyMethod_Check(__pyx_t_2))) {
     __pyx_t_3 = PyMethod_GET_SELF(__pyx_t_2);
     if (likely(__pyx_t_3)) {
       PyObject* function = PyMethod_GET_FUNCTION(__pyx_t_2);
       __Pyx_INCREF(__pyx_t_3);
       __Pyx_INCREF(function);
       __Pyx_DECREF_SET(__pyx_t_2, function);
     }
   }
   __pyx_t_1 = (__pyx_t_3) ? __Pyx_PyObject_CallOneArg(__pyx_t_2, __pyx_t_3) : __Pyx_PyObject_CallNoArg(__pyx_t_2);
   __Pyx_XDECREF(__pyx_t_3); __pyx_t_3 = 0;
-  if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 350, __pyx_L1_error)
+  if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 352, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_1);
   __Pyx_DECREF(__pyx_t_2); __pyx_t_2 = 0;
   __pyx_r = __pyx_t_1;
   __pyx_t_1 = 0;
   goto __pyx_L0;
 
-  /* "river/metrics/confusion.pyx":349
+  /* "river/metrics/confusion.pyx":351
  *         self.__init__(labels=self._init_labels)
  * 
  *     def __str__(self):             # <<<<<<<<<<<<<<
  *         return self.data.__str__()
  * 
  */
 
@@ -9311,15 +9318,15 @@
   __pyx_r = NULL;
   __pyx_L0:;
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "river/metrics/confusion.pyx":352
+/* "river/metrics/confusion.pyx":354
  *         return self.data.__str__()
  * 
  *     def __repr__(self):             # <<<<<<<<<<<<<<
  *         # The labels are sorted alphabetically for reproducibility reasons
  *         labels = sorted(self.labels)
  */
 
@@ -9333,15 +9340,15 @@
 
   /* function exit code */
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 static PyObject *__pyx_gb_5river_7metrics_9confusion_25MultiLabelConfusionMatrix_8__repr___2generator6(__pyx_CoroutineObject *__pyx_generator, CYTHON_UNUSED PyThreadState *__pyx_tstate, PyObject *__pyx_sent_value); /* proto */
 
-/* "river/metrics/confusion.pyx":360
+/* "river/metrics/confusion.pyx":362
  * 
  *         # Determine the required width of each column in the table
  *         largest_label_len = max(len(str(label)) for label in labels)             # <<<<<<<<<<<<<<
  *         largest_value_len = len(str(self.data[:].max()))
  *         width = max(5, largest_label_len, largest_value_len) + 2   # Min value is 5=len('label')
  */
 
@@ -9353,23 +9360,23 @@
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("genexpr", 0);
   __pyx_cur_scope = (struct __pyx_obj_5river_7metrics_9confusion___pyx_scope_struct_9_genexpr *)__pyx_tp_new_5river_7metrics_9confusion___pyx_scope_struct_9_genexpr(__pyx_ptype_5river_7metrics_9confusion___pyx_scope_struct_9_genexpr, __pyx_empty_tuple, NULL);
   if (unlikely(!__pyx_cur_scope)) {
     __pyx_cur_scope = ((struct __pyx_obj_5river_7metrics_9confusion___pyx_scope_struct_9_genexpr *)Py_None);
     __Pyx_INCREF(Py_None);
-    __PYX_ERR(0, 360, __pyx_L1_error)
+    __PYX_ERR(0, 362, __pyx_L1_error)
   } else {
     __Pyx_GOTREF(__pyx_cur_scope);
   }
   __pyx_cur_scope->__pyx_outer_scope = (struct __pyx_obj_5river_7metrics_9confusion___pyx_scope_struct_8___repr__ *) __pyx_self;
   __Pyx_INCREF(((PyObject *)__pyx_cur_scope->__pyx_outer_scope));
   __Pyx_GIVEREF(__pyx_cur_scope->__pyx_outer_scope);
   {
-    __pyx_CoroutineObject *gen = __Pyx_Generator_New((__pyx_coroutine_body_t) __pyx_gb_5river_7metrics_9confusion_25MultiLabelConfusionMatrix_8__repr___2generator6, NULL, (PyObject *) __pyx_cur_scope, __pyx_n_s_genexpr, __pyx_n_s_repr___locals_genexpr, __pyx_n_s_river_metrics_confusion); if (unlikely(!gen)) __PYX_ERR(0, 360, __pyx_L1_error)
+    __pyx_CoroutineObject *gen = __Pyx_Generator_New((__pyx_coroutine_body_t) __pyx_gb_5river_7metrics_9confusion_25MultiLabelConfusionMatrix_8__repr___2generator6, NULL, (PyObject *) __pyx_cur_scope, __pyx_n_s_genexpr, __pyx_n_s_repr___locals_genexpr, __pyx_n_s_river_metrics_confusion); if (unlikely(!gen)) __PYX_ERR(0, 362, __pyx_L1_error)
     __Pyx_DECREF(__pyx_cur_scope);
     __Pyx_RefNannyFinishContext();
     return (PyObject *) gen;
   }
 
   /* function exit code */
   __pyx_L1_error:;
@@ -9398,38 +9405,38 @@
     case 0: goto __pyx_L3_first_run;
     case 1: goto __pyx_L6_resume_from_yield;
     default: /* CPython raises the right error here */
     __Pyx_RefNannyFinishContext();
     return NULL;
   }
   __pyx_L3_first_run:;
-  if (unlikely(!__pyx_sent_value)) __PYX_ERR(0, 360, __pyx_L1_error)
-  if (unlikely(!__pyx_cur_scope->__pyx_outer_scope->__pyx_v_labels)) { __Pyx_RaiseClosureNameError("labels"); __PYX_ERR(0, 360, __pyx_L1_error) }
+  if (unlikely(!__pyx_sent_value)) __PYX_ERR(0, 362, __pyx_L1_error)
+  if (unlikely(!__pyx_cur_scope->__pyx_outer_scope->__pyx_v_labels)) { __Pyx_RaiseClosureNameError("labels"); __PYX_ERR(0, 362, __pyx_L1_error) }
   if (unlikely(__pyx_cur_scope->__pyx_outer_scope->__pyx_v_labels == Py_None)) {
     PyErr_SetString(PyExc_TypeError, "'NoneType' object is not iterable");
-    __PYX_ERR(0, 360, __pyx_L1_error)
+    __PYX_ERR(0, 362, __pyx_L1_error)
   }
   __pyx_t_1 = __pyx_cur_scope->__pyx_outer_scope->__pyx_v_labels; __Pyx_INCREF(__pyx_t_1); __pyx_t_2 = 0;
   for (;;) {
     if (__pyx_t_2 >= PyList_GET_SIZE(__pyx_t_1)) break;
     #if CYTHON_ASSUME_SAFE_MACROS && !CYTHON_AVOID_BORROWED_REFS
-    __pyx_t_3 = PyList_GET_ITEM(__pyx_t_1, __pyx_t_2); __Pyx_INCREF(__pyx_t_3); __pyx_t_2++; if (unlikely(0 < 0)) __PYX_ERR(0, 360, __pyx_L1_error)
+    __pyx_t_3 = PyList_GET_ITEM(__pyx_t_1, __pyx_t_2); __Pyx_INCREF(__pyx_t_3); __pyx_t_2++; if (unlikely(0 < 0)) __PYX_ERR(0, 362, __pyx_L1_error)
     #else
-    __pyx_t_3 = PySequence_ITEM(__pyx_t_1, __pyx_t_2); __pyx_t_2++; if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 360, __pyx_L1_error)
+    __pyx_t_3 = PySequence_ITEM(__pyx_t_1, __pyx_t_2); __pyx_t_2++; if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 362, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_3);
     #endif
     __Pyx_XGOTREF(__pyx_cur_scope->__pyx_v_label);
     __Pyx_XDECREF_SET(__pyx_cur_scope->__pyx_v_label, __pyx_t_3);
     __Pyx_GIVEREF(__pyx_t_3);
     __pyx_t_3 = 0;
-    __pyx_t_3 = __Pyx_PyObject_CallOneArg(((PyObject *)(&PyUnicode_Type)), __pyx_cur_scope->__pyx_v_label); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 360, __pyx_L1_error)
+    __pyx_t_3 = __Pyx_PyObject_CallOneArg(((PyObject *)(&PyUnicode_Type)), __pyx_cur_scope->__pyx_v_label); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 362, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_3);
-    __pyx_t_4 = __Pyx_PyUnicode_GET_LENGTH(__pyx_t_3); if (unlikely(__pyx_t_4 == ((Py_ssize_t)-1))) __PYX_ERR(0, 360, __pyx_L1_error)
+    __pyx_t_4 = __Pyx_PyUnicode_GET_LENGTH(__pyx_t_3); if (unlikely(__pyx_t_4 == ((Py_ssize_t)-1))) __PYX_ERR(0, 362, __pyx_L1_error)
     __Pyx_DECREF(__pyx_t_3); __pyx_t_3 = 0;
-    __pyx_t_3 = PyInt_FromSsize_t(__pyx_t_4); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 360, __pyx_L1_error)
+    __pyx_t_3 = PyInt_FromSsize_t(__pyx_t_4); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 362, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_3);
     __pyx_r = __pyx_t_3;
     __pyx_t_3 = 0;
     __Pyx_XGIVEREF(__pyx_t_1);
     __pyx_cur_scope->__pyx_t_0 = __pyx_t_1;
     __pyx_cur_scope->__pyx_t_1 = __pyx_t_2;
     __Pyx_XGIVEREF(__pyx_r);
@@ -9439,15 +9446,15 @@
     __pyx_generator->resume_label = 1;
     return __pyx_r;
     __pyx_L6_resume_from_yield:;
     __pyx_t_1 = __pyx_cur_scope->__pyx_t_0;
     __pyx_cur_scope->__pyx_t_0 = 0;
     __Pyx_XGOTREF(__pyx_t_1);
     __pyx_t_2 = __pyx_cur_scope->__pyx_t_1;
-    if (unlikely(!__pyx_sent_value)) __PYX_ERR(0, 360, __pyx_L1_error)
+    if (unlikely(!__pyx_sent_value)) __PYX_ERR(0, 362, __pyx_L1_error)
   }
   __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;
   CYTHON_MAYBE_UNUSED_VAR(__pyx_cur_scope);
 
   /* function exit code */
   PyErr_SetNone(PyExc_StopIteration);
   goto __pyx_L0;
@@ -9462,15 +9469,15 @@
   #endif
   __pyx_generator->resume_label = -1;
   __Pyx_Coroutine_clear((PyObject*)__pyx_generator);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "river/metrics/confusion.pyx":352
+/* "river/metrics/confusion.pyx":354
  *         return self.data.__str__()
  * 
  *     def __repr__(self):             # <<<<<<<<<<<<<<
  *         # The labels are sorted alphabetically for reproducibility reasons
  *         labels = sorted(self.labels)
  */
 
@@ -9503,327 +9510,327 @@
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("__repr__", 0);
   __pyx_cur_scope = (struct __pyx_obj_5river_7metrics_9confusion___pyx_scope_struct_8___repr__ *)__pyx_tp_new_5river_7metrics_9confusion___pyx_scope_struct_8___repr__(__pyx_ptype_5river_7metrics_9confusion___pyx_scope_struct_8___repr__, __pyx_empty_tuple, NULL);
   if (unlikely(!__pyx_cur_scope)) {
     __pyx_cur_scope = ((struct __pyx_obj_5river_7metrics_9confusion___pyx_scope_struct_8___repr__ *)Py_None);
     __Pyx_INCREF(Py_None);
-    __PYX_ERR(0, 352, __pyx_L1_error)
+    __PYX_ERR(0, 354, __pyx_L1_error)
   } else {
     __Pyx_GOTREF(__pyx_cur_scope);
   }
 
-  /* "river/metrics/confusion.pyx":354
+  /* "river/metrics/confusion.pyx":356
  *     def __repr__(self):
  *         # The labels are sorted alphabetically for reproducibility reasons
  *         labels = sorted(self.labels)             # <<<<<<<<<<<<<<
  * 
  *         if not labels:
  */
   __pyx_t_2 = __pyx_v_self->labels;
   __Pyx_INCREF(__pyx_t_2);
-  __pyx_t_3 = PySequence_List(__pyx_t_2); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 354, __pyx_L1_error)
+  __pyx_t_3 = PySequence_List(__pyx_t_2); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 356, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_3);
   __Pyx_DECREF(__pyx_t_2); __pyx_t_2 = 0;
   __pyx_t_1 = ((PyObject*)__pyx_t_3);
   __pyx_t_3 = 0;
-  __pyx_t_4 = PyList_Sort(__pyx_t_1); if (unlikely(__pyx_t_4 == ((int)-1))) __PYX_ERR(0, 354, __pyx_L1_error)
+  __pyx_t_4 = PyList_Sort(__pyx_t_1); if (unlikely(__pyx_t_4 == ((int)-1))) __PYX_ERR(0, 356, __pyx_L1_error)
   __Pyx_GIVEREF(__pyx_t_1);
   __pyx_cur_scope->__pyx_v_labels = ((PyObject*)__pyx_t_1);
   __pyx_t_1 = 0;
 
-  /* "river/metrics/confusion.pyx":356
+  /* "river/metrics/confusion.pyx":358
  *         labels = sorted(self.labels)
  * 
  *         if not labels:             # <<<<<<<<<<<<<<
  *             return  ''
  * 
  */
   __pyx_t_5 = (__pyx_cur_scope->__pyx_v_labels != Py_None)&&(PyList_GET_SIZE(__pyx_cur_scope->__pyx_v_labels) != 0);
   __pyx_t_6 = ((!__pyx_t_5) != 0);
   if (__pyx_t_6) {
 
-    /* "river/metrics/confusion.pyx":357
+    /* "river/metrics/confusion.pyx":359
  * 
  *         if not labels:
  *             return  ''             # <<<<<<<<<<<<<<
  * 
  *         # Determine the required width of each column in the table
  */
     __Pyx_XDECREF(__pyx_r);
     __Pyx_INCREF(__pyx_kp_u_);
     __pyx_r = __pyx_kp_u_;
     goto __pyx_L0;
 
-    /* "river/metrics/confusion.pyx":356
+    /* "river/metrics/confusion.pyx":358
  *         labels = sorted(self.labels)
  * 
  *         if not labels:             # <<<<<<<<<<<<<<
  *             return  ''
  * 
  */
   }
 
-  /* "river/metrics/confusion.pyx":360
+  /* "river/metrics/confusion.pyx":362
  * 
  *         # Determine the required width of each column in the table
  *         largest_label_len = max(len(str(label)) for label in labels)             # <<<<<<<<<<<<<<
  *         largest_value_len = len(str(self.data[:].max()))
  *         width = max(5, largest_label_len, largest_value_len) + 2   # Min value is 5=len('label')
  */
-  __pyx_t_1 = __pyx_pf_5river_7metrics_9confusion_25MultiLabelConfusionMatrix_8__repr___genexpr(((PyObject*)__pyx_cur_scope)); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 360, __pyx_L1_error)
+  __pyx_t_1 = __pyx_pf_5river_7metrics_9confusion_25MultiLabelConfusionMatrix_8__repr___genexpr(((PyObject*)__pyx_cur_scope)); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 362, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_1);
-  __pyx_t_3 = __Pyx_PyObject_CallOneArg(__pyx_builtin_max, __pyx_t_1); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 360, __pyx_L1_error)
+  __pyx_t_3 = __Pyx_PyObject_CallOneArg(__pyx_builtin_max, __pyx_t_1); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 362, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_3);
   __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;
   __pyx_v_largest_label_len = __pyx_t_3;
   __pyx_t_3 = 0;
 
-  /* "river/metrics/confusion.pyx":361
+  /* "river/metrics/confusion.pyx":363
  *         # Determine the required width of each column in the table
  *         largest_label_len = max(len(str(label)) for label in labels)
  *         largest_value_len = len(str(self.data[:].max()))             # <<<<<<<<<<<<<<
  *         width = max(5, largest_label_len, largest_value_len) + 2   # Min value is 5=len('label')
  * 
  */
-  __pyx_t_1 = __Pyx_PyObject_GetSlice(__pyx_v_self->data, 0, 0, NULL, NULL, &__pyx_slice__6, 0, 0, 1); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 361, __pyx_L1_error)
+  __pyx_t_1 = __Pyx_PyObject_GetSlice(__pyx_v_self->data, 0, 0, NULL, NULL, &__pyx_slice__6, 0, 0, 1); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 363, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_1);
-  __pyx_t_2 = __Pyx_PyObject_GetAttrStr(__pyx_t_1, __pyx_n_s_max); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 361, __pyx_L1_error)
+  __pyx_t_2 = __Pyx_PyObject_GetAttrStr(__pyx_t_1, __pyx_n_s_max); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 363, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_2);
   __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;
   __pyx_t_1 = NULL;
   if (CYTHON_UNPACK_METHODS && likely(PyMethod_Check(__pyx_t_2))) {
     __pyx_t_1 = PyMethod_GET_SELF(__pyx_t_2);
     if (likely(__pyx_t_1)) {
       PyObject* function = PyMethod_GET_FUNCTION(__pyx_t_2);
       __Pyx_INCREF(__pyx_t_1);
       __Pyx_INCREF(function);
       __Pyx_DECREF_SET(__pyx_t_2, function);
     }
   }
   __pyx_t_3 = (__pyx_t_1) ? __Pyx_PyObject_CallOneArg(__pyx_t_2, __pyx_t_1) : __Pyx_PyObject_CallNoArg(__pyx_t_2);
   __Pyx_XDECREF(__pyx_t_1); __pyx_t_1 = 0;
-  if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 361, __pyx_L1_error)
+  if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 363, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_3);
   __Pyx_DECREF(__pyx_t_2); __pyx_t_2 = 0;
-  __pyx_t_2 = __Pyx_PyObject_CallOneArg(((PyObject *)(&PyUnicode_Type)), __pyx_t_3); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 361, __pyx_L1_error)
+  __pyx_t_2 = __Pyx_PyObject_CallOneArg(((PyObject *)(&PyUnicode_Type)), __pyx_t_3); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 363, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_2);
   __Pyx_DECREF(__pyx_t_3); __pyx_t_3 = 0;
-  __pyx_t_7 = __Pyx_PyUnicode_GET_LENGTH(__pyx_t_2); if (unlikely(__pyx_t_7 == ((Py_ssize_t)-1))) __PYX_ERR(0, 361, __pyx_L1_error)
+  __pyx_t_7 = __Pyx_PyUnicode_GET_LENGTH(__pyx_t_2); if (unlikely(__pyx_t_7 == ((Py_ssize_t)-1))) __PYX_ERR(0, 363, __pyx_L1_error)
   __Pyx_DECREF(__pyx_t_2); __pyx_t_2 = 0;
   __pyx_v_largest_value_len = __pyx_t_7;
 
-  /* "river/metrics/confusion.pyx":362
+  /* "river/metrics/confusion.pyx":364
  *         largest_label_len = max(len(str(label)) for label in labels)
  *         largest_value_len = len(str(self.data[:].max()))
  *         width = max(5, largest_label_len, largest_value_len) + 2   # Min value is 5=len('label')             # <<<<<<<<<<<<<<
  * 
  *         # Make a template to print out rows one by one
  */
   __Pyx_INCREF(__pyx_v_largest_label_len);
   __pyx_t_2 = __pyx_v_largest_label_len;
   __pyx_t_7 = __pyx_v_largest_value_len;
   __pyx_t_8 = 5;
-  __pyx_t_1 = __Pyx_PyInt_From_long(__pyx_t_8); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 362, __pyx_L1_error)
+  __pyx_t_1 = __Pyx_PyInt_From_long(__pyx_t_8); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 364, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_1);
-  __pyx_t_9 = PyObject_RichCompare(__pyx_t_2, __pyx_t_1, Py_GT); __Pyx_XGOTREF(__pyx_t_9); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 362, __pyx_L1_error)
+  __pyx_t_9 = PyObject_RichCompare(__pyx_t_2, __pyx_t_1, Py_GT); __Pyx_XGOTREF(__pyx_t_9); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 364, __pyx_L1_error)
   __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;
-  __pyx_t_6 = __Pyx_PyObject_IsTrue(__pyx_t_9); if (unlikely(__pyx_t_6 < 0)) __PYX_ERR(0, 362, __pyx_L1_error)
+  __pyx_t_6 = __Pyx_PyObject_IsTrue(__pyx_t_9); if (unlikely(__pyx_t_6 < 0)) __PYX_ERR(0, 364, __pyx_L1_error)
   __Pyx_DECREF(__pyx_t_9); __pyx_t_9 = 0;
   if (__pyx_t_6) {
     __Pyx_INCREF(__pyx_t_2);
     __pyx_t_3 = __pyx_t_2;
   } else {
-    __pyx_t_9 = __Pyx_PyInt_From_long(__pyx_t_8); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 362, __pyx_L1_error)
+    __pyx_t_9 = __Pyx_PyInt_From_long(__pyx_t_8); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 364, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_9);
     __pyx_t_3 = __pyx_t_9;
     __pyx_t_9 = 0;
   }
   __Pyx_INCREF(__pyx_t_3);
   __pyx_t_9 = __pyx_t_3;
   __Pyx_DECREF(__pyx_t_3); __pyx_t_3 = 0;
-  __pyx_t_1 = PyInt_FromSsize_t(__pyx_t_7); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 362, __pyx_L1_error)
+  __pyx_t_1 = PyInt_FromSsize_t(__pyx_t_7); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 364, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_1);
-  __pyx_t_10 = PyObject_RichCompare(__pyx_t_1, __pyx_t_9, Py_GT); __Pyx_XGOTREF(__pyx_t_10); if (unlikely(!__pyx_t_10)) __PYX_ERR(0, 362, __pyx_L1_error)
+  __pyx_t_10 = PyObject_RichCompare(__pyx_t_1, __pyx_t_9, Py_GT); __Pyx_XGOTREF(__pyx_t_10); if (unlikely(!__pyx_t_10)) __PYX_ERR(0, 364, __pyx_L1_error)
   __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;
-  __pyx_t_6 = __Pyx_PyObject_IsTrue(__pyx_t_10); if (unlikely(__pyx_t_6 < 0)) __PYX_ERR(0, 362, __pyx_L1_error)
+  __pyx_t_6 = __Pyx_PyObject_IsTrue(__pyx_t_10); if (unlikely(__pyx_t_6 < 0)) __PYX_ERR(0, 364, __pyx_L1_error)
   __Pyx_DECREF(__pyx_t_10); __pyx_t_10 = 0;
   if (__pyx_t_6) {
-    __pyx_t_10 = PyInt_FromSsize_t(__pyx_t_7); if (unlikely(!__pyx_t_10)) __PYX_ERR(0, 362, __pyx_L1_error)
+    __pyx_t_10 = PyInt_FromSsize_t(__pyx_t_7); if (unlikely(!__pyx_t_10)) __PYX_ERR(0, 364, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_10);
     __pyx_t_3 = __pyx_t_10;
     __pyx_t_10 = 0;
   } else {
     __Pyx_INCREF(__pyx_t_9);
     __pyx_t_3 = __pyx_t_9;
   }
   __Pyx_DECREF(__pyx_t_9); __pyx_t_9 = 0;
   __Pyx_DECREF(__pyx_t_2); __pyx_t_2 = 0;
-  __pyx_t_2 = __Pyx_PyInt_AddObjC(__pyx_t_3, __pyx_int_2, 2, 0, 0); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 362, __pyx_L1_error)
+  __pyx_t_2 = __Pyx_PyInt_AddObjC(__pyx_t_3, __pyx_int_2, 2, 0, 0); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 364, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_2);
   __Pyx_DECREF(__pyx_t_3); __pyx_t_3 = 0;
   __pyx_v_width = __pyx_t_2;
   __pyx_t_2 = 0;
 
-  /* "river/metrics/confusion.pyx":365
+  /* "river/metrics/confusion.pyx":367
  * 
  *         # Make a template to print out rows one by one
  *         row_format = '{:>{width}}' * 5    # Label, TP, FP, FN, TN             # <<<<<<<<<<<<<<
  * 
  *         # Write down the header
  */
   __Pyx_INCREF(__pyx_kp_u_width_width_width_width_width);
   __pyx_v_row_format = __pyx_kp_u_width_width_width_width_width;
 
-  /* "river/metrics/confusion.pyx":368
+  /* "river/metrics/confusion.pyx":370
  * 
  *         # Write down the header
  *         table = row_format.format('Label', 'TP', 'FP', 'FN', 'TN', width=width) + '\n'             # <<<<<<<<<<<<<<
  * 
  *         # Write down the values per labels row by row
  */
-  __pyx_t_2 = __Pyx_PyObject_GetAttrStr(__pyx_v_row_format, __pyx_n_s_format); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 368, __pyx_L1_error)
+  __pyx_t_2 = __Pyx_PyObject_GetAttrStr(__pyx_v_row_format, __pyx_n_s_format); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 370, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_2);
-  __pyx_t_3 = __Pyx_PyDict_NewPresized(1); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 368, __pyx_L1_error)
+  __pyx_t_3 = __Pyx_PyDict_NewPresized(1); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 370, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_3);
-  if (PyDict_SetItem(__pyx_t_3, __pyx_n_s_width, __pyx_v_width) < 0) __PYX_ERR(0, 368, __pyx_L1_error)
-  __pyx_t_9 = __Pyx_PyObject_Call(__pyx_t_2, __pyx_tuple__7, __pyx_t_3); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 368, __pyx_L1_error)
+  if (PyDict_SetItem(__pyx_t_3, __pyx_n_s_width, __pyx_v_width) < 0) __PYX_ERR(0, 370, __pyx_L1_error)
+  __pyx_t_9 = __Pyx_PyObject_Call(__pyx_t_2, __pyx_tuple__7, __pyx_t_3); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 370, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_9);
   __Pyx_DECREF(__pyx_t_2); __pyx_t_2 = 0;
   __Pyx_DECREF(__pyx_t_3); __pyx_t_3 = 0;
-  __pyx_t_3 = PyNumber_Add(__pyx_t_9, __pyx_kp_u__3); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 368, __pyx_L1_error)
+  __pyx_t_3 = PyNumber_Add(__pyx_t_9, __pyx_kp_u__3); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 370, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_3);
   __Pyx_DECREF(__pyx_t_9); __pyx_t_9 = 0;
   __pyx_v_table = __pyx_t_3;
   __pyx_t_3 = 0;
 
-  /* "river/metrics/confusion.pyx":371
+  /* "river/metrics/confusion.pyx":373
  * 
  *         # Write down the values per labels row by row
  *         for label in labels:             # <<<<<<<<<<<<<<
  *             label_idx = self._map_label(label, add_label=False)
  *             table += ''.join(
  */
   if (unlikely(__pyx_cur_scope->__pyx_v_labels == Py_None)) {
     PyErr_SetString(PyExc_TypeError, "'NoneType' object is not iterable");
-    __PYX_ERR(0, 371, __pyx_L1_error)
+    __PYX_ERR(0, 373, __pyx_L1_error)
   }
   __pyx_t_3 = __pyx_cur_scope->__pyx_v_labels; __Pyx_INCREF(__pyx_t_3); __pyx_t_7 = 0;
   for (;;) {
     if (__pyx_t_7 >= PyList_GET_SIZE(__pyx_t_3)) break;
     #if CYTHON_ASSUME_SAFE_MACROS && !CYTHON_AVOID_BORROWED_REFS
-    __pyx_t_9 = PyList_GET_ITEM(__pyx_t_3, __pyx_t_7); __Pyx_INCREF(__pyx_t_9); __pyx_t_7++; if (unlikely(0 < 0)) __PYX_ERR(0, 371, __pyx_L1_error)
+    __pyx_t_9 = PyList_GET_ITEM(__pyx_t_3, __pyx_t_7); __Pyx_INCREF(__pyx_t_9); __pyx_t_7++; if (unlikely(0 < 0)) __PYX_ERR(0, 373, __pyx_L1_error)
     #else
-    __pyx_t_9 = PySequence_ITEM(__pyx_t_3, __pyx_t_7); __pyx_t_7++; if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 371, __pyx_L1_error)
+    __pyx_t_9 = PySequence_ITEM(__pyx_t_3, __pyx_t_7); __pyx_t_7++; if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 373, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_9);
     #endif
     __Pyx_XDECREF_SET(__pyx_v_label, __pyx_t_9);
     __pyx_t_9 = 0;
 
-    /* "river/metrics/confusion.pyx":372
+    /* "river/metrics/confusion.pyx":374
  *         # Write down the values per labels row by row
  *         for label in labels:
  *             label_idx = self._map_label(label, add_label=False)             # <<<<<<<<<<<<<<
  *             table += ''.join(
  *                 row_format.format(
  */
     __pyx_v_label_idx = ((struct __pyx_vtabstruct_5river_7metrics_9confusion_MultiLabelConfusionMatrix *)__pyx_v_self->__pyx_vtab)->_map_label(__pyx_v_self, __pyx_v_label, 0);
 
-    /* "river/metrics/confusion.pyx":374
+    /* "river/metrics/confusion.pyx":376
  *             label_idx = self._map_label(label, add_label=False)
  *             table += ''.join(
  *                 row_format.format(             # <<<<<<<<<<<<<<
  *                     str(label),                         # Label
  *                     self.data[label_idx][1][1],         # TP
  */
-    __pyx_t_9 = __Pyx_PyObject_GetAttrStr(__pyx_v_row_format, __pyx_n_s_format); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 374, __pyx_L1_error)
+    __pyx_t_9 = __Pyx_PyObject_GetAttrStr(__pyx_v_row_format, __pyx_n_s_format); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 376, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_9);
 
-    /* "river/metrics/confusion.pyx":375
+    /* "river/metrics/confusion.pyx":377
  *             table += ''.join(
  *                 row_format.format(
  *                     str(label),                         # Label             # <<<<<<<<<<<<<<
  *                     self.data[label_idx][1][1],         # TP
  *                     self.data[label_idx][0][1],         # FP
  */
-    __pyx_t_2 = __Pyx_PyObject_CallOneArg(((PyObject *)(&PyUnicode_Type)), __pyx_v_label); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 375, __pyx_L1_error)
+    __pyx_t_2 = __Pyx_PyObject_CallOneArg(((PyObject *)(&PyUnicode_Type)), __pyx_v_label); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 377, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_2);
 
-    /* "river/metrics/confusion.pyx":376
+    /* "river/metrics/confusion.pyx":378
  *                 row_format.format(
  *                     str(label),                         # Label
  *                     self.data[label_idx][1][1],         # TP             # <<<<<<<<<<<<<<
  *                     self.data[label_idx][0][1],         # FP
  *                     self.data[label_idx][1][0],         # FN
  */
-    __pyx_t_10 = __Pyx_GetItemInt(__pyx_v_self->data, __pyx_v_label_idx, int, 1, __Pyx_PyInt_From_int, 0, 1, 1); if (unlikely(!__pyx_t_10)) __PYX_ERR(0, 376, __pyx_L1_error)
+    __pyx_t_10 = __Pyx_GetItemInt(__pyx_v_self->data, __pyx_v_label_idx, int, 1, __Pyx_PyInt_From_int, 0, 1, 1); if (unlikely(!__pyx_t_10)) __PYX_ERR(0, 378, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_10);
-    __pyx_t_1 = __Pyx_GetItemInt(__pyx_t_10, 1, long, 1, __Pyx_PyInt_From_long, 0, 0, 1); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 376, __pyx_L1_error)
+    __pyx_t_1 = __Pyx_GetItemInt(__pyx_t_10, 1, long, 1, __Pyx_PyInt_From_long, 0, 0, 1); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 378, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_1);
     __Pyx_DECREF(__pyx_t_10); __pyx_t_10 = 0;
-    __pyx_t_10 = __Pyx_GetItemInt(__pyx_t_1, 1, long, 1, __Pyx_PyInt_From_long, 0, 0, 1); if (unlikely(!__pyx_t_10)) __PYX_ERR(0, 376, __pyx_L1_error)
+    __pyx_t_10 = __Pyx_GetItemInt(__pyx_t_1, 1, long, 1, __Pyx_PyInt_From_long, 0, 0, 1); if (unlikely(!__pyx_t_10)) __PYX_ERR(0, 378, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_10);
     __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;
 
-    /* "river/metrics/confusion.pyx":377
+    /* "river/metrics/confusion.pyx":379
  *                     str(label),                         # Label
  *                     self.data[label_idx][1][1],         # TP
  *                     self.data[label_idx][0][1],         # FP             # <<<<<<<<<<<<<<
  *                     self.data[label_idx][1][0],         # FN
  *                     self.data[label_idx][0][0],         # TN
  */
-    __pyx_t_1 = __Pyx_GetItemInt(__pyx_v_self->data, __pyx_v_label_idx, int, 1, __Pyx_PyInt_From_int, 0, 1, 1); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 377, __pyx_L1_error)
+    __pyx_t_1 = __Pyx_GetItemInt(__pyx_v_self->data, __pyx_v_label_idx, int, 1, __Pyx_PyInt_From_int, 0, 1, 1); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 379, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_1);
-    __pyx_t_11 = __Pyx_GetItemInt(__pyx_t_1, 0, long, 1, __Pyx_PyInt_From_long, 0, 0, 1); if (unlikely(!__pyx_t_11)) __PYX_ERR(0, 377, __pyx_L1_error)
+    __pyx_t_11 = __Pyx_GetItemInt(__pyx_t_1, 0, long, 1, __Pyx_PyInt_From_long, 0, 0, 1); if (unlikely(!__pyx_t_11)) __PYX_ERR(0, 379, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_11);
     __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;
-    __pyx_t_1 = __Pyx_GetItemInt(__pyx_t_11, 1, long, 1, __Pyx_PyInt_From_long, 0, 0, 1); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 377, __pyx_L1_error)
+    __pyx_t_1 = __Pyx_GetItemInt(__pyx_t_11, 1, long, 1, __Pyx_PyInt_From_long, 0, 0, 1); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 379, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_1);
     __Pyx_DECREF(__pyx_t_11); __pyx_t_11 = 0;
 
-    /* "river/metrics/confusion.pyx":378
+    /* "river/metrics/confusion.pyx":380
  *                     self.data[label_idx][1][1],         # TP
  *                     self.data[label_idx][0][1],         # FP
  *                     self.data[label_idx][1][0],         # FN             # <<<<<<<<<<<<<<
  *                     self.data[label_idx][0][0],         # TN
  *                     width=width))
  */
-    __pyx_t_11 = __Pyx_GetItemInt(__pyx_v_self->data, __pyx_v_label_idx, int, 1, __Pyx_PyInt_From_int, 0, 1, 1); if (unlikely(!__pyx_t_11)) __PYX_ERR(0, 378, __pyx_L1_error)
+    __pyx_t_11 = __Pyx_GetItemInt(__pyx_v_self->data, __pyx_v_label_idx, int, 1, __Pyx_PyInt_From_int, 0, 1, 1); if (unlikely(!__pyx_t_11)) __PYX_ERR(0, 380, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_11);
-    __pyx_t_12 = __Pyx_GetItemInt(__pyx_t_11, 1, long, 1, __Pyx_PyInt_From_long, 0, 0, 1); if (unlikely(!__pyx_t_12)) __PYX_ERR(0, 378, __pyx_L1_error)
+    __pyx_t_12 = __Pyx_GetItemInt(__pyx_t_11, 1, long, 1, __Pyx_PyInt_From_long, 0, 0, 1); if (unlikely(!__pyx_t_12)) __PYX_ERR(0, 380, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_12);
     __Pyx_DECREF(__pyx_t_11); __pyx_t_11 = 0;
-    __pyx_t_11 = __Pyx_GetItemInt(__pyx_t_12, 0, long, 1, __Pyx_PyInt_From_long, 0, 0, 1); if (unlikely(!__pyx_t_11)) __PYX_ERR(0, 378, __pyx_L1_error)
+    __pyx_t_11 = __Pyx_GetItemInt(__pyx_t_12, 0, long, 1, __Pyx_PyInt_From_long, 0, 0, 1); if (unlikely(!__pyx_t_11)) __PYX_ERR(0, 380, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_11);
     __Pyx_DECREF(__pyx_t_12); __pyx_t_12 = 0;
 
-    /* "river/metrics/confusion.pyx":379
+    /* "river/metrics/confusion.pyx":381
  *                     self.data[label_idx][0][1],         # FP
  *                     self.data[label_idx][1][0],         # FN
  *                     self.data[label_idx][0][0],         # TN             # <<<<<<<<<<<<<<
  *                     width=width))
  *             table += '\n'
  */
-    __pyx_t_12 = __Pyx_GetItemInt(__pyx_v_self->data, __pyx_v_label_idx, int, 1, __Pyx_PyInt_From_int, 0, 1, 1); if (unlikely(!__pyx_t_12)) __PYX_ERR(0, 379, __pyx_L1_error)
+    __pyx_t_12 = __Pyx_GetItemInt(__pyx_v_self->data, __pyx_v_label_idx, int, 1, __Pyx_PyInt_From_int, 0, 1, 1); if (unlikely(!__pyx_t_12)) __PYX_ERR(0, 381, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_12);
-    __pyx_t_13 = __Pyx_GetItemInt(__pyx_t_12, 0, long, 1, __Pyx_PyInt_From_long, 0, 0, 1); if (unlikely(!__pyx_t_13)) __PYX_ERR(0, 379, __pyx_L1_error)
+    __pyx_t_13 = __Pyx_GetItemInt(__pyx_t_12, 0, long, 1, __Pyx_PyInt_From_long, 0, 0, 1); if (unlikely(!__pyx_t_13)) __PYX_ERR(0, 381, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_13);
     __Pyx_DECREF(__pyx_t_12); __pyx_t_12 = 0;
-    __pyx_t_12 = __Pyx_GetItemInt(__pyx_t_13, 0, long, 1, __Pyx_PyInt_From_long, 0, 0, 1); if (unlikely(!__pyx_t_12)) __PYX_ERR(0, 379, __pyx_L1_error)
+    __pyx_t_12 = __Pyx_GetItemInt(__pyx_t_13, 0, long, 1, __Pyx_PyInt_From_long, 0, 0, 1); if (unlikely(!__pyx_t_12)) __PYX_ERR(0, 381, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_12);
     __Pyx_DECREF(__pyx_t_13); __pyx_t_13 = 0;
 
-    /* "river/metrics/confusion.pyx":374
+    /* "river/metrics/confusion.pyx":376
  *             label_idx = self._map_label(label, add_label=False)
  *             table += ''.join(
  *                 row_format.format(             # <<<<<<<<<<<<<<
  *                     str(label),                         # Label
  *                     self.data[label_idx][1][1],         # TP
  */
-    __pyx_t_13 = PyTuple_New(5); if (unlikely(!__pyx_t_13)) __PYX_ERR(0, 374, __pyx_L1_error)
+    __pyx_t_13 = PyTuple_New(5); if (unlikely(!__pyx_t_13)) __PYX_ERR(0, 376, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_13);
     __Pyx_GIVEREF(__pyx_t_2);
     PyTuple_SET_ITEM(__pyx_t_13, 0, __pyx_t_2);
     __Pyx_GIVEREF(__pyx_t_10);
     PyTuple_SET_ITEM(__pyx_t_13, 1, __pyx_t_10);
     __Pyx_GIVEREF(__pyx_t_1);
     PyTuple_SET_ITEM(__pyx_t_13, 2, __pyx_t_1);
@@ -9833,87 +9840,87 @@
     PyTuple_SET_ITEM(__pyx_t_13, 4, __pyx_t_12);
     __pyx_t_2 = 0;
     __pyx_t_10 = 0;
     __pyx_t_1 = 0;
     __pyx_t_11 = 0;
     __pyx_t_12 = 0;
 
-    /* "river/metrics/confusion.pyx":380
+    /* "river/metrics/confusion.pyx":382
  *                     self.data[label_idx][1][0],         # FN
  *                     self.data[label_idx][0][0],         # TN
  *                     width=width))             # <<<<<<<<<<<<<<
  *             table += '\n'
  * 
  */
-    __pyx_t_12 = __Pyx_PyDict_NewPresized(1); if (unlikely(!__pyx_t_12)) __PYX_ERR(0, 380, __pyx_L1_error)
+    __pyx_t_12 = __Pyx_PyDict_NewPresized(1); if (unlikely(!__pyx_t_12)) __PYX_ERR(0, 382, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_12);
-    if (PyDict_SetItem(__pyx_t_12, __pyx_n_s_width, __pyx_v_width) < 0) __PYX_ERR(0, 380, __pyx_L1_error)
+    if (PyDict_SetItem(__pyx_t_12, __pyx_n_s_width, __pyx_v_width) < 0) __PYX_ERR(0, 382, __pyx_L1_error)
 
-    /* "river/metrics/confusion.pyx":374
+    /* "river/metrics/confusion.pyx":376
  *             label_idx = self._map_label(label, add_label=False)
  *             table += ''.join(
  *                 row_format.format(             # <<<<<<<<<<<<<<
  *                     str(label),                         # Label
  *                     self.data[label_idx][1][1],         # TP
  */
-    __pyx_t_11 = __Pyx_PyObject_Call(__pyx_t_9, __pyx_t_13, __pyx_t_12); if (unlikely(!__pyx_t_11)) __PYX_ERR(0, 374, __pyx_L1_error)
+    __pyx_t_11 = __Pyx_PyObject_Call(__pyx_t_9, __pyx_t_13, __pyx_t_12); if (unlikely(!__pyx_t_11)) __PYX_ERR(0, 376, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_11);
     __Pyx_DECREF(__pyx_t_9); __pyx_t_9 = 0;
     __Pyx_DECREF(__pyx_t_13); __pyx_t_13 = 0;
     __Pyx_DECREF(__pyx_t_12); __pyx_t_12 = 0;
 
-    /* "river/metrics/confusion.pyx":373
+    /* "river/metrics/confusion.pyx":375
  *         for label in labels:
  *             label_idx = self._map_label(label, add_label=False)
  *             table += ''.join(             # <<<<<<<<<<<<<<
  *                 row_format.format(
  *                     str(label),                         # Label
  */
-    __pyx_t_12 = PyUnicode_Join(__pyx_kp_u_, __pyx_t_11); if (unlikely(!__pyx_t_12)) __PYX_ERR(0, 373, __pyx_L1_error)
+    __pyx_t_12 = PyUnicode_Join(__pyx_kp_u_, __pyx_t_11); if (unlikely(!__pyx_t_12)) __PYX_ERR(0, 375, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_12);
     __Pyx_DECREF(__pyx_t_11); __pyx_t_11 = 0;
-    __pyx_t_11 = PyNumber_InPlaceAdd(__pyx_v_table, __pyx_t_12); if (unlikely(!__pyx_t_11)) __PYX_ERR(0, 373, __pyx_L1_error)
+    __pyx_t_11 = PyNumber_InPlaceAdd(__pyx_v_table, __pyx_t_12); if (unlikely(!__pyx_t_11)) __PYX_ERR(0, 375, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_11);
     __Pyx_DECREF(__pyx_t_12); __pyx_t_12 = 0;
     __Pyx_DECREF_SET(__pyx_v_table, __pyx_t_11);
     __pyx_t_11 = 0;
 
-    /* "river/metrics/confusion.pyx":381
+    /* "river/metrics/confusion.pyx":383
  *                     self.data[label_idx][0][0],         # TN
  *                     width=width))
  *             table += '\n'             # <<<<<<<<<<<<<<
  * 
  *         return table
  */
-    __pyx_t_11 = PyNumber_InPlaceAdd(__pyx_v_table, __pyx_kp_u__3); if (unlikely(!__pyx_t_11)) __PYX_ERR(0, 381, __pyx_L1_error)
+    __pyx_t_11 = PyNumber_InPlaceAdd(__pyx_v_table, __pyx_kp_u__3); if (unlikely(!__pyx_t_11)) __PYX_ERR(0, 383, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_11);
     __Pyx_DECREF_SET(__pyx_v_table, __pyx_t_11);
     __pyx_t_11 = 0;
 
-    /* "river/metrics/confusion.pyx":371
+    /* "river/metrics/confusion.pyx":373
  * 
  *         # Write down the values per labels row by row
  *         for label in labels:             # <<<<<<<<<<<<<<
  *             label_idx = self._map_label(label, add_label=False)
  *             table += ''.join(
  */
   }
   __Pyx_DECREF(__pyx_t_3); __pyx_t_3 = 0;
 
-  /* "river/metrics/confusion.pyx":383
+  /* "river/metrics/confusion.pyx":385
  *             table += '\n'
  * 
  *         return table             # <<<<<<<<<<<<<<
  */
   __Pyx_XDECREF(__pyx_r);
   __Pyx_INCREF(__pyx_v_table);
   __pyx_r = __pyx_v_table;
   goto __pyx_L0;
 
-  /* "river/metrics/confusion.pyx":352
+  /* "river/metrics/confusion.pyx":354
  *         return self.data.__str__()
  * 
  *     def __repr__(self):             # <<<<<<<<<<<<<<
  *         # The labels are sorted alphabetically for reproducibility reasons
  *         labels = sorted(self.labels)
  */
 
@@ -11977,15 +11984,15 @@
   __pyx_r = 0;
   __pyx_L0:;
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":735
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":735
  * ctypedef npy_cdouble     complex_t
  * 
  * cdef inline object PyArray_MultiIterNew1(a):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(1, <void*>a)
  * 
  */
 
@@ -11994,29 +12001,29 @@
   __Pyx_RefNannyDeclarations
   PyObject *__pyx_t_1 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("PyArray_MultiIterNew1", 0);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":736
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":736
  * 
  * cdef inline object PyArray_MultiIterNew1(a):
  *     return PyArray_MultiIterNew(1, <void*>a)             # <<<<<<<<<<<<<<
  * 
  * cdef inline object PyArray_MultiIterNew2(a, b):
  */
   __Pyx_XDECREF(__pyx_r);
   __pyx_t_1 = PyArray_MultiIterNew(1, ((void *)__pyx_v_a)); if (unlikely(!__pyx_t_1)) __PYX_ERR(3, 736, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_1);
   __pyx_r = __pyx_t_1;
   __pyx_t_1 = 0;
   goto __pyx_L0;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":735
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":735
  * ctypedef npy_cdouble     complex_t
  * 
  * cdef inline object PyArray_MultiIterNew1(a):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(1, <void*>a)
  * 
  */
 
@@ -12027,15 +12034,15 @@
   __pyx_r = 0;
   __pyx_L0:;
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":738
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":738
  *     return PyArray_MultiIterNew(1, <void*>a)
  * 
  * cdef inline object PyArray_MultiIterNew2(a, b):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(2, <void*>a, <void*>b)
  * 
  */
 
@@ -12044,29 +12051,29 @@
   __Pyx_RefNannyDeclarations
   PyObject *__pyx_t_1 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("PyArray_MultiIterNew2", 0);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":739
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":739
  * 
  * cdef inline object PyArray_MultiIterNew2(a, b):
  *     return PyArray_MultiIterNew(2, <void*>a, <void*>b)             # <<<<<<<<<<<<<<
  * 
  * cdef inline object PyArray_MultiIterNew3(a, b, c):
  */
   __Pyx_XDECREF(__pyx_r);
   __pyx_t_1 = PyArray_MultiIterNew(2, ((void *)__pyx_v_a), ((void *)__pyx_v_b)); if (unlikely(!__pyx_t_1)) __PYX_ERR(3, 739, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_1);
   __pyx_r = __pyx_t_1;
   __pyx_t_1 = 0;
   goto __pyx_L0;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":738
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":738
  *     return PyArray_MultiIterNew(1, <void*>a)
  * 
  * cdef inline object PyArray_MultiIterNew2(a, b):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(2, <void*>a, <void*>b)
  * 
  */
 
@@ -12077,15 +12084,15 @@
   __pyx_r = 0;
   __pyx_L0:;
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":741
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":741
  *     return PyArray_MultiIterNew(2, <void*>a, <void*>b)
  * 
  * cdef inline object PyArray_MultiIterNew3(a, b, c):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(3, <void*>a, <void*>b, <void*> c)
  * 
  */
 
@@ -12094,29 +12101,29 @@
   __Pyx_RefNannyDeclarations
   PyObject *__pyx_t_1 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("PyArray_MultiIterNew3", 0);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":742
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":742
  * 
  * cdef inline object PyArray_MultiIterNew3(a, b, c):
  *     return PyArray_MultiIterNew(3, <void*>a, <void*>b, <void*> c)             # <<<<<<<<<<<<<<
  * 
  * cdef inline object PyArray_MultiIterNew4(a, b, c, d):
  */
   __Pyx_XDECREF(__pyx_r);
   __pyx_t_1 = PyArray_MultiIterNew(3, ((void *)__pyx_v_a), ((void *)__pyx_v_b), ((void *)__pyx_v_c)); if (unlikely(!__pyx_t_1)) __PYX_ERR(3, 742, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_1);
   __pyx_r = __pyx_t_1;
   __pyx_t_1 = 0;
   goto __pyx_L0;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":741
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":741
  *     return PyArray_MultiIterNew(2, <void*>a, <void*>b)
  * 
  * cdef inline object PyArray_MultiIterNew3(a, b, c):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(3, <void*>a, <void*>b, <void*> c)
  * 
  */
 
@@ -12127,15 +12134,15 @@
   __pyx_r = 0;
   __pyx_L0:;
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":744
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":744
  *     return PyArray_MultiIterNew(3, <void*>a, <void*>b, <void*> c)
  * 
  * cdef inline object PyArray_MultiIterNew4(a, b, c, d):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(4, <void*>a, <void*>b, <void*>c, <void*> d)
  * 
  */
 
@@ -12144,29 +12151,29 @@
   __Pyx_RefNannyDeclarations
   PyObject *__pyx_t_1 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("PyArray_MultiIterNew4", 0);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":745
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":745
  * 
  * cdef inline object PyArray_MultiIterNew4(a, b, c, d):
  *     return PyArray_MultiIterNew(4, <void*>a, <void*>b, <void*>c, <void*> d)             # <<<<<<<<<<<<<<
  * 
  * cdef inline object PyArray_MultiIterNew5(a, b, c, d, e):
  */
   __Pyx_XDECREF(__pyx_r);
   __pyx_t_1 = PyArray_MultiIterNew(4, ((void *)__pyx_v_a), ((void *)__pyx_v_b), ((void *)__pyx_v_c), ((void *)__pyx_v_d)); if (unlikely(!__pyx_t_1)) __PYX_ERR(3, 745, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_1);
   __pyx_r = __pyx_t_1;
   __pyx_t_1 = 0;
   goto __pyx_L0;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":744
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":744
  *     return PyArray_MultiIterNew(3, <void*>a, <void*>b, <void*> c)
  * 
  * cdef inline object PyArray_MultiIterNew4(a, b, c, d):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(4, <void*>a, <void*>b, <void*>c, <void*> d)
  * 
  */
 
@@ -12177,15 +12184,15 @@
   __pyx_r = 0;
   __pyx_L0:;
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":747
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":747
  *     return PyArray_MultiIterNew(4, <void*>a, <void*>b, <void*>c, <void*> d)
  * 
  * cdef inline object PyArray_MultiIterNew5(a, b, c, d, e):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(5, <void*>a, <void*>b, <void*>c, <void*> d, <void*> e)
  * 
  */
 
@@ -12194,29 +12201,29 @@
   __Pyx_RefNannyDeclarations
   PyObject *__pyx_t_1 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("PyArray_MultiIterNew5", 0);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":748
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":748
  * 
  * cdef inline object PyArray_MultiIterNew5(a, b, c, d, e):
  *     return PyArray_MultiIterNew(5, <void*>a, <void*>b, <void*>c, <void*> d, <void*> e)             # <<<<<<<<<<<<<<
  * 
  * cdef inline tuple PyDataType_SHAPE(dtype d):
  */
   __Pyx_XDECREF(__pyx_r);
   __pyx_t_1 = PyArray_MultiIterNew(5, ((void *)__pyx_v_a), ((void *)__pyx_v_b), ((void *)__pyx_v_c), ((void *)__pyx_v_d), ((void *)__pyx_v_e)); if (unlikely(!__pyx_t_1)) __PYX_ERR(3, 748, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_1);
   __pyx_r = __pyx_t_1;
   __pyx_t_1 = 0;
   goto __pyx_L0;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":747
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":747
  *     return PyArray_MultiIterNew(4, <void*>a, <void*>b, <void*>c, <void*> d)
  * 
  * cdef inline object PyArray_MultiIterNew5(a, b, c, d, e):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(5, <void*>a, <void*>b, <void*>c, <void*> d, <void*> e)
  * 
  */
 
@@ -12227,212 +12234,212 @@
   __pyx_r = 0;
   __pyx_L0:;
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":750
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":750
  *     return PyArray_MultiIterNew(5, <void*>a, <void*>b, <void*>c, <void*> d, <void*> e)
  * 
  * cdef inline tuple PyDataType_SHAPE(dtype d):             # <<<<<<<<<<<<<<
  *     if PyDataType_HASSUBARRAY(d):
  *         return <tuple>d.subarray.shape
  */
 
 static CYTHON_INLINE PyObject *__pyx_f_5numpy_PyDataType_SHAPE(PyArray_Descr *__pyx_v_d) {
   PyObject *__pyx_r = NULL;
   __Pyx_RefNannyDeclarations
   int __pyx_t_1;
   __Pyx_RefNannySetupContext("PyDataType_SHAPE", 0);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":751
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":751
  * 
  * cdef inline tuple PyDataType_SHAPE(dtype d):
  *     if PyDataType_HASSUBARRAY(d):             # <<<<<<<<<<<<<<
  *         return <tuple>d.subarray.shape
  *     else:
  */
   __pyx_t_1 = (PyDataType_HASSUBARRAY(__pyx_v_d) != 0);
   if (__pyx_t_1) {
 
-    /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":752
+    /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":752
  * cdef inline tuple PyDataType_SHAPE(dtype d):
  *     if PyDataType_HASSUBARRAY(d):
  *         return <tuple>d.subarray.shape             # <<<<<<<<<<<<<<
  *     else:
  *         return ()
  */
     __Pyx_XDECREF(__pyx_r);
     __Pyx_INCREF(((PyObject*)__pyx_v_d->subarray->shape));
     __pyx_r = ((PyObject*)__pyx_v_d->subarray->shape);
     goto __pyx_L0;
 
-    /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":751
+    /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":751
  * 
  * cdef inline tuple PyDataType_SHAPE(dtype d):
  *     if PyDataType_HASSUBARRAY(d):             # <<<<<<<<<<<<<<
  *         return <tuple>d.subarray.shape
  *     else:
  */
   }
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":754
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":754
  *         return <tuple>d.subarray.shape
  *     else:
  *         return ()             # <<<<<<<<<<<<<<
  * 
  * 
  */
   /*else*/ {
     __Pyx_XDECREF(__pyx_r);
     __Pyx_INCREF(__pyx_empty_tuple);
     __pyx_r = __pyx_empty_tuple;
     goto __pyx_L0;
   }
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":750
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":750
  *     return PyArray_MultiIterNew(5, <void*>a, <void*>b, <void*>c, <void*> d, <void*> e)
  * 
  * cdef inline tuple PyDataType_SHAPE(dtype d):             # <<<<<<<<<<<<<<
  *     if PyDataType_HASSUBARRAY(d):
  *         return <tuple>d.subarray.shape
  */
 
   /* function exit code */
   __pyx_L0:;
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":931
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":929
  *     int _import_umath() except -1
  * 
  * cdef inline void set_array_base(ndarray arr, object base):             # <<<<<<<<<<<<<<
  *     Py_INCREF(base) # important to do this before stealing the reference below!
  *     PyArray_SetBaseObject(arr, base)
  */
 
 static CYTHON_INLINE void __pyx_f_5numpy_set_array_base(PyArrayObject *__pyx_v_arr, PyObject *__pyx_v_base) {
   __Pyx_RefNannyDeclarations
   __Pyx_RefNannySetupContext("set_array_base", 0);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":932
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":930
  * 
  * cdef inline void set_array_base(ndarray arr, object base):
  *     Py_INCREF(base) # important to do this before stealing the reference below!             # <<<<<<<<<<<<<<
  *     PyArray_SetBaseObject(arr, base)
  * 
  */
   Py_INCREF(__pyx_v_base);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":933
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":931
  * cdef inline void set_array_base(ndarray arr, object base):
  *     Py_INCREF(base) # important to do this before stealing the reference below!
  *     PyArray_SetBaseObject(arr, base)             # <<<<<<<<<<<<<<
  * 
  * cdef inline object get_array_base(ndarray arr):
  */
   (void)(PyArray_SetBaseObject(__pyx_v_arr, __pyx_v_base));
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":931
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":929
  *     int _import_umath() except -1
  * 
  * cdef inline void set_array_base(ndarray arr, object base):             # <<<<<<<<<<<<<<
  *     Py_INCREF(base) # important to do this before stealing the reference below!
  *     PyArray_SetBaseObject(arr, base)
  */
 
   /* function exit code */
   __Pyx_RefNannyFinishContext();
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":935
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":933
  *     PyArray_SetBaseObject(arr, base)
  * 
  * cdef inline object get_array_base(ndarray arr):             # <<<<<<<<<<<<<<
  *     base = PyArray_BASE(arr)
  *     if base is NULL:
  */
 
 static CYTHON_INLINE PyObject *__pyx_f_5numpy_get_array_base(PyArrayObject *__pyx_v_arr) {
   PyObject *__pyx_v_base;
   PyObject *__pyx_r = NULL;
   __Pyx_RefNannyDeclarations
   int __pyx_t_1;
   __Pyx_RefNannySetupContext("get_array_base", 0);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":936
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":934
  * 
  * cdef inline object get_array_base(ndarray arr):
  *     base = PyArray_BASE(arr)             # <<<<<<<<<<<<<<
  *     if base is NULL:
  *         return None
  */
   __pyx_v_base = PyArray_BASE(__pyx_v_arr);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":937
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":935
  * cdef inline object get_array_base(ndarray arr):
  *     base = PyArray_BASE(arr)
  *     if base is NULL:             # <<<<<<<<<<<<<<
  *         return None
  *     return <object>base
  */
   __pyx_t_1 = ((__pyx_v_base == NULL) != 0);
   if (__pyx_t_1) {
 
-    /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":938
+    /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":936
  *     base = PyArray_BASE(arr)
  *     if base is NULL:
  *         return None             # <<<<<<<<<<<<<<
  *     return <object>base
  * 
  */
     __Pyx_XDECREF(__pyx_r);
     __pyx_r = Py_None; __Pyx_INCREF(Py_None);
     goto __pyx_L0;
 
-    /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":937
+    /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":935
  * cdef inline object get_array_base(ndarray arr):
  *     base = PyArray_BASE(arr)
  *     if base is NULL:             # <<<<<<<<<<<<<<
  *         return None
  *     return <object>base
  */
   }
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":939
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":937
  *     if base is NULL:
  *         return None
  *     return <object>base             # <<<<<<<<<<<<<<
  * 
  * # Versions of the import_* functions which are more suitable for
  */
   __Pyx_XDECREF(__pyx_r);
   __Pyx_INCREF(((PyObject *)__pyx_v_base));
   __pyx_r = ((PyObject *)__pyx_v_base);
   goto __pyx_L0;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":935
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":933
  *     PyArray_SetBaseObject(arr, base)
  * 
  * cdef inline object get_array_base(ndarray arr):             # <<<<<<<<<<<<<<
  *     base = PyArray_BASE(arr)
  *     if base is NULL:
  */
 
   /* function exit code */
   __pyx_L0:;
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":943
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":941
  * # Versions of the import_* functions which are more suitable for
  * # Cython code.
  * cdef inline int import_array() except -1:             # <<<<<<<<<<<<<<
  *     try:
  *         __pyx_import_array()
  */
 
@@ -12448,15 +12455,15 @@
   PyObject *__pyx_t_7 = NULL;
   PyObject *__pyx_t_8 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("import_array", 0);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":944
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":942
  * # Cython code.
  * cdef inline int import_array() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         __pyx_import_array()
  *     except Exception:
  */
   {
@@ -12464,84 +12471,84 @@
     __Pyx_PyThreadState_assign
     __Pyx_ExceptionSave(&__pyx_t_1, &__pyx_t_2, &__pyx_t_3);
     __Pyx_XGOTREF(__pyx_t_1);
     __Pyx_XGOTREF(__pyx_t_2);
     __Pyx_XGOTREF(__pyx_t_3);
     /*try:*/ {
 
-      /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":945
+      /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":943
  * cdef inline int import_array() except -1:
  *     try:
  *         __pyx_import_array()             # <<<<<<<<<<<<<<
  *     except Exception:
  *         raise ImportError("numpy.core.multiarray failed to import")
  */
-      __pyx_t_4 = _import_array(); if (unlikely(__pyx_t_4 == ((int)-1))) __PYX_ERR(3, 945, __pyx_L3_error)
+      __pyx_t_4 = _import_array(); if (unlikely(__pyx_t_4 == ((int)-1))) __PYX_ERR(3, 943, __pyx_L3_error)
 
-      /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":944
+      /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":942
  * # Cython code.
  * cdef inline int import_array() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         __pyx_import_array()
  *     except Exception:
  */
     }
     __Pyx_XDECREF(__pyx_t_1); __pyx_t_1 = 0;
     __Pyx_XDECREF(__pyx_t_2); __pyx_t_2 = 0;
     __Pyx_XDECREF(__pyx_t_3); __pyx_t_3 = 0;
     goto __pyx_L8_try_end;
     __pyx_L3_error:;
 
-    /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":946
+    /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":944
  *     try:
  *         __pyx_import_array()
  *     except Exception:             # <<<<<<<<<<<<<<
  *         raise ImportError("numpy.core.multiarray failed to import")
  * 
  */
     __pyx_t_4 = __Pyx_PyErr_ExceptionMatches(((PyObject *)(&((PyTypeObject*)PyExc_Exception)[0])));
     if (__pyx_t_4) {
       __Pyx_AddTraceback("numpy.import_array", __pyx_clineno, __pyx_lineno, __pyx_filename);
-      if (__Pyx_GetException(&__pyx_t_5, &__pyx_t_6, &__pyx_t_7) < 0) __PYX_ERR(3, 946, __pyx_L5_except_error)
+      if (__Pyx_GetException(&__pyx_t_5, &__pyx_t_6, &__pyx_t_7) < 0) __PYX_ERR(3, 944, __pyx_L5_except_error)
       __Pyx_GOTREF(__pyx_t_5);
       __Pyx_GOTREF(__pyx_t_6);
       __Pyx_GOTREF(__pyx_t_7);
 
-      /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":947
+      /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":945
  *         __pyx_import_array()
  *     except Exception:
  *         raise ImportError("numpy.core.multiarray failed to import")             # <<<<<<<<<<<<<<
  * 
  * cdef inline int import_umath() except -1:
  */
-      __pyx_t_8 = __Pyx_PyObject_Call(__pyx_builtin_ImportError, __pyx_tuple__8, NULL); if (unlikely(!__pyx_t_8)) __PYX_ERR(3, 947, __pyx_L5_except_error)
+      __pyx_t_8 = __Pyx_PyObject_Call(__pyx_builtin_ImportError, __pyx_tuple__8, NULL); if (unlikely(!__pyx_t_8)) __PYX_ERR(3, 945, __pyx_L5_except_error)
       __Pyx_GOTREF(__pyx_t_8);
       __Pyx_Raise(__pyx_t_8, 0, 0, 0);
       __Pyx_DECREF(__pyx_t_8); __pyx_t_8 = 0;
-      __PYX_ERR(3, 947, __pyx_L5_except_error)
+      __PYX_ERR(3, 945, __pyx_L5_except_error)
     }
     goto __pyx_L5_except_error;
     __pyx_L5_except_error:;
 
-    /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":944
+    /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":942
  * # Cython code.
  * cdef inline int import_array() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         __pyx_import_array()
  *     except Exception:
  */
     __Pyx_XGIVEREF(__pyx_t_1);
     __Pyx_XGIVEREF(__pyx_t_2);
     __Pyx_XGIVEREF(__pyx_t_3);
     __Pyx_ExceptionReset(__pyx_t_1, __pyx_t_2, __pyx_t_3);
     goto __pyx_L1_error;
     __pyx_L8_try_end:;
   }
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":943
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":941
  * # Versions of the import_* functions which are more suitable for
  * # Cython code.
  * cdef inline int import_array() except -1:             # <<<<<<<<<<<<<<
  *     try:
  *         __pyx_import_array()
  */
 
@@ -12556,15 +12563,15 @@
   __Pyx_AddTraceback("numpy.import_array", __pyx_clineno, __pyx_lineno, __pyx_filename);
   __pyx_r = -1;
   __pyx_L0:;
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":949
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":947
  *         raise ImportError("numpy.core.multiarray failed to import")
  * 
  * cdef inline int import_umath() except -1:             # <<<<<<<<<<<<<<
  *     try:
  *         _import_umath()
  */
 
@@ -12580,15 +12587,15 @@
   PyObject *__pyx_t_7 = NULL;
   PyObject *__pyx_t_8 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("import_umath", 0);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":950
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":948
  * 
  * cdef inline int import_umath() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         _import_umath()
  *     except Exception:
  */
   {
@@ -12596,84 +12603,84 @@
     __Pyx_PyThreadState_assign
     __Pyx_ExceptionSave(&__pyx_t_1, &__pyx_t_2, &__pyx_t_3);
     __Pyx_XGOTREF(__pyx_t_1);
     __Pyx_XGOTREF(__pyx_t_2);
     __Pyx_XGOTREF(__pyx_t_3);
     /*try:*/ {
 
-      /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":951
+      /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":949
  * cdef inline int import_umath() except -1:
  *     try:
  *         _import_umath()             # <<<<<<<<<<<<<<
  *     except Exception:
  *         raise ImportError("numpy.core.umath failed to import")
  */
-      __pyx_t_4 = _import_umath(); if (unlikely(__pyx_t_4 == ((int)-1))) __PYX_ERR(3, 951, __pyx_L3_error)
+      __pyx_t_4 = _import_umath(); if (unlikely(__pyx_t_4 == ((int)-1))) __PYX_ERR(3, 949, __pyx_L3_error)
 
-      /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":950
+      /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":948
  * 
  * cdef inline int import_umath() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         _import_umath()
  *     except Exception:
  */
     }
     __Pyx_XDECREF(__pyx_t_1); __pyx_t_1 = 0;
     __Pyx_XDECREF(__pyx_t_2); __pyx_t_2 = 0;
     __Pyx_XDECREF(__pyx_t_3); __pyx_t_3 = 0;
     goto __pyx_L8_try_end;
     __pyx_L3_error:;
 
-    /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":952
+    /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":950
  *     try:
  *         _import_umath()
  *     except Exception:             # <<<<<<<<<<<<<<
  *         raise ImportError("numpy.core.umath failed to import")
  * 
  */
     __pyx_t_4 = __Pyx_PyErr_ExceptionMatches(((PyObject *)(&((PyTypeObject*)PyExc_Exception)[0])));
     if (__pyx_t_4) {
       __Pyx_AddTraceback("numpy.import_umath", __pyx_clineno, __pyx_lineno, __pyx_filename);
-      if (__Pyx_GetException(&__pyx_t_5, &__pyx_t_6, &__pyx_t_7) < 0) __PYX_ERR(3, 952, __pyx_L5_except_error)
+      if (__Pyx_GetException(&__pyx_t_5, &__pyx_t_6, &__pyx_t_7) < 0) __PYX_ERR(3, 950, __pyx_L5_except_error)
       __Pyx_GOTREF(__pyx_t_5);
       __Pyx_GOTREF(__pyx_t_6);
       __Pyx_GOTREF(__pyx_t_7);
 
-      /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":953
+      /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":951
  *         _import_umath()
  *     except Exception:
  *         raise ImportError("numpy.core.umath failed to import")             # <<<<<<<<<<<<<<
  * 
  * cdef inline int import_ufunc() except -1:
  */
-      __pyx_t_8 = __Pyx_PyObject_Call(__pyx_builtin_ImportError, __pyx_tuple__9, NULL); if (unlikely(!__pyx_t_8)) __PYX_ERR(3, 953, __pyx_L5_except_error)
+      __pyx_t_8 = __Pyx_PyObject_Call(__pyx_builtin_ImportError, __pyx_tuple__9, NULL); if (unlikely(!__pyx_t_8)) __PYX_ERR(3, 951, __pyx_L5_except_error)
       __Pyx_GOTREF(__pyx_t_8);
       __Pyx_Raise(__pyx_t_8, 0, 0, 0);
       __Pyx_DECREF(__pyx_t_8); __pyx_t_8 = 0;
-      __PYX_ERR(3, 953, __pyx_L5_except_error)
+      __PYX_ERR(3, 951, __pyx_L5_except_error)
     }
     goto __pyx_L5_except_error;
     __pyx_L5_except_error:;
 
-    /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":950
+    /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":948
  * 
  * cdef inline int import_umath() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         _import_umath()
  *     except Exception:
  */
     __Pyx_XGIVEREF(__pyx_t_1);
     __Pyx_XGIVEREF(__pyx_t_2);
     __Pyx_XGIVEREF(__pyx_t_3);
     __Pyx_ExceptionReset(__pyx_t_1, __pyx_t_2, __pyx_t_3);
     goto __pyx_L1_error;
     __pyx_L8_try_end:;
   }
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":949
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":947
  *         raise ImportError("numpy.core.multiarray failed to import")
  * 
  * cdef inline int import_umath() except -1:             # <<<<<<<<<<<<<<
  *     try:
  *         _import_umath()
  */
 
@@ -12688,15 +12695,15 @@
   __Pyx_AddTraceback("numpy.import_umath", __pyx_clineno, __pyx_lineno, __pyx_filename);
   __pyx_r = -1;
   __pyx_L0:;
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":955
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":953
  *         raise ImportError("numpy.core.umath failed to import")
  * 
  * cdef inline int import_ufunc() except -1:             # <<<<<<<<<<<<<<
  *     try:
  *         _import_umath()
  */
 
@@ -12712,15 +12719,15 @@
   PyObject *__pyx_t_7 = NULL;
   PyObject *__pyx_t_8 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("import_ufunc", 0);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":956
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":954
  * 
  * cdef inline int import_ufunc() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         _import_umath()
  *     except Exception:
  */
   {
@@ -12728,84 +12735,84 @@
     __Pyx_PyThreadState_assign
     __Pyx_ExceptionSave(&__pyx_t_1, &__pyx_t_2, &__pyx_t_3);
     __Pyx_XGOTREF(__pyx_t_1);
     __Pyx_XGOTREF(__pyx_t_2);
     __Pyx_XGOTREF(__pyx_t_3);
     /*try:*/ {
 
-      /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":957
+      /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":955
  * cdef inline int import_ufunc() except -1:
  *     try:
  *         _import_umath()             # <<<<<<<<<<<<<<
  *     except Exception:
  *         raise ImportError("numpy.core.umath failed to import")
  */
-      __pyx_t_4 = _import_umath(); if (unlikely(__pyx_t_4 == ((int)-1))) __PYX_ERR(3, 957, __pyx_L3_error)
+      __pyx_t_4 = _import_umath(); if (unlikely(__pyx_t_4 == ((int)-1))) __PYX_ERR(3, 955, __pyx_L3_error)
 
-      /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":956
+      /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":954
  * 
  * cdef inline int import_ufunc() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         _import_umath()
  *     except Exception:
  */
     }
     __Pyx_XDECREF(__pyx_t_1); __pyx_t_1 = 0;
     __Pyx_XDECREF(__pyx_t_2); __pyx_t_2 = 0;
     __Pyx_XDECREF(__pyx_t_3); __pyx_t_3 = 0;
     goto __pyx_L8_try_end;
     __pyx_L3_error:;
 
-    /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":958
+    /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":956
  *     try:
  *         _import_umath()
  *     except Exception:             # <<<<<<<<<<<<<<
  *         raise ImportError("numpy.core.umath failed to import")
  * 
  */
     __pyx_t_4 = __Pyx_PyErr_ExceptionMatches(((PyObject *)(&((PyTypeObject*)PyExc_Exception)[0])));
     if (__pyx_t_4) {
       __Pyx_AddTraceback("numpy.import_ufunc", __pyx_clineno, __pyx_lineno, __pyx_filename);
-      if (__Pyx_GetException(&__pyx_t_5, &__pyx_t_6, &__pyx_t_7) < 0) __PYX_ERR(3, 958, __pyx_L5_except_error)
+      if (__Pyx_GetException(&__pyx_t_5, &__pyx_t_6, &__pyx_t_7) < 0) __PYX_ERR(3, 956, __pyx_L5_except_error)
       __Pyx_GOTREF(__pyx_t_5);
       __Pyx_GOTREF(__pyx_t_6);
       __Pyx_GOTREF(__pyx_t_7);
 
-      /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":959
+      /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":957
  *         _import_umath()
  *     except Exception:
  *         raise ImportError("numpy.core.umath failed to import")             # <<<<<<<<<<<<<<
  * 
  * cdef extern from *:
  */
-      __pyx_t_8 = __Pyx_PyObject_Call(__pyx_builtin_ImportError, __pyx_tuple__9, NULL); if (unlikely(!__pyx_t_8)) __PYX_ERR(3, 959, __pyx_L5_except_error)
+      __pyx_t_8 = __Pyx_PyObject_Call(__pyx_builtin_ImportError, __pyx_tuple__9, NULL); if (unlikely(!__pyx_t_8)) __PYX_ERR(3, 957, __pyx_L5_except_error)
       __Pyx_GOTREF(__pyx_t_8);
       __Pyx_Raise(__pyx_t_8, 0, 0, 0);
       __Pyx_DECREF(__pyx_t_8); __pyx_t_8 = 0;
-      __PYX_ERR(3, 959, __pyx_L5_except_error)
+      __PYX_ERR(3, 957, __pyx_L5_except_error)
     }
     goto __pyx_L5_except_error;
     __pyx_L5_except_error:;
 
-    /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":956
+    /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":954
  * 
  * cdef inline int import_ufunc() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         _import_umath()
  *     except Exception:
  */
     __Pyx_XGIVEREF(__pyx_t_1);
     __Pyx_XGIVEREF(__pyx_t_2);
     __Pyx_XGIVEREF(__pyx_t_3);
     __Pyx_ExceptionReset(__pyx_t_1, __pyx_t_2, __pyx_t_3);
     goto __pyx_L1_error;
     __pyx_L8_try_end:;
   }
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":955
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":953
  *         raise ImportError("numpy.core.umath failed to import")
  * 
  * cdef inline int import_ufunc() except -1:             # <<<<<<<<<<<<<<
  *     try:
  *         _import_umath()
  */
 
@@ -12820,176 +12827,176 @@
   __Pyx_AddTraceback("numpy.import_ufunc", __pyx_clineno, __pyx_lineno, __pyx_filename);
   __pyx_r = -1;
   __pyx_L0:;
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":969
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":967
  * 
  * 
  * cdef inline bint is_timedelta64_object(object obj):             # <<<<<<<<<<<<<<
  *     """
  *     Cython equivalent of `isinstance(obj, np.timedelta64)`
  */
 
 static CYTHON_INLINE int __pyx_f_5numpy_is_timedelta64_object(PyObject *__pyx_v_obj) {
   int __pyx_r;
   __Pyx_RefNannyDeclarations
   __Pyx_RefNannySetupContext("is_timedelta64_object", 0);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":981
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":979
  *     bool
  *     """
  *     return PyObject_TypeCheck(obj, &PyTimedeltaArrType_Type)             # <<<<<<<<<<<<<<
  * 
  * 
  */
   __pyx_r = PyObject_TypeCheck(__pyx_v_obj, (&PyTimedeltaArrType_Type));
   goto __pyx_L0;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":969
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":967
  * 
  * 
  * cdef inline bint is_timedelta64_object(object obj):             # <<<<<<<<<<<<<<
  *     """
  *     Cython equivalent of `isinstance(obj, np.timedelta64)`
  */
 
   /* function exit code */
   __pyx_L0:;
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":984
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":982
  * 
  * 
  * cdef inline bint is_datetime64_object(object obj):             # <<<<<<<<<<<<<<
  *     """
  *     Cython equivalent of `isinstance(obj, np.datetime64)`
  */
 
 static CYTHON_INLINE int __pyx_f_5numpy_is_datetime64_object(PyObject *__pyx_v_obj) {
   int __pyx_r;
   __Pyx_RefNannyDeclarations
   __Pyx_RefNannySetupContext("is_datetime64_object", 0);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":996
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":994
  *     bool
  *     """
  *     return PyObject_TypeCheck(obj, &PyDatetimeArrType_Type)             # <<<<<<<<<<<<<<
  * 
  * 
  */
   __pyx_r = PyObject_TypeCheck(__pyx_v_obj, (&PyDatetimeArrType_Type));
   goto __pyx_L0;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":984
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":982
  * 
  * 
  * cdef inline bint is_datetime64_object(object obj):             # <<<<<<<<<<<<<<
  *     """
  *     Cython equivalent of `isinstance(obj, np.datetime64)`
  */
 
   /* function exit code */
   __pyx_L0:;
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":999
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":997
  * 
  * 
  * cdef inline npy_datetime get_datetime64_value(object obj) nogil:             # <<<<<<<<<<<<<<
  *     """
  *     returns the int64 value underlying scalar numpy datetime64 object
  */
 
 static CYTHON_INLINE npy_datetime __pyx_f_5numpy_get_datetime64_value(PyObject *__pyx_v_obj) {
   npy_datetime __pyx_r;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":1006
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":1004
  *     also needed.  That can be found using `get_datetime64_unit`.
  *     """
  *     return (<PyDatetimeScalarObject*>obj).obval             # <<<<<<<<<<<<<<
  * 
  * 
  */
   __pyx_r = ((PyDatetimeScalarObject *)__pyx_v_obj)->obval;
   goto __pyx_L0;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":999
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":997
  * 
  * 
  * cdef inline npy_datetime get_datetime64_value(object obj) nogil:             # <<<<<<<<<<<<<<
  *     """
  *     returns the int64 value underlying scalar numpy datetime64 object
  */
 
   /* function exit code */
   __pyx_L0:;
   return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":1009
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":1007
  * 
  * 
  * cdef inline npy_timedelta get_timedelta64_value(object obj) nogil:             # <<<<<<<<<<<<<<
  *     """
  *     returns the int64 value underlying scalar numpy timedelta64 object
  */
 
 static CYTHON_INLINE npy_timedelta __pyx_f_5numpy_get_timedelta64_value(PyObject *__pyx_v_obj) {
   npy_timedelta __pyx_r;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":1013
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":1011
  *     returns the int64 value underlying scalar numpy timedelta64 object
  *     """
  *     return (<PyTimedeltaScalarObject*>obj).obval             # <<<<<<<<<<<<<<
  * 
  * 
  */
   __pyx_r = ((PyTimedeltaScalarObject *)__pyx_v_obj)->obval;
   goto __pyx_L0;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":1009
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":1007
  * 
  * 
  * cdef inline npy_timedelta get_timedelta64_value(object obj) nogil:             # <<<<<<<<<<<<<<
  *     """
  *     returns the int64 value underlying scalar numpy timedelta64 object
  */
 
   /* function exit code */
   __pyx_L0:;
   return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":1016
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":1014
  * 
  * 
  * cdef inline NPY_DATETIMEUNIT get_datetime64_unit(object obj) nogil:             # <<<<<<<<<<<<<<
  *     """
  *     returns the unit part of the dtype for a numpy datetime64 object.
  */
 
 static CYTHON_INLINE NPY_DATETIMEUNIT __pyx_f_5numpy_get_datetime64_unit(PyObject *__pyx_v_obj) {
   NPY_DATETIMEUNIT __pyx_r;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":1020
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":1018
  *     returns the unit part of the dtype for a numpy datetime64 object.
  *     """
  *     return <NPY_DATETIMEUNIT>(<PyDatetimeScalarObject*>obj).obmeta.base             # <<<<<<<<<<<<<<
  */
   __pyx_r = ((NPY_DATETIMEUNIT)((PyDatetimeScalarObject *)__pyx_v_obj)->obmeta.base);
   goto __pyx_L0;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":1016
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":1014
  * 
  * 
  * cdef inline NPY_DATETIMEUNIT get_datetime64_unit(object obj) nogil:             # <<<<<<<<<<<<<<
  *     """
  *     returns the unit part of the dtype for a numpy datetime64 object.
  */
 
@@ -14893,16 +14900,16 @@
   {&__pyx_n_s_y_true, __pyx_k_y_true, sizeof(__pyx_k_y_true), 0, 0, 1, 1},
   {&__pyx_n_s_zeros, __pyx_k_zeros, sizeof(__pyx_k_zeros), 0, 0, 1, 1},
   {0, 0, 0, 0, 0, 0, 0}
 };
 static CYTHON_SMALL_CODE int __Pyx_InitCachedBuiltins(void) {
   __pyx_builtin_max = __Pyx_GetBuiltinName(__pyx_n_s_max); if (!__pyx_builtin_max) __PYX_ERR(0, 148, __pyx_L1_error)
   __pyx_builtin_map = __Pyx_GetBuiltinName(__pyx_n_s_map); if (!__pyx_builtin_map) __PYX_ERR(0, 156, __pyx_L1_error)
-  __pyx_builtin_KeyError = __Pyx_GetBuiltinName(__pyx_n_s_KeyError); if (!__pyx_builtin_KeyError) __PYX_ERR(0, 317, __pyx_L1_error)
-  __pyx_builtin_ImportError = __Pyx_GetBuiltinName(__pyx_n_s_ImportError); if (!__pyx_builtin_ImportError) __PYX_ERR(3, 947, __pyx_L1_error)
+  __pyx_builtin_KeyError = __Pyx_GetBuiltinName(__pyx_n_s_KeyError); if (!__pyx_builtin_KeyError) __PYX_ERR(0, 319, __pyx_L1_error)
+  __pyx_builtin_ImportError = __Pyx_GetBuiltinName(__pyx_n_s_ImportError); if (!__pyx_builtin_ImportError) __PYX_ERR(3, 945, __pyx_L1_error)
   return 0;
   __pyx_L1_error:;
   return -1;
 }
 
 static CYTHON_SMALL_CODE int __Pyx_InitCachedConstants(void) {
   __Pyx_RefNannyDeclarations
@@ -14926,66 +14933,66 @@
  *         self._label_dict = dict()
  *         self._label_idx_cnt = 0
  */
   __pyx_tuple__4 = PyTuple_Pack(3, __pyx_int_2, __pyx_int_2, __pyx_int_2); if (unlikely(!__pyx_tuple__4)) __PYX_ERR(0, 232, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_tuple__4);
   __Pyx_GIVEREF(__pyx_tuple__4);
 
-  /* "river/metrics/confusion.pyx":340
+  /* "river/metrics/confusion.pyx":342
  * 
  *     cdef void _reshape(self):
  *         self.data = np.vstack((self.data, np.zeros((1, 2, 2))))             # <<<<<<<<<<<<<<
  * 
  *     @property
  */
-  __pyx_tuple__5 = PyTuple_Pack(3, __pyx_int_1, __pyx_int_2, __pyx_int_2); if (unlikely(!__pyx_tuple__5)) __PYX_ERR(0, 340, __pyx_L1_error)
+  __pyx_tuple__5 = PyTuple_Pack(3, __pyx_int_1, __pyx_int_2, __pyx_int_2); if (unlikely(!__pyx_tuple__5)) __PYX_ERR(0, 342, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_tuple__5);
   __Pyx_GIVEREF(__pyx_tuple__5);
 
-  /* "river/metrics/confusion.pyx":361
+  /* "river/metrics/confusion.pyx":363
  *         # Determine the required width of each column in the table
  *         largest_label_len = max(len(str(label)) for label in labels)
  *         largest_value_len = len(str(self.data[:].max()))             # <<<<<<<<<<<<<<
  *         width = max(5, largest_label_len, largest_value_len) + 2   # Min value is 5=len('label')
  * 
  */
-  __pyx_slice__6 = PySlice_New(Py_None, Py_None, Py_None); if (unlikely(!__pyx_slice__6)) __PYX_ERR(0, 361, __pyx_L1_error)
+  __pyx_slice__6 = PySlice_New(Py_None, Py_None, Py_None); if (unlikely(!__pyx_slice__6)) __PYX_ERR(0, 363, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_slice__6);
   __Pyx_GIVEREF(__pyx_slice__6);
 
-  /* "river/metrics/confusion.pyx":368
+  /* "river/metrics/confusion.pyx":370
  * 
  *         # Write down the header
  *         table = row_format.format('Label', 'TP', 'FP', 'FN', 'TN', width=width) + '\n'             # <<<<<<<<<<<<<<
  * 
  *         # Write down the values per labels row by row
  */
-  __pyx_tuple__7 = PyTuple_Pack(5, __pyx_n_u_Label, __pyx_n_u_TP, __pyx_n_u_FP, __pyx_n_u_FN, __pyx_n_u_TN); if (unlikely(!__pyx_tuple__7)) __PYX_ERR(0, 368, __pyx_L1_error)
+  __pyx_tuple__7 = PyTuple_Pack(5, __pyx_n_u_Label, __pyx_n_u_TP, __pyx_n_u_FP, __pyx_n_u_FN, __pyx_n_u_TN); if (unlikely(!__pyx_tuple__7)) __PYX_ERR(0, 370, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_tuple__7);
   __Pyx_GIVEREF(__pyx_tuple__7);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":947
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":945
  *         __pyx_import_array()
  *     except Exception:
  *         raise ImportError("numpy.core.multiarray failed to import")             # <<<<<<<<<<<<<<
  * 
  * cdef inline int import_umath() except -1:
  */
-  __pyx_tuple__8 = PyTuple_Pack(1, __pyx_kp_u_numpy_core_multiarray_failed_to); if (unlikely(!__pyx_tuple__8)) __PYX_ERR(3, 947, __pyx_L1_error)
+  __pyx_tuple__8 = PyTuple_Pack(1, __pyx_kp_u_numpy_core_multiarray_failed_to); if (unlikely(!__pyx_tuple__8)) __PYX_ERR(3, 945, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_tuple__8);
   __Pyx_GIVEREF(__pyx_tuple__8);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":953
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":951
  *         _import_umath()
  *     except Exception:
  *         raise ImportError("numpy.core.umath failed to import")             # <<<<<<<<<<<<<<
  * 
  * cdef inline int import_ufunc() except -1:
  */
-  __pyx_tuple__9 = PyTuple_Pack(1, __pyx_kp_u_numpy_core_umath_failed_to_impor); if (unlikely(!__pyx_tuple__9)) __PYX_ERR(3, 953, __pyx_L1_error)
+  __pyx_tuple__9 = PyTuple_Pack(1, __pyx_kp_u_numpy_core_umath_failed_to_impor); if (unlikely(!__pyx_tuple__9)) __PYX_ERR(3, 951, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_tuple__9);
   __Pyx_GIVEREF(__pyx_tuple__9);
 
   /* "river/metrics/confusion.pyx":75
  *         return self.data[key]
  * 
  *     def update(self, y_true, y_pred, sample_weight=1.):             # <<<<<<<<<<<<<<
@@ -15110,37 +15117,37 @@
  *         cdef int is_equal = 1
  */
   __pyx_tuple__30 = PyTuple_Pack(12, __pyx_n_s_self, __pyx_n_s_y_true, __pyx_n_s_y_pred, __pyx_n_s_sample_weight, __pyx_n_s_is_equal, __pyx_n_s_inter_cnt, __pyx_n_s_union_cnt, __pyx_n_s_ones_true_cnt, __pyx_n_s_ones_pred_cnt, __pyx_n_s_val, __pyx_n_s_label, __pyx_n_s_label_idx); if (unlikely(!__pyx_tuple__30)) __PYX_ERR(0, 247, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_tuple__30);
   __Pyx_GIVEREF(__pyx_tuple__30);
   __pyx_codeobj__31 = (PyObject*)__Pyx_PyCode_New(4, 0, 12, 0, CO_OPTIMIZED|CO_NEWLOCALS, __pyx_empty_bytes, __pyx_empty_tuple, __pyx_empty_tuple, __pyx_tuple__30, __pyx_empty_tuple, __pyx_empty_tuple, __pyx_kp_s_river_metrics_confusion_pyx, __pyx_n_s_update, 247, __pyx_empty_bytes); if (unlikely(!__pyx_codeobj__31)) __PYX_ERR(0, 247, __pyx_L1_error)
 
-  /* "river/metrics/confusion.pyx":298
+  /* "river/metrics/confusion.pyx":300
  * 
  * 
  *     def revert(self, y_true, y_pred, sample_weight=1., correction=None):             # <<<<<<<<<<<<<<
  *         self.n_samples -= 1
  *         # Revert is equal to subtracting so we pass the negative sample_weight
  */
-  __pyx_tuple__32 = PyTuple_Pack(7, __pyx_n_s_self, __pyx_n_s_y_true, __pyx_n_s_y_pred, __pyx_n_s_sample_weight, __pyx_n_s_correction, __pyx_n_s_label, __pyx_n_s_label_idx); if (unlikely(!__pyx_tuple__32)) __PYX_ERR(0, 298, __pyx_L1_error)
+  __pyx_tuple__32 = PyTuple_Pack(7, __pyx_n_s_self, __pyx_n_s_y_true, __pyx_n_s_y_pred, __pyx_n_s_sample_weight, __pyx_n_s_correction, __pyx_n_s_label, __pyx_n_s_label_idx); if (unlikely(!__pyx_tuple__32)) __PYX_ERR(0, 300, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_tuple__32);
   __Pyx_GIVEREF(__pyx_tuple__32);
-  __pyx_codeobj__33 = (PyObject*)__Pyx_PyCode_New(5, 0, 7, 0, CO_OPTIMIZED|CO_NEWLOCALS, __pyx_empty_bytes, __pyx_empty_tuple, __pyx_empty_tuple, __pyx_tuple__32, __pyx_empty_tuple, __pyx_empty_tuple, __pyx_kp_s_river_metrics_confusion_pyx, __pyx_n_s_revert, 298, __pyx_empty_bytes); if (unlikely(!__pyx_codeobj__33)) __PYX_ERR(0, 298, __pyx_L1_error)
+  __pyx_codeobj__33 = (PyObject*)__Pyx_PyCode_New(5, 0, 7, 0, CO_OPTIMIZED|CO_NEWLOCALS, __pyx_empty_bytes, __pyx_empty_tuple, __pyx_empty_tuple, __pyx_tuple__32, __pyx_empty_tuple, __pyx_empty_tuple, __pyx_kp_s_river_metrics_confusion_pyx, __pyx_n_s_revert, 300, __pyx_empty_bytes); if (unlikely(!__pyx_codeobj__33)) __PYX_ERR(0, 300, __pyx_L1_error)
 
-  /* "river/metrics/confusion.pyx":346
+  /* "river/metrics/confusion.pyx":348
  *         return self.data.shape
  * 
  *     def reset(self):             # <<<<<<<<<<<<<<
  *         self.__init__(labels=self._init_labels)
  * 
  */
-  __pyx_tuple__34 = PyTuple_Pack(1, __pyx_n_s_self); if (unlikely(!__pyx_tuple__34)) __PYX_ERR(0, 346, __pyx_L1_error)
+  __pyx_tuple__34 = PyTuple_Pack(1, __pyx_n_s_self); if (unlikely(!__pyx_tuple__34)) __PYX_ERR(0, 348, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_tuple__34);
   __Pyx_GIVEREF(__pyx_tuple__34);
-  __pyx_codeobj__35 = (PyObject*)__Pyx_PyCode_New(1, 0, 1, 0, CO_OPTIMIZED|CO_NEWLOCALS, __pyx_empty_bytes, __pyx_empty_tuple, __pyx_empty_tuple, __pyx_tuple__34, __pyx_empty_tuple, __pyx_empty_tuple, __pyx_kp_s_river_metrics_confusion_pyx, __pyx_n_s_reset, 346, __pyx_empty_bytes); if (unlikely(!__pyx_codeobj__35)) __PYX_ERR(0, 346, __pyx_L1_error)
+  __pyx_codeobj__35 = (PyObject*)__Pyx_PyCode_New(1, 0, 1, 0, CO_OPTIMIZED|CO_NEWLOCALS, __pyx_empty_bytes, __pyx_empty_tuple, __pyx_empty_tuple, __pyx_tuple__34, __pyx_empty_tuple, __pyx_empty_tuple, __pyx_kp_s_river_metrics_confusion_pyx, __pyx_n_s_reset, 348, __pyx_empty_bytes); if (unlikely(!__pyx_codeobj__35)) __PYX_ERR(0, 348, __pyx_L1_error)
 
   /* "(tree fragment)":1
  * def __reduce_cython__(self):             # <<<<<<<<<<<<<<
  *     cdef tuple state
  *     cdef object _dict
  */
   __pyx_tuple__36 = PyTuple_Pack(4, __pyx_n_s_self, __pyx_n_s_state, __pyx_n_s_dict_2, __pyx_n_s_use_setstate); if (unlikely(!__pyx_tuple__36)) __PYX_ERR(2, 1, __pyx_L1_error)
@@ -15331,23 +15338,23 @@
   #if PY_VERSION_HEX < 0x030800B1
   __pyx_type_5river_7metrics_9confusion___pyx_scope_struct_7_genexpr.tp_print = 0;
   #endif
   if ((CYTHON_USE_TYPE_SLOTS && CYTHON_USE_PYTYPE_LOOKUP) && likely(!__pyx_type_5river_7metrics_9confusion___pyx_scope_struct_7_genexpr.tp_dictoffset && __pyx_type_5river_7metrics_9confusion___pyx_scope_struct_7_genexpr.tp_getattro == PyObject_GenericGetAttr)) {
     __pyx_type_5river_7metrics_9confusion___pyx_scope_struct_7_genexpr.tp_getattro = __Pyx_PyObject_GenericGetAttrNoDict;
   }
   __pyx_ptype_5river_7metrics_9confusion___pyx_scope_struct_7_genexpr = &__pyx_type_5river_7metrics_9confusion___pyx_scope_struct_7_genexpr;
-  if (PyType_Ready(&__pyx_type_5river_7metrics_9confusion___pyx_scope_struct_8___repr__) < 0) __PYX_ERR(0, 352, __pyx_L1_error)
+  if (PyType_Ready(&__pyx_type_5river_7metrics_9confusion___pyx_scope_struct_8___repr__) < 0) __PYX_ERR(0, 354, __pyx_L1_error)
   #if PY_VERSION_HEX < 0x030800B1
   __pyx_type_5river_7metrics_9confusion___pyx_scope_struct_8___repr__.tp_print = 0;
   #endif
   if ((CYTHON_USE_TYPE_SLOTS && CYTHON_USE_PYTYPE_LOOKUP) && likely(!__pyx_type_5river_7metrics_9confusion___pyx_scope_struct_8___repr__.tp_dictoffset && __pyx_type_5river_7metrics_9confusion___pyx_scope_struct_8___repr__.tp_getattro == PyObject_GenericGetAttr)) {
     __pyx_type_5river_7metrics_9confusion___pyx_scope_struct_8___repr__.tp_getattro = __Pyx_PyObject_GenericGetAttrNoDict;
   }
   __pyx_ptype_5river_7metrics_9confusion___pyx_scope_struct_8___repr__ = &__pyx_type_5river_7metrics_9confusion___pyx_scope_struct_8___repr__;
-  if (PyType_Ready(&__pyx_type_5river_7metrics_9confusion___pyx_scope_struct_9_genexpr) < 0) __PYX_ERR(0, 360, __pyx_L1_error)
+  if (PyType_Ready(&__pyx_type_5river_7metrics_9confusion___pyx_scope_struct_9_genexpr) < 0) __PYX_ERR(0, 362, __pyx_L1_error)
   #if PY_VERSION_HEX < 0x030800B1
   __pyx_type_5river_7metrics_9confusion___pyx_scope_struct_9_genexpr.tp_print = 0;
   #endif
   if ((CYTHON_USE_TYPE_SLOTS && CYTHON_USE_PYTYPE_LOOKUP) && likely(!__pyx_type_5river_7metrics_9confusion___pyx_scope_struct_9_genexpr.tp_dictoffset && __pyx_type_5river_7metrics_9confusion___pyx_scope_struct_9_genexpr.tp_getattro == PyObject_GenericGetAttr)) {
     __pyx_type_5river_7metrics_9confusion___pyx_scope_struct_9_genexpr.tp_getattro = __Pyx_PyObject_GenericGetAttrNoDict;
   }
   __pyx_ptype_5river_7metrics_9confusion___pyx_scope_struct_9_genexpr = &__pyx_type_5river_7metrics_9confusion___pyx_scope_struct_9_genexpr;
@@ -15823,37 +15830,37 @@
  */
   __pyx_t_2 = __Pyx_CyFunction_New(&__pyx_mdef_5river_7metrics_9confusion_25MultiLabelConfusionMatrix_3update, __Pyx_CYFUNCTION_CCLASS, __pyx_n_s_MultiLabelConfusionMatrix_update, NULL, __pyx_n_s_river_metrics_confusion, __pyx_d, ((PyObject *)__pyx_codeobj__31)); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 247, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_2);
   if (PyDict_SetItem((PyObject *)__pyx_ptype_5river_7metrics_9confusion_MultiLabelConfusionMatrix->tp_dict, __pyx_n_s_update, __pyx_t_2) < 0) __PYX_ERR(0, 247, __pyx_L1_error)
   __Pyx_DECREF(__pyx_t_2); __pyx_t_2 = 0;
   PyType_Modified(__pyx_ptype_5river_7metrics_9confusion_MultiLabelConfusionMatrix);
 
-  /* "river/metrics/confusion.pyx":298
+  /* "river/metrics/confusion.pyx":300
  * 
  * 
  *     def revert(self, y_true, y_pred, sample_weight=1., correction=None):             # <<<<<<<<<<<<<<
  *         self.n_samples -= 1
  *         # Revert is equal to subtracting so we pass the negative sample_weight
  */
-  __pyx_t_2 = __Pyx_CyFunction_New(&__pyx_mdef_5river_7metrics_9confusion_25MultiLabelConfusionMatrix_5revert, __Pyx_CYFUNCTION_CCLASS, __pyx_n_s_MultiLabelConfusionMatrix_revert, NULL, __pyx_n_s_river_metrics_confusion, __pyx_d, ((PyObject *)__pyx_codeobj__33)); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 298, __pyx_L1_error)
+  __pyx_t_2 = __Pyx_CyFunction_New(&__pyx_mdef_5river_7metrics_9confusion_25MultiLabelConfusionMatrix_5revert, __Pyx_CYFUNCTION_CCLASS, __pyx_n_s_MultiLabelConfusionMatrix_revert, NULL, __pyx_n_s_river_metrics_confusion, __pyx_d, ((PyObject *)__pyx_codeobj__33)); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 300, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_2);
-  if (PyDict_SetItem((PyObject *)__pyx_ptype_5river_7metrics_9confusion_MultiLabelConfusionMatrix->tp_dict, __pyx_n_s_revert, __pyx_t_2) < 0) __PYX_ERR(0, 298, __pyx_L1_error)
+  if (PyDict_SetItem((PyObject *)__pyx_ptype_5river_7metrics_9confusion_MultiLabelConfusionMatrix->tp_dict, __pyx_n_s_revert, __pyx_t_2) < 0) __PYX_ERR(0, 300, __pyx_L1_error)
   __Pyx_DECREF(__pyx_t_2); __pyx_t_2 = 0;
   PyType_Modified(__pyx_ptype_5river_7metrics_9confusion_MultiLabelConfusionMatrix);
 
-  /* "river/metrics/confusion.pyx":346
+  /* "river/metrics/confusion.pyx":348
  *         return self.data.shape
  * 
  *     def reset(self):             # <<<<<<<<<<<<<<
  *         self.__init__(labels=self._init_labels)
  * 
  */
-  __pyx_t_2 = __Pyx_CyFunction_New(&__pyx_mdef_5river_7metrics_9confusion_25MultiLabelConfusionMatrix_9reset, __Pyx_CYFUNCTION_CCLASS, __pyx_n_s_MultiLabelConfusionMatrix_reset, NULL, __pyx_n_s_river_metrics_confusion, __pyx_d, ((PyObject *)__pyx_codeobj__35)); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 346, __pyx_L1_error)
+  __pyx_t_2 = __Pyx_CyFunction_New(&__pyx_mdef_5river_7metrics_9confusion_25MultiLabelConfusionMatrix_9reset, __Pyx_CYFUNCTION_CCLASS, __pyx_n_s_MultiLabelConfusionMatrix_reset, NULL, __pyx_n_s_river_metrics_confusion, __pyx_d, ((PyObject *)__pyx_codeobj__35)); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 348, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_2);
-  if (PyDict_SetItem((PyObject *)__pyx_ptype_5river_7metrics_9confusion_MultiLabelConfusionMatrix->tp_dict, __pyx_n_s_reset, __pyx_t_2) < 0) __PYX_ERR(0, 346, __pyx_L1_error)
+  if (PyDict_SetItem((PyObject *)__pyx_ptype_5river_7metrics_9confusion_MultiLabelConfusionMatrix->tp_dict, __pyx_n_s_reset, __pyx_t_2) < 0) __PYX_ERR(0, 348, __pyx_L1_error)
   __Pyx_DECREF(__pyx_t_2); __pyx_t_2 = 0;
   PyType_Modified(__pyx_ptype_5river_7metrics_9confusion_MultiLabelConfusionMatrix);
 
   /* "(tree fragment)":1
  * def __reduce_cython__(self):             # <<<<<<<<<<<<<<
  *     cdef tuple state
  *     cdef object _dict
@@ -15904,15 +15911,15 @@
  * 
  */
   __pyx_t_2 = __Pyx_PyDict_NewPresized(0); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 1, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_2);
   if (PyDict_SetItem(__pyx_d, __pyx_n_s_test, __pyx_t_2) < 0) __PYX_ERR(0, 1, __pyx_L1_error)
   __Pyx_DECREF(__pyx_t_2); __pyx_t_2 = 0;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":1016
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":1014
  * 
  * 
  * cdef inline NPY_DATETIMEUNIT get_datetime64_unit(object obj) nogil:             # <<<<<<<<<<<<<<
  *     """
  *     returns the unit part of the dtype for a numpy datetime64 object.
  */
```

### Comparing `river-0.8.0/river/metrics/confusion.pxd` & `river-0.9.0/river/metrics/confusion.pxd`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,47 +1,47 @@
-# See _confusion_matrix.pyx for implementation details.
-
-cdef class ConfusionMatrix:
-    # This confusion matrix is a 2D matrix of shape ``(n_classes, n_classes)``, corresponding
-    # to a single-target (binary and multi-class) classification task.
-
-    # Internal variables
-    cdef readonly set _init_classes             # Initial set of classes
-    cdef readonly double sum_diag               # Sum across the diagonal
-    cdef readonly sum_row                       # Sum per row
-    cdef readonly sum_col                       # Sum per column
-    cdef readonly data                          # The actual data (dictionary)
-    cdef readonly int n_samples                 # Number of samples seen
-    cdef readonly float total_weight            # Sum of sample_weights seen
-    cdef readonly last_y_true                   # Last y_true value seen
-    cdef readonly last_y_pred                   # Last y_pred value seen
-    cdef readonly sample_correction             # Used to apply corrections during revert
-    cdef readonly float weight_majority_classifier      # Correctly classified: majority class
-    cdef readonly float weight_no_change_classifier     # Correctly classified: no-change
-
-    # Methods
-    cdef _majority_class(self)
-
-cdef class MultiLabelConfusionMatrix:
-    # This confusion matrix corresponds to a 3D matrix of shape ``(n_labels, 2, 2)`` meaning
-    # that each ``label`` has a corresponding binary ``(2x2)`` confusion matrix.
-
-    # Internal variables
-    cdef readonly set _init_labels              # Initial set of labels
-    cdef readonly set labels                    # Set of labels
-    cdef readonly int n_labels                  # Number of labels
-    cdef readonly data                          # The actual data (3D np.ndarray)
-    cdef readonly dict _label_dict              # Dictionary to map labels and their label-index
-    cdef readonly int _label_idx_cnt            # Internal label-index counter
-    cdef readonly last_y_true                   # Last y_true value seen
-    cdef readonly last_y_pred                   # Last y_pred value seen
-    cdef readonly int n_samples                 # Number of samples seen
-    cdef readonly sample_correction             # Used to apply corrections during revert
-    cdef readonly int exact_match_cnt           # Exact match count
-    cdef readonly double precision_sum          # Precision sum
-    cdef readonly double recall_sum             # Recall sum
-    cdef readonly double jaccard_sum            # Jaccard-index sum
-
-    # Methods
-    cdef int _map_label(self, label, bint add_label)
-    cdef void _add_label(self, label)
-    cdef void _reshape(self)
+# See _confusion_matrix.pyx for implementation details.
+
+cdef class ConfusionMatrix:
+    # This confusion matrix is a 2D matrix of shape ``(n_classes, n_classes)``, corresponding
+    # to a single-target (binary and multi-class) classification task.
+
+    # Internal variables
+    cdef readonly set _init_classes             # Initial set of classes
+    cdef readonly double sum_diag               # Sum across the diagonal
+    cdef readonly sum_row                       # Sum per row
+    cdef readonly sum_col                       # Sum per column
+    cdef readonly data                          # The actual data (dictionary)
+    cdef readonly int n_samples                 # Number of samples seen
+    cdef readonly float total_weight            # Sum of sample_weights seen
+    cdef readonly last_y_true                   # Last y_true value seen
+    cdef readonly last_y_pred                   # Last y_pred value seen
+    cdef readonly sample_correction             # Used to apply corrections during revert
+    cdef readonly float weight_majority_classifier      # Correctly classified: majority class
+    cdef readonly float weight_no_change_classifier     # Correctly classified: no-change
+
+    # Methods
+    cdef _majority_class(self)
+
+cdef class MultiLabelConfusionMatrix:
+    # This confusion matrix corresponds to a 3D matrix of shape ``(n_labels, 2, 2)`` meaning
+    # that each ``label`` has a corresponding binary ``(2x2)`` confusion matrix.
+
+    # Internal variables
+    cdef readonly set _init_labels              # Initial set of labels
+    cdef readonly set labels                    # Set of labels
+    cdef readonly int n_labels                  # Number of labels
+    cdef readonly data                          # The actual data (3D np.ndarray)
+    cdef readonly dict _label_dict              # Dictionary to map labels and their label-index
+    cdef readonly int _label_idx_cnt            # Internal label-index counter
+    cdef readonly last_y_true                   # Last y_true value seen
+    cdef readonly last_y_pred                   # Last y_pred value seen
+    cdef readonly int n_samples                 # Number of samples seen
+    cdef readonly sample_correction             # Used to apply corrections during revert
+    cdef readonly int exact_match_cnt           # Exact match count
+    cdef readonly double precision_sum          # Precision sum
+    cdef readonly double recall_sum             # Recall sum
+    cdef readonly double jaccard_sum            # Jaccard-index sum
+
+    # Methods
+    cdef int _map_label(self, label, bint add_label)
+    cdef void _add_label(self, label)
+    cdef void _reshape(self)
```

### Comparing `river-0.8.0/river/metrics/confusion.pyx` & `river-0.9.0/river/metrics/confusion.pyx`

 * *Files 15% similar despite different names*

```diff
@@ -1,383 +1,385 @@
-import functools
-from collections import defaultdict
-
-import numpy as np
-
-cimport cython
-cimport numpy as np
-
-
-cdef class ConfusionMatrix:
-    """Confusion Matrix for binary-class and multi-class classification.
-
-    Parameters
-    ----------
-    classes
-        The initial set of classes.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = ['cat', 'ant', 'cat', 'cat', 'ant', 'bird']
-    >>> y_pred = ['ant', 'ant', 'cat', 'cat', 'ant', 'cat']
-
-    >>> cm = metrics.ConfusionMatrix()
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     cm = cm.update(yt, yp)
-
-    >>> cm
-           ant  bird   cat
-     ant     2     0     0
-    bird     0     0     1
-     cat     1     0     2
-
-    >>> cm['bird']['cat']
-    1.0
-
-    Notes
-    -----
-    This confusion matrix is a 2D matrix of shape `(n_classes, n_classes)`, corresponding
-    to a single-target (binary and multi-class) classification task.
-
-    Each row represents `true` (actual) class-labels, while each column corresponds
-    to the `predicted` class-labels. For example, an entry in position `[1, 2]` means
-    that the true class-label is 1, and the predicted class-label is 2 (incorrect prediction).
-
-    This structure is used to keep updated statistics about a single-output classifier's
-    performance and to compute multiple evaluation metrics.
-
-    """
-
-    _fmt = '0.0f'
-
-    def __init__(self, classes=None):
-        self._init_classes = set(classes) if classes is not None else set()
-        self.sum_diag = 0.0
-        self.sum_row = defaultdict(float)
-        self.sum_col = defaultdict(float)
-        self.data = defaultdict(functools.partial(defaultdict, float))
-        self.n_samples = 0
-        self.total_weight = 0
-        # Auxiliary variables
-        self.last_y_true = 0
-        self.last_y_pred = 0
-        self.sample_correction = dict()             # Used to apply corrections during revert
-        self.weight_majority_classifier = 0.        # Correctly classified: majority class
-        self.weight_no_change_classifier = 0.       # Correctly classified: no-change
-
-    def __getitem__(self, key):
-        """Syntactic sugar for accessing the counts directly."""
-        return self.data[key]
-
-    def update(self, y_true, y_pred, sample_weight=1.):
-        self.n_samples += 1
-        self.total_weight += sample_weight
-        self._update_matrix(y_true, y_pred, sample_weight)
-        self.sample_correction = dict()
-
-        self.sample_correction['MCC'] = 0           # MCC: majority-class classifier correction
-        if self.majority_class == y_true:
-            self.weight_majority_classifier += sample_weight
-            self.sample_correction['MCC'] = 1
-
-        self.sample_correction['NCC'] = 0           # NCC: no-change classifier correction
-        if self.last_y_true == y_true:
-            self.weight_no_change_classifier += sample_weight
-            self.sample_correction['NCC'] = 1
-
-        # Keep track of last entry
-        self.last_y_true = y_true
-        self.last_y_pred = y_pred
-
-        return self
-
-    def _update_matrix(self, y_true, y_pred, sample_weight=1.):
-        self.data[y_true][y_pred] += sample_weight
-
-        if y_true == y_pred:
-            self.sum_diag += sample_weight
-        self.sum_row[y_true] += sample_weight
-        self.sum_col[y_pred] += sample_weight
-
-        return self
-
-    def revert(self, y_true, y_pred, sample_weight=1., correction=None):
-        self.n_samples -= 1
-        self.total_weight -= sample_weight
-        # Revert is equal to subtracting so we pass the negative sample_weight
-        self._update_matrix(y_true, y_pred, -sample_weight)
-
-        if correction['MCC'] == 1:
-            self.weight_majority_classifier -= sample_weight
-
-        if correction['NCC'] == 1:
-            self.weight_no_change_classifier -= sample_weight
-
-        return self
-
-    @property
-    def classes(self):
-        return list(
-            set(c for c, n in self.sum_row.items() if n) |
-            set(c for c, n in self.sum_col.items() if n)
-        )
-
-    @property
-    def n_classes(self):
-        return len(self.classes)
-
-    @property
-    def shape(self):
-        return self.n_classes, self.n_classes
-
-    def reset(self):
-        self.__init__(classes=self._init_classes)
-
-    def __repr__(self):
-
-        # The classes are sorted alphabetically for reproducibility reasons
-        classes = sorted(self.classes)
-
-        if not classes:
-            return ''
-
-        # Determine the required width of each column in the table
-        largest_label_len = max(len(str(c)) for c in classes)
-        largest_number_len = len(str(max(max(v for v in c.values()) for c in self.data.values())))
-        width = max(largest_label_len, largest_number_len) + 2
-
-        # Make a template to print out rows one by one
-        row_format = '{:>{width}}' * (len(classes) + 1)
-
-        # Write down the header
-        table = row_format.format('', *map(str, classes), width=width) + '\n'
-
-        # Write down the true labels row by row
-        table += '\n'.join((
-            row_format.format(
-                str(y_true),
-                *[f'{self.data[y_true][y_pred]:{self._fmt}}' for y_pred in classes],
-                width=width
-            )
-            for y_true in classes
-        ))
-
-        return table
-
-    @cython.boundscheck(False)  # Deactivate bounds checking
-    @cython.wraparound(False)   # Deactivate negative indexing.
-    cdef _majority_class(self):
-        majority_class = 0
-        cdef double max_value = 0.0
-        cdef double max_proba_class = 0.0
-        classes_str = list(map(str, self.classes))
-        sorted_labels_idx = np.argsort(classes_str)
-        for idx in sorted_labels_idx:
-            class_label = self.classes[idx]
-            max_proba_class = self.sum_row[class_label] / self.n_samples
-            if max_proba_class > max_value:
-                max_value = max_proba_class
-                majority_class = class_label
-        return majority_class
-
-    @property
-    def majority_class(self):
-        return self._majority_class()
-
-    def true_positives(self, label):
-        return self.data[label][label]
-
-    def true_negatives(self, label):
-        return self.sum_diag - self.data[label][label]
-
-    def false_positives(self, label):
-        return self.sum_col[label] - self.data[label][label]
-
-    def false_negatives(self, label):
-        return self.sum_row[label] - self.data[label][label]
-
-
-cdef class MultiLabelConfusionMatrix:
-    """Multi-label Confusion Matrix.
-
-    Notes
-    -----
-    This confusion matrix corresponds to a 3D matrix of shape `(n_labels, 2, 2)` meaning
-    that each `label` has a corresponding binary `(2x2)` confusion matrix.
-
-    The first dimension corresponds to the `label`, the second and third dimensions
-    are binary indicators for the `true` (actual) vs `predicted` values. For example,
-    an entry in position `[2, 0, 1]` represents a miss-classification of label 2.
-
-    This structure is used to keep updated statistics about a multi-output classifier's
-    performance and to compute multiple evaluation metrics.
-
-    Parameters
-    ----------
-    labels: set, list, optional, (default=None)
-        The set of (initial) labels.
-
-    """
-    def __init__(self, labels=None):
-        self._init_labels = set() if labels is None else set(labels)
-        self.labels = self._init_labels
-        self.n_labels = len(self.labels)
-        if self.n_labels > 2:
-            self.data = np.zeros((self.n_labels, 2, 2))
-        else:
-            # default to 2 labels
-            self.data = np.zeros((2, 2, 2))
-        self._label_dict = dict()
-        self._label_idx_cnt = 0
-        for label in self.labels:
-            self._add_label(label)
-        self.n_samples = 0
-        # Auxiliary variables
-        self.last_y_true = 0
-        self.last_y_pred = 0
-        self.sample_correction = dict()             # Used to apply corrections during revert
-        self.exact_match_cnt = 0                    # Exact match count
-        self.jaccard_sum = 0.                       # Jaccard-index sum
-        self.precision_sum = 0.                     # Precision sum
-        self.recall_sum = 0.                        # Recall sum
-
-    def update(self, y_true, y_pred, sample_weight=1.0):
-
-        cdef int is_equal = 1
-        cdef double inter_cnt = 0.
-        cdef double union_cnt = 0.
-        cdef double ones_true_cnt = 0.
-        cdef double ones_pred_cnt = 0.
-        cdef double val = 0.
-        self.sample_correction = dict()
-
-        if not y_pred:
-            # Corner case where the predictions are empty, e.g. if the model is empty.
-            return
-        # Increase sample count, negative sample_weight indicates that we are removing samples
-        self.n_samples += 1
-
-        for label in y_true.keys():
-            label_idx = self._map_label(label, add_label=True)
-            self.data[label_idx, y_true[label], y_pred[label]] += sample_weight
-            if y_true[label] != y_pred[label]:
-                is_equal = 0        # Not equal
-            inter_cnt += float(y_true[label] and y_pred[label])
-            union_cnt += float(y_true[label] or y_pred[label])
-            if y_true[label] == 1:
-                ones_true_cnt += 1.
-            if y_pred[label] == 1:
-                ones_pred_cnt += 1.
-
-
-        # Update auxiliary variables
-        # Exact match
-        self.sample_correction['IS_EQUAL'] = is_equal   # IS_EQUAL: exact match correction
-        self.exact_match_cnt += is_equal
-        # Example-based precision
-        val = (inter_cnt / ones_pred_cnt) if ones_pred_cnt > 0. else 0.
-        self.precision_sum += val
-        self.sample_correction['P_SUM'] = val           # P_SUM: precision sum correction
-        # Example-based recall
-        val = (inter_cnt / ones_true_cnt) if ones_true_cnt > 0. else 0.
-        self.recall_sum += val
-        self.sample_correction['R_SUM'] = val           # R_SUM: recall sum correction
-        # Jaccard-index
-        val = (inter_cnt / union_cnt) if union_cnt > 0. else 0.
-        self.jaccard_sum += val
-        self.sample_correction['J_SUM'] = val           # J_SUM: jaccard sum correction
-
-        # Keep track of last entry
-        self.last_y_true = y_true
-        self.last_y_pred = y_pred
-
-
-    def revert(self, y_true, y_pred, sample_weight=1., correction=None):
-        self.n_samples -= 1
-        # Revert is equal to subtracting so we pass the negative sample_weight
-        for label in y_true.keys():
-            label_idx = self._map_label(label, add_label=True)
-            self.data[label_idx, y_true[label], y_pred[label]] += -sample_weight
-
-        # Update auxiliary variables
-        self.exact_match_cnt -= correction['IS_EQUAL']
-        self.precision_sum -= correction['P_SUM']
-        self.recall_sum -= correction['R_SUM']
-        self.jaccard_sum -= correction['J_SUM']
-
-        return self
-
-    def __getitem__(self, label):
-        if label in self.labels:
-            label_idx = self._map_label(label, add_label=False)
-            return self.data[label_idx]
-        raise KeyError(f'Unknown label: {label}')
-
-    cdef int _map_label(self, label, bint add_label):
-        try:
-            label_key = self._label_dict[label]
-        except KeyError:
-            if add_label:
-                self._add_label(label)
-                label_key = self._label_dict[label]
-            else:
-                label_key = -1
-                raise KeyError(f'Unknown label: {label}')
-        return label_key
-
-    cdef void _add_label(self, label):
-        self._label_dict[label] = self._label_idx_cnt
-        if self._label_idx_cnt > self.data.shape[0] - 1:
-            self._reshape()
-        self._label_idx_cnt += 1
-        self.labels.add(label)
-        self.n_labels = len(self.labels)
-
-    cdef void _reshape(self):
-        self.data = np.vstack((self.data, np.zeros((1, 2, 2))))
-
-    @property
-    def shape(self):
-        return self.data.shape
-
-    def reset(self):
-        self.__init__(labels=self._init_labels)
-
-    def __str__(self):
-        return self.data.__str__()
-
-    def __repr__(self):
-        # The labels are sorted alphabetically for reproducibility reasons
-        labels = sorted(self.labels)
-
-        if not labels:
-            return  ''
-
-        # Determine the required width of each column in the table
-        largest_label_len = max(len(str(label)) for label in labels)
-        largest_value_len = len(str(self.data[:].max()))
-        width = max(5, largest_label_len, largest_value_len) + 2   # Min value is 5=len('label')
-
-        # Make a template to print out rows one by one
-        row_format = '{:>{width}}' * 5    # Label, TP, FP, FN, TN
-
-        # Write down the header
-        table = row_format.format('Label', 'TP', 'FP', 'FN', 'TN', width=width) + '\n'
-
-        # Write down the values per labels row by row
-        for label in labels:
-            label_idx = self._map_label(label, add_label=False)
-            table += ''.join(
-                row_format.format(
-                    str(label),                         # Label
-                    self.data[label_idx][1][1],         # TP
-                    self.data[label_idx][0][1],         # FP
-                    self.data[label_idx][1][0],         # FN
-                    self.data[label_idx][0][0],         # TN
-                    width=width))
-            table += '\n'
-
-        return table
+import functools
+from collections import defaultdict
+
+import numpy as np
+
+cimport cython
+cimport numpy as np
+
+
+cdef class ConfusionMatrix:
+    """Confusion Matrix for binary-class and multi-class classification.
+
+    Parameters
+    ----------
+    classes
+        The initial set of classes.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = ['cat', 'ant', 'cat', 'cat', 'ant', 'bird']
+    >>> y_pred = ['ant', 'ant', 'cat', 'cat', 'ant', 'cat']
+
+    >>> cm = metrics.ConfusionMatrix()
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     cm = cm.update(yt, yp)
+
+    >>> cm
+           ant  bird   cat
+     ant     2     0     0
+    bird     0     0     1
+     cat     1     0     2
+
+    >>> cm['bird']['cat']
+    1.0
+
+    Notes
+    -----
+    This confusion matrix is a 2D matrix of shape `(n_classes, n_classes)`, corresponding
+    to a single-target (binary and multi-class) classification task.
+
+    Each row represents `true` (actual) class-labels, while each column corresponds
+    to the `predicted` class-labels. For example, an entry in position `[1, 2]` means
+    that the true class-label is 1, and the predicted class-label is 2 (incorrect prediction).
+
+    This structure is used to keep updated statistics about a single-output classifier's
+    performance and to compute multiple evaluation metrics.
+
+    """
+
+    _fmt = '0.0f'
+
+    def __init__(self, classes=None):
+        self._init_classes = set(classes) if classes is not None else set()
+        self.sum_diag = 0.0
+        self.sum_row = defaultdict(float)
+        self.sum_col = defaultdict(float)
+        self.data = defaultdict(functools.partial(defaultdict, float))
+        self.n_samples = 0
+        self.total_weight = 0
+        # Auxiliary variables
+        self.last_y_true = 0
+        self.last_y_pred = 0
+        self.sample_correction = dict()             # Used to apply corrections during revert
+        self.weight_majority_classifier = 0.        # Correctly classified: majority class
+        self.weight_no_change_classifier = 0.       # Correctly classified: no-change
+
+    def __getitem__(self, key):
+        """Syntactic sugar for accessing the counts directly."""
+        return self.data[key]
+
+    def update(self, y_true, y_pred, sample_weight=1.):
+        self.n_samples += 1
+        self.total_weight += sample_weight
+        self._update_matrix(y_true, y_pred, sample_weight)
+        self.sample_correction = dict()
+
+        self.sample_correction['MCC'] = 0           # MCC: majority-class classifier correction
+        if self.majority_class == y_true:
+            self.weight_majority_classifier += sample_weight
+            self.sample_correction['MCC'] = 1
+
+        self.sample_correction['NCC'] = 0           # NCC: no-change classifier correction
+        if self.last_y_true == y_true:
+            self.weight_no_change_classifier += sample_weight
+            self.sample_correction['NCC'] = 1
+
+        # Keep track of last entry
+        self.last_y_true = y_true
+        self.last_y_pred = y_pred
+
+        return self
+
+    def _update_matrix(self, y_true, y_pred, sample_weight=1.):
+        self.data[y_true][y_pred] += sample_weight
+
+        if y_true == y_pred:
+            self.sum_diag += sample_weight
+        self.sum_row[y_true] += sample_weight
+        self.sum_col[y_pred] += sample_weight
+
+        return self
+
+    def revert(self, y_true, y_pred, sample_weight=1., correction=None):
+        self.n_samples -= 1
+        self.total_weight -= sample_weight
+        # Revert is equal to subtracting so we pass the negative sample_weight
+        self._update_matrix(y_true, y_pred, -sample_weight)
+
+        if correction['MCC'] == 1:
+            self.weight_majority_classifier -= sample_weight
+
+        if correction['NCC'] == 1:
+            self.weight_no_change_classifier -= sample_weight
+
+        return self
+
+    @property
+    def classes(self):
+        return list(
+            set(c for c, n in self.sum_row.items() if n) |
+            set(c for c, n in self.sum_col.items() if n)
+        )
+
+    @property
+    def n_classes(self):
+        return len(self.classes)
+
+    @property
+    def shape(self):
+        return self.n_classes, self.n_classes
+
+    def reset(self):
+        self.__init__(classes=self._init_classes)
+
+    def __repr__(self):
+
+        # The classes are sorted alphabetically for reproducibility reasons
+        classes = sorted(self.classes)
+
+        if not classes:
+            return ''
+
+        # Determine the required width of each column in the table
+        largest_label_len = max(len(str(c)) for c in classes)
+        largest_number_len = len(str(max(max(v for v in c.values()) for c in self.data.values())))
+        width = max(largest_label_len, largest_number_len) + 2
+
+        # Make a template to print out rows one by one
+        row_format = '{:>{width}}' * (len(classes) + 1)
+
+        # Write down the header
+        table = row_format.format('', *map(str, classes), width=width) + '\n'
+
+        # Write down the true labels row by row
+        table += '\n'.join((
+            row_format.format(
+                str(y_true),
+                *[f'{self.data[y_true][y_pred]:{self._fmt}}' for y_pred in classes],
+                width=width
+            )
+            for y_true in classes
+        ))
+
+        return table
+
+    @cython.boundscheck(False)  # Deactivate bounds checking
+    @cython.wraparound(False)   # Deactivate negative indexing.
+    cdef _majority_class(self):
+        majority_class = 0
+        cdef double max_value = 0.0
+        cdef double max_proba_class = 0.0
+        classes_str = list(map(str, self.classes))
+        sorted_labels_idx = np.argsort(classes_str)
+        for idx in sorted_labels_idx:
+            class_label = self.classes[idx]
+            max_proba_class = self.sum_row[class_label] / self.n_samples
+            if max_proba_class > max_value:
+                max_value = max_proba_class
+                majority_class = class_label
+        return majority_class
+
+    @property
+    def majority_class(self):
+        return self._majority_class()
+
+    def true_positives(self, label):
+        return self.data[label][label]
+
+    def true_negatives(self, label):
+        return self.sum_diag - self.data[label][label]
+
+    def false_positives(self, label):
+        return self.sum_col[label] - self.data[label][label]
+
+    def false_negatives(self, label):
+        return self.sum_row[label] - self.data[label][label]
+
+
+cdef class MultiLabelConfusionMatrix:
+    """Multi-label Confusion Matrix.
+
+    Notes
+    -----
+    This confusion matrix corresponds to a 3D matrix of shape `(n_labels, 2, 2)` meaning
+    that each `label` has a corresponding binary `(2x2)` confusion matrix.
+
+    The first dimension corresponds to the `label`, the second and third dimensions
+    are binary indicators for the `true` (actual) vs `predicted` values. For example,
+    an entry in position `[2, 0, 1]` represents a miss-classification of label 2.
+
+    This structure is used to keep updated statistics about a multi-output classifier's
+    performance and to compute multiple evaluation metrics.
+
+    Parameters
+    ----------
+    labels: set, list, optional, (default=None)
+        The set of (initial) labels.
+
+    """
+    def __init__(self, labels=None):
+        self._init_labels = set() if labels is None else set(labels)
+        self.labels = self._init_labels
+        self.n_labels = len(self.labels)
+        if self.n_labels > 2:
+            self.data = np.zeros((self.n_labels, 2, 2))
+        else:
+            # default to 2 labels
+            self.data = np.zeros((2, 2, 2))
+        self._label_dict = dict()
+        self._label_idx_cnt = 0
+        for label in self.labels:
+            self._add_label(label)
+        self.n_samples = 0
+        # Auxiliary variables
+        self.last_y_true = 0
+        self.last_y_pred = 0
+        self.sample_correction = dict()             # Used to apply corrections during revert
+        self.exact_match_cnt = 0                    # Exact match count
+        self.jaccard_sum = 0.                       # Jaccard-index sum
+        self.precision_sum = 0.                     # Precision sum
+        self.recall_sum = 0.                        # Recall sum
+
+    def update(self, y_true, y_pred, sample_weight=1.0):
+
+        cdef int is_equal = 1
+        cdef double inter_cnt = 0.
+        cdef double union_cnt = 0.
+        cdef double ones_true_cnt = 0.
+        cdef double ones_pred_cnt = 0.
+        cdef double val = 0.
+        self.sample_correction = dict()
+
+        if not y_pred:
+            # Corner case where the predictions are empty, e.g. if the model is empty.
+            return
+        # Increase sample count, negative sample_weight indicates that we are removing samples
+        self.n_samples += 1
+
+        for label in y_true.keys():
+            label_idx = self._map_label(label, add_label=True)
+            self.data[label_idx, y_true[label], y_pred[label]] += sample_weight
+            if y_true[label] != y_pred[label]:
+                is_equal = 0        # Not equal
+            inter_cnt += float(y_true[label] and y_pred[label])
+            union_cnt += float(y_true[label] or y_pred[label])
+            if y_true[label] == 1:
+                ones_true_cnt += 1.
+            if y_pred[label] == 1:
+                ones_pred_cnt += 1.
+
+
+        # Update auxiliary variables
+        # Exact match
+        self.sample_correction['IS_EQUAL'] = is_equal   # IS_EQUAL: exact match correction
+        self.exact_match_cnt += is_equal
+        # Example-based precision
+        val = (inter_cnt / ones_pred_cnt) if ones_pred_cnt > 0. else 0.
+        self.precision_sum += val
+        self.sample_correction['P_SUM'] = val           # P_SUM: precision sum correction
+        # Example-based recall
+        val = (inter_cnt / ones_true_cnt) if ones_true_cnt > 0. else 0.
+        self.recall_sum += val
+        self.sample_correction['R_SUM'] = val           # R_SUM: recall sum correction
+        # Jaccard-index
+        val = (inter_cnt / union_cnt) if union_cnt > 0. else 0.
+        self.jaccard_sum += val
+        self.sample_correction['J_SUM'] = val           # J_SUM: jaccard sum correction
+
+        # Keep track of last entry
+        self.last_y_true = y_true
+        self.last_y_pred = y_pred
+
+        return self
+
+
+    def revert(self, y_true, y_pred, sample_weight=1., correction=None):
+        self.n_samples -= 1
+        # Revert is equal to subtracting so we pass the negative sample_weight
+        for label in y_true.keys():
+            label_idx = self._map_label(label, add_label=True)
+            self.data[label_idx, y_true[label], y_pred[label]] += -sample_weight
+
+        # Update auxiliary variables
+        self.exact_match_cnt -= correction['IS_EQUAL']
+        self.precision_sum -= correction['P_SUM']
+        self.recall_sum -= correction['R_SUM']
+        self.jaccard_sum -= correction['J_SUM']
+
+        return self
+
+    def __getitem__(self, label):
+        if label in self.labels:
+            label_idx = self._map_label(label, add_label=False)
+            return self.data[label_idx]
+        raise KeyError(f'Unknown label: {label}')
+
+    cdef int _map_label(self, label, bint add_label):
+        try:
+            label_key = self._label_dict[label]
+        except KeyError:
+            if add_label:
+                self._add_label(label)
+                label_key = self._label_dict[label]
+            else:
+                label_key = -1
+                raise KeyError(f'Unknown label: {label}')
+        return label_key
+
+    cdef void _add_label(self, label):
+        self._label_dict[label] = self._label_idx_cnt
+        if self._label_idx_cnt > self.data.shape[0] - 1:
+            self._reshape()
+        self._label_idx_cnt += 1
+        self.labels.add(label)
+        self.n_labels = len(self.labels)
+
+    cdef void _reshape(self):
+        self.data = np.vstack((self.data, np.zeros((1, 2, 2))))
+
+    @property
+    def shape(self):
+        return self.data.shape
+
+    def reset(self):
+        self.__init__(labels=self._init_labels)
+
+    def __str__(self):
+        return self.data.__str__()
+
+    def __repr__(self):
+        # The labels are sorted alphabetically for reproducibility reasons
+        labels = sorted(self.labels)
+
+        if not labels:
+            return  ''
+
+        # Determine the required width of each column in the table
+        largest_label_len = max(len(str(label)) for label in labels)
+        largest_value_len = len(str(self.data[:].max()))
+        width = max(5, largest_label_len, largest_value_len) + 2   # Min value is 5=len('label')
+
+        # Make a template to print out rows one by one
+        row_format = '{:>{width}}' * 5    # Label, TP, FP, FN, TN
+
+        # Write down the header
+        table = row_format.format('Label', 'TP', 'FP', 'FN', 'TN', width=width) + '\n'
+
+        # Write down the values per labels row by row
+        for label in labels:
+            label_idx = self._map_label(label, add_label=False)
+            table += ''.join(
+                row_format.format(
+                    str(label),                         # Label
+                    self.data[label_idx][1][1],         # TP
+                    self.data[label_idx][0][1],         # FP
+                    self.data[label_idx][1][0],         # FN
+                    self.data[label_idx][0][0],         # TN
+                    width=width))
+            table += '\n'
+
+        return table
```

### Comparing `river-0.8.0/river/metrics/exact_match.py` & `river-0.9.0/river/metrics/exact_match.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,58 +1,58 @@
-from . import base
-
-__all__ = ["ExactMatch"]
-
-
-class ExactMatch(base.MultiOutputClassificationMetric):
-    """Exact match score.
-
-    This is the most strict multi-label metric, defined as the number of
-    samples that have all their labels correctly classified, divided by the
-    total number of samples.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
-        confusion matrix reduces the amount of storage and computation time.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [
-    ...     {0: False, 1: True, 2: True},
-    ...     {0: True, 1: True, 2: False},
-    ...     {0: True, 1: True, 2: False},
-    ... ]
-
-    >>> y_pred = [
-    ...     {0: True, 1: True, 2: True},
-    ...     {0: True, 1: False, 2: False},
-    ...     {0: True, 1: True, 2: False},
-    ... ]
-
-    >>> metric = metrics.ExactMatch()
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     metric = metric.update(yt, yp)
-
-    >>> metric
-    ExactMatch: 0.333333
-
-    """
-
-    @property
-    def bigger_is_better(self):
-        return True
-
-    @property
-    def requires_labels(self):
-        return True
-
-    def get(self):
-
-        try:
-            return self.cm.exact_match_cnt / self.cm.n_samples
-        except ZeroDivisionError:
-            return 0.0
+from . import base
+
+__all__ = ["ExactMatch"]
+
+
+class ExactMatch(base.MultiOutputClassificationMetric):
+    """Exact match score.
+
+    This is the most strict multi-label metric, defined as the number of
+    samples that have all their labels correctly classified, divided by the
+    total number of samples.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
+        confusion matrix reduces the amount of storage and computation time.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [
+    ...     {0: False, 1: True, 2: True},
+    ...     {0: True, 1: True, 2: False},
+    ...     {0: True, 1: True, 2: False},
+    ... ]
+
+    >>> y_pred = [
+    ...     {0: True, 1: True, 2: True},
+    ...     {0: True, 1: False, 2: False},
+    ...     {0: True, 1: True, 2: False},
+    ... ]
+
+    >>> metric = metrics.ExactMatch()
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     metric = metric.update(yt, yp)
+
+    >>> metric
+    ExactMatch: 0.333333
+
+    """
+
+    @property
+    def bigger_is_better(self):
+        return True
+
+    @property
+    def requires_labels(self):
+        return True
+
+    def get(self):
+
+        try:
+            return self.cm.exact_match_cnt / self.cm.n_samples
+        except ZeroDivisionError:
+            return 0.0
```

### Comparing `river-0.8.0/river/metrics/expected_mutual_info.c` & `river-0.9.0/river/metrics/expected_mutual_info.c`

 * *Files 4% similar despite different names*

```diff
@@ -6,29 +6,26 @@
         "define_macros": [
             [
                 "NPY_NO_DEPRECATED_API",
                 "NPY_1_7_API_VERSION"
             ]
         ],
         "depends": [
-            "/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/core/include/numpy/arrayobject.h",
-            "/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/core/include/numpy/arrayscalars.h",
-            "/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/core/include/numpy/ndarrayobject.h",
-            "/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/core/include/numpy/ndarraytypes.h",
-            "/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/core/include/numpy/ufuncobject.h"
+            "C:\\cibw\\python\\python.3.9.9\\tools\\lib\\site-packages\\numpy\\core\\include\\numpy\\arrayobject.h",
+            "C:\\cibw\\python\\python.3.9.9\\tools\\lib\\site-packages\\numpy\\core\\include\\numpy\\arrayscalars.h",
+            "C:\\cibw\\python\\python.3.9.9\\tools\\lib\\site-packages\\numpy\\core\\include\\numpy\\ndarrayobject.h",
+            "C:\\cibw\\python\\python.3.9.9\\tools\\lib\\site-packages\\numpy\\core\\include\\numpy\\ndarraytypes.h",
+            "C:\\cibw\\python\\python.3.9.9\\tools\\lib\\site-packages\\numpy\\core\\include\\numpy\\ufuncobject.h"
         ],
         "include_dirs": [
-            "/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/core/include"
-        ],
-        "libraries": [
-            "m"
+            "C:\\cibw\\python\\python.3.9.9\\tools\\lib\\site-packages\\numpy\\core\\include"
         ],
         "name": "river.metrics.expected_mutual_info",
         "sources": [
-            "river/metrics/expected_mutual_info.pyx"
+            "river\\metrics\\expected_mutual_info.pyx"
         ]
     },
     "module_name": "river.metrics.expected_mutual_info"
 }
 END: Cython Metadata */
 
 #ifndef PY_SSIZE_T_CLEAN
@@ -883,15 +880,15 @@
 #if CYTHON_CCOMPLEX && !defined(__cplusplus) && defined(__sun__) && defined(__GNUC__)
   #undef _Complex_I
   #define _Complex_I 1.0fj
 #endif
 
 
 static const char *__pyx_f[] = {
-  "river/metrics/expected_mutual_info.pyx",
+  "river\\metrics\\expected_mutual_info.pyx",
   "__init__.pxd",
   "type.pxd",
 };
 /* BufferFormatStructs.proto */
 #define IS_UNSIGNED(type) (((type) -1) > 0)
 struct __Pyx_StructField_;
 #define __PYX_BUF_FLAGS_PACKED_STRUCT (1 << 0)
@@ -924,195 +921,195 @@
   char enc_type;
   char new_packmode;
   char enc_packmode;
   char is_valid_array;
 } __Pyx_BufFmt_Context;
 
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":690
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":690
  * # in Cython to enable them only on the right systems.
  * 
  * ctypedef npy_int8       int8_t             # <<<<<<<<<<<<<<
  * ctypedef npy_int16      int16_t
  * ctypedef npy_int32      int32_t
  */
 typedef npy_int8 __pyx_t_5numpy_int8_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":691
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":691
  * 
  * ctypedef npy_int8       int8_t
  * ctypedef npy_int16      int16_t             # <<<<<<<<<<<<<<
  * ctypedef npy_int32      int32_t
  * ctypedef npy_int64      int64_t
  */
 typedef npy_int16 __pyx_t_5numpy_int16_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":692
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":692
  * ctypedef npy_int8       int8_t
  * ctypedef npy_int16      int16_t
  * ctypedef npy_int32      int32_t             # <<<<<<<<<<<<<<
  * ctypedef npy_int64      int64_t
  * #ctypedef npy_int96      int96_t
  */
 typedef npy_int32 __pyx_t_5numpy_int32_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":693
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":693
  * ctypedef npy_int16      int16_t
  * ctypedef npy_int32      int32_t
  * ctypedef npy_int64      int64_t             # <<<<<<<<<<<<<<
  * #ctypedef npy_int96      int96_t
  * #ctypedef npy_int128     int128_t
  */
 typedef npy_int64 __pyx_t_5numpy_int64_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":697
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":697
  * #ctypedef npy_int128     int128_t
  * 
  * ctypedef npy_uint8      uint8_t             # <<<<<<<<<<<<<<
  * ctypedef npy_uint16     uint16_t
  * ctypedef npy_uint32     uint32_t
  */
 typedef npy_uint8 __pyx_t_5numpy_uint8_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":698
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":698
  * 
  * ctypedef npy_uint8      uint8_t
  * ctypedef npy_uint16     uint16_t             # <<<<<<<<<<<<<<
  * ctypedef npy_uint32     uint32_t
  * ctypedef npy_uint64     uint64_t
  */
 typedef npy_uint16 __pyx_t_5numpy_uint16_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":699
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":699
  * ctypedef npy_uint8      uint8_t
  * ctypedef npy_uint16     uint16_t
  * ctypedef npy_uint32     uint32_t             # <<<<<<<<<<<<<<
  * ctypedef npy_uint64     uint64_t
  * #ctypedef npy_uint96     uint96_t
  */
 typedef npy_uint32 __pyx_t_5numpy_uint32_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":700
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":700
  * ctypedef npy_uint16     uint16_t
  * ctypedef npy_uint32     uint32_t
  * ctypedef npy_uint64     uint64_t             # <<<<<<<<<<<<<<
  * #ctypedef npy_uint96     uint96_t
  * #ctypedef npy_uint128    uint128_t
  */
 typedef npy_uint64 __pyx_t_5numpy_uint64_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":704
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":704
  * #ctypedef npy_uint128    uint128_t
  * 
  * ctypedef npy_float32    float32_t             # <<<<<<<<<<<<<<
  * ctypedef npy_float64    float64_t
  * #ctypedef npy_float80    float80_t
  */
 typedef npy_float32 __pyx_t_5numpy_float32_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":705
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":705
  * 
  * ctypedef npy_float32    float32_t
  * ctypedef npy_float64    float64_t             # <<<<<<<<<<<<<<
  * #ctypedef npy_float80    float80_t
  * #ctypedef npy_float128   float128_t
  */
 typedef npy_float64 __pyx_t_5numpy_float64_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":714
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":714
  * # The int types are mapped a bit surprising --
  * # numpy.int corresponds to 'l' and numpy.long to 'q'
  * ctypedef npy_long       int_t             # <<<<<<<<<<<<<<
  * ctypedef npy_longlong   long_t
  * ctypedef npy_longlong   longlong_t
  */
 typedef npy_long __pyx_t_5numpy_int_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":715
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":715
  * # numpy.int corresponds to 'l' and numpy.long to 'q'
  * ctypedef npy_long       int_t
  * ctypedef npy_longlong   long_t             # <<<<<<<<<<<<<<
  * ctypedef npy_longlong   longlong_t
  * 
  */
 typedef npy_longlong __pyx_t_5numpy_long_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":716
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":716
  * ctypedef npy_long       int_t
  * ctypedef npy_longlong   long_t
  * ctypedef npy_longlong   longlong_t             # <<<<<<<<<<<<<<
  * 
  * ctypedef npy_ulong      uint_t
  */
 typedef npy_longlong __pyx_t_5numpy_longlong_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":718
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":718
  * ctypedef npy_longlong   longlong_t
  * 
  * ctypedef npy_ulong      uint_t             # <<<<<<<<<<<<<<
  * ctypedef npy_ulonglong  ulong_t
  * ctypedef npy_ulonglong  ulonglong_t
  */
 typedef npy_ulong __pyx_t_5numpy_uint_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":719
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":719
  * 
  * ctypedef npy_ulong      uint_t
  * ctypedef npy_ulonglong  ulong_t             # <<<<<<<<<<<<<<
  * ctypedef npy_ulonglong  ulonglong_t
  * 
  */
 typedef npy_ulonglong __pyx_t_5numpy_ulong_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":720
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":720
  * ctypedef npy_ulong      uint_t
  * ctypedef npy_ulonglong  ulong_t
  * ctypedef npy_ulonglong  ulonglong_t             # <<<<<<<<<<<<<<
  * 
  * ctypedef npy_intp       intp_t
  */
 typedef npy_ulonglong __pyx_t_5numpy_ulonglong_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":722
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":722
  * ctypedef npy_ulonglong  ulonglong_t
  * 
  * ctypedef npy_intp       intp_t             # <<<<<<<<<<<<<<
  * ctypedef npy_uintp      uintp_t
  * 
  */
 typedef npy_intp __pyx_t_5numpy_intp_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":723
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":723
  * 
  * ctypedef npy_intp       intp_t
  * ctypedef npy_uintp      uintp_t             # <<<<<<<<<<<<<<
  * 
  * ctypedef npy_double     float_t
  */
 typedef npy_uintp __pyx_t_5numpy_uintp_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":725
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":725
  * ctypedef npy_uintp      uintp_t
  * 
  * ctypedef npy_double     float_t             # <<<<<<<<<<<<<<
  * ctypedef npy_double     double_t
  * ctypedef npy_longdouble longdouble_t
  */
 typedef npy_double __pyx_t_5numpy_float_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":726
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":726
  * 
  * ctypedef npy_double     float_t
  * ctypedef npy_double     double_t             # <<<<<<<<<<<<<<
  * ctypedef npy_longdouble longdouble_t
  * 
  */
 typedef npy_double __pyx_t_5numpy_double_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":727
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":727
  * ctypedef npy_double     float_t
  * ctypedef npy_double     double_t
  * ctypedef npy_longdouble longdouble_t             # <<<<<<<<<<<<<<
  * 
  * ctypedef npy_cfloat      cfloat_t
  */
 typedef npy_longdouble __pyx_t_5numpy_longdouble_t;
@@ -1148,42 +1145,42 @@
     typedef struct { double real, imag; } __pyx_t_double_complex;
 #endif
 static CYTHON_INLINE __pyx_t_double_complex __pyx_t_double_complex_from_parts(double, double);
 
 
 /*--- Type declarations ---*/
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":729
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":729
  * ctypedef npy_longdouble longdouble_t
  * 
  * ctypedef npy_cfloat      cfloat_t             # <<<<<<<<<<<<<<
  * ctypedef npy_cdouble     cdouble_t
  * ctypedef npy_clongdouble clongdouble_t
  */
 typedef npy_cfloat __pyx_t_5numpy_cfloat_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":730
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":730
  * 
  * ctypedef npy_cfloat      cfloat_t
  * ctypedef npy_cdouble     cdouble_t             # <<<<<<<<<<<<<<
  * ctypedef npy_clongdouble clongdouble_t
  * 
  */
 typedef npy_cdouble __pyx_t_5numpy_cdouble_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":731
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":731
  * ctypedef npy_cfloat      cfloat_t
  * ctypedef npy_cdouble     cdouble_t
  * ctypedef npy_clongdouble clongdouble_t             # <<<<<<<<<<<<<<
  * 
  * ctypedef npy_cdouble     complex_t
  */
 typedef npy_clongdouble __pyx_t_5numpy_clongdouble_t;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":733
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":733
  * ctypedef npy_clongdouble clongdouble_t
  * 
  * ctypedef npy_cdouble     complex_t             # <<<<<<<<<<<<<<
  * 
  * cdef inline object PyArray_MultiIterNew1(a):
  */
 typedef npy_cdouble __pyx_t_5numpy_complex_t;
@@ -1934,15 +1931,15 @@
 static const char __pyx_k_ImportError[] = "ImportError";
 static const char __pyx_k_scipy_special[] = "scipy.special";
 static const char __pyx_k_confusion_matrix[] = "confusion_matrix";
 static const char __pyx_k_cline_in_traceback[] = "cline_in_traceback";
 static const char __pyx_k_expected_mutual_info[] = "expected_mutual_info";
 static const char __pyx_k_numpy_core_multiarray_failed_to[] = "numpy.core.multiarray failed to import";
 static const char __pyx_k_numpy_core_umath_failed_to_impor[] = "numpy.core.umath failed to import";
-static const char __pyx_k_river_metrics_expected_mutual_in[] = "river/metrics/expected_mutual_info.pyx";
+static const char __pyx_k_river_metrics_expected_mutual_in[] = "river\\metrics\\expected_mutual_info.pyx";
 static const char __pyx_k_river_metrics_expected_mutual_in_2[] = "river.metrics.expected_mutual_info";
 static PyObject *__pyx_n_s_C;
 static PyObject *__pyx_n_s_ImportError;
 static PyObject *__pyx_n_s_N;
 static PyObject *__pyx_n_s_R;
 static PyObject *__pyx_n_s_T;
 static PyObject *__pyx_n_s_a;
@@ -3827,15 +3824,15 @@
 __Pyx_XDECREF(__pyx_8genexpr4__pyx_v_v);
 __Pyx_XDECREF(__pyx_8genexpr5__pyx_v_w);
 __Pyx_XGIVEREF(__pyx_r);
 __Pyx_RefNannyFinishContext();
 return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":735
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":735
  * ctypedef npy_cdouble     complex_t
  * 
  * cdef inline object PyArray_MultiIterNew1(a):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(1, <void*>a)
  * 
  */
 
@@ -3844,29 +3841,29 @@
 __Pyx_RefNannyDeclarations
 PyObject *__pyx_t_1 = NULL;
 int __pyx_lineno = 0;
 const char *__pyx_filename = NULL;
 int __pyx_clineno = 0;
 __Pyx_RefNannySetupContext("PyArray_MultiIterNew1", 0);
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":736
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":736
  * 
  * cdef inline object PyArray_MultiIterNew1(a):
  *     return PyArray_MultiIterNew(1, <void*>a)             # <<<<<<<<<<<<<<
  * 
  * cdef inline object PyArray_MultiIterNew2(a, b):
  */
 __Pyx_XDECREF(__pyx_r);
 __pyx_t_1 = PyArray_MultiIterNew(1, ((void *)__pyx_v_a)); if (unlikely(!__pyx_t_1)) __PYX_ERR(1, 736, __pyx_L1_error)
 __Pyx_GOTREF(__pyx_t_1);
 __pyx_r = __pyx_t_1;
 __pyx_t_1 = 0;
 goto __pyx_L0;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":735
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":735
  * ctypedef npy_cdouble     complex_t
  * 
  * cdef inline object PyArray_MultiIterNew1(a):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(1, <void*>a)
  * 
  */
 
@@ -3877,15 +3874,15 @@
 __pyx_r = 0;
 __pyx_L0:;
 __Pyx_XGIVEREF(__pyx_r);
 __Pyx_RefNannyFinishContext();
 return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":738
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":738
  *     return PyArray_MultiIterNew(1, <void*>a)
  * 
  * cdef inline object PyArray_MultiIterNew2(a, b):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(2, <void*>a, <void*>b)
  * 
  */
 
@@ -3894,29 +3891,29 @@
 __Pyx_RefNannyDeclarations
 PyObject *__pyx_t_1 = NULL;
 int __pyx_lineno = 0;
 const char *__pyx_filename = NULL;
 int __pyx_clineno = 0;
 __Pyx_RefNannySetupContext("PyArray_MultiIterNew2", 0);
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":739
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":739
  * 
  * cdef inline object PyArray_MultiIterNew2(a, b):
  *     return PyArray_MultiIterNew(2, <void*>a, <void*>b)             # <<<<<<<<<<<<<<
  * 
  * cdef inline object PyArray_MultiIterNew3(a, b, c):
  */
 __Pyx_XDECREF(__pyx_r);
 __pyx_t_1 = PyArray_MultiIterNew(2, ((void *)__pyx_v_a), ((void *)__pyx_v_b)); if (unlikely(!__pyx_t_1)) __PYX_ERR(1, 739, __pyx_L1_error)
 __Pyx_GOTREF(__pyx_t_1);
 __pyx_r = __pyx_t_1;
 __pyx_t_1 = 0;
 goto __pyx_L0;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":738
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":738
  *     return PyArray_MultiIterNew(1, <void*>a)
  * 
  * cdef inline object PyArray_MultiIterNew2(a, b):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(2, <void*>a, <void*>b)
  * 
  */
 
@@ -3927,15 +3924,15 @@
 __pyx_r = 0;
 __pyx_L0:;
 __Pyx_XGIVEREF(__pyx_r);
 __Pyx_RefNannyFinishContext();
 return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":741
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":741
  *     return PyArray_MultiIterNew(2, <void*>a, <void*>b)
  * 
  * cdef inline object PyArray_MultiIterNew3(a, b, c):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(3, <void*>a, <void*>b, <void*> c)
  * 
  */
 
@@ -3944,29 +3941,29 @@
 __Pyx_RefNannyDeclarations
 PyObject *__pyx_t_1 = NULL;
 int __pyx_lineno = 0;
 const char *__pyx_filename = NULL;
 int __pyx_clineno = 0;
 __Pyx_RefNannySetupContext("PyArray_MultiIterNew3", 0);
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":742
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":742
  * 
  * cdef inline object PyArray_MultiIterNew3(a, b, c):
  *     return PyArray_MultiIterNew(3, <void*>a, <void*>b, <void*> c)             # <<<<<<<<<<<<<<
  * 
  * cdef inline object PyArray_MultiIterNew4(a, b, c, d):
  */
 __Pyx_XDECREF(__pyx_r);
 __pyx_t_1 = PyArray_MultiIterNew(3, ((void *)__pyx_v_a), ((void *)__pyx_v_b), ((void *)__pyx_v_c)); if (unlikely(!__pyx_t_1)) __PYX_ERR(1, 742, __pyx_L1_error)
 __Pyx_GOTREF(__pyx_t_1);
 __pyx_r = __pyx_t_1;
 __pyx_t_1 = 0;
 goto __pyx_L0;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":741
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":741
  *     return PyArray_MultiIterNew(2, <void*>a, <void*>b)
  * 
  * cdef inline object PyArray_MultiIterNew3(a, b, c):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(3, <void*>a, <void*>b, <void*> c)
  * 
  */
 
@@ -3977,15 +3974,15 @@
 __pyx_r = 0;
 __pyx_L0:;
 __Pyx_XGIVEREF(__pyx_r);
 __Pyx_RefNannyFinishContext();
 return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":744
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":744
  *     return PyArray_MultiIterNew(3, <void*>a, <void*>b, <void*> c)
  * 
  * cdef inline object PyArray_MultiIterNew4(a, b, c, d):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(4, <void*>a, <void*>b, <void*>c, <void*> d)
  * 
  */
 
@@ -3994,29 +3991,29 @@
 __Pyx_RefNannyDeclarations
 PyObject *__pyx_t_1 = NULL;
 int __pyx_lineno = 0;
 const char *__pyx_filename = NULL;
 int __pyx_clineno = 0;
 __Pyx_RefNannySetupContext("PyArray_MultiIterNew4", 0);
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":745
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":745
  * 
  * cdef inline object PyArray_MultiIterNew4(a, b, c, d):
  *     return PyArray_MultiIterNew(4, <void*>a, <void*>b, <void*>c, <void*> d)             # <<<<<<<<<<<<<<
  * 
  * cdef inline object PyArray_MultiIterNew5(a, b, c, d, e):
  */
 __Pyx_XDECREF(__pyx_r);
 __pyx_t_1 = PyArray_MultiIterNew(4, ((void *)__pyx_v_a), ((void *)__pyx_v_b), ((void *)__pyx_v_c), ((void *)__pyx_v_d)); if (unlikely(!__pyx_t_1)) __PYX_ERR(1, 745, __pyx_L1_error)
 __Pyx_GOTREF(__pyx_t_1);
 __pyx_r = __pyx_t_1;
 __pyx_t_1 = 0;
 goto __pyx_L0;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":744
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":744
  *     return PyArray_MultiIterNew(3, <void*>a, <void*>b, <void*> c)
  * 
  * cdef inline object PyArray_MultiIterNew4(a, b, c, d):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(4, <void*>a, <void*>b, <void*>c, <void*> d)
  * 
  */
 
@@ -4027,15 +4024,15 @@
 __pyx_r = 0;
 __pyx_L0:;
 __Pyx_XGIVEREF(__pyx_r);
 __Pyx_RefNannyFinishContext();
 return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":747
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":747
  *     return PyArray_MultiIterNew(4, <void*>a, <void*>b, <void*>c, <void*> d)
  * 
  * cdef inline object PyArray_MultiIterNew5(a, b, c, d, e):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(5, <void*>a, <void*>b, <void*>c, <void*> d, <void*> e)
  * 
  */
 
@@ -4044,29 +4041,29 @@
 __Pyx_RefNannyDeclarations
 PyObject *__pyx_t_1 = NULL;
 int __pyx_lineno = 0;
 const char *__pyx_filename = NULL;
 int __pyx_clineno = 0;
 __Pyx_RefNannySetupContext("PyArray_MultiIterNew5", 0);
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":748
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":748
  * 
  * cdef inline object PyArray_MultiIterNew5(a, b, c, d, e):
  *     return PyArray_MultiIterNew(5, <void*>a, <void*>b, <void*>c, <void*> d, <void*> e)             # <<<<<<<<<<<<<<
  * 
  * cdef inline tuple PyDataType_SHAPE(dtype d):
  */
 __Pyx_XDECREF(__pyx_r);
 __pyx_t_1 = PyArray_MultiIterNew(5, ((void *)__pyx_v_a), ((void *)__pyx_v_b), ((void *)__pyx_v_c), ((void *)__pyx_v_d), ((void *)__pyx_v_e)); if (unlikely(!__pyx_t_1)) __PYX_ERR(1, 748, __pyx_L1_error)
 __Pyx_GOTREF(__pyx_t_1);
 __pyx_r = __pyx_t_1;
 __pyx_t_1 = 0;
 goto __pyx_L0;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":747
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":747
  *     return PyArray_MultiIterNew(4, <void*>a, <void*>b, <void*>c, <void*> d)
  * 
  * cdef inline object PyArray_MultiIterNew5(a, b, c, d, e):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(5, <void*>a, <void*>b, <void*>c, <void*> d, <void*> e)
  * 
  */
 
@@ -4077,212 +4074,212 @@
 __pyx_r = 0;
 __pyx_L0:;
 __Pyx_XGIVEREF(__pyx_r);
 __Pyx_RefNannyFinishContext();
 return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":750
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":750
  *     return PyArray_MultiIterNew(5, <void*>a, <void*>b, <void*>c, <void*> d, <void*> e)
  * 
  * cdef inline tuple PyDataType_SHAPE(dtype d):             # <<<<<<<<<<<<<<
  *     if PyDataType_HASSUBARRAY(d):
  *         return <tuple>d.subarray.shape
  */
 
 static CYTHON_INLINE PyObject *__pyx_f_5numpy_PyDataType_SHAPE(PyArray_Descr *__pyx_v_d) {
 PyObject *__pyx_r = NULL;
 __Pyx_RefNannyDeclarations
 int __pyx_t_1;
 __Pyx_RefNannySetupContext("PyDataType_SHAPE", 0);
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":751
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":751
  * 
  * cdef inline tuple PyDataType_SHAPE(dtype d):
  *     if PyDataType_HASSUBARRAY(d):             # <<<<<<<<<<<<<<
  *         return <tuple>d.subarray.shape
  *     else:
  */
 __pyx_t_1 = (PyDataType_HASSUBARRAY(__pyx_v_d) != 0);
 if (__pyx_t_1) {
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":752
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":752
  * cdef inline tuple PyDataType_SHAPE(dtype d):
  *     if PyDataType_HASSUBARRAY(d):
  *         return <tuple>d.subarray.shape             # <<<<<<<<<<<<<<
  *     else:
  *         return ()
  */
   __Pyx_XDECREF(__pyx_r);
   __Pyx_INCREF(((PyObject*)__pyx_v_d->subarray->shape));
   __pyx_r = ((PyObject*)__pyx_v_d->subarray->shape);
   goto __pyx_L0;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":751
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":751
  * 
  * cdef inline tuple PyDataType_SHAPE(dtype d):
  *     if PyDataType_HASSUBARRAY(d):             # <<<<<<<<<<<<<<
  *         return <tuple>d.subarray.shape
  *     else:
  */
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":754
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":754
  *         return <tuple>d.subarray.shape
  *     else:
  *         return ()             # <<<<<<<<<<<<<<
  * 
  * 
  */
 /*else*/ {
   __Pyx_XDECREF(__pyx_r);
   __Pyx_INCREF(__pyx_empty_tuple);
   __pyx_r = __pyx_empty_tuple;
   goto __pyx_L0;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":750
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":750
  *     return PyArray_MultiIterNew(5, <void*>a, <void*>b, <void*>c, <void*> d, <void*> e)
  * 
  * cdef inline tuple PyDataType_SHAPE(dtype d):             # <<<<<<<<<<<<<<
  *     if PyDataType_HASSUBARRAY(d):
  *         return <tuple>d.subarray.shape
  */
 
 /* function exit code */
 __pyx_L0:;
 __Pyx_XGIVEREF(__pyx_r);
 __Pyx_RefNannyFinishContext();
 return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":931
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":929
  *     int _import_umath() except -1
  * 
  * cdef inline void set_array_base(ndarray arr, object base):             # <<<<<<<<<<<<<<
  *     Py_INCREF(base) # important to do this before stealing the reference below!
  *     PyArray_SetBaseObject(arr, base)
  */
 
 static CYTHON_INLINE void __pyx_f_5numpy_set_array_base(PyArrayObject *__pyx_v_arr, PyObject *__pyx_v_base) {
 __Pyx_RefNannyDeclarations
 __Pyx_RefNannySetupContext("set_array_base", 0);
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":932
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":930
  * 
  * cdef inline void set_array_base(ndarray arr, object base):
  *     Py_INCREF(base) # important to do this before stealing the reference below!             # <<<<<<<<<<<<<<
  *     PyArray_SetBaseObject(arr, base)
  * 
  */
 Py_INCREF(__pyx_v_base);
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":933
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":931
  * cdef inline void set_array_base(ndarray arr, object base):
  *     Py_INCREF(base) # important to do this before stealing the reference below!
  *     PyArray_SetBaseObject(arr, base)             # <<<<<<<<<<<<<<
  * 
  * cdef inline object get_array_base(ndarray arr):
  */
 (void)(PyArray_SetBaseObject(__pyx_v_arr, __pyx_v_base));
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":931
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":929
  *     int _import_umath() except -1
  * 
  * cdef inline void set_array_base(ndarray arr, object base):             # <<<<<<<<<<<<<<
  *     Py_INCREF(base) # important to do this before stealing the reference below!
  *     PyArray_SetBaseObject(arr, base)
  */
 
 /* function exit code */
 __Pyx_RefNannyFinishContext();
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":935
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":933
  *     PyArray_SetBaseObject(arr, base)
  * 
  * cdef inline object get_array_base(ndarray arr):             # <<<<<<<<<<<<<<
  *     base = PyArray_BASE(arr)
  *     if base is NULL:
  */
 
 static CYTHON_INLINE PyObject *__pyx_f_5numpy_get_array_base(PyArrayObject *__pyx_v_arr) {
 PyObject *__pyx_v_base;
 PyObject *__pyx_r = NULL;
 __Pyx_RefNannyDeclarations
 int __pyx_t_1;
 __Pyx_RefNannySetupContext("get_array_base", 0);
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":936
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":934
  * 
  * cdef inline object get_array_base(ndarray arr):
  *     base = PyArray_BASE(arr)             # <<<<<<<<<<<<<<
  *     if base is NULL:
  *         return None
  */
 __pyx_v_base = PyArray_BASE(__pyx_v_arr);
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":937
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":935
  * cdef inline object get_array_base(ndarray arr):
  *     base = PyArray_BASE(arr)
  *     if base is NULL:             # <<<<<<<<<<<<<<
  *         return None
  *     return <object>base
  */
 __pyx_t_1 = ((__pyx_v_base == NULL) != 0);
 if (__pyx_t_1) {
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":938
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":936
  *     base = PyArray_BASE(arr)
  *     if base is NULL:
  *         return None             # <<<<<<<<<<<<<<
  *     return <object>base
  * 
  */
   __Pyx_XDECREF(__pyx_r);
   __pyx_r = Py_None; __Pyx_INCREF(Py_None);
   goto __pyx_L0;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":937
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":935
  * cdef inline object get_array_base(ndarray arr):
  *     base = PyArray_BASE(arr)
  *     if base is NULL:             # <<<<<<<<<<<<<<
  *         return None
  *     return <object>base
  */
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":939
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":937
  *     if base is NULL:
  *         return None
  *     return <object>base             # <<<<<<<<<<<<<<
  * 
  * # Versions of the import_* functions which are more suitable for
  */
 __Pyx_XDECREF(__pyx_r);
 __Pyx_INCREF(((PyObject *)__pyx_v_base));
 __pyx_r = ((PyObject *)__pyx_v_base);
 goto __pyx_L0;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":935
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":933
  *     PyArray_SetBaseObject(arr, base)
  * 
  * cdef inline object get_array_base(ndarray arr):             # <<<<<<<<<<<<<<
  *     base = PyArray_BASE(arr)
  *     if base is NULL:
  */
 
 /* function exit code */
 __pyx_L0:;
 __Pyx_XGIVEREF(__pyx_r);
 __Pyx_RefNannyFinishContext();
 return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":943
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":941
  * # Versions of the import_* functions which are more suitable for
  * # Cython code.
  * cdef inline int import_array() except -1:             # <<<<<<<<<<<<<<
  *     try:
  *         __pyx_import_array()
  */
 
@@ -4298,15 +4295,15 @@
 PyObject *__pyx_t_7 = NULL;
 PyObject *__pyx_t_8 = NULL;
 int __pyx_lineno = 0;
 const char *__pyx_filename = NULL;
 int __pyx_clineno = 0;
 __Pyx_RefNannySetupContext("import_array", 0);
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":944
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":942
  * # Cython code.
  * cdef inline int import_array() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         __pyx_import_array()
  *     except Exception:
  */
 {
@@ -4314,84 +4311,84 @@
   __Pyx_PyThreadState_assign
   __Pyx_ExceptionSave(&__pyx_t_1, &__pyx_t_2, &__pyx_t_3);
   __Pyx_XGOTREF(__pyx_t_1);
   __Pyx_XGOTREF(__pyx_t_2);
   __Pyx_XGOTREF(__pyx_t_3);
   /*try:*/ {
 
-    /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":945
+    /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":943
  * cdef inline int import_array() except -1:
  *     try:
  *         __pyx_import_array()             # <<<<<<<<<<<<<<
  *     except Exception:
  *         raise ImportError("numpy.core.multiarray failed to import")
  */
-    __pyx_t_4 = _import_array(); if (unlikely(__pyx_t_4 == ((int)-1))) __PYX_ERR(1, 945, __pyx_L3_error)
+    __pyx_t_4 = _import_array(); if (unlikely(__pyx_t_4 == ((int)-1))) __PYX_ERR(1, 943, __pyx_L3_error)
 
-    /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":944
+    /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":942
  * # Cython code.
  * cdef inline int import_array() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         __pyx_import_array()
  *     except Exception:
  */
   }
   __Pyx_XDECREF(__pyx_t_1); __pyx_t_1 = 0;
   __Pyx_XDECREF(__pyx_t_2); __pyx_t_2 = 0;
   __Pyx_XDECREF(__pyx_t_3); __pyx_t_3 = 0;
   goto __pyx_L8_try_end;
   __pyx_L3_error:;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":946
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":944
  *     try:
  *         __pyx_import_array()
  *     except Exception:             # <<<<<<<<<<<<<<
  *         raise ImportError("numpy.core.multiarray failed to import")
  * 
  */
   __pyx_t_4 = __Pyx_PyErr_ExceptionMatches(((PyObject *)(&((PyTypeObject*)PyExc_Exception)[0])));
   if (__pyx_t_4) {
     __Pyx_AddTraceback("numpy.import_array", __pyx_clineno, __pyx_lineno, __pyx_filename);
-    if (__Pyx_GetException(&__pyx_t_5, &__pyx_t_6, &__pyx_t_7) < 0) __PYX_ERR(1, 946, __pyx_L5_except_error)
+    if (__Pyx_GetException(&__pyx_t_5, &__pyx_t_6, &__pyx_t_7) < 0) __PYX_ERR(1, 944, __pyx_L5_except_error)
     __Pyx_GOTREF(__pyx_t_5);
     __Pyx_GOTREF(__pyx_t_6);
     __Pyx_GOTREF(__pyx_t_7);
 
-    /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":947
+    /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":945
  *         __pyx_import_array()
  *     except Exception:
  *         raise ImportError("numpy.core.multiarray failed to import")             # <<<<<<<<<<<<<<
  * 
  * cdef inline int import_umath() except -1:
  */
-    __pyx_t_8 = __Pyx_PyObject_Call(__pyx_builtin_ImportError, __pyx_tuple_, NULL); if (unlikely(!__pyx_t_8)) __PYX_ERR(1, 947, __pyx_L5_except_error)
+    __pyx_t_8 = __Pyx_PyObject_Call(__pyx_builtin_ImportError, __pyx_tuple_, NULL); if (unlikely(!__pyx_t_8)) __PYX_ERR(1, 945, __pyx_L5_except_error)
     __Pyx_GOTREF(__pyx_t_8);
     __Pyx_Raise(__pyx_t_8, 0, 0, 0);
     __Pyx_DECREF(__pyx_t_8); __pyx_t_8 = 0;
-    __PYX_ERR(1, 947, __pyx_L5_except_error)
+    __PYX_ERR(1, 945, __pyx_L5_except_error)
   }
   goto __pyx_L5_except_error;
   __pyx_L5_except_error:;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":944
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":942
  * # Cython code.
  * cdef inline int import_array() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         __pyx_import_array()
  *     except Exception:
  */
   __Pyx_XGIVEREF(__pyx_t_1);
   __Pyx_XGIVEREF(__pyx_t_2);
   __Pyx_XGIVEREF(__pyx_t_3);
   __Pyx_ExceptionReset(__pyx_t_1, __pyx_t_2, __pyx_t_3);
   goto __pyx_L1_error;
   __pyx_L8_try_end:;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":943
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":941
  * # Versions of the import_* functions which are more suitable for
  * # Cython code.
  * cdef inline int import_array() except -1:             # <<<<<<<<<<<<<<
  *     try:
  *         __pyx_import_array()
  */
 
@@ -4406,15 +4403,15 @@
 __Pyx_AddTraceback("numpy.import_array", __pyx_clineno, __pyx_lineno, __pyx_filename);
 __pyx_r = -1;
 __pyx_L0:;
 __Pyx_RefNannyFinishContext();
 return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":949
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":947
  *         raise ImportError("numpy.core.multiarray failed to import")
  * 
  * cdef inline int import_umath() except -1:             # <<<<<<<<<<<<<<
  *     try:
  *         _import_umath()
  */
 
@@ -4430,15 +4427,15 @@
 PyObject *__pyx_t_7 = NULL;
 PyObject *__pyx_t_8 = NULL;
 int __pyx_lineno = 0;
 const char *__pyx_filename = NULL;
 int __pyx_clineno = 0;
 __Pyx_RefNannySetupContext("import_umath", 0);
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":950
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":948
  * 
  * cdef inline int import_umath() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         _import_umath()
  *     except Exception:
  */
 {
@@ -4446,84 +4443,84 @@
   __Pyx_PyThreadState_assign
   __Pyx_ExceptionSave(&__pyx_t_1, &__pyx_t_2, &__pyx_t_3);
   __Pyx_XGOTREF(__pyx_t_1);
   __Pyx_XGOTREF(__pyx_t_2);
   __Pyx_XGOTREF(__pyx_t_3);
   /*try:*/ {
 
-    /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":951
+    /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":949
  * cdef inline int import_umath() except -1:
  *     try:
  *         _import_umath()             # <<<<<<<<<<<<<<
  *     except Exception:
  *         raise ImportError("numpy.core.umath failed to import")
  */
-    __pyx_t_4 = _import_umath(); if (unlikely(__pyx_t_4 == ((int)-1))) __PYX_ERR(1, 951, __pyx_L3_error)
+    __pyx_t_4 = _import_umath(); if (unlikely(__pyx_t_4 == ((int)-1))) __PYX_ERR(1, 949, __pyx_L3_error)
 
-    /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":950
+    /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":948
  * 
  * cdef inline int import_umath() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         _import_umath()
  *     except Exception:
  */
   }
   __Pyx_XDECREF(__pyx_t_1); __pyx_t_1 = 0;
   __Pyx_XDECREF(__pyx_t_2); __pyx_t_2 = 0;
   __Pyx_XDECREF(__pyx_t_3); __pyx_t_3 = 0;
   goto __pyx_L8_try_end;
   __pyx_L3_error:;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":952
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":950
  *     try:
  *         _import_umath()
  *     except Exception:             # <<<<<<<<<<<<<<
  *         raise ImportError("numpy.core.umath failed to import")
  * 
  */
   __pyx_t_4 = __Pyx_PyErr_ExceptionMatches(((PyObject *)(&((PyTypeObject*)PyExc_Exception)[0])));
   if (__pyx_t_4) {
     __Pyx_AddTraceback("numpy.import_umath", __pyx_clineno, __pyx_lineno, __pyx_filename);
-    if (__Pyx_GetException(&__pyx_t_5, &__pyx_t_6, &__pyx_t_7) < 0) __PYX_ERR(1, 952, __pyx_L5_except_error)
+    if (__Pyx_GetException(&__pyx_t_5, &__pyx_t_6, &__pyx_t_7) < 0) __PYX_ERR(1, 950, __pyx_L5_except_error)
     __Pyx_GOTREF(__pyx_t_5);
     __Pyx_GOTREF(__pyx_t_6);
     __Pyx_GOTREF(__pyx_t_7);
 
-    /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":953
+    /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":951
  *         _import_umath()
  *     except Exception:
  *         raise ImportError("numpy.core.umath failed to import")             # <<<<<<<<<<<<<<
  * 
  * cdef inline int import_ufunc() except -1:
  */
-    __pyx_t_8 = __Pyx_PyObject_Call(__pyx_builtin_ImportError, __pyx_tuple__2, NULL); if (unlikely(!__pyx_t_8)) __PYX_ERR(1, 953, __pyx_L5_except_error)
+    __pyx_t_8 = __Pyx_PyObject_Call(__pyx_builtin_ImportError, __pyx_tuple__2, NULL); if (unlikely(!__pyx_t_8)) __PYX_ERR(1, 951, __pyx_L5_except_error)
     __Pyx_GOTREF(__pyx_t_8);
     __Pyx_Raise(__pyx_t_8, 0, 0, 0);
     __Pyx_DECREF(__pyx_t_8); __pyx_t_8 = 0;
-    __PYX_ERR(1, 953, __pyx_L5_except_error)
+    __PYX_ERR(1, 951, __pyx_L5_except_error)
   }
   goto __pyx_L5_except_error;
   __pyx_L5_except_error:;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":950
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":948
  * 
  * cdef inline int import_umath() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         _import_umath()
  *     except Exception:
  */
   __Pyx_XGIVEREF(__pyx_t_1);
   __Pyx_XGIVEREF(__pyx_t_2);
   __Pyx_XGIVEREF(__pyx_t_3);
   __Pyx_ExceptionReset(__pyx_t_1, __pyx_t_2, __pyx_t_3);
   goto __pyx_L1_error;
   __pyx_L8_try_end:;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":949
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":947
  *         raise ImportError("numpy.core.multiarray failed to import")
  * 
  * cdef inline int import_umath() except -1:             # <<<<<<<<<<<<<<
  *     try:
  *         _import_umath()
  */
 
@@ -4538,15 +4535,15 @@
 __Pyx_AddTraceback("numpy.import_umath", __pyx_clineno, __pyx_lineno, __pyx_filename);
 __pyx_r = -1;
 __pyx_L0:;
 __Pyx_RefNannyFinishContext();
 return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":955
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":953
  *         raise ImportError("numpy.core.umath failed to import")
  * 
  * cdef inline int import_ufunc() except -1:             # <<<<<<<<<<<<<<
  *     try:
  *         _import_umath()
  */
 
@@ -4562,15 +4559,15 @@
 PyObject *__pyx_t_7 = NULL;
 PyObject *__pyx_t_8 = NULL;
 int __pyx_lineno = 0;
 const char *__pyx_filename = NULL;
 int __pyx_clineno = 0;
 __Pyx_RefNannySetupContext("import_ufunc", 0);
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":956
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":954
  * 
  * cdef inline int import_ufunc() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         _import_umath()
  *     except Exception:
  */
 {
@@ -4578,84 +4575,84 @@
   __Pyx_PyThreadState_assign
   __Pyx_ExceptionSave(&__pyx_t_1, &__pyx_t_2, &__pyx_t_3);
   __Pyx_XGOTREF(__pyx_t_1);
   __Pyx_XGOTREF(__pyx_t_2);
   __Pyx_XGOTREF(__pyx_t_3);
   /*try:*/ {
 
-    /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":957
+    /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":955
  * cdef inline int import_ufunc() except -1:
  *     try:
  *         _import_umath()             # <<<<<<<<<<<<<<
  *     except Exception:
  *         raise ImportError("numpy.core.umath failed to import")
  */
-    __pyx_t_4 = _import_umath(); if (unlikely(__pyx_t_4 == ((int)-1))) __PYX_ERR(1, 957, __pyx_L3_error)
+    __pyx_t_4 = _import_umath(); if (unlikely(__pyx_t_4 == ((int)-1))) __PYX_ERR(1, 955, __pyx_L3_error)
 
-    /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":956
+    /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":954
  * 
  * cdef inline int import_ufunc() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         _import_umath()
  *     except Exception:
  */
   }
   __Pyx_XDECREF(__pyx_t_1); __pyx_t_1 = 0;
   __Pyx_XDECREF(__pyx_t_2); __pyx_t_2 = 0;
   __Pyx_XDECREF(__pyx_t_3); __pyx_t_3 = 0;
   goto __pyx_L8_try_end;
   __pyx_L3_error:;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":958
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":956
  *     try:
  *         _import_umath()
  *     except Exception:             # <<<<<<<<<<<<<<
  *         raise ImportError("numpy.core.umath failed to import")
  * 
  */
   __pyx_t_4 = __Pyx_PyErr_ExceptionMatches(((PyObject *)(&((PyTypeObject*)PyExc_Exception)[0])));
   if (__pyx_t_4) {
     __Pyx_AddTraceback("numpy.import_ufunc", __pyx_clineno, __pyx_lineno, __pyx_filename);
-    if (__Pyx_GetException(&__pyx_t_5, &__pyx_t_6, &__pyx_t_7) < 0) __PYX_ERR(1, 958, __pyx_L5_except_error)
+    if (__Pyx_GetException(&__pyx_t_5, &__pyx_t_6, &__pyx_t_7) < 0) __PYX_ERR(1, 956, __pyx_L5_except_error)
     __Pyx_GOTREF(__pyx_t_5);
     __Pyx_GOTREF(__pyx_t_6);
     __Pyx_GOTREF(__pyx_t_7);
 
-    /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":959
+    /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":957
  *         _import_umath()
  *     except Exception:
  *         raise ImportError("numpy.core.umath failed to import")             # <<<<<<<<<<<<<<
  * 
  * cdef extern from *:
  */
-    __pyx_t_8 = __Pyx_PyObject_Call(__pyx_builtin_ImportError, __pyx_tuple__2, NULL); if (unlikely(!__pyx_t_8)) __PYX_ERR(1, 959, __pyx_L5_except_error)
+    __pyx_t_8 = __Pyx_PyObject_Call(__pyx_builtin_ImportError, __pyx_tuple__2, NULL); if (unlikely(!__pyx_t_8)) __PYX_ERR(1, 957, __pyx_L5_except_error)
     __Pyx_GOTREF(__pyx_t_8);
     __Pyx_Raise(__pyx_t_8, 0, 0, 0);
     __Pyx_DECREF(__pyx_t_8); __pyx_t_8 = 0;
-    __PYX_ERR(1, 959, __pyx_L5_except_error)
+    __PYX_ERR(1, 957, __pyx_L5_except_error)
   }
   goto __pyx_L5_except_error;
   __pyx_L5_except_error:;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":956
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":954
  * 
  * cdef inline int import_ufunc() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         _import_umath()
  *     except Exception:
  */
   __Pyx_XGIVEREF(__pyx_t_1);
   __Pyx_XGIVEREF(__pyx_t_2);
   __Pyx_XGIVEREF(__pyx_t_3);
   __Pyx_ExceptionReset(__pyx_t_1, __pyx_t_2, __pyx_t_3);
   goto __pyx_L1_error;
   __pyx_L8_try_end:;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":955
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":953
  *         raise ImportError("numpy.core.umath failed to import")
  * 
  * cdef inline int import_ufunc() except -1:             # <<<<<<<<<<<<<<
  *     try:
  *         _import_umath()
  */
 
@@ -4670,176 +4667,176 @@
 __Pyx_AddTraceback("numpy.import_ufunc", __pyx_clineno, __pyx_lineno, __pyx_filename);
 __pyx_r = -1;
 __pyx_L0:;
 __Pyx_RefNannyFinishContext();
 return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":969
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":967
  * 
  * 
  * cdef inline bint is_timedelta64_object(object obj):             # <<<<<<<<<<<<<<
  *     """
  *     Cython equivalent of `isinstance(obj, np.timedelta64)`
  */
 
 static CYTHON_INLINE int __pyx_f_5numpy_is_timedelta64_object(PyObject *__pyx_v_obj) {
 int __pyx_r;
 __Pyx_RefNannyDeclarations
 __Pyx_RefNannySetupContext("is_timedelta64_object", 0);
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":981
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":979
  *     bool
  *     """
  *     return PyObject_TypeCheck(obj, &PyTimedeltaArrType_Type)             # <<<<<<<<<<<<<<
  * 
  * 
  */
 __pyx_r = PyObject_TypeCheck(__pyx_v_obj, (&PyTimedeltaArrType_Type));
 goto __pyx_L0;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":969
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":967
  * 
  * 
  * cdef inline bint is_timedelta64_object(object obj):             # <<<<<<<<<<<<<<
  *     """
  *     Cython equivalent of `isinstance(obj, np.timedelta64)`
  */
 
 /* function exit code */
 __pyx_L0:;
 __Pyx_RefNannyFinishContext();
 return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":984
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":982
  * 
  * 
  * cdef inline bint is_datetime64_object(object obj):             # <<<<<<<<<<<<<<
  *     """
  *     Cython equivalent of `isinstance(obj, np.datetime64)`
  */
 
 static CYTHON_INLINE int __pyx_f_5numpy_is_datetime64_object(PyObject *__pyx_v_obj) {
 int __pyx_r;
 __Pyx_RefNannyDeclarations
 __Pyx_RefNannySetupContext("is_datetime64_object", 0);
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":996
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":994
  *     bool
  *     """
  *     return PyObject_TypeCheck(obj, &PyDatetimeArrType_Type)             # <<<<<<<<<<<<<<
  * 
  * 
  */
 __pyx_r = PyObject_TypeCheck(__pyx_v_obj, (&PyDatetimeArrType_Type));
 goto __pyx_L0;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":984
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":982
  * 
  * 
  * cdef inline bint is_datetime64_object(object obj):             # <<<<<<<<<<<<<<
  *     """
  *     Cython equivalent of `isinstance(obj, np.datetime64)`
  */
 
 /* function exit code */
 __pyx_L0:;
 __Pyx_RefNannyFinishContext();
 return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":999
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":997
  * 
  * 
  * cdef inline npy_datetime get_datetime64_value(object obj) nogil:             # <<<<<<<<<<<<<<
  *     """
  *     returns the int64 value underlying scalar numpy datetime64 object
  */
 
 static CYTHON_INLINE npy_datetime __pyx_f_5numpy_get_datetime64_value(PyObject *__pyx_v_obj) {
 npy_datetime __pyx_r;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":1006
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":1004
  *     also needed.  That can be found using `get_datetime64_unit`.
  *     """
  *     return (<PyDatetimeScalarObject*>obj).obval             # <<<<<<<<<<<<<<
  * 
  * 
  */
 __pyx_r = ((PyDatetimeScalarObject *)__pyx_v_obj)->obval;
 goto __pyx_L0;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":999
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":997
  * 
  * 
  * cdef inline npy_datetime get_datetime64_value(object obj) nogil:             # <<<<<<<<<<<<<<
  *     """
  *     returns the int64 value underlying scalar numpy datetime64 object
  */
 
 /* function exit code */
 __pyx_L0:;
 return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":1009
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":1007
  * 
  * 
  * cdef inline npy_timedelta get_timedelta64_value(object obj) nogil:             # <<<<<<<<<<<<<<
  *     """
  *     returns the int64 value underlying scalar numpy timedelta64 object
  */
 
 static CYTHON_INLINE npy_timedelta __pyx_f_5numpy_get_timedelta64_value(PyObject *__pyx_v_obj) {
 npy_timedelta __pyx_r;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":1013
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":1011
  *     returns the int64 value underlying scalar numpy timedelta64 object
  *     """
  *     return (<PyTimedeltaScalarObject*>obj).obval             # <<<<<<<<<<<<<<
  * 
  * 
  */
 __pyx_r = ((PyTimedeltaScalarObject *)__pyx_v_obj)->obval;
 goto __pyx_L0;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":1009
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":1007
  * 
  * 
  * cdef inline npy_timedelta get_timedelta64_value(object obj) nogil:             # <<<<<<<<<<<<<<
  *     """
  *     returns the int64 value underlying scalar numpy timedelta64 object
  */
 
 /* function exit code */
 __pyx_L0:;
 return __pyx_r;
 }
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":1016
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":1014
  * 
  * 
  * cdef inline NPY_DATETIMEUNIT get_datetime64_unit(object obj) nogil:             # <<<<<<<<<<<<<<
  *     """
  *     returns the unit part of the dtype for a numpy datetime64 object.
  */
 
 static CYTHON_INLINE NPY_DATETIMEUNIT __pyx_f_5numpy_get_datetime64_unit(PyObject *__pyx_v_obj) {
 NPY_DATETIMEUNIT __pyx_r;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":1020
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":1018
  *     returns the unit part of the dtype for a numpy datetime64 object.
  *     """
  *     return <NPY_DATETIMEUNIT>(<PyDatetimeScalarObject*>obj).obmeta.base             # <<<<<<<<<<<<<<
  */
 __pyx_r = ((NPY_DATETIMEUNIT)((PyDatetimeScalarObject *)__pyx_v_obj)->obmeta.base);
 goto __pyx_L0;
 
-/* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":1016
+/* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":1014
  * 
  * 
  * cdef inline NPY_DATETIMEUNIT get_datetime64_unit(object obj) nogil:             # <<<<<<<<<<<<<<
  *     """
  *     returns the unit part of the dtype for a numpy datetime64 object.
  */
 
@@ -4958,43 +4955,43 @@
   {&__pyx_n_s_val, __pyx_k_val, sizeof(__pyx_k_val), 0, 0, 1, 1},
   {&__pyx_n_s_values, __pyx_k_values, sizeof(__pyx_k_values), 0, 0, 1, 1},
   {&__pyx_n_s_w, __pyx_k_w, sizeof(__pyx_k_w), 0, 0, 1, 1},
   {0, 0, 0, 0, 0, 0, 0}
 };
 static CYTHON_SMALL_CODE int __Pyx_InitCachedBuiltins(void) {
   __pyx_builtin_range = __Pyx_GetBuiltinName(__pyx_n_s_range); if (!__pyx_builtin_range) __PYX_ERR(0, 115, __pyx_L1_error)
-  __pyx_builtin_ImportError = __Pyx_GetBuiltinName(__pyx_n_s_ImportError); if (!__pyx_builtin_ImportError) __PYX_ERR(1, 947, __pyx_L1_error)
+  __pyx_builtin_ImportError = __Pyx_GetBuiltinName(__pyx_n_s_ImportError); if (!__pyx_builtin_ImportError) __PYX_ERR(1, 945, __pyx_L1_error)
   return 0;
   __pyx_L1_error:;
   return -1;
 }
 
 static CYTHON_SMALL_CODE int __Pyx_InitCachedConstants(void) {
   __Pyx_RefNannyDeclarations
   __Pyx_RefNannySetupContext("__Pyx_InitCachedConstants", 0);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":947
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":945
  *         __pyx_import_array()
  *     except Exception:
  *         raise ImportError("numpy.core.multiarray failed to import")             # <<<<<<<<<<<<<<
  * 
  * cdef inline int import_umath() except -1:
  */
-  __pyx_tuple_ = PyTuple_Pack(1, __pyx_kp_u_numpy_core_multiarray_failed_to); if (unlikely(!__pyx_tuple_)) __PYX_ERR(1, 947, __pyx_L1_error)
+  __pyx_tuple_ = PyTuple_Pack(1, __pyx_kp_u_numpy_core_multiarray_failed_to); if (unlikely(!__pyx_tuple_)) __PYX_ERR(1, 945, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_tuple_);
   __Pyx_GIVEREF(__pyx_tuple_);
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":953
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":951
  *         _import_umath()
  *     except Exception:
  *         raise ImportError("numpy.core.umath failed to import")             # <<<<<<<<<<<<<<
  * 
  * cdef inline int import_ufunc() except -1:
  */
-  __pyx_tuple__2 = PyTuple_Pack(1, __pyx_kp_u_numpy_core_umath_failed_to_impor); if (unlikely(!__pyx_tuple__2)) __PYX_ERR(1, 953, __pyx_L1_error)
+  __pyx_tuple__2 = PyTuple_Pack(1, __pyx_kp_u_numpy_core_umath_failed_to_impor); if (unlikely(!__pyx_tuple__2)) __PYX_ERR(1, 951, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_tuple__2);
   __Pyx_GIVEREF(__pyx_tuple__2);
 
   /* "river/metrics/expected_mutual_info.pyx":15
  * 
  * 
  * def expected_mutual_info(confusion_matrix):             # <<<<<<<<<<<<<<
@@ -5400,15 +5397,15 @@
  * # cython: wraparound=False
  */
   __pyx_t_2 = __Pyx_PyDict_NewPresized(0); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 1, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_2);
   if (PyDict_SetItem(__pyx_d, __pyx_n_s_test, __pyx_t_2) < 0) __PYX_ERR(0, 1, __pyx_L1_error)
   __Pyx_DECREF(__pyx_t_2); __pyx_t_2 = 0;
 
-  /* "../../../../../opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/__init__.pxd":1016
+  /* "C:/cibw/python/python.3.9.9/tools/lib/site-packages/numpy/__init__.pxd":1014
  * 
  * 
  * cdef inline NPY_DATETIMEUNIT get_datetime64_unit(object obj) nogil:             # <<<<<<<<<<<<<<
  *     """
  *     returns the unit part of the dtype for a numpy datetime64 object.
  */
```

### Comparing `river-0.8.0/river/metrics/expected_mutual_info.pyx` & `river-0.9.0/river/metrics/expected_mutual_info.pyx`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,127 +1,127 @@
-# cython: cdivision=True
-# cython: boundscheck=False
-# cython: wraparound=False
-
-from libc.math cimport exp, lgamma
-from scipy.special import gammaln
-import numpy as np
-cimport cython
-cimport numpy as np
-
-np.import_array()
-ctypedef np.float64_t DOUBLE
-
-
-def expected_mutual_info(confusion_matrix):
-    r"""Expected Mutual Information.
-
-    Like the Rand index, the baseline value of mutual information between two
-    random clusterings is not necessarily a constant value, and tends to become
-    larger when the two partitions have a higher number of cluster (with a fixed
-    number of samples $N$). Using a hypergeometric model of randomness, it can be
-    shown that the expected mutual information between two random clusterings [^1] is:
-
-    $$
-    E(MI(U, V)) = \sum_{i=1}^R \sum_{i=1}^C \sum{n_{ij}=(a_i+b_j-N)^+}^{\min(a_i,b_j)} \frac{n_{ij}}{N} \log(\frac{N n_{ij}}{a_i b_j}) \frac{a_i! b_j! (N-a_i)! (N-b_j)!}{N! n_{ij}! (a_i - n_{ij})! (b_j - n_{ij})! (N - a_i - b_j + n_{ij})!}}
-    $$
-
-    where
-
-    * $(a_i + b_j - N)^+$ denotes $\max(1, a_i + b_j - N)$,
-
-    * $a_i$ is the sum of row i-th of the contingency table, and
-
-    * $b_j$ is the sum of column j-th of the contingency table.
-
-    The Adjusted Mutual Information score (AMI) relies on the Expected Mutual Information (EMI) score.
-
-    From the formula, we note that this metric is very expensive to calculate. As such,
-    the AMI will be one order of magnitude slower than most other implemented metrics.
-
-    Note that, different form most of the implementations of other mutual information metrics,
-    the expected mutual information wil be implemented using numpy arrays. This implementation
-    inherits from the implementation of the expected mutual information in scikit-learn.
-
-    Parameters
-    ----------
-    confusion_matrix
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-
-    References
-    ----------
-    [^1]: Wikipedia contributors. (2021, March 17). Mutual information.
-          In Wikipedia, The Free Encyclopedia,
-          from https://en.wikipedia.org/w/index.php?title=Mutual_information&oldid=1012714929
-    [^2]: Andrew Rosenberg and Julia Hirschberg (2007).
-          V-Measure: A conditional entropy-based external cluster evaluation measure.
-          Proceedings of the 2007 Joing Conference on Empirical Methods in Natural Language
-          Processing and Computational Natural Language Learning, pp. 410 - 420,
-          Prague, June 2007.
-    """
-
-    cdef int R, C
-    cdef DOUBLE N, gln_N, emi, term2, term3, gln
-    cdef np.ndarray[DOUBLE] gln_a, gln_b, gln_Na, gln_Nb, gln_nij, log_Nnij
-    cdef np.ndarray[DOUBLE] nijs, term1
-    cdef np.ndarray[DOUBLE] log_a, log_b
-    cdef np.ndarray[np.int32_t] a, b
-    #cdef np.ndarray[DOUBLE, ndim=2] start, end
-
-    N = confusion_matrix.n_samples
-
-    a = np.array([confusion_matrix.sum_row[key] for key in confusion_matrix.classes if confusion_matrix.sum_row[key]]).astype(np.int32)
-    b = np.array([confusion_matrix.sum_col[key] for key in confusion_matrix.classes if confusion_matrix.sum_col[key]]).astype(np.int32)
-
-    # we do not take into consideration the order of classes in numpy arrays constructed below,
-    # as they will be consistent with each other, which is enough
-
-    cdef int val
-    R = len([val for val in confusion_matrix.sum_row.values() if val != 0])
-    C = len([val for val in confusion_matrix.sum_col.values() if val != 0])
-
-    # There are three major terms to the EMI equation, which are multiplied to
-    # and then summed over varying nij values.
-    # Although nijs[0] will never be used, having it simplifies the indexing.
-    # It will also be set to 1 to stop divide by zero warnings.
-
-    nijs = np.arange(0, max(np.max(a), np.max(b)) + 1, dtype='float')
-    nijs[0] = 1
-
-    # calculation of the first term
-    term1 = nijs / N
-
-    # calculation of the second term
-    log_a, log_b = np.log(a), np.log(b)
-    log_Nnij = np.log(N) + np.log(nijs)
-
-    # calculation of the third/final term. All elements are calculated in log space
-    # to prevent overflow
-
-    gln_a, gln_b = gammaln(a + 1), gammaln(b + 1)
-    gln_Na, gln_Nb = gammaln(N - a + 1), gammaln(N - b + 1)
-    gln_N = gammaln(N + 1)
-    gln_nij = gammaln(nijs + 1)
-
-    # start and end values for nij terms for each summation.
-    start = np.array([[v - N + w for w in b] for v in a], dtype='int')
-    start = np.maximum(start, 1)
-    end = np.minimum(np.resize(a, (C, R)).T, np.resize(b, (R, C))) + 1
-
-    # emi itself is a summation over the various values.
-    expected_mutual_info = 0.0
-    cdef Py_ssize_t i, j, nij
-    for i in range(R):
-        for j in range(C):
-            for nij in range(start[i,j], end[i,j]):
-                term2 = log_Nnij[nij] - log_a[i] - log_b[j]
-                # Numerators are positive, denominators are negative.
-                gln = (gln_a[i] + gln_b[j] + gln_Na[i] + gln_Nb[j]
-                     - gln_N - gln_nij[nij] - lgamma(a[i] - nij + 1)
-                     - lgamma(b[j] - nij + 1)
-                     - lgamma(N - a[i] - b[j] + nij + 1))
-                term3 = exp(gln)
-                expected_mutual_info += (term1[nij] * term2 * term3)
-
-    return expected_mutual_info
+# cython: cdivision=True
+# cython: boundscheck=False
+# cython: wraparound=False
+
+from libc.math cimport exp, lgamma
+from scipy.special import gammaln
+import numpy as np
+cimport cython
+cimport numpy as np
+
+np.import_array()
+ctypedef np.float64_t DOUBLE
+
+
+def expected_mutual_info(confusion_matrix):
+    r"""Expected Mutual Information.
+
+    Like the Rand index, the baseline value of mutual information between two
+    random clusterings is not necessarily a constant value, and tends to become
+    larger when the two partitions have a higher number of cluster (with a fixed
+    number of samples $N$). Using a hypergeometric model of randomness, it can be
+    shown that the expected mutual information between two random clusterings [^1] is:
+
+    $$
+    E(MI(U, V)) = \sum_{i=1}^R \sum_{i=1}^C \sum{n_{ij}=(a_i+b_j-N)^+}^{\min(a_i,b_j)} \frac{n_{ij}}{N} \log(\frac{N n_{ij}}{a_i b_j}) \frac{a_i! b_j! (N-a_i)! (N-b_j)!}{N! n_{ij}! (a_i - n_{ij})! (b_j - n_{ij})! (N - a_i - b_j + n_{ij})!}}
+    $$
+
+    where
+
+    * $(a_i + b_j - N)^+$ denotes $\max(1, a_i + b_j - N)$,
+
+    * $a_i$ is the sum of row i-th of the contingency table, and
+
+    * $b_j$ is the sum of column j-th of the contingency table.
+
+    The Adjusted Mutual Information score (AMI) relies on the Expected Mutual Information (EMI) score.
+
+    From the formula, we note that this metric is very expensive to calculate. As such,
+    the AMI will be one order of magnitude slower than most other implemented metrics.
+
+    Note that, different form most of the implementations of other mutual information metrics,
+    the expected mutual information wil be implemented using numpy arrays. This implementation
+    inherits from the implementation of the expected mutual information in scikit-learn.
+
+    Parameters
+    ----------
+    confusion_matrix
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+
+    References
+    ----------
+    [^1]: Wikipedia contributors. (2021, March 17). Mutual information.
+          In Wikipedia, The Free Encyclopedia,
+          from https://en.wikipedia.org/w/index.php?title=Mutual_information&oldid=1012714929
+    [^2]: Andrew Rosenberg and Julia Hirschberg (2007).
+          V-Measure: A conditional entropy-based external cluster evaluation measure.
+          Proceedings of the 2007 Joing Conference on Empirical Methods in Natural Language
+          Processing and Computational Natural Language Learning, pp. 410 - 420,
+          Prague, June 2007.
+    """
+
+    cdef int R, C
+    cdef DOUBLE N, gln_N, emi, term2, term3, gln
+    cdef np.ndarray[DOUBLE] gln_a, gln_b, gln_Na, gln_Nb, gln_nij, log_Nnij
+    cdef np.ndarray[DOUBLE] nijs, term1
+    cdef np.ndarray[DOUBLE] log_a, log_b
+    cdef np.ndarray[np.int32_t] a, b
+    #cdef np.ndarray[DOUBLE, ndim=2] start, end
+
+    N = confusion_matrix.n_samples
+
+    a = np.array([confusion_matrix.sum_row[key] for key in confusion_matrix.classes if confusion_matrix.sum_row[key]]).astype(np.int32)
+    b = np.array([confusion_matrix.sum_col[key] for key in confusion_matrix.classes if confusion_matrix.sum_col[key]]).astype(np.int32)
+
+    # we do not take into consideration the order of classes in numpy arrays constructed below,
+    # as they will be consistent with each other, which is enough
+
+    cdef int val
+    R = len([val for val in confusion_matrix.sum_row.values() if val != 0])
+    C = len([val for val in confusion_matrix.sum_col.values() if val != 0])
+
+    # There are three major terms to the EMI equation, which are multiplied to
+    # and then summed over varying nij values.
+    # Although nijs[0] will never be used, having it simplifies the indexing.
+    # It will also be set to 1 to stop divide by zero warnings.
+
+    nijs = np.arange(0, max(np.max(a), np.max(b)) + 1, dtype='float')
+    nijs[0] = 1
+
+    # calculation of the first term
+    term1 = nijs / N
+
+    # calculation of the second term
+    log_a, log_b = np.log(a), np.log(b)
+    log_Nnij = np.log(N) + np.log(nijs)
+
+    # calculation of the third/final term. All elements are calculated in log space
+    # to prevent overflow
+
+    gln_a, gln_b = gammaln(a + 1), gammaln(b + 1)
+    gln_Na, gln_Nb = gammaln(N - a + 1), gammaln(N - b + 1)
+    gln_N = gammaln(N + 1)
+    gln_nij = gammaln(nijs + 1)
+
+    # start and end values for nij terms for each summation.
+    start = np.array([[v - N + w for w in b] for v in a], dtype='int')
+    start = np.maximum(start, 1)
+    end = np.minimum(np.resize(a, (C, R)).T, np.resize(b, (R, C))) + 1
+
+    # emi itself is a summation over the various values.
+    expected_mutual_info = 0.0
+    cdef Py_ssize_t i, j, nij
+    for i in range(R):
+        for j in range(C):
+            for nij in range(start[i,j], end[i,j]):
+                term2 = log_Nnij[nij] - log_a[i] - log_b[j]
+                # Numerators are positive, denominators are negative.
+                gln = (gln_a[i] + gln_b[j] + gln_Na[i] + gln_Nb[j]
+                     - gln_N - gln_nij[nij] - lgamma(a[i] - nij + 1)
+                     - lgamma(b[j] - nij + 1)
+                     - lgamma(N - a[i] - b[j] + nij + 1))
+                term3 = exp(gln)
+                expected_mutual_info += (term1[nij] * term2 * term3)
+
+    return expected_mutual_info
```

### Comparing `river-0.8.0/river/metrics/fbeta.py` & `river-0.9.0/river/metrics/fbeta.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,588 +1,588 @@
-import collections
-import functools
-
-from river import metrics
-
-__all__ = [
-    "F1",
-    "FBeta",
-    "MacroF1",
-    "MacroFBeta",
-    "MicroF1",
-    "MicroFBeta",
-    "MultiFBeta",
-    "WeightedF1",
-    "WeightedFBeta",
-    "ExampleF1",
-    "ExampleFBeta",
-]
-
-
-class FBeta(metrics.BinaryMetric):
-    """Binary F-Beta score.
-
-    The FBeta score is a weighted harmonic mean between precision and recall. The higher the
-    `beta` value, the higher the recall will be taken into account. When `beta` equals 1,
-    precision and recall and equivalently weighted, which results in the F1 score (see
-    `metrics.F1`).
-
-    Parameters
-    ----------
-    beta
-        Weight of precision in the harmonic mean.
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-    pos_val
-        Value to treat as "positive".
-
-    Attributes
-    ----------
-    precision : metrics.Precision
-    recall : metrics.Recall
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [False, False, False, True, True, True]
-    >>> y_pred = [False, False, True, True, False, False]
-
-    >>> metric = metrics.FBeta(beta=2)
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     metric = metric.update(yt, yp)
-
-    >>> metric
-    FBeta: 0.357143
-
-    """
-
-    def __init__(self, beta: float, cm=None, pos_val=True):
-        super().__init__(cm, pos_val)
-        self.beta = beta
-        self.precision = metrics.Precision(self.cm, self.pos_val)
-        self.recall = metrics.Recall(self.cm, self.pos_val)
-
-    def get(self):
-        p = self.precision.get()
-        r = self.recall.get()
-        b2 = self.beta ** 2
-        try:
-            return (1 + b2) * p * r / (b2 * p + r)
-        except ZeroDivisionError:
-            return 0.0
-
-
-class MacroFBeta(metrics.MultiClassMetric):
-    """Macro-average F-Beta score.
-
-    This works by computing the F-Beta score per class, and then performs a global average.
-
-    Parameters
-    ----------
-    beta
-        Weight of precision in harmonic mean.
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [0, 1, 2, 2, 2]
-    >>> y_pred = [0, 0, 2, 2, 1]
-
-    >>> metric = metrics.MacroFBeta(beta=.8)
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     print(metric.update(yt, yp))
-    MacroFBeta: 1.
-    MacroFBeta: 0.310606
-    MacroFBeta: 0.540404
-    MacroFBeta: 0.540404
-    MacroFBeta: 0.485982
-
-    """
-
-    def __init__(self, beta, cm=None):
-        super().__init__(cm)
-        self.beta = beta
-
-    def get(self):
-        total = 0
-        b2 = self.beta ** 2
-
-        for c in self.cm.classes:
-
-            try:
-                p = self.cm[c][c] / self.cm.sum_col[c]
-            except ZeroDivisionError:
-                p = 0
-
-            try:
-                r = self.cm[c][c] / self.cm.sum_row[c]
-            except ZeroDivisionError:
-                r = 0
-
-            try:
-                total += (1 + b2) * p * r / (b2 * p + r)
-            except ZeroDivisionError:
-                continue
-
-        try:
-            return total / len(self.cm.classes)
-        except ZeroDivisionError:
-            return 0.0
-
-
-class MicroFBeta(metrics.MultiClassMetric):
-    """Micro-average F-Beta score.
-
-    This computes the F-Beta score by merging all the predictions and true labels, and then
-    computes a global F-Beta score.
-
-    Parameters
-    ----------
-    beta
-        Weight of precision in the harmonic mean.
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [0, 1, 2, 2, 0]
-    >>> y_pred = [0, 1, 1, 2, 1]
-
-    >>> metric = metrics.MicroFBeta(beta=2)
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     metric = metric.update(yt, yp)
-
-    >>> metric
-    MicroFBeta: 0.6
-
-    References
-    ----------
-    1. [Why are precision, recall and F1 score equal when using micro averaging in a multi-class problem?](https://simonhessner.de/why-are-precision-recall-and-f1-score-equal-when-using-micro-averaging-in-a-multi-class-problem/)
-
-    """
-
-    def __init__(self, beta: float, cm=None):
-        super().__init__(cm)
-        self.beta = beta
-        self.precision = metrics.MicroPrecision(self.cm)
-        self.recall = metrics.MicroRecall(self.cm)
-
-    def get(self):
-        p = self.precision.get()
-        r = self.recall.get()
-        b2 = self.beta ** 2
-        try:
-            return (1 + b2) * p * r / (b2 * p + r)
-        except ZeroDivisionError:
-            return 0.0
-
-
-class WeightedFBeta(metrics.MultiClassMetric):
-    """Weighted-average F-Beta score.
-
-    This works by computing the F-Beta score per class, and then performs a global weighted average
-    according to the support of each class.
-
-    Parameters
-    ----------
-    beta
-        Weight of precision in the harmonic mean.
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [0, 1, 2, 2, 2]
-    >>> y_pred = [0, 0, 2, 2, 1]
-
-    >>> metric = metrics.WeightedFBeta(beta=0.8)
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     print(metric.update(yt, yp))
-    WeightedFBeta: 1.
-    WeightedFBeta: 0.310606
-    WeightedFBeta: 0.540404
-    WeightedFBeta: 0.655303
-    WeightedFBeta: 0.626283
-
-    """
-
-    def __init__(self, beta, cm=None):
-        super().__init__(cm)
-        self.beta = beta
-
-    def get(self):
-        total = 0
-        b2 = self.beta ** 2
-
-        for c in self.cm.classes:
-
-            try:
-                p = self.cm.sum_row[c] * self.cm[c][c] / self.cm.sum_col[c]
-            except ZeroDivisionError:
-                p = 0
-
-            try:
-                r = self.cm.sum_row[c] * self.cm[c][c] / self.cm.sum_row[c]
-            except ZeroDivisionError:
-                r = 0
-
-            try:
-                total += (1 + b2) * p * r / (b2 * p + r)
-            except ZeroDivisionError:
-                continue
-
-        try:
-            return total / self.cm.total_weight
-        except ZeroDivisionError:
-            return 0.0
-
-
-class MultiFBeta(metrics.MultiClassMetric):
-    """Multi-class F-Beta score with different betas per class.
-
-    The multiclass F-Beta score is the arithmetic average of the binary F-Beta scores of each class.
-    The mean can be weighted by providing class weights.
-
-    Parameters
-    ----------
-    betas
-        Weight of precision in the harmonic mean of each class.
-    weights
-        Class weights. If not provided then uniform weights will be used.
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [0, 1, 2, 2, 2]
-    >>> y_pred = [0, 0, 2, 2, 1]
-
-    >>> metric = metrics.MultiFBeta(
-    ...     betas={0: 0.25, 1: 1, 2: 4},
-    ...     weights={0: 1, 1: 1, 2: 2}
-    ... )
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     print(metric.update(yt, yp))
-    MultiFBeta: 1.
-    MultiFBeta: 0.257576
-    MultiFBeta: 0.628788
-    MultiFBeta: 0.628788
-    MultiFBeta: 0.468788
-
-    """
-
-    def __init__(self, betas, weights, cm=None):
-        super().__init__(cm)
-        self.betas = betas
-        self.weights = (
-            collections.defaultdict(functools.partial(int, 1))
-            if weights is None
-            else weights
-        )
-
-    def get(self):
-        total = 0
-
-        for c in self.cm.classes:
-
-            b2 = self.betas[c] ** 2
-
-            try:
-                p = self.cm[c][c] / self.cm.sum_col[c]
-            except ZeroDivisionError:
-                p = 0
-
-            try:
-                r = self.cm[c][c] / self.cm.sum_row[c]
-            except ZeroDivisionError:
-                r = 0
-
-            try:
-                total += self.weights[c] * (1 + b2) * p * r / (b2 * p + r)
-            except ZeroDivisionError:
-                continue
-
-        try:
-            return total / sum(self.weights[c] for c in self.cm.classes)
-        except ZeroDivisionError:
-            return 0.0
-
-
-class ExampleFBeta(metrics.MultiOutputClassificationMetric):
-    """Example-based F-Beta score.
-
-    Parameters
-    ----------
-    beta
-        Weight of precision in the harmonic mean.
-    cm
-        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
-        confusion matrix reduces the amount of storage and computation time.
-
-    Attributes
-    ----------
-    precision : metrics.ExamplePrecision
-    recall : metrics.ExampleRecall
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [
-    ...     {0: False, 1: True, 2: True},
-    ...     {0: True, 1: True, 2: False},
-    ...     {0: True, 1: True, 2: False},
-    ... ]
-
-    >>> y_pred = [
-    ...     {0: True, 1: True, 2: True},
-    ...     {0: True, 1: False, 2: False},
-    ...     {0: True, 1: True, 2: False},
-    ... ]
-
-    >>> metric = metrics.ExampleFBeta(beta=2)
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     metric = metric.update(yt, yp)
-
-    >>> metric
-    ExampleFBeta: 0.843882
-
-    """
-
-    @property
-    def bigger_is_better(self):
-        return True
-
-    @property
-    def requires_labels(self):
-        return True
-
-    def __init__(self, beta: float, cm=None):
-        super().__init__(cm)
-        self.beta = beta
-        self.precision = metrics.ExamplePrecision(self.cm)
-        self.recall = metrics.ExampleRecall(self.cm)
-
-    def get(self):
-        p = self.precision.get()
-        r = self.recall.get()
-        b2 = self.beta * self.beta
-        try:
-            return (1 + b2) * p * r / (b2 * p + r)
-        except ZeroDivisionError:
-            return 0.0
-
-
-class F1(FBeta):
-    """Binary F1 score.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-    pos_val
-        Value to treat as "positive".
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [False, False, False, True, True, True]
-    >>> y_pred = [False, False, True, True, False, False]
-
-    >>> metric = metrics.F1()
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     metric = metric.update(yt, yp)
-
-    >>> metric
-    F1: 0.4
-
-    """
-
-    def __init__(self, cm=None, pos_val=True):
-        super().__init__(beta=1.0, cm=cm, pos_val=pos_val)
-
-
-class MacroF1(MacroFBeta):
-    """Macro-average F1 score.
-
-    This works by computing the F1 score per class, and then performs a global average.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [0, 1, 2, 2, 2]
-    >>> y_pred = [0, 0, 2, 2, 1]
-
-    >>> metric = metrics.MacroF1()
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     print(metric.update(yt, yp))
-    MacroF1: 1.
-    MacroF1: 0.333333
-    MacroF1: 0.555556
-    MacroF1: 0.555556
-    MacroF1: 0.488889
-
-    """
-
-    def __init__(self, cm=None):
-        super().__init__(beta=1.0, cm=cm)
-
-
-class MicroF1(MicroFBeta):
-    """Micro-average F1 score.
-
-    This computes the F1 score by merging all the predictions and true labels, and then computes a
-    global F1 score.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [0, 1, 2, 2, 0]
-    >>> y_pred = [0, 1, 1, 2, 1]
-
-    >>> metric = metrics.MicroF1()
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     metric = metric.update(yt, yp)
-
-    >>> metric
-    MicroF1: 0.6
-
-    References
-    ----------
-    [^1]: [Why are precision, recall and F1 score equal when using micro averaging in a multi-class problem?](https://simonhessner.de/why-are-precision-recall-and-f1-score-equal-when-using-micro-averaging-in-a-multi-class-problem/)
-
-    """
-
-    def __init__(self, cm=None):
-        super().__init__(beta=1.0, cm=cm)
-
-
-class WeightedF1(WeightedFBeta):
-    """Weighted-average F1 score.
-
-    This works by computing the F1 score per class, and then performs a global weighted average by
-    using the support of each class.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-    pos_val
-        Value to treat as "positive".
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [0, 1, 2, 2, 2]
-    >>> y_pred = [0, 0, 2, 2, 1]
-
-    >>> metric = metrics.WeightedF1()
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     print(metric.update(yt, yp))
-    WeightedF1: 1.
-    WeightedF1: 0.333333
-    WeightedF1: 0.555556
-    WeightedF1: 0.666667
-    WeightedF1: 0.613333
-
-    """
-
-    def __init__(self, cm=None):
-        super().__init__(beta=1.0, cm=cm)
-
-
-class ExampleF1(ExampleFBeta):
-    """Example-based F1 score for multilabel classification.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [
-    ...     {0: False, 1: True, 2: True},
-    ...     {0: True, 1: True, 2: False},
-    ...     {0: True, 1: True, 2: False},
-    ... ]
-
-    >>> y_pred = [
-    ...     {0: True, 1: True, 2: True},
-    ...     {0: True, 1: False, 2: False},
-    ...     {0: True, 1: True, 2: False},
-    ... ]
-
-    >>> metric = metrics.ExampleF1()
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     metric = metric.update(yt, yp)
-
-    >>> metric
-    ExampleF1: 0.860215
-
-    """
-
-    def __init__(self, cm=None):
-        super().__init__(beta=1.0, cm=cm)
+import collections
+import functools
+
+from river import metrics
+
+__all__ = [
+    "F1",
+    "FBeta",
+    "MacroF1",
+    "MacroFBeta",
+    "MicroF1",
+    "MicroFBeta",
+    "MultiFBeta",
+    "WeightedF1",
+    "WeightedFBeta",
+    "ExampleF1",
+    "ExampleFBeta",
+]
+
+
+class FBeta(metrics.BinaryMetric):
+    """Binary F-Beta score.
+
+    The FBeta score is a weighted harmonic mean between precision and recall. The higher the
+    `beta` value, the higher the recall will be taken into account. When `beta` equals 1,
+    precision and recall and equivalently weighted, which results in the F1 score (see
+    `metrics.F1`).
+
+    Parameters
+    ----------
+    beta
+        Weight of precision in the harmonic mean.
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+    pos_val
+        Value to treat as "positive".
+
+    Attributes
+    ----------
+    precision : metrics.Precision
+    recall : metrics.Recall
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [False, False, False, True, True, True]
+    >>> y_pred = [False, False, True, True, False, False]
+
+    >>> metric = metrics.FBeta(beta=2)
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     metric = metric.update(yt, yp)
+
+    >>> metric
+    FBeta: 0.357143
+
+    """
+
+    def __init__(self, beta: float, cm=None, pos_val=True):
+        super().__init__(cm, pos_val)
+        self.beta = beta
+        self.precision = metrics.Precision(self.cm, self.pos_val)
+        self.recall = metrics.Recall(self.cm, self.pos_val)
+
+    def get(self):
+        p = self.precision.get()
+        r = self.recall.get()
+        b2 = self.beta ** 2
+        try:
+            return (1 + b2) * p * r / (b2 * p + r)
+        except ZeroDivisionError:
+            return 0.0
+
+
+class MacroFBeta(metrics.MultiClassMetric):
+    """Macro-average F-Beta score.
+
+    This works by computing the F-Beta score per class, and then performs a global average.
+
+    Parameters
+    ----------
+    beta
+        Weight of precision in harmonic mean.
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [0, 1, 2, 2, 2]
+    >>> y_pred = [0, 0, 2, 2, 1]
+
+    >>> metric = metrics.MacroFBeta(beta=.8)
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     print(metric.update(yt, yp))
+    MacroFBeta: 1.
+    MacroFBeta: 0.310606
+    MacroFBeta: 0.540404
+    MacroFBeta: 0.540404
+    MacroFBeta: 0.485982
+
+    """
+
+    def __init__(self, beta, cm=None):
+        super().__init__(cm)
+        self.beta = beta
+
+    def get(self):
+        total = 0
+        b2 = self.beta ** 2
+
+        for c in self.cm.classes:
+
+            try:
+                p = self.cm[c][c] / self.cm.sum_col[c]
+            except ZeroDivisionError:
+                p = 0
+
+            try:
+                r = self.cm[c][c] / self.cm.sum_row[c]
+            except ZeroDivisionError:
+                r = 0
+
+            try:
+                total += (1 + b2) * p * r / (b2 * p + r)
+            except ZeroDivisionError:
+                continue
+
+        try:
+            return total / len(self.cm.classes)
+        except ZeroDivisionError:
+            return 0.0
+
+
+class MicroFBeta(metrics.MultiClassMetric):
+    """Micro-average F-Beta score.
+
+    This computes the F-Beta score by merging all the predictions and true labels, and then
+    computes a global F-Beta score.
+
+    Parameters
+    ----------
+    beta
+        Weight of precision in the harmonic mean.
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [0, 1, 2, 2, 0]
+    >>> y_pred = [0, 1, 1, 2, 1]
+
+    >>> metric = metrics.MicroFBeta(beta=2)
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     metric = metric.update(yt, yp)
+
+    >>> metric
+    MicroFBeta: 0.6
+
+    References
+    ----------
+    1. [Why are precision, recall and F1 score equal when using micro averaging in a multi-class problem?](https://simonhessner.de/why-are-precision-recall-and-f1-score-equal-when-using-micro-averaging-in-a-multi-class-problem/)
+
+    """
+
+    def __init__(self, beta: float, cm=None):
+        super().__init__(cm)
+        self.beta = beta
+        self.precision = metrics.MicroPrecision(self.cm)
+        self.recall = metrics.MicroRecall(self.cm)
+
+    def get(self):
+        p = self.precision.get()
+        r = self.recall.get()
+        b2 = self.beta ** 2
+        try:
+            return (1 + b2) * p * r / (b2 * p + r)
+        except ZeroDivisionError:
+            return 0.0
+
+
+class WeightedFBeta(metrics.MultiClassMetric):
+    """Weighted-average F-Beta score.
+
+    This works by computing the F-Beta score per class, and then performs a global weighted average
+    according to the support of each class.
+
+    Parameters
+    ----------
+    beta
+        Weight of precision in the harmonic mean.
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [0, 1, 2, 2, 2]
+    >>> y_pred = [0, 0, 2, 2, 1]
+
+    >>> metric = metrics.WeightedFBeta(beta=0.8)
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     print(metric.update(yt, yp))
+    WeightedFBeta: 1.
+    WeightedFBeta: 0.310606
+    WeightedFBeta: 0.540404
+    WeightedFBeta: 0.655303
+    WeightedFBeta: 0.626283
+
+    """
+
+    def __init__(self, beta, cm=None):
+        super().__init__(cm)
+        self.beta = beta
+
+    def get(self):
+        total = 0
+        b2 = self.beta ** 2
+
+        for c in self.cm.classes:
+
+            try:
+                p = self.cm.sum_row[c] * self.cm[c][c] / self.cm.sum_col[c]
+            except ZeroDivisionError:
+                p = 0
+
+            try:
+                r = self.cm.sum_row[c] * self.cm[c][c] / self.cm.sum_row[c]
+            except ZeroDivisionError:
+                r = 0
+
+            try:
+                total += (1 + b2) * p * r / (b2 * p + r)
+            except ZeroDivisionError:
+                continue
+
+        try:
+            return total / self.cm.total_weight
+        except ZeroDivisionError:
+            return 0.0
+
+
+class MultiFBeta(metrics.MultiClassMetric):
+    """Multi-class F-Beta score with different betas per class.
+
+    The multiclass F-Beta score is the arithmetic average of the binary F-Beta scores of each class.
+    The mean can be weighted by providing class weights.
+
+    Parameters
+    ----------
+    betas
+        Weight of precision in the harmonic mean of each class.
+    weights
+        Class weights. If not provided then uniform weights will be used.
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [0, 1, 2, 2, 2]
+    >>> y_pred = [0, 0, 2, 2, 1]
+
+    >>> metric = metrics.MultiFBeta(
+    ...     betas={0: 0.25, 1: 1, 2: 4},
+    ...     weights={0: 1, 1: 1, 2: 2}
+    ... )
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     print(metric.update(yt, yp))
+    MultiFBeta: 1.
+    MultiFBeta: 0.257576
+    MultiFBeta: 0.628788
+    MultiFBeta: 0.628788
+    MultiFBeta: 0.468788
+
+    """
+
+    def __init__(self, betas, weights, cm=None):
+        super().__init__(cm)
+        self.betas = betas
+        self.weights = (
+            collections.defaultdict(functools.partial(int, 1))
+            if weights is None
+            else weights
+        )
+
+    def get(self):
+        total = 0
+
+        for c in self.cm.classes:
+
+            b2 = self.betas[c] ** 2
+
+            try:
+                p = self.cm[c][c] / self.cm.sum_col[c]
+            except ZeroDivisionError:
+                p = 0
+
+            try:
+                r = self.cm[c][c] / self.cm.sum_row[c]
+            except ZeroDivisionError:
+                r = 0
+
+            try:
+                total += self.weights[c] * (1 + b2) * p * r / (b2 * p + r)
+            except ZeroDivisionError:
+                continue
+
+        try:
+            return total / sum(self.weights[c] for c in self.cm.classes)
+        except ZeroDivisionError:
+            return 0.0
+
+
+class ExampleFBeta(metrics.MultiOutputClassificationMetric):
+    """Example-based F-Beta score.
+
+    Parameters
+    ----------
+    beta
+        Weight of precision in the harmonic mean.
+    cm
+        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
+        confusion matrix reduces the amount of storage and computation time.
+
+    Attributes
+    ----------
+    precision : metrics.ExamplePrecision
+    recall : metrics.ExampleRecall
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [
+    ...     {0: False, 1: True, 2: True},
+    ...     {0: True, 1: True, 2: False},
+    ...     {0: True, 1: True, 2: False},
+    ... ]
+
+    >>> y_pred = [
+    ...     {0: True, 1: True, 2: True},
+    ...     {0: True, 1: False, 2: False},
+    ...     {0: True, 1: True, 2: False},
+    ... ]
+
+    >>> metric = metrics.ExampleFBeta(beta=2)
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     metric = metric.update(yt, yp)
+
+    >>> metric
+    ExampleFBeta: 0.843882
+
+    """
+
+    @property
+    def bigger_is_better(self):
+        return True
+
+    @property
+    def requires_labels(self):
+        return True
+
+    def __init__(self, beta: float, cm=None):
+        super().__init__(cm)
+        self.beta = beta
+        self.precision = metrics.ExamplePrecision(self.cm)
+        self.recall = metrics.ExampleRecall(self.cm)
+
+    def get(self):
+        p = self.precision.get()
+        r = self.recall.get()
+        b2 = self.beta * self.beta
+        try:
+            return (1 + b2) * p * r / (b2 * p + r)
+        except ZeroDivisionError:
+            return 0.0
+
+
+class F1(FBeta):
+    """Binary F1 score.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+    pos_val
+        Value to treat as "positive".
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [False, False, False, True, True, True]
+    >>> y_pred = [False, False, True, True, False, False]
+
+    >>> metric = metrics.F1()
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     metric = metric.update(yt, yp)
+
+    >>> metric
+    F1: 0.4
+
+    """
+
+    def __init__(self, cm=None, pos_val=True):
+        super().__init__(beta=1.0, cm=cm, pos_val=pos_val)
+
+
+class MacroF1(MacroFBeta):
+    """Macro-average F1 score.
+
+    This works by computing the F1 score per class, and then performs a global average.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [0, 1, 2, 2, 2]
+    >>> y_pred = [0, 0, 2, 2, 1]
+
+    >>> metric = metrics.MacroF1()
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     print(metric.update(yt, yp))
+    MacroF1: 1.
+    MacroF1: 0.333333
+    MacroF1: 0.555556
+    MacroF1: 0.555556
+    MacroF1: 0.488889
+
+    """
+
+    def __init__(self, cm=None):
+        super().__init__(beta=1.0, cm=cm)
+
+
+class MicroF1(MicroFBeta):
+    """Micro-average F1 score.
+
+    This computes the F1 score by merging all the predictions and true labels, and then computes a
+    global F1 score.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [0, 1, 2, 2, 0]
+    >>> y_pred = [0, 1, 1, 2, 1]
+
+    >>> metric = metrics.MicroF1()
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     metric = metric.update(yt, yp)
+
+    >>> metric
+    MicroF1: 0.6
+
+    References
+    ----------
+    [^1]: [Why are precision, recall and F1 score equal when using micro averaging in a multi-class problem?](https://simonhessner.de/why-are-precision-recall-and-f1-score-equal-when-using-micro-averaging-in-a-multi-class-problem/)
+
+    """
+
+    def __init__(self, cm=None):
+        super().__init__(beta=1.0, cm=cm)
+
+
+class WeightedF1(WeightedFBeta):
+    """Weighted-average F1 score.
+
+    This works by computing the F1 score per class, and then performs a global weighted average by
+    using the support of each class.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+    pos_val
+        Value to treat as "positive".
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [0, 1, 2, 2, 2]
+    >>> y_pred = [0, 0, 2, 2, 1]
+
+    >>> metric = metrics.WeightedF1()
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     print(metric.update(yt, yp))
+    WeightedF1: 1.
+    WeightedF1: 0.333333
+    WeightedF1: 0.555556
+    WeightedF1: 0.666667
+    WeightedF1: 0.613333
+
+    """
+
+    def __init__(self, cm=None):
+        super().__init__(beta=1.0, cm=cm)
+
+
+class ExampleF1(ExampleFBeta):
+    """Example-based F1 score for multilabel classification.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [
+    ...     {0: False, 1: True, 2: True},
+    ...     {0: True, 1: True, 2: False},
+    ...     {0: True, 1: True, 2: False},
+    ... ]
+
+    >>> y_pred = [
+    ...     {0: True, 1: True, 2: True},
+    ...     {0: True, 1: False, 2: False},
+    ...     {0: True, 1: True, 2: False},
+    ... ]
+
+    >>> metric = metrics.ExampleF1()
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     metric = metric.update(yt, yp)
+
+    >>> metric
+    ExampleF1: 0.860215
+
+    """
+
+    def __init__(self, cm=None):
+        super().__init__(beta=1.0, cm=cm)
```

### Comparing `river-0.8.0/river/metrics/fowlkes_mallows.py` & `river-0.9.0/river/metrics/fowlkes_mallows.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,98 +1,98 @@
-import math
-
-from river import metrics
-
-from . import base
-
-__all__ = ["FowlkesMallows"]
-
-
-class FowlkesMallows(base.MultiClassMetric):
-    r"""Fowlkes-Mallows Index.
-
-    The Fowlkes-Mallows Index [^1] [^2] is an external evaluation method that is
-    used to determine the similarity between two clusterings, and also a mmetric
-    to measure confusion matrices. The measure of similarity could be either between
-    two hierarchical clusterings or a clustering and a benchmark classification. A
-    higher value for the Fowlkes-Mallows index indicates a greater similarity between
-    the clusters and the benchmark classifications.
-
-    The Fowlkes-Mallows Index, for two cluster algorithms, is defined as:
-
-    $$
-    FM = \sqrt{PPV \times TPR} = \sqrt{\frac{TP}{TP+FP} \times \frac{TP}{TP+FN}}
-    $$
-
-    where
-
-    * TP, FP, FN are respectively the number of true positives, false positives and
-    false negatives;
-
-    * TPR is the True Positive Rate (or Sensitivity/Recall), PPV is the Positive Predictive
-    Rate (or Precision).
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-
-    Examples
-    --------
-    >>> from river import metrics
-
-    >>> y_true = [1, 1, 2, 2, 3, 3]
-    >>> y_pred = [1, 1, 1, 2, 2, 2]
-
-    >>> metric = metrics.FowlkesMallows()
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     print(metric.update(yt, yp).get())
-    0.0
-    1.0
-    0.5773502691896257
-    0.408248290463863
-    0.3535533905932738
-    0.4714045207910317
-
-    >>> metric
-    FowlkesMallows: 0.471405
-
-    References
-    ----------
-    [^1]: Wikipedia contributors. (2020, December 22).
-          Fowlkes–Mallows index. In Wikipedia, The Free Encyclopedia,
-          from https://en.wikipedia.org/w/index.php?title=Fowlkes%E2%80%93Mallows_index&oldid=995714222
-    [^2]: E. B. Fowkles and C. L. Mallows (1983).
-          “A method for comparing two hierarchical clusterings”.
-          Journal of the American Statistical Association
-
-    """
-
-    def __init__(self, cm=None):
-        super().__init__(cm)
-
-    @property
-    def works_with_weights(self):
-        return False
-
-    def get(self):
-
-        pair_confusion_matrix = metrics.PairConfusionMatrix(self.cm).get()
-
-        true_positives = pair_confusion_matrix[1][1]
-        false_positives = pair_confusion_matrix[0][1]
-        false_negatives = pair_confusion_matrix[1][0]
-
-        try:
-            ppv = true_positives / (true_positives + false_positives)
-        except ZeroDivisionError:
-            ppv = 0.0
-
-        try:
-            tpr = true_positives / (true_positives + false_negatives)
-        except ZeroDivisionError:
-            tpr = 0.0
-
-        return math.sqrt(ppv * tpr)
+import math
+
+from river import metrics
+
+from . import base
+
+__all__ = ["FowlkesMallows"]
+
+
+class FowlkesMallows(base.MultiClassMetric):
+    r"""Fowlkes-Mallows Index.
+
+    The Fowlkes-Mallows Index [^1] [^2] is an external evaluation method that is
+    used to determine the similarity between two clusterings, and also a mmetric
+    to measure confusion matrices. The measure of similarity could be either between
+    two hierarchical clusterings or a clustering and a benchmark classification. A
+    higher value for the Fowlkes-Mallows index indicates a greater similarity between
+    the clusters and the benchmark classifications.
+
+    The Fowlkes-Mallows Index, for two cluster algorithms, is defined as:
+
+    $$
+    FM = \sqrt{PPV \times TPR} = \sqrt{\frac{TP}{TP+FP} \times \frac{TP}{TP+FN}}
+    $$
+
+    where
+
+    * TP, FP, FN are respectively the number of true positives, false positives and
+    false negatives;
+
+    * TPR is the True Positive Rate (or Sensitivity/Recall), PPV is the Positive Predictive
+    Rate (or Precision).
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+
+    Examples
+    --------
+    >>> from river import metrics
+
+    >>> y_true = [1, 1, 2, 2, 3, 3]
+    >>> y_pred = [1, 1, 1, 2, 2, 2]
+
+    >>> metric = metrics.FowlkesMallows()
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     print(metric.update(yt, yp).get())
+    0.0
+    1.0
+    0.5773502691896257
+    0.408248290463863
+    0.3535533905932738
+    0.4714045207910317
+
+    >>> metric
+    FowlkesMallows: 0.471405
+
+    References
+    ----------
+    [^1]: Wikipedia contributors. (2020, December 22).
+          Fowlkes–Mallows index. In Wikipedia, The Free Encyclopedia,
+          from https://en.wikipedia.org/w/index.php?title=Fowlkes%E2%80%93Mallows_index&oldid=995714222
+    [^2]: E. B. Fowkles and C. L. Mallows (1983).
+          “A method for comparing two hierarchical clusterings”.
+          Journal of the American Statistical Association
+
+    """
+
+    def __init__(self, cm=None):
+        super().__init__(cm)
+
+    @property
+    def works_with_weights(self):
+        return False
+
+    def get(self):
+
+        pair_confusion_matrix = metrics.PairConfusionMatrix(self.cm).get()
+
+        true_positives = pair_confusion_matrix[1][1]
+        false_positives = pair_confusion_matrix[0][1]
+        false_negatives = pair_confusion_matrix[1][0]
+
+        try:
+            ppv = true_positives / (true_positives + false_positives)
+        except ZeroDivisionError:
+            ppv = 0.0
+
+        try:
+            tpr = true_positives / (true_positives + false_negatives)
+        except ZeroDivisionError:
+            tpr = 0.0
+
+        return math.sqrt(ppv * tpr)
```

### Comparing `river-0.8.0/river/metrics/geometric_mean.py` & `river-0.9.0/river/metrics/geometric_mean.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,62 +1,62 @@
-import numpy as np
-from scipy import stats
-
-from . import base
-
-__all__ = ["GeometricMean"]
-
-
-class GeometricMean(base.MultiClassMetric):
-    r"""Geometric mean score.
-
-    The geometric mean is a good indicator of a classifier's performance in the presence of class
-    imbalance because it is independent of the distribution of examples between classes. This
-    implementation computes the geometric mean of class-wise sensitivity (recall).
-
-    $$
-    gm = \sqrt[n]{s_1\cdot s_2\cdot s_3\cdot \ldots\cdot s_n}
-    $$
-
-    where $s_i$ is the sensitivity (recall) of class $i$ and $n$ is the
-    number of classes.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
-        confusion matrix reduces the amount of storage and computation time.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = ['cat', 'ant', 'cat', 'cat', 'ant', 'bird', 'bird']
-    >>> y_pred = ['ant', 'ant', 'cat', 'cat', 'ant', 'cat', 'bird']
-
-    >>> metric = metrics.GeometricMean()
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     metric = metric.update(yt, yp)
-
-    >>> metric
-    GeometricMean: 0.693361
-
-    References
-    ----------
-    [^1]: Barandela, R. et al. “Strategies for learning in class imbalance problems”, Pattern Recognition, 36(3), (2003), pp 849-851.
-
-    """
-
-    def get(self):
-
-        if self.cm.n_classes > 0:
-            sensitivity_per_class = np.zeros(self.cm.n_classes, float)
-            for i, c in enumerate(self.cm.classes):
-                try:
-                    sensitivity_per_class[i] = self.cm[c][c] / self.cm.sum_row[c]
-                except ZeroDivisionError:
-                    continue
-            with np.errstate(divide="ignore", invalid="ignore"):
-                return stats.gmean(sensitivity_per_class)
-        return 0.0
+import numpy as np
+from scipy import stats
+
+from . import base
+
+__all__ = ["GeometricMean"]
+
+
+class GeometricMean(base.MultiClassMetric):
+    r"""Geometric mean score.
+
+    The geometric mean is a good indicator of a classifier's performance in the presence of class
+    imbalance because it is independent of the distribution of examples between classes. This
+    implementation computes the geometric mean of class-wise sensitivity (recall).
+
+    $$
+    gm = \sqrt[n]{s_1\cdot s_2\cdot s_3\cdot \ldots\cdot s_n}
+    $$
+
+    where $s_i$ is the sensitivity (recall) of class $i$ and $n$ is the
+    number of classes.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
+        confusion matrix reduces the amount of storage and computation time.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = ['cat', 'ant', 'cat', 'cat', 'ant', 'bird', 'bird']
+    >>> y_pred = ['ant', 'ant', 'cat', 'cat', 'ant', 'cat', 'bird']
+
+    >>> metric = metrics.GeometricMean()
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     metric = metric.update(yt, yp)
+
+    >>> metric
+    GeometricMean: 0.693361
+
+    References
+    ----------
+    [^1]: Barandela, R. et al. “Strategies for learning in class imbalance problems”, Pattern Recognition, 36(3), (2003), pp 849-851.
+
+    """
+
+    def get(self):
+
+        if self.cm.n_classes > 0:
+            sensitivity_per_class = np.zeros(self.cm.n_classes, float)
+            for i, c in enumerate(self.cm.classes):
+                try:
+                    sensitivity_per_class[i] = self.cm[c][c] / self.cm.sum_row[c]
+                except ZeroDivisionError:
+                    continue
+            with np.errstate(divide="ignore", invalid="ignore"):
+                return stats.gmean(sensitivity_per_class)
+        return 0.0
```

### Comparing `river-0.8.0/river/metrics/hamming.py` & `river-0.9.0/river/metrics/hamming.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,115 +1,115 @@
-import numpy as np
-
-from . import base
-
-__all__ = ["Hamming"]
-
-
-class Hamming(base.MultiOutputClassificationMetric):
-    """Hamming score.
-
-    The Hamming score is the fraction of labels that are correctly predicted.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
-        confusion matrix reduces the amount of storage and computation time.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [
-    ...     {0: False, 1: True, 2: True},
-    ...     {0: True, 1: True, 2: False},
-    ...     {0: True, 1: True, 2: False},
-    ... ]
-
-    >>> y_pred = [
-    ...     {0: True, 1: True, 2: True},
-    ...     {0: True, 1: False, 2: False},
-    ...     {0: True, 1: True, 2: False},
-    ... ]
-
-    >>> metric = metrics.Hamming()
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     metric = metric.update(yt, yp)
-
-    >>> metric
-    Hamming: 0.555556
-
-    """
-
-    @property
-    def bigger_is_better(self):
-        return True
-
-    @property
-    def requires_labels(self):
-        return True
-
-    def get(self):
-
-        try:
-            return np.sum(self.cm.data[:, 1, 1]) / (
-                self.cm.n_samples * self.cm.n_labels
-            )
-        except ZeroDivisionError:
-            return 0.0
-
-
-class HammingLoss(base.MultiOutputClassificationMetric):
-    """Hamming loss score.
-
-    The Hamming loss is the complement of the Hamming score.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
-        confusion matrix reduces the amount of storage and computation time.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [
-    ...     {0: False, 1: True, 2: True},
-    ...     {0: True, 1: True, 2: False},
-    ...     {0: True, 1: True, 2: False},
-    ... ]
-
-    >>> y_pred = [
-    ...     {0: True, 1: True, 2: True},
-    ...     {0: True, 1: False, 2: False},
-    ...     {0: True, 1: True, 2: False},
-    ... ]
-
-    >>> metric = metrics.HammingLoss()
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     metric = metric.update(yt, yp)
-
-    >>> metric
-    HammingLoss: 0.444444
-
-    """
-
-    @property
-    def bigger_is_better(self):
-        return True
-
-    @property
-    def requires_labels(self):
-        return True
-
-    def get(self):
-
-        try:
-            return 1.0 - np.sum(self.cm.data[:, 1, 1]) / (
-                self.cm.n_samples * self.cm.n_labels
-            )
-        except ZeroDivisionError:
-            return 0.0
+import numpy as np
+
+from . import base
+
+__all__ = ["Hamming"]
+
+
+class Hamming(base.MultiOutputClassificationMetric):
+    """Hamming score.
+
+    The Hamming score is the fraction of labels that are correctly predicted.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
+        confusion matrix reduces the amount of storage and computation time.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [
+    ...     {0: False, 1: True, 2: True},
+    ...     {0: True, 1: True, 2: False},
+    ...     {0: True, 1: True, 2: False},
+    ... ]
+
+    >>> y_pred = [
+    ...     {0: True, 1: True, 2: True},
+    ...     {0: True, 1: False, 2: False},
+    ...     {0: True, 1: True, 2: False},
+    ... ]
+
+    >>> metric = metrics.Hamming()
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     metric = metric.update(yt, yp)
+
+    >>> metric
+    Hamming: 0.555556
+
+    """
+
+    @property
+    def bigger_is_better(self):
+        return True
+
+    @property
+    def requires_labels(self):
+        return True
+
+    def get(self):
+
+        try:
+            return np.sum(self.cm.data[:, 1, 1]) / (
+                self.cm.n_samples * self.cm.n_labels
+            )
+        except ZeroDivisionError:
+            return 0.0
+
+
+class HammingLoss(base.MultiOutputClassificationMetric):
+    """Hamming loss score.
+
+    The Hamming loss is the complement of the Hamming score.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
+        confusion matrix reduces the amount of storage and computation time.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [
+    ...     {0: False, 1: True, 2: True},
+    ...     {0: True, 1: True, 2: False},
+    ...     {0: True, 1: True, 2: False},
+    ... ]
+
+    >>> y_pred = [
+    ...     {0: True, 1: True, 2: True},
+    ...     {0: True, 1: False, 2: False},
+    ...     {0: True, 1: True, 2: False},
+    ... ]
+
+    >>> metric = metrics.HammingLoss()
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     metric = metric.update(yt, yp)
+
+    >>> metric
+    HammingLoss: 0.444444
+
+    """
+
+    @property
+    def bigger_is_better(self):
+        return True
+
+    @property
+    def requires_labels(self):
+        return True
+
+    def get(self):
+
+        try:
+            return 1.0 - np.sum(self.cm.data[:, 1, 1]) / (
+                self.cm.n_samples * self.cm.n_labels
+            )
+        except ZeroDivisionError:
+            return 0.0
```

### Comparing `river-0.8.0/river/metrics/jaccard.py` & `river-0.9.0/river/metrics/jaccard.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,128 +1,128 @@
-from . import base
-
-__all__ = ["Jaccard", "SorensenDice"]
-
-
-class Jaccard(base.MultiOutputClassificationMetric):
-    """Jaccard index for binary multi-outputs.
-
-    The Jaccard index, or Jaccard similarity coefficient, defined as the size of the intersection
-    divided by the size of the union of two label sets, is used to compare the set of predicted
-    labels for a sample with the corresponding set of labels in `y_true`.
-
-    The Jaccard index may be a poor metric if there are no positives for some samples or labels.
-    The Jaccard index is undefined if there are no true or predicted labels, this implementation
-    will return a score of 0.0 if this is the case.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
-        confusion matrix reduces the amount of storage and computation time.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [
-    ...     {0: False, 1: True, 2: True},
-    ...     {0: True, 1: True, 2: False},
-    ... ]
-
-    >>> y_pred = [
-    ...     {0: True, 1: True, 2: True},
-    ...     {0: True, 1: False, 2: False},
-    ... ]
-
-    >>> jac = metrics.Jaccard()
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     jac = jac.update(yt, yp)
-
-    >>> jac
-    Jaccard: 0.583333
-
-    References
-    ----------
-    [^1]: [Wikipedia section on similarity of asymmetric binary attributes](https://www.wikiwand.com/en/Jaccard_index#/Similarity_of_asymmetric_binary_attributes)
-
-    """
-
-    @property
-    def bigger_is_better(self):
-        return True
-
-    @property
-    def requires_labels(self):
-        return True
-
-    def get(self):
-
-        try:
-            return self.cm.jaccard_sum / self.cm.n_samples
-        except ZeroDivisionError:
-            return 0.0
-
-
-class SorensenDice(Jaccard):
-    r"""Sørensen-Dice coefficient.
-
-    Sørensen-Dice coefficient [^1] (or Sørensen Index, Dice's coefficient) is a statistic used to gauge
-    the similarity of two samples. Sørensen's original formula was intended to be applied to discrete
-    data. Given two sets, $X$ and $Y$, it is defined as:
-
-    $$
-    DSC = \frac{2 |X \cap Y|}{|X| + |Y|}.
-    $$
-
-    It is equal to twice the number of elements common to both sets divided by the sum of the number of
-    elements in each set.
-
-    The coefficient is not very different in form from the Jaccard index. The only difference between the
-    two metrics is that the Jaccard index only counts true positives once in both the numerator and
-    denominator. In fact, both are equivalent in the sense that given a value for the Sorensen-Dice index,
-    once can canculate the respective Jaccard value and vice versa, using the equations
-
-    $$
-    \begin{equation}
-    J = \frac{S}{2-S}, \\ S = \frac{2J}{1+J}.
-    \end{equation}
-    $$
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
-        confusion matrix reduces the amount of storage and computation time.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [
-    ...     {0: False, 1: True, 2: True},
-    ...     {0: True, 1: True, 2: False},
-    ... ]
-
-    >>> y_pred = [
-    ...     {0: True, 1: True, 2: True},
-    ...     {0: True, 1: False, 2: False},
-    ... ]
-
-    >>> sorensen_dice = metrics.SorensenDice()
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     sorensen_dice = sorensen_dice.update(yt, yp)
-
-    >>> sorensen_dice
-    SorensenDice: 0.736842
-
-    References
-    ----------
-    [^1]: [Wikipedia article on Sørensen-Dice coefficient](https://en.wikipedia.org/wiki/Sørensen-Dice_coefficient)
-
-    """
-
-    def get(self):
-        j = super().get()
-        return 2 * j / (1 + j)
+from . import base
+
+__all__ = ["Jaccard", "SorensenDice"]
+
+
+class Jaccard(base.MultiOutputClassificationMetric):
+    """Jaccard index for binary multi-outputs.
+
+    The Jaccard index, or Jaccard similarity coefficient, defined as the size of the intersection
+    divided by the size of the union of two label sets, is used to compare the set of predicted
+    labels for a sample with the corresponding set of labels in `y_true`.
+
+    The Jaccard index may be a poor metric if there are no positives for some samples or labels.
+    The Jaccard index is undefined if there are no true or predicted labels, this implementation
+    will return a score of 0.0 if this is the case.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
+        confusion matrix reduces the amount of storage and computation time.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [
+    ...     {0: False, 1: True, 2: True},
+    ...     {0: True, 1: True, 2: False},
+    ... ]
+
+    >>> y_pred = [
+    ...     {0: True, 1: True, 2: True},
+    ...     {0: True, 1: False, 2: False},
+    ... ]
+
+    >>> jac = metrics.Jaccard()
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     jac = jac.update(yt, yp)
+
+    >>> jac
+    Jaccard: 0.583333
+
+    References
+    ----------
+    [^1]: [Wikipedia section on similarity of asymmetric binary attributes](https://www.wikiwand.com/en/Jaccard_index#/Similarity_of_asymmetric_binary_attributes)
+
+    """
+
+    @property
+    def bigger_is_better(self):
+        return True
+
+    @property
+    def requires_labels(self):
+        return True
+
+    def get(self):
+
+        try:
+            return self.cm.jaccard_sum / self.cm.n_samples
+        except ZeroDivisionError:
+            return 0.0
+
+
+class SorensenDice(Jaccard):
+    r"""Sørensen-Dice coefficient.
+
+    Sørensen-Dice coefficient [^1] (or Sørensen Index, Dice's coefficient) is a statistic used to gauge
+    the similarity of two samples. Sørensen's original formula was intended to be applied to discrete
+    data. Given two sets, $X$ and $Y$, it is defined as:
+
+    $$
+    DSC = \frac{2 |X \cap Y|}{|X| + |Y|}.
+    $$
+
+    It is equal to twice the number of elements common to both sets divided by the sum of the number of
+    elements in each set.
+
+    The coefficient is not very different in form from the Jaccard index. The only difference between the
+    two metrics is that the Jaccard index only counts true positives once in both the numerator and
+    denominator. In fact, both are equivalent in the sense that given a value for the Sorensen-Dice index,
+    once can canculate the respective Jaccard value and vice versa, using the equations
+
+    $$
+    \begin{equation}
+    J = \frac{S}{2-S}, \\ S = \frac{2J}{1+J}.
+    \end{equation}
+    $$
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
+        confusion matrix reduces the amount of storage and computation time.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [
+    ...     {0: False, 1: True, 2: True},
+    ...     {0: True, 1: True, 2: False},
+    ... ]
+
+    >>> y_pred = [
+    ...     {0: True, 1: True, 2: True},
+    ...     {0: True, 1: False, 2: False},
+    ... ]
+
+    >>> sorensen_dice = metrics.SorensenDice()
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     sorensen_dice = sorensen_dice.update(yt, yp)
+
+    >>> sorensen_dice
+    SorensenDice: 0.736842
+
+    References
+    ----------
+    [^1]: [Wikipedia article on Sørensen-Dice coefficient](https://en.wikipedia.org/wiki/Sørensen-Dice_coefficient)
+
+    """
+
+    def get(self):
+        j = super().get()
+        return 2 * j / (1 + j)
```

### Comparing `river-0.8.0/river/metrics/kappa.py` & `river-0.9.0/river/metrics/kappa.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,183 +1,183 @@
-from . import base
-
-__all__ = ["CohenKappa", "KappaT", "KappaM"]
-
-
-class CohenKappa(base.MultiClassMetric):
-    r"""Cohen's Kappa score.
-
-    Cohen's Kappa expresses the level of agreement between two annotators on a classification
-    problem. It is defined as
-
-    $$
-    \kappa = (p_o - p_e) / (1 - p_e)
-    $$
-
-    where $p_o$ is the empirical probability of agreement on the label
-    assigned to any sample (prequential accuracy), and $p_e$ is
-    the expected agreement when both annotators assign labels randomly.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = ['cat', 'ant', 'cat', 'cat', 'ant', 'bird']
-    >>> y_pred = ['ant', 'ant', 'cat', 'cat', 'ant', 'cat']
-
-    >>> metric = metrics.CohenKappa()
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     metric = metric.update(yt, yp)
-
-    >>> metric
-    CohenKappa: 0.428571
-
-    References
-    ----------
-    [^1]: J. Cohen (1960). "A coefficient of agreement for nominal scales". Educational and Psychological Measurement 20(1):37-46. doi:10.1177/001316446002000104.
-    """
-
-    def get(self):
-
-        try:
-            p0 = self.cm.sum_diag / self.cm.n_samples  # same as accuracy
-        except ZeroDivisionError:
-            p0 = 0
-
-        pe = 0
-
-        for c in self.cm.classes:
-            estimation_row = self.cm.sum_row[c] / self.cm.n_samples
-            estimation_col = self.cm.sum_col[c] / self.cm.n_samples
-            pe += estimation_row * estimation_col
-
-        try:
-            return (p0 - pe) / (1 - pe)
-        except ZeroDivisionError:
-            return 0.0
-
-
-class KappaM(base.MultiClassMetric):
-    r"""Kappa-M score.
-
-    The Kappa-M statistic compares performance with the majority class classifier.
-    It is defined as
-
-    $$
-    \kappa_{m} = (p_o - p_e) / (1 - p_e)
-    $$
-
-    where $p_o$ is the empirical probability of agreement on the label
-    assigned to any sample (prequential accuracy), and $p_e$ is
-    the prequential accuracy of the `majority classifier`.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
-        confusion matrix reduces the amount of storage and computation time.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = ['cat', 'ant', 'cat', 'cat', 'ant', 'bird', 'cat', 'ant', 'cat', 'cat', 'ant']
-    >>> y_pred = ['ant', 'ant', 'cat', 'cat', 'ant', 'cat', 'ant', 'ant', 'cat', 'cat', 'ant']
-
-    >>> metric = metrics.KappaM()
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     metric = metric.update(yt, yp)
-
-    >>> metric
-    KappaM: 0.25
-
-    References
-    ----------
-    [1^]: A. Bifet et al. "Efficient online evaluation of big data stream classifiers."
-        In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery
-        and data mining, pp. 59-68. ACM, 2015.
-
-    """
-
-    def get(self):
-
-        try:
-            p0 = self.cm.sum_diag / self.cm.n_samples  # same as accuracy
-        except ZeroDivisionError:
-            p0 = 0
-
-        try:
-            pe = self.cm.weight_majority_classifier / self.cm.n_samples
-            return (p0 - pe) / (1.0 - pe)
-        except ZeroDivisionError:
-            return 0.0
-
-
-class KappaT(base.MultiClassMetric):
-    r"""Kappa-T score.
-
-    The Kappa-T measures the temporal correlation between samples.
-    It is defined as
-
-    $$
-    \kappa_{t} = (p_o - p_e) / (1 - p_e)
-    $$
-
-    where $p_o$ is the empirical probability of agreement on the label
-    assigned to any sample (prequential accuracy), and $p_e$ is
-    the prequential accuracy of the `no-change classifier` that predicts
-    only using the last class seen by the classifier.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
-        confusion matrix reduces the amount of storage and computation time.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = ['cat', 'ant', 'cat', 'cat', 'ant', 'bird']
-    >>> y_pred = ['ant', 'ant', 'cat', 'cat', 'ant', 'cat']
-
-    >>> metric = metrics.KappaT()
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     metric = metric.update(yt, yp)
-
-    >>> metric
-    KappaT: 0.6
-
-    References
-    ----------
-    [^1]: A. Bifet et al. (2013). "Pitfalls in benchmarking data stream classification
-        and how to avoid them." Proc. of the European Conference on Machine Learning
-        and Principles and Practice of Knowledge Discovery in Databases (ECMLPKDD'13),
-        Springer LNAI 8188, p. 465-479.
-
-    """
-
-    def get(self):
-
-        try:
-            p0 = self.cm.sum_diag / self.cm.n_samples  # same as accuracy
-        except ZeroDivisionError:
-            p0 = 0
-
-        try:
-            pe = self.cm.weight_no_change_classifier / self.cm.n_samples
-            return (p0 - pe) / (1.0 - pe)
-        except ZeroDivisionError:
-            return 0.0
+from . import base
+
+__all__ = ["CohenKappa", "KappaT", "KappaM"]
+
+
+class CohenKappa(base.MultiClassMetric):
+    r"""Cohen's Kappa score.
+
+    Cohen's Kappa expresses the level of agreement between two annotators on a classification
+    problem. It is defined as
+
+    $$
+    \kappa = (p_o - p_e) / (1 - p_e)
+    $$
+
+    where $p_o$ is the empirical probability of agreement on the label
+    assigned to any sample (prequential accuracy), and $p_e$ is
+    the expected agreement when both annotators assign labels randomly.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = ['cat', 'ant', 'cat', 'cat', 'ant', 'bird']
+    >>> y_pred = ['ant', 'ant', 'cat', 'cat', 'ant', 'cat']
+
+    >>> metric = metrics.CohenKappa()
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     metric = metric.update(yt, yp)
+
+    >>> metric
+    CohenKappa: 0.428571
+
+    References
+    ----------
+    [^1]: J. Cohen (1960). "A coefficient of agreement for nominal scales". Educational and Psychological Measurement 20(1):37-46. doi:10.1177/001316446002000104.
+    """
+
+    def get(self):
+
+        try:
+            p0 = self.cm.sum_diag / self.cm.n_samples  # same as accuracy
+        except ZeroDivisionError:
+            p0 = 0
+
+        pe = 0
+
+        for c in self.cm.classes:
+            estimation_row = self.cm.sum_row[c] / self.cm.n_samples
+            estimation_col = self.cm.sum_col[c] / self.cm.n_samples
+            pe += estimation_row * estimation_col
+
+        try:
+            return (p0 - pe) / (1 - pe)
+        except ZeroDivisionError:
+            return 0.0
+
+
+class KappaM(base.MultiClassMetric):
+    r"""Kappa-M score.
+
+    The Kappa-M statistic compares performance with the majority class classifier.
+    It is defined as
+
+    $$
+    \kappa_{m} = (p_o - p_e) / (1 - p_e)
+    $$
+
+    where $p_o$ is the empirical probability of agreement on the label
+    assigned to any sample (prequential accuracy), and $p_e$ is
+    the prequential accuracy of the `majority classifier`.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
+        confusion matrix reduces the amount of storage and computation time.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = ['cat', 'ant', 'cat', 'cat', 'ant', 'bird', 'cat', 'ant', 'cat', 'cat', 'ant']
+    >>> y_pred = ['ant', 'ant', 'cat', 'cat', 'ant', 'cat', 'ant', 'ant', 'cat', 'cat', 'ant']
+
+    >>> metric = metrics.KappaM()
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     metric = metric.update(yt, yp)
+
+    >>> metric
+    KappaM: 0.25
+
+    References
+    ----------
+    [1^]: A. Bifet et al. "Efficient online evaluation of big data stream classifiers."
+        In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery
+        and data mining, pp. 59-68. ACM, 2015.
+
+    """
+
+    def get(self):
+
+        try:
+            p0 = self.cm.sum_diag / self.cm.n_samples  # same as accuracy
+        except ZeroDivisionError:
+            p0 = 0
+
+        try:
+            pe = self.cm.weight_majority_classifier / self.cm.n_samples
+            return (p0 - pe) / (1.0 - pe)
+        except ZeroDivisionError:
+            return 0.0
+
+
+class KappaT(base.MultiClassMetric):
+    r"""Kappa-T score.
+
+    The Kappa-T measures the temporal correlation between samples.
+    It is defined as
+
+    $$
+    \kappa_{t} = (p_o - p_e) / (1 - p_e)
+    $$
+
+    where $p_o$ is the empirical probability of agreement on the label
+    assigned to any sample (prequential accuracy), and $p_e$ is
+    the prequential accuracy of the `no-change classifier` that predicts
+    only using the last class seen by the classifier.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
+        confusion matrix reduces the amount of storage and computation time.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = ['cat', 'ant', 'cat', 'cat', 'ant', 'bird']
+    >>> y_pred = ['ant', 'ant', 'cat', 'cat', 'ant', 'cat']
+
+    >>> metric = metrics.KappaT()
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     metric = metric.update(yt, yp)
+
+    >>> metric
+    KappaT: 0.6
+
+    References
+    ----------
+    [^1]: A. Bifet et al. (2013). "Pitfalls in benchmarking data stream classification
+        and how to avoid them." Proc. of the European Conference on Machine Learning
+        and Principles and Practice of Knowledge Discovery in Databases (ECMLPKDD'13),
+        Springer LNAI 8188, p. 465-479.
+
+    """
+
+    def get(self):
+
+        try:
+            p0 = self.cm.sum_diag / self.cm.n_samples  # same as accuracy
+        except ZeroDivisionError:
+            p0 = 0
+
+        try:
+            pe = self.cm.weight_no_change_classifier / self.cm.n_samples
+            return (p0 - pe) / (1.0 - pe)
+        except ZeroDivisionError:
+            return 0.0
```

### Comparing `river-0.8.0/river/metrics/log_loss.py` & `river-0.9.0/river/metrics/log_loss.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,46 +1,46 @@
-import math
-
-from . import base
-
-__all__ = ["LogLoss"]
-
-
-class LogLoss(base.MeanMetric, base.BinaryMetric):
-    """Binary logarithmic loss.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [True, False, False, True]
-    >>> y_pred = [0.9,  0.1,   0.2,   0.65]
-
-    >>> metric = metrics.LogLoss()
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     metric = metric.update(yt, yp)
-    ...     print(metric.get())
-    0.105360
-    0.105360
-    0.144621
-    0.216161
-
-    >>> metric
-    LogLoss: 0.216162
-
-    """
-
-    @property
-    def bigger_is_better(self):
-        return False
-
-    @property
-    def requires_labels(self):
-        return False
-
-    def _eval(self, y_true, y_pred):
-        p_true = y_pred.get(True, 0.0) if isinstance(y_pred, dict) else y_pred
-        p_true = self._clamp_proba(p_true)
-        if y_true:
-            return -math.log(p_true)
-        return -math.log(1 - p_true)
+import math
+
+from . import base
+
+__all__ = ["LogLoss"]
+
+
+class LogLoss(base.MeanMetric, base.BinaryMetric):
+    """Binary logarithmic loss.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [True, False, False, True]
+    >>> y_pred = [0.9,  0.1,   0.2,   0.65]
+
+    >>> metric = metrics.LogLoss()
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     metric = metric.update(yt, yp)
+    ...     print(metric.get())
+    0.105360
+    0.105360
+    0.144621
+    0.216161
+
+    >>> metric
+    LogLoss: 0.216162
+
+    """
+
+    @property
+    def bigger_is_better(self):
+        return False
+
+    @property
+    def requires_labels(self):
+        return False
+
+    def _eval(self, y_true, y_pred):
+        p_true = y_pred.get(True, 0.0) if isinstance(y_pred, dict) else y_pred
+        p_true = self._clamp_proba(p_true)
+        if y_true:
+            return -math.log(p_true)
+        return -math.log(1 - p_true)
```

### Comparing `river-0.8.0/river/metrics/mcc.py` & `river-0.9.0/river/metrics/mcc.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,54 +1,54 @@
-import math
-
-from . import base
-
-__all__ = ["MCC"]
-
-
-class MCC(base.BinaryMetric):
-    """Matthews correlation coefficient.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
-        confusion matrix reduces the amount of storage and computation time.
-    pos_val
-        Value to treat as "positive".
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [True, True, True, False]
-    >>> y_pred = [True, False, True, True]
-
-    >>> mcc = metrics.MCC()
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     mcc = mcc.update(yt, yp)
-
-    >>> mcc
-    MCC: -0.333333
-
-    References
-    ----------
-    [^1]: [Wikipedia article](https://www.wikiwand.com/en/Matthews_correlation_coefficient)
-
-    """
-
-    def get(self):
-        tp = self.cm.true_positives(self.pos_val)
-        tn = self.cm.true_negatives(self.pos_val)
-        fp = self.cm.false_positives(self.pos_val)
-        fn = self.cm.false_negatives(self.pos_val)
-
-        n = (tp + tn + fp + fn) or 1
-        s = (tp + fn) / n
-        p = (tp + fp) / n
-
-        try:
-            return (tp / n - s * p) / math.sqrt(p * s * (1 - s) * (1 - p))
-        except ZeroDivisionError:
-            return 0.0
+import math
+
+from . import base
+
+__all__ = ["MCC"]
+
+
+class MCC(base.BinaryMetric):
+    """Matthews correlation coefficient.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
+        confusion matrix reduces the amount of storage and computation time.
+    pos_val
+        Value to treat as "positive".
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [True, True, True, False]
+    >>> y_pred = [True, False, True, True]
+
+    >>> mcc = metrics.MCC()
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     mcc = mcc.update(yt, yp)
+
+    >>> mcc
+    MCC: -0.333333
+
+    References
+    ----------
+    [^1]: [Wikipedia article](https://www.wikiwand.com/en/Matthews_correlation_coefficient)
+
+    """
+
+    def get(self):
+        tp = self.cm.true_positives(self.pos_val)
+        tn = self.cm.true_negatives(self.pos_val)
+        fp = self.cm.false_positives(self.pos_val)
+        fn = self.cm.false_negatives(self.pos_val)
+
+        n = (tp + tn + fp + fn) or 1
+        s = (tp + fn) / n
+        p = (tp + fp) / n
+
+        try:
+            return (tp / n - s * p) / math.sqrt(p * s * (1 - s) * (1 - p))
+        except ZeroDivisionError:
+            return 0.0
```

### Comparing `river-0.8.0/river/metrics/mutual_info.py` & `river-0.9.0/river/metrics/mutual_info.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,368 +1,368 @@
-import math
-
-import numpy as np
-
-from river import metrics
-
-__all__ = [
-    "AdjustedMutualInfo",
-    "MutualInfo",
-    "NormalizedMutualInfo",
-]
-
-
-class MutualInfo(metrics.MultiClassMetric):
-    r"""Mutual Information between two clusterings.
-
-    The Mutual Information [^1] is a measure of the similarity between two labels of
-    the same data. Where $|U_i|$ is the number of samples in cluster $U_i$ and $|V_j|$
-    is the number of the samples in cluster $V_j$, the Mutual Information between
-    clusterings $U$ and $V$ can be calculated as:
-
-    $$
-    MI(U,V) = \sum_{i=1}^{|U|} \sum_{v=1}^{|V|} \frac{|U_i \cup V_j|}{N} \log \frac{N |U_i \cup V_j|}{|U_i| |V_j|}
-    $$
-
-    This metric is independent of the absolute values of the labels: a permutation
-    of the class or cluster label values won't change the score.
-
-    This metric is furthermore symmetric: switching `y_true` and `y_pred` will return
-    the same score value. This can be useful to measure the agreement of two independent
-    label assignments strategies on the same dataset when the real ground truth is
-    not known.
-
-    The Mutual Information can be equivalently expressed as:
-
-    $$
-    MI(U,V) = H(U) - H(U | V) = H(V) - H(V | U)
-    $$
-
-    where $H(U)$ and $H(V)$ are the marginal entropies, $H(U | V)$ and $H(V | U)$ are the
-    conditional entropies.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [1, 1, 2, 2, 3, 3]
-    >>> y_pred = [1, 1, 1, 2, 2, 2]
-
-    >>> metric = metrics.MutualInfo()
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     print(metric.update(yt, yp).get())
-    0.0
-    0.0
-    0.0
-    0.215761
-    0.395752
-    0.462098
-
-    >>> metric
-    MutualInfo: 0.462098
-
-    References
-    ----------
-    [^1]: Wikipedia contributors. (2021, March 17). Mutual information.
-          In Wikipedia, The Free Encyclopedia,
-          from https://en.wikipedia.org/w/index.php?title=Mutual_information&oldid=1012714929
-    """
-
-    def __init__(self, cm=None):
-        super().__init__(cm)
-
-    @property
-    def works_with_weights(self):
-        return False
-
-    def get(self):
-
-        mutual_info_score = 0.0
-
-        for i in self.cm.classes:
-            for j in self.cm.classes:
-                try:
-                    temp = (
-                        self.cm[i][j]
-                        / self.cm.n_samples
-                        * (
-                            math.log(self.cm.n_samples * self.cm[i][j])
-                            - math.log(self.cm.sum_row[i] * self.cm.sum_col[j])
-                        )
-                    )
-                except (ValueError, ZeroDivisionError):
-                    continue
-                temp = 0.0 if (abs(temp) < np.finfo("float64").eps) else temp
-                # temp = 0.0 if temp < 0.0 else temp   # TODO confirm if we need to clip here
-                mutual_info_score += temp
-        return mutual_info_score
-
-
-class NormalizedMutualInfo(metrics.MultiClassMetric):
-    r"""Normalized Mutual Information between two clusterings.
-
-    Normalized Mutual Information (NMI) is a normalized version of the Mutual Information (MI) score
-    to scale the results between the range of 0 (no mutual information) and 1 (perfectly mutual
-    information). In the formula, the mutual information will be normalized by a generalized mean of
-    the entropy of true and predicted labels, defined by the `average_method`.
-
-    We note that this measure is not adjusted for chance (i.e corrected the effect of result
-    agreement solely due to chance); as a result, the Adjusted Mutual Info Score will mostly be preferred.
-    However, this metric is still symmetric, which means that switching true and predicted labels will not
-    alter the score value. This fact can be useful when the metric is used to measure the agreement between
-    two indepedent label solutions on the same dataset, when the ground truth remains unknown.
-
-    Another advantage of the metric is that as it is based on the calculation of entropy-related measures,
-    it is independent of the permutation of class/cluster labels.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-
-    average_method
-        This parameter defines how to compute the normalizer in the denominator.
-        Possible options include `min`, `max`, `arithmetic` and `geometric`.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [1, 1, 2, 2, 3, 3]
-    >>> y_pred = [1, 1, 1, 2, 2, 2]
-
-    >>> metric = metrics.NormalizedMutualInfo()
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     print(metric.update(yt, yp).get())
-    1.0
-    1.0
-    0.0
-    0.343711
-    0.458065
-    0.515803
-
-    >>> metric
-    NormalizedMutualInfo: 0.515804
-
-    References
-    ----------
-    [^1]: Wikipedia contributors. (2021, March 17). Mutual information.
-          In Wikipedia, The Free Encyclopedia,
-          from https://en.wikipedia.org/w/index.php?title=Mutual_information&oldid=1012714929
-    """
-    _AVERAGE_MIN = "min"
-    _AVERAGE_MAX = "max"
-    _AVERAGE_GEOMETRIC = "geometric"
-    _AVERAGE_ARITHMETIC = "arithmetic"
-    _VALID_AVERAGE = [
-        _AVERAGE_MIN,
-        _AVERAGE_MAX,
-        _AVERAGE_GEOMETRIC,
-        _AVERAGE_ARITHMETIC,
-    ]
-
-    def __init__(self, cm=None, average_method="arithmetic"):
-        super().__init__(cm)
-        if average_method not in self._VALID_AVERAGE:
-            raise ValueError(
-                f"Valid 'average_methods' are {self._VALID_AVERAGE}, "
-                f"but {average_method} was passed."
-            )
-        self.average_method = average_method
-        if average_method == self._AVERAGE_MIN:
-            self._generalized_average = _average_min
-        elif average_method == self._AVERAGE_MAX:
-            self._generalized_average = _average_max
-        elif average_method == self._AVERAGE_GEOMETRIC:
-            self._generalized_average = _average_geometric
-        else:  # average_method == self._AVERAGE_ARITHMETIC
-            self._generalized_average = _average_arithmetic
-
-    @property
-    def works_with_weights(self):
-        return False
-
-    def get(self):
-
-        n_classes = len([i for i in self.cm.sum_row.values() if i != 0])
-        n_clusters = len([i for i in self.cm.sum_col.values() if i != 0])
-
-        if (n_classes == n_clusters == 1) or (n_classes == n_clusters == 0):
-            return 1.0
-
-        mutual_info_score = metrics.MutualInfo(self.cm).get()
-
-        entropy_true = _entropy(cm=self.cm, y_true=True)
-        entropy_pred = _entropy(cm=self.cm, y_true=False)
-
-        normalizer = self._generalized_average(entropy_true, entropy_pred)
-
-        normalizer = max(normalizer, np.finfo("float64").eps)
-
-        return mutual_info_score / normalizer
-
-
-class AdjustedMutualInfo(metrics.MultiClassMetric):
-    r"""Adjusted Mutual Information between two clusterings.
-
-    Adjusted Mutual Information (AMI) is an adjustment of the Mutual Information score
-    that accounts for chance. It corrects the effect of agreement solely due to chance
-    between clusterings, similar to the way the Adjusted Rand Index corrects the Rand Index.
-    It is closely related to variation of information. The adjusted measure, however, is
-    no longer metrical.
-
-    For two clusterings $U$ and $V$, the Adjusted Mutual Information is calculated as:
-
-    $$
-    AMI(U, V) = \frac{MI(U, V) - E(MI(U, V))}{avg(H(U), H(V)) - E(MI(U, V))}
-    $$
-
-    This metric is independent of the permutation of the class or cluster label values;
-    furthermore, it is also symmetric. This can be useful to measure the agreement of
-    two label assignments strategies on the same dataset, regardless of the ground truth.
-
-    However, due to the complexity of the Expected Mutual Info Score, the computation of
-    this metric is an order of magnitude slower than most other metrics, in general.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-
-    average_method
-        This parameter defines how to compute the normalizer in the denominator.
-        Possible options include `min`, `max`, `arithmetic` and `geometric`.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [1, 1, 2, 2, 3, 3]
-    >>> y_pred = [1, 1, 1, 2, 2, 2]
-
-    >>> metric = metrics.AdjustedMutualInfo()
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     print(metric.update(yt, yp).get())
-    1.0
-    1.0
-    0.0
-    0.0
-    0.105891
-    0.298792
-
-    >>> metric
-    AdjustedMutualInfo: 0.298792
-
-    References
-    ----------
-    [^1]: Wikipedia contributors. (2021, March 17). Mutual information.
-          In Wikipedia, The Free Encyclopedia,
-          from https://en.wikipedia.org/w/index.php?title=Mutual_information&oldid=1012714929
-    """
-    _AVERAGE_MIN = "min"
-    _AVERAGE_MAX = "max"
-    _AVERAGE_GEOMETRIC = "geometric"
-    _AVERAGE_ARITHMETIC = "arithmetic"
-    _VALID_AVERAGE = [
-        _AVERAGE_MIN,
-        _AVERAGE_MAX,
-        _AVERAGE_GEOMETRIC,
-        _AVERAGE_ARITHMETIC,
-    ]
-
-    def __init__(self, cm=None, average_method="arithmetic"):
-        super().__init__(cm)
-        self.average_method = average_method
-        if average_method not in self._VALID_AVERAGE:
-            raise ValueError(
-                f"Valid 'average_methods' are {self._VALID_AVERAGE}, "
-                f"but {average_method} was passed."
-            )
-        self.average_method = average_method
-        if average_method == self._AVERAGE_MIN:
-            self._generalized_average = _average_min
-        elif average_method == self._AVERAGE_MAX:
-            self._generalized_average = _average_max
-        elif average_method == self._AVERAGE_GEOMETRIC:
-            self._generalized_average = _average_geometric
-        else:  # average_method == self._AVERAGE_ARITHMETIC
-            self._generalized_average = _average_arithmetic
-
-    @property
-    def works_with_weights(self):
-        return False
-
-    def get(self):
-
-        n_classes = len([i for i in self.cm.sum_row.values() if i != 0])
-        n_clusters = len([i for i in self.cm.sum_col.values() if i != 0])
-
-        if (n_classes == n_clusters == 1) or (n_classes == n_clusters == 0):
-            return 1.0
-
-        mutual_info_score = metrics.MutualInfo(self.cm).get()
-
-        expected_mutual_info_score = metrics.expected_mutual_info(self.cm)
-
-        entropy_true = _entropy(cm=self.cm, y_true=True)
-        entropy_pred = _entropy(cm=self.cm, y_true=False)
-
-        normalizer = self._generalized_average(entropy_true, entropy_pred)
-
-        denominator = normalizer - expected_mutual_info_score
-
-        if denominator < 0:
-            denominator = min(denominator, -np.finfo("float64").eps)
-        else:
-            denominator = max(denominator, np.finfo("float64").eps)
-
-        adjusted_mutual_info_score = (
-            mutual_info_score - expected_mutual_info_score
-        ) / denominator
-
-        return adjusted_mutual_info_score
-
-
-def _entropy(cm, y_true):
-    n_samples = cm.n_samples
-    if n_samples == 0:
-        return 1.0
-
-    if y_true:
-        values = cm.sum_row
-    else:
-        values = cm.sum_col
-    entropy = 0.0
-    for i in cm.classes:
-        if i in values and values[i] > 0:
-            entropy -= (values[i] / n_samples) * (np.log(values[i]) - np.log(n_samples))
-    return entropy
-
-
-def _average_min(u, v):
-    return min(u, v)
-
-
-def _average_max(u, v):
-    return max(u, v)
-
-
-def _average_geometric(u, v):
-    return math.sqrt(u * v)
-
-
-def _average_arithmetic(u, v):
-    return (u + v) / 2
+import math
+
+import numpy as np
+
+from river import metrics
+
+__all__ = [
+    "AdjustedMutualInfo",
+    "MutualInfo",
+    "NormalizedMutualInfo",
+]
+
+
+class MutualInfo(metrics.MultiClassMetric):
+    r"""Mutual Information between two clusterings.
+
+    The Mutual Information [^1] is a measure of the similarity between two labels of
+    the same data. Where $|U_i|$ is the number of samples in cluster $U_i$ and $|V_j|$
+    is the number of the samples in cluster $V_j$, the Mutual Information between
+    clusterings $U$ and $V$ can be calculated as:
+
+    $$
+    MI(U,V) = \sum_{i=1}^{|U|} \sum_{v=1}^{|V|} \frac{|U_i \cup V_j|}{N} \log \frac{N |U_i \cup V_j|}{|U_i| |V_j|}
+    $$
+
+    This metric is independent of the absolute values of the labels: a permutation
+    of the class or cluster label values won't change the score.
+
+    This metric is furthermore symmetric: switching `y_true` and `y_pred` will return
+    the same score value. This can be useful to measure the agreement of two independent
+    label assignments strategies on the same dataset when the real ground truth is
+    not known.
+
+    The Mutual Information can be equivalently expressed as:
+
+    $$
+    MI(U,V) = H(U) - H(U | V) = H(V) - H(V | U)
+    $$
+
+    where $H(U)$ and $H(V)$ are the marginal entropies, $H(U | V)$ and $H(V | U)$ are the
+    conditional entropies.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [1, 1, 2, 2, 3, 3]
+    >>> y_pred = [1, 1, 1, 2, 2, 2]
+
+    >>> metric = metrics.MutualInfo()
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     print(metric.update(yt, yp).get())
+    0.0
+    0.0
+    0.0
+    0.215761
+    0.395752
+    0.462098
+
+    >>> metric
+    MutualInfo: 0.462098
+
+    References
+    ----------
+    [^1]: Wikipedia contributors. (2021, March 17). Mutual information.
+          In Wikipedia, The Free Encyclopedia,
+          from https://en.wikipedia.org/w/index.php?title=Mutual_information&oldid=1012714929
+    """
+
+    def __init__(self, cm=None):
+        super().__init__(cm)
+
+    @property
+    def works_with_weights(self):
+        return False
+
+    def get(self):
+
+        mutual_info_score = 0.0
+
+        for i in self.cm.classes:
+            for j in self.cm.classes:
+                try:
+                    temp = (
+                        self.cm[i][j]
+                        / self.cm.n_samples
+                        * (
+                            math.log(self.cm.n_samples * self.cm[i][j])
+                            - math.log(self.cm.sum_row[i] * self.cm.sum_col[j])
+                        )
+                    )
+                except (ValueError, ZeroDivisionError):
+                    continue
+                temp = 0.0 if (abs(temp) < np.finfo("float64").eps) else temp
+                # temp = 0.0 if temp < 0.0 else temp   # TODO confirm if we need to clip here
+                mutual_info_score += temp
+        return mutual_info_score
+
+
+class NormalizedMutualInfo(metrics.MultiClassMetric):
+    r"""Normalized Mutual Information between two clusterings.
+
+    Normalized Mutual Information (NMI) is a normalized version of the Mutual Information (MI) score
+    to scale the results between the range of 0 (no mutual information) and 1 (perfectly mutual
+    information). In the formula, the mutual information will be normalized by a generalized mean of
+    the entropy of true and predicted labels, defined by the `average_method`.
+
+    We note that this measure is not adjusted for chance (i.e corrected the effect of result
+    agreement solely due to chance); as a result, the Adjusted Mutual Info Score will mostly be preferred.
+    However, this metric is still symmetric, which means that switching true and predicted labels will not
+    alter the score value. This fact can be useful when the metric is used to measure the agreement between
+    two indepedent label solutions on the same dataset, when the ground truth remains unknown.
+
+    Another advantage of the metric is that as it is based on the calculation of entropy-related measures,
+    it is independent of the permutation of class/cluster labels.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+
+    average_method
+        This parameter defines how to compute the normalizer in the denominator.
+        Possible options include `min`, `max`, `arithmetic` and `geometric`.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [1, 1, 2, 2, 3, 3]
+    >>> y_pred = [1, 1, 1, 2, 2, 2]
+
+    >>> metric = metrics.NormalizedMutualInfo()
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     print(metric.update(yt, yp).get())
+    1.0
+    1.0
+    0.0
+    0.343711
+    0.458065
+    0.515803
+
+    >>> metric
+    NormalizedMutualInfo: 0.515804
+
+    References
+    ----------
+    [^1]: Wikipedia contributors. (2021, March 17). Mutual information.
+          In Wikipedia, The Free Encyclopedia,
+          from https://en.wikipedia.org/w/index.php?title=Mutual_information&oldid=1012714929
+    """
+    _AVERAGE_MIN = "min"
+    _AVERAGE_MAX = "max"
+    _AVERAGE_GEOMETRIC = "geometric"
+    _AVERAGE_ARITHMETIC = "arithmetic"
+    _VALID_AVERAGE = [
+        _AVERAGE_MIN,
+        _AVERAGE_MAX,
+        _AVERAGE_GEOMETRIC,
+        _AVERAGE_ARITHMETIC,
+    ]
+
+    def __init__(self, cm=None, average_method="arithmetic"):
+        super().__init__(cm)
+        if average_method not in self._VALID_AVERAGE:
+            raise ValueError(
+                f"Valid 'average_methods' are {self._VALID_AVERAGE}, "
+                f"but {average_method} was passed."
+            )
+        self.average_method = average_method
+        if average_method == self._AVERAGE_MIN:
+            self._generalized_average = _average_min
+        elif average_method == self._AVERAGE_MAX:
+            self._generalized_average = _average_max
+        elif average_method == self._AVERAGE_GEOMETRIC:
+            self._generalized_average = _average_geometric
+        else:  # average_method == self._AVERAGE_ARITHMETIC
+            self._generalized_average = _average_arithmetic
+
+    @property
+    def works_with_weights(self):
+        return False
+
+    def get(self):
+
+        n_classes = len([i for i in self.cm.sum_row.values() if i != 0])
+        n_clusters = len([i for i in self.cm.sum_col.values() if i != 0])
+
+        if (n_classes == n_clusters == 1) or (n_classes == n_clusters == 0):
+            return 1.0
+
+        mutual_info_score = metrics.MutualInfo(self.cm).get()
+
+        entropy_true = _entropy(cm=self.cm, y_true=True)
+        entropy_pred = _entropy(cm=self.cm, y_true=False)
+
+        normalizer = self._generalized_average(entropy_true, entropy_pred)
+
+        normalizer = max(normalizer, np.finfo("float64").eps)
+
+        return mutual_info_score / normalizer
+
+
+class AdjustedMutualInfo(metrics.MultiClassMetric):
+    r"""Adjusted Mutual Information between two clusterings.
+
+    Adjusted Mutual Information (AMI) is an adjustment of the Mutual Information score
+    that accounts for chance. It corrects the effect of agreement solely due to chance
+    between clusterings, similar to the way the Adjusted Rand Index corrects the Rand Index.
+    It is closely related to variation of information. The adjusted measure, however, is
+    no longer metrical.
+
+    For two clusterings $U$ and $V$, the Adjusted Mutual Information is calculated as:
+
+    $$
+    AMI(U, V) = \frac{MI(U, V) - E(MI(U, V))}{avg(H(U), H(V)) - E(MI(U, V))}
+    $$
+
+    This metric is independent of the permutation of the class or cluster label values;
+    furthermore, it is also symmetric. This can be useful to measure the agreement of
+    two label assignments strategies on the same dataset, regardless of the ground truth.
+
+    However, due to the complexity of the Expected Mutual Info Score, the computation of
+    this metric is an order of magnitude slower than most other metrics, in general.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+
+    average_method
+        This parameter defines how to compute the normalizer in the denominator.
+        Possible options include `min`, `max`, `arithmetic` and `geometric`.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [1, 1, 2, 2, 3, 3]
+    >>> y_pred = [1, 1, 1, 2, 2, 2]
+
+    >>> metric = metrics.AdjustedMutualInfo()
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     print(metric.update(yt, yp).get())
+    1.0
+    1.0
+    0.0
+    0.0
+    0.105891
+    0.298792
+
+    >>> metric
+    AdjustedMutualInfo: 0.298792
+
+    References
+    ----------
+    [^1]: Wikipedia contributors. (2021, March 17). Mutual information.
+          In Wikipedia, The Free Encyclopedia,
+          from https://en.wikipedia.org/w/index.php?title=Mutual_information&oldid=1012714929
+    """
+    _AVERAGE_MIN = "min"
+    _AVERAGE_MAX = "max"
+    _AVERAGE_GEOMETRIC = "geometric"
+    _AVERAGE_ARITHMETIC = "arithmetic"
+    _VALID_AVERAGE = [
+        _AVERAGE_MIN,
+        _AVERAGE_MAX,
+        _AVERAGE_GEOMETRIC,
+        _AVERAGE_ARITHMETIC,
+    ]
+
+    def __init__(self, cm=None, average_method="arithmetic"):
+        super().__init__(cm)
+        self.average_method = average_method
+        if average_method not in self._VALID_AVERAGE:
+            raise ValueError(
+                f"Valid 'average_methods' are {self._VALID_AVERAGE}, "
+                f"but {average_method} was passed."
+            )
+        self.average_method = average_method
+        if average_method == self._AVERAGE_MIN:
+            self._generalized_average = _average_min
+        elif average_method == self._AVERAGE_MAX:
+            self._generalized_average = _average_max
+        elif average_method == self._AVERAGE_GEOMETRIC:
+            self._generalized_average = _average_geometric
+        else:  # average_method == self._AVERAGE_ARITHMETIC
+            self._generalized_average = _average_arithmetic
+
+    @property
+    def works_with_weights(self):
+        return False
+
+    def get(self):
+
+        n_classes = len([i for i in self.cm.sum_row.values() if i != 0])
+        n_clusters = len([i for i in self.cm.sum_col.values() if i != 0])
+
+        if (n_classes == n_clusters == 1) or (n_classes == n_clusters == 0):
+            return 1.0
+
+        mutual_info_score = metrics.MutualInfo(self.cm).get()
+
+        expected_mutual_info_score = metrics.expected_mutual_info(self.cm)
+
+        entropy_true = _entropy(cm=self.cm, y_true=True)
+        entropy_pred = _entropy(cm=self.cm, y_true=False)
+
+        normalizer = self._generalized_average(entropy_true, entropy_pred)
+
+        denominator = normalizer - expected_mutual_info_score
+
+        if denominator < 0:
+            denominator = min(denominator, -np.finfo("float64").eps)
+        else:
+            denominator = max(denominator, np.finfo("float64").eps)
+
+        adjusted_mutual_info_score = (
+            mutual_info_score - expected_mutual_info_score
+        ) / denominator
+
+        return adjusted_mutual_info_score
+
+
+def _entropy(cm, y_true):
+    n_samples = cm.n_samples
+    if n_samples == 0:
+        return 1.0
+
+    if y_true:
+        values = cm.sum_row
+    else:
+        values = cm.sum_col
+    entropy = 0.0
+    for i in cm.classes:
+        if i in values and values[i] > 0:
+            entropy -= (values[i] / n_samples) * (np.log(values[i]) - np.log(n_samples))
+    return entropy
+
+
+def _average_min(u, v):
+    return min(u, v)
+
+
+def _average_max(u, v):
+    return max(u, v)
+
+
+def _average_geometric(u, v):
+    return math.sqrt(u * v)
+
+
+def _average_arithmetic(u, v):
+    return (u + v) / 2
```

### Comparing `river-0.8.0/river/metrics/pair_confusion.py` & `river-0.9.0/river/metrics/pair_confusion.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,104 +1,104 @@
-import collections
-
-from river import metrics
-
-
-class PairConfusionMatrix(metrics.MultiClassMetric):
-    r"""Pair Confusion Matrix.
-
-    The pair confusion matrix $C$ is a 2 by 2 similarity matrix between two
-    clusterings by considering all pairs of samples and counting pairs that are
-    assigned into the same or into different clusters under the true and predicted
-    clusterings.
-
-    The pair confusion matrix has the following entries:
-
-    * $C[0][0]$ (**True Negatives**): number of pairs of points that are in different clusters
-    in both true and predicted labels
-
-    * $C[0][1]$ (**False Positives**): number of pairs of points that are in the same cluster
-    in predicted labels but not in predicted labels;
-
-    * $C[1][0]$ (**False Negatives**): number of pairs of points that are in the same cluster
-    in true labels but not in predicted labels;
-
-    * $C[1][1]$ (**True Positives**): number of pairs of points that are in the same cluster
-    in both true and predicted labels.
-
-    We can also show that the four counts have the following property
-
-    $$
-    TP + FP + FN + TV = \frac{n(n-1)}{2}
-    $$
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [0, 1, 2, 2, 2]
-    >>> y_pred = [0, 0, 2, 2, 1]
-
-    >>> pair_confusion_matrix = metrics.PairConfusionMatrix()
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     pair_confusion_matrix = pair_confusion_matrix.update(yt, yp)
-
-    >>> pair_confusion_matrix
-    PairConfusionMatrix: {0: defaultdict(<class 'int'>, {0: 12.0, 1: 2.0}), 1: defaultdict(<class 'int'>, {0: 4.0, 1: 2.0})}
-
-    """
-
-    def __init__(self, cm=None):
-        super().__init__(cm)
-
-    @property
-    def works_with_weights(self):
-        return False
-
-    def get(self):
-
-        pair_confusion_matrix = {i: collections.defaultdict(int) for i in range(2)}
-
-        sum_squares = 0
-        false_positives = 0
-        false_negatives = 0
-
-        for i in self.cm.classes:
-            for j in self.cm.classes:
-                sum_squares += self.cm[i][j] * self.cm[i][j]
-                false_positives += self.cm[i][j] * self.cm.sum_col[j]
-                false_negatives += self.cm[j][i] * self.cm.sum_row[j]
-
-        true_positives = sum_squares - self.cm.n_samples
-
-        false_positives = false_positives - sum_squares
-
-        false_negatives = false_negatives - sum_squares
-
-        true_negatives = (
-            self.cm.n_samples * self.cm.n_samples
-            - (false_positives + false_negatives)
-            - sum_squares
-        )
-
-        pair_confusion_matrix[0][0] = true_negatives
-        pair_confusion_matrix[0][1] = false_positives
-        pair_confusion_matrix[1][0] = false_negatives
-        pair_confusion_matrix[1][1] = true_positives
-
-        return pair_confusion_matrix
-
-    @property
-    def bigger_is_better(self):
-        raise NotImplementedError
-
-    def __repr__(self):
-        return f"{self.__class__.__name__}: {self.get()}"
+import collections
+
+from river import metrics
+
+
+class PairConfusionMatrix(metrics.MultiClassMetric):
+    r"""Pair Confusion Matrix.
+
+    The pair confusion matrix $C$ is a 2 by 2 similarity matrix between two
+    clusterings by considering all pairs of samples and counting pairs that are
+    assigned into the same or into different clusters under the true and predicted
+    clusterings.
+
+    The pair confusion matrix has the following entries:
+
+    * $C[0][0]$ (**True Negatives**): number of pairs of points that are in different clusters
+    in both true and predicted labels
+
+    * $C[0][1]$ (**False Positives**): number of pairs of points that are in the same cluster
+    in predicted labels but not in predicted labels;
+
+    * $C[1][0]$ (**False Negatives**): number of pairs of points that are in the same cluster
+    in true labels but not in predicted labels;
+
+    * $C[1][1]$ (**True Positives**): number of pairs of points that are in the same cluster
+    in both true and predicted labels.
+
+    We can also show that the four counts have the following property
+
+    $$
+    TP + FP + FN + TV = \frac{n(n-1)}{2}
+    $$
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [0, 1, 2, 2, 2]
+    >>> y_pred = [0, 0, 2, 2, 1]
+
+    >>> pair_confusion_matrix = metrics.PairConfusionMatrix()
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     pair_confusion_matrix = pair_confusion_matrix.update(yt, yp)
+
+    >>> pair_confusion_matrix
+    PairConfusionMatrix: {0: defaultdict(<class 'int'>, {0: 12.0, 1: 2.0}), 1: defaultdict(<class 'int'>, {0: 4.0, 1: 2.0})}
+
+    """
+
+    def __init__(self, cm=None):
+        super().__init__(cm)
+
+    @property
+    def works_with_weights(self):
+        return False
+
+    def get(self):
+
+        pair_confusion_matrix = {i: collections.defaultdict(int) for i in range(2)}
+
+        sum_squares = 0
+        false_positives = 0
+        false_negatives = 0
+
+        for i in self.cm.classes:
+            for j in self.cm.classes:
+                sum_squares += self.cm[i][j] * self.cm[i][j]
+                false_positives += self.cm[i][j] * self.cm.sum_col[j]
+                false_negatives += self.cm[j][i] * self.cm.sum_row[j]
+
+        true_positives = sum_squares - self.cm.n_samples
+
+        false_positives -= sum_squares
+
+        false_negatives -= sum_squares
+
+        true_negatives = (
+            self.cm.n_samples * self.cm.n_samples
+            - (false_positives + false_negatives)
+            - sum_squares
+        )
+
+        pair_confusion_matrix[0][0] = true_negatives
+        pair_confusion_matrix[0][1] = false_positives
+        pair_confusion_matrix[1][0] = false_negatives
+        pair_confusion_matrix[1][1] = true_positives
+
+        return pair_confusion_matrix
+
+    @property
+    def bigger_is_better(self):
+        raise NotImplementedError
+
+    def __repr__(self):
+        return f"{self.__class__.__name__}: {self.get()}"
```

### Comparing `river-0.8.0/river/metrics/precision.py` & `river-0.9.0/river/metrics/precision.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,238 +1,238 @@
-from river import metrics
-
-__all__ = [
-    "MacroPrecision",
-    "MicroPrecision",
-    "Precision",
-    "WeightedPrecision",
-    "ExamplePrecision",
-]
-
-
-class Precision(metrics.BinaryMetric):
-    """Binary precision score.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing
-        a confusion matrix reduces the amount of storage and computation time.
-    pos_val
-        Value to treat as "positive".
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [True, False, True, True, True]
-    >>> y_pred = [True, True, False, True, True]
-
-    >>> metric = metrics.Precision()
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     print(metric.update(yt, yp))
-    Precision: 1.
-    Precision: 0.5
-    Precision: 0.5
-    Precision: 0.666667
-    Precision: 0.75
-
-    """
-
-    def get(self):
-        tp = self.cm.true_positives(self.pos_val)
-        fp = self.cm.false_positives(self.pos_val)
-        try:
-            return tp / (tp + fp)
-        except ZeroDivisionError:
-            return 0.0
-
-
-class MacroPrecision(metrics.MultiClassMetric):
-    """Macro-average precision score.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [0, 1, 2, 2, 2]
-    >>> y_pred = [0, 0, 2, 2, 1]
-
-    >>> metric = metrics.MacroPrecision()
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     print(metric.update(yt, yp))
-    MacroPrecision: 1.
-    MacroPrecision: 0.25
-    MacroPrecision: 0.5
-    MacroPrecision: 0.5
-    MacroPrecision: 0.5
-
-    """
-
-    def get(self):
-        total = 0
-        for c in self.cm.classes:
-            try:
-                total += self.cm[c][c] / self.cm.sum_col[c]
-            except ZeroDivisionError:
-                continue
-        try:
-            return total / len(self.cm.classes)
-        except ZeroDivisionError:
-            return 0.0
-
-
-class MicroPrecision(metrics.MultiClassMetric):
-    """Micro-average precision score.
-
-    The micro-average precision score is exactly equivalent to the micro-average recall as well as
-    the micro-average F1 score.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [0, 1, 2, 2, 2]
-    >>> y_pred = [0, 0, 2, 2, 1]
-
-    >>> metric = metrics.MicroPrecision()
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     print(metric.update(yt, yp))
-    MicroPrecision: 1.
-    MicroPrecision: 0.5
-    MicroPrecision: 0.666667
-    MicroPrecision: 0.75
-    MicroPrecision: 0.6
-
-    References
-    ----------
-    [^1]: [Why are precision, recall and F1 score equal when using micro averaging in a multi-class problem?](https://simonhessner.de/why-are-precision-recall-and-f1-score-equal-when-using-micro-averaging-in-a-multi-class-problem/)
-
-    """
-
-    def get(self):
-        num = 0
-        den = 0
-        for c in self.cm.classes:
-            num += self.cm[c][c]
-            den += self.cm.sum_col[c]
-        try:
-            return num / den
-        except ZeroDivisionError:
-            return 0.0
-
-
-class WeightedPrecision(metrics.MultiClassMetric):
-    """Weighted-average precision score.
-
-    This uses the support of each label to compute an average score, whereas
-    `metrics.MacroPrecision` ignores the support.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [0, 1, 2, 2, 2]
-    >>> y_pred = [0, 0, 2, 2, 1]
-
-    >>> metric = metrics.WeightedPrecision()
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     print(metric.update(yt, yp))
-    WeightedPrecision: 1.
-    WeightedPrecision: 0.25
-    WeightedPrecision: 0.5
-    WeightedPrecision: 0.625
-    WeightedPrecision: 0.7
-
-    """
-
-    def get(self):
-        total = 0
-        for c in self.cm.classes:
-            try:
-                total += self.cm.sum_row[c] * self.cm[c][c] / self.cm.sum_col[c]
-            except ZeroDivisionError:
-                continue
-        try:
-            return total / self.cm.total_weight
-        except ZeroDivisionError:
-            return 0.0
-
-
-class ExamplePrecision(metrics.MultiOutputClassificationMetric):
-    """Example-based precision score for multilabel classification.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
-        confusion matrix reduces the amount of storage and computation time.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [
-    ...     {0: False, 1: True, 2: True},
-    ...     {0: True, 1: True, 2: False},
-    ...     {0: True, 1: True, 2: False},
-    ... ]
-
-    >>> y_pred = [
-    ...     {0: True, 1: True, 2: True},
-    ...     {0: True, 1: False, 2: False},
-    ...     {0: True, 1: True, 2: False},
-    ... ]
-
-    >>> metric = metrics.ExamplePrecision()
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     metric = metric.update(yt, yp)
-
-    >>> metric
-    ExamplePrecision: 0.888889
-
-    """
-
-    @property
-    def bigger_is_better(self):
-        return True
-
-    @property
-    def requires_labels(self):
-        return True
-
-    def get(self):
-
-        try:
-            return self.cm.precision_sum / self.cm.n_samples
-        except ZeroDivisionError:
-            return 0.0
+from river import metrics
+
+__all__ = [
+    "MacroPrecision",
+    "MicroPrecision",
+    "Precision",
+    "WeightedPrecision",
+    "ExamplePrecision",
+]
+
+
+class Precision(metrics.BinaryMetric):
+    """Binary precision score.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing
+        a confusion matrix reduces the amount of storage and computation time.
+    pos_val
+        Value to treat as "positive".
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [True, False, True, True, True]
+    >>> y_pred = [True, True, False, True, True]
+
+    >>> metric = metrics.Precision()
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     print(metric.update(yt, yp))
+    Precision: 1.
+    Precision: 0.5
+    Precision: 0.5
+    Precision: 0.666667
+    Precision: 0.75
+
+    """
+
+    def get(self):
+        tp = self.cm.true_positives(self.pos_val)
+        fp = self.cm.false_positives(self.pos_val)
+        try:
+            return tp / (tp + fp)
+        except ZeroDivisionError:
+            return 0.0
+
+
+class MacroPrecision(metrics.MultiClassMetric):
+    """Macro-average precision score.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [0, 1, 2, 2, 2]
+    >>> y_pred = [0, 0, 2, 2, 1]
+
+    >>> metric = metrics.MacroPrecision()
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     print(metric.update(yt, yp))
+    MacroPrecision: 1.
+    MacroPrecision: 0.25
+    MacroPrecision: 0.5
+    MacroPrecision: 0.5
+    MacroPrecision: 0.5
+
+    """
+
+    def get(self):
+        total = 0
+        for c in self.cm.classes:
+            try:
+                total += self.cm[c][c] / self.cm.sum_col[c]
+            except ZeroDivisionError:
+                continue
+        try:
+            return total / len(self.cm.classes)
+        except ZeroDivisionError:
+            return 0.0
+
+
+class MicroPrecision(metrics.MultiClassMetric):
+    """Micro-average precision score.
+
+    The micro-average precision score is exactly equivalent to the micro-average recall as well as
+    the micro-average F1 score.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [0, 1, 2, 2, 2]
+    >>> y_pred = [0, 0, 2, 2, 1]
+
+    >>> metric = metrics.MicroPrecision()
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     print(metric.update(yt, yp))
+    MicroPrecision: 1.
+    MicroPrecision: 0.5
+    MicroPrecision: 0.666667
+    MicroPrecision: 0.75
+    MicroPrecision: 0.6
+
+    References
+    ----------
+    [^1]: [Why are precision, recall and F1 score equal when using micro averaging in a multi-class problem?](https://simonhessner.de/why-are-precision-recall-and-f1-score-equal-when-using-micro-averaging-in-a-multi-class-problem/)
+
+    """
+
+    def get(self):
+        num = 0
+        den = 0
+        for c in self.cm.classes:
+            num += self.cm[c][c]
+            den += self.cm.sum_col[c]
+        try:
+            return num / den
+        except ZeroDivisionError:
+            return 0.0
+
+
+class WeightedPrecision(metrics.MultiClassMetric):
+    """Weighted-average precision score.
+
+    This uses the support of each label to compute an average score, whereas
+    `metrics.MacroPrecision` ignores the support.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [0, 1, 2, 2, 2]
+    >>> y_pred = [0, 0, 2, 2, 1]
+
+    >>> metric = metrics.WeightedPrecision()
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     print(metric.update(yt, yp))
+    WeightedPrecision: 1.
+    WeightedPrecision: 0.25
+    WeightedPrecision: 0.5
+    WeightedPrecision: 0.625
+    WeightedPrecision: 0.7
+
+    """
+
+    def get(self):
+        total = 0
+        for c in self.cm.classes:
+            try:
+                total += self.cm.sum_row[c] * self.cm[c][c] / self.cm.sum_col[c]
+            except ZeroDivisionError:
+                continue
+        try:
+            return total / self.cm.total_weight
+        except ZeroDivisionError:
+            return 0.0
+
+
+class ExamplePrecision(metrics.MultiOutputClassificationMetric):
+    """Example-based precision score for multilabel classification.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
+        confusion matrix reduces the amount of storage and computation time.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [
+    ...     {0: False, 1: True, 2: True},
+    ...     {0: True, 1: True, 2: False},
+    ...     {0: True, 1: True, 2: False},
+    ... ]
+
+    >>> y_pred = [
+    ...     {0: True, 1: True, 2: True},
+    ...     {0: True, 1: False, 2: False},
+    ...     {0: True, 1: True, 2: False},
+    ... ]
+
+    >>> metric = metrics.ExamplePrecision()
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     metric = metric.update(yt, yp)
+
+    >>> metric
+    ExamplePrecision: 0.888889
+
+    """
+
+    @property
+    def bigger_is_better(self):
+        return True
+
+    @property
+    def requires_labels(self):
+        return True
+
+    def get(self):
+
+        try:
+            return self.cm.precision_sum / self.cm.n_samples
+        except ZeroDivisionError:
+            return 0.0
```

### Comparing `river-0.8.0/river/metrics/prevalence_threshold.py` & `river-0.9.0/river/metrics/prevalence_threshold.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,95 +1,95 @@
-import math
-
-from river import metrics
-
-__all__ = ["PrevalenceThreshold"]
-
-
-class PrevalenceThreshold(metrics.BinaryMetric):
-    r"""Prevalence Threshold (PT).
-
-    The relationship between a positive predicted value and its target prevalence
-    is propotional - though not linear in all but a special case. In consequence,
-    there is a point of local extrema and maximum curvature defined only as a function
-    of the sensitivity and specificity beyond which the rate of change of a test's positive
-    predictive value drops at a differential pace relative to the disease prevalence.
-    Using differential equations, this point was first defined by Balayla et al. [^1] and
-    is termed the **prevalence threshold** (\phi_e).
-
-    The equation for the prevalence threshold [^2] is given by the following formula
-
-    $$
-    \phi_e = \frac{\sqrt{TPR(1 - TNR)} + TNR - 1}{TPR + TNR - 1}
-    $$
-
-    with
-
-    $$
-    TPR = \frac{TP}{P} = \frac{TP}{TP + FN}, TNR = = \frac{TN}{N} = \frac{TN}{TN + FP}
-    $$
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-    pos_val
-        Value to treat as "positive".
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [False, False, False, True, True, True]
-    >>> y_pred = [False, False, True, True, False, True]
-
-    >>> metric = metrics.PrevalenceThreshold()
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     print(metric.update(yt, yp).get())
-    0.0
-    0.0
-    1.0
-    0.36602540378443876
-    0.44948974278317827
-    0.41421356237309503
-
-    >>> metric
-    PrevalenceThreshold: 0.414214
-
-    References
-    ----------
-    [^1]: Balayla, J. (2020). Prevalence threshold ($\phi$_e) and the geometry of screening curves.
-          PLOS ONE, 15(10), e0240215. DOI: 10.1371/journal.pone.0240215
-    [^2]: Wikipedia contributors. (2021, March 19). Sensitivity and specificity.
-          In Wikipedia, The Free Encyclopedia,
-          from https://en.wikipedia.org/w/index.php?title=Sensitivity_and_specificity&oldid=1013004476
-
-    """
-
-    def __init__(self, cm=None, pos_val=True):
-        super().__init__(cm, pos_val)
-
-    def get(self):
-
-        try:
-            tpr = self.cm.true_positives(self.pos_val) / (
-                self.cm.true_positives(self.pos_val)
-                + self.cm.false_negatives(self.pos_val)
-            )
-        except ZeroDivisionError:
-            tpr = 0.0
-
-        try:
-            tnr = self.cm.true_negatives(self.pos_val) / (
-                self.cm.true_negatives(self.pos_val)
-                + self.cm.false_positives(self.pos_val)
-            )
-        except ZeroDivisionError:
-            tnr = 0.0
-
-        try:
-            return (math.sqrt(tpr * (1 - tnr)) + tnr - 1) / (tpr + tnr - 1)
-        except (ZeroDivisionError, ValueError):
-            return 0.0
+import math
+
+from river import metrics
+
+__all__ = ["PrevalenceThreshold"]
+
+
+class PrevalenceThreshold(metrics.BinaryMetric):
+    r"""Prevalence Threshold (PT).
+
+    The relationship between a positive predicted value and its target prevalence
+    is propotional - though not linear in all but a special case. In consequence,
+    there is a point of local extrema and maximum curvature defined only as a function
+    of the sensitivity and specificity beyond which the rate of change of a test's positive
+    predictive value drops at a differential pace relative to the disease prevalence.
+    Using differential equations, this point was first defined by Balayla et al. [^1] and
+    is termed the **prevalence threshold** (\phi_e).
+
+    The equation for the prevalence threshold [^2] is given by the following formula
+
+    $$
+    \phi_e = \frac{\sqrt{TPR(1 - TNR)} + TNR - 1}{TPR + TNR - 1}
+    $$
+
+    with
+
+    $$
+    TPR = \frac{TP}{P} = \frac{TP}{TP + FN}, TNR = = \frac{TN}{N} = \frac{TN}{TN + FP}
+    $$
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+    pos_val
+        Value to treat as "positive".
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [False, False, False, True, True, True]
+    >>> y_pred = [False, False, True, True, False, True]
+
+    >>> metric = metrics.PrevalenceThreshold()
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     print(metric.update(yt, yp).get())
+    0.0
+    0.0
+    1.0
+    0.36602540378443876
+    0.44948974278317827
+    0.41421356237309503
+
+    >>> metric
+    PrevalenceThreshold: 0.414214
+
+    References
+    ----------
+    [^1]: Balayla, J. (2020). Prevalence threshold ($\phi$_e) and the geometry of screening curves.
+          PLOS ONE, 15(10), e0240215. DOI: 10.1371/journal.pone.0240215
+    [^2]: Wikipedia contributors. (2021, March 19). Sensitivity and specificity.
+          In Wikipedia, The Free Encyclopedia,
+          from https://en.wikipedia.org/w/index.php?title=Sensitivity_and_specificity&oldid=1013004476
+
+    """
+
+    def __init__(self, cm=None, pos_val=True):
+        super().__init__(cm, pos_val)
+
+    def get(self):
+
+        try:
+            tpr = self.cm.true_positives(self.pos_val) / (
+                self.cm.true_positives(self.pos_val)
+                + self.cm.false_negatives(self.pos_val)
+            )
+        except ZeroDivisionError:
+            tpr = 0.0
+
+        try:
+            tnr = self.cm.true_negatives(self.pos_val) / (
+                self.cm.true_negatives(self.pos_val)
+                + self.cm.false_positives(self.pos_val)
+            )
+        except ZeroDivisionError:
+            tnr = 0.0
+
+        try:
+            return (math.sqrt(tpr * (1 - tnr)) + tnr - 1) / (tpr + tnr - 1)
+        except (ZeroDivisionError, ValueError):
+            return 0.0
```

### Comparing `river-0.8.0/river/metrics/purity.py` & `river-0.9.0/river/metrics/purity.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,70 +1,70 @@
-from river import metrics
-
-__all__ = ["Purity"]
-
-
-class Purity(metrics.MultiClassMetric):
-    r"""Purity.
-
-    In a similar fashion with Entropy, the purity of a clustering solution,
-    compared to the original true label is defined to be the fraction of the
-    overall cluster size that the largest class of documents assigned to that
-    cluster represents. The overall purity of the clustering solution is obtained
-    as a weighted sum of the individual cluster purities and is given by:
-
-    $$
-    Purity = \sum_{r=1}^k \frac{n_r}{n} \times \left( \frac{1}{n_r} \max_i (n^i_r) \right)
-    = \sum_{r=1}^k \frac{1}{n} \max_i (n^i_r)
-    $$
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [1, 1, 2, 2, 3, 3]
-    >>> y_pred = [1, 1, 1, 2, 2, 2]
-
-    >>> metric = metrics.Purity()
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     print(metric.update(yt, yp).get())
-    1.0
-    1.0
-    0.6666666666666666
-    0.75
-    0.6
-    0.6666666666666666
-
-    >>> metric
-    Purity: 0.666667
-
-    References
-    ----------
-    [^1]: Ying Zhao and George Karypis. 2001. Criterion functions for
-          ducument clustering: Experiments and analysis. Technical
-          Report TR 01–40, Department of Computer Science, University of Minnesota.
-
-    """
-
-    def __init__(self, cm=None):
-        super().__init__(cm)
-
-    def get(self):
-
-        purity = 0
-
-        for i in self.cm.classes:
-            max_entry_cluster_i = 0
-            for j in self.cm.classes:
-                if self.cm[j][i] > max_entry_cluster_i:
-                    max_entry_cluster_i = self.cm[j][i]
-            purity += max_entry_cluster_i
-
-        return purity / self.cm.n_samples
+from river import metrics
+
+__all__ = ["Purity"]
+
+
+class Purity(metrics.MultiClassMetric):
+    r"""Purity.
+
+    In a similar fashion with Entropy, the purity of a clustering solution,
+    compared to the original true label is defined to be the fraction of the
+    overall cluster size that the largest class of documents assigned to that
+    cluster represents. The overall purity of the clustering solution is obtained
+    as a weighted sum of the individual cluster purities and is given by:
+
+    $$
+    Purity = \sum_{r=1}^k \frac{n_r}{n} \times \left( \frac{1}{n_r} \max_i (n^i_r) \right)
+    = \sum_{r=1}^k \frac{1}{n} \max_i (n^i_r)
+    $$
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [1, 1, 2, 2, 3, 3]
+    >>> y_pred = [1, 1, 1, 2, 2, 2]
+
+    >>> metric = metrics.Purity()
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     print(metric.update(yt, yp).get())
+    1.0
+    1.0
+    0.6666666666666666
+    0.75
+    0.6
+    0.6666666666666666
+
+    >>> metric
+    Purity: 0.666667
+
+    References
+    ----------
+    [^1]: Ying Zhao and George Karypis. 2001. Criterion functions for
+          ducument clustering: Experiments and analysis. Technical
+          Report TR 01–40, Department of Computer Science, University of Minnesota.
+
+    """
+
+    def __init__(self, cm=None):
+        super().__init__(cm)
+
+    def get(self):
+
+        purity = 0
+
+        for i in self.cm.classes:
+            max_entry_cluster_i = 0
+            for j in self.cm.classes:
+                if self.cm[j][i] > max_entry_cluster_i:
+                    max_entry_cluster_i = self.cm[j][i]
+            purity += max_entry_cluster_i
+
+        return purity / self.cm.n_samples
```

### Comparing `river-0.8.0/river/metrics/q0.py` & `river-0.9.0/river/metrics/q0.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,197 +1,197 @@
-import math
-
-from scipy.special import factorial
-
-from . import base
-
-__all__ = ["Q0", "Q2"]
-
-
-class Q0(base.MultiClassMetric):
-    r"""Q0 index.
-
-    Dom's Q0 measure [^2] uses conditional entropy to calculate the goodness of
-    a clustering solution. However, this term only evaluates the homogeneity of
-    a solution. To measure the completeness of the hypothesized clustering, Dom
-    includes a model cost term calculated using a coding theory argument. The
-    overall clustering quality measure presented is the sum of the costs of
-    representing the data's conditional entropy and the model.
-
-    The motivation for this approach is an appeal to parsimony: Given identical
-    conditional entropies, H(C|K), the clustering solution with the fewest clusters
-    should be preferred.
-
-    The Q0 measure can be calculated using the following formula [^1]
-
-    $$
-    Q_0(C, K) = H(C|K) + \frac{1}{n} \sum_{k=1}^{|K|} \log \binom{h(c) + |C| - 1}{|C| - 1}.
-    $$
-
-    Due to the complexity of the formula, this metric and its associated normalized version (Q2)
-    is one order of magnitude slower than most other implemented metrics.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-
-    Examples
-    --------
-    >>> from river import metrics
-
-    >>> y_true = [1, 1, 2, 2, 3, 3]
-    >>> y_pred = [1, 1, 1, 2, 2, 2]
-
-    >>> metric = metrics.Q0()
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     print(metric.update(yt, yp).get())
-    0.0
-    0.0
-    0.9182958340544896
-    1.208582260960826
-    1.4479588303902937
-    1.3803939544277863
-
-    >>> metric
-    Q0: 1.380394
-
-    References
-    ----------
-    [^1]: Andrew Rosenberg and Julia Hirschberg (2007).
-          V-Measure: A conditional entropy-based external cluster evaluation measure.
-          Proceedings of the 2007 Joing Conference on Empirical Methods in Natural Language
-          Processing and Computational Natural Language Learning, pp. 410 - 420,
-          Prague, June 2007. URL: https://www.aclweb.org/anthology/D07-1043.pdf.
-    [^2]: Byron E. Dom. 2001. An information-theoretic external
-          cluster-validity measure. Technical Report RJ10219, IBM, October.
-
-    """
-
-    def __init__(self, cm=None):
-        super().__init__(cm)
-
-    @staticmethod
-    def binomial_coeff(n, k):
-        return factorial(n) / (factorial(k) * factorial(n - k))
-
-    def get(self):
-
-        conditional_entropy_c_k = 0.0
-
-        sum_logs = 0.0
-
-        n_true_clusters = sum(1 for i in self.cm.sum_col.values() if i > 0)
-
-        for i in self.cm.classes:
-
-            for j in self.cm.classes:
-
-                try:
-                    conditional_entropy_c_k -= (
-                        self.cm[j][i]
-                        / self.cm.n_samples
-                        * math.log(self.cm[j][i] / self.cm.sum_col[i], 2)
-                    )
-                except (ValueError, ZeroDivisionError):
-                    continue
-
-            try:
-                sum_logs += math.log(
-                    self.binomial_coeff(
-                        self.cm.sum_col[i] + n_true_clusters - 1, n_true_clusters - 1
-                    )
-                )
-            except ValueError:
-                continue
-
-        return conditional_entropy_c_k + sum_logs / self.cm.n_samples
-
-
-class Q2(Q0):
-    r"""Q2 index.
-
-    Q2 index is presented by Dom [^2] as a normalized version of the original Q0 index.
-    This index has a range of $(0, 1]$ [^1], with greater scores being representing more
-    preferred clustering.
-
-    The Q2 index can be calculated as follows [^1]
-
-    $$
-    Q2(C, K) = \frac{\frac{1}{n} \sum_{c=1}^{|C|} \log \binom{h(c) + |C| - 1}{|C| - 1} }{Q_0(C, K)}
-    $$
-
-    where $C$ is the target partition, $K$ is the hypothesized partition and $h(k)$ is
-    the size of cluster $k$.
-
-    Due to the complexity of the formula, this metric is one order of magnitude slower than
-    its original version (Q0) and most other implemented metrics.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-
-    Examples
-    --------
-    >>> from river import metrics
-
-    >>> y_true = [1, 1, 2, 2, 3, 3]
-    >>> y_pred = [1, 1, 1, 2, 2, 2]
-
-    >>> metric = metrics.Q2()
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     print(metric.update(yt, yp).get())
-    0.0
-    0.0
-    0.0
-    0.4545045563529578
-    0.39923396953448914
-    0.3979343306829813
-
-    >>> metric
-    Q2: 0.397934
-
-    References
-    ----------
-    [^1]: Andrew Rosenberg and Julia Hirschberg (2007).
-          V-Measure: A conditional entropy-based external cluster evaluation measure.
-          Proceedings of the 2007 Joing Conference on Empirical Methods in Natural Language
-          Processing and Computational Natural Language Learning, pp. 410 - 420,
-          Prague, June 2007. URL: https://www.aclweb.org/anthology/D07-1043.pdf.
-    [^2]: Byron E. Dom. 2001. An information-theoretic external
-          cluster-validity measure. Technical Report RJ10219, IBM, October.
-
-    """
-
-    def __init__(self, cm=None):
-        super().__init__(cm)
-
-    def get(self):
-
-        q0 = super().get()
-
-        sum_logs = 0.0
-
-        n_true_clusters = sum(1 for i in self.cm.sum_col.values() if i > 0)
-
-        for i in self.cm.classes:
-
-            try:
-                sum_logs += math.log(
-                    self.binomial_coeff(
-                        self.cm.sum_row[i] + n_true_clusters - 1, n_true_clusters - 1
-                    )
-                )
-            except ValueError:
-                continue
-
-        try:
-            return (sum_logs / self.cm.n_samples) / q0
-        except ZeroDivisionError:
-            return 0.0
+import math
+
+from scipy.special import factorial
+
+from . import base
+
+__all__ = ["Q0", "Q2"]
+
+
+class Q0(base.MultiClassMetric):
+    r"""Q0 index.
+
+    Dom's Q0 measure [^2] uses conditional entropy to calculate the goodness of
+    a clustering solution. However, this term only evaluates the homogeneity of
+    a solution. To measure the completeness of the hypothesized clustering, Dom
+    includes a model cost term calculated using a coding theory argument. The
+    overall clustering quality measure presented is the sum of the costs of
+    representing the data's conditional entropy and the model.
+
+    The motivation for this approach is an appeal to parsimony: Given identical
+    conditional entropies, H(C|K), the clustering solution with the fewest clusters
+    should be preferred.
+
+    The Q0 measure can be calculated using the following formula [^1]
+
+    $$
+    Q_0(C, K) = H(C|K) + \frac{1}{n} \sum_{k=1}^{|K|} \log \binom{h(c) + |C| - 1}{|C| - 1}.
+    $$
+
+    Due to the complexity of the formula, this metric and its associated normalized version (Q2)
+    is one order of magnitude slower than most other implemented metrics.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+
+    Examples
+    --------
+    >>> from river import metrics
+
+    >>> y_true = [1, 1, 2, 2, 3, 3]
+    >>> y_pred = [1, 1, 1, 2, 2, 2]
+
+    >>> metric = metrics.Q0()
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     print(metric.update(yt, yp).get())
+    0.0
+    0.0
+    0.9182958340544896
+    1.208582260960826
+    1.4479588303902937
+    1.3803939544277863
+
+    >>> metric
+    Q0: 1.380394
+
+    References
+    ----------
+    [^1]: Andrew Rosenberg and Julia Hirschberg (2007).
+          V-Measure: A conditional entropy-based external cluster evaluation measure.
+          Proceedings of the 2007 Joing Conference on Empirical Methods in Natural Language
+          Processing and Computational Natural Language Learning, pp. 410 - 420,
+          Prague, June 2007. URL: https://www.aclweb.org/anthology/D07-1043.pdf.
+    [^2]: Byron E. Dom. 2001. An information-theoretic external
+          cluster-validity measure. Technical Report RJ10219, IBM, October.
+
+    """
+
+    def __init__(self, cm=None):
+        super().__init__(cm)
+
+    @staticmethod
+    def binomial_coeff(n, k):
+        return factorial(n) / (factorial(k) * factorial(n - k))
+
+    def get(self):
+
+        conditional_entropy_c_k = 0.0
+
+        sum_logs = 0.0
+
+        n_true_clusters = sum(1 for i in self.cm.sum_col.values() if i > 0)
+
+        for i in self.cm.classes:
+
+            for j in self.cm.classes:
+
+                try:
+                    conditional_entropy_c_k -= (
+                        self.cm[j][i]
+                        / self.cm.n_samples
+                        * math.log(self.cm[j][i] / self.cm.sum_col[i], 2)
+                    )
+                except (ValueError, ZeroDivisionError):
+                    continue
+
+            try:
+                sum_logs += math.log(
+                    self.binomial_coeff(
+                        self.cm.sum_col[i] + n_true_clusters - 1, n_true_clusters - 1
+                    )
+                )
+            except ValueError:
+                continue
+
+        return conditional_entropy_c_k + sum_logs / self.cm.n_samples
+
+
+class Q2(Q0):
+    r"""Q2 index.
+
+    Q2 index is presented by Dom [^2] as a normalized version of the original Q0 index.
+    This index has a range of $(0, 1]$ [^1], with greater scores being representing more
+    preferred clustering.
+
+    The Q2 index can be calculated as follows [^1]
+
+    $$
+    Q2(C, K) = \frac{\frac{1}{n} \sum_{c=1}^{|C|} \log \binom{h(c) + |C| - 1}{|C| - 1} }{Q_0(C, K)}
+    $$
+
+    where $C$ is the target partition, $K$ is the hypothesized partition and $h(k)$ is
+    the size of cluster $k$.
+
+    Due to the complexity of the formula, this metric is one order of magnitude slower than
+    its original version (Q0) and most other implemented metrics.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+
+    Examples
+    --------
+    >>> from river import metrics
+
+    >>> y_true = [1, 1, 2, 2, 3, 3]
+    >>> y_pred = [1, 1, 1, 2, 2, 2]
+
+    >>> metric = metrics.Q2()
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     print(metric.update(yt, yp).get())
+    0.0
+    0.0
+    0.0
+    0.4545045563529578
+    0.39923396953448914
+    0.3979343306829813
+
+    >>> metric
+    Q2: 0.397934
+
+    References
+    ----------
+    [^1]: Andrew Rosenberg and Julia Hirschberg (2007).
+          V-Measure: A conditional entropy-based external cluster evaluation measure.
+          Proceedings of the 2007 Joing Conference on Empirical Methods in Natural Language
+          Processing and Computational Natural Language Learning, pp. 410 - 420,
+          Prague, June 2007. URL: https://www.aclweb.org/anthology/D07-1043.pdf.
+    [^2]: Byron E. Dom. 2001. An information-theoretic external
+          cluster-validity measure. Technical Report RJ10219, IBM, October.
+
+    """
+
+    def __init__(self, cm=None):
+        super().__init__(cm)
+
+    def get(self):
+
+        q0 = super().get()
+
+        sum_logs = 0.0
+
+        n_true_clusters = sum(1 for i in self.cm.sum_col.values() if i > 0)
+
+        for i in self.cm.classes:
+
+            try:
+                sum_logs += math.log(
+                    self.binomial_coeff(
+                        self.cm.sum_row[i] + n_true_clusters - 1, n_true_clusters - 1
+                    )
+                )
+            except ValueError:
+                continue
+
+        try:
+            return (sum_logs / self.cm.n_samples) / q0
+        except ZeroDivisionError:
+            return 0.0
```

### Comparing `river-0.8.0/river/metrics/r2.py` & `river-0.9.0/river/metrics/r2.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,81 +1,81 @@
-from river import stats
-
-from . import base
-
-__all__ = ["R2"]
-
-
-class R2(base.RegressionMetric):
-    """Coefficient of determination ($R^2$) score
-
-    The coefficient of determination, denoted $R^2$ or $r^2$, is the proportion
-    of the variance in the dependent variable that is predictable from the
-    independent variable(s). [^1]
-
-    Best possible score is 1.0 and it can be negative (because the model can be
-    arbitrarily worse). A constant model that always predicts the expected
-    value of $y$, disregarding the input features, would get a $R^2$ score of
-    0.0.
-
-    $R^2$ is not defined when less than 2 samples have been observed. This
-    implementation returns 0.0 in this case.
-
-    Examples
-    --------
-    >>> from river import metrics
-
-    >>> y_true = [3, -0.5, 2, 7]
-    >>> y_pred = [2.5, 0.0, 2, 8]
-
-    >>> metric = metrics.R2()
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     print(metric.update(yt, yp).get())
-    0.0
-    0.9183
-    0.9230
-    0.9486
-
-    References
-    ----------
-    [^1]: [Coefficient of determination (Wikipedia)](https://en.wikipedia.org/wiki/Coefficient_of_determination)
-
-    """
-
-    def __init__(self):
-        super().__init__()
-        self._y_var = stats.Var()
-        self._total_sum_of_squares = 0
-        self._residual_sum_of_squares = 0
-        self.sample_correction = {}
-
-    @property
-    def bigger_is_better(self):
-        return True
-
-    def update(self, y_true, y_pred, sample_weight=1.0):
-        self._y_var.update(y_true, w=sample_weight)
-        squared_error = (y_true - y_pred) * (y_true - y_pred) * sample_weight
-        self._residual_sum_of_squares += squared_error
-
-        # To track back
-        self.sample_correction = {"squared_error": squared_error}
-
-        return self
-
-    def revert(self, y_true, y_pred, sample_weight, correction=None):
-        self._y_var.update(y_true, w=-sample_weight)
-        self._residual_sum_of_squares -= correction["squared_error"]
-
-        return self
-
-    def get(self):
-        if self._y_var.mean.n > 1:
-            try:
-                total_sum_of_squares = (self._y_var.mean.n - 1) * self._y_var.get()
-                return 1 - (self._residual_sum_of_squares / total_sum_of_squares)
-            except ZeroDivisionError:
-                return 0.0
-
-        # Not defined for n_samples < 2
-        return 0.0
+from river import stats
+
+from . import base
+
+__all__ = ["R2"]
+
+
+class R2(base.RegressionMetric):
+    """Coefficient of determination ($R^2$) score
+
+    The coefficient of determination, denoted $R^2$ or $r^2$, is the proportion
+    of the variance in the dependent variable that is predictable from the
+    independent variable(s). [^1]
+
+    Best possible score is 1.0 and it can be negative (because the model can be
+    arbitrarily worse). A constant model that always predicts the expected
+    value of $y$, disregarding the input features, would get a $R^2$ score of
+    0.0.
+
+    $R^2$ is not defined when less than 2 samples have been observed. This
+    implementation returns 0.0 in this case.
+
+    Examples
+    --------
+    >>> from river import metrics
+
+    >>> y_true = [3, -0.5, 2, 7]
+    >>> y_pred = [2.5, 0.0, 2, 8]
+
+    >>> metric = metrics.R2()
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     print(metric.update(yt, yp).get())
+    0.0
+    0.9183
+    0.9230
+    0.9486
+
+    References
+    ----------
+    [^1]: [Coefficient of determination (Wikipedia)](https://en.wikipedia.org/wiki/Coefficient_of_determination)
+
+    """
+
+    def __init__(self):
+        super().__init__()
+        self._y_var = stats.Var()
+        self._total_sum_of_squares = 0
+        self._residual_sum_of_squares = 0
+        self.sample_correction = {}
+
+    @property
+    def bigger_is_better(self):
+        return True
+
+    def update(self, y_true, y_pred, sample_weight=1.0):
+        self._y_var.update(y_true, w=sample_weight)
+        squared_error = (y_true - y_pred) * (y_true - y_pred) * sample_weight
+        self._residual_sum_of_squares += squared_error
+
+        # To track back
+        self.sample_correction = {"squared_error": squared_error}
+
+        return self
+
+    def revert(self, y_true, y_pred, sample_weight, correction=None):
+        self._y_var.update(y_true, w=-sample_weight)
+        self._residual_sum_of_squares -= correction["squared_error"]
+
+        return self
+
+    def get(self):
+        if self._y_var.mean.n > 1:
+            try:
+                total_sum_of_squares = (self._y_var.mean.n - 1) * self._y_var.get()
+                return 1 - (self._residual_sum_of_squares / total_sum_of_squares)
+            except ZeroDivisionError:
+                return 0.0
+
+        # Not defined for n_samples < 2
+        return 0.0
```

### Comparing `river-0.8.0/river/metrics/recall.py` & `river-0.9.0/river/metrics/recall.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,218 +1,218 @@
-from river import metrics
-
-__all__ = ["MacroRecall", "MicroRecall", "Recall", "WeightedRecall", "ExampleRecall"]
-
-
-class Recall(metrics.BinaryMetric):
-    """Binary recall score.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
-        confusion matrix reduces the amount of storage and computation time.
-    pos_val
-        Value to treat as "positive".
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [True, False, True, True, True]
-    >>> y_pred = [True, True, False, True, True]
-
-    >>> metric = metrics.Recall()
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     print(metric.update(yt, yp))
-    Recall: 1.
-    Recall: 1.
-    Recall: 0.5
-    Recall: 0.666667
-    Recall: 0.75
-
-    """
-
-    def get(self):
-        tp = self.cm.true_positives(self.pos_val)
-        fn = self.cm.false_negatives(self.pos_val)
-        try:
-            return tp / (tp + fn)
-        except ZeroDivisionError:
-            return 0.0
-
-
-class MacroRecall(metrics.MultiClassMetric):
-    """Macro-average recall score.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
-        confusion matrix reduces the amount of storage and computation time.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [0, 1, 2, 2, 2]
-    >>> y_pred = [0, 0, 2, 2, 1]
-
-    >>> metric = metrics.MacroRecall()
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     print(metric.update(yt, yp))
-    MacroRecall: 1.
-    MacroRecall: 0.5
-    MacroRecall: 0.666667
-    MacroRecall: 0.666667
-    MacroRecall: 0.555556
-
-    """
-
-    def get(self):
-        total = 0
-        for c in self.cm.classes:
-            try:
-                total += self.cm[c][c] / self.cm.sum_row[c]
-            except ZeroDivisionError:
-                continue
-        try:
-            return total / len(self.cm.classes)
-        except ZeroDivisionError:
-            return 0.0
-
-
-class MicroRecall(metrics.MicroPrecision):
-    """Micro-average recall score.
-
-    The micro-average recall is exactly equivalent to the micro-average precision as well as the
-    micro-average F1 score.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
-        confusion matrix reduces the amount of storage and computation time.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [0, 1, 2, 2, 2]
-    >>> y_pred = [0, 0, 2, 2, 1]
-
-    >>> metric = metrics.MicroRecall()
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     print(metric.update(yt, yp))
-    MicroRecall: 1.
-    MicroRecall: 0.5
-    MicroRecall: 0.666667
-    MicroRecall: 0.75
-    MicroRecall: 0.6
-
-    References
-    ----------
-    [^1]: [Why are precision, recall and F1 score equal when using micro averaging in a multi-class problem?](https://simonhessner.de/why-are-precision-recall-and-f1-score-equal-when-using-micro-averaging-in-a-multi-class-problem/)
-
-    """
-
-
-class WeightedRecall(metrics.MultiClassMetric):
-    """Weighted-average recall score.
-
-    This uses the support of each label to compute an average score, whereas `MacroRecall`
-    ignores the support.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
-        confusion matrix reduces the amount of storage and computation time.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [0, 1, 2, 2, 2]
-    >>> y_pred = [0, 0, 2, 2, 1]
-
-    >>> metric = metrics.WeightedRecall()
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     print(metric.update(yt, yp))
-    WeightedRecall: 1.
-    WeightedRecall: 0.5
-    WeightedRecall: 0.666667
-    WeightedRecall: 0.75
-    WeightedRecall: 0.6
-
-    """
-
-    def get(self):
-        total = 0
-        for c in self.cm.classes:
-            try:
-                total += self.cm[c][c]
-            except ZeroDivisionError:
-                continue
-        try:
-            return total / self.cm.total_weight
-        except ZeroDivisionError:
-            return 0.0
-
-
-class ExampleRecall(metrics.MultiOutputClassificationMetric):
-    """Example-based recall score for multilabel classification.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
-        confusion matrix reduces the amount of storage and computation time.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [
-    ...     {0: False, 1: True, 2: True},
-    ...     {0: True, 1: True, 2: False},
-    ...     {0: True, 1: True, 2: False},
-    ... ]
-
-    >>> y_pred = [
-    ...     {0: True, 1: True, 2: True},
-    ...     {0: True, 1: False, 2: False},
-    ...     {0: True, 1: True, 2: False},
-    ... ]
-
-    >>> metric = metrics.ExampleRecall()
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     metric = metric.update(yt, yp)
-
-    >>> metric
-    ExampleRecall: 0.833333
-
-    """
-
-    @property
-    def bigger_is_better(self):
-        return True
-
-    @property
-    def requires_labels(self):
-        return True
-
-    def get(self):
-
-        try:
-            return self.cm.recall_sum / self.cm.n_samples
-        except ZeroDivisionError:
-            return 0.0
+from river import metrics
+
+__all__ = ["MacroRecall", "MicroRecall", "Recall", "WeightedRecall", "ExampleRecall"]
+
+
+class Recall(metrics.BinaryMetric):
+    """Binary recall score.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
+        confusion matrix reduces the amount of storage and computation time.
+    pos_val
+        Value to treat as "positive".
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [True, False, True, True, True]
+    >>> y_pred = [True, True, False, True, True]
+
+    >>> metric = metrics.Recall()
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     print(metric.update(yt, yp))
+    Recall: 1.
+    Recall: 1.
+    Recall: 0.5
+    Recall: 0.666667
+    Recall: 0.75
+
+    """
+
+    def get(self):
+        tp = self.cm.true_positives(self.pos_val)
+        fn = self.cm.false_negatives(self.pos_val)
+        try:
+            return tp / (tp + fn)
+        except ZeroDivisionError:
+            return 0.0
+
+
+class MacroRecall(metrics.MultiClassMetric):
+    """Macro-average recall score.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
+        confusion matrix reduces the amount of storage and computation time.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [0, 1, 2, 2, 2]
+    >>> y_pred = [0, 0, 2, 2, 1]
+
+    >>> metric = metrics.MacroRecall()
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     print(metric.update(yt, yp))
+    MacroRecall: 1.
+    MacroRecall: 0.5
+    MacroRecall: 0.666667
+    MacroRecall: 0.666667
+    MacroRecall: 0.555556
+
+    """
+
+    def get(self):
+        total = 0
+        for c in self.cm.classes:
+            try:
+                total += self.cm[c][c] / self.cm.sum_row[c]
+            except ZeroDivisionError:
+                continue
+        try:
+            return total / len(self.cm.classes)
+        except ZeroDivisionError:
+            return 0.0
+
+
+class MicroRecall(metrics.MicroPrecision):
+    """Micro-average recall score.
+
+    The micro-average recall is exactly equivalent to the micro-average precision as well as the
+    micro-average F1 score.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
+        confusion matrix reduces the amount of storage and computation time.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [0, 1, 2, 2, 2]
+    >>> y_pred = [0, 0, 2, 2, 1]
+
+    >>> metric = metrics.MicroRecall()
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     print(metric.update(yt, yp))
+    MicroRecall: 1.
+    MicroRecall: 0.5
+    MicroRecall: 0.666667
+    MicroRecall: 0.75
+    MicroRecall: 0.6
+
+    References
+    ----------
+    [^1]: [Why are precision, recall and F1 score equal when using micro averaging in a multi-class problem?](https://simonhessner.de/why-are-precision-recall-and-f1-score-equal-when-using-micro-averaging-in-a-multi-class-problem/)
+
+    """
+
+
+class WeightedRecall(metrics.MultiClassMetric):
+    """Weighted-average recall score.
+
+    This uses the support of each label to compute an average score, whereas `MacroRecall`
+    ignores the support.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
+        confusion matrix reduces the amount of storage and computation time.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [0, 1, 2, 2, 2]
+    >>> y_pred = [0, 0, 2, 2, 1]
+
+    >>> metric = metrics.WeightedRecall()
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     print(metric.update(yt, yp))
+    WeightedRecall: 1.
+    WeightedRecall: 0.5
+    WeightedRecall: 0.666667
+    WeightedRecall: 0.75
+    WeightedRecall: 0.6
+
+    """
+
+    def get(self):
+        total = 0
+        for c in self.cm.classes:
+            try:
+                total += self.cm[c][c]
+            except ZeroDivisionError:
+                continue
+        try:
+            return total / self.cm.total_weight
+        except ZeroDivisionError:
+            return 0.0
+
+
+class ExampleRecall(metrics.MultiOutputClassificationMetric):
+    """Example-based recall score for multilabel classification.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a
+        confusion matrix reduces the amount of storage and computation time.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [
+    ...     {0: False, 1: True, 2: True},
+    ...     {0: True, 1: True, 2: False},
+    ...     {0: True, 1: True, 2: False},
+    ... ]
+
+    >>> y_pred = [
+    ...     {0: True, 1: True, 2: True},
+    ...     {0: True, 1: False, 2: False},
+    ...     {0: True, 1: True, 2: False},
+    ... ]
+
+    >>> metric = metrics.ExampleRecall()
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     metric = metric.update(yt, yp)
+
+    >>> metric
+    ExampleRecall: 0.833333
+
+    """
+
+    @property
+    def bigger_is_better(self):
+        return True
+
+    @property
+    def requires_labels(self):
+        return True
+
+    def get(self):
+
+        try:
+            return self.cm.recall_sum / self.cm.n_samples
+        except ZeroDivisionError:
+            return 0.0
```

### Comparing `river-0.8.0/river/metrics/report.py` & `river-0.9.0/river/metrics/report.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,139 +1,139 @@
-from .. import utils
-from . import accuracy, base, fbeta, precision, recall
-
-
-class ClassificationReport(base.MultiClassMetric):
-    """A report for monitoring a classifier.
-
-    This class maintains a set of metrics and updates each of them every time `update` is called.
-    You can print this class at any time during a model's lifetime to get a tabular visualization
-    of various metrics.
-
-    You can wrap a `metrics.ClassificationReport` with `metrics.Rolling` in order to obtain a
-    classification report over a window of observations. You can also wrap it with
-    `metrics.TimeRolling` to obtain a report over a period of time.
-
-    Parameters
-    ----------
-    decimals
-        The number of decimals to display in each cell.
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = ['pear', 'apple', 'banana', 'banana', 'banana']
-    >>> y_pred = ['apple', 'pear', 'banana', 'banana', 'apple']
-
-    >>> report = metrics.ClassificationReport()
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     report = report.update(yt, yp)
-
-    >>> print(report)
-               Precision   Recall   F1      Support
-    <BLANKLINE>
-       apple       0.000    0.000   0.000         1
-      banana       1.000    0.667   0.800         3
-        pear       0.000    0.000   0.000         1
-    <BLANKLINE>
-       Macro       0.333    0.222   0.267
-       Micro       0.400    0.400   0.400
-    Weighted       0.600    0.400   0.480
-    <BLANKLINE>
-                     40.0% accuracy
-
-    """
-
-    def __init__(self, decimals=3, cm=None):
-        super().__init__(cm)
-        self.decimals = decimals
-        self._f1s = {}
-        self._macro_precision = precision.MacroPrecision(self.cm)
-        self._macro_recall = recall.MacroRecall(self.cm)
-        self._macro_f1 = fbeta.MacroF1(self.cm)
-        self._micro_precision = precision.MicroPrecision(self.cm)
-        self._micro_recall = recall.MicroRecall(self.cm)
-        self._micro_f1 = fbeta.MicroF1(self.cm)
-        self._weighted_precision = precision.WeightedPrecision(self.cm)
-        self._weighted_recall = recall.WeightedRecall(self.cm)
-        self._weighted_f1 = fbeta.WeightedF1(self.cm)
-        self._accuracy = accuracy.Accuracy(self.cm)
-
-    def get(self):
-        raise NotImplementedError
-
-    def __repr__(self):
-        def fmt_float(x):
-            return f"{x:.{self.decimals}f}"
-
-        headers = ["", "Precision", "Recall", "F1", "Support"]
-        classes = sorted(self.cm.classes)
-
-        for c in classes:
-            if c not in self._f1s:
-                self._f1s[c] = fbeta.F1(cm=self.cm, pos_val=c)
-
-        columns = [
-            # Row names
-            ["", *map(str, classes), "", "Macro", "Micro", "Weighted"],
-            # Precision values
-            [
-                "",
-                *[fmt_float(self._f1s[c].precision.get()) for c in classes],
-                "",
-                *map(
-                    fmt_float,
-                    [
-                        self._macro_precision.get(),
-                        self._micro_precision.get(),
-                        self._weighted_precision.get(),
-                    ],
-                ),
-            ],
-            # Recall values
-            [
-                "",
-                *[fmt_float(self._f1s[c].recall.get()) for c in classes],
-                "",
-                *map(
-                    fmt_float,
-                    [
-                        self._macro_recall.get(),
-                        self._micro_recall.get(),
-                        self._weighted_recall.get(),
-                    ],
-                ),
-            ],
-            # F1 values
-            [
-                "",
-                *[fmt_float(self._f1s[c].get()) for c in classes],
-                "",
-                *map(
-                    fmt_float,
-                    [
-                        self._macro_f1.get(),
-                        self._micro_f1.get(),
-                        self._weighted_f1.get(),
-                    ],
-                ),
-            ],
-            # Support
-            ["", *[str(self.cm.sum_row[c]).rstrip(".0") for c in classes], *[""] * 4],
-        ]
-
-        # Build the table
-        table = utils.pretty.print_table(headers, columns)
-
-        # Write down the accuracy
-        width = len(table.splitlines()[0])
-        accuracy = f"{self._accuracy.get():.{self.decimals - 2}%}" + " accuracy"
-        table += "\n\n" + f"{accuracy:^{width}}"
-
-        return table
+from .. import utils
+from . import accuracy, base, fbeta, precision, recall
+
+
+class ClassificationReport(base.MultiClassMetric):
+    """A report for monitoring a classifier.
+
+    This class maintains a set of metrics and updates each of them every time `update` is called.
+    You can print this class at any time during a model's lifetime to get a tabular visualization
+    of various metrics.
+
+    You can wrap a `metrics.ClassificationReport` with `metrics.Rolling` in order to obtain a
+    classification report over a window of observations. You can also wrap it with
+    `metrics.TimeRolling` to obtain a report over a period of time.
+
+    Parameters
+    ----------
+    decimals
+        The number of decimals to display in each cell.
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = ['pear', 'apple', 'banana', 'banana', 'banana']
+    >>> y_pred = ['apple', 'pear', 'banana', 'banana', 'apple']
+
+    >>> report = metrics.ClassificationReport()
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     report = report.update(yt, yp)
+
+    >>> print(report)
+               Precision   Recall   F1      Support
+    <BLANKLINE>
+       apple       0.000    0.000   0.000         1
+      banana       1.000    0.667   0.800         3
+        pear       0.000    0.000   0.000         1
+    <BLANKLINE>
+       Macro       0.333    0.222   0.267
+       Micro       0.400    0.400   0.400
+    Weighted       0.600    0.400   0.480
+    <BLANKLINE>
+                     40.0% accuracy
+
+    """
+
+    def __init__(self, decimals=3, cm=None):
+        super().__init__(cm)
+        self.decimals = decimals
+        self._f1s = {}
+        self._macro_precision = precision.MacroPrecision(self.cm)
+        self._macro_recall = recall.MacroRecall(self.cm)
+        self._macro_f1 = fbeta.MacroF1(self.cm)
+        self._micro_precision = precision.MicroPrecision(self.cm)
+        self._micro_recall = recall.MicroRecall(self.cm)
+        self._micro_f1 = fbeta.MicroF1(self.cm)
+        self._weighted_precision = precision.WeightedPrecision(self.cm)
+        self._weighted_recall = recall.WeightedRecall(self.cm)
+        self._weighted_f1 = fbeta.WeightedF1(self.cm)
+        self._accuracy = accuracy.Accuracy(self.cm)
+
+    def get(self):
+        raise NotImplementedError
+
+    def __repr__(self):
+        def fmt_float(x):
+            return f"{x:.{self.decimals}f}"
+
+        headers = ["", "Precision", "Recall", "F1", "Support"]
+        classes = sorted(self.cm.classes)
+
+        for c in classes:
+            if c not in self._f1s:
+                self._f1s[c] = fbeta.F1(cm=self.cm, pos_val=c)
+
+        columns = [
+            # Row names
+            ["", *map(str, classes), "", "Macro", "Micro", "Weighted"],
+            # Precision values
+            [
+                "",
+                *[fmt_float(self._f1s[c].precision.get()) for c in classes],
+                "",
+                *map(
+                    fmt_float,
+                    [
+                        self._macro_precision.get(),
+                        self._micro_precision.get(),
+                        self._weighted_precision.get(),
+                    ],
+                ),
+            ],
+            # Recall values
+            [
+                "",
+                *[fmt_float(self._f1s[c].recall.get()) for c in classes],
+                "",
+                *map(
+                    fmt_float,
+                    [
+                        self._macro_recall.get(),
+                        self._micro_recall.get(),
+                        self._weighted_recall.get(),
+                    ],
+                ),
+            ],
+            # F1 values
+            [
+                "",
+                *[fmt_float(self._f1s[c].get()) for c in classes],
+                "",
+                *map(
+                    fmt_float,
+                    [
+                        self._macro_f1.get(),
+                        self._micro_f1.get(),
+                        self._weighted_f1.get(),
+                    ],
+                ),
+            ],
+            # Support
+            ["", *[str(self.cm.sum_row[c]).rstrip(".0") for c in classes], *[""] * 4],
+        ]
+
+        # Build the table
+        table = utils.pretty.print_table(headers, columns)
+
+        # Write down the accuracy
+        width = len(table.splitlines()[0])
+        accuracy = f"{self._accuracy.get():.{self.decimals - 2}%}" + " accuracy"
+        table += "\n\n" + f"{accuracy:^{width}}"
+
+        return table
```

### Comparing `river-0.8.0/river/metrics/roc_auc.py` & `river-0.9.0/river/metrics/roc_auc.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,97 +1,97 @@
-from scipy import integrate
-
-from . import base, confusion
-
-__all__ = ["ROCAUC"]
-
-
-class ROCAUC(base.BinaryMetric):
-    """Receiving Operating Characteristic Area Under the Curve.
-
-    This metric is an approximation of the true ROC AUC. Computing the true ROC AUC would
-    require storing all the predictions and ground truths, which isn't desirable. The approximation
-    error is not significant as long as the predicted probabilities are well calibrated. In any
-    case, this metric can still be used to reliably compare models between each other.
-
-    Parameters
-    ----------
-    n_thresholds
-        The number of thresholds used for discretizing the ROC curve. A higher value will lead to
-        more accurate results, but will also cost more time and memory.
-    pos_val
-        Value to treat as "positive".
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [ 0,  0,   1,  1]
-    >>> y_pred = [.1, .4, .35, .8]
-
-    >>> metric = metrics.ROCAUC()
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     metric = metric.update(yt, yp)
-
-    >>> metric
-    ROCAUC: 0.875
-
-    The true ROC AUC is in fact 0.75. We can improve the accuracy by increasing the amount
-    of thresholds. This comes at the cost more computation time and more memory usage.
-
-    >>> metric = metrics.ROCAUC(n_thresholds=20)
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     metric = metric.update(yt, yp)
-
-    >>> metric
-    ROCAUC: 0.75
-
-    """
-
-    def __init__(self, n_thresholds=10, pos_val=True):
-        self.n_thresholds = n_thresholds
-        self.pos_val = pos_val
-        self.thresholds = [i / (n_thresholds - 1) for i in range(n_thresholds)]
-        self.thresholds[0] -= 1e-7
-        self.thresholds[-1] += 1e-7
-        self.cms = [confusion.ConfusionMatrix() for _ in range(n_thresholds)]
-
-    def update(self, y_true, y_pred, sample_weight=1.0):
-        p_true = y_pred.get(True, 0.0) if isinstance(y_pred, dict) else y_pred
-        for t, cm in zip(self.thresholds, self.cms):
-            cm.update(y_true == self.pos_val, p_true > t, sample_weight)
-        return self
-
-    def revert(self, y_true, y_pred, sample_weight=1.0):
-        p_true = y_pred.get(True, 0.0) if isinstance(y_pred, dict) else y_pred
-        for t, cm in zip(self.thresholds, self.cms):
-            cm.revert(y_true == self.pos_val, p_true > t, sample_weight)
-        return self
-
-    @property
-    def requires_labels(self):
-        return False
-
-    def get(self):
-
-        tprs = [0] * self.n_thresholds
-        fprs = [0] * self.n_thresholds
-
-        def safe_div(a, b):
-            try:
-                return a / b
-            except ZeroDivisionError:
-                return 0.0
-
-        for i, cm in enumerate(self.cms):
-            tp = cm.true_positives(self.pos_val)
-            tn = cm.true_negatives(self.pos_val)
-            fp = cm.false_positives(self.pos_val)
-            fn = cm.false_negatives(self.pos_val)
-
-            tprs[i] = safe_div(a=tp, b=tp + fn)
-            fprs[i] = safe_div(a=fp, b=fp + tn)
-
-        return -integrate.trapz(x=fprs, y=tprs)
+from scipy import integrate
+
+from . import base, confusion
+
+__all__ = ["ROCAUC"]
+
+
+class ROCAUC(base.BinaryMetric):
+    """Receiving Operating Characteristic Area Under the Curve.
+
+    This metric is an approximation of the true ROC AUC. Computing the true ROC AUC would
+    require storing all the predictions and ground truths, which isn't desirable. The approximation
+    error is not significant as long as the predicted probabilities are well calibrated. In any
+    case, this metric can still be used to reliably compare models between each other.
+
+    Parameters
+    ----------
+    n_thresholds
+        The number of thresholds used for discretizing the ROC curve. A higher value will lead to
+        more accurate results, but will also cost more time and memory.
+    pos_val
+        Value to treat as "positive".
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [ 0,  0,   1,  1]
+    >>> y_pred = [.1, .4, .35, .8]
+
+    >>> metric = metrics.ROCAUC()
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     metric = metric.update(yt, yp)
+
+    >>> metric
+    ROCAUC: 0.875
+
+    The true ROC AUC is in fact 0.75. We can improve the accuracy by increasing the amount
+    of thresholds. This comes at the cost more computation time and more memory usage.
+
+    >>> metric = metrics.ROCAUC(n_thresholds=20)
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     metric = metric.update(yt, yp)
+
+    >>> metric
+    ROCAUC: 0.75
+
+    """
+
+    def __init__(self, n_thresholds=10, pos_val=True):
+        self.n_thresholds = n_thresholds
+        self.pos_val = pos_val
+        self.thresholds = [i / (n_thresholds - 1) for i in range(n_thresholds)]
+        self.thresholds[0] -= 1e-7
+        self.thresholds[-1] += 1e-7
+        self.cms = [confusion.ConfusionMatrix() for _ in range(n_thresholds)]
+
+    def update(self, y_true, y_pred, sample_weight=1.0):
+        p_true = y_pred.get(True, 0.0) if isinstance(y_pred, dict) else y_pred
+        for t, cm in zip(self.thresholds, self.cms):
+            cm.update(y_true == self.pos_val, p_true > t, sample_weight)
+        return self
+
+    def revert(self, y_true, y_pred, sample_weight=1.0):
+        p_true = y_pred.get(True, 0.0) if isinstance(y_pred, dict) else y_pred
+        for t, cm in zip(self.thresholds, self.cms):
+            cm.revert(y_true == self.pos_val, p_true > t, sample_weight)
+        return self
+
+    @property
+    def requires_labels(self):
+        return False
+
+    def get(self):
+
+        tprs = [0] * self.n_thresholds
+        fprs = [0] * self.n_thresholds
+
+        def safe_div(a, b):
+            try:
+                return a / b
+            except ZeroDivisionError:
+                return 0.0
+
+        for i, cm in enumerate(self.cms):
+            tp = cm.true_positives(self.pos_val)
+            tn = cm.true_negatives(self.pos_val)
+            fp = cm.false_positives(self.pos_val)
+            fn = cm.false_negatives(self.pos_val)
+
+            tprs[i] = safe_div(a=tp, b=tp + fn)
+            fprs[i] = safe_div(a=fp, b=fp + tn)
+
+        return -integrate.trapz(x=fprs, y=tprs)
```

### Comparing `river-0.8.0/river/metrics/rolling.py` & `river-0.9.0/river/metrics/rolling.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,71 +1,72 @@
-from .. import utils
-from . import base, report
-
-__all__ = ["Rolling"]
-
-
-class Rolling(base.WrapperMetric, utils.Window):
-    """Wrapper for computing metrics over a window.
-
-    This wrapper metric allows you to apply a metric over a window of observations. Under the hood,
-    a buffer with the `window_size` most recent pairs of `(y_true, y_pred)` is memorised. When the
-    buffer is full, the oldest pair is removed and the `revert` method of the metric is called with
-    said pair.
-
-    You should use `metrics.Rolling` to evaluate a metric over a window of fixed sized. You can use
-    `metrics.TimeRolling` to instead evaluate a metric over a period of time.
-
-    Parameters
-    ----------
-    metric
-        A metric.
-    window_size
-        The number of most recent `(y_true, y_pred)` pairs on which to evaluate the metric.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [3, -0.5, 2, 7]
-    >>> y_pred = [2.5, 0.0, 2, 8]
-
-    >>> metric = metrics.Rolling(metrics.MSE(), window_size=2)
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     print(metric.update(yt, yp))
-    MSE: 0.25   (rolling 2)
-    MSE: 0.25   (rolling 2)
-    MSE: 0.125  (rolling 2)
-    MSE: 0.5    (rolling 2)
-    """
-
-    def __init__(self, metric: base.Metric, window_size: int):
-        super().__init__(size=window_size)
-        self.window_size = window_size
-        self._metric = metric
-
-    @property
-    def metric(self):
-        return self._metric
-
-    def update(self, y_true, y_pred, sample_weight=1.0):
-        if len(self) == self.window_size:
-            self.metric.revert(*self[0])
-        self.metric.update(y_true, y_pred, sample_weight)
-        try:
-            # For classification metrics that require additional information
-            self.append((y_true, y_pred, sample_weight, self.metric.sample_correction))
-        except AttributeError:
-            # Default case
-            self.append((y_true, y_pred, sample_weight))
-        return self
-
-    def revert(self, y_true, y_pred, sample_weight=1.0):
-        self.metric.revert(y_true, y_pred, sample_weight)
-        return self
-
-    def __repr__(self):
-        if isinstance(self.metric, report.ClassificationReport):
-            return self.metric.__repr__()
-        return f"{str(self.metric)}\t(rolling {self.window_size})"
+from .. import utils
+from . import base, report
+
+__all__ = ["Rolling"]
+
+
+class Rolling(base.WrapperMetric, utils.Window):
+    """Wrapper for computing metrics over a window.
+
+    This wrapper metric allows you to apply a metric over a window of observations. Under the hood,
+    a buffer with the `window_size` most recent pairs of `(y_true, y_pred)` is memorised. When the
+    buffer is full, the oldest pair is removed and the `revert` method of the metric is called with
+    said pair.
+
+    You should use `metrics.Rolling` to evaluate a metric over a window of fixed sized. You can use
+    `metrics.TimeRolling` to instead evaluate a metric over a period of time.
+
+    Parameters
+    ----------
+    metric
+        A metric.
+    window_size
+        The number of most recent `(y_true, y_pred)` pairs on which to evaluate the metric.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [3, -0.5, 2, 7]
+    >>> y_pred = [2.5, 0.0, 2, 8]
+
+    >>> metric = metrics.Rolling(metrics.MSE(), window_size=2)
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     print(metric.update(yt, yp))
+    MSE: 0.25   (rolling 2)
+    MSE: 0.25   (rolling 2)
+    MSE: 0.125  (rolling 2)
+    MSE: 0.5    (rolling 2)
+
+    """
+
+    def __init__(self, metric: base.Metric, window_size: int):
+        super().__init__(size=window_size)
+        self.window_size = window_size
+        self._metric = metric
+
+    @property
+    def metric(self):
+        return self._metric
+
+    def update(self, y_true, y_pred, sample_weight=1.0):
+        if len(self) == self.window_size:
+            self.metric.revert(*self[0])
+        self.metric.update(y_true, y_pred, sample_weight)
+        try:
+            # For classification metrics that require additional information
+            self.append((y_true, y_pred, sample_weight, self.metric.sample_correction))
+        except AttributeError:
+            # Default case
+            self.append((y_true, y_pred, sample_weight))
+        return self
+
+    def revert(self, y_true, y_pred, sample_weight=1.0):
+        self.metric.revert(y_true, y_pred, sample_weight)
+        return self
+
+    def __repr__(self):
+        if isinstance(self.metric, report.ClassificationReport):
+            return self.metric.__repr__()
+        return f"{str(self.metric)}\t(rolling {self.window_size})"
```

### Comparing `river-0.8.0/river/metrics/time_rolling.py` & `river-0.9.0/river/metrics/time_rolling.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,82 +1,82 @@
-import bisect
-import datetime as dt
-import typing
-
-from . import base, report
-
-__all__ = ["TimeRolling"]
-
-
-class TimeRolling(base.WrapperMetric):
-    """Wrapper for computing metrics over a period of time.
-
-    Parameters
-    ----------
-    metric
-        A metric.
-    period
-        A period of time.
-
-    Examples
-    --------
-
-    >>> import datetime as dt
-    >>> from river import metrics
-
-    >>> y_true = [3, -0.5, 2, 7]
-    >>> y_pred = [2.5, 0.0, 2, 9]
-    >>> days = [1, 2, 3, 4]
-
-    >>> metric = metrics.TimeRolling(metrics.MAE(), period=dt.timedelta(days=2))
-
-    >>> for yt, yp, day in zip(y_true, y_pred, days):
-    ...     t = dt.datetime(2019, 1, day)
-    ...     print(metric.update(yt, yp, t))
-    MAE: 0.5    (rolling 2 days, 0:00:00)
-    MAE: 0.5    (rolling 2 days, 0:00:00)
-    MAE: 0.25   (rolling 2 days, 0:00:00)
-    MAE: 1. (rolling 2 days, 0:00:00)
-
-    """
-
-    def __init__(self, metric: base.Metric, period: dt.timedelta):
-        self._metric = metric
-        self.period = period
-        self._events: typing.List[
-            typing.Tuple[dt.datetime, typing.Any, typing.Any]
-        ] = []
-        self._latest = dt.datetime(1, 1, 1)
-
-    @property
-    def metric(self):
-        return self._metric
-
-    def update(self, y_true, y_pred, t):
-        self.metric.update(y_true, y_pred)
-        bisect.insort_left(self._events, (t, y_true, y_pred))
-
-        # There will only be events to revert if the new event if younger than the previously seen
-        # youngest event
-        if t > self._latest:
-            self._latest = t
-
-            i = 0
-            for ti, yt, yp in self._events:
-                if ti > t - self.period:
-                    break
-                self.metric.revert(yt, yp)
-                i += 1
-
-            # Remove expired events
-            if i > 0:
-                self._events = self._events[i:]
-
-        return self
-
-    def revert(self, y_true, y_pred):
-        raise NotImplementedError
-
-    def __repr__(self):
-        if isinstance(self.metric, report.ClassificationReport):
-            return self.metric.__repr__()
-        return f"{str(self.metric)}\t(rolling {self.period})"
+import bisect
+import datetime as dt
+import typing
+
+from . import base, report
+
+__all__ = ["TimeRolling"]
+
+
+class TimeRolling(base.WrapperMetric):
+    """Wrapper for computing metrics over a period of time.
+
+    Parameters
+    ----------
+    metric
+        A metric.
+    period
+        A period of time.
+
+    Examples
+    --------
+
+    >>> import datetime as dt
+    >>> from river import metrics
+
+    >>> y_true = [3, -0.5, 2, 7]
+    >>> y_pred = [2.5, 0.0, 2, 9]
+    >>> days = [1, 2, 3, 4]
+
+    >>> metric = metrics.TimeRolling(metrics.MAE(), period=dt.timedelta(days=2))
+
+    >>> for yt, yp, day in zip(y_true, y_pred, days):
+    ...     t = dt.datetime(2019, 1, day)
+    ...     print(metric.update(yt, yp, t))
+    MAE: 0.5    (rolling 2 days, 0:00:00)
+    MAE: 0.5    (rolling 2 days, 0:00:00)
+    MAE: 0.25   (rolling 2 days, 0:00:00)
+    MAE: 1. (rolling 2 days, 0:00:00)
+
+    """
+
+    def __init__(self, metric: base.Metric, period: dt.timedelta):
+        self._metric = metric
+        self.period = period
+        self._events: typing.List[
+            typing.Tuple[dt.datetime, typing.Any, typing.Any]
+        ] = []
+        self._latest = dt.datetime(1, 1, 1)
+
+    @property
+    def metric(self):
+        return self._metric
+
+    def update(self, y_true, y_pred, t):
+        self.metric.update(y_true, y_pred)
+        bisect.insort_left(self._events, (t, y_true, y_pred))
+
+        # There will only be events to revert if the new event if younger than the previously seen
+        # youngest event
+        if t > self._latest:
+            self._latest = t
+
+            i = 0
+            for ti, yt, yp in self._events:
+                if ti > t - self.period:
+                    break
+                self.metric.revert(yt, yp)
+                i += 1
+
+            # Remove expired events
+            if i > 0:
+                self._events = self._events[i:]
+
+        return self
+
+    def revert(self, y_true, y_pred):
+        raise NotImplementedError
+
+    def __repr__(self):
+        if isinstance(self.metric, report.ClassificationReport):
+            return self.metric.__repr__()
+        return f"{str(self.metric)}\t(rolling {self.period})"
```

### Comparing `river-0.8.0/river/metrics/variation_info.py` & `river-0.9.0/river/metrics/variation_info.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,110 +1,110 @@
-import math
-
-from . import base
-
-__all__ = ["VariationInfo"]
-
-
-class VariationInfo(base.MultiClassMetric):
-    r"""Variation of Information.
-
-    Variation of Information (VI) [^1] [^2] is an information-based clustering measure.
-    It is presented as a distance measure for comparing partitions (or clusterings)
-    of the same data. It therefore does not distinguish between hypothesised and
-    target clustering. VI has a number of useful properties, as follows
-
-    * VI satisifes the metric axioms
-
-    * VI is convexly additive. This means that, if a cluster is split, the distance
-    from the new cluster to the original is the distance induced by the split times
-    the size of the cluster. This guarantees that all changes to the metrics are "local".
-
-    * VI is not affected by the number of data points in the cluster. However, it is bounded
-    by the logarithm of the maximum number of clusters in true and predicted labels.
-
-    The Variation of Information is calculated using the following formula
-
-    $$
-    VI(C, K) = H(C) + H(K) - 2 H(C, K) = H(C|K) + H(K|C)
-    $$
-
-    The bound of the variation of information [^3] can be written in terms of the number of elements,
-    $VI(C, K) \leq \log(n)$, or with respect to the maximum number of clusters $K^*$,
-    $VI(C, K) \leq 2 \log(K^*)$.
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-
-    Examples
-    --------
-    >>> from river import metrics
-
-    >>> y_true = [1, 1, 2, 2, 3, 3]
-    >>> y_pred = [1, 1, 1, 2, 2, 2]
-
-    >>> metric = metrics.VariationInfo()
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     print(metric.update(yt, yp).get())
-    0.0
-    0.0
-    0.9182958340544896
-    1.1887218755408673
-    1.3509775004326938
-    1.2516291673878228
-
-    >>> metric
-    VariationInfo: 1.251629
-
-    References
-    ----------
-    [^1]: Andrew Rosenberg and Julia Hirschberg (2007).
-          V-Measure: A conditional entropy-based external cluster evaluation measure.
-          Proceedings of the 2007 Joing Conference on Empirical Methods in Natural Language
-          Processing and Computational Natural Language Learning, pp. 410 - 420,
-          Prague, June 2007.
-    [^2]: Marina Meila and David Heckerman. 2001.
-          An experimental comparison of model-based clustering methods.
-          Mach. Learn., 42(1/2):9–29.
-    [^3]: Wikipedia contributors. (2021, February 18).
-          Variation of information. In Wikipedia, The Free Encyclopedia,
-          from https://en.wikipedia.org/w/index.php?title=Variation_of_information&oldid=1007562715
-
-    """
-
-    def __init__(self, cm=None):
-        super().__init__(cm)
-
-    def get(self):
-
-        conditional_entropy_c_k = 0.0
-
-        conditional_entropy_k_c = 0.0
-
-        for i in self.cm.classes:
-
-            for j in self.cm.classes:
-
-                try:
-                    conditional_entropy_c_k -= (
-                        self.cm[j][i]
-                        / self.cm.n_samples
-                        * math.log(self.cm[j][i] / self.cm.sum_col[i], 2)
-                    )
-                except (ValueError, ZeroDivisionError):
-                    pass
-
-                try:
-                    conditional_entropy_k_c -= (
-                        self.cm[i][j]
-                        / self.cm.n_samples
-                        * math.log(self.cm[i][j] / self.cm.sum_row[i], 2)
-                    )
-                except (ValueError, ZeroDivisionError):
-                    pass
-
-        return conditional_entropy_c_k + conditional_entropy_k_c
+import math
+
+from . import base
+
+__all__ = ["VariationInfo"]
+
+
+class VariationInfo(base.MultiClassMetric):
+    r"""Variation of Information.
+
+    Variation of Information (VI) [^1] [^2] is an information-based clustering measure.
+    It is presented as a distance measure for comparing partitions (or clusterings)
+    of the same data. It therefore does not distinguish between hypothesised and
+    target clustering. VI has a number of useful properties, as follows
+
+    * VI satisifes the metric axioms
+
+    * VI is convexly additive. This means that, if a cluster is split, the distance
+    from the new cluster to the original is the distance induced by the split times
+    the size of the cluster. This guarantees that all changes to the metrics are "local".
+
+    * VI is not affected by the number of data points in the cluster. However, it is bounded
+    by the logarithm of the maximum number of clusters in true and predicted labels.
+
+    The Variation of Information is calculated using the following formula
+
+    $$
+    VI(C, K) = H(C) + H(K) - 2 H(C, K) = H(C|K) + H(K|C)
+    $$
+
+    The bound of the variation of information [^3] can be written in terms of the number of elements,
+    $VI(C, K) \leq \log(n)$, or with respect to the maximum number of clusters $K^*$,
+    $VI(C, K) \leq 2 \log(K^*)$.
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+
+    Examples
+    --------
+    >>> from river import metrics
+
+    >>> y_true = [1, 1, 2, 2, 3, 3]
+    >>> y_pred = [1, 1, 1, 2, 2, 2]
+
+    >>> metric = metrics.VariationInfo()
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     print(metric.update(yt, yp).get())
+    0.0
+    0.0
+    0.9182958340544896
+    1.1887218755408673
+    1.3509775004326938
+    1.2516291673878228
+
+    >>> metric
+    VariationInfo: 1.251629
+
+    References
+    ----------
+    [^1]: Andrew Rosenberg and Julia Hirschberg (2007).
+          V-Measure: A conditional entropy-based external cluster evaluation measure.
+          Proceedings of the 2007 Joing Conference on Empirical Methods in Natural Language
+          Processing and Computational Natural Language Learning, pp. 410 - 420,
+          Prague, June 2007.
+    [^2]: Marina Meila and David Heckerman. 2001.
+          An experimental comparison of model-based clustering methods.
+          Mach. Learn., 42(1/2):9–29.
+    [^3]: Wikipedia contributors. (2021, February 18).
+          Variation of information. In Wikipedia, The Free Encyclopedia,
+          from https://en.wikipedia.org/w/index.php?title=Variation_of_information&oldid=1007562715
+
+    """
+
+    def __init__(self, cm=None):
+        super().__init__(cm)
+
+    def get(self):
+
+        conditional_entropy_c_k = 0.0
+
+        conditional_entropy_k_c = 0.0
+
+        for i in self.cm.classes:
+
+            for j in self.cm.classes:
+
+                try:
+                    conditional_entropy_c_k -= (
+                        self.cm[j][i]
+                        / self.cm.n_samples
+                        * math.log(self.cm[j][i] / self.cm.sum_col[i], 2)
+                    )
+                except (ValueError, ZeroDivisionError):
+                    pass
+
+                try:
+                    conditional_entropy_k_c -= (
+                        self.cm[i][j]
+                        / self.cm.n_samples
+                        * math.log(self.cm[i][j] / self.cm.sum_row[i], 2)
+                    )
+                except (ValueError, ZeroDivisionError):
+                    pass
+
+        return conditional_entropy_c_k + conditional_entropy_k_c
```

### Comparing `river-0.8.0/river/metrics/vbeta.py` & `river-0.9.0/river/metrics/vbeta.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,289 +1,289 @@
-import math
-
-from river import metrics
-
-__all__ = ["Completeness", "Homogeneity", "VBeta"]
-
-
-class Homogeneity(metrics.MultiClassMetric):
-    r"""Homogeneity Score.
-
-    Homogeneity metric [^1] of a cluster labeling given a ground truth.
-
-    In order to satisfy the homogeneity criteria, a clustering must assign only
-    those data points that are members of a single class to a single cluster. That
-    is, the class distribution within each cluster should be skewed to a single
-    class, that is, zero entropy. We determine how close a given clustering is to
-    this ideal by examining the conditional entropy of the class distribution given
-    the proposed clustering.
-
-    However, in an imperfect situation, the size of this value is dependent on the
-    size of the dataset and the distribution of class sizes. Therefore, instead of
-    taking the raw conditional entropy, we normalize by the maximum reduction in
-    entropy the clustering information could provide.
-
-    As such, we define homogeneity as:
-
-    $$
-    h = \begin{cases}
-    1 if H(C) = 0, \\
-    1 - \frac{H(C|K)}{H(C)} otherwise.
-    \end{cases}.
-    $$
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [1, 1, 2, 2, 3, 3]
-    >>> y_pred = [1, 1, 1, 2, 2, 2]
-
-    >>> metric = metrics.Homogeneity()
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     print(metric.update(yt, yp).get())
-    1.0
-    1.0
-    0.0
-    0.311278
-    0.37515
-    0.42062
-
-    >>> metric
-    Homogeneity: 0.42062
-
-    References
-    ----------
-    [^1]: Andrew Rosenberg and Julia Hirschberg (2007).
-          V-Measure: A conditional entropy-based external cluster evaluation measure.
-          Proceedings of the 2007 Joing Conference on Empirical Methods in Natural Language
-          Processing and Computational Natural Language Learning, pp. 410 - 420,
-          Prague, June 2007.
-
-    """
-
-    def __init__(self, cm=None):
-        super().__init__(cm)
-
-    @property
-    def works_with_weights(self):
-        return False
-
-    def get(self):
-        raw_conditional_entropy = 0.0
-        max_reduction_entropy = 0.0
-
-        for i in self.cm.classes:
-
-            for j in self.cm.classes:
-                try:
-                    raw_conditional_entropy -= (
-                        self.cm[j][i]
-                        / self.cm.n_samples
-                        * math.log(self.cm[j][i] / self.cm.sum_col[i], 2)
-                    )
-                except (ValueError, ZeroDivisionError):
-                    continue
-
-            try:
-                max_reduction_entropy -= (
-                    self.cm.sum_row[i]
-                    / self.cm.n_samples
-                    * math.log(self.cm.sum_row[i] / self.cm.n_samples, 2)
-                )
-            except (ValueError, ZeroDivisionError):
-                continue
-
-        try:
-            return 1.0 - raw_conditional_entropy / max_reduction_entropy
-        except ZeroDivisionError:
-            return 1.0
-
-
-class Completeness(metrics.MultiClassMetric):
-    r"""Completeness Score.
-
-    Completeness [^1] is symmetrical to homogeneity. In order to satisfy the
-    completeness criteria, a clustering must assign all of those datapoints
-    that are members of a single class to a single cluster. To evaluate completeness,
-    we examine the distribution cluster assignments within each class. In a
-    perfectly complete clustering solution, each of these distributions will be
-    completely skewed to a single cluster.
-
-    We can evaluate this degree of skew by calculating the conditional entropy of
-    the proposed cluster distribution given the class of the component data points.
-    However, in the worst case scenario, each class is represented by every cluster
-    with a distribution equal to the distribution of cluster sizes. Therefore,
-    symmetric to the claculation above, we define completeness as:
-
-    $$
-    c = \begin{cases}
-    1 if H(K) = 0, \\
-    1 - \frac{H(K|C)}{H(K)} otherwise.
-    \end{cases}.
-    $$
-
-    Parameters
-    ----------
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [1, 1, 2, 2, 3, 3]
-    >>> y_pred = [1, 1, 1, 2, 2, 2]
-
-    >>> metric = metrics.Completeness()
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     print(metric.update(yt, yp).get())
-    1.0
-    1.0
-    1.0
-    0.3836885465963443
-    0.5880325916843805
-    0.6666666666666667
-
-    >>> metric
-    Completeness: 0.666667
-
-    References
-    ----------
-    [^1]: Andrew Rosenberg and Julia Hirschberg (2007).
-          V-Measure: A conditional entropy-based external cluster evaluation measure.
-          Proceedings of the 2007 Joing Conference on Empirical Methods in Natural Language
-          Processing and Computational Natural Language Learning, pp. 410 - 420,
-          Prague, June 2007.
-
-    """
-
-    def __init__(self, cm=None):
-        super().__init__(cm)
-
-    @property
-    def works_with_weights(self):
-        return False
-
-    def get(self):
-        raw_conditional_entropy = 0
-        max_reduction_entropy = 0
-
-        for i in self.cm.classes:
-
-            for j in self.cm.classes:
-                try:
-                    raw_conditional_entropy -= (
-                        self.cm[i][j]
-                        / self.cm.n_samples
-                        * math.log(self.cm[i][j] / self.cm.sum_row[i])
-                    )
-                except (ValueError, ZeroDivisionError):
-                    continue
-
-            try:
-                max_reduction_entropy -= (
-                    self.cm.sum_col[i]
-                    / self.cm.n_samples
-                    * math.log(self.cm.sum_col[i] / self.cm.n_samples)
-                )
-            except (ValueError, ZeroDivisionError):
-                continue
-
-        try:
-            return 1.0 - raw_conditional_entropy / max_reduction_entropy
-        except ZeroDivisionError:
-            return 1.0
-
-
-class VBeta(metrics.MultiClassMetric):
-    r"""VBeta.
-
-    VBeta (or V-Measure) [^1] is an external entropy-based cluster evaluation measure.
-    It provides an elegant solution to many problems that affect previously defined
-    cluster evaluation measures including
-
-    * Dependance of clustering algorithm or dataset,
-
-    * The "problem of matching", where the clustering of only a portion of data
-    points are evaluated, and
-
-    * Accurate evaluation and combination of two desirable aspects of clustering,
-    homogeneity and completeness.
-
-    Based upon the calculations of homogeneity and completeness, a clustering
-    solution's V-measure is calculated by computing the weighted harmonic mean
-    of homogeneity and completeness,
-
-    $$
-    V_{\beta} = \frac{(1 + \beta) \times h \times c}{\beta \times h + c}.
-    $$
-
-    Parameters
-    ----------
-    beta
-        Weight of Homogeneity in the harmonic mean.
-    cm
-        This parameter allows sharing the same confusion
-        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
-        and computation time.
-
-    Examples
-    --------
-
-    >>> from river import metrics
-
-    >>> y_true = [1, 1, 2, 2, 3, 3]
-    >>> y_pred = [1, 1, 1, 2, 2, 2]
-
-    >>> metric = metrics.VBeta(beta=1.0)
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     print(metric.update(yt, yp).get())
-    1.0
-    1.0
-    0.0
-    0.3437110184854507
-    0.4580652856440158
-    0.5158037429793888
-
-    >>> metric
-    VBeta: 0.515804
-
-    References
-    ----------
-    [^1]: Andrew Rosenberg and Julia Hirschberg (2007).
-          V-Measure: A conditional entropy-based external cluster evaluation measure.
-          Proceedings of the 2007 Joing Conference on Empirical Methods in Natural Language
-          Processing and Computational Natural Language Learning, pp. 410 - 420,
-          Prague, June 2007.
-
-    """
-
-    def __init__(self, beta: float = 1.0, cm=None):
-        super().__init__(cm)
-        self.beta = beta
-        self.homogeneity = metrics.Homogeneity(self.cm)
-        self.completeness = metrics.Completeness(self.cm)
-
-    @property
-    def works_with_weights(self):
-        return False
-
-    def get(self):
-        h = self.homogeneity.get()
-        c = self.completeness.get()
-
-        try:
-            return (1 + self.beta) * h * c / (self.beta * h + c)
-        except ZeroDivisionError:
-            return 0.0
+import math
+
+from river import metrics
+
+__all__ = ["Completeness", "Homogeneity", "VBeta"]
+
+
+class Homogeneity(metrics.MultiClassMetric):
+    r"""Homogeneity Score.
+
+    Homogeneity metric [^1] of a cluster labeling given a ground truth.
+
+    In order to satisfy the homogeneity criteria, a clustering must assign only
+    those data points that are members of a single class to a single cluster. That
+    is, the class distribution within each cluster should be skewed to a single
+    class, that is, zero entropy. We determine how close a given clustering is to
+    this ideal by examining the conditional entropy of the class distribution given
+    the proposed clustering.
+
+    However, in an imperfect situation, the size of this value is dependent on the
+    size of the dataset and the distribution of class sizes. Therefore, instead of
+    taking the raw conditional entropy, we normalize by the maximum reduction in
+    entropy the clustering information could provide.
+
+    As such, we define homogeneity as:
+
+    $$
+    h = \begin{cases}
+    1 if H(C) = 0, \\
+    1 - \frac{H(C|K)}{H(C)} otherwise.
+    \end{cases}.
+    $$
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [1, 1, 2, 2, 3, 3]
+    >>> y_pred = [1, 1, 1, 2, 2, 2]
+
+    >>> metric = metrics.Homogeneity()
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     print(metric.update(yt, yp).get())
+    1.0
+    1.0
+    0.0
+    0.311278
+    0.37515
+    0.42062
+
+    >>> metric
+    Homogeneity: 0.42062
+
+    References
+    ----------
+    [^1]: Andrew Rosenberg and Julia Hirschberg (2007).
+          V-Measure: A conditional entropy-based external cluster evaluation measure.
+          Proceedings of the 2007 Joing Conference on Empirical Methods in Natural Language
+          Processing and Computational Natural Language Learning, pp. 410 - 420,
+          Prague, June 2007.
+
+    """
+
+    def __init__(self, cm=None):
+        super().__init__(cm)
+
+    @property
+    def works_with_weights(self):
+        return False
+
+    def get(self):
+        raw_conditional_entropy = 0.0
+        max_reduction_entropy = 0.0
+
+        for i in self.cm.classes:
+
+            for j in self.cm.classes:
+                try:
+                    raw_conditional_entropy -= (
+                        self.cm[j][i]
+                        / self.cm.n_samples
+                        * math.log(self.cm[j][i] / self.cm.sum_col[i], 2)
+                    )
+                except (ValueError, ZeroDivisionError):
+                    continue
+
+            try:
+                max_reduction_entropy -= (
+                    self.cm.sum_row[i]
+                    / self.cm.n_samples
+                    * math.log(self.cm.sum_row[i] / self.cm.n_samples, 2)
+                )
+            except (ValueError, ZeroDivisionError):
+                continue
+
+        try:
+            return 1.0 - raw_conditional_entropy / max_reduction_entropy
+        except ZeroDivisionError:
+            return 1.0
+
+
+class Completeness(metrics.MultiClassMetric):
+    r"""Completeness Score.
+
+    Completeness [^1] is symmetrical to homogeneity. In order to satisfy the
+    completeness criteria, a clustering must assign all of those datapoints
+    that are members of a single class to a single cluster. To evaluate completeness,
+    we examine the distribution cluster assignments within each class. In a
+    perfectly complete clustering solution, each of these distributions will be
+    completely skewed to a single cluster.
+
+    We can evaluate this degree of skew by calculating the conditional entropy of
+    the proposed cluster distribution given the class of the component data points.
+    However, in the worst case scenario, each class is represented by every cluster
+    with a distribution equal to the distribution of cluster sizes. Therefore,
+    symmetric to the claculation above, we define completeness as:
+
+    $$
+    c = \begin{cases}
+    1 if H(K) = 0, \\
+    1 - \frac{H(K|C)}{H(K)} otherwise.
+    \end{cases}.
+    $$
+
+    Parameters
+    ----------
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [1, 1, 2, 2, 3, 3]
+    >>> y_pred = [1, 1, 1, 2, 2, 2]
+
+    >>> metric = metrics.Completeness()
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     print(metric.update(yt, yp).get())
+    1.0
+    1.0
+    1.0
+    0.3836885465963443
+    0.5880325916843805
+    0.6666666666666667
+
+    >>> metric
+    Completeness: 0.666667
+
+    References
+    ----------
+    [^1]: Andrew Rosenberg and Julia Hirschberg (2007).
+          V-Measure: A conditional entropy-based external cluster evaluation measure.
+          Proceedings of the 2007 Joing Conference on Empirical Methods in Natural Language
+          Processing and Computational Natural Language Learning, pp. 410 - 420,
+          Prague, June 2007.
+
+    """
+
+    def __init__(self, cm=None):
+        super().__init__(cm)
+
+    @property
+    def works_with_weights(self):
+        return False
+
+    def get(self):
+        raw_conditional_entropy = 0
+        max_reduction_entropy = 0
+
+        for i in self.cm.classes:
+
+            for j in self.cm.classes:
+                try:
+                    raw_conditional_entropy -= (
+                        self.cm[i][j]
+                        / self.cm.n_samples
+                        * math.log(self.cm[i][j] / self.cm.sum_row[i])
+                    )
+                except (ValueError, ZeroDivisionError):
+                    continue
+
+            try:
+                max_reduction_entropy -= (
+                    self.cm.sum_col[i]
+                    / self.cm.n_samples
+                    * math.log(self.cm.sum_col[i] / self.cm.n_samples)
+                )
+            except (ValueError, ZeroDivisionError):
+                continue
+
+        try:
+            return 1.0 - raw_conditional_entropy / max_reduction_entropy
+        except ZeroDivisionError:
+            return 1.0
+
+
+class VBeta(metrics.MultiClassMetric):
+    r"""VBeta.
+
+    VBeta (or V-Measure) [^1] is an external entropy-based cluster evaluation measure.
+    It provides an elegant solution to many problems that affect previously defined
+    cluster evaluation measures including
+
+    * Dependance of clustering algorithm or dataset,
+
+    * The "problem of matching", where the clustering of only a portion of data
+    points are evaluated, and
+
+    * Accurate evaluation and combination of two desirable aspects of clustering,
+    homogeneity and completeness.
+
+    Based upon the calculations of homogeneity and completeness, a clustering
+    solution's V-measure is calculated by computing the weighted harmonic mean
+    of homogeneity and completeness,
+
+    $$
+    V_{\beta} = \frac{(1 + \beta) \times h \times c}{\beta \times h + c}.
+    $$
+
+    Parameters
+    ----------
+    beta
+        Weight of Homogeneity in the harmonic mean.
+    cm
+        This parameter allows sharing the same confusion
+        matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage
+        and computation time.
+
+    Examples
+    --------
+
+    >>> from river import metrics
+
+    >>> y_true = [1, 1, 2, 2, 3, 3]
+    >>> y_pred = [1, 1, 1, 2, 2, 2]
+
+    >>> metric = metrics.VBeta(beta=1.0)
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     print(metric.update(yt, yp).get())
+    1.0
+    1.0
+    0.0
+    0.3437110184854507
+    0.4580652856440158
+    0.5158037429793888
+
+    >>> metric
+    VBeta: 0.515804
+
+    References
+    ----------
+    [^1]: Andrew Rosenberg and Julia Hirschberg (2007).
+          V-Measure: A conditional entropy-based external cluster evaluation measure.
+          Proceedings of the 2007 Joing Conference on Empirical Methods in Natural Language
+          Processing and Computational Natural Language Learning, pp. 410 - 420,
+          Prague, June 2007.
+
+    """
+
+    def __init__(self, beta: float = 1.0, cm=None):
+        super().__init__(cm)
+        self.beta = beta
+        self.homogeneity = metrics.Homogeneity(self.cm)
+        self.completeness = metrics.Completeness(self.cm)
+
+    @property
+    def works_with_weights(self):
+        return False
+
+    def get(self):
+        h = self.homogeneity.get()
+        c = self.completeness.get()
+
+        try:
+            return (1 + self.beta) * h * c / (self.beta * h + c)
+        except ZeroDivisionError:
+            return 0.0
```

### Comparing `river-0.8.0/river/multiclass/occ.py` & `river-0.9.0/river/multiclass/occ.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,130 +1,130 @@
-import collections
-import copy
-import random
-
-from river import base, linear_model
-
-__all__ = ["OutputCodeClassifier"]
-
-
-def l1_dist(a, b):
-    return sum(abs(ai - bi) for ai, bi in zip(a, b))
-
-
-class OutputCodeClassifier(base.WrapperMixin, base.Classifier):
-    """Output-code multiclass strategy.
-
-    This also referred to as "error-correcting output codes".
-
-    This class allows to learn a multi-class classification problem with a binary classifier. Each
-    class is converted to a code of 0s and 1s. The length of the code is called  the code size. A
-    copy of the classifier made for code. The codes associated with the classes are stored in a
-    code book.
-
-    When a new sample arrives, the label's code is retrieved from the code book. Then, each
-    classifier is trained on the relevant part of code, which is either a 0 or a 1.
-
-    For predicting, each classifier outputs a probability. These are then compared to each code in
-    the code book, and the label which is the "closest" is chosen as the most likely class.
-    Closeness is determined in terms of Manhattan distance.
-
-    One specificity of online learning is that we don't how many classes there are initially.
-    Therefore, a random procedure generates random codes on the fly whenever a previously unseed
-    label appears.
-
-    Parameters
-    ----------
-    classifier
-        A binary classifier, although a multi-class classifier will work too.
-    code_size
-        The code size, which dictates how many copies of the provided classifiers to train. Must be
-        strictly positive.
-    seed
-        A random seed number that can be set for reproducibility.
-
-    Examples
-    --------
-
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import linear_model
-    >>> from river import metrics
-    >>> from river import multiclass
-    >>> from river import preprocessing
-
-    >>> dataset = datasets.ImageSegments()
-
-    >>> scaler = preprocessing.StandardScaler()
-    >>> ooc = OutputCodeClassifier(
-    ...     classifier=linear_model.LogisticRegression(),
-    ...     code_size=10,
-    ...     seed=24
-    ... )
-    >>> model = scaler | ooc
-
-    >>> metric = metrics.MacroF1()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    MacroF1: 0.797119
-
-    References
-    ----------
-    [^1]: [Dietterich, T.G. and Bakiri, G., 1994. Solving multiclass learning problems via error-correcting output codes. Journal of artificial intelligence research, 2, pp.263-286.](https://arxiv.org/pdf/cs/9501101.pdf)
-    [^2]: [Allwein, E.L., Schapire, R.E. and Singer, Y., 2000. Reducing multiclass to binary: A unifying approach for margin classifiers. Journal of machine learning research, 1(Dec), pp.113-141.](https://www.cs.princeton.edu/~schapire/talks/ecoc-icml10.pdf)
-
-    """
-
-    def __init__(self, classifier: base.Classifier, code_size: int, seed: int = None):
-        self.classifier = classifier
-        self.code_size = code_size
-        self.seed = seed
-        self._rng = random.Random(seed)
-
-        self.classifiers = {i: copy.deepcopy(classifier) for i in range(code_size)}
-
-        # We don't how many classes there are, therefore we can't generate the code book
-        # from the start. Therefore, we define a random queue of integers. When a new class
-        # appears, we get the next integer and convert it to a code.
-        integers = list(range(2 ** code_size))
-        self._rng.shuffle(integers)
-        self._ints = iter(integers)
-        self.code_book = collections.defaultdict(self._next_code)
-
-    def _next_code(self):
-        i = next(self._ints)
-        b = bin(i)[2:]  # convert to a string of 0s and 1s
-        b = b.zfill(self.code_size)  # ensure the code is of length code_size
-        return tuple(int(c) for c in b)
-
-    @property
-    def _multiclass(self):
-        return True
-
-    @property
-    def _wrapped_model(self):
-        return self.classifier
-
-    @classmethod
-    def _unit_test_params(cls):
-        return {"classifier": linear_model.LogisticRegression(), "code_size": 6}
-
-    def learn_one(self, x, y):
-
-        code = self.code_book[y]
-
-        for i, c in enumerate(code):
-            self.classifiers[i].learn_one(x, c)
-
-        return self
-
-    def predict_one(self, x):
-
-        if not self.code_book:  # it's empty
-            return None
-
-        output = [None for _ in range(self.code_size)]
-
-        for i, clf in self.classifiers.items():
-            output[i] = clf.predict_proba_one(x).get(True, 0.0)
-
-        return min(self.code_book, key=lambda c: l1_dist(self.code_book[c], output))
+import collections
+import copy
+import random
+
+from river import base, linear_model
+
+__all__ = ["OutputCodeClassifier"]
+
+
+def l1_dist(a, b):
+    return sum(abs(ai - bi) for ai, bi in zip(a, b))
+
+
+class OutputCodeClassifier(base.Wrapper, base.Classifier):
+    """Output-code multiclass strategy.
+
+    This also referred to as "error-correcting output codes".
+
+    This class allows to learn a multi-class classification problem with a binary classifier. Each
+    class is converted to a code of 0s and 1s. The length of the code is called  the code size. A
+    copy of the classifier made for code. The codes associated with the classes are stored in a
+    code book.
+
+    When a new sample arrives, the label's code is retrieved from the code book. Then, each
+    classifier is trained on the relevant part of code, which is either a 0 or a 1.
+
+    For predicting, each classifier outputs a probability. These are then compared to each code in
+    the code book, and the label which is the "closest" is chosen as the most likely class.
+    Closeness is determined in terms of Manhattan distance.
+
+    One specificity of online learning is that we don't how many classes there are initially.
+    Therefore, a random procedure generates random codes on the fly whenever a previously unseed
+    label appears.
+
+    Parameters
+    ----------
+    classifier
+        A binary classifier, although a multi-class classifier will work too.
+    code_size
+        The code size, which dictates how many copies of the provided classifiers to train. Must be
+        strictly positive.
+    seed
+        A random seed number that can be set for reproducibility.
+
+    Examples
+    --------
+
+    >>> from river import datasets
+    >>> from river import evaluate
+    >>> from river import linear_model
+    >>> from river import metrics
+    >>> from river import multiclass
+    >>> from river import preprocessing
+
+    >>> dataset = datasets.ImageSegments()
+
+    >>> scaler = preprocessing.StandardScaler()
+    >>> ooc = OutputCodeClassifier(
+    ...     classifier=linear_model.LogisticRegression(),
+    ...     code_size=10,
+    ...     seed=24
+    ... )
+    >>> model = scaler | ooc
+
+    >>> metric = metrics.MacroF1()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    MacroF1: 0.797119
+
+    References
+    ----------
+    [^1]: [Dietterich, T.G. and Bakiri, G., 1994. Solving multiclass learning problems via error-correcting output codes. Journal of artificial intelligence research, 2, pp.263-286.](https://arxiv.org/pdf/cs/9501101.pdf)
+    [^2]: [Allwein, E.L., Schapire, R.E. and Singer, Y., 2000. Reducing multiclass to binary: A unifying approach for margin classifiers. Journal of machine learning research, 1(Dec), pp.113-141.](https://www.cs.princeton.edu/~schapire/talks/ecoc-icml10.pdf)
+
+    """
+
+    def __init__(self, classifier: base.Classifier, code_size: int, seed: int = None):
+        self.classifier = classifier
+        self.code_size = code_size
+        self.seed = seed
+        self._rng = random.Random(seed)
+
+        self.classifiers = {i: copy.deepcopy(classifier) for i in range(code_size)}
+
+        # We don't know how many classes there are, therefore we can't generate the code book
+        # from the start. Therefore, we define a random queue of integers. When a new class
+        # appears, we get the next integer and convert it to a code.
+        integers = list(range(2 ** code_size))
+        self._rng.shuffle(integers)
+        self._ints = iter(integers)
+        self.code_book = collections.defaultdict(self._next_code)
+
+    def _next_code(self):
+        i = next(self._ints)
+        b = bin(i)[2:]  # convert to a string of 0s and 1s
+        b = b.zfill(self.code_size)  # ensure the code is of length code_size
+        return tuple(int(c) for c in b)
+
+    @property
+    def _multiclass(self):
+        return True
+
+    @property
+    def _wrapped_model(self):
+        return self.classifier
+
+    @classmethod
+    def _unit_test_params(cls):
+        yield {"classifier": linear_model.LogisticRegression(), "code_size": 6}
+
+    def learn_one(self, x, y):
+
+        code = self.code_book[y]
+
+        for i, c in enumerate(code):
+            self.classifiers[i].learn_one(x, c)
+
+        return self
+
+    def predict_one(self, x):
+
+        if not self.code_book:  # it's empty
+            return None
+
+        output = [None for _ in range(self.code_size)]
+
+        for i, clf in self.classifiers.items():
+            output[i] = clf.predict_proba_one(x).get(True, 0.0)
+
+        return min(self.code_book, key=lambda c: l1_dist(self.code_book[c], output))
```

### Comparing `river-0.8.0/river/naive_bayes/base.py` & `river-0.9.0/river/naive_bayes/base.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,93 +1,93 @@
-import abc
-import math
-
-import numpy as np
-import pandas as pd
-from scipy import sparse, special
-
-from river import base
-
-
-class BaseNB(base.MiniBatchClassifier):
-    """Base Naive Bayes class."""
-
-    @abc.abstractmethod
-    def joint_log_likelihood(self, x: dict) -> float:
-        """Compute the unnormalized posterior log-likelihood of x.
-
-        The log-likelihood is `log P(c) + log P(x|c)`.
-
-        """
-
-    @abc.abstractmethod
-    def joint_log_likelihood_many(self, X: pd.DataFrame) -> pd.DataFrame:
-        """Compute the unnormalized posterior log-likelihood of x in mini-batches.
-
-        The log-likelihood is `log P(c) + log P(x|c)`.
-
-        """
-
-    def predict_proba_one(self, x):
-        """Return probabilities using the log-likelihoods."""
-        jll = self.joint_log_likelihood(x)
-        if not jll:
-            return {}
-        lse = special.logsumexp(list(jll.values()))
-        return {label: math.exp(ll - lse) for label, ll in jll.items()}
-
-    def predict_proba_many(self, X: pd.DataFrame) -> pd.DataFrame:
-        """Return probabilities using the log-likelihoods in mini-batchs setting."""
-        jll = self.joint_log_likelihood_many(X)
-        if jll.empty:
-            return jll
-        lse = pd.Series(special.logsumexp(jll, axis=1))
-        return np.exp(jll.subtract(lse.values, axis="rows"))
-
-    @property
-    def _multiclass(self):
-        return True
-
-
-def from_dict(data: dict) -> pd.DataFrame:
-    """Convert a dict into a pandas dataframe.
-    This function is faster than pd.from_dict (01/02/2021).
-
-    Parameters
-    ----------
-    data
-        Input data as dict.
-
-    Returns
-    --------
-        Dict to pandas dataframe.
-
-    """
-    dict_data, index = list(data.values()), list(data.keys())
-    return pd.DataFrame(data=dict_data, index=index, dtype="float32")
-
-
-def one_hot_encode(y: pd.Series) -> pd.DataFrame:
-    """One hot encode input pandas series into sparse pandas DataFrame.
-
-    Parameters
-    ----------
-    y
-        Pandas Series of strings.
-
-    Returns
-    --------
-    One hot encoded sparse dataframe.
-
-    """
-    classes = np.unique(y)
-    indices = np.searchsorted(classes, y)
-    indptr = np.hstack((0, np.cumsum(np.in1d(y, classes))))
-    data = np.empty_like(indices)
-    data.fill(1)
-    return pd.DataFrame.sparse.from_spmatrix(
-        sparse.csr_matrix(
-            (data, indices, indptr), shape=(y.shape[0], classes.shape[0])
-        ),
-        index=y.index,
-        columns=[str(c) for c in classes],
-    )
+import abc
+import math
+
+import numpy as np
+import pandas as pd
+from scipy import sparse, special
+
+from river import base
+
+
+class BaseNB(base.MiniBatchClassifier):
+    """Base Naive Bayes class."""
+
+    @abc.abstractmethod
+    def joint_log_likelihood(self, x: dict) -> float:
+        """Compute the unnormalized posterior log-likelihood of x.
+
+        The log-likelihood is `log P(c) + log P(x|c)`.
+
+        """
+
+    @abc.abstractmethod
+    def joint_log_likelihood_many(self, X: pd.DataFrame) -> pd.DataFrame:
+        """Compute the unnormalized posterior log-likelihood of x in mini-batches.
+
+        The log-likelihood is `log P(c) + log P(x|c)`.
+
+        """
+
+    def predict_proba_one(self, x):
+        """Return probabilities using the log-likelihoods."""
+        jll = self.joint_log_likelihood(x)
+        if not jll:
+            return {}
+        lse = special.logsumexp(list(jll.values()))
+        return {label: math.exp(ll - lse) for label, ll in jll.items()}
+
+    def predict_proba_many(self, X: pd.DataFrame) -> pd.DataFrame:
+        """Return probabilities using the log-likelihoods in mini-batchs setting."""
+        jll = self.joint_log_likelihood_many(X)
+        if jll.empty:
+            return jll
+        lse = pd.Series(special.logsumexp(jll, axis=1))
+        return np.exp(jll.subtract(lse.values, axis="rows"))
+
+    @property
+    def _multiclass(self):
+        return True
+
+
+def from_dict(data: dict) -> pd.DataFrame:
+    """Convert a dict into a pandas dataframe.
+    This function is faster than pd.from_dict (01/02/2021).
+
+    Parameters
+    ----------
+    data
+        Input data as dict.
+
+    Returns
+    --------
+        Dict to pandas dataframe.
+
+    """
+    dict_data, index = list(data.values()), list(data.keys())
+    return pd.DataFrame(data=dict_data, index=index, dtype="float32")
+
+
+def one_hot_encode(y: pd.Series) -> pd.DataFrame:
+    """One hot encode input pandas series into sparse pandas DataFrame.
+
+    Parameters
+    ----------
+    y
+        Pandas Series of strings.
+
+    Returns
+    --------
+    One hot encoded sparse dataframe.
+
+    """
+    classes = np.unique(y)
+    indices = np.searchsorted(classes, y)
+    indptr = np.hstack((0, np.cumsum(np.in1d(y, classes))))
+    data = np.empty_like(indices)
+    data.fill(1)
+    return pd.DataFrame.sparse.from_spmatrix(
+        sparse.csr_matrix(
+            (data, indices, indptr), shape=(y.shape[0], classes.shape[0])
+        ),
+        index=y.index,
+        columns=[str(c) for c in classes],
+    )
```

### Comparing `river-0.8.0/river/naive_bayes/gaussian.py` & `river-0.9.0/river/naive_bayes/gaussian.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,78 +1,79 @@
-import collections
-import functools
-import math
-
-import pandas as pd
-from scipy import special
-
-from river import base, proba
-
-__all__ = ["GaussianNB"]
-
-
-class GaussianNB(base.Classifier):
-    """Gaussian Naive Bayes.
-
-    A Gaussian distribution $G_{cf}$ is maintained for each class $c$ and each feature $f$. Each
-    Gaussian is updated using the amount associated with each feature; the details can be be found
-    in `proba.Gaussian`. The joint log-likelihood is then obtained by summing the log probabilities
-    of each feature associated with each class.
-
-    Examples
-    --------
-
-    >>> from river import naive_bayes
-    >>> from river import stream
-    >>> import numpy as np
-
-    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
-    >>> Y = np.array([1, 1, 1, 2, 2, 2])
-
-    >>> model = naive_bayes.GaussianNB()
-
-    >>> for x, y in stream.iter_array(X, Y):
-    ...     _ = model.learn_one(x, y)
-
-    >>> model.predict_one({0: -0.8, 1: -1})
-    1
-
-    """
-
-    def __init__(self):
-        self.class_counts = collections.Counter()
-        defaultdict = collections.defaultdict
-        self.gaussians = defaultdict(functools.partial(defaultdict, proba.Gaussian))
-
-    def learn_one(self, x, y):
-
-        self.class_counts.update((y,))
-
-        for i, xi in x.items():
-            self.gaussians[y][i].update(xi)
-
-        return self
-
-    def predict_proba_one(self, x):
-        """Return probabilities using the log-likelihoods."""
-        jll = self.joint_log_likelihood(x)
-        if not jll:
-            return {}
-        lse = special.logsumexp(list(jll.values()))
-        return {label: math.exp(ll - lse) for label, ll in jll.items()}
-
-    def p_class(self, c):
-        return self.class_counts[c] / sum(self.class_counts.values())
-
-    def joint_log_likelihood(self, x):
-        return {
-            c: math.log(self.p_class(c))
-            + sum(math.log(10e-10 + gaussians[i].pdf(xi)) for i, xi in x.items())
-            for c, gaussians in self.gaussians.items()
-        }
-
-    def joint_log_likelihood_many(self, X: pd.DataFrame):
-        pass
-
-    @property
-    def _multiclass(self):
-        return True
+import collections
+import functools
+import math
+
+import pandas as pd
+from scipy import special
+
+from river import base, proba
+
+__all__ = ["GaussianNB"]
+
+
+class GaussianNB(base.Classifier):
+    """Gaussian Naive Bayes.
+
+    A Gaussian distribution $G_{cf}$ is maintained for each class $c$ and each feature $f$. Each
+    Gaussian is updated using the amount associated with each feature; the details can be be found
+    in `proba.Gaussian`. The joint log-likelihood is then obtained by summing the log probabilities
+    of each feature associated with each class.
+
+    Examples
+    --------
+
+    >>> from river import naive_bayes
+    >>> from river import stream
+    >>> import numpy as np
+
+    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
+    >>> Y = np.array([1, 1, 1, 2, 2, 2])
+
+    >>> model = naive_bayes.GaussianNB()
+
+    >>> for x, y in stream.iter_array(X, Y):
+    ...     _ = model.learn_one(x, y)
+
+    >>> model.predict_one({0: -0.8, 1: -1})
+    1
+
+    """
+
+    def __init__(self):
+        self.class_counts = collections.Counter()
+        self.gaussians = collections.defaultdict(
+            functools.partial(collections.defaultdict, proba.Gaussian)
+        )
+
+    def learn_one(self, x, y):
+
+        self.class_counts.update((y,))
+
+        for i, xi in x.items():
+            self.gaussians[y][i].update(xi)
+
+        return self
+
+    def predict_proba_one(self, x):
+        """Return probabilities using the log-likelihoods."""
+        jll = self.joint_log_likelihood(x)
+        if not jll:
+            return {}
+        lse = special.logsumexp(list(jll.values()))
+        return {label: math.exp(ll - lse) for label, ll in jll.items()}
+
+    def p_class(self, c):
+        return self.class_counts[c] / sum(self.class_counts.values())
+
+    def joint_log_likelihood(self, x):
+        return {
+            c: math.log(self.p_class(c))
+            + sum(math.log(10e-10 + gaussians[i].pdf(xi)) for i, xi in x.items())
+            for c, gaussians in self.gaussians.items()
+        }
+
+    def joint_log_likelihood_many(self, X: pd.DataFrame):
+        pass
+
+    @property
+    def _multiclass(self):
+        return True
```

### Comparing `river-0.8.0/river/neighbors/base_neighbors.py` & `river-0.9.0/river/neighbors/base_neighbors.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,233 +1,233 @@
-import itertools
-import typing
-
-import numpy as np
-from scipy.spatial import cKDTree
-
-from river import base
-from river.utils.skmultiflow_utils import get_dimensions
-
-
-class KNeighborsBuffer:
-    """Keep a fixed-size sliding window of the most recent data samples.
-
-    Parameters
-    ----------
-    window_size
-        The size of the window.
-
-    Raises
-    ------
-    ValueError
-        If at any moment, a sample with a different number of attributes than
-         those already observed is passed.
-
-    Notes
-    -----
-    It updates its stored samples by the FIFO method, which means
-    that when size limit is reached, old samples are dumped to give
-    place to new samples.
-
-    The internal buffer does not keep order of the stored samples,
-    when the size limit is reached, the older samples are overwritten
-    with new ones (circular buffer).
-
-    """
-
-    def __init__(self, window_size: int = 1000):
-        self.window_size = window_size
-        self._n_features: int = -1
-        self._n_targets: int = -1
-        self._size: int = 0
-        self._next_insert: int = 0
-        self._oldest: int = 0
-        self._imask: np.ndarray
-        self._X: np.ndarray
-        self._y: typing.List
-        self._is_initialized: bool = False
-
-    def _configure(self):
-        # Binary instance mask to filter data in the buffer
-        self._imask = np.zeros(self.window_size, dtype=bool)
-        self._X = np.zeros((self.window_size, self._n_features))
-        self._y = [None for _ in range(self.window_size)]
-        self._is_initialized = True
-
-    def reset(self):
-        """Reset the sliding window. """
-        self._n_features = -1
-        self._n_targets = -1
-        self._size = 0
-        self._next_insert = 0
-        self._oldest = 0
-        self._imask = None
-        self._X = None
-        self._y = None
-        self._is_initialized = False
-
-        return self
-
-    def append(self, x: np.ndarray, y: base.typing.Target) -> "KNeighborsBuffer":
-        """Add a (single) sample to the sample window.
-
-        x
-            1D-array of feature for a single sample.
-
-        y
-            The target data for a single sample.
-
-        Raises
-        ------
-        ValueError: If at any moment, a sample with a different number of
-        attributes than that of the n_attributes parameter is passed, a
-        ValueError is raised.
-
-        TypeError: If the buffer type is altered by the user, or is not
-        correctly initialized, a TypeError may be raised.
-
-        """
-        if not self._is_initialized:
-            self._n_features = get_dimensions(x)[1]
-            self._n_targets = get_dimensions(y)[1]
-            self._configure()
-
-        if self._n_features != get_dimensions(x)[1]:
-            raise ValueError(
-                "Inconsistent number of features in X: {}, previously observed {}.".format(
-                    get_dimensions(x)[1], self._n_features
-                )
-            )
-
-        self._X[self._next_insert, :] = x
-        self._y[self._next_insert] = y
-
-        slot_replaced = self._imask[self._next_insert]
-
-        # Update the instance storing logic
-        self._imask[self._next_insert] = True  # Mark slot as filled
-        self._next_insert = (
-            self._next_insert + 1 if self._next_insert < self.window_size - 1 else 0
-        )
-
-        if (
-            slot_replaced
-        ):  # The oldest sample was replaced (complete cycle in the buffer)
-            self._oldest = self._next_insert
-        else:  # Actual buffer increased
-            self._size += 1
-
-        return self
-
-    def pop(self) -> typing.Union[typing.Tuple[np.ndarray, base.typing.Target], None]:
-        """Remove and return the most recent element added to the buffer. """
-        if self.size > 0:
-            self._next_insert = (
-                self._next_insert - 1 if self._next_insert > 0 else self.window_size - 1
-            )
-            x, y = self._X[self._next_insert], self._y[self._next_insert]
-            self._imask[self._next_insert] = False  # Mark slot as free
-            self._size -= 1
-
-            return x, y
-        else:
-            return None
-
-    def popleft(
-        self,
-    ) -> typing.Union[typing.Tuple[np.ndarray, base.typing.Target], None]:
-        """Remove and return the oldest element in the buffer. """
-        if self.size > 0:
-            x, y = self._X[self._oldest], self._y[self._oldest]
-            self._imask[self._oldest] = False  # Mark slot as free
-            self._oldest = (
-                self._oldest + 1 if self._oldest < self.window_size - 1 else 0
-            )
-            if self._oldest == self._next_insert:
-                # Shift circular buffer and make its starting point be the index 0
-                self._oldest = self._next_insert = 0
-            self._size -= 1
-
-            return x, y
-
-    def clear(self) -> "KNeighborsBuffer":
-        """Clear all stored elements."""
-        self._next_insert = 0
-        self._oldest = 0
-        self._size = 0
-        # Just reset the instance filtering mask, not the buffers
-        self._imask = np.zeros(self.window_size, dtype=bool)
-
-        return self
-
-    @property
-    def features_buffer(self) -> np.ndarray:
-        """Get the features buffer.
-
-        The shape of the buffer is (window_size, n_features).
-        """
-        return self._X[self._imask]  # Only return the actually filled instances
-
-    @property
-    def targets_buffer(self) -> typing.List:
-        """Get the targets buffer
-
-        The shape of the buffer is (window_size, n_targets).
-        """
-        return list(itertools.compress(self._y, self._imask))
-
-    @property
-    def n_targets(self) -> int:
-        """Get the number of targets. """
-        return self._n_targets
-
-    @property
-    def n_features(self) -> int:
-        """Get the number of features. """
-        return self._n_features
-
-    @property
-    def size(self) -> int:
-        """Get the window size. """
-        return self._size
-
-
-class BaseNeighbors:
-    """Base class for neighbors-based estimators. """
-
-    def __init__(
-        self,
-        n_neighbors: int = 5,
-        window_size: int = 1000,
-        leaf_size: int = 30,
-        p: float = 2,
-        **kwargs
-    ):
-        self.n_neighbors = n_neighbors
-        self.window_size = window_size
-        self.leaf_size = leaf_size
-        self._kwargs = kwargs
-
-        if p < 1:
-            raise ValueError(
-                "Invalid Minkowski p-norm value: {}.\n"
-                "Values must be greater than or equal to 1".format(p)
-            )
-        self.p = p
-        self.data_window = KNeighborsBuffer(window_size=window_size)
-
-    def _get_neighbors(self, x):
-        X = self.data_window.features_buffer
-        tree = cKDTree(X, leafsize=self.leaf_size, **self._kwargs)
-        dist, idx = tree.query(x.reshape(1, -1), k=self.n_neighbors, p=self.p)
-
-        # We make sure dist and idx is 2D since when k = 1 dist is one dimensional.
-        if not isinstance(dist[0], np.ndarray):
-            dist = [dist]
-            idx = [idx]
-        return dist, idx
-
-    def reset(self) -> "BaseNeighbors":
-        """Reset estimator. """
-        self.data_window.reset()
-
-        return self
+import itertools
+import typing
+
+import numpy as np
+from scipy.spatial import cKDTree
+
+from river import base
+from river.utils.skmultiflow_utils import get_dimensions
+
+
+class KNeighborsBuffer:
+    """Keep a fixed-size sliding window of the most recent data samples.
+
+    Parameters
+    ----------
+    window_size
+        The size of the window.
+
+    Raises
+    ------
+    ValueError
+        If at any moment, a sample with a different number of attributes than
+         those already observed is passed.
+
+    Notes
+    -----
+    It updates its stored samples by the FIFO method, which means
+    that when size limit is reached, old samples are dumped to give
+    place to new samples.
+
+    The internal buffer does not keep order of the stored samples,
+    when the size limit is reached, the older samples are overwritten
+    with new ones (circular buffer).
+
+    """
+
+    def __init__(self, window_size: int = 1000):
+        self.window_size = window_size
+        self._n_features: int = -1
+        self._n_targets: int = -1
+        self._size: int = 0
+        self._next_insert: int = 0
+        self._oldest: int = 0
+        self._imask: np.ndarray
+        self._X: np.ndarray
+        self._y: typing.List
+        self._is_initialized: bool = False
+
+    def _configure(self):
+        # Binary instance mask to filter data in the buffer
+        self._imask = np.zeros(self.window_size, dtype=bool)
+        self._X = np.zeros((self.window_size, self._n_features))
+        self._y = [None for _ in range(self.window_size)]
+        self._is_initialized = True
+
+    def reset(self):
+        """Reset the sliding window. """
+        self._n_features = -1
+        self._n_targets = -1
+        self._size = 0
+        self._next_insert = 0
+        self._oldest = 0
+        self._imask = None
+        self._X = None
+        self._y = None
+        self._is_initialized = False
+
+        return self
+
+    def append(self, x: np.ndarray, y: base.typing.Target) -> "KNeighborsBuffer":
+        """Add a (single) sample to the sample window.
+
+        x
+            1D-array of feature for a single sample.
+
+        y
+            The target data for a single sample.
+
+        Raises
+        ------
+        ValueError: If at any moment, a sample with a different number of
+        attributes than that of the n_attributes parameter is passed, a
+        ValueError is raised.
+
+        TypeError: If the buffer type is altered by the user, or is not
+        correctly initialized, a TypeError may be raised.
+
+        """
+        if not self._is_initialized:
+            self._n_features = get_dimensions(x)[1]
+            self._n_targets = get_dimensions(y)[1]
+            self._configure()
+
+        if self._n_features != get_dimensions(x)[1]:
+            raise ValueError(
+                "Inconsistent number of features in X: {}, previously observed {}.".format(
+                    get_dimensions(x)[1], self._n_features
+                )
+            )
+
+        self._X[self._next_insert, :] = x
+        self._y[self._next_insert] = y
+
+        slot_replaced = self._imask[self._next_insert]
+
+        # Update the instance storing logic
+        self._imask[self._next_insert] = True  # Mark slot as filled
+        self._next_insert = (
+            self._next_insert + 1 if self._next_insert < self.window_size - 1 else 0
+        )
+
+        if (
+            slot_replaced
+        ):  # The oldest sample was replaced (complete cycle in the buffer)
+            self._oldest = self._next_insert
+        else:  # Actual buffer increased
+            self._size += 1
+
+        return self
+
+    def pop(self) -> typing.Union[typing.Tuple[np.ndarray, base.typing.Target], None]:
+        """Remove and return the most recent element added to the buffer. """
+        if self.size > 0:
+            self._next_insert = (
+                self._next_insert - 1 if self._next_insert > 0 else self.window_size - 1
+            )
+            x, y = self._X[self._next_insert], self._y[self._next_insert]
+            self._imask[self._next_insert] = False  # Mark slot as free
+            self._size -= 1
+
+            return x, y
+        else:
+            return None
+
+    def popleft(
+        self,
+    ) -> typing.Union[typing.Tuple[np.ndarray, base.typing.Target], None]:
+        """Remove and return the oldest element in the buffer. """
+        if self.size > 0:
+            x, y = self._X[self._oldest], self._y[self._oldest]
+            self._imask[self._oldest] = False  # Mark slot as free
+            self._oldest = (
+                self._oldest + 1 if self._oldest < self.window_size - 1 else 0
+            )
+            if self._oldest == self._next_insert:
+                # Shift circular buffer and make its starting point be the index 0
+                self._oldest = self._next_insert = 0
+            self._size -= 1
+
+            return x, y
+
+    def clear(self) -> "KNeighborsBuffer":
+        """Clear all stored elements."""
+        self._next_insert = 0
+        self._oldest = 0
+        self._size = 0
+        # Just reset the instance filtering mask, not the buffers
+        self._imask = np.zeros(self.window_size, dtype=bool)
+
+        return self
+
+    @property
+    def features_buffer(self) -> np.ndarray:
+        """Get the features buffer.
+
+        The shape of the buffer is (window_size, n_features).
+        """
+        return self._X[self._imask]  # Only return the actually filled instances
+
+    @property
+    def targets_buffer(self) -> typing.List:
+        """Get the targets buffer
+
+        The shape of the buffer is (window_size, n_targets).
+        """
+        return list(itertools.compress(self._y, self._imask))
+
+    @property
+    def n_targets(self) -> int:
+        """Get the number of targets. """
+        return self._n_targets
+
+    @property
+    def n_features(self) -> int:
+        """Get the number of features. """
+        return self._n_features
+
+    @property
+    def size(self) -> int:
+        """Get the window size. """
+        return self._size
+
+
+class BaseNeighbors:
+    """Base class for neighbors-based estimators. """
+
+    def __init__(
+        self,
+        n_neighbors: int = 5,
+        window_size: int = 1000,
+        leaf_size: int = 30,
+        p: float = 2,
+        **kwargs
+    ):
+        self.n_neighbors = n_neighbors
+        self.window_size = window_size
+        self.leaf_size = leaf_size
+        self._kwargs = kwargs
+
+        if p < 1:
+            raise ValueError(
+                "Invalid Minkowski p-norm value: {}.\n"
+                "Values must be greater than or equal to 1".format(p)
+            )
+        self.p = p
+        self.data_window = KNeighborsBuffer(window_size=window_size)
+
+    def _get_neighbors(self, x):
+        X = self.data_window.features_buffer
+        tree = cKDTree(X, leafsize=self.leaf_size, **self._kwargs)
+        dist, idx = tree.query(x.reshape(1, -1), k=self.n_neighbors, p=self.p)
+
+        # We make sure dist and idx is 2D since when k = 1 dist is one dimensional.
+        if not isinstance(dist[0], np.ndarray):
+            dist = [dist]
+            idx = [idx]
+        return dist, idx
+
+    def reset(self) -> "BaseNeighbors":
+        """Reset estimator. """
+        self.data_window.reset()
+
+        return self
```

### Comparing `river-0.8.0/river/neighbors/knn_adwin.py` & `river-0.9.0/river/neighbors/knn_adwin.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,102 +1,102 @@
-from river.drift import ADWIN
-from river.utils import dict2numpy
-
-from .knn_classifier import KNNClassifier
-
-
-class KNNADWINClassifier(KNNClassifier):
-    """K-Nearest Neighbors classifier with ADWIN change detector.
-
-    This classifier is an improvement from the regular kNN method,
-    as it is resistant to concept drift. It uses the `ADWIN` change
-    detector to decide which samples to keep and which ones to forget,
-    and by doing so it regulates the sample window size.
-
-    Parameters
-    ----------
-    n_neighbors
-        The number of nearest neighbors to search for.
-    window_size
-        The maximum size of the window storing the last viewed samples.
-    leaf_size
-        The maximum number of samples that can be stored in one leaf node,
-        which determines from which point the algorithm will switch for a
-        brute-force approach. The bigger this number the faster the tree
-        construction time, but the slower the query time will be.
-    p
-        p-norm value for the Minkowski metric. When `p=1`, this corresponds to the
-        Manhattan distance, while `p=2` corresponds to the Euclidean distance. Valid
-        values are in the interval $[1, +\\infty)$
-
-    Notes
-    -----
-    - This estimator is not optimal for a mixture of categorical and numerical
-    features. This implementation treats all features from a given stream as
-    numerical.
-    - This implementation is extended from the KNNClassifier, with the main
-    difference that it keeps a dynamic window whose size changes in agreement
-    with the amount of change detected by the ADWIN drift detector.
-
-    Examples
-    --------
-    >>> from river import synth
-    >>> from river import evaluate
-    >>> from river import metrics
-    >>> from river import neighbors
-
-    >>> dataset = synth.ConceptDriftStream(position=500, width=20, seed=1).take(1000)
-
-    >>> model = neighbors.KNNADWINClassifier(window_size=100)
-
-    >>> metric = metrics.Accuracy()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    Accuracy: 56.66%
-
-    """
-
-    def __init__(self, n_neighbors=5, window_size=1000, leaf_size=30, p=2):
-        super().__init__(
-            n_neighbors=n_neighbors, window_size=window_size, leaf_size=leaf_size, p=p
-        )
-        self.adwin = ADWIN()
-
-    def _unit_test_skips(self):
-        return {"check_emerging_features", "check_disappearing_features"}
-
-    def learn_one(self, x, y):
-        """Update the model with a set of features `x` and a label `y`.
-
-        Parameters
-        ----------
-        x
-            A dictionary of features.
-        y
-            The class label.
-
-        Returns
-        -------
-            self
-
-        Notes
-        -----
-        For the K-Nearest Neighbors Classifier, fitting the model is the
-        equivalent of inserting the newer samples in the observed window,
-        and if the size_limit is reached, removing older results.
-
-        """
-        self.classes_.add(y)
-
-        self.data_window.append(dict2numpy(x), y)
-        if self.data_window.size >= self.n_neighbors:
-            correctly_classifies = int(self.predict_one(x) == y)
-            self.adwin.update(correctly_classifies)
-        else:
-            self.adwin.update(0)
-
-        if self.data_window.size >= self.n_neighbors:
-            if self.adwin.change_detected:
-                if self.adwin.width < self.data_window.size:
-                    for i in range(self.data_window.size, self.adwin.width, -1):
-                        self.data_window.popleft()
-        return self
+from river.drift import ADWIN
+from river.utils import dict2numpy
+
+from .knn_classifier import KNNClassifier
+
+
+class KNNADWINClassifier(KNNClassifier):
+    """K-Nearest Neighbors classifier with ADWIN change detector.
+
+    This classifier is an improvement from the regular kNN method,
+    as it is resistant to concept drift. It uses the `ADWIN` change
+    detector to decide which samples to keep and which ones to forget,
+    and by doing so it regulates the sample window size.
+
+    Parameters
+    ----------
+    n_neighbors
+        The number of nearest neighbors to search for.
+    window_size
+        The maximum size of the window storing the last viewed samples.
+    leaf_size
+        The maximum number of samples that can be stored in one leaf node,
+        which determines from which point the algorithm will switch for a
+        brute-force approach. The bigger this number the faster the tree
+        construction time, but the slower the query time will be.
+    p
+        p-norm value for the Minkowski metric. When `p=1`, this corresponds to the
+        Manhattan distance, while `p=2` corresponds to the Euclidean distance. Valid
+        values are in the interval $[1, +\\infty)$
+
+    Notes
+    -----
+    - This estimator is not optimal for a mixture of categorical and numerical
+    features. This implementation treats all features from a given stream as
+    numerical.
+    - This implementation is extended from the KNNClassifier, with the main
+    difference that it keeps a dynamic window whose size changes in agreement
+    with the amount of change detected by the ADWIN drift detector.
+
+    Examples
+    --------
+    >>> from river import synth
+    >>> from river import evaluate
+    >>> from river import metrics
+    >>> from river import neighbors
+
+    >>> dataset = synth.ConceptDriftStream(position=500, width=20, seed=1).take(1000)
+
+    >>> model = neighbors.KNNADWINClassifier(window_size=100)
+
+    >>> metric = metrics.Accuracy()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    Accuracy: 56.66%
+
+    """
+
+    def __init__(self, n_neighbors=5, window_size=1000, leaf_size=30, p=2):
+        super().__init__(
+            n_neighbors=n_neighbors, window_size=window_size, leaf_size=leaf_size, p=p
+        )
+        self.adwin = ADWIN()
+
+    def _unit_test_skips(self):
+        return {"check_emerging_features", "check_disappearing_features"}
+
+    def learn_one(self, x, y):
+        """Update the model with a set of features `x` and a label `y`.
+
+        Parameters
+        ----------
+        x
+            A dictionary of features.
+        y
+            The class label.
+
+        Returns
+        -------
+            self
+
+        Notes
+        -----
+        For the K-Nearest Neighbors Classifier, fitting the model is the
+        equivalent of inserting the newer samples in the observed window,
+        and if the size_limit is reached, removing older results.
+
+        """
+        self.classes_.add(y)
+
+        self.data_window.append(dict2numpy(x), y)
+        if self.data_window.size >= self.n_neighbors:
+            correctly_classifies = int(self.predict_one(x) == y)
+            self.adwin.update(correctly_classifies)
+        else:
+            self.adwin.update(0)
+
+        if self.data_window.size >= self.n_neighbors:
+            if self.adwin.change_detected:
+                if self.adwin.width < self.data_window.size:
+                    for i in range(self.data_window.size, self.adwin.width, -1):
+                        self.data_window.popleft()
+        return self
```

### Comparing `river-0.8.0/river/neighbors/knn_classifier.py` & `river-0.9.0/river/neighbors/knn_classifier.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,176 +1,176 @@
-import typing
-
-from river import base
-from river.utils import dict2numpy
-from river.utils.math import softmax
-
-from .base_neighbors import BaseNeighbors
-
-
-class KNNClassifier(BaseNeighbors, base.Classifier):
-    """k-Nearest Neighbors classifier.
-
-    This non-parametric classification method keeps track of the last
-    `window_size` training samples. The predicted class-label for a
-    given query sample is obtained in two steps:
-
-    1. Find the closest `n_neighbors` to the query sample in the data window.
-    2. Aggregate the class-labels of the `n_neighbors` to define the predicted
-       class for the query sample.
-
-    Parameters
-    ----------
-    n_neighbors
-        The number of nearest neighbors to search for.
-    window_size
-        The maximum size of the window storing the last observed samples.
-    leaf_size
-        scipy.spatial.cKDTree parameter. The maximum number of samples that can be
-        stored in one leaf node, which determines from which point the algorithm will
-        switch for a brute-force approach. The bigger this number the faster the
-        tree construction time, but the slower the query time will be.
-    p
-        p-norm value for the Minkowski metric. When `p=1`, this corresponds to the
-        Manhattan distance, while `p=2` corresponds to the Euclidean distance. Valid
-        values are in the interval $[1, +\\infty)$
-    weighted
-        Whether to weight the contribution of each neighbor by it's inverse
-        distance or not.
-    kwargs
-        Other parameters passed to `scipy.spatial.cKDTree`.
-
-    Notes
-    -----
-    This estimator is not optimal for a mixture of categorical and numerical
-    features. This implementation treats all features from a given stream as
-    numerical.
-
-    Examples
-    --------
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import metrics
-    >>> from river import neighbors
-    >>> from river import preprocessing
-
-    >>> dataset = datasets.Phishing()
-
-    >>> model = (
-    ...     preprocessing.StandardScaler() |
-    ...     neighbors.KNNClassifier()
-    ... )
-
-    >>> metric = metrics.Accuracy()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    Accuracy: 88.07%
-
-    """
-
-    def __init__(
-        self,
-        n_neighbors: int = 5,
-        window_size: int = 1000,
-        leaf_size: int = 30,
-        p: float = 2,
-        weighted: bool = True,
-        **kwargs
-    ):
-        super().__init__(
-            n_neighbors=n_neighbors,
-            window_size=window_size,
-            leaf_size=leaf_size,
-            p=p,
-            **kwargs
-        )
-        self.weighted = weighted
-        self.classes_: typing.Set = set()
-        self.kwargs = kwargs
-
-    def _unit_test_skips(self):
-        return {"check_emerging_features", "check_disappearing_features"}
-
-    def learn_one(self, x, y):
-        """Update the model with a set of features `x` and a label `y`.
-
-        Parameters
-        ----------
-        x
-            A dictionary of features.
-        y
-            The class label.
-
-        Returns
-        -------
-            self
-
-        Notes
-        -----
-        For the K-Nearest Neighbors Classifier, fitting the model is the
-        equivalent of inserting the newer samples in the observed window,
-        and if the size_limit is reached, removing older results.
-
-        """
-
-        self.classes_.add(y)
-        x_arr = dict2numpy(x)
-
-        self.data_window.append(x_arr, y)
-
-        return self
-
-    def predict_proba_one(self, x):
-        """Predict the probability of each label for a dictionary of features `x`.
-
-        Parameters
-        ----------
-        x
-            A dictionary of features.
-
-        Returns
-        -------
-        proba
-            A dictionary which associates a probability which each label.
-
-        """
-
-        proba = {class_idx: 0.0 for class_idx in self.classes_}
-        if self.data_window.size == 0:
-            # The model is empty, default to None
-            return proba
-
-        x_arr = dict2numpy(x)
-
-        dists, neighbor_idx = self._get_neighbors(x_arr)
-        target_buffer = self.data_window.targets_buffer
-
-        # If the closest neighbor has a distance of 0, then return it's output
-        if dists[0][0] == 0:
-            proba[target_buffer[neighbor_idx[0][0]]] = 1.0
-            return proba
-
-        if self.data_window.size < self.n_neighbors:  # Select only the valid neighbors
-            neighbor_idx = [
-                index
-                for cnt, index in enumerate(neighbor_idx[0])
-                if cnt < self.data_window.size
-            ]
-            dists = [
-                dist for cnt, dist in enumerate(dists[0]) if cnt < self.data_window.size
-            ]
-        else:
-            neighbor_idx = neighbor_idx[0]
-            dists = dists[0]
-
-        if not self.weighted:  # Uniform weights
-            for index in neighbor_idx:
-                proba[target_buffer[index]] += 1.0
-        else:  # Use the inverse of the distance to weight the votes
-            for d, index in zip(dists, neighbor_idx):
-                proba[target_buffer[index]] += 1.0 / d
-
-        return softmax(proba)
-
-    @property
-    def _multiclass(self):
-        return True
+import typing
+
+from river import base
+from river.utils import dict2numpy
+from river.utils.math import softmax
+
+from .base_neighbors import BaseNeighbors
+
+
+class KNNClassifier(BaseNeighbors, base.Classifier):
+    """k-Nearest Neighbors classifier.
+
+    This non-parametric classification method keeps track of the last
+    `window_size` training samples. The predicted class-label for a
+    given query sample is obtained in two steps:
+
+    1. Find the closest `n_neighbors` to the query sample in the data window.
+    2. Aggregate the class-labels of the `n_neighbors` to define the predicted
+       class for the query sample.
+
+    Parameters
+    ----------
+    n_neighbors
+        The number of nearest neighbors to search for.
+    window_size
+        The maximum size of the window storing the last observed samples.
+    leaf_size
+        scipy.spatial.cKDTree parameter. The maximum number of samples that can be
+        stored in one leaf node, which determines from which point the algorithm will
+        switch for a brute-force approach. The bigger this number the faster the
+        tree construction time, but the slower the query time will be.
+    p
+        p-norm value for the Minkowski metric. When `p=1`, this corresponds to the
+        Manhattan distance, while `p=2` corresponds to the Euclidean distance. Valid
+        values are in the interval $[1, +\\infty)$
+    weighted
+        Whether to weight the contribution of each neighbor by it's inverse
+        distance or not.
+    kwargs
+        Other parameters passed to `scipy.spatial.cKDTree`.
+
+    Notes
+    -----
+    This estimator is not optimal for a mixture of categorical and numerical
+    features. This implementation treats all features from a given stream as
+    numerical.
+
+    Examples
+    --------
+    >>> from river import datasets
+    >>> from river import evaluate
+    >>> from river import metrics
+    >>> from river import neighbors
+    >>> from river import preprocessing
+
+    >>> dataset = datasets.Phishing()
+
+    >>> model = (
+    ...     preprocessing.StandardScaler() |
+    ...     neighbors.KNNClassifier()
+    ... )
+
+    >>> metric = metrics.Accuracy()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    Accuracy: 88.07%
+
+    """
+
+    def __init__(
+        self,
+        n_neighbors: int = 5,
+        window_size: int = 1000,
+        leaf_size: int = 30,
+        p: float = 2,
+        weighted: bool = True,
+        **kwargs
+    ):
+        super().__init__(
+            n_neighbors=n_neighbors,
+            window_size=window_size,
+            leaf_size=leaf_size,
+            p=p,
+            **kwargs
+        )
+        self.weighted = weighted
+        self.classes_: typing.Set = set()
+        self.kwargs = kwargs
+
+    def _unit_test_skips(self):
+        return {"check_emerging_features", "check_disappearing_features"}
+
+    def learn_one(self, x, y):
+        """Update the model with a set of features `x` and a label `y`.
+
+        Parameters
+        ----------
+        x
+            A dictionary of features.
+        y
+            The class label.
+
+        Returns
+        -------
+            self
+
+        Notes
+        -----
+        For the K-Nearest Neighbors Classifier, fitting the model is the
+        equivalent of inserting the newer samples in the observed window,
+        and if the size_limit is reached, removing older results.
+
+        """
+
+        self.classes_.add(y)
+        x_arr = dict2numpy(x)
+
+        self.data_window.append(x_arr, y)
+
+        return self
+
+    def predict_proba_one(self, x):
+        """Predict the probability of each label for a dictionary of features `x`.
+
+        Parameters
+        ----------
+        x
+            A dictionary of features.
+
+        Returns
+        -------
+        proba
+            A dictionary which associates a probability which each label.
+
+        """
+
+        proba = {class_idx: 0.0 for class_idx in self.classes_}
+        if self.data_window.size == 0:
+            # The model is empty, default to None
+            return proba
+
+        x_arr = dict2numpy(x)
+
+        dists, neighbor_idx = self._get_neighbors(x_arr)
+        target_buffer = self.data_window.targets_buffer
+
+        # If the closest neighbor has a distance of 0, then return it's output
+        if dists[0][0] == 0:
+            proba[target_buffer[neighbor_idx[0][0]]] = 1.0
+            return proba
+
+        if self.data_window.size < self.n_neighbors:  # Select only the valid neighbors
+            neighbor_idx = [
+                index
+                for cnt, index in enumerate(neighbor_idx[0])
+                if cnt < self.data_window.size
+            ]
+            dists = [
+                dist for cnt, dist in enumerate(dists[0]) if cnt < self.data_window.size
+            ]
+        else:
+            neighbor_idx = neighbor_idx[0]
+            dists = dists[0]
+
+        if not self.weighted:  # Uniform weights
+            for index in neighbor_idx:
+                proba[target_buffer[index]] += 1.0
+        else:  # Use the inverse of the distance to weight the votes
+            for d, index in zip(dists, neighbor_idx):
+                proba[target_buffer[index]] += 1.0 / d
+
+        return softmax(proba)
+
+    @property
+    def _multiclass(self):
+        return True
```

### Comparing `river-0.8.0/river/neighbors/knn_regressor.py` & `river-0.9.0/river/neighbors/knn_regressor.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,178 +1,178 @@
-import numpy as np
-
-from river import base
-from river.utils import dict2numpy
-
-from .base_neighbors import BaseNeighbors
-
-
-class KNNRegressor(BaseNeighbors, base.Regressor):
-    """k-Nearest Neighbors regressor.
-
-    This non-parametric regression method keeps track of the last
-    `window_size` training samples. Predictions are obtained by
-    aggregating the values of the closest n_neighbors stored-samples with
-    respect to a query sample.
-
-    Parameters
-    ----------
-    n_neighbors
-        The number of nearest neighbors to search for.
-    window_size
-        The maximum size of the window storing the last observed samples.
-    leaf_size
-        scipy.spatial.cKDTree parameter. The maximum number of samples that can
-        be stored in one leaf node, which determines from which point the algorithm
-        will switch for a brute-force approach. The bigger this number the faster
-        the tree construction time, but the slower the query time will be.
-    p
-        p-norm value for the Minkowski metric. When `p=1`, this corresponds to the
-        Manhattan distance, while `p=2` corresponds to the Euclidean distance.
-        Valid values are in the interval $[1, +\\infty)$
-    aggregation_method
-        The method to aggregate the target values of neighbors.
-            | 'mean'
-            | 'median'
-            | 'weighted_mean'
-    kwargs
-        Other parameters passed to scipy.spatial.cKDTree.
-
-    Notes
-    -----
-    This estimator is not optimal for a mixture of categorical and numerical
-    features. This implementation treats all features from a given stream as
-    numerical.
-
-    Examples
-    --------
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import metrics
-    >>> from river import neighbors
-    >>> from river import preprocessing
-
-    >>> dataset = datasets.TrumpApproval()
-
-    >>> model = (
-    ...     preprocessing.StandardScaler() |
-    ...     neighbors.KNNRegressor(window_size=50)
-    ... )
-
-    >>> metric = metrics.MAE()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    MAE: 0.441308
-
-    """
-
-    _MEAN = "mean"
-    _MEDIAN = "median"
-    _WEIGHTED_MEAN = "weighted_mean"
-
-    def __init__(
-        self,
-        n_neighbors: int = 5,
-        window_size: int = 1000,
-        leaf_size: int = 30,
-        p: float = 2,
-        aggregation_method: str = "mean",
-        **kwargs
-    ):
-
-        super().__init__(
-            n_neighbors=n_neighbors,
-            window_size=window_size,
-            leaf_size=leaf_size,
-            p=p,
-            **kwargs
-        )
-        if aggregation_method not in {self._MEAN, self._MEDIAN, self._WEIGHTED_MEAN}:
-            raise ValueError(
-                "Invalid aggregation_method: {}.\n"
-                "Valid options are: {}".format(
-                    aggregation_method, {self._MEAN, self._MEDIAN, self._WEIGHTED_MEAN}
-                )
-            )
-        self.aggregation_method = aggregation_method
-        self.kwargs = kwargs
-
-    def _unit_test_skips(self):
-        return {"check_emerging_features", "check_disappearing_features"}
-
-    def learn_one(self, x, y):
-        """Update the model with a set of features `x` and a real target value `y`.
-
-        Parameters
-        ----------
-        x
-            A dictionary of features.
-        y
-            A numeric target.
-
-        Returns
-        -------
-            self
-
-        Notes
-        -----
-        For the K-Nearest Neighbors regressor, fitting the model is the
-        equivalent of inserting the newer samples in the observed window,
-        and if the `window_size` is reached, removing older results.
-
-        """
-
-        x_arr = dict2numpy(x)
-        self.data_window.append(x_arr, y)
-
-        return self
-
-    def predict_one(self, x):
-        """Predict the target value of a set of features `x`.
-
-        Search the KDTree for the `n_neighbors` nearest neighbors.
-
-        Parameters
-        ----------
-        x
-            A dictionary of features.
-
-        Returns
-        -------
-            The prediction.
-
-        """
-
-        if self.data_window.size == 0:
-            # Not enough information available, return default prediction
-            return 0.0
-
-        x_arr = dict2numpy(x)
-
-        dists, neighbor_idx = self._get_neighbors(x_arr)
-        target_buffer = self.data_window.targets_buffer
-
-        # If the closest neighbor has a distance of 0, then return it's output
-        if dists[0][0] == 0:
-            return target_buffer[neighbor_idx[0][0]]
-
-        if self.data_window.size < self.n_neighbors:  # Select only the valid neighbors
-            neighbor_vals = [
-                target_buffer[index]
-                for cnt, index in enumerate(neighbor_idx[0])
-                if cnt < self.data_window.size
-            ]
-            dists = [
-                dist for cnt, dist in enumerate(dists[0]) if cnt < self.data_window.size
-            ]
-        else:
-            neighbor_vals = [target_buffer[index] for index in neighbor_idx[0]]
-            dists = dists[0]
-
-        if self.aggregation_method == self._MEAN:
-            return np.mean(neighbor_vals)
-        elif self.aggregation_method == self._MEDIAN:
-            return np.median(neighbor_vals)
-        else:  # weighted mean
-            return sum(y / d for y, d in zip(neighbor_vals, dists)) / sum(
-                1 / d for d in dists
-            )
+import numpy as np
+
+from river import base
+from river.utils import dict2numpy
+
+from .base_neighbors import BaseNeighbors
+
+
+class KNNRegressor(BaseNeighbors, base.Regressor):
+    """k-Nearest Neighbors regressor.
+
+    This non-parametric regression method keeps track of the last
+    `window_size` training samples. Predictions are obtained by
+    aggregating the values of the closest n_neighbors stored-samples with
+    respect to a query sample.
+
+    Parameters
+    ----------
+    n_neighbors
+        The number of nearest neighbors to search for.
+    window_size
+        The maximum size of the window storing the last observed samples.
+    leaf_size
+        scipy.spatial.cKDTree parameter. The maximum number of samples that can
+        be stored in one leaf node, which determines from which point the algorithm
+        will switch for a brute-force approach. The bigger this number the faster
+        the tree construction time, but the slower the query time will be.
+    p
+        p-norm value for the Minkowski metric. When `p=1`, this corresponds to the
+        Manhattan distance, while `p=2` corresponds to the Euclidean distance.
+        Valid values are in the interval $[1, +\\infty)$
+    aggregation_method
+        The method to aggregate the target values of neighbors.
+            | 'mean'
+            | 'median'
+            | 'weighted_mean'
+    kwargs
+        Other parameters passed to scipy.spatial.cKDTree.
+
+    Notes
+    -----
+    This estimator is not optimal for a mixture of categorical and numerical
+    features. This implementation treats all features from a given stream as
+    numerical.
+
+    Examples
+    --------
+    >>> from river import datasets
+    >>> from river import evaluate
+    >>> from river import metrics
+    >>> from river import neighbors
+    >>> from river import preprocessing
+
+    >>> dataset = datasets.TrumpApproval()
+
+    >>> model = (
+    ...     preprocessing.StandardScaler() |
+    ...     neighbors.KNNRegressor(window_size=50)
+    ... )
+
+    >>> metric = metrics.MAE()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    MAE: 0.441308
+
+    """
+
+    _MEAN = "mean"
+    _MEDIAN = "median"
+    _WEIGHTED_MEAN = "weighted_mean"
+
+    def __init__(
+        self,
+        n_neighbors: int = 5,
+        window_size: int = 1000,
+        leaf_size: int = 30,
+        p: float = 2,
+        aggregation_method: str = "mean",
+        **kwargs
+    ):
+
+        super().__init__(
+            n_neighbors=n_neighbors,
+            window_size=window_size,
+            leaf_size=leaf_size,
+            p=p,
+            **kwargs
+        )
+        if aggregation_method not in {self._MEAN, self._MEDIAN, self._WEIGHTED_MEAN}:
+            raise ValueError(
+                "Invalid aggregation_method: {}.\n"
+                "Valid options are: {}".format(
+                    aggregation_method, {self._MEAN, self._MEDIAN, self._WEIGHTED_MEAN}
+                )
+            )
+        self.aggregation_method = aggregation_method
+        self.kwargs = kwargs
+
+    def _unit_test_skips(self):
+        return {"check_emerging_features", "check_disappearing_features"}
+
+    def learn_one(self, x, y):
+        """Update the model with a set of features `x` and a real target value `y`.
+
+        Parameters
+        ----------
+        x
+            A dictionary of features.
+        y
+            A numeric target.
+
+        Returns
+        -------
+            self
+
+        Notes
+        -----
+        For the K-Nearest Neighbors regressor, fitting the model is the
+        equivalent of inserting the newer samples in the observed window,
+        and if the `window_size` is reached, removing older results.
+
+        """
+
+        x_arr = dict2numpy(x)
+        self.data_window.append(x_arr, y)
+
+        return self
+
+    def predict_one(self, x):
+        """Predict the target value of a set of features `x`.
+
+        Search the KDTree for the `n_neighbors` nearest neighbors.
+
+        Parameters
+        ----------
+        x
+            A dictionary of features.
+
+        Returns
+        -------
+            The prediction.
+
+        """
+
+        if self.data_window.size == 0:
+            # Not enough information available, return default prediction
+            return 0.0
+
+        x_arr = dict2numpy(x)
+
+        dists, neighbor_idx = self._get_neighbors(x_arr)
+        target_buffer = self.data_window.targets_buffer
+
+        # If the closest neighbor has a distance of 0, then return it's output
+        if dists[0][0] == 0:
+            return target_buffer[neighbor_idx[0][0]]
+
+        if self.data_window.size < self.n_neighbors:  # Select only the valid neighbors
+            neighbor_vals = [
+                target_buffer[index]
+                for cnt, index in enumerate(neighbor_idx[0])
+                if cnt < self.data_window.size
+            ]
+            dists = [
+                dist for cnt, dist in enumerate(dists[0]) if cnt < self.data_window.size
+            ]
+        else:
+            neighbor_vals = [target_buffer[index] for index in neighbor_idx[0]]
+            dists = dists[0]
+
+        if self.aggregation_method == self._MEAN:
+            return np.mean(neighbor_vals)
+        elif self.aggregation_method == self._MEDIAN:
+            return np.median(neighbor_vals)
+        else:  # weighted mean
+            return sum(y / d for y, d in zip(neighbor_vals, dists)) / sum(
+                1 / d for d in dists
+            )
```

### Comparing `river-0.8.0/river/neighbors/sam_knn.py` & `river-0.9.0/river/neighbors/sam_knn.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,809 +1,809 @@
-import copy as cp
-import logging
-from collections import deque
-
-import numpy as np
-
-from river.base import Classifier
-from river.utils import dict2numpy
-
-from . import libNearestNeighbor
-
-
-class SAMKNNClassifier(Classifier):
-    """Self Adjusting Memory coupled with the kNN classifier.
-
-    The Self Adjusting Memory (SAM) [^1] model builds an ensemble with models targeting current
-    or former concepts. SAM is built using two memories: STM for the current concept, and
-    the LTM to retain information about past concepts. A cleaning process is in charge of
-    controlling the size of the STM while keeping the information in the LTM consistent
-    with the STM.
-
-    Parameters
-    ----------
-    n_neighbors
-        number of evaluated nearest neighbors.
-    distance_weighting
-        Type of weighting of the nearest neighbors. It `True`  will use 'distance'.
-        Otherwise, will use 'uniform' (majority voting).
-    window_size
-         Maximum number of overall stored data points.
-    ltm_size
-        Proportion of the overall instances that may be used for the LTM. This is
-        only relevant when the maximum number(maxSize) of stored instances is reached.
-    stm_aprox_adaption
-        Type of STM size adaption.<br/>
-            - If `True` approximates the interleaved test-train error and is
-               significantly faster than the exact version.<br/>
-            - If `False` calculates the interleaved test-train error exactly for each of the
-              evaluated window sizes, which often has to be recalculated from the scratch.<br/>
-            - If `None`, the STM is not  adapted at all. If additionally `use_ltm=False`, then
-              this algorithm is simply a kNN with fixed sliding window size.
-    min_stm_size
-        Minimum STM size which is evaluated during the STM size adaption.
-    use_ltm
-        Specifies whether the LTM should be used at all.
-
-    Examples
-    --------
-    >>> from river import synth
-    >>> from river import evaluate
-    >>> from river import metrics
-    >>> from river import neighbors
-
-    >>> dataset = synth.ConceptDriftStream(position=500, width=20, seed=1).take(1000)
-
-    >>> model = neighbors.SAMKNNClassifier(window_size=100)
-
-    >>> metric = metrics.Accuracy()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)  # doctest: +SKIP
-    Accuracy: 56.70%
-
-    Notes
-    -----
-    This modules uses libNearestNeighbor, a C++ library used to speed up some of
-    the algorithm's computations. When invoking the library's functions it's important
-    to pass the right argument type. Although most of this framework's functionality
-    will work with python standard types, the C++ library will work with 8-bit labels,
-    which is already done by the SAMKNN class, but may be absent in custom classes that
-    use SAMKNN static methods, or other custom functions that use the C++ library.
-
-    References
-    ----------
-    [^1]: Losing, Viktor, Barbara Hammer, and Heiko Wersing.
-          "Knn classifier with self adjusting memory for heterogeneous concept drift."
-          In Data Mining (ICDM), 2016 IEEE 16th International Conference on,
-          pp. 291-300. IEEE, 2016.
-
-    """
-
-    def __init__(
-        self,
-        n_neighbors: int = 5,
-        distance_weighting=True,
-        window_size: int = 5000,
-        ltm_size: float = 0.4,
-        min_stm_size: int = 50,
-        stm_aprox_adaption=True,
-        use_ltm=True,
-    ):
-        super().__init__()
-        self.n_neighbors = n_neighbors
-        self.distance_weighting = distance_weighting
-        self.window_size = window_size
-        self.ltm_size = ltm_size
-        self.min_stm_size = min_stm_size
-        self.use_ltm = use_ltm
-
-        self._stm_samples = None
-        self._stm_labels = np.empty(shape=0, dtype=np.int32)
-        self._ltm_samples = None
-        self._ltm_labels = np.empty(shape=0, dtype=np.int32)
-        self.max_ltm_size = self.ltm_size * self.window_size
-        self.max_stm_size = self.window_size - self.max_ltm_size
-        self.min_stm_size = self.min_stm_size
-        self.stm_aprox_adaption = stm_aprox_adaption
-
-        self.stm_distances = np.zeros(shape=(window_size + 1, window_size + 1))
-        if self.distance_weighting:
-            self.get_labels_fct = SAMKNNClassifier._get_distance_weighted_label
-        else:
-            self.get_labels_fct = SAMKNNClassifier._get_maj_label
-        if self.use_ltm:
-            self.predict_fct = self._predict_by_all_memories
-            self.size_check_fct = self._size_check_stmltm
-        else:
-            self.predict_fct = self._predict_by_stm
-            self.size_check_fct = self._size_check_fade_out
-
-        self.interleaved_pred_histories = {}
-        self.ltm_pred_history = deque([])
-        self.stm_pred_history = deque([])
-        self.cmp_pred_history = deque([])
-
-        self.train_step_count = 0
-        self.stm_sizes = []
-        self.ltm_sizes = []
-        self.n_stm_correct = 0
-        self.n_ltm_correct = 0
-        self.n_cm_correct = 0
-        self.n_possible_correct_predictions = 0
-        self.n_correct_predictions = 0
-        self.classifier_choice = []
-        self.pred_history = []
-
-    def _unit_test_skips(self):
-        return {"check_emerging_features", "check_disappearing_features"}
-
-    @staticmethod
-    def _get_distances(sample, samples):
-        """Calculate distances from sample to all samples."""
-        return libNearestNeighbor.get1ToNDistances(sample, samples)
-
-    def _cluster_down(self, samples, labels):
-        """
-        Performs class-wise kMeans++ clustering for given samples with corresponding labels.
-        The number of samples is halved per class.
-        """
-        from sklearn.cluster import KMeans
-
-        logging.debug("cluster Down %d" % self.train_step_count)
-        uniqueLabels = np.unique(labels)
-        newSamples = np.empty(shape=(0, samples.shape[1]))
-        newLabels = np.empty(shape=0, dtype=np.int32)
-        for label in uniqueLabels:
-            tmpSamples = samples[labels == label]
-            newLength = int(max(tmpSamples.shape[0] / 2, 1))
-            clustering = KMeans(n_clusters=newLength, n_init=1, random_state=0)
-            clustering.fit(tmpSamples)
-            newSamples = np.vstack([newSamples, clustering.cluster_centers_])
-            newLabels = np.append(
-                newLabels, label * np.ones(shape=newLength, dtype=np.int32)
-            )
-        return newSamples, newLabels
-
-    def _size_check_fade_out(self):
-        """
-        Makes sure that the STM does not surpass the maximum size.
-        Only used when use_ltm=False.
-        """
-        STMShortened = False
-        if len(self._stm_labels) > self.max_stm_size + self.max_ltm_size:
-            STMShortened = True
-            self._stm_samples = np.delete(self._stm_samples, 0, 0)
-            self._stm_labels = np.delete(self._stm_labels, 0, 0)
-            self.stm_distances[
-                : len(self._stm_labels), : len(self._stm_labels)
-            ] = self.stm_distances[
-                1 : len(self._stm_labels) + 1, 1 : len(self._stm_labels) + 1
-            ]
-
-            if self.stm_aprox_adaption:
-                key_set = list(self.interleaved_pred_histories.keys())
-                # if self.interleaved_pred_histories.has_key(0):
-                if 0 in key_set:
-                    self.interleaved_pred_histories[0].pop(0)
-                updated_histories = cp.deepcopy(self.interleaved_pred_histories)
-                for key in self.interleaved_pred_histories.keys():
-                    if key > 0:
-                        if key == 1:
-                            updated_histories.pop(0, None)
-                        tmp = updated_histories[key]
-                        updated_histories.pop(key, None)
-                        updated_histories[key - 1] = tmp
-                self.interleaved_pred_histories = updated_histories
-            else:
-                self.interleaved_pred_histories = {}
-        return STMShortened
-
-    def _size_check_stmltm(self):
-        """
-        Makes sure that the STM and LTM combined doe not surpass the maximum size.
-        Only used when use_ltm=True.
-        """
-        stm_shortened = False
-        if (
-            len(self._stm_labels) + len(self._ltm_labels)
-            > self.max_stm_size + self.max_ltm_size
-        ):
-            if len(self._ltm_labels) > self.max_ltm_size:
-                self._ltm_samples, self._ltm_labels = self._cluster_down(
-                    self._ltm_samples, self._ltm_labels
-                )
-            else:
-                if (
-                    len(self._stm_labels) + len(self._ltm_labels)
-                    > self.max_stm_size + self.max_ltm_size
-                ):
-                    stm_shortened = True
-                    n_shifts = int(self.max_ltm_size - len(self._ltm_labels) + 1)
-                    shift_range = range(n_shifts)
-                    self._ltm_samples = np.vstack(
-                        [self._ltm_samples, self._stm_samples[:n_shifts, :]]
-                    )
-                    self._ltm_labels = np.append(
-                        self._ltm_labels, self._stm_labels[:n_shifts]
-                    )
-                    self._ltm_samples, self._ltm_labels = self._cluster_down(
-                        self._ltm_samples, self._ltm_labels
-                    )
-                    self._stm_samples = np.delete(self._stm_samples, shift_range, 0)
-                    self._stm_labels = np.delete(self._stm_labels, shift_range, 0)
-                    self.stm_distances[
-                        : len(self._stm_labels), : len(self._stm_labels)
-                    ] = self.stm_distances[
-                        n_shifts : len(self._stm_labels) + n_shifts,
-                        n_shifts : len(self._stm_labels) + n_shifts,
-                    ]
-                    for _ in shift_range:
-                        self.ltm_pred_history.popleft()
-                        self.stm_pred_history.popleft()
-                        self.cmp_pred_history.popleft()
-                    self.interleaved_pred_histories = {}
-        return stm_shortened
-
-    def _clean_samples(self, samples_cl, labels_cl, only_last=False):
-        """
-        Removes distance-based all instances from the input samples that
-        contradict those in the STM.
-        """
-        if len(self._stm_labels) > self.n_neighbors and samples_cl.shape[0] > 0:
-            if only_last:
-                loop_range = [len(self._stm_labels) - 1]
-            else:
-                loop_range = range(len(self._stm_labels))
-            for i in loop_range:
-                if len(labels_cl) == 0:
-                    break
-                samples_shortened = np.delete(self._stm_samples, i, 0)
-                labels_shortened = np.delete(self._stm_labels, i, 0)
-                distances_stm = SAMKNNClassifier._get_distances(
-                    self._stm_samples[i, :], samples_shortened
-                )
-                nn_indices_stm = libNearestNeighbor.nArgMin(
-                    self.n_neighbors, distances_stm
-                )[0]
-                distances_ltm = SAMKNNClassifier._get_distances(
-                    self._stm_samples[i, :], samples_cl
-                )
-                nn_indices_ltm = libNearestNeighbor.nArgMin(
-                    min(len(distances_ltm), self.n_neighbors), distances_ltm
-                )[0]
-                correct_indices_stm = nn_indices_stm[
-                    labels_shortened[nn_indices_stm] == self._stm_labels[i]
-                ]
-                if len(correct_indices_stm) > 0:
-                    dist_threshold = np.max(distances_stm[correct_indices_stm])
-                    wrong_indices_ltm = nn_indices_ltm[
-                        labels_cl[nn_indices_ltm] != self._stm_labels[i]
-                    ]
-                    del_indices = np.where(
-                        distances_ltm[wrong_indices_ltm] <= dist_threshold
-                    )[0]
-                    samples_cl = np.delete(
-                        samples_cl, wrong_indices_ltm[del_indices], 0
-                    )
-                    labels_cl = np.delete(labels_cl, wrong_indices_ltm[del_indices], 0)
-        return samples_cl, labels_cl
-
-    def _learn_one(self, x, y):
-        """Processes a new sample."""
-        distances_stm = SAMKNNClassifier._get_distances(x, self._stm_samples)
-        if not self.use_ltm:
-            self._learn_one_by_stm(x, y, distances_stm)
-        else:
-            self._learn_one_by_all_memories(x, y, distances_stm)
-
-        self.train_step_count += 1
-        self._stm_samples = np.vstack([self._stm_samples, x])
-        self._stm_labels = np.append(self._stm_labels, y)
-        stm_shortened = self.size_check_fct()
-
-        self._ltm_samples, self._ltm_labels = self._clean_samples(
-            self._ltm_samples, self._ltm_labels, only_last=True
-        )
-
-        if self.stm_aprox_adaption is not None:
-            if stm_shortened:
-                distances_stm = SAMKNNClassifier._get_distances(
-                    x, self._stm_samples[:-1, :]
-                )
-
-            self.stm_distances[
-                len(self._stm_labels) - 1, : len(self._stm_labels) - 1
-            ] = distances_stm
-            old_window_size = len(self._stm_labels)
-            (
-                new_window_size,
-                self.interleaved_pred_histories,
-            ) = STMSizer.get_new_stm_size(
-                self.stm_aprox_adaption,
-                self._stm_labels,
-                self.n_neighbors,
-                self.get_labels_fct,
-                self.interleaved_pred_histories,
-                self.stm_distances,
-                self.min_stm_size,
-            )
-
-            if new_window_size < old_window_size:
-                del_range = range(old_window_size - new_window_size)
-                old_stm_samples = self._stm_samples[del_range, :]
-                old_stm_labels = self._stm_labels[del_range]
-                self._stm_samples = np.delete(self._stm_samples, del_range, 0)
-                self._stm_labels = np.delete(self._stm_labels, del_range, 0)
-                self.stm_distances[
-                    : len(self._stm_labels), : len(self._stm_labels)
-                ] = self.stm_distances[
-                    (old_window_size - new_window_size) : (
-                        (old_window_size - new_window_size) + len(self._stm_labels)
-                    ),
-                    (old_window_size - new_window_size) : (
-                        (old_window_size - new_window_size) + len(self._stm_labels)
-                    ),
-                ]
-
-                if self.use_ltm:
-                    for _ in del_range:
-                        self.stm_pred_history.popleft()
-                        self.ltm_pred_history.popleft()
-                        self.cmp_pred_history.popleft()
-
-                    old_stm_samples, old_stm_labels = self._clean_samples(
-                        old_stm_samples, old_stm_labels
-                    )
-                    self._ltm_samples = np.vstack([self._ltm_samples, old_stm_samples])
-                    self._ltm_labels = np.append(self._ltm_labels, old_stm_labels)
-                    self.size_check_fct()
-        self.stm_sizes.append(len(self._stm_labels))
-        self.ltm_sizes.append(len(self._ltm_labels))
-
-    def _learn_one_by_all_memories(self, sample, label, distances_stm):
-        """
-        Predicts the label of a given sample by using the STM, LTM and the CM.
-        Only used when use_ltm=True.
-        """
-        predicted_label_ltm = 0
-        predicted_label_stm = 0
-        predicted_label_both = 0
-        classifier_choice = 0
-        if len(self._stm_labels) == 0:
-            predicted_label = predicted_label_stm
-        else:
-            if len(self._stm_labels) < self.n_neighbors:
-                predicted_label_stm = self.get_labels_fct(
-                    distances_stm, self._stm_labels, len(self._stm_labels)
-                )[0]
-                predicted_label = predicted_label_stm
-            else:
-                distances_ltm = SAMKNNClassifier._get_distances(
-                    sample, self._ltm_samples
-                )
-                predicted_label_stm = self.get_labels_fct(
-                    distances_stm, self._stm_labels, self.n_neighbors
-                )[0]
-                predicted_label_both = self.get_labels_fct(
-                    np.append(distances_stm, distances_ltm),
-                    np.append(self._stm_labels, self._ltm_labels),
-                    self.n_neighbors,
-                )[0]
-
-                if len(self._ltm_labels) >= self.n_neighbors:  # noqa
-                    predicted_label_ltm = self.get_labels_fct(
-                        distances_ltm, self._ltm_labels, self.n_neighbors
-                    )[0]
-                    correct_ltm = np.sum(self.ltm_pred_history)
-                    correct_stm = np.sum(self.stm_pred_history)
-                    correct_both = np.sum(self.cmp_pred_history)
-                    labels = [
-                        predicted_label_stm,
-                        predicted_label_ltm,
-                        predicted_label_both,
-                    ]
-                    classifier_choice = np.argmax(
-                        [correct_stm, correct_ltm, correct_both]
-                    )
-                    predicted_label = labels[classifier_choice]  # noqa
-                else:
-                    predicted_label = predicted_label_stm
-
-        self.classifier_choice.append(classifier_choice)
-        self.cmp_pred_history.append(predicted_label_both == label)
-        self.n_cm_correct += predicted_label_both == label
-        self.stm_pred_history.append(predicted_label_stm == label)
-        self.n_stm_correct += predicted_label_stm == label
-        self.ltm_pred_history.append(predicted_label_ltm == label)
-        self.n_ltm_correct += predicted_label_ltm == label
-        self.n_possible_correct_predictions += label in [
-            predicted_label_stm,
-            predicted_label_both,
-            predicted_label_ltm,
-        ]
-        self.n_correct_predictions += predicted_label == label
-        return predicted_label
-
-    def _predict_by_all_memories(self, sample, label, distances_stm):  # noqa
-        predicted_label_stm = 0
-        if len(self._stm_labels) == 0:
-            predicted_label = predicted_label_stm
-        else:
-            if len(self._stm_labels) < self.n_neighbors:
-                predicted_label_stm = self.get_labels_fct(
-                    distances_stm, self._stm_labels, len(self._stm_labels)
-                )[0]
-                predicted_label = predicted_label_stm
-            else:
-                distances_ltm = SAMKNNClassifier._get_distances(
-                    sample, self._ltm_samples
-                )
-                predicted_label_stm = self.get_labels_fct(
-                    distances_stm, self._stm_labels, self.n_neighbors
-                )[0]
-                distances_new = cp.deepcopy(distances_stm)
-                stm_labels_new = cp.deepcopy(self._stm_labels)
-                predicted_label_both = self.get_labels_fct(
-                    np.append(distances_new, distances_ltm),
-                    np.append(stm_labels_new, self._ltm_labels),
-                    self.n_neighbors,
-                )[0]
-                if len(self._ltm_labels) >= self.n_neighbors:  # noqa
-                    predicted_label_ltm = self.get_labels_fct(
-                        distances_ltm, self._ltm_labels, self.n_neighbors
-                    )[0]
-                    correct_ltm = np.sum(self.ltm_pred_history)
-                    correct_stm = np.sum(self.stm_pred_history)
-                    correct_both = np.sum(self.cmp_pred_history)
-                    labels = [
-                        predicted_label_stm,
-                        predicted_label_ltm,
-                        predicted_label_both,
-                    ]
-                    classifier_choice = np.argmax(
-                        [correct_stm, correct_ltm, correct_both]
-                    )
-                    predicted_label = labels[classifier_choice]  # noqa
-                else:
-                    predicted_label = predicted_label_stm
-
-        return predicted_label
-
-    def _learn_one_by_stm(self, sample, label, distances_stm):
-        pass
-
-    def _predict_by_stm(self, sample, label, distances_stm):  # noqa
-        """Predicts the label of a given sample by the STM, only used when use_ltm=False."""
-        predicted_label = 0
-        curr_len = len(self._stm_labels)
-        if curr_len > 0:
-            predicted_label = self.get_labels_fct(
-                distances_stm, self._stm_labels, min(self.n_neighbors, curr_len)
-            )[0]
-        return predicted_label
-
-    def learn_one(self, x, y) -> "Classifier":
-        """Update the model with a set of features `x` and a label `y`.
-
-        Parameters
-        ----------
-        x
-            The sample's features
-        y
-            The sample's class label.
-
-        Returns
-        -------
-        self
-        """
-        x_array = dict2numpy(x)
-        c = len(x_array)
-        if self._stm_samples is None:
-            self._stm_samples = np.empty(shape=(0, c))
-            self._ltm_samples = np.empty(shape=(0, c))
-
-        self._learn_one(x_array, y)
-
-        return self
-
-    def predict_one(self, x: dict):
-        x_array = dict2numpy(x)
-        c = len(x_array)
-        if self._stm_samples is None:
-            self._stm_samples = np.empty(shape=(0, c))
-            self._ltm_samples = np.empty(shape=(0, c))
-
-        distances_stm = SAMKNNClassifier._get_distances(x_array, self._stm_samples)
-        return self.predict_fct(x_array, None, distances_stm)
-
-    def predict_proba_one(self, x):
-        raise NotImplementedError
-
-    @staticmethod
-    def _get_maj_label(distances, labels, n_neighbors):
-        """Returns the majority label of the k nearest neighbors."""
-
-        nn_indices = libNearestNeighbor.nArgMin(n_neighbors, distances)
-
-        if not isinstance(labels, type(np.array([]))):
-            labels = np.asarray(labels, dtype=np.int8)
-        else:
-            labels = np.int8(labels)
-
-        pred_labels = libNearestNeighbor.mostCommon(labels[nn_indices])
-
-        return pred_labels
-
-    @staticmethod
-    def _get_distance_weighted_label(distances, labels, n_neighbors):
-        """Returns the the distance weighted label of the k nearest neighbors."""
-        nn_indices = libNearestNeighbor.nArgMin(n_neighbors, distances)
-        sqrtDistances = np.sqrt(distances[nn_indices])
-        if not isinstance(labels, type(np.array([]))):
-            labels = np.asarray(labels, dtype=np.int8)
-        else:
-            labels = np.int8(labels)
-
-        predLabels = libNearestNeighbor.getLinearWeightedLabels(
-            labels[nn_indices], sqrtDistances
-        )
-        return predLabels
-
-    @property
-    def STMSamples(self):  # noqa
-        """Samples in the STM."""
-        return self._stm_samples
-
-    @property
-    def STMLabels(self):  # noqa
-        """Class labels in the STM."""
-        return self._stm_labels
-
-    @property
-    def LTMSamples(self):  # noqa
-        """Samples in the LTM."""
-        return self._ltm_samples
-
-    @property
-    def LTMLabels(self):  # noqa
-        """Class labels in the LTM."""
-        return self._ltm_labels
-
-
-class STMSizer:
-    """Utility class to adapt the size of the sliding window of the STM."""
-
-    @staticmethod
-    def get_new_stm_size(
-        aprox_adaption_strategy,
-        labels,
-        n_neighbours,
-        get_labels_fct,
-        prediction_histories,
-        distances_stm,
-        min_stm_size,
-    ):
-        """Returns the new STM size."""
-        if aprox_adaption_strategy:
-            "Use approximate interleaved test-train error"
-            return STMSizer._get_max_acc_approx_window_size(
-                labels,
-                n_neighbours,
-                get_labels_fct,
-                prediction_histories,
-                distances_stm,
-                min_size=min_stm_size,
-            )
-        elif aprox_adaption_strategy is not None and not aprox_adaption_strategy:
-            "Use exact interleaved test-train error"
-            return STMSizer._get_max_acc_window_size(
-                labels,
-                n_neighbours,
-                get_labels_fct,
-                prediction_histories,
-                distances_stm,
-                min_size=min_stm_size,
-            )
-        elif aprox_adaption_strategy is None:
-            "No stm adaption"
-            return len(labels), prediction_histories
-        else:
-            raise Exception(f"Invalid adaption_strategy: {aprox_adaption_strategy}")
-
-    @staticmethod
-    def _acc_score(y_pred, y_true):
-        """Calculates the achieved accuracy."""
-        return np.sum(y_pred == y_true) / float(len(y_pred))
-
-    @staticmethod
-    def _get_interleaved_test_train_acc(
-        labels, n_neighbours, get_labels_fct, distances_stm
-    ):
-        """Calculates the interleaved test train accuracy from the scratch."""
-        predLabels = []
-        for i in range(n_neighbours, len(labels)):
-            distances = distances_stm[i, :i]
-            predLabels.append(get_labels_fct(distances, labels[:i], n_neighbours)[0])
-        return (
-            STMSizer._acc_score(predLabels[:], labels[n_neighbours:]),
-            (predLabels == labels[n_neighbours:]).tolist(),
-        )
-
-    @staticmethod
-    def _get_interleaved_test_train_acc_pred_history(
-        labels, n_neighbours, get_labels_fct, prediction_history, distances_stm
-    ):
-        """
-        Calculates the interleaved test train accuracy incrementally
-        by using the previous predictions.
-        """
-        for i in range(len(prediction_history) + n_neighbours, len(labels)):
-            distances = distances_stm[i, :i]
-            label = get_labels_fct(distances, labels[:i], n_neighbours)[0]
-            prediction_history.append(label == labels[i])
-        return (
-            np.sum(prediction_history) / float(len(prediction_history)),
-            prediction_history,
-        )
-
-    @staticmethod
-    def _adapt_histories(n_deletions, prediction_histories):
-        """
-        Removes predictions of the largest window size and shifts
-        the remaining ones accordingly.
-        """
-        for i in range(n_deletions):
-            sortedKeys = np.sort(list(prediction_histories.keys()))
-            prediction_histories.pop(sortedKeys[0], None)
-            delta = sortedKeys[1]
-            for j in range(1, len(sortedKeys)):
-                prediction_histories[sortedKeys[j] - delta] = prediction_histories.pop(
-                    sortedKeys[j]
-                )
-        return prediction_histories
-
-    @staticmethod
-    def _get_max_acc_window_size(
-        labels,
-        n_neighbours,
-        get_labels_fct,
-        prediction_histories,
-        distances_stm,
-        min_size=50,
-    ):
-        """
-        Returns the window size with the minimum interleaved
-        test-train error (exact calculation).
-        """
-        n_samples = len(labels)
-        if n_samples < 2 * min_size:
-            return n_samples, prediction_histories
-        else:
-            numSamplesRange = [n_samples]
-            while numSamplesRange[-1] / 2 >= min_size:
-                numSamplesRange.append(numSamplesRange[-1] / 2)
-
-            accuracies = []
-            keys_to_remove = []
-            for key in prediction_histories.keys():
-                if key not in (n_samples - np.array(numSamplesRange)):
-                    keys_to_remove.append(key)
-            for key in keys_to_remove:
-                prediction_histories.pop(key, None)
-
-            for numSamplesIt in numSamplesRange:
-                idx = int(n_samples - numSamplesIt)
-                keyset = list(prediction_histories.keys())
-                # if predictionHistories.has_key(idx):
-                if idx in keyset:
-                    (
-                        accuracy,
-                        predHistory,
-                    ) = STMSizer._get_interleaved_test_train_acc_pred_history(
-                        labels[idx:],
-                        n_neighbours,
-                        get_labels_fct,
-                        prediction_histories[idx],
-                        distances_stm[idx:, idx:],
-                    )
-                else:
-                    accuracy, predHistory = STMSizer._get_interleaved_test_train_acc(
-                        labels[idx:],
-                        n_neighbours,
-                        get_labels_fct,
-                        distances_stm[idx:, idx:],
-                    )
-                prediction_histories[idx] = predHistory
-                accuracies.append(accuracy)
-            accuracies = np.round(accuracies, decimals=4)
-            best_n_train_idx = np.argmax(accuracies)
-            window_size = numSamplesRange[best_n_train_idx]  # noqa
-
-            if window_size < n_samples:
-                prediction_histories = STMSizer._adapt_histories(
-                    best_n_train_idx, prediction_histories
-                )
-            return int(window_size), prediction_histories
-
-    @staticmethod
-    def _get_max_acc_approx_window_size(
-        labels,
-        n_neighbours,
-        get_labels_fct,
-        prediction_histories,
-        distances_stm,
-        min_size=50,
-    ):
-        """
-        Returns the window size with the minimum interleaved
-        test-train error (using an approximation).
-        """
-        n_samples = len(labels)
-        if n_samples < 2 * min_size:
-            return n_samples, prediction_histories
-        else:
-            n_samples_range = [n_samples]
-            while n_samples_range[-1] / 2 >= min_size:
-                n_samples_range.append(n_samples_range[-1] / 2)
-            accuracies = []
-            for numSamplesIt in n_samples_range:
-                idx = int(n_samples - numSamplesIt)
-                keyset = list(prediction_histories.keys())
-                # if predictionHistories.has_key(idx):
-                if idx in keyset:
-                    (
-                        accuracy,
-                        predHistory,
-                    ) = STMSizer._get_interleaved_test_train_acc_pred_history(
-                        labels[idx:],
-                        n_neighbours,
-                        get_labels_fct,
-                        prediction_histories[idx],
-                        distances_stm[idx:, idx:],
-                    )
-                # elif predictionHistories.has_key(idx-1):
-                elif idx - 1 in keyset:
-                    predHistory = prediction_histories[idx - 1]
-                    prediction_histories.pop(idx - 1, None)
-                    predHistory.pop(0)
-                    (
-                        accuracy,
-                        predHistory,
-                    ) = STMSizer._get_interleaved_test_train_acc_pred_history(
-                        labels[idx:],
-                        n_neighbours,
-                        get_labels_fct,
-                        predHistory,
-                        distances_stm[idx:, idx:],
-                    )
-                else:
-                    accuracy, predHistory = STMSizer._get_interleaved_test_train_acc(
-                        labels[idx:],
-                        n_neighbours,
-                        get_labels_fct,
-                        distances_stm[idx:, idx:],
-                    )
-                prediction_histories[idx] = predHistory
-                accuracies.append(accuracy)
-            accuracies = np.round(accuracies, decimals=4)
-            best_n_train_idx = np.argmax(accuracies)
-            if best_n_train_idx > 0:
-                moreAccurateIndices = np.where(accuracies > accuracies[0])[0]
-                for i in moreAccurateIndices:
-                    idx = int(n_samples - n_samples_range[i])
-                    accuracy, predHistory = STMSizer._get_interleaved_test_train_acc(
-                        labels[idx:],
-                        n_neighbours,
-                        get_labels_fct,
-                        distances_stm[idx:, idx:],
-                    )
-                    prediction_histories[idx] = predHistory
-                    accuracies[i] = accuracy
-                accuracies = np.round(accuracies, decimals=4)
-                best_n_train_idx = np.argmax(accuracies)
-            window_size = n_samples_range[best_n_train_idx]  # noqa
-
-            if window_size < n_samples:
-                prediction_histories = STMSizer._adapt_histories(
-                    best_n_train_idx, prediction_histories
-                )
-            return int(window_size), prediction_histories
+import copy as cp
+import logging
+from collections import deque
+
+import numpy as np
+
+from river.base import Classifier
+from river.utils import dict2numpy
+
+from . import libNearestNeighbor
+
+
+class SAMKNNClassifier(Classifier):
+    """Self Adjusting Memory coupled with the kNN classifier.
+
+    The Self Adjusting Memory (SAM) [^1] model builds an ensemble with models targeting current
+    or former concepts. SAM is built using two memories: STM for the current concept, and
+    the LTM to retain information about past concepts. A cleaning process is in charge of
+    controlling the size of the STM while keeping the information in the LTM consistent
+    with the STM.
+
+    Parameters
+    ----------
+    n_neighbors
+        number of evaluated nearest neighbors.
+    distance_weighting
+        Type of weighting of the nearest neighbors. It `True`  will use 'distance'.
+        Otherwise, will use 'uniform' (majority voting).
+    window_size
+         Maximum number of overall stored data points.
+    ltm_size
+        Proportion of the overall instances that may be used for the LTM. This is
+        only relevant when the maximum number(maxSize) of stored instances is reached.
+    stm_aprox_adaption
+        Type of STM size adaption.<br/>
+            - If `True` approximates the interleaved test-train error and is
+               significantly faster than the exact version.<br/>
+            - If `False` calculates the interleaved test-train error exactly for each of the
+              evaluated window sizes, which often has to be recalculated from the scratch.<br/>
+            - If `None`, the STM is not  adapted at all. If additionally `use_ltm=False`, then
+              this algorithm is simply a kNN with fixed sliding window size.
+    min_stm_size
+        Minimum STM size which is evaluated during the STM size adaption.
+    use_ltm
+        Specifies whether the LTM should be used at all.
+
+    Examples
+    --------
+    >>> from river import synth
+    >>> from river import evaluate
+    >>> from river import metrics
+    >>> from river import neighbors
+
+    >>> dataset = synth.ConceptDriftStream(position=500, width=20, seed=1).take(1000)
+
+    >>> model = neighbors.SAMKNNClassifier(window_size=100)
+
+    >>> metric = metrics.Accuracy()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)  # doctest: +SKIP
+    Accuracy: 56.70%
+
+    Notes
+    -----
+    This modules uses libNearestNeighbor, a C++ library used to speed up some of
+    the algorithm's computations. When invoking the library's functions it's important
+    to pass the right argument type. Although most of this framework's functionality
+    will work with python standard types, the C++ library will work with 8-bit labels,
+    which is already done by the SAMKNN class, but may be absent in custom classes that
+    use SAMKNN static methods, or other custom functions that use the C++ library.
+
+    References
+    ----------
+    [^1]: Losing, Viktor, Barbara Hammer, and Heiko Wersing.
+          "Knn classifier with self adjusting memory for heterogeneous concept drift."
+          In Data Mining (ICDM), 2016 IEEE 16th International Conference on,
+          pp. 291-300. IEEE, 2016.
+
+    """
+
+    def __init__(
+        self,
+        n_neighbors: int = 5,
+        distance_weighting=True,
+        window_size: int = 5000,
+        ltm_size: float = 0.4,
+        min_stm_size: int = 50,
+        stm_aprox_adaption=True,
+        use_ltm=True,
+    ):
+        super().__init__()
+        self.n_neighbors = n_neighbors
+        self.distance_weighting = distance_weighting
+        self.window_size = window_size
+        self.ltm_size = ltm_size
+        self.min_stm_size = min_stm_size
+        self.use_ltm = use_ltm
+
+        self._stm_samples = None
+        self._stm_labels = np.empty(shape=0, dtype=np.int32)
+        self._ltm_samples = None
+        self._ltm_labels = np.empty(shape=0, dtype=np.int32)
+        self.max_ltm_size = self.ltm_size * self.window_size
+        self.max_stm_size = self.window_size - self.max_ltm_size
+        self.min_stm_size = self.min_stm_size
+        self.stm_aprox_adaption = stm_aprox_adaption
+
+        self.stm_distances = np.zeros(shape=(window_size + 1, window_size + 1))
+        if self.distance_weighting:
+            self.get_labels_fct = SAMKNNClassifier._get_distance_weighted_label
+        else:
+            self.get_labels_fct = SAMKNNClassifier._get_maj_label
+        if self.use_ltm:
+            self.predict_fct = self._predict_by_all_memories
+            self.size_check_fct = self._size_check_stmltm
+        else:
+            self.predict_fct = self._predict_by_stm
+            self.size_check_fct = self._size_check_fade_out
+
+        self.interleaved_pred_histories = {}
+        self.ltm_pred_history = deque([])
+        self.stm_pred_history = deque([])
+        self.cmp_pred_history = deque([])
+
+        self.train_step_count = 0
+        self.stm_sizes = []
+        self.ltm_sizes = []
+        self.n_stm_correct = 0
+        self.n_ltm_correct = 0
+        self.n_cm_correct = 0
+        self.n_possible_correct_predictions = 0
+        self.n_correct_predictions = 0
+        self.classifier_choice = []
+        self.pred_history = []
+
+    def _unit_test_skips(self):
+        return {"check_emerging_features", "check_disappearing_features"}
+
+    @staticmethod
+    def _get_distances(sample, samples):
+        """Calculate distances from sample to all samples."""
+        return libNearestNeighbor.get1ToNDistances(sample, samples)
+
+    def _cluster_down(self, samples, labels):
+        """
+        Performs class-wise kMeans++ clustering for given samples with corresponding labels.
+        The number of samples is halved per class.
+        """
+        from sklearn.cluster import KMeans
+
+        logging.debug("cluster Down %d" % self.train_step_count)
+        uniqueLabels = np.unique(labels)
+        newSamples = np.empty(shape=(0, samples.shape[1]))
+        newLabels = np.empty(shape=0, dtype=np.int32)
+        for label in uniqueLabels:
+            tmpSamples = samples[labels == label]
+            newLength = int(max(tmpSamples.shape[0] / 2, 1))
+            clustering = KMeans(n_clusters=newLength, n_init=1, random_state=0)
+            clustering.fit(tmpSamples)
+            newSamples = np.vstack([newSamples, clustering.cluster_centers_])
+            newLabels = np.append(
+                newLabels, label * np.ones(shape=newLength, dtype=np.int32)
+            )
+        return newSamples, newLabels
+
+    def _size_check_fade_out(self):
+        """
+        Makes sure that the STM does not surpass the maximum size.
+        Only used when use_ltm=False.
+        """
+        STMShortened = False
+        if len(self._stm_labels) > self.max_stm_size + self.max_ltm_size:
+            STMShortened = True
+            self._stm_samples = np.delete(self._stm_samples, 0, 0)
+            self._stm_labels = np.delete(self._stm_labels, 0, 0)
+            self.stm_distances[
+                : len(self._stm_labels), : len(self._stm_labels)
+            ] = self.stm_distances[
+                1 : len(self._stm_labels) + 1, 1 : len(self._stm_labels) + 1
+            ]
+
+            if self.stm_aprox_adaption:
+                key_set = list(self.interleaved_pred_histories.keys())
+                # if self.interleaved_pred_histories.has_key(0):
+                if 0 in key_set:
+                    self.interleaved_pred_histories[0].pop(0)
+                updated_histories = cp.deepcopy(self.interleaved_pred_histories)
+                for key in self.interleaved_pred_histories.keys():
+                    if key > 0:
+                        if key == 1:
+                            updated_histories.pop(0, None)
+                        tmp = updated_histories[key]
+                        updated_histories.pop(key, None)
+                        updated_histories[key - 1] = tmp
+                self.interleaved_pred_histories = updated_histories
+            else:
+                self.interleaved_pred_histories = {}
+        return STMShortened
+
+    def _size_check_stmltm(self):
+        """
+        Makes sure that the STM and LTM combined doe not surpass the maximum size.
+        Only used when use_ltm=True.
+        """
+        stm_shortened = False
+        if (
+            len(self._stm_labels) + len(self._ltm_labels)
+            > self.max_stm_size + self.max_ltm_size
+        ):
+            if len(self._ltm_labels) > self.max_ltm_size:
+                self._ltm_samples, self._ltm_labels = self._cluster_down(
+                    self._ltm_samples, self._ltm_labels
+                )
+            else:
+                if (
+                    len(self._stm_labels) + len(self._ltm_labels)
+                    > self.max_stm_size + self.max_ltm_size
+                ):
+                    stm_shortened = True
+                    n_shifts = int(self.max_ltm_size - len(self._ltm_labels) + 1)
+                    shift_range = range(n_shifts)
+                    self._ltm_samples = np.vstack(
+                        [self._ltm_samples, self._stm_samples[:n_shifts, :]]
+                    )
+                    self._ltm_labels = np.append(
+                        self._ltm_labels, self._stm_labels[:n_shifts]
+                    )
+                    self._ltm_samples, self._ltm_labels = self._cluster_down(
+                        self._ltm_samples, self._ltm_labels
+                    )
+                    self._stm_samples = np.delete(self._stm_samples, shift_range, 0)
+                    self._stm_labels = np.delete(self._stm_labels, shift_range, 0)
+                    self.stm_distances[
+                        : len(self._stm_labels), : len(self._stm_labels)
+                    ] = self.stm_distances[
+                        n_shifts : len(self._stm_labels) + n_shifts,
+                        n_shifts : len(self._stm_labels) + n_shifts,
+                    ]
+                    for _ in shift_range:
+                        self.ltm_pred_history.popleft()
+                        self.stm_pred_history.popleft()
+                        self.cmp_pred_history.popleft()
+                    self.interleaved_pred_histories = {}
+        return stm_shortened
+
+    def _clean_samples(self, samples_cl, labels_cl, only_last=False):
+        """
+        Removes distance-based all instances from the input samples that
+        contradict those in the STM.
+        """
+        if len(self._stm_labels) > self.n_neighbors and samples_cl.shape[0] > 0:
+            if only_last:
+                loop_range = [len(self._stm_labels) - 1]
+            else:
+                loop_range = range(len(self._stm_labels))
+            for i in loop_range:
+                if len(labels_cl) == 0:
+                    break
+                samples_shortened = np.delete(self._stm_samples, i, 0)
+                labels_shortened = np.delete(self._stm_labels, i, 0)
+                distances_stm = SAMKNNClassifier._get_distances(
+                    self._stm_samples[i, :], samples_shortened
+                )
+                nn_indices_stm = libNearestNeighbor.nArgMin(
+                    self.n_neighbors, distances_stm
+                )[0]
+                distances_ltm = SAMKNNClassifier._get_distances(
+                    self._stm_samples[i, :], samples_cl
+                )
+                nn_indices_ltm = libNearestNeighbor.nArgMin(
+                    min(len(distances_ltm), self.n_neighbors), distances_ltm
+                )[0]
+                correct_indices_stm = nn_indices_stm[
+                    labels_shortened[nn_indices_stm] == self._stm_labels[i]
+                ]
+                if len(correct_indices_stm) > 0:
+                    dist_threshold = np.max(distances_stm[correct_indices_stm])
+                    wrong_indices_ltm = nn_indices_ltm[
+                        labels_cl[nn_indices_ltm] != self._stm_labels[i]
+                    ]
+                    del_indices = np.where(
+                        distances_ltm[wrong_indices_ltm] <= dist_threshold
+                    )[0]
+                    samples_cl = np.delete(
+                        samples_cl, wrong_indices_ltm[del_indices], 0
+                    )
+                    labels_cl = np.delete(labels_cl, wrong_indices_ltm[del_indices], 0)
+        return samples_cl, labels_cl
+
+    def _learn_one(self, x, y):
+        """Processes a new sample."""
+        distances_stm = SAMKNNClassifier._get_distances(x, self._stm_samples)
+        if not self.use_ltm:
+            self._learn_one_by_stm(x, y, distances_stm)
+        else:
+            self._learn_one_by_all_memories(x, y, distances_stm)
+
+        self.train_step_count += 1
+        self._stm_samples = np.vstack([self._stm_samples, x])
+        self._stm_labels = np.append(self._stm_labels, y)
+        stm_shortened = self.size_check_fct()
+
+        self._ltm_samples, self._ltm_labels = self._clean_samples(
+            self._ltm_samples, self._ltm_labels, only_last=True
+        )
+
+        if self.stm_aprox_adaption is not None:
+            if stm_shortened:
+                distances_stm = SAMKNNClassifier._get_distances(
+                    x, self._stm_samples[:-1, :]
+                )
+
+            self.stm_distances[
+                len(self._stm_labels) - 1, : len(self._stm_labels) - 1
+            ] = distances_stm
+            old_window_size = len(self._stm_labels)
+            (
+                new_window_size,
+                self.interleaved_pred_histories,
+            ) = STMSizer.get_new_stm_size(
+                self.stm_aprox_adaption,
+                self._stm_labels,
+                self.n_neighbors,
+                self.get_labels_fct,
+                self.interleaved_pred_histories,
+                self.stm_distances,
+                self.min_stm_size,
+            )
+
+            if new_window_size < old_window_size:
+                del_range = range(old_window_size - new_window_size)
+                old_stm_samples = self._stm_samples[del_range, :]
+                old_stm_labels = self._stm_labels[del_range]
+                self._stm_samples = np.delete(self._stm_samples, del_range, 0)
+                self._stm_labels = np.delete(self._stm_labels, del_range, 0)
+                self.stm_distances[
+                    : len(self._stm_labels), : len(self._stm_labels)
+                ] = self.stm_distances[
+                    (old_window_size - new_window_size) : (
+                        (old_window_size - new_window_size) + len(self._stm_labels)
+                    ),
+                    (old_window_size - new_window_size) : (
+                        (old_window_size - new_window_size) + len(self._stm_labels)
+                    ),
+                ]
+
+                if self.use_ltm:
+                    for _ in del_range:
+                        self.stm_pred_history.popleft()
+                        self.ltm_pred_history.popleft()
+                        self.cmp_pred_history.popleft()
+
+                    old_stm_samples, old_stm_labels = self._clean_samples(
+                        old_stm_samples, old_stm_labels
+                    )
+                    self._ltm_samples = np.vstack([self._ltm_samples, old_stm_samples])
+                    self._ltm_labels = np.append(self._ltm_labels, old_stm_labels)
+                    self.size_check_fct()
+        self.stm_sizes.append(len(self._stm_labels))
+        self.ltm_sizes.append(len(self._ltm_labels))
+
+    def _learn_one_by_all_memories(self, sample, label, distances_stm):
+        """
+        Predicts the label of a given sample by using the STM, LTM and the CM.
+        Only used when use_ltm=True.
+        """
+        predicted_label_ltm = 0
+        predicted_label_stm = 0
+        predicted_label_both = 0
+        classifier_choice = 0
+        if len(self._stm_labels) == 0:
+            predicted_label = predicted_label_stm
+        else:
+            if len(self._stm_labels) < self.n_neighbors:
+                predicted_label_stm = self.get_labels_fct(
+                    distances_stm, self._stm_labels, len(self._stm_labels)
+                )[0]
+                predicted_label = predicted_label_stm
+            else:
+                distances_ltm = SAMKNNClassifier._get_distances(
+                    sample, self._ltm_samples
+                )
+                predicted_label_stm = self.get_labels_fct(
+                    distances_stm, self._stm_labels, self.n_neighbors
+                )[0]
+                predicted_label_both = self.get_labels_fct(
+                    np.append(distances_stm, distances_ltm),
+                    np.append(self._stm_labels, self._ltm_labels),
+                    self.n_neighbors,
+                )[0]
+
+                if len(self._ltm_labels) >= self.n_neighbors:  # noqa
+                    predicted_label_ltm = self.get_labels_fct(
+                        distances_ltm, self._ltm_labels, self.n_neighbors
+                    )[0]
+                    correct_ltm = np.sum(self.ltm_pred_history)
+                    correct_stm = np.sum(self.stm_pred_history)
+                    correct_both = np.sum(self.cmp_pred_history)
+                    labels = [
+                        predicted_label_stm,
+                        predicted_label_ltm,
+                        predicted_label_both,
+                    ]
+                    classifier_choice = np.argmax(
+                        [correct_stm, correct_ltm, correct_both]
+                    )
+                    predicted_label = labels[classifier_choice]  # noqa
+                else:
+                    predicted_label = predicted_label_stm
+
+        self.classifier_choice.append(classifier_choice)
+        self.cmp_pred_history.append(predicted_label_both == label)
+        self.n_cm_correct += predicted_label_both == label
+        self.stm_pred_history.append(predicted_label_stm == label)
+        self.n_stm_correct += predicted_label_stm == label
+        self.ltm_pred_history.append(predicted_label_ltm == label)
+        self.n_ltm_correct += predicted_label_ltm == label
+        self.n_possible_correct_predictions += label in [
+            predicted_label_stm,
+            predicted_label_both,
+            predicted_label_ltm,
+        ]
+        self.n_correct_predictions += predicted_label == label
+        return predicted_label
+
+    def _predict_by_all_memories(self, sample, label, distances_stm):  # noqa
+        predicted_label_stm = 0
+        if len(self._stm_labels) == 0:
+            predicted_label = predicted_label_stm
+        else:
+            if len(self._stm_labels) < self.n_neighbors:
+                predicted_label_stm = self.get_labels_fct(
+                    distances_stm, self._stm_labels, len(self._stm_labels)
+                )[0]
+                predicted_label = predicted_label_stm
+            else:
+                distances_ltm = SAMKNNClassifier._get_distances(
+                    sample, self._ltm_samples
+                )
+                predicted_label_stm = self.get_labels_fct(
+                    distances_stm, self._stm_labels, self.n_neighbors
+                )[0]
+                distances_new = cp.deepcopy(distances_stm)
+                stm_labels_new = cp.deepcopy(self._stm_labels)
+                predicted_label_both = self.get_labels_fct(
+                    np.append(distances_new, distances_ltm),
+                    np.append(stm_labels_new, self._ltm_labels),
+                    self.n_neighbors,
+                )[0]
+                if len(self._ltm_labels) >= self.n_neighbors:  # noqa
+                    predicted_label_ltm = self.get_labels_fct(
+                        distances_ltm, self._ltm_labels, self.n_neighbors
+                    )[0]
+                    correct_ltm = np.sum(self.ltm_pred_history)
+                    correct_stm = np.sum(self.stm_pred_history)
+                    correct_both = np.sum(self.cmp_pred_history)
+                    labels = [
+                        predicted_label_stm,
+                        predicted_label_ltm,
+                        predicted_label_both,
+                    ]
+                    classifier_choice = np.argmax(
+                        [correct_stm, correct_ltm, correct_both]
+                    )
+                    predicted_label = labels[classifier_choice]  # noqa
+                else:
+                    predicted_label = predicted_label_stm
+
+        return predicted_label
+
+    def _learn_one_by_stm(self, sample, label, distances_stm):
+        pass
+
+    def _predict_by_stm(self, sample, label, distances_stm):  # noqa
+        """Predicts the label of a given sample by the STM, only used when use_ltm=False."""
+        predicted_label = 0
+        curr_len = len(self._stm_labels)
+        if curr_len > 0:
+            predicted_label = self.get_labels_fct(
+                distances_stm, self._stm_labels, min(self.n_neighbors, curr_len)
+            )[0]
+        return predicted_label
+
+    def learn_one(self, x, y) -> "Classifier":
+        """Update the model with a set of features `x` and a label `y`.
+
+        Parameters
+        ----------
+        x
+            The sample's features
+        y
+            The sample's class label.
+
+        Returns
+        -------
+        self
+        """
+        x_array = dict2numpy(x)
+        c = len(x_array)
+        if self._stm_samples is None:
+            self._stm_samples = np.empty(shape=(0, c))
+            self._ltm_samples = np.empty(shape=(0, c))
+
+        self._learn_one(x_array, y)
+
+        return self
+
+    def predict_one(self, x: dict):
+        x_array = dict2numpy(x)
+        c = len(x_array)
+        if self._stm_samples is None:
+            self._stm_samples = np.empty(shape=(0, c))
+            self._ltm_samples = np.empty(shape=(0, c))
+
+        distances_stm = SAMKNNClassifier._get_distances(x_array, self._stm_samples)
+        return self.predict_fct(x_array, None, distances_stm)
+
+    def predict_proba_one(self, x):
+        raise NotImplementedError
+
+    @staticmethod
+    def _get_maj_label(distances, labels, n_neighbors):
+        """Returns the majority label of the k nearest neighbors."""
+
+        nn_indices = libNearestNeighbor.nArgMin(n_neighbors, distances)
+
+        if not isinstance(labels, type(np.array([]))):
+            labels = np.asarray(labels, dtype=np.int8)
+        else:
+            labels = np.int8(labels)
+
+        pred_labels = libNearestNeighbor.mostCommon(labels[nn_indices])
+
+        return pred_labels
+
+    @staticmethod
+    def _get_distance_weighted_label(distances, labels, n_neighbors):
+        """Returns the the distance weighted label of the k nearest neighbors."""
+        nn_indices = libNearestNeighbor.nArgMin(n_neighbors, distances)
+        sqrtDistances = np.sqrt(distances[nn_indices])
+        if not isinstance(labels, type(np.array([]))):
+            labels = np.asarray(labels, dtype=np.int8)
+        else:
+            labels = np.int8(labels)
+
+        predLabels = libNearestNeighbor.getLinearWeightedLabels(
+            labels[nn_indices], sqrtDistances
+        )
+        return predLabels
+
+    @property
+    def STMSamples(self):  # noqa
+        """Samples in the STM."""
+        return self._stm_samples
+
+    @property
+    def STMLabels(self):  # noqa
+        """Class labels in the STM."""
+        return self._stm_labels
+
+    @property
+    def LTMSamples(self):  # noqa
+        """Samples in the LTM."""
+        return self._ltm_samples
+
+    @property
+    def LTMLabels(self):  # noqa
+        """Class labels in the LTM."""
+        return self._ltm_labels
+
+
+class STMSizer:
+    """Utility class to adapt the size of the sliding window of the STM."""
+
+    @staticmethod
+    def get_new_stm_size(
+        aprox_adaption_strategy,
+        labels,
+        n_neighbours,
+        get_labels_fct,
+        prediction_histories,
+        distances_stm,
+        min_stm_size,
+    ):
+        """Returns the new STM size."""
+        if aprox_adaption_strategy:
+            "Use approximate interleaved test-train error"
+            return STMSizer._get_max_acc_approx_window_size(
+                labels,
+                n_neighbours,
+                get_labels_fct,
+                prediction_histories,
+                distances_stm,
+                min_size=min_stm_size,
+            )
+        elif aprox_adaption_strategy is not None and not aprox_adaption_strategy:
+            "Use exact interleaved test-train error"
+            return STMSizer._get_max_acc_window_size(
+                labels,
+                n_neighbours,
+                get_labels_fct,
+                prediction_histories,
+                distances_stm,
+                min_size=min_stm_size,
+            )
+        elif aprox_adaption_strategy is None:
+            "No stm adaption"
+            return len(labels), prediction_histories
+        else:
+            raise Exception(f"Invalid adaption_strategy: {aprox_adaption_strategy}")
+
+    @staticmethod
+    def _acc_score(y_pred, y_true):
+        """Calculates the achieved accuracy."""
+        return np.sum(y_pred == y_true) / float(len(y_pred))
+
+    @staticmethod
+    def _get_interleaved_test_train_acc(
+        labels, n_neighbours, get_labels_fct, distances_stm
+    ):
+        """Calculates the interleaved test train accuracy from the scratch."""
+        predLabels = []
+        for i in range(n_neighbours, len(labels)):
+            distances = distances_stm[i, :i]
+            predLabels.append(get_labels_fct(distances, labels[:i], n_neighbours)[0])
+        return (
+            STMSizer._acc_score(predLabels[:], labels[n_neighbours:]),
+            (predLabels == labels[n_neighbours:]).tolist(),
+        )
+
+    @staticmethod
+    def _get_interleaved_test_train_acc_pred_history(
+        labels, n_neighbours, get_labels_fct, prediction_history, distances_stm
+    ):
+        """
+        Calculates the interleaved test train accuracy incrementally
+        by using the previous predictions.
+        """
+        for i in range(len(prediction_history) + n_neighbours, len(labels)):
+            distances = distances_stm[i, :i]
+            label = get_labels_fct(distances, labels[:i], n_neighbours)[0]
+            prediction_history.append(label == labels[i])
+        return (
+            np.sum(prediction_history) / float(len(prediction_history)),
+            prediction_history,
+        )
+
+    @staticmethod
+    def _adapt_histories(n_deletions, prediction_histories):
+        """
+        Removes predictions of the largest window size and shifts
+        the remaining ones accordingly.
+        """
+        for i in range(n_deletions):
+            sortedKeys = np.sort(list(prediction_histories.keys()))
+            prediction_histories.pop(sortedKeys[0], None)
+            delta = sortedKeys[1]
+            for j in range(1, len(sortedKeys)):
+                prediction_histories[sortedKeys[j] - delta] = prediction_histories.pop(
+                    sortedKeys[j]
+                )
+        return prediction_histories
+
+    @staticmethod
+    def _get_max_acc_window_size(
+        labels,
+        n_neighbours,
+        get_labels_fct,
+        prediction_histories,
+        distances_stm,
+        min_size=50,
+    ):
+        """
+        Returns the window size with the minimum interleaved
+        test-train error (exact calculation).
+        """
+        n_samples = len(labels)
+        if n_samples < 2 * min_size:
+            return n_samples, prediction_histories
+        else:
+            numSamplesRange = [n_samples]
+            while numSamplesRange[-1] / 2 >= min_size:
+                numSamplesRange.append(numSamplesRange[-1] / 2)
+
+            accuracies = []
+            keys_to_remove = []
+            for key in prediction_histories.keys():
+                if key not in (n_samples - np.array(numSamplesRange)):
+                    keys_to_remove.append(key)
+            for key in keys_to_remove:
+                prediction_histories.pop(key, None)
+
+            for numSamplesIt in numSamplesRange:
+                idx = int(n_samples - numSamplesIt)
+                keyset = list(prediction_histories.keys())
+                # if predictionHistories.has_key(idx):
+                if idx in keyset:
+                    (
+                        accuracy,
+                        predHistory,
+                    ) = STMSizer._get_interleaved_test_train_acc_pred_history(
+                        labels[idx:],
+                        n_neighbours,
+                        get_labels_fct,
+                        prediction_histories[idx],
+                        distances_stm[idx:, idx:],
+                    )
+                else:
+                    accuracy, predHistory = STMSizer._get_interleaved_test_train_acc(
+                        labels[idx:],
+                        n_neighbours,
+                        get_labels_fct,
+                        distances_stm[idx:, idx:],
+                    )
+                prediction_histories[idx] = predHistory
+                accuracies.append(accuracy)
+            accuracies = np.round(accuracies, decimals=4)
+            best_n_train_idx = np.argmax(accuracies)
+            window_size = numSamplesRange[best_n_train_idx]  # noqa
+
+            if window_size < n_samples:
+                prediction_histories = STMSizer._adapt_histories(
+                    best_n_train_idx, prediction_histories
+                )
+            return int(window_size), prediction_histories
+
+    @staticmethod
+    def _get_max_acc_approx_window_size(
+        labels,
+        n_neighbours,
+        get_labels_fct,
+        prediction_histories,
+        distances_stm,
+        min_size=50,
+    ):
+        """
+        Returns the window size with the minimum interleaved
+        test-train error (using an approximation).
+        """
+        n_samples = len(labels)
+        if n_samples < 2 * min_size:
+            return n_samples, prediction_histories
+        else:
+            n_samples_range = [n_samples]
+            while n_samples_range[-1] / 2 >= min_size:
+                n_samples_range.append(n_samples_range[-1] / 2)
+            accuracies = []
+            for numSamplesIt in n_samples_range:
+                idx = int(n_samples - numSamplesIt)
+                keyset = list(prediction_histories.keys())
+                # if predictionHistories.has_key(idx):
+                if idx in keyset:
+                    (
+                        accuracy,
+                        predHistory,
+                    ) = STMSizer._get_interleaved_test_train_acc_pred_history(
+                        labels[idx:],
+                        n_neighbours,
+                        get_labels_fct,
+                        prediction_histories[idx],
+                        distances_stm[idx:, idx:],
+                    )
+                # elif predictionHistories.has_key(idx-1):
+                elif idx - 1 in keyset:
+                    predHistory = prediction_histories[idx - 1]
+                    prediction_histories.pop(idx - 1, None)
+                    predHistory.pop(0)
+                    (
+                        accuracy,
+                        predHistory,
+                    ) = STMSizer._get_interleaved_test_train_acc_pred_history(
+                        labels[idx:],
+                        n_neighbours,
+                        get_labels_fct,
+                        predHistory,
+                        distances_stm[idx:, idx:],
+                    )
+                else:
+                    accuracy, predHistory = STMSizer._get_interleaved_test_train_acc(
+                        labels[idx:],
+                        n_neighbours,
+                        get_labels_fct,
+                        distances_stm[idx:, idx:],
+                    )
+                prediction_histories[idx] = predHistory
+                accuracies.append(accuracy)
+            accuracies = np.round(accuracies, decimals=4)
+            best_n_train_idx = np.argmax(accuracies)
+            if best_n_train_idx > 0:
+                moreAccurateIndices = np.where(accuracies > accuracies[0])[0]
+                for i in moreAccurateIndices:
+                    idx = int(n_samples - n_samples_range[i])
+                    accuracy, predHistory = STMSizer._get_interleaved_test_train_acc(
+                        labels[idx:],
+                        n_neighbours,
+                        get_labels_fct,
+                        distances_stm[idx:, idx:],
+                    )
+                    prediction_histories[idx] = predHistory
+                    accuracies[i] = accuracy
+                accuracies = np.round(accuracies, decimals=4)
+                best_n_train_idx = np.argmax(accuracies)
+            window_size = n_samples_range[best_n_train_idx]  # noqa
+
+            if window_size < n_samples:
+                prediction_histories = STMSizer._adapt_histories(
+                    best_n_train_idx, prediction_histories
+                )
+            return int(window_size), prediction_histories
```

### Comparing `river-0.8.0/river/neighbors/src/libNearestNeighbor/nearestNeighbor.cpp` & `river-0.9.0/river/neighbors/src/libNearestNeighbor/nearestNeighbor.cpp`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,389 +1,389 @@
-#include "Python.h"
-#include "numpy/arrayobject.h"
-#include <vector>
-#include <iostream>
-#include <iterator>
-#include <algorithm>
-#include <limits>
-#include <map>
-
-
-using namespace std;
-
-void nArgMin(const int &n, const double* values, int* indices, const int &numValues){
-	for (int i=0; i<n; i++){
-		double minValue = std::numeric_limits<double>::max();
-		for (int j=0; j<numValues; j++){
-			if (values[j] < minValue){
-				bool alreadyUsed = false;
-				for (int k=0; k<i; k++){
-					if (indices[k]==j){
-						alreadyUsed = true;
-					}
-				}
-				if (!alreadyUsed){
-					indices[i] = j;
-					minValue = values[j];
-				}
-			}
-		}
-	}
-}
-void nArgMinN(const int &n, const double* values, int* indices, const int &nRows, const int &numValues){
-	for (int i=0; i<nRows; i++)
-	{
-		nArgMin(n, &values[i*numValues], &indices[i*n], numValues);
-	}
-}
-
-void mostCommon(const int* values, int* result, const int &numValues){
-	map<int,int> dict;
-	for (int i=0; i<numValues; i++)
-	{
-		if (dict.find(values[i])==dict.end()){
-			dict[values[i]] = 1;
-		} else{
-			dict[values[i]]++;
-		}
-	}
-	int maxValue = 0;
-	int maxKey = -1;
-	for(std::map<int,int>::iterator iter = dict.begin(); iter != dict.end(); ++iter)
-	{
-		 if (iter->second > maxValue){
-			 maxValue = iter->second;
-			 maxKey = iter->first;
-		 }
-	}
-	*result = maxKey;
-}
-
-
-void mostCommonN(const int* values, int* result, const int &nRows, const int &numValues){
-	for (int i=0; i<nRows; i++)
-	{
-		mostCommon(&values[i*numValues], &result[i], numValues);
-	}
-}
-
-void linearWeightedLabels(const int* labels, const double* distances, int* result, const int &numValues){
-	map<int,double> dict;
-	for (int i=0; i<numValues; i++)
-	{
-		if (dict.find(labels[i])==dict.end()){
-			dict[labels[i]] = 1/max(distances[i], 0.000000001);
-		} else{
-			dict[labels[i]] += 1/max(distances[i], 0.000000001);
-		}
-	}
-	double maxValue = 0;
-	int maxKey = -1;
-	for(std::map<int,double>::iterator iter = dict.begin(); iter != dict.end(); ++iter)
-	{
-		 if (iter->second > maxValue){
-			 maxValue = iter->second;
-			 maxKey = iter->first;
-		 }
-	}
-	*result = maxKey;
-}
-
-void linearWeightedLabelsN(const int* labels, const double* distances, int* result, const int &nRows, const int &numValues){
-	for (int i=0; i<nRows; i++)
-	{
-		linearWeightedLabels(&labels[i*numValues], &distances[i*numValues], &result[i], numValues);
-	}
-}
-
-double getDistance(const double* sample, const double *sample2, const int &numFeatures)
-{
-	double sum=0;
-	for (int i=0; i<numFeatures; i++)
-	{
-		double diff = sample[i]-sample2[i];
-		sum += diff*diff;
-	}
-	return sum;
-}
-
-void get1ToNDistances(const double* sample, const double* samples, double* distances, const int& numSamples, const int& numFeatures){
-	for (int i=0; i<numSamples; i++){
-		distances[i] = getDistance(sample, &samples[i*numFeatures], numFeatures);
-	}
-}
-
-void getNToNDistances(const double* samples, const double* samples2, double* distances, const int& numSamples, const int& numSamples2, const int& numFeatures){
-	for (int i=0; i<numSamples; i++){
-		get1ToNDistances(&samples[i*numFeatures], samples2, &distances[numSamples2*i], numSamples2, numFeatures);
-	}
-}
-
-
-
-static PyObject *py_getNToNDistances(PyObject *self, PyObject *args) {
-	PyObject *sampleData;
-	PyObject *sampleData2;
-	if (!PyArg_ParseTuple(args, "OO", &sampleData, &sampleData2))
-		return NULL;
-
-	PyArrayObject *matSampleData;
-	matSampleData = (PyArrayObject *) PyArray_ContiguousFromObject(sampleData,
-			PyArray_DOUBLE, 1, 2);
-	double *carrSampleData = (double*) (matSampleData->data);
-	npy_intp sampleDataRows = matSampleData->dimensions[0];
-	npy_intp sampleDataCols = matSampleData->dimensions[1];
-
-	if (matSampleData->nd == 1) {
-		sampleDataCols = sampleDataRows;
-		sampleDataRows = 1;
-	}
-	PyArrayObject *matSampleData2;
-	matSampleData2 = (PyArrayObject *) PyArray_ContiguousFromObject(sampleData2,
-			PyArray_DOUBLE, 1, 2);
-	double *carrSampleData2 = (double*) (matSampleData2->data);
-	npy_intp sampleData2Rows = matSampleData2->dimensions[0];
-
-	if (matSampleData2->nd == 1) {
-		sampleData2Rows = 1;
-	}
-	npy_intp Dims[2];
-	Dims[0]= sampleDataRows;
-	Dims[1] = sampleData2Rows;
-
-	PyArrayObject *distances = (PyArrayObject *) PyArray_SimpleNew(2,
-			Dims, PyArray_DOUBLE);
-	PyArrayObject *matDistances;
-	matDistances = (PyArrayObject *) PyArray_ContiguousFromObject(
-			(PyObject* )distances, PyArray_DOUBLE, 1, 2);
-
-	double *carrDistances = (double*) (matDistances->data);
-
-	getNToNDistances(carrSampleData, carrSampleData2, carrDistances, sampleDataRows, sampleData2Rows, sampleDataCols);
-
-	Py_DECREF(matSampleData);
-	Py_DECREF(matSampleData2);
-	PyObject *result = Py_BuildValue("O", matDistances);
-	Py_DECREF(matDistances);
-	Py_DECREF(distances);
-	return result;
-}
-
-PyDoc_STRVAR(py_getNToNDistances__doc__,
-		"name, data 2D-arr. Returns label result matrix");
-
-
-
-
-static PyObject *py_get1ToNDistances(PyObject *self, PyObject *args) {
-	PyObject *sampleData;
-	PyObject *samplesData;
-	if (!PyArg_ParseTuple(args, "OO", &sampleData, &samplesData))
-		return NULL;
-
-	PyArrayObject *matSampleData;
-	matSampleData = (PyArrayObject *) PyArray_ContiguousFromObject(sampleData,
-			PyArray_DOUBLE, 1, 1);
-	double *carrSampleData = (double*) (matSampleData->data);
-	npy_intp sampleDataRows = matSampleData->dimensions[0];
-	npy_intp sampleDataCols = matSampleData->dimensions[1];
-
-	if (matSampleData->nd == 1) {
-		sampleDataCols = sampleDataRows;
-		sampleDataRows = 1;
-	}
-
-	PyArrayObject *matSamplesData;
-	matSamplesData = (PyArrayObject *) PyArray_ContiguousFromObject(samplesData,
-			PyArray_DOUBLE, 1, 2);
-	double *carrSamplesData = (double*) (matSamplesData->data);
-	npy_intp samplesDataRows = matSamplesData->dimensions[0];
-
-	if (matSamplesData->nd == 1) {
-		samplesDataRows = 1;
-	}
-
-	PyArrayObject *distances = (PyArrayObject *) PyArray_SimpleNew(1,
-			&samplesDataRows, PyArray_DOUBLE);
-	PyArrayObject *matDistances;
-	matDistances = (PyArrayObject *) PyArray_ContiguousFromObject(
-			(PyObject* )distances, PyArray_DOUBLE, 1, 1);
-	double *carrDistances = (double*) (matDistances->data);
-	get1ToNDistances(carrSampleData, carrSamplesData, carrDistances, samplesDataRows, sampleDataCols);
-	Py_DECREF(matSampleData);
-	Py_DECREF(matSamplesData);
-	PyObject *result = Py_BuildValue("O", matDistances);
-	Py_DECREF(matDistances);
-	Py_DECREF(distances);
-	return result;
-}
-
-PyDoc_STRVAR(py_get1ToNDistances__doc__,
-		"name, data 2D-arr. Returns label result matrix");
-
-static PyObject *py_nArgMin(PyObject *self, PyObject *args) {
-	PyObject *values;
-	int n;
-	if (!PyArg_ParseTuple(args, "iO", &n, &values))
-		return NULL;
-
-	//create c-arrays:
-	PyArrayObject *matValues;
-	matValues = (PyArrayObject *) PyArray_ContiguousFromObject(values,
-			PyArray_DOUBLE, 1, 2);
-	double *carrValues = (double*) (matValues->data);
-	npy_intp numRows = matValues->dimensions[0];
-	npy_intp numValues = matValues->dimensions[1];
-
-	if (matValues->nd == 1) {
-		numValues = numRows;
-		numRows = 1;
-	}
-	npy_intp Dims[2];
-	Dims[0]= numRows;
-	Dims[1] = n;
-
-	PyArrayObject *indices = (PyArrayObject *) PyArray_SimpleNew(2,
-			Dims, PyArray_INT);
-	PyArrayObject *matIndices;
-	matIndices = (PyArrayObject *) PyArray_ContiguousFromObject(
-			(PyObject* )indices, PyArray_INT, 1, 2);
-	int *carrIndices = (int*) (matIndices->data);
-	nArgMinN(n, carrValues, carrIndices, numRows, numValues);
-	Py_DECREF(matValues);
-	PyObject *result = Py_BuildValue("O", matIndices);
-	Py_DECREF(matIndices);
-	Py_DECREF(indices);
-	return result;
-}
-
-PyDoc_STRVAR(py_nArgMin__doc__,
-		"name, data 2D-arr. Returns label result matrix");
-
-static PyObject *py_mostCommon(PyObject *self, PyObject *args) {
-	PyObject *values;
-	if (!PyArg_ParseTuple(args, "O", &values))
-		return NULL;
-
-	//create c-arrays:
-	PyArrayObject *matValues;
-	matValues = (PyArrayObject *) PyArray_ContiguousFromObject(values,
-			PyArray_INT, 1, 2);
-	int *carrValues = (int*) (matValues->data);
-	npy_intp numRows = matValues->dimensions[0];
-	npy_intp numValues = matValues->dimensions[1];
-
-	if (matValues->nd == 1) {
-		numValues = numRows;
-		numRows = 1;
-	}
-	PyArrayObject *indices = (PyArrayObject *) PyArray_SimpleNew(1,
-			&numRows, PyArray_INT);
-	PyArrayObject *matIndices;
-	matIndices = (PyArrayObject *) PyArray_ContiguousFromObject(
-			(PyObject* )indices, PyArray_INT, 1, 1);
-	int *carrIndices = (int*) (matIndices->data);
-	mostCommonN(carrValues, carrIndices, numRows, numValues);
-	Py_DECREF(matValues);
-	PyObject *result = Py_BuildValue("O", matIndices);
-	Py_DECREF(matIndices);
-	Py_DECREF(indices);
-	return result;
-}
-PyDoc_STRVAR(py_mostCommon__doc__,
-		"name, data 2D-arr. Returns label result matrix");
-
-static PyObject *py_getLinearWeightedLabels(PyObject *self, PyObject *args) {
-	PyObject *labels, *distances;
-	if (!PyArg_ParseTuple(args, "OO", &labels, &distances))
-		return NULL;
-
-	//create c-arrays:
-	PyArrayObject *matLabels;
-	matLabels = (PyArrayObject *) PyArray_ContiguousFromObject(labels,
-			PyArray_INT, 1, 2);
-	int *carrLabels = (int*) (matLabels->data);
-	npy_intp numRows = matLabels->dimensions[0];
-	npy_intp numValues = matLabels->dimensions[1];
-
-	if (matLabels->nd == 1) {
-		numValues = numRows;
-		numRows = 1;
-	}
-	PyArrayObject *matDistances;
-	matDistances = (PyArrayObject *) PyArray_ContiguousFromObject(distances,
-			PyArray_DOUBLE, 1, 2);
-	double *carrDistances = (double*) (matDistances->data);
-
-	PyArrayObject *resultLabels = (PyArrayObject *) PyArray_SimpleNew(1,
-			&numRows, PyArray_INT);
-	PyArrayObject *matResultLabels;
-	matResultLabels = (PyArrayObject *) PyArray_ContiguousFromObject(
-			(PyObject* )resultLabels, PyArray_INT, 1, 1);
-	int *carrResultLabels = (int*) (matResultLabels->data);
-	linearWeightedLabelsN(carrLabels, carrDistances, carrResultLabels, numRows, numValues);
-	Py_DECREF(matLabels);
-	Py_DECREF(matDistances);
-	PyObject *result = Py_BuildValue("O", matResultLabels);
-	Py_DECREF(matResultLabels);
-	Py_DECREF(resultLabels);
-	return result;
-}
-PyDoc_STRVAR(py_getLinearWeightedLabels__doc__,
-		"name, data 2D-arr. Returns label result matrix");
-
-/* The module doc string */
-PyDoc_STRVAR(ORF__doc__, "Nearest neighbor python interface");
-
-
-
-/* A list of all the methods defined by this module. */
-/* "iterate_point" is the name seen inside of Python */
-/* "py_iterate_point" is the name of the C function handling the Python call */
-/* "METH_VARGS" tells Python how to call the handler */
-/* The {NULL, NULL} entry indicates the end of the method definitions */
-static PyMethodDef NN_methods[] = { {"getNToNDistances", py_getNToNDistances, METH_VARARGS, py_getNToNDistances__doc__},
-		{"get1ToNDistances", py_get1ToNDistances, METH_VARARGS, py_get1ToNDistances__doc__},
-		{"nArgMin", py_nArgMin, METH_VARARGS, py_nArgMin__doc__},
-		{"mostCommon", py_mostCommon, METH_VARARGS, py_mostCommon__doc__},
-		{"getLinearWeightedLabels", py_getLinearWeightedLabels, METH_VARARGS, py_getLinearWeightedLabels__doc__},
-		{ NULL, NULL } /* sentinel */
-};
-
-/* When Python imports a C module named 'X' it loads the module */
-/* then looks for a method named "init"+X and calls it.  Hence */
-/* for the module "mandelbrot" the initialization function is */
-/* "initmandelbrot".  The PyMODINIT_FUNC helps with portability */
-/* across operating systems and between C and C++ compilers */
-PyMODINIT_FUNC
-#if PY_MAJOR_VERSION >= 3
-PyInit_libNearestNeighbor(void)
-#else
-initlibNearestNeighbor(void)
-#endif
-{
-	import_array()
-
-	#if PY_MAJOR_VERSION >= 3
-		static struct PyModuleDef moduledef = {
-			PyModuleDef_HEAD_INIT,
-			"libNearestNeighbor",
-			ORF__doc__,
-			-1,
-			NN_methods,
-			NULL,
-			NULL,
-			NULL,
-			NULL,
-		};
-	#endif
-
-	#if PY_MAJOR_VERSION >= 3
-		return PyModule_Create(&moduledef);
-
-	#else
-		/* There have been several InitModule functions over time */
-		return Py_InitModule3("libNearestNeighbor", NN_methods, ORF__doc__);
-
-	#endif
-
-}
+#include "Python.h"
+#include "numpy/arrayobject.h"
+#include <vector>
+#include <iostream>
+#include <iterator>
+#include <algorithm>
+#include <limits>
+#include <map>
+
+
+using namespace std;
+
+void nArgMin(const int &n, const double* values, int* indices, const int &numValues){
+	for (int i=0; i<n; i++){
+		double minValue = std::numeric_limits<double>::max();
+		for (int j=0; j<numValues; j++){
+			if (values[j] < minValue){
+				bool alreadyUsed = false;
+				for (int k=0; k<i; k++){
+					if (indices[k]==j){
+						alreadyUsed = true;
+					}
+				}
+				if (!alreadyUsed){
+					indices[i] = j;
+					minValue = values[j];
+				}
+			}
+		}
+	}
+}
+void nArgMinN(const int &n, const double* values, int* indices, const int &nRows, const int &numValues){
+	for (int i=0; i<nRows; i++)
+	{
+		nArgMin(n, &values[i*numValues], &indices[i*n], numValues);
+	}
+}
+
+void mostCommon(const int* values, int* result, const int &numValues){
+	map<int,int> dict;
+	for (int i=0; i<numValues; i++)
+	{
+		if (dict.find(values[i])==dict.end()){
+			dict[values[i]] = 1;
+		} else{
+			dict[values[i]]++;
+		}
+	}
+	int maxValue = 0;
+	int maxKey = -1;
+	for(std::map<int,int>::iterator iter = dict.begin(); iter != dict.end(); ++iter)
+	{
+		 if (iter->second > maxValue){
+			 maxValue = iter->second;
+			 maxKey = iter->first;
+		 }
+	}
+	*result = maxKey;
+}
+
+
+void mostCommonN(const int* values, int* result, const int &nRows, const int &numValues){
+	for (int i=0; i<nRows; i++)
+	{
+		mostCommon(&values[i*numValues], &result[i], numValues);
+	}
+}
+
+void linearWeightedLabels(const int* labels, const double* distances, int* result, const int &numValues){
+	map<int,double> dict;
+	for (int i=0; i<numValues; i++)
+	{
+		if (dict.find(labels[i])==dict.end()){
+			dict[labels[i]] = 1/max(distances[i], 0.000000001);
+		} else{
+			dict[labels[i]] += 1/max(distances[i], 0.000000001);
+		}
+	}
+	double maxValue = 0;
+	int maxKey = -1;
+	for(std::map<int,double>::iterator iter = dict.begin(); iter != dict.end(); ++iter)
+	{
+		 if (iter->second > maxValue){
+			 maxValue = iter->second;
+			 maxKey = iter->first;
+		 }
+	}
+	*result = maxKey;
+}
+
+void linearWeightedLabelsN(const int* labels, const double* distances, int* result, const int &nRows, const int &numValues){
+	for (int i=0; i<nRows; i++)
+	{
+		linearWeightedLabels(&labels[i*numValues], &distances[i*numValues], &result[i], numValues);
+	}
+}
+
+double getDistance(const double* sample, const double *sample2, const int &numFeatures)
+{
+	double sum=0;
+	for (int i=0; i<numFeatures; i++)
+	{
+		double diff = sample[i]-sample2[i];
+		sum += diff*diff;
+	}
+	return sum;
+}
+
+void get1ToNDistances(const double* sample, const double* samples, double* distances, const int& numSamples, const int& numFeatures){
+	for (int i=0; i<numSamples; i++){
+		distances[i] = getDistance(sample, &samples[i*numFeatures], numFeatures);
+	}
+}
+
+void getNToNDistances(const double* samples, const double* samples2, double* distances, const int& numSamples, const int& numSamples2, const int& numFeatures){
+	for (int i=0; i<numSamples; i++){
+		get1ToNDistances(&samples[i*numFeatures], samples2, &distances[numSamples2*i], numSamples2, numFeatures);
+	}
+}
+
+
+
+static PyObject *py_getNToNDistances(PyObject *self, PyObject *args) {
+	PyObject *sampleData;
+	PyObject *sampleData2;
+	if (!PyArg_ParseTuple(args, "OO", &sampleData, &sampleData2))
+		return NULL;
+
+	PyArrayObject *matSampleData;
+	matSampleData = (PyArrayObject *) PyArray_ContiguousFromObject(sampleData,
+			PyArray_DOUBLE, 1, 2);
+	double *carrSampleData = (double*) (matSampleData->data);
+	npy_intp sampleDataRows = matSampleData->dimensions[0];
+	npy_intp sampleDataCols = matSampleData->dimensions[1];
+
+	if (matSampleData->nd == 1) {
+		sampleDataCols = sampleDataRows;
+		sampleDataRows = 1;
+	}
+	PyArrayObject *matSampleData2;
+	matSampleData2 = (PyArrayObject *) PyArray_ContiguousFromObject(sampleData2,
+			PyArray_DOUBLE, 1, 2);
+	double *carrSampleData2 = (double*) (matSampleData2->data);
+	npy_intp sampleData2Rows = matSampleData2->dimensions[0];
+
+	if (matSampleData2->nd == 1) {
+		sampleData2Rows = 1;
+	}
+	npy_intp Dims[2];
+	Dims[0]= sampleDataRows;
+	Dims[1] = sampleData2Rows;
+
+	PyArrayObject *distances = (PyArrayObject *) PyArray_SimpleNew(2,
+			Dims, PyArray_DOUBLE);
+	PyArrayObject *matDistances;
+	matDistances = (PyArrayObject *) PyArray_ContiguousFromObject(
+			(PyObject* )distances, PyArray_DOUBLE, 1, 2);
+
+	double *carrDistances = (double*) (matDistances->data);
+
+	getNToNDistances(carrSampleData, carrSampleData2, carrDistances, sampleDataRows, sampleData2Rows, sampleDataCols);
+
+	Py_DECREF(matSampleData);
+	Py_DECREF(matSampleData2);
+	PyObject *result = Py_BuildValue("O", matDistances);
+	Py_DECREF(matDistances);
+	Py_DECREF(distances);
+	return result;
+}
+
+PyDoc_STRVAR(py_getNToNDistances__doc__,
+		"name, data 2D-arr. Returns label result matrix");
+
+
+
+
+static PyObject *py_get1ToNDistances(PyObject *self, PyObject *args) {
+	PyObject *sampleData;
+	PyObject *samplesData;
+	if (!PyArg_ParseTuple(args, "OO", &sampleData, &samplesData))
+		return NULL;
+
+	PyArrayObject *matSampleData;
+	matSampleData = (PyArrayObject *) PyArray_ContiguousFromObject(sampleData,
+			PyArray_DOUBLE, 1, 1);
+	double *carrSampleData = (double*) (matSampleData->data);
+	npy_intp sampleDataRows = matSampleData->dimensions[0];
+	npy_intp sampleDataCols = matSampleData->dimensions[1];
+
+	if (matSampleData->nd == 1) {
+		sampleDataCols = sampleDataRows;
+		sampleDataRows = 1;
+	}
+
+	PyArrayObject *matSamplesData;
+	matSamplesData = (PyArrayObject *) PyArray_ContiguousFromObject(samplesData,
+			PyArray_DOUBLE, 1, 2);
+	double *carrSamplesData = (double*) (matSamplesData->data);
+	npy_intp samplesDataRows = matSamplesData->dimensions[0];
+
+	if (matSamplesData->nd == 1) {
+		samplesDataRows = 1;
+	}
+
+	PyArrayObject *distances = (PyArrayObject *) PyArray_SimpleNew(1,
+			&samplesDataRows, PyArray_DOUBLE);
+	PyArrayObject *matDistances;
+	matDistances = (PyArrayObject *) PyArray_ContiguousFromObject(
+			(PyObject* )distances, PyArray_DOUBLE, 1, 1);
+	double *carrDistances = (double*) (matDistances->data);
+	get1ToNDistances(carrSampleData, carrSamplesData, carrDistances, samplesDataRows, sampleDataCols);
+	Py_DECREF(matSampleData);
+	Py_DECREF(matSamplesData);
+	PyObject *result = Py_BuildValue("O", matDistances);
+	Py_DECREF(matDistances);
+	Py_DECREF(distances);
+	return result;
+}
+
+PyDoc_STRVAR(py_get1ToNDistances__doc__,
+		"name, data 2D-arr. Returns label result matrix");
+
+static PyObject *py_nArgMin(PyObject *self, PyObject *args) {
+	PyObject *values;
+	int n;
+	if (!PyArg_ParseTuple(args, "iO", &n, &values))
+		return NULL;
+
+	//create c-arrays:
+	PyArrayObject *matValues;
+	matValues = (PyArrayObject *) PyArray_ContiguousFromObject(values,
+			PyArray_DOUBLE, 1, 2);
+	double *carrValues = (double*) (matValues->data);
+	npy_intp numRows = matValues->dimensions[0];
+	npy_intp numValues = matValues->dimensions[1];
+
+	if (matValues->nd == 1) {
+		numValues = numRows;
+		numRows = 1;
+	}
+	npy_intp Dims[2];
+	Dims[0]= numRows;
+	Dims[1] = n;
+
+	PyArrayObject *indices = (PyArrayObject *) PyArray_SimpleNew(2,
+			Dims, PyArray_INT);
+	PyArrayObject *matIndices;
+	matIndices = (PyArrayObject *) PyArray_ContiguousFromObject(
+			(PyObject* )indices, PyArray_INT, 1, 2);
+	int *carrIndices = (int*) (matIndices->data);
+	nArgMinN(n, carrValues, carrIndices, numRows, numValues);
+	Py_DECREF(matValues);
+	PyObject *result = Py_BuildValue("O", matIndices);
+	Py_DECREF(matIndices);
+	Py_DECREF(indices);
+	return result;
+}
+
+PyDoc_STRVAR(py_nArgMin__doc__,
+		"name, data 2D-arr. Returns label result matrix");
+
+static PyObject *py_mostCommon(PyObject *self, PyObject *args) {
+	PyObject *values;
+	if (!PyArg_ParseTuple(args, "O", &values))
+		return NULL;
+
+	//create c-arrays:
+	PyArrayObject *matValues;
+	matValues = (PyArrayObject *) PyArray_ContiguousFromObject(values,
+			PyArray_INT, 1, 2);
+	int *carrValues = (int*) (matValues->data);
+	npy_intp numRows = matValues->dimensions[0];
+	npy_intp numValues = matValues->dimensions[1];
+
+	if (matValues->nd == 1) {
+		numValues = numRows;
+		numRows = 1;
+	}
+	PyArrayObject *indices = (PyArrayObject *) PyArray_SimpleNew(1,
+			&numRows, PyArray_INT);
+	PyArrayObject *matIndices;
+	matIndices = (PyArrayObject *) PyArray_ContiguousFromObject(
+			(PyObject* )indices, PyArray_INT, 1, 1);
+	int *carrIndices = (int*) (matIndices->data);
+	mostCommonN(carrValues, carrIndices, numRows, numValues);
+	Py_DECREF(matValues);
+	PyObject *result = Py_BuildValue("O", matIndices);
+	Py_DECREF(matIndices);
+	Py_DECREF(indices);
+	return result;
+}
+PyDoc_STRVAR(py_mostCommon__doc__,
+		"name, data 2D-arr. Returns label result matrix");
+
+static PyObject *py_getLinearWeightedLabels(PyObject *self, PyObject *args) {
+	PyObject *labels, *distances;
+	if (!PyArg_ParseTuple(args, "OO", &labels, &distances))
+		return NULL;
+
+	//create c-arrays:
+	PyArrayObject *matLabels;
+	matLabels = (PyArrayObject *) PyArray_ContiguousFromObject(labels,
+			PyArray_INT, 1, 2);
+	int *carrLabels = (int*) (matLabels->data);
+	npy_intp numRows = matLabels->dimensions[0];
+	npy_intp numValues = matLabels->dimensions[1];
+
+	if (matLabels->nd == 1) {
+		numValues = numRows;
+		numRows = 1;
+	}
+	PyArrayObject *matDistances;
+	matDistances = (PyArrayObject *) PyArray_ContiguousFromObject(distances,
+			PyArray_DOUBLE, 1, 2);
+	double *carrDistances = (double*) (matDistances->data);
+
+	PyArrayObject *resultLabels = (PyArrayObject *) PyArray_SimpleNew(1,
+			&numRows, PyArray_INT);
+	PyArrayObject *matResultLabels;
+	matResultLabels = (PyArrayObject *) PyArray_ContiguousFromObject(
+			(PyObject* )resultLabels, PyArray_INT, 1, 1);
+	int *carrResultLabels = (int*) (matResultLabels->data);
+	linearWeightedLabelsN(carrLabels, carrDistances, carrResultLabels, numRows, numValues);
+	Py_DECREF(matLabels);
+	Py_DECREF(matDistances);
+	PyObject *result = Py_BuildValue("O", matResultLabels);
+	Py_DECREF(matResultLabels);
+	Py_DECREF(resultLabels);
+	return result;
+}
+PyDoc_STRVAR(py_getLinearWeightedLabels__doc__,
+		"name, data 2D-arr. Returns label result matrix");
+
+/* The module doc string */
+PyDoc_STRVAR(ORF__doc__, "Nearest neighbor python interface");
+
+
+
+/* A list of all the methods defined by this module. */
+/* "iterate_point" is the name seen inside of Python */
+/* "py_iterate_point" is the name of the C function handling the Python call */
+/* "METH_VARGS" tells Python how to call the handler */
+/* The {NULL, NULL} entry indicates the end of the method definitions */
+static PyMethodDef NN_methods[] = { {"getNToNDistances", py_getNToNDistances, METH_VARARGS, py_getNToNDistances__doc__},
+		{"get1ToNDistances", py_get1ToNDistances, METH_VARARGS, py_get1ToNDistances__doc__},
+		{"nArgMin", py_nArgMin, METH_VARARGS, py_nArgMin__doc__},
+		{"mostCommon", py_mostCommon, METH_VARARGS, py_mostCommon__doc__},
+		{"getLinearWeightedLabels", py_getLinearWeightedLabels, METH_VARARGS, py_getLinearWeightedLabels__doc__},
+		{ NULL, NULL } /* sentinel */
+};
+
+/* When Python imports a C module named 'X' it loads the module */
+/* then looks for a method named "init"+X and calls it.  Hence */
+/* for the module "mandelbrot" the initialization function is */
+/* "initmandelbrot".  The PyMODINIT_FUNC helps with portability */
+/* across operating systems and between C and C++ compilers */
+PyMODINIT_FUNC
+#if PY_MAJOR_VERSION >= 3
+PyInit_libNearestNeighbor(void)
+#else
+initlibNearestNeighbor(void)
+#endif
+{
+	import_array()
+
+	#if PY_MAJOR_VERSION >= 3
+		static struct PyModuleDef moduledef = {
+			PyModuleDef_HEAD_INIT,
+			"libNearestNeighbor",
+			ORF__doc__,
+			-1,
+			NN_methods,
+			NULL,
+			NULL,
+			NULL,
+			NULL,
+		};
+	#endif
+
+	#if PY_MAJOR_VERSION >= 3
+		return PyModule_Create(&moduledef);
+
+	#else
+		/* There have been several InitModule functions over time */
+		return Py_InitModule3("libNearestNeighbor", NN_methods, ORF__doc__);
+
+	#endif
+
+}
```

### Comparing `river-0.8.0/river/neural_net/activations.py` & `river-0.9.0/river/neural_net/activations.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,65 +1,65 @@
-import abc
-
-import numpy as np
-
-__all__ = ["ReLU", "Sigmoid", "Identity"]
-
-
-class Activation:
-    """An activation function.
-
-    Each activation function is represented by a class and implements two methods. The first method
-    is `apply`, which evaluates the activation on an array. The second method is `gradient`, which
-    computes the gradient with respect to the input array. Both methods are intended to be pure
-    with no side-effects. In other words they do not modify their inputs.
-
-    """
-
-    @abc.abstractstaticmethod
-    def apply(self, z):
-        """Apply the activation function to a layer output z."""
-
-    @abc.abstractstaticmethod
-    def gradient(self, z):
-        """Return the gradient with respect to a layer output z."""
-
-
-class ReLU(Activation):
-    """Rectified Linear Unit (ReLU) activation function."""
-
-    @staticmethod
-    def apply(z):
-        a = np.copy(z)
-        a[a < 0] = 0
-        return a
-
-    @staticmethod
-    def gradient(z):
-        a = np.zeros_like(z, dtype=z.dtype)
-        a[z > 0] = 1
-        return a
-
-
-class Sigmoid(Activation):
-    """Sigmoid activation function."""
-
-    @staticmethod
-    def apply(z):
-        return 1 / (1 + np.exp(-z))
-
-    @staticmethod
-    def gradient(z):
-        s = Sigmoid.apply(z)
-        return s * (1 - s)
-
-
-class Identity(Activation):
-    """Identity activation function."""
-
-    @staticmethod
-    def apply(z):
-        return np.copy(z)
-
-    @staticmethod
-    def gradient(z):
-        return np.ones_like(z, dtype=z.dtype)
+import abc
+
+import numpy as np
+
+__all__ = ["ReLU", "Sigmoid", "Identity"]
+
+
+class Activation:
+    """An activation function.
+
+    Each activation function is represented by a class and implements two methods. The first method
+    is `apply`, which evaluates the activation on an array. The second method is `gradient`, which
+    computes the gradient with respect to the input array. Both methods are intended to be pure
+    with no side-effects. In other words they do not modify their inputs.
+
+    """
+
+    @abc.abstractstaticmethod
+    def apply(self, z):
+        """Apply the activation function to a layer output z."""
+
+    @abc.abstractstaticmethod
+    def gradient(self, z):
+        """Return the gradient with respect to a layer output z."""
+
+
+class ReLU(Activation):
+    """Rectified Linear Unit (ReLU) activation function."""
+
+    @staticmethod
+    def apply(z):
+        a = np.copy(z)
+        a[a < 0] = 0
+        return a
+
+    @staticmethod
+    def gradient(z):
+        a = np.zeros_like(z, dtype=z.dtype)
+        a[z > 0] = 1
+        return a
+
+
+class Sigmoid(Activation):
+    """Sigmoid activation function."""
+
+    @staticmethod
+    def apply(z):
+        return 1 / (1 + np.exp(-z))
+
+    @staticmethod
+    def gradient(z):
+        s = Sigmoid.apply(z)
+        return s * (1 - s)
+
+
+class Identity(Activation):
+    """Identity activation function."""
+
+    @staticmethod
+    def apply(z):
+        return np.copy(z)
+
+    @staticmethod
+    def gradient(z):
+        return np.ones_like(z, dtype=z.dtype)
```

### Comparing `river-0.8.0/river/neural_net/mlp.py` & `river-0.9.0/river/neural_net/mlp.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,323 +1,326 @@
-import typing
-
-import numpy as np
-import pandas as pd
-
-from river import base, optim
-
-__all__ = ["MLPRegressor"]
-
-
-def xavier_init(dims: typing.Tuple[int], seed: int = None):
-    """Xavier weight initialization.
-
-    References
-    ----------
-    [^1]: Glorot, X. and Bengio, Y., 2010, March. Understanding the difficulty of training deep
-        feedforward neural networks. In Proceedings of the thirteenth international conference on
-        artificial intelligence and statistics (pp. 249-256).
-
-    """
-
-    rng = np.random.RandomState(seed)
-
-    w = {}  # weights
-    b = {}  # biases
-
-    for i in range(len(dims) - 1):
-        w[i + 1] = rng.randn(dims[i], dims[i + 1]) / np.sqrt(dims[i])
-        b[i + 1] = np.zeros(dims[i + 1], dtype=w[i + 1].dtype)
-
-    return w, b
-
-
-class MLP:
-    """Multi-layer Perceptron.
-
-    The `_forward` method propagates input forward through the network and stores the outputs
-    of each layer along the way. The `_backward` method then goes through each layer in reverse
-    order and updates the weights within each layer.
-
-    """
-
-    def __init__(
-        self,
-        hidden_dims: typing.Tuple[int],
-        activations,
-        loss: optim.losses.Loss,
-        optimizer: optim.Optimizer,
-        seed: int = None,
-    ):
-
-        self.activations = activations
-        self.hidden_dims = hidden_dims
-        self.loss = loss
-        self.optimizer = optimizer
-        self.seed = seed
-
-    @property
-    def n_layers(self) -> int:
-        """Return the number of layers in the network.
-
-        The number of layers is equal to the number of hidden layers plus 2. The 2 accounts for the
-        input layer and the output layer.
-
-        """
-        return len(self.hidden_dims) + 2
-
-    def _forward(self, X: pd.DataFrame) -> (dict, dict):
-        """Execute a forward pass through the neural network.
-
-        Parameters
-        ----------
-        X
-            A DataFrame of shape (batch_size, n_features).
-
-        Returns
-        -------
-        The output of each layer.
-        The output of each activation function.
-
-        """
-
-        z = {}  # z = w(x) + b
-        a = {}  # a = f(z)
-
-        # As a convention, we sort the columns of the input DataFrame. This allows the user to
-        # pass DataFrames with different column orders without having to worry.
-        a[1] = X.values[:, np.argsort(X.columns.to_numpy())]
-
-        for i in range(2, self.n_layers + 1):
-            z[i] = np.dot(a[i - 1], self.w[i - 1]) + self.b[i - 1]
-            a[i] = self.activations[i - 2].apply(z[i])
-
-        return z, a
-
-    def _backward(self, z: dict, a: dict, y: pd.DataFrame):
-        """Execute a backward pass through the neural network, aka. backpropagation.
-
-        Parameters
-        ----------
-        z
-            The output of each layer.
-        a
-            The output of each activation function.
-        y
-            A DataFrame of shape (batch_size, n_targets).
-
-        """
-
-        y = y.values[:, np.argsort(y.columns.to_numpy())]
-
-        # Determine the partial derivative and delta for the output layer
-        y_pred = a[self.n_layers]
-        final_activation = self.activations[self.n_layers - 2]
-        delta = self.loss.gradient(y, y_pred) * final_activation.gradient(y_pred)
-        dw = np.dot(a[self.n_layers - 1].T, delta)
-
-        update_params = {self.n_layers - 1: (dw, delta)}
-
-        # Go through the layers in reverse order
-        for i in range(self.n_layers - 2, 0, -1):
-            delta = np.dot(delta, self.w[i + 1].T) * self.activations[i - 1].gradient(
-                z[i + 1]
-            )
-            dw = np.dot(a[i].T, delta)
-            update_params[i] = (dw, delta)
-
-        # Update the parameters
-        for i, (dw, delta) in update_params.items():
-            self.optimizer.step(w=self.w[i], g=dw)
-            self.optimizer.step(w=self.b[i], g=np.mean(delta, axis=0))
-
-    def learn_many(self, X: pd.DataFrame, y: pd.DataFrame):
-        """Train the network.
-
-        Parameters
-        ----------
-        X
-            A DataFrame of shape (batch_size, n_features).
-        y
-            A DataFrame of shape (batch_size, n_targets).
-
-        """
-
-        # We expect y to be 2D, even if there is only one target to predict
-        if isinstance(y, pd.Series):
-            y = y.to_frame()
-
-        # The weights need initializing during the first call to partial_fit
-        if not hasattr(self, "w"):
-            self.w, self.b = xavier_init(
-                dims=(X.shape[1], *self.hidden_dims, y.shape[1]), seed=self.seed
-            )
-            self.features = X.columns.to_numpy()
-            self.features.sort()
-            self.targets = y.columns.to_numpy()
-
-        z, a = self._forward(X)
-        self._backward(z, a, y)
-
-        return self
-
-    def __call__(self, X: pd.DataFrame):
-        """Make predictions.
-
-        Parameters
-        ----------
-        X
-            A DataFrame of shape (batch_size, n_features).
-
-        """
-        _, a = self._forward(X)
-        y_pred = a[self.n_layers]
-        return pd.DataFrame(y_pred, columns=self.targets, index=X.index)
-
-
-class MLPRegressor(base.Regressor, MLP):
-    """Multi-layer Perceptron for regression.
-
-    This model is still work in progress. Here are some features that still need implementing:
-
-    - `learn_one` and `predict_one` just cast the input `dict` to a single row dataframe and then
-        call `learn_many` and `predict_many` respectively. This is very inefficient.
-    - Not all of the optimizers in the `optim` module can be used as they are not all vectorised.
-    - Emerging and disappearing features are not supported. Each instance/batch has to have the
-        same features.
-    - The gradient haven't been numerically checked.
-
-    Parameters
-    ----------
-    hidden_dims
-        The dimensions of the hidden layers. For example, specifying `(10, 20)` means that there
-        are two hidden layers with 10 and 20 neurons, respectively. Note that the number of layers
-        the network contains is equal to the number of hidden layers plus two (to account for the
-        input and output layers).
-    activations
-        The activation functions to use at each layer, including the input and output layers.
-        Therefore you need to specify three activation if you specify one hidden layer.
-    loss
-        Loss function. Defaults to `optim.losses.Squared`.
-    optimizer
-        Optimizer. Defaults to `optim.SGD(.01)`.
-    seed
-        Random number generation seed. Set this for reproducibility.
-
-    Examples
-    --------
-
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import neural_net as nn
-    >>> from river import optim
-    >>> from river import preprocessing as pp
-    >>> from river import metrics
-
-    >>> model = (
-    ...     pp.StandardScaler() |
-    ...     nn.MLPRegressor(
-    ...         hidden_dims=(5,),
-    ...         activations=(
-    ...             nn.activations.ReLU,
-    ...             nn.activations.ReLU,
-    ...             nn.activations.Identity
-    ...         ),
-    ...         optimizer=optim.SGD(1e-3),
-    ...         seed=42
-    ...     )
-    ... )
-
-    >>> dataset = datasets.TrumpApproval()
-
-    >>> metric = metrics.MAE()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    MAE: 1.589827
-
-    You can also use this to process mini-batches of data.
-
-    >>> model = (
-    ...     pp.StandardScaler() |
-    ...     nn.MLPRegressor(
-    ...         hidden_dims=(10,),
-    ...         activations=(
-    ...             nn.activations.ReLU,
-    ...             nn.activations.ReLU,
-    ...             nn.activations.ReLU
-    ...         ),
-    ...         optimizer=optim.SGD(1e-4),
-    ...         seed=42
-    ...     )
-    ... )
-
-    >>> dataset = datasets.TrumpApproval()
-    >>> batch_size = 32
-
-    >>> for epoch in range(10):
-    ...     for xb in pd.read_csv(dataset.path, chunksize=batch_size):
-    ...         yb = xb.pop('five_thirty_eight')
-    ...         y_pred = model.predict_many(xb)
-    ...         model = model.learn_many(xb, yb)
-
-    >>> model.predict_many(xb)
-          five_thirty_eight
-    992           39.361609
-    993           46.398536
-    994           42.094086
-    995           40.195802
-    996           40.782954
-    997           40.839678
-    998           40.896403
-    999           48.362659
-    1000          42.021849
-
-    """
-
-    def __init__(
-        self,
-        hidden_dims,
-        activations,
-        loss: optim.losses.Loss = None,
-        optimizer: optim.Optimizer = None,
-        seed: int = None,
-    ):
-        super().__init__(
-            hidden_dims=hidden_dims,
-            activations=activations,
-            loss=loss or optim.losses.Squared(),
-            optimizer=optimizer or optim.SGD(0.01),
-            seed=seed,
-        )
-
-    @classmethod
-    def _default_params(self):
-        from . import activations
-
-        return {
-            "hidden_dims": (20,),
-            "activations": (activations.ReLU, activations.ReLU, activations.Identity),
-        }
-
-    def predict_many(self, X):
-        if not hasattr(self, "w"):
-            return pd.DataFrame({0: 0}, index=X.index)
-        return self(X)
-
-    def learn_one(self, x, y):
-
-        # Multi-output
-        if isinstance(y, dict):
-            return self.learn_many(X=pd.DataFrame([x]), y=pd.DataFrame([y]))
-
-        # Single output
-        return self.learn_many(X=pd.DataFrame([x]), y=pd.Series([y]))
-
-    def predict_one(self, x):
-        y_pred = self.predict_many(X=pd.DataFrame([x]))
-
-        # Multi-output
-        if len(y_pred.columns) > 1:
-            return y_pred.iloc[0].to_dict()
-
-        # Single output
-        return y_pred.iloc[0, 0]
+import collections
+import copy
+import typing
+
+import numpy as np
+import pandas as pd
+
+from river import base, optim
+
+__all__ = ["MLPRegressor"]
+
+
+def xavier_init(dims: typing.Tuple[int], seed: int = None):
+    """Xavier weight initialization.
+
+    References
+    ----------
+    [^1]: Glorot, X. and Bengio, Y., 2010, March. Understanding the difficulty of training deep
+        feedforward neural networks. In Proceedings of the thirteenth international conference on
+        artificial intelligence and statistics (pp. 249-256).
+
+    """
+
+    rng = np.random.RandomState(seed)
+
+    w = {}  # weights
+    b = {}  # biases
+
+    for i in range(len(dims) - 1):
+        w[i + 1] = rng.randn(dims[i], dims[i + 1]) / np.sqrt(dims[i])
+        b[i + 1] = np.zeros(dims[i + 1], dtype=w[i + 1].dtype)
+
+    return w, b
+
+
+class MLP:
+    """Multi-layer Perceptron.
+
+    The `_forward` method propagates input forward through the network and stores the outputs
+    of each layer along the way. The `_backward` method then goes through each layer in reverse
+    order and updates the weights within each layer.
+
+    """
+
+    def __init__(
+        self,
+        hidden_dims: typing.Tuple[int],
+        activations,
+        loss: optim.losses.Loss,
+        optimizer: optim.Optimizer,
+        seed: int = None,
+    ):
+
+        self.activations = activations
+        self.hidden_dims = hidden_dims
+        self.loss = loss
+        self.optimizer = optimizer
+        self.seed = seed
+        self._optimizers = collections.defaultdict(lambda: copy.deepcopy(optimizer))
+
+    @property
+    def n_layers(self) -> int:
+        """Return the number of layers in the network.
+
+        The number of layers is equal to the number of hidden layers plus 2. The 2 accounts for the
+        input layer and the output layer.
+
+        """
+        return len(self.hidden_dims) + 2
+
+    def _forward(self, X: pd.DataFrame) -> (dict, dict):
+        """Execute a forward pass through the neural network.
+
+        Parameters
+        ----------
+        X
+            A DataFrame of shape (batch_size, n_features).
+
+        Returns
+        -------
+        The output of each layer.
+        The output of each activation function.
+
+        """
+
+        z = {}  # z = w(x) + b
+        a = {}  # a = f(z)
+
+        # As a convention, we sort the columns of the input DataFrame. This allows the user to
+        # pass DataFrames with different column orders without having to worry.
+        a[1] = X.values[:, np.argsort(X.columns.to_numpy())]
+
+        for i in range(2, self.n_layers + 1):
+            z[i] = np.dot(a[i - 1], self.w[i - 1]) + self.b[i - 1]
+            a[i] = self.activations[i - 2].apply(z[i])
+
+        return z, a
+
+    def _backward(self, z: dict, a: dict, y: pd.DataFrame):
+        """Execute a backward pass through the neural network, aka. backpropagation.
+
+        Parameters
+        ----------
+        z
+            The output of each layer.
+        a
+            The output of each activation function.
+        y
+            A DataFrame of shape (batch_size, n_targets).
+
+        """
+
+        y = y.values[:, np.argsort(y.columns.to_numpy())]
+
+        # Determine the partial derivative and delta for the output layer
+        y_pred = a[self.n_layers]
+        final_activation = self.activations[self.n_layers - 2]
+        delta = self.loss.gradient(y, y_pred) * final_activation.gradient(y_pred)
+        dw = np.dot(a[self.n_layers - 1].T, delta)
+
+        update_params = {self.n_layers - 1: (dw, delta)}
+
+        # Go through the layers in reverse order
+        for i in range(self.n_layers - 2, 0, -1):
+            delta = np.dot(delta, self.w[i + 1].T) * self.activations[i - 1].gradient(
+                z[i + 1]
+            )
+            dw = np.dot(a[i].T, delta)
+            update_params[i] = (dw, delta)
+
+        # Update the parameters
+        for i, (dw, delta) in update_params.items():
+            self._optimizers[i, 0].step(w=self.w[i], g=dw)
+            self._optimizers[i, 1].step(w=self.b[i], g=np.mean(delta, axis=0))
+
+    def learn_many(self, X: pd.DataFrame, y: pd.DataFrame):
+        """Train the network.
+
+        Parameters
+        ----------
+        X
+            A DataFrame of shape (batch_size, n_features).
+        y
+            A DataFrame of shape (batch_size, n_targets).
+
+        """
+
+        # We expect y to be 2D, even if there is only one target to predict
+        if isinstance(y, pd.Series):
+            y = y.to_frame()
+
+        # The weights need initializing during the first call to partial_fit
+        if not hasattr(self, "w"):
+            self.w, self.b = xavier_init(
+                dims=(X.shape[1], *self.hidden_dims, y.shape[1]), seed=self.seed
+            )
+            self.features = X.columns.to_numpy()
+            self.features.sort()
+            self.targets = y.columns.to_numpy()
+
+        z, a = self._forward(X)
+        self._backward(z, a, y)
+
+        return self
+
+    def __call__(self, X: pd.DataFrame):
+        """Make predictions.
+
+        Parameters
+        ----------
+        X
+            A DataFrame of shape (batch_size, n_features).
+
+        """
+        _, a = self._forward(X)
+        y_pred = a[self.n_layers]
+        return pd.DataFrame(y_pred, columns=self.targets, index=X.index)
+
+
+class MLPRegressor(base.Regressor, MLP):
+    """Multi-layer Perceptron for regression.
+
+    This model is still work in progress. Here are some features that still need implementing:
+
+    - `learn_one` and `predict_one` just cast the input `dict` to a single row dataframe and then
+        call `learn_many` and `predict_many` respectively. This is very inefficient.
+    - Not all of the optimizers in the `optim` module can be used as they are not all vectorised.
+    - Emerging and disappearing features are not supported. Each instance/batch has to have the
+        same features.
+    - The gradient haven't been numerically checked.
+
+    Parameters
+    ----------
+    hidden_dims
+        The dimensions of the hidden layers. For example, specifying `(10, 20)` means that there
+        are two hidden layers with 10 and 20 neurons, respectively. Note that the number of layers
+        the network contains is equal to the number of hidden layers plus two (to account for the
+        input and output layers).
+    activations
+        The activation functions to use at each layer, including the input and output layers.
+        Therefore you need to specify three activation if you specify one hidden layer.
+    loss
+        Loss function. Defaults to `optim.losses.Squared`.
+    optimizer
+        Optimizer. Defaults to `optim.SGD(.01)`.
+    seed
+        Random number generation seed. Set this for reproducibility.
+
+    Examples
+    --------
+
+    >>> from river import datasets
+    >>> from river import evaluate
+    >>> from river import neural_net as nn
+    >>> from river import optim
+    >>> from river import preprocessing as pp
+    >>> from river import metrics
+
+    >>> model = (
+    ...     pp.StandardScaler() |
+    ...     nn.MLPRegressor(
+    ...         hidden_dims=(5,),
+    ...         activations=(
+    ...             nn.activations.ReLU,
+    ...             nn.activations.ReLU,
+    ...             nn.activations.Identity
+    ...         ),
+    ...         optimizer=optim.SGD(1e-3),
+    ...         seed=42
+    ...     )
+    ... )
+
+    >>> dataset = datasets.TrumpApproval()
+
+    >>> metric = metrics.MAE()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    MAE: 1.589827
+
+    You can also use this to process mini-batches of data.
+
+    >>> model = (
+    ...     pp.StandardScaler() |
+    ...     nn.MLPRegressor(
+    ...         hidden_dims=(10,),
+    ...         activations=(
+    ...             nn.activations.ReLU,
+    ...             nn.activations.ReLU,
+    ...             nn.activations.ReLU
+    ...         ),
+    ...         optimizer=optim.SGD(1e-4),
+    ...         seed=42
+    ...     )
+    ... )
+
+    >>> dataset = datasets.TrumpApproval()
+    >>> batch_size = 32
+
+    >>> for epoch in range(10):
+    ...     for xb in pd.read_csv(dataset.path, chunksize=batch_size):
+    ...         yb = xb.pop('five_thirty_eight')
+    ...         y_pred = model.predict_many(xb)
+    ...         model = model.learn_many(xb, yb)
+
+    >>> model.predict_many(xb)
+          five_thirty_eight
+    992           39.361609
+    993           46.398536
+    994           42.094086
+    995           40.195802
+    996           40.782954
+    997           40.839678
+    998           40.896403
+    999           48.362659
+    1000          42.021849
+
+    """
+
+    def __init__(
+        self,
+        hidden_dims,
+        activations,
+        loss: optim.losses.Loss = None,
+        optimizer: optim.Optimizer = None,
+        seed: int = None,
+    ):
+        super().__init__(
+            hidden_dims=hidden_dims,
+            activations=activations,
+            loss=loss or optim.losses.Squared(),
+            optimizer=optimizer or optim.SGD(0.01),
+            seed=seed,
+        )
+
+    @classmethod
+    def _default_params(self):
+        from . import activations
+
+        return {
+            "hidden_dims": (20,),
+            "activations": (activations.ReLU, activations.ReLU, activations.Identity),
+        }
+
+    def predict_many(self, X):
+        if not hasattr(self, "w"):
+            return pd.DataFrame({0: 0}, index=X.index)
+        return self(X)
+
+    def learn_one(self, x, y):
+
+        # Multi-output
+        if isinstance(y, dict):
+            return self.learn_many(X=pd.DataFrame([x]), y=pd.DataFrame([y]))
+
+        # Single output
+        return self.learn_many(X=pd.DataFrame([x]), y=pd.Series([y]))
+
+    def predict_one(self, x):
+        y_pred = self.predict_many(X=pd.DataFrame([x]))
+
+        # Multi-output
+        if len(y_pred.columns) > 1:
+            return y_pred.iloc[0].to_dict()
+
+        # Single output
+        return y_pred.iloc[0, 0]
```

### Comparing `river-0.8.0/river/optim/ada_bound.py` & `river-0.9.0/river/optim/ada_max.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,101 +1,71 @@
-import collections
-import math
-import numbers
-
-from .. import utils
-from . import base
-
-__all__ = ["AdaBound"]
-
-
-class AdaBound(base.Optimizer):
-    """AdaBound optimizer.
-
-    Parameters
-    ----------
-    lr
-        The learning rate.
-    beta_1
-    beta_2
-    eps
-    gamma
-    final_lr
-
-    Attributes
-    ----------
-    m : collections.defaultdict
-    s : collections.defaultdict
-
-    Examples
-    --------
-
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import linear_model
-    >>> from river import metrics
-    >>> from river import optim
-    >>> from river import preprocessing
-
-    >>> dataset = datasets.Phishing()
-    >>> optimizer = optim.AdaBound()
-    >>> model = (
-    ...     preprocessing.StandardScaler() |
-    ...     linear_model.LogisticRegression(optimizer)
-    ... )
-    >>> metric = metrics.F1()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    F1: 0.879004
-
-    References
-    ----------
-    [^1]: [Luo, L., Xiong, Y., Liu, Y. and Sun, X., 2019. Adaptive gradient methods with dynamic bound of learning rate. arXiv preprint arXiv:1902.09843](https://arxiv.org/abs/1902.09843)
-
-    """
-
-    def __init__(
-        self, lr=1e-3, beta_1=0.9, beta_2=0.999, eps=1e-8, gamma=1e-3, final_lr=0.1
-    ):
-
-        if not isinstance(lr, numbers.Number):
-            raise ValueError(f"lr in AdaBound should be numeric but got {type(lr)}")
-
-        if not isinstance(final_lr, numbers.Number):
-            raise ValueError(
-                f"final_lr in AdaBound should be numeric but got {type(final_lr)}"
-            )
-
-        super().__init__(lr)
-        self.base_lr = lr
-        self.final_lr = final_lr
-        self.beta_1 = beta_1
-        self.beta_2 = beta_2
-        self.eps = eps
-        self.gamma = gamma
-        self.m = collections.defaultdict(float)
-        self.v = collections.defaultdict(float)
-
-    def _step(self, w, g):
-
-        bias_1 = 1 - self.beta_1 ** (self.n_iterations + 1)
-        bias_2 = 1 - self.beta_2 ** (self.n_iterations + 1)
-
-        step_size = self.learning_rate * math.sqrt(bias_2) / bias_1
-        self.final_lr *= self.learning_rate / self.base_lr
-
-        lower_bound = self.final_lr * (
-            1 - 1 / (self.gamma * (self.n_iterations + 1) + 1)
-        )
-        upper_bound = self.final_lr * (1 + 1 / (self.gamma * (self.n_iterations + 1)))
-
-        for i, gi in g.items():
-            self.m[i] = self.beta_1 * self.m[i] + (1 - self.beta_1) * gi
-            self.v[i] = self.beta_2 * self.v[i] + (1 - self.beta_2) * gi ** 2
-
-            step_size_bound = step_size / (math.sqrt(self.v[i]) + self.eps)
-
-            w[i] -= (
-                utils.math.clamp(step_size_bound, lower_bound, upper_bound) * self.m[i]
-            )
-
-        return w
+import collections
+
+from . import base
+
+__all__ = ["AdaMax"]
+
+
+class AdaMax(base.Optimizer):
+    """AdaMax optimizer.
+
+    Parameters
+    ----------
+    lr
+    beta_1
+    beta_2
+    eps
+
+    Attributes
+    ----------
+    m : collections.defaultdict
+    v : collections.defaultdict
+
+    Examples
+    --------
+
+    >>> from river import datasets
+    >>> from river import evaluate
+    >>> from river import linear_model
+    >>> from river import metrics
+    >>> from river import optim
+    >>> from river import preprocessing
+
+    >>> dataset = datasets.Phishing()
+    >>> optimizer = optim.AdaMax()
+    >>> model = (
+    ...     preprocessing.StandardScaler() |
+    ...     linear_model.LogisticRegression(optimizer)
+    ... )
+    >>> metric = metrics.F1()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    F1: 0.875332
+
+    References
+    ----------
+    [^1]: [Kingma, D.P. and Ba, J., 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.](https://arxiv.org/pdf/1412.6980.pdf)
+    [^2]: [Ruder, S., 2016. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.](http://ruder.io/optimizing-gradient-descent/index.html#adamax)
+
+    """
+
+    def __init__(self, lr=0.1, beta_1=0.9, beta_2=0.999, eps=1e-8):
+        super().__init__(lr)
+        self.beta_1 = beta_1
+        self.beta_2 = beta_2
+        self.eps = eps
+        self.m = collections.defaultdict(float)
+        self.u = collections.defaultdict(float)
+
+    def _step_with_dict(self, w, g):
+
+        # Correct bias for `m`
+        learning_rate = self.learning_rate / (
+            1 - self.beta_1 ** (self.n_iterations + 1)
+        )
+
+        for i, gi in g.items():
+            self.m[i] = self.beta_1 * self.m[i] + (1 - self.beta_1) * gi
+            self.u[i] = max(self.beta_2 * self.u[i], abs(gi))
+            w[i] -= learning_rate * self.m[i] / (self.u[i] + self.eps)
+
+        return w
```

### Comparing `river-0.8.0/river/optim/ada_delta.py` & `river-0.9.0/river/optim/ada_delta.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,74 +1,74 @@
-import collections
-
-from . import base
-
-__all__ = ["AdaDelta"]
-
-
-class AdaDelta(base.Optimizer):
-    """AdaDelta optimizer.
-
-    Parameters
-    ----------
-    rho
-    eps
-
-    Attributes
-    ----------
-    g2 : collections.defaultdict
-    s2 : collections.defaultdict
-
-    Examples
-    --------
-
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import linear_model
-    >>> from river import metrics
-    >>> from river import optim
-    >>> from river import preprocessing
-
-    >>> dataset = datasets.Phishing()
-    >>> optimizer = optim.AdaDelta()
-    >>> model = (
-    ...     preprocessing.StandardScaler() |
-    ...     linear_model.LogisticRegression(optimizer)
-    ... )
-    >>> metric = metrics.F1()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    F1: 0.805611
-
-    References
-    ----------
-    [^1]: [Zeiler, M.D., 2012. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701.](https://arxiv.org/pdf/1212.5701.pdf)
-
-    """
-
-    def __init__(self, rho=0.95, eps=1e-8):
-        super().__init__(lr=None)
-        self.rho = rho
-        self.eps = eps
-        self.g2 = collections.defaultdict(float)
-        self.s2 = collections.defaultdict(float)
-
-    def _rms(self, x):
-        return (x + self.eps) ** 0.5
-
-    def _step(self, w, g):
-
-        for i, gi in g.items():
-
-            # Accumulate the gradient
-            self.g2[i] = self.rho * self.g2[i] + (1 - self.rho) * gi ** 2
-
-            # Compute the update
-            step = -self._rms(self.s2[i]) / self._rms(self.g2[i]) * gi
-
-            # Accumulate the update
-            self.s2[i] = self.rho * self.s2[i] + (1 - self.rho) * step ** 2
-
-            # Apply the update
-            w[i] += step
-
-        return w
+import collections
+
+from . import base
+
+__all__ = ["AdaDelta"]
+
+
+class AdaDelta(base.Optimizer):
+    """AdaDelta optimizer.
+
+    Parameters
+    ----------
+    rho
+    eps
+
+    Attributes
+    ----------
+    g2 : collections.defaultdict
+    s2 : collections.defaultdict
+
+    Examples
+    --------
+
+    >>> from river import datasets
+    >>> from river import evaluate
+    >>> from river import linear_model
+    >>> from river import metrics
+    >>> from river import optim
+    >>> from river import preprocessing
+
+    >>> dataset = datasets.Phishing()
+    >>> optimizer = optim.AdaDelta()
+    >>> model = (
+    ...     preprocessing.StandardScaler() |
+    ...     linear_model.LogisticRegression(optimizer)
+    ... )
+    >>> metric = metrics.F1()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    F1: 0.805611
+
+    References
+    ----------
+    [^1]: [Zeiler, M.D., 2012. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701.](https://arxiv.org/pdf/1212.5701.pdf)
+
+    """
+
+    def __init__(self, rho=0.95, eps=1e-8):
+        super().__init__(lr=None)
+        self.rho = rho
+        self.eps = eps
+        self.g2 = collections.defaultdict(float)
+        self.s2 = collections.defaultdict(float)
+
+    def _rms(self, x):
+        return (x + self.eps) ** 0.5
+
+    def _step_with_dict(self, w, g):
+
+        for i, gi in g.items():
+
+            # Accumulate the gradient
+            self.g2[i] = self.rho * self.g2[i] + (1 - self.rho) * gi ** 2
+
+            # Compute the update
+            step = -self._rms(self.s2[i]) / self._rms(self.g2[i]) * gi
+
+            # Accumulate the update
+            self.s2[i] = self.rho * self.s2[i] + (1 - self.rho) * step ** 2
+
+            # Apply the update
+            w[i] += step
+
+        return w
```

### Comparing `river-0.8.0/river/optim/ada_grad.py` & `river-0.9.0/river/optim/sgd.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,58 +1,50 @@
-import collections
-
-from . import base
-
-__all__ = ["AdaGrad"]
-
-
-class AdaGrad(base.Optimizer):
-    """AdaGrad optimizer.
-
-    Parameters
-    ----------
-    lr
-    eps
-
-    Attributes
-    ----------
-    g2 : collections.defaultdict
-
-    Examples
-    --------
-
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import linear_model
-    >>> from river import metrics
-    >>> from river import optim
-    >>> from river import preprocessing
-
-    >>> dataset = datasets.Phishing()
-    >>> optimizer = optim.AdaGrad()
-    >>> model = (
-    ...     preprocessing.StandardScaler() |
-    ...     linear_model.LogisticRegression(optimizer)
-    ... )
-    >>> metric = metrics.F1()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    F1: 0.880143
-
-    References
-    ----------
-    [^1]: [Duchi, J., Hazan, E. and Singer, Y., 2011. Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research, 12(Jul), pp.2121-2159.](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)
-
-    """
-
-    def __init__(self, lr=0.1, eps=1e-8):
-        super().__init__(lr)
-        self.eps = eps
-        self.g2 = collections.defaultdict(float)
-
-    def _step(self, w, g):
-
-        for i, gi in g.items():
-            self.g2[i] += gi ** 2
-            w[i] -= self.learning_rate / (self.g2[i] + self.eps) ** 0.5 * gi
-
-        return w
+from . import base
+
+__all__ = ["SGD"]
+
+
+class SGD(base.Optimizer):
+    """Plain stochastic gradient descent.
+
+    Parameters
+    ----------
+    lr
+
+    Examples
+    --------
+
+    >>> from river import datasets
+    >>> from river import evaluate
+    >>> from river import linear_model
+    >>> from river import metrics
+    >>> from river import optim
+    >>> from river import preprocessing
+
+    >>> dataset = datasets.Phishing()
+    >>> optimizer = optim.SGD(0.1)
+    >>> model = (
+    ...     preprocessing.StandardScaler() |
+    ...     linear_model.LogisticRegression(optimizer)
+    ... )
+    >>> metric = metrics.F1()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    F1: 0.878521
+
+    References
+    ----------
+    [^1]: [Robbins, H. and Monro, S., 1951. A stochastic approximation method. The annals of mathematical statistics, pp.400-407](https://pdfs.semanticscholar.org/34dd/d8865569c2c32dec9bf7ffc817ff42faaa01.pdf)
+
+    """
+
+    def __init__(self, lr=0.01):
+        super().__init__(lr)
+
+    def _step_with_dict(self, w, g):
+        for i, gi in g.items():
+            w[i] -= self.learning_rate * gi
+        return w
+
+    def _step_with_vector(self, w, g):
+        w -= self.learning_rate * g
+        return w
```

### Comparing `river-0.8.0/river/optim/ada_max.py` & `river-0.9.0/river/optim/ams_grad.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,71 +1,88 @@
-import collections
-
-from . import base
-
-__all__ = ["AdaMax"]
-
-
-class AdaMax(base.Optimizer):
-    """AdaMax optimizer.
-
-    Parameters
-    ----------
-    lr
-    beta_1
-    beta_2
-    eps
-
-    Attributes
-    ----------
-    m : collections.defaultdict
-    v : collections.defaultdict
-
-    Examples
-    --------
-
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import linear_model
-    >>> from river import metrics
-    >>> from river import optim
-    >>> from river import preprocessing
-
-    >>> dataset = datasets.Phishing()
-    >>> optimizer = optim.AdaMax()
-    >>> model = (
-    ...     preprocessing.StandardScaler() |
-    ...     linear_model.LogisticRegression(optimizer)
-    ... )
-    >>> metric = metrics.F1()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    F1: 0.875332
-
-    References
-    ----------
-    [^1]: [Kingma, D.P. and Ba, J., 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.](https://arxiv.org/pdf/1412.6980.pdf)
-    [^2]: [Ruder, S., 2016. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.](http://ruder.io/optimizing-gradient-descent/index.html#adamax)
-
-    """
-
-    def __init__(self, lr=0.1, beta_1=0.9, beta_2=0.999, eps=1e-8):
-        super().__init__(lr)
-        self.beta_1 = beta_1
-        self.beta_2 = beta_2
-        self.eps = eps
-        self.m = collections.defaultdict(float)
-        self.u = collections.defaultdict(float)
-
-    def _step(self, w, g):
-
-        # Correct bias for `m`
-        learning_rate = self.learning_rate / (
-            1 - self.beta_1 ** (self.n_iterations + 1)
-        )
-
-        for i, gi in g.items():
-            self.m[i] = self.beta_1 * self.m[i] + (1 - self.beta_1) * gi
-            self.u[i] = max(self.beta_2 * self.u[i], abs(gi))
-            w[i] -= learning_rate * self.m[i] / (self.u[i] + self.eps)
-
-        return w
+import collections
+import typing
+
+from . import base, schedulers
+
+__all__ = ["AMSGrad"]
+
+
+class AMSGrad(base.Optimizer):
+    """AMSGrad optimizer.
+
+    Parameters
+    ----------
+    lr
+        The learning rate.
+    beta_1
+    beta_2
+    eps
+    correct_bias
+
+    Attributes
+    ----------
+    m : collections.defaultdict
+    v : collections.defaultdict
+    v_hat : collections.defaultdict
+
+    Examples
+    --------
+
+    >>> from river import datasets
+    >>> from river import evaluate
+    >>> from river import linear_model
+    >>> from river import metrics
+    >>> from river import optim
+    >>> from river import preprocessing
+
+    >>> dataset = datasets.Phishing()
+    >>> optimizer = optim.AMSGrad()
+    >>> model = (
+    ...     preprocessing.StandardScaler() |
+    ...     linear_model.LogisticRegression(optimizer)
+    ... )
+    >>> metric = metrics.F1()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    F1: 0.865724
+
+    References
+    ----------
+    [^1]: [Reddi, S.J., Kale, S. and Kumar, S., 2019. On the convergence of adam and beyond. arXiv preprint arXiv:1904.09237](https://arxiv.org/pdf/1904.09237.pdf)
+
+    """
+
+    def __init__(
+        self,
+        lr: typing.Union[float, schedulers.Scheduler] = 0.1,
+        beta_1=0.9,
+        beta_2=0.999,
+        eps=1e-8,
+        correct_bias=True,
+    ):
+        super().__init__(lr)
+        self.beta_1 = beta_1
+        self.beta_2 = beta_2
+        self.eps = eps
+        self.correct_bias = correct_bias
+        self.m = collections.defaultdict(float)
+        self.v = collections.defaultdict(float)
+        self.v_hat = collections.defaultdict(float)
+
+    def _step_with_dict(self, w, g):
+
+        lr = self.learning_rate
+
+        if self.correct_bias:
+            # Correct bias for `v`
+            lr *= (1 - self.beta_2 ** (self.n_iterations + 1)) ** 0.5
+            # Correct bias for `m`
+            lr /= 1 - self.beta_1 ** (self.n_iterations + 1)
+
+        for i, gi in g.items():
+            self.m[i] = self.beta_1 * self.m[i] + (1 - self.beta_1) * gi
+            self.v[i] = self.beta_2 * self.v[i] + (1 - self.beta_2) * gi ** 2
+            self.v_hat[i] = max(self.v_hat[i], self.v[i])
+
+            w[i] -= lr * self.m[i] / (self.v_hat[i] ** 0.5 + self.eps)
+
+        return w
```

### Comparing `river-0.8.0/river/optim/ams_grad.py` & `river-0.9.0/river/optim/ftrl.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,88 +1,82 @@
-import collections
-import typing
-
-from . import base, schedulers
-
-__all__ = ["AMSGrad"]
-
-
-class AMSGrad(base.Optimizer):
-    """AMSGrad optimizer.
-
-    Parameters
-    ----------
-    lr
-        The learning rate.
-    beta_1
-    beta_2
-    eps
-    correct_bias
-
-    Attributes
-    ----------
-    m : collections.defaultdict
-    v : collections.defaultdict
-    v_hat : collections.defaultdict
-
-    Examples
-    --------
-
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import linear_model
-    >>> from river import metrics
-    >>> from river import optim
-    >>> from river import preprocessing
-
-    >>> dataset = datasets.Phishing()
-    >>> optimizer = optim.AMSGrad()
-    >>> model = (
-    ...     preprocessing.StandardScaler() |
-    ...     linear_model.LogisticRegression(optimizer)
-    ... )
-    >>> metric = metrics.F1()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    F1: 0.865724
-
-    References
-    ----------
-    [^1]: [Reddi, S.J., Kale, S. and Kumar, S., 2019. On the convergence of adam and beyond. arXiv preprint arXiv:1904.09237](https://arxiv.org/pdf/1904.09237.pdf)
-
-    """
-
-    def __init__(
-        self,
-        lr: typing.Union[float, schedulers.Scheduler] = 0.1,
-        beta_1=0.9,
-        beta_2=0.999,
-        eps=1e-8,
-        correct_bias=True,
-    ):
-        super().__init__(lr)
-        self.beta_1 = beta_1
-        self.beta_2 = beta_2
-        self.eps = eps
-        self.correct_bias = correct_bias
-        self.m = collections.defaultdict(float)
-        self.v = collections.defaultdict(float)
-        self.v_hat = collections.defaultdict(float)
-
-    def _step(self, w, g):
-
-        lr = self.learning_rate
-
-        if self.correct_bias:
-            # Correct bias for `v`
-            lr *= (1 - self.beta_2 ** (self.n_iterations + 1)) ** 0.5
-            # Correct bias for `m`
-            lr /= 1 - self.beta_1 ** (self.n_iterations + 1)
-
-        for i, gi in g.items():
-            self.m[i] = self.beta_1 * self.m[i] + (1 - self.beta_1) * gi
-            self.v[i] = self.beta_2 * self.v[i] + (1 - self.beta_2) * gi ** 2
-            self.v_hat[i] = max(self.v_hat[i], self.v[i])
-
-            w[i] -= lr * self.m[i] / (self.v_hat[i] ** 0.5 + self.eps)
-
-        return w
+import collections
+
+import numpy as np
+
+from . import base
+
+__all__ = ["FTRLProximal"]
+
+
+class FTRLProximal(base.Optimizer):
+    """FTRL-Proximal optimizer.
+
+    Parameters
+    ----------
+    alpha
+    beta
+    l1
+    l2
+
+    Attributes
+    ----------
+    z : collections.defaultdict
+    n : collections.defaultdict
+
+    Examples
+    --------
+
+    >>> from river import datasets
+    >>> from river import evaluate
+    >>> from river import linear_model
+    >>> from river import metrics
+    >>> from river import optim
+    >>> from river import preprocessing
+
+    >>> dataset = datasets.Phishing()
+    >>> optimizer = optim.FTRLProximal()
+    >>> model = (
+    ...     preprocessing.StandardScaler() |
+    ...     linear_model.LogisticRegression(optimizer)
+    ... )
+    >>> metric = metrics.F1()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    F1: 0.876588
+
+    References
+    ----------
+    [^1]: [McMahan, H.B., Holt, G., Sculley, D., Young, M., Ebner, D., Grady, J., Nie, L., Phillips, T., Davydov, E., Golovin, D. and Chikkerur, S., 2013, August. Ad click prediction: a view from the trenches. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 1222-1230)](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41159.pdf)
+    [^2]: [Tensorflow's `FtrlOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/FtrlOptimizer)
+
+    """
+
+    def __init__(self, alpha=0.05, beta=1.0, l1=0.0, l2=1.0):
+        self.alpha = alpha
+        self.beta = beta
+        self.l1 = l1
+        self.l2 = l2
+        self.z = collections.defaultdict(float)
+        self.n = collections.defaultdict(float)
+        self.n_iterations = 0
+
+    def _step_with_dict(self, w, g):
+
+        alpha = self.alpha
+        beta = self.beta
+        l1 = self.l1
+        l2 = self.l2
+        z = self.z
+        n = self.n
+
+        for i in g:
+            if abs(z[i]) > l1:
+                w[i] = -(((beta + n[i] ** 0.5) / alpha + l2) ** -1) * (
+                    z[i] - np.sign(z[i]) * l1
+                )
+
+        for i, gi in g.items():
+            s = ((self.n[i] + gi ** 2) ** 0.5 - self.n[i] ** 0.5) / self.alpha
+            self.z[i] += gi - s * w.get(i, 0)
+            self.n[i] += gi ** 2
+
+        return w
```

### Comparing `river-0.8.0/river/optim/average.py` & `river-0.9.0/river/optim/adam.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,71 +1,97 @@
-import collections
-
-from . import base
-
-
-class Averager(base.Optimizer):
-    """Averaged stochastic gradient descent.
-
-    This is a wrapper that can be applied to any stochastic gradient descent optimiser. Note that
-    this implementation differs than what may be found elsewhere. Essentially, the average of the
-    weights is usually only used at the end of the optimisation, once all the data has been seen.
-    However, in this implementation the optimiser returns the current averaged weights.
-
-    Parameters
-    ----------
-    optimizer
-        An optimizer for which the produced weights will be averaged.
-    start
-        Indicates the number of iterations to wait before starting the average. Essentially,
-        nothing happens differently before the number of iterations reaches this value.
-
-    Examples
-    --------
-
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import linear_model
-    >>> from river import metrics
-    >>> from river import optim
-    >>> from river import preprocessing
-
-    >>> dataset = datasets.Phishing()
-    >>> optimizer = optim.Averager(optim.SGD(0.01), 100)
-    >>> model = (
-    ...     preprocessing.StandardScaler() |
-    ...     linear_model.LogisticRegression(optimizer)
-    ... )
-    >>> metric = metrics.F1()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    F1: 0.878924
-
-    References
-    ----------
-    [^1]: [Bottou, L., 2010. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT'2010 (pp. 177-186). Physica-Verlag HD.](https://leon.bottou.org/publications/pdf/compstat-2010.pdf)
-    [^2]: [Stochastic Algorithms for One-Pass Learning slides by Léon Bottou](https://leon.bottou.org/slides/onepass/onepass.pdf)
-    [^3]: [Xu, W., 2011. Towards optimal one pass large scale learning with averaged stochastic gradient descent. arXiv preprint arXiv:1107.2490.](https://arxiv.org/pdf/1107.2490.pdf)
-
-    """
-
-    def __init__(self, optimizer: base.Optimizer, start: int = 0):
-        self.optimizer = optimizer
-        self.start = start
-        self.avg_w = collections.defaultdict(float)
-        self.n_iterations = 0
-
-    def look_ahead(self, w):
-        return self.optimizer.look_ahead(w)
-
-    def _step(self, w, g):
-
-        w = self.optimizer.step(w, g)
-
-        # No averaging occurs during the first start iterations
-        if self.n_iterations < self.start:
-            return w
-
-        for i, wi in w.items():
-            self.avg_w[i] += (wi - self.avg_w[i]) / (self.n_iterations - self.start + 1)
-
-        return self.avg_w
+import collections
+
+import numpy as np
+
+from river import utils
+
+from . import base
+
+__all__ = ["Adam"]
+
+
+class Adam(base.Optimizer):
+    """Adam optimizer.
+
+    Parameters
+    ----------
+    lr
+    beta_1
+    beta_2
+    eps
+
+    Attributes
+    ----------
+    m : collections.defaultdict
+    v : collections.defaultdict
+
+    Examples
+    --------
+
+    >>> from river import datasets
+    >>> from river import evaluate
+    >>> from river import linear_model
+    >>> from river import metrics
+    >>> from river import optim
+    >>> from river import preprocessing
+
+    >>> dataset = datasets.Phishing()
+    >>> optimizer = optim.Adam()
+    >>> model = (
+    ...     preprocessing.StandardScaler() |
+    ...     linear_model.LogisticRegression(optimizer)
+    ... )
+    >>> metric = metrics.F1()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    F1: 0.86496
+
+    References
+    ----------
+    [^1]: [Kingma, D.P. and Ba, J., 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.](https://arxiv.org/pdf/1412.6980.pdf)
+
+    """
+
+    def __init__(self, lr=0.1, beta_1=0.9, beta_2=0.999, eps=1e-8):
+        super().__init__(lr)
+        self.beta_1 = beta_1
+        self.beta_2 = beta_2
+        self.eps = eps
+        self.m = None
+        self.v = None
+
+    def _step_with_dict(self, w, g):
+
+        if self.m is None:
+            self.m = collections.defaultdict(float)
+            self.v = collections.defaultdict(float)
+
+        # Correct bias for `v`
+        lr = self.learning_rate * (1 - self.beta_2 ** (self.n_iterations + 1)) ** 0.5
+        # Correct bias for `m`
+        lr /= 1 - self.beta_1 ** (self.n_iterations + 1)
+
+        for i, gi in g.items():
+            self.m[i] = self.beta_1 * self.m[i] + (1 - self.beta_1) * gi
+            self.v[i] = self.beta_2 * self.v[i] + (1 - self.beta_2) * gi ** 2
+            w[i] -= lr * self.m[i] / (self.v[i] ** 0.5 + self.eps)
+
+        return w
+
+    def _step_with_vector(self, w, g):
+
+        if self.m is None:
+            if isinstance(w, np.ndarray):
+                self.m = np.zeros_like(w)
+                self.v = np.zeros_like(w)
+            else:
+                self.m = utils.VectorDict()
+                self.v = utils.VectorDict()
+
+        lr = self.learning_rate * (1 - self.beta_2 ** (self.n_iterations + 1)) ** 0.5
+        lr /= 1 - self.beta_1 ** (self.n_iterations + 1)
+
+        self.m = self.beta_1 * self.m + (1 - self.beta_1) * g
+        self.v = self.beta_2 * self.v + (1 - self.beta_2) * g ** 2
+        w -= lr * self.m / (self.v ** 0.5 + self.eps)
+
+        return w
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `river-0.8.0/river/optim/ftrl.py` & `river-0.9.0/river/optim/ada_grad.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,82 +1,58 @@
-import collections
-
-import numpy as np
-
-from . import base
-
-__all__ = ["FTRLProximal"]
-
-
-class FTRLProximal(base.Optimizer):
-    """FTRL-Proximal optimizer.
-
-    Parameters
-    ----------
-    alpha
-    beta
-    l1
-    l2
-
-    Attributes
-    ----------
-    z : collections.defaultdict
-    n : collections.defaultdict
-
-    Examples
-    --------
-
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import linear_model
-    >>> from river import metrics
-    >>> from river import optim
-    >>> from river import preprocessing
-
-    >>> dataset = datasets.Phishing()
-    >>> optimizer = optim.FTRLProximal()
-    >>> model = (
-    ...     preprocessing.StandardScaler() |
-    ...     linear_model.LogisticRegression(optimizer)
-    ... )
-    >>> metric = metrics.F1()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    F1: 0.876588
-
-    References
-    ----------
-    [^1]: [McMahan, H.B., Holt, G., Sculley, D., Young, M., Ebner, D., Grady, J., Nie, L., Phillips, T., Davydov, E., Golovin, D. and Chikkerur, S., 2013, August. Ad click prediction: a view from the trenches. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 1222-1230)](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41159.pdf)
-    [^2]: [Tensorflow's `FtrlOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/FtrlOptimizer)
-
-    """
-
-    def __init__(self, alpha=0.05, beta=1.0, l1=0.0, l2=1.0):
-        self.alpha = alpha
-        self.beta = beta
-        self.l1 = l1
-        self.l2 = l2
-        self.z = collections.defaultdict(float)
-        self.n = collections.defaultdict(float)
-        self.n_iterations = 0
-
-    def _step(self, w, g):
-
-        alpha = self.alpha
-        beta = self.beta
-        l1 = self.l1
-        l2 = self.l2
-        z = self.z
-        n = self.n
-
-        for i in g:
-            if abs(z[i]) > l1:
-                w[i] = -(((beta + n[i] ** 0.5) / alpha + l2) ** -1) * (
-                    z[i] - np.sign(z[i]) * l1
-                )
-
-        for i, gi in g.items():
-            s = ((self.n[i] + gi ** 2) ** 0.5 - self.n[i] ** 0.5) / self.alpha
-            self.z[i] += gi - s * w.get(i, 0)
-            self.n[i] += gi ** 2
-
-        return w
+import collections
+
+from . import base
+
+__all__ = ["AdaGrad"]
+
+
+class AdaGrad(base.Optimizer):
+    """AdaGrad optimizer.
+
+    Parameters
+    ----------
+    lr
+    eps
+
+    Attributes
+    ----------
+    g2 : collections.defaultdict
+
+    Examples
+    --------
+
+    >>> from river import datasets
+    >>> from river import evaluate
+    >>> from river import linear_model
+    >>> from river import metrics
+    >>> from river import optim
+    >>> from river import preprocessing
+
+    >>> dataset = datasets.Phishing()
+    >>> optimizer = optim.AdaGrad()
+    >>> model = (
+    ...     preprocessing.StandardScaler() |
+    ...     linear_model.LogisticRegression(optimizer)
+    ... )
+    >>> metric = metrics.F1()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    F1: 0.880143
+
+    References
+    ----------
+    [^1]: [Duchi, J., Hazan, E. and Singer, Y., 2011. Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research, 12(Jul), pp.2121-2159.](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)
+
+    """
+
+    def __init__(self, lr=0.1, eps=1e-8):
+        super().__init__(lr)
+        self.eps = eps
+        self.g2 = collections.defaultdict(float)
+
+    def _step_with_dict(self, w, g):
+
+        for i, gi in g.items():
+            self.g2[i] += gi ** 2
+            w[i] -= self.learning_rate / (self.g2[i] + self.eps) ** 0.5 * gi
+
+        return w
```

### Comparing `river-0.8.0/river/optim/initializers.py` & `river-0.9.0/river/optim/initializers.py`

 * *Files identical despite different names*

### Comparing `river-0.8.0/river/optim/losses.py` & `river-0.9.0/river/optim/losses.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,580 +1,632 @@
-"""Loss functions.
-
-Each loss function is intended to work with both single values as well as numpy vectors.
-
-"""
-import abc
-import math
-import typing
-
-import numpy as np
-from scipy import special
-
-from river import base, utils
-
-__all__ = [
-    "Absolute",
-    "BinaryLoss",
-    "BinaryFocalLoss",
-    "Cauchy",
-    "CrossEntropy",
-    "Hinge",
-    "EpsilonInsensitiveHinge",
-    "Log",
-    "MultiClassLoss",
-    "Poisson",
-    "Quantile",
-    "RegressionLoss",
-    "Squared",
-]
-
-
-def clamp_proba(p):
-    return max(min(p, 1 - 1e-15), 1e-15)
-
-
-class Loss(base.Base, abc.ABC):
-    """Base class for all loss functions."""
-
-    def __repr__(self):
-        return f"{self.__class__.__name__}({vars(self)})"
-
-    @abc.abstractmethod
-    def __call__(self, y_true, y_pred):
-        """Returns the loss.
-
-        Parameters
-        ----------
-        y_true
-            Ground truth(s).
-        y_pred
-            Prediction(s).
-
-        Returns
-        -------
-        The loss(es).
-
-        """
-
-    @abc.abstractmethod
-    def gradient(self, y_true, y_pred):
-        """Return the gradient with respect to y_pred.
-
-        Parameters
-        ----------
-        y_true
-            Ground truth(s).
-        y_pred
-            Prediction(s).
-
-        Returns
-        -------
-        The gradient(s).
-
-        """
-
-    @abc.abstractmethod
-    def mean_func(self, y_pred):
-        """Mean function.
-
-        This is the inverse of the link function. Typically, a loss function takes as input the raw
-        output of a model. In the case of classification, the raw output would be logits. The mean
-        function can be used to convert the raw output into a value that makes sense to the user,
-        such as a probability.
-
-        Parameters
-        ----------
-        y_pred
-            Raw prediction(s).
-
-        Returns
-        -------
-        The adjusted prediction(s).
-
-        References
-        ----------
-        [^1]: [Wikipedia section on link and mean function](https://www.wikiwand.com/en/Generalized_linear_model#/Link_function)
-
-        """
-
-
-class BinaryLoss(Loss):
-    """A loss appropriate for binary classification tasks."""
-
-    def mean_func(self, y_pred):
-        if isinstance(y_pred, np.ndarray):
-            return 1.0 / (1.0 + np.exp(-y_pred))
-        return utils.math.sigmoid(y_pred)
-
-
-class MultiClassLoss(Loss):
-    """A loss appropriate for multi-class classification tasks."""
-
-    def mean_func(self, y_pred):
-        if isinstance(y_pred, np.ndarray):
-            return special.softmax(y_pred)
-        return utils.math.softmax(y_pred)
-
-
-class RegressionLoss(Loss):
-    """A loss appropriate for regression tasks."""
-
-    def mean_func(self, y_pred):
-        return y_pred
-
-
-class Absolute(RegressionLoss):
-    """Absolute loss, also known as the mean absolute error or L1 loss.
-
-    Mathematically, it is defined as
-
-    $$L = |p_i - y_i|$$
-
-    It's gradient w.r.t. to $p_i$ is
-
-    $$\\frac{\\partial L}{\\partial p_i} = sgn(p_i - y_i)$$
-
-    Examples
-    --------
-
-    >>> from river import optim
-
-    >>> loss = optim.losses.Absolute()
-    >>> loss(-42, 42)
-    84
-    >>> loss.gradient(1, 2)
-    1
-    >>> loss.gradient(2, 1)
-    -1
-
-    """
-
-    def __call__(self, y_true, y_pred):
-        if isinstance(y_true, np.ndarray):
-            return np.abs(y_pred - y_true)
-        return abs(y_pred - y_true)
-
-    def gradient(self, y_true, y_pred):
-
-        if isinstance(y_true, np.ndarray):
-            return np.where(y_pred > y_true, 1, -1)
-
-        if y_pred > y_true:
-            return 1
-        return -1
-
-
-class Cauchy(RegressionLoss):
-    """Cauchy loss function.
-
-    Parameters
-    ----------
-    C
-
-    References
-    ----------
-    [^1]: ["Effect of MAE" Kaggle discussion](https://www.kaggle.com/c/allstate-claims-severity/discussion/24520#140163)
-    [^2]: [Paris Madness Kaggle kernel](https://www.kaggle.com/raddar/paris-madness)
-
-    """
-
-    def __init__(self, C=80):
-        self.C = C
-
-    def __call__(self, y_true, y_pred):
-        if isinstance(y_true, np.ndarray):
-            return np.abs(y_pred - y_true)
-        return abs(y_pred - y_true)
-
-    def gradient(self, y_true, y_pred):
-        diff = y_pred - y_true
-        return diff / ((diff / self.C) ** 2 + 1)
-
-
-class CrossEntropy(MultiClassLoss):
-    """Cross entropy loss.
-
-    This is a generalization of logistic loss to multiple classes.
-
-    Parameters
-    ----------
-    class_weight
-        A dictionary that indicates what weight to associate with each class.
-
-    Examples
-    --------
-
-    >>> from river import optim
-
-    >>> y_true = [0, 1, 2, 2]
-    >>> y_pred = [
-    ...     {0: 0.29450637, 1: 0.34216758, 2: 0.36332605},
-    ...     {0: 0.21290077, 1: 0.32728332, 2: 0.45981591},
-    ...     {0: 0.42860913, 1: 0.33380113, 2: 0.23758974},
-    ...     {0: 0.44941979, 1: 0.32962558, 2: 0.22095463}
-    ... ]
-
-    >>> loss = optim.losses.CrossEntropy()
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     print(loss(yt, yp))
-    1.222454
-    1.116929
-    1.437209
-    1.509797
-
-    >>> for yt, yp in zip(y_true, y_pred):
-    ...     print(loss.gradient(yt, yp))
-    {0: -0.70549363, 1: 0.34216758, 2: 0.36332605}
-    {0: 0.21290077, 1: -0.67271668, 2: 0.45981591}
-    {0: 0.42860913, 1: 0.33380113, 2: -0.76241026}
-    {0: 0.44941979, 1: 0.32962558, 2: -0.77904537}
-
-    References
-    ----------
-    [^1]: [What is Softmax regression and how is it related to Logistic regression?](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression.md)
-
-    """
-
-    def __init__(self, class_weight: typing.Dict[base.typing.ClfTarget, float] = None):
-        if class_weight is None:
-            class_weight = {}
-        self.class_weight = class_weight
-
-    def __call__(self, y_true, y_pred):
-        total = 0
-
-        for label, proba in y_pred.items():
-            if y_true == label:
-                total += self.class_weight.get(label, 1.0) * math.log(
-                    clamp_proba(proba)
-                )
-
-        return -total
-
-    def gradient(self, y_true, y_pred):
-        return {
-            label: (
-                self.class_weight.get(label, 1.0)
-                * (clamp_proba(y_pred.get(label, 0.0)) - (y_true == label))
-            )
-            for label in {*y_pred.keys(), y_true}
-        }
-
-
-class Hinge(BinaryLoss):
-    r"""Computes the hinge loss.
-
-    Mathematically, it is defined as
-
-    $$L = max(0, 1 - p_i * y_i)$$
-
-    It's gradient w.r.t. to $p_i$ is
-
-    $$
-    \\frac{\\partial L}{\\partial y_i} = \\left\{
-    \\begin{array}{ll}
-        \\ 0  &   p_iy_i \geqslant 1  \\\\
-        \\ - y_i & p_iy_i < 1
-    \\end{array}
-    \\right.
-    $$
-
-    Parameters
-    ----------
-    threshold
-        Margin threshold. 1 yield the loss used in SVMs, whilst 0 is equivalent to the loss used in
-        the Perceptron algorithm.
-
-    Examples
-    --------
-
-    >>> from river import optim
-
-    >>> loss = optim.losses.Hinge(threshold=1)
-    >>> loss(1, .2)
-    0.8
-
-    >>> loss.gradient(1, .2)
-    -1
-
-    """
-
-    def __init__(self, threshold=1.0):
-        self.threshold = threshold
-
-    def __call__(self, y_true, y_pred):
-        y_true = y_true * 2 - 1  # [0, 1] -> [-1, 1]
-
-        if isinstance(y_true, np.ndarray):
-            return np.maximum(self.threshold - y_true * y_pred, 0)
-
-        return max(self.threshold - y_true * y_pred, 0)
-
-    def gradient(self, y_true, y_pred):
-        y_true = y_true * 2 - 1  # [0, 1] -> [-1, 1]
-
-        if isinstance(y_true, np.ndarray):
-            return np.where(y_true * y_pred < self.threshold, -y_true, 0)
-
-        if y_true * y_pred <= self.threshold:
-            return -y_true
-        return 0
-
-
-class EpsilonInsensitiveHinge(RegressionLoss):
-    """Epsilon-insensitive hinge loss.
-
-    Parameters
-    ----------
-    eps
-
-    """
-
-    def __init__(self, eps=0.1):
-        self.eps = eps
-
-    def __call__(self, y_true, y_pred):
-        y_true = y_true * 2 - 1  # [0, 1] -> [-1, 1]
-
-        if isinstance(y_true, np.ndarray):
-            return np.maximum(np.abs(y_pred - y_true) - self.eps, 0)
-
-        return max(math.fabs(y_pred - y_true) - self.eps, 0)
-
-    def gradient(self, y_true, y_pred):
-        y_true = y_true * 2 - 1  # [0, 1] -> [-1, 1]
-
-        if isinstance(y_true, np.ndarray):
-            gradients = np.zeros_like(y_true)
-            gradients[y_pred > y_true + self.eps] = 1
-            gradients[y_pred + self.eps < y_true] = -1
-            return gradients
-
-        if y_pred > y_true + self.eps:
-            return 1
-        elif y_pred + self.eps < y_true:
-            return -1
-        return 0
-
-
-class Log(BinaryLoss):
-    """Logarithmic loss.
-
-    This loss function expects each provided `y_pred` to be a logit. In other words if must be
-    the raw output of a linear model or a neural network.
-
-    Parameters
-    ----------
-    weight_pos
-    weight_neg
-
-    References
-    ----------
-    [^1]: [Logit Wikipedia page](https://www.wikiwand.com/en/Logit>)
-
-    """
-
-    def __init__(self, weight_pos=1.0, weight_neg=1.0):
-        self.weight_pos = weight_pos
-        self.weight_neg = weight_neg
-
-    def __call__(self, y_true, y_pred):
-
-        if isinstance(y_true, np.ndarray):
-            weights = np.where(y_true == 0, self.weight_neg, self.weight_pos)
-            y_true = 2 * y_true - 1  # map {0, 1} to {-1, 1}
-            z = y_pred * y_true
-            return weights * np.log(1.0 + np.exp(-z))
-
-        weight = self.weight_pos
-        if y_true == 0:
-            y_true = -1
-            weight = self.weight_neg
-        else:
-            y_true = int(y_true)
-
-        z = y_pred * y_true
-        if z > 18.0:
-            return weight * math.exp(-z)
-        if z < -18.0:
-            return weight * -z
-        return weight * math.log(1.0 + math.exp(-z))
-
-    def gradient(self, y_true, y_pred):
-
-        if isinstance(y_true, np.ndarray):
-            weights = np.where(y_true == 0, self.weight_neg, self.weight_pos)
-            y_true = 2 * y_true - 1  # map {0, 1} to {-1, 1}
-            z = y_pred * y_true
-            return weights * -y_true / (np.exp(z) + 1.0)
-
-        weight = self.weight_pos
-        if y_true == 0:
-            y_true = -1
-            weight = self.weight_neg
-        else:
-            y_true = int(y_true)
-
-        z = y_pred * y_true
-        if z > 18.0:
-            return weight * math.exp(-z) * -y_true
-        if z < -18.0:
-            return weight * -y_true
-        return weight * -y_true / (math.exp(z) + 1.0)
-
-
-class Quantile(RegressionLoss):
-    """Quantile loss.
-
-    Parameters
-    ----------
-    alpha
-        Desired quantile to attain.
-
-    Examples
-    --------
-
-    >>> from river import optim
-
-    >>> loss = optim.losses.Quantile(0.5)
-    >>> loss(1, 3)
-    1.0
-
-    >>> loss.gradient(1, 3)
-    0.5
-
-    >>> loss.gradient(3, 1)
-    -0.5
-
-    References
-    ----------
-    [^1]: [Wikipedia article on quantile regression](https://www.wikiwand.com/en/Quantile_regression)
-    [^2]: [Derivative from WolframAlpha](https://www.wolframalpha.com/input/?i=derivative+(y+-+p)+*+(alpha+-+Boole(y+-+p))+wrt+p)
-
-    """
-
-    def __init__(self, alpha=0.5):
-        self.alpha = alpha
-
-    def __call__(self, y_true, y_pred):
-        diff = y_pred - y_true
-        return (self.alpha - (diff < 0)) * diff
-
-    def gradient(self, y_true, y_pred):
-        return (y_true < y_pred) - self.alpha
-
-
-class Squared(RegressionLoss):
-    """Squared loss, also known as the L2 loss.
-
-    Mathematically, it is defined as
-
-    $$L = (p_i - y_i) ^ 2$$
-
-    It's gradient w.r.t. to $p_i$ is
-
-    $$\\frac{\\partial L}{\\partial p_i} = 2 \times (p_i - y_i)$$
-
-    One thing to note is that this convention is consistent with Vowpal Wabbit and PyTorch, but
-    not with scikit-learn. Indeed, scikit-learn divides the loss by 2, making the 2 disappear in
-    the gradient.
-
-    Examples
-    --------
-
-    >>> from river import optim
-
-    >>> loss = optim.losses.Squared()
-    >>> loss(-4, 5)
-    81
-    >>> loss.gradient(-4, 5)
-    18
-    >>> loss.gradient(5, -4)
-    -18
-
-    """
-
-    def __call__(self, y_true, y_pred):
-        return (y_pred - y_true) * (y_pred - y_true)
-
-    def gradient(self, y_true, y_pred):
-        return 2 * (y_pred - y_true)
-
-
-class BinaryFocalLoss(BinaryLoss):
-    """Binary focal loss.
-
-    This implements the "star" algorithm from the appendix of the focal loss paper.
-
-    Parameters
-    ----------
-    gamma
-    beta
-
-    References
-    ----------
-    1. [Lin, T.Y., Goyal, P., Girshick, R., He, K. and Dollár, P., 2017. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision (pp. 2980-2988)](https://arxiv.org/pdf/1708.02002.pdf)
-
-    """
-
-    def __init__(self, gamma=2, beta=1):
-        self.gamma = gamma
-        self.beta = beta
-
-    def __call__(self, y_true, y_pred):
-
-        y_true = y_true * 2 - 1  # [0, 1] -> [-1, 1]
-
-        xt = y_true * y_pred
-
-        if isinstance(y_true, np.ndarray):
-            pt = 1.0 / (1 + np.exp(-(self.gamma * xt + self.beta)))
-            return -np.log(pt) / self.gamma
-
-        pt = utils.math.sigmoid(self.gamma * xt + self.beta)
-        return -math.log(pt) / self.gamma
-
-    def gradient(self, y_true, y_pred):
-
-        y_true = y_true * 2 - 1  # [0, 1] -> [-1, 1]
-
-        xt = y_true * y_pred
-
-        if isinstance(y_true, np.ndarray):
-            pt = 1.0 / (1 + np.exp(-(self.gamma * xt + self.beta)))
-            return y_true * (pt - 1)
-
-        pt = utils.math.sigmoid(self.gamma * xt + self.beta)
-        return y_true * (pt - 1)
-
-
-class Poisson(RegressionLoss):
-    """Poisson loss.
-
-    The Poisson loss is usually more suited for regression with count data than the squared loss.
-
-    Mathematically, it is defined as
-
-    $$L = exp(p_i) - y_i \\times p_i$$
-
-    It's gradient w.r.t. to $p_i$ is
-
-    $$\\frac{\\partial L}{\\partial p_i} = exp(p_i) - y_i$$
-
-    """
-
-    def __call__(self, y_true, y_pred):
-        if isinstance(y_pred, np.ndarray):
-            return np.exp(y_pred) - y_true * y_pred
-        return math.exp(y_pred) - y_true * y_pred
-
-    def gradient(self, y_true, y_pred):
-        if isinstance(y_pred, np.ndarray):
-            return np.exp(y_pred) - y_true
-        return math.exp(y_pred) - y_true
-
-    def mean_func(self, y_pred):
-        if isinstance(y_pred, np.ndarray):
-            return np.exp(y_pred)
-        return math.exp(y_pred)
+"""Loss functions.
+
+Each loss function is intended to work with both single values as well as numpy vectors.
+
+"""
+import abc
+import math
+import typing
+
+import numpy as np
+from scipy import special
+
+from river import base, utils
+
+__all__ = [
+    "Absolute",
+    "BinaryLoss",
+    "BinaryFocalLoss",
+    "Cauchy",
+    "CrossEntropy",
+    "Hinge",
+    "Huber",
+    "EpsilonInsensitiveHinge",
+    "Log",
+    "MultiClassLoss",
+    "Poisson",
+    "Quantile",
+    "RegressionLoss",
+    "Squared",
+]
+
+
+def clamp_proba(p):
+    return max(min(p, 1 - 1e-15), 1e-15)
+
+
+class Loss(base.Base, abc.ABC):
+    """Base class for all loss functions."""
+
+    def __repr__(self):
+        return f"{self.__class__.__name__}({vars(self)})"
+
+    @abc.abstractmethod
+    def __call__(self, y_true, y_pred):
+        """Returns the loss.
+
+        Parameters
+        ----------
+        y_true
+            Ground truth(s).
+        y_pred
+            Prediction(s).
+
+        Returns
+        -------
+        The loss(es).
+
+        """
+
+    @abc.abstractmethod
+    def gradient(self, y_true, y_pred):
+        """Return the gradient with respect to y_pred.
+
+        Parameters
+        ----------
+        y_true
+            Ground truth(s).
+        y_pred
+            Prediction(s).
+
+        Returns
+        -------
+        The gradient(s).
+
+        """
+
+    @abc.abstractmethod
+    def mean_func(self, y_pred):
+        """Mean function.
+
+        This is the inverse of the link function. Typically, a loss function takes as input the raw
+        output of a model. In the case of classification, the raw output would be logits. The mean
+        function can be used to convert the raw output into a value that makes sense to the user,
+        such as a probability.
+
+        Parameters
+        ----------
+        y_pred
+            Raw prediction(s).
+
+        Returns
+        -------
+        The adjusted prediction(s).
+
+        References
+        ----------
+        [^1]: [Wikipedia section on link and mean function](https://www.wikiwand.com/en/Generalized_linear_model#/Link_function)
+
+        """
+
+
+class BinaryLoss(Loss):
+    """A loss appropriate for binary classification tasks."""
+
+    def mean_func(self, y_pred):
+        if isinstance(y_pred, np.ndarray):
+            return 1.0 / (1.0 + np.exp(-y_pred))
+        return utils.math.sigmoid(y_pred)
+
+
+class MultiClassLoss(Loss):
+    """A loss appropriate for multi-class classification tasks."""
+
+    def mean_func(self, y_pred):
+        if isinstance(y_pred, np.ndarray):
+            return special.softmax(y_pred)
+        return utils.math.softmax(y_pred)
+
+
+class RegressionLoss(Loss):
+    """A loss appropriate for regression tasks."""
+
+    def mean_func(self, y_pred):
+        return y_pred
+
+
+class Absolute(RegressionLoss):
+    """Absolute loss, also known as the mean absolute error or L1 loss.
+
+    Mathematically, it is defined as
+
+    $$L = |p_i - y_i|$$
+
+    It's gradient w.r.t. to $p_i$ is
+
+    $$\\frac{\\partial L}{\\partial p_i} = sgn(p_i - y_i)$$
+
+    Examples
+    --------
+
+    >>> from river import optim
+
+    >>> loss = optim.losses.Absolute()
+    >>> loss(-42, 42)
+    84
+    >>> loss.gradient(1, 2)
+    1
+    >>> loss.gradient(2, 1)
+    -1
+
+    """
+
+    def __call__(self, y_true, y_pred):
+        if isinstance(y_true, np.ndarray):
+            return np.abs(y_pred - y_true)
+        return abs(y_pred - y_true)
+
+    def gradient(self, y_true, y_pred):
+
+        if isinstance(y_true, np.ndarray):
+            return np.where(y_pred > y_true, 1, -1)
+
+        if y_pred > y_true:
+            return 1
+        return -1
+
+
+class Cauchy(RegressionLoss):
+    """Cauchy loss function.
+
+    Parameters
+    ----------
+    C
+
+    References
+    ----------
+    [^1]: ["Effect of MAE" Kaggle discussion](https://www.kaggle.com/c/allstate-claims-severity/discussion/24520#140163)
+    [^2]: [Paris Madness Kaggle kernel](https://www.kaggle.com/raddar/paris-madness)
+
+    """
+
+    def __init__(self, C=80):
+        self.C = C
+
+    def __call__(self, y_true, y_pred):
+        if isinstance(y_true, np.ndarray):
+            return np.abs(y_pred - y_true)
+        return abs(y_pred - y_true)
+
+    def gradient(self, y_true, y_pred):
+        diff = y_pred - y_true
+        return diff / ((diff / self.C) ** 2 + 1)
+
+
+class CrossEntropy(MultiClassLoss):
+    """Cross entropy loss.
+
+    This is a generalization of logistic loss to multiple classes.
+
+    Parameters
+    ----------
+    class_weight
+        A dictionary that indicates what weight to associate with each class.
+
+    Examples
+    --------
+
+    >>> from river import optim
+
+    >>> y_true = [0, 1, 2, 2]
+    >>> y_pred = [
+    ...     {0: 0.29450637, 1: 0.34216758, 2: 0.36332605},
+    ...     {0: 0.21290077, 1: 0.32728332, 2: 0.45981591},
+    ...     {0: 0.42860913, 1: 0.33380113, 2: 0.23758974},
+    ...     {0: 0.44941979, 1: 0.32962558, 2: 0.22095463}
+    ... ]
+
+    >>> loss = optim.losses.CrossEntropy()
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     print(loss(yt, yp))
+    1.222454
+    1.116929
+    1.437209
+    1.509797
+
+    >>> for yt, yp in zip(y_true, y_pred):
+    ...     print(loss.gradient(yt, yp))
+    {0: -0.70549363, 1: 0.34216758, 2: 0.36332605}
+    {0: 0.21290077, 1: -0.67271668, 2: 0.45981591}
+    {0: 0.42860913, 1: 0.33380113, 2: -0.76241026}
+    {0: 0.44941979, 1: 0.32962558, 2: -0.77904537}
+
+    References
+    ----------
+    [^1]: [What is Softmax regression and how is it related to Logistic regression?](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression.md)
+
+    """
+
+    def __init__(self, class_weight: typing.Dict[base.typing.ClfTarget, float] = None):
+        if class_weight is None:
+            class_weight = {}
+        self.class_weight = class_weight
+
+    def __call__(self, y_true, y_pred):
+        total = 0
+
+        for label, proba in y_pred.items():
+            if y_true == label:
+                total += self.class_weight.get(label, 1.0) * math.log(
+                    clamp_proba(proba)
+                )
+
+        return -total
+
+    def gradient(self, y_true, y_pred):
+        return {
+            label: (
+                self.class_weight.get(label, 1.0)
+                * (clamp_proba(y_pred.get(label, 0.0)) - (y_true == label))
+            )
+            for label in {*y_pred.keys(), y_true}
+        }
+
+
+class Hinge(BinaryLoss):
+    r"""Computes the hinge loss.
+
+    Mathematically, it is defined as
+
+    $$L = max(0, 1 - p_i * y_i)$$
+
+    It's gradient w.r.t. to $p_i$ is
+
+    $$
+    \\frac{\\partial L}{\\partial y_i} = \\left\{
+    \\begin{array}{ll}
+        \\ 0  &   p_iy_i \geqslant 1  \\\\
+        \\ - y_i & p_iy_i < 1
+    \\end{array}
+    \\right.
+    $$
+
+    Parameters
+    ----------
+    threshold
+        Margin threshold. 1 yield the loss used in SVMs, whilst 0 is equivalent to the loss used in
+        the Perceptron algorithm.
+
+    Examples
+    --------
+
+    >>> from river import optim
+
+    >>> loss = optim.losses.Hinge(threshold=1)
+    >>> loss(1, .2)
+    0.8
+
+    >>> loss.gradient(1, .2)
+    -1
+
+    """
+
+    def __init__(self, threshold=1.0):
+        self.threshold = threshold
+
+    def __call__(self, y_true, y_pred):
+        y_true = y_true * 2 - 1  # [0, 1] -> [-1, 1]
+
+        if isinstance(y_true, np.ndarray):
+            return np.maximum(self.threshold - y_true * y_pred, 0)
+
+        return max(self.threshold - y_true * y_pred, 0)
+
+    def gradient(self, y_true, y_pred):
+        y_true = y_true * 2 - 1  # [0, 1] -> [-1, 1]
+
+        if isinstance(y_true, np.ndarray):
+            return np.where(y_true * y_pred < self.threshold, -y_true, 0)
+
+        if y_true * y_pred <= self.threshold:
+            return -y_true
+        return 0
+
+
+class EpsilonInsensitiveHinge(RegressionLoss):
+    """Epsilon-insensitive hinge loss.
+
+    Parameters
+    ----------
+    eps
+
+    """
+
+    def __init__(self, eps=0.1):
+        self.eps = eps
+
+    def __call__(self, y_true, y_pred):
+        y_true = y_true * 2 - 1  # [0, 1] -> [-1, 1]
+
+        if isinstance(y_true, np.ndarray):
+            return np.maximum(np.abs(y_pred - y_true) - self.eps, 0)
+
+        return max(math.fabs(y_pred - y_true) - self.eps, 0)
+
+    def gradient(self, y_true, y_pred):
+        y_true = y_true * 2 - 1  # [0, 1] -> [-1, 1]
+
+        if isinstance(y_true, np.ndarray):
+            gradients = np.zeros_like(y_true)
+            gradients[y_pred > y_true + self.eps] = 1
+            gradients[y_pred + self.eps < y_true] = -1
+            return gradients
+
+        if y_pred > y_true + self.eps:
+            return 1
+        elif y_pred + self.eps < y_true:
+            return -1
+        return 0
+
+
+class Log(BinaryLoss):
+    """Logarithmic loss.
+
+    This loss function expects each provided `y_pred` to be a logit. In other words if must be
+    the raw output of a linear model or a neural network.
+
+    Parameters
+    ----------
+    weight_pos
+    weight_neg
+
+    References
+    ----------
+    [^1]: [Logit Wikipedia page](https://www.wikiwand.com/en/Logit>)
+
+    """
+
+    def __init__(self, weight_pos=1.0, weight_neg=1.0):
+        self.weight_pos = weight_pos
+        self.weight_neg = weight_neg
+
+    def __call__(self, y_true, y_pred):
+
+        if isinstance(y_true, np.ndarray):
+            weights = np.where(y_true == 0, self.weight_neg, self.weight_pos)
+            y_true = 2 * y_true - 1  # map {0, 1} to {-1, 1}
+            z = y_pred * y_true
+            return weights * np.log(1.0 + np.exp(-z))
+
+        weight = self.weight_pos
+        if y_true == 0:
+            y_true = -1
+            weight = self.weight_neg
+        else:
+            y_true = int(y_true)
+
+        z = y_pred * y_true
+        if z > 18.0:
+            return weight * math.exp(-z)
+        if z < -18.0:
+            return weight * -z
+        return weight * math.log(1.0 + math.exp(-z))
+
+    def gradient(self, y_true, y_pred):
+
+        if isinstance(y_true, np.ndarray):
+            weights = np.where(y_true == 0, self.weight_neg, self.weight_pos)
+            y_true = 2 * y_true - 1  # map {0, 1} to {-1, 1}
+            z = y_pred * y_true
+            return weights * -y_true / (np.exp(z) + 1.0)
+
+        weight = self.weight_pos
+        if y_true == 0:
+            y_true = -1
+            weight = self.weight_neg
+        else:
+            y_true = int(y_true)
+
+        z = y_pred * y_true
+        if z > 18.0:
+            return weight * math.exp(-z) * -y_true
+        if z < -18.0:
+            return weight * -y_true
+        return weight * -y_true / (math.exp(z) + 1.0)
+
+
+class Quantile(RegressionLoss):
+    """Quantile loss.
+
+    Parameters
+    ----------
+    alpha
+        Desired quantile to attain.
+
+    Examples
+    --------
+
+    >>> from river import optim
+
+    >>> loss = optim.losses.Quantile(0.5)
+    >>> loss(1, 3)
+    1.0
+
+    >>> loss.gradient(1, 3)
+    0.5
+
+    >>> loss.gradient(3, 1)
+    -0.5
+
+    References
+    ----------
+    [^1]: [Wikipedia article on quantile regression](https://www.wikiwand.com/en/Quantile_regression)
+    [^2]: [Derivative from WolframAlpha](https://www.wolframalpha.com/input/?i=derivative+(y+-+p)+*+(alpha+-+Boole(y+-+p))+wrt+p)
+
+    """
+
+    def __init__(self, alpha=0.5):
+        self.alpha = alpha
+
+    def __call__(self, y_true, y_pred):
+        diff = y_pred - y_true
+        return (self.alpha - (diff < 0)) * diff
+
+    def gradient(self, y_true, y_pred):
+        return (y_true < y_pred) - self.alpha
+
+
+class Squared(RegressionLoss):
+    """Squared loss, also known as the L2 loss.
+
+    Mathematically, it is defined as
+
+    $$L = (p_i - y_i) ^ 2$$
+
+    It's gradient w.r.t. to $p_i$ is
+
+    $$\\frac{\\partial L}{\\partial p_i} = 2 \times (p_i - y_i)$$
+
+    One thing to note is that this convention is consistent with Vowpal Wabbit and PyTorch, but
+    not with scikit-learn. Indeed, scikit-learn divides the loss by 2, making the 2 disappear in
+    the gradient.
+
+    Examples
+    --------
+
+    >>> from river import optim
+
+    >>> loss = optim.losses.Squared()
+    >>> loss(-4, 5)
+    81
+    >>> loss.gradient(-4, 5)
+    18
+    >>> loss.gradient(5, -4)
+    -18
+
+    """
+
+    def __call__(self, y_true, y_pred):
+        return (y_pred - y_true) * (y_pred - y_true)
+
+    def gradient(self, y_true, y_pred):
+        return 2 * (y_pred - y_true)
+
+
+class Huber(RegressionLoss):
+    """Huber loss.
+
+    Variant of the squared loss that is robust to outliers.
+
+    Parameters
+    ----------
+    epsilon
+
+    References
+    ----------
+    1. [Huber loss function - Wikipedia](https://en.wikipedia.org/wiki/Huber_Loss_Function)
+
+    """
+
+    def __init__(self, epsilon=0.1):
+        self.epsilon = epsilon
+
+    def __call__(self, y_true, y_pred):
+        r = y_pred - y_true
+
+        if isinstance(y_true, np.ndarray):
+            abs_r = np.abs(r)
+            return np.where(
+                abs_r <= self.epsilon,
+                0.5 * r * r,
+                self.epsilon * abs_r - (0.5 * self.epsilon * self.epsilon),
+            )
+
+        abs_r = abs(r)
+        if abs_r <= self.epsilon:
+            return 0.5 * r * r
+        return self.epsilon * abs_r - (0.5 * self.epsilon * self.epsilon)
+
+    def gradient(self, y_true, y_pred):
+        r = y_pred - y_true
+
+        if isinstance(y_true, np.ndarray):
+            abs_r = np.abs(r)
+            return np.where(
+                abs_r <= self.epsilon, r, np.where(r > 0.0, self.epsilon, -self.epsilon)
+            )
+
+        abs_r = abs(r)
+        if abs_r <= self.epsilon:
+            return r
+        elif r > 0.0:
+            return self.epsilon
+        return -self.epsilon
+
+
+class BinaryFocalLoss(BinaryLoss):
+    """Binary focal loss.
+
+    This implements the "star" algorithm from the appendix of the focal loss paper.
+
+    Parameters
+    ----------
+    gamma
+    beta
+
+    References
+    ----------
+    1. [Lin, T.Y., Goyal, P., Girshick, R., He, K. and Dollár, P., 2017. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision (pp. 2980-2988)](https://arxiv.org/pdf/1708.02002.pdf)
+
+    """
+
+    def __init__(self, gamma=2, beta=1):
+        self.gamma = gamma
+        self.beta = beta
+
+    def __call__(self, y_true, y_pred):
+
+        y_true = y_true * 2 - 1  # [0, 1] -> [-1, 1]
+
+        xt = y_true * y_pred
+
+        if isinstance(y_true, np.ndarray):
+            pt = 1.0 / (1 + np.exp(-(self.gamma * xt + self.beta)))
+            return -np.log(pt) / self.gamma
+
+        pt = utils.math.sigmoid(self.gamma * xt + self.beta)
+        return -math.log(pt) / self.gamma
+
+    def gradient(self, y_true, y_pred):
+
+        y_true = y_true * 2 - 1  # [0, 1] -> [-1, 1]
+
+        xt = y_true * y_pred
+
+        if isinstance(y_true, np.ndarray):
+            pt = 1.0 / (1 + np.exp(-(self.gamma * xt + self.beta)))
+            return y_true * (pt - 1)
+
+        pt = utils.math.sigmoid(self.gamma * xt + self.beta)
+        return y_true * (pt - 1)
+
+
+class Poisson(RegressionLoss):
+    """Poisson loss.
+
+    The Poisson loss is usually more suited for regression with count data than the squared loss.
+
+    Mathematically, it is defined as
+
+    $$L = exp(p_i) - y_i \\times p_i$$
+
+    It's gradient w.r.t. to $p_i$ is
+
+    $$\\frac{\\partial L}{\\partial p_i} = exp(p_i) - y_i$$
+
+    """
+
+    def __call__(self, y_true, y_pred):
+        if isinstance(y_pred, np.ndarray):
+            return np.exp(y_pred) - y_true * y_pred
+        return math.exp(y_pred) - y_true * y_pred
+
+    def gradient(self, y_true, y_pred):
+        if isinstance(y_pred, np.ndarray):
+            return np.exp(y_pred) - y_true
+        return math.exp(y_pred) - y_true
+
+    def mean_func(self, y_pred):
+        if isinstance(y_pred, np.ndarray):
+            return np.exp(y_pred)
+        return math.exp(y_pred)
```

### Comparing `river-0.8.0/river/optim/momentum.py` & `river-0.9.0/river/optim/momentum.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,50 +1,50 @@
-import collections
-
-from . import base
-
-__all__ = ["Momentum"]
-
-
-class Momentum(base.Optimizer):
-    """Momentum optimizer.
-
-    Parameters
-    ----------
-    lr
-    rho
-
-    Examples
-    --------
-
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import linear_model
-    >>> from river import metrics
-    >>> from river import optim
-    >>> from river import preprocessing
-
-    >>> dataset = datasets.Phishing()
-    >>> optimizer = optim.Momentum()
-    >>> model = (
-    ...     preprocessing.StandardScaler() |
-    ...     linear_model.LogisticRegression(optimizer)
-    ... )
-    >>> metric = metrics.F1()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    F1: 0.841645
-
-    """
-
-    def __init__(self, lr=0.1, rho=0.9):
-        super().__init__(lr)
-        self.rho = rho
-        self.s = collections.defaultdict(float)
-
-    def _step(self, w, g):
-
-        for i, gi in g.items():
-            self.s[i] = self.rho * self.s[i] + self.learning_rate * gi
-            w[i] -= self.s[i]
-
-        return w
+import collections
+
+from . import base
+
+__all__ = ["Momentum"]
+
+
+class Momentum(base.Optimizer):
+    """Momentum optimizer.
+
+    Parameters
+    ----------
+    lr
+    rho
+
+    Examples
+    --------
+
+    >>> from river import datasets
+    >>> from river import evaluate
+    >>> from river import linear_model
+    >>> from river import metrics
+    >>> from river import optim
+    >>> from river import preprocessing
+
+    >>> dataset = datasets.Phishing()
+    >>> optimizer = optim.Momentum()
+    >>> model = (
+    ...     preprocessing.StandardScaler() |
+    ...     linear_model.LogisticRegression(optimizer)
+    ... )
+    >>> metric = metrics.F1()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    F1: 0.841645
+
+    """
+
+    def __init__(self, lr=0.1, rho=0.9):
+        super().__init__(lr)
+        self.rho = rho
+        self.s = collections.defaultdict(float)
+
+    def _step_with_dict(self, w, g):
+
+        for i, gi in g.items():
+            self.s[i] = self.rho * self.s[i] + self.learning_rate * gi
+            w[i] -= self.s[i]
+
+        return w
```

### Comparing `river-0.8.0/river/optim/nadam.py` & `river-0.9.0/river/optim/ada_bound.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,74 +1,92 @@
-import collections
-import math
-
-from . import base
-
-__all__ = ["Nadam"]
-
-
-class Nadam(base.Optimizer):
-    """Nadam optimizer.
-
-    Parameters
-    ----------
-    lr
-    beta_1
-    beta_2
-    eps
-
-    Examples
-    --------
-
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import linear_model
-    >>> from river import metrics
-    >>> from river import optim
-    >>> from river import preprocessing
-
-    >>> dataset = datasets.Phishing()
-    >>> optimizer = optim.Nadam()
-    >>> model = (
-    ...     preprocessing.StandardScaler() |
-    ...     linear_model.LogisticRegression(optimizer)
-    ... )
-    >>> metric = metrics.F1()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    F1: 0.865961
-
-    References
-    ----------
-    [^1]: [Nadam: A combination of adam and nesterov](https://ruder.io/optimizing-gradient-descent/index.html#nadam)
-
-    """
-
-    def __init__(self, lr=0.1, beta_1=0.9, beta_2=0.999, eps=1e-8):
-        super().__init__(lr)
-        self.beta_1 = beta_1
-        self.beta_2 = beta_2
-        self.eps = eps
-        self.m = collections.defaultdict(float)
-        self.v = collections.defaultdict(float)
-
-    def _step(self, w, g):
-
-        for i, gi in g.items():
-            self.m[i] = self.beta_1 * self.m[i] + (1 - self.beta_1) * gi
-            m_hat = self.m[i] / (1 - math.pow(self.beta_1, self.n_iterations + 1))
-
-            self.v[i] = self.beta_2 * self.v[i] + (1 - self.beta_2) * gi ** 2
-            v_hat = self.v[i] / (1 - math.pow(self.beta_2, self.n_iterations + 1))
-
-            w[i] -= (
-                self.learning_rate
-                * (
-                    self.beta_1 * m_hat
-                    + (1 - self.beta_1)
-                    * gi
-                    / (1 - math.pow(self.beta_1, self.n_iterations + 1))
-                )
-                / (v_hat ** 0.5 + self.eps)
-            )
-
-        return w
+import collections
+import math
+
+from river import utils
+
+from . import base
+
+__all__ = ["AdaBound"]
+
+
+class AdaBound(base.Optimizer):
+    """AdaBound optimizer.
+
+    Parameters
+    ----------
+    lr
+        The learning rate.
+    beta_1
+    beta_2
+    eps
+    gamma
+    final_lr
+
+    Attributes
+    ----------
+    m : collections.defaultdict
+    s : collections.defaultdict
+
+    Examples
+    --------
+
+    >>> from river import datasets
+    >>> from river import evaluate
+    >>> from river import linear_model
+    >>> from river import metrics
+    >>> from river import optim
+    >>> from river import preprocessing
+
+    >>> dataset = datasets.Phishing()
+    >>> optimizer = optim.AdaBound()
+    >>> model = (
+    ...     preprocessing.StandardScaler() |
+    ...     linear_model.LogisticRegression(optimizer)
+    ... )
+    >>> metric = metrics.F1()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    F1: 0.879004
+
+    References
+    ----------
+    [^1]: [Luo, L., Xiong, Y., Liu, Y. and Sun, X., 2019. Adaptive gradient methods with dynamic bound of learning rate. arXiv preprint arXiv:1902.09843](https://arxiv.org/abs/1902.09843)
+
+    """
+
+    def __init__(
+        self, lr=1e-3, beta_1=0.9, beta_2=0.999, eps=1e-8, gamma=1e-3, final_lr=0.1
+    ):
+        super().__init__(lr)
+        self.base_lr = lr
+        self.final_lr = final_lr
+        self.beta_1 = beta_1
+        self.beta_2 = beta_2
+        self.eps = eps
+        self.gamma = gamma
+        self.m = collections.defaultdict(float)
+        self.v = collections.defaultdict(float)
+
+    def _step_with_dict(self, w, g):
+
+        bias_1 = 1 - self.beta_1 ** (self.n_iterations + 1)
+        bias_2 = 1 - self.beta_2 ** (self.n_iterations + 1)
+
+        step_size = self.learning_rate * math.sqrt(bias_2) / bias_1
+        self.final_lr *= self.learning_rate / self.base_lr
+
+        lower_bound = self.final_lr * (
+            1 - 1 / (self.gamma * (self.n_iterations + 1) + 1)
+        )
+        upper_bound = self.final_lr * (1 + 1 / (self.gamma * (self.n_iterations + 1)))
+
+        for i, gi in g.items():
+            self.m[i] = self.beta_1 * self.m[i] + (1 - self.beta_1) * gi
+            self.v[i] = self.beta_2 * self.v[i] + (1 - self.beta_2) * gi ** 2
+
+            step_size_bound = step_size / (math.sqrt(self.v[i]) + self.eps)
+
+            w[i] -= (
+                utils.math.clamp(step_size_bound, lower_bound, upper_bound) * self.m[i]
+            )
+
+        return w
```

### Comparing `river-0.8.0/river/optim/newton.py` & `river-0.9.0/river/optim/newton.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,45 +1,45 @@
-from .. import utils
-from . import base
-
-__all__ = ["Newton"]
-
-
-class Newton(base.Optimizer):
-    """Online Newton Step (ONS) optimizer.
-
-    This optimizer uses second-order information (i.e. the Hessian of the cost function) in
-    addition to first-order information (i.e. the gradient of the cost function).
-
-    Parameters
-    ----------
-    lr
-    eps
-
-    References
-    ----------
-    [^1]: [Hazan, E., Agarwal, A. and Kale, S., 2007. Logarithmic regret algorithms for online convex optimization. Machine Learning, 69(2-3), pp.169-192](https://www.cs.princeton.edu/~ehazan/papers/log-journal.pdf)
-
-    """
-
-    def __init__(self, lr=0.1, eps=1e-5):
-        super().__init__(lr)
-        self.eps = eps
-        self.H_inv = {}
-
-    def _step(self, w, g):
-
-        for i in g:
-            if (i, i) not in self.H_inv:
-                self.H_inv[i, i] = self.eps
-
-        # Update the Hessian
-        self.H = utils.math.sherman_morrison(A_inv=self.H_inv, u=g, v=g)
-
-        # Calculate the update step
-        step = utils.math.dotvecmat(x=g, A=self.H_inv)
-
-        # Update the weights
-        for i, s in step.items():
-            w[i] -= self.learning_rate * s
-
-        return w
+from .. import utils
+from . import base
+
+__all__ = ["Newton"]
+
+
+class Newton(base.Optimizer):
+    """Online Newton Step (ONS) optimizer.
+
+    This optimizer uses second-order information (i.e. the Hessian of the cost function) in
+    addition to first-order information (i.e. the gradient of the cost function).
+
+    Parameters
+    ----------
+    lr
+    eps
+
+    References
+    ----------
+    [^1]: [Hazan, E., Agarwal, A. and Kale, S., 2007. Logarithmic regret algorithms for online convex optimization. Machine Learning, 69(2-3), pp.169-192](https://www.cs.princeton.edu/~ehazan/papers/log-journal.pdf)
+
+    """
+
+    def __init__(self, lr=0.1, eps=1e-5):
+        super().__init__(lr)
+        self.eps = eps
+        self.H_inv = {}
+
+    def _step_with_dict(self, w, g):
+
+        for i in g:
+            if (i, i) not in self.H_inv:
+                self.H_inv[i, i] = self.eps
+
+        # Update the Hessian
+        self.H = utils.math.sherman_morrison(A_inv=self.H_inv, u=g, v=g)
+
+        # Calculate the update step
+        step = utils.math.dotvecmat(x=g, A=self.H_inv)
+
+        # Update the weights
+        for i, s in step.items():
+            w[i] -= self.learning_rate * s
+
+        return w
```

### Comparing `river-0.8.0/river/preprocessing/__init__.py` & `river-0.9.0/river/preprocessing/__init__.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,36 +1,40 @@
-"""Feature preprocessing.
-
-The purpose of this module is to modify an existing set of features so that they can be processed
-by a machine learning algorithm. This may be done by scaling numeric parts of the data or by
-one-hot encoding categorical features. The difference with the `feature_extraction` module is that
-the latter extracts new information from the data
-
-"""
-from .feature_hasher import FeatureHasher
-from .impute import PreviousImputer, StatImputer
-from .lda import LDA
-from .one_hot import OneHotEncoder
-from .scale import (
-    AdaptiveStandardScaler,
-    Binarizer,
-    MaxAbsScaler,
-    MinMaxScaler,
-    Normalizer,
-    RobustScaler,
-    StandardScaler,
-)
-
-__all__ = [
-    "AdaptiveStandardScaler",
-    "Binarizer",
-    "FeatureHasher",
-    "LDA",
-    "MaxAbsScaler",
-    "MinMaxScaler",
-    "Normalizer",
-    "OneHotEncoder",
-    "PreviousImputer",
-    "RobustScaler",
-    "StandardScaler",
-    "StatImputer",
-]
+"""Feature preprocessing.
+
+The purpose of this module is to modify an existing set of features so that they can be processed
+by a machine learning algorithm. This may be done by scaling numeric parts of the data or by
+one-hot encoding categorical features. The difference with the `feature_extraction` module is that
+the latter extracts new information from the data
+
+"""
+from .feature_hasher import FeatureHasher
+from .impute import PreviousImputer, StatImputer
+from .lda import LDA
+from .one_hot import OneHotEncoder
+from .pred_clipper import PredClipper
+from .scale import (
+    AdaptiveStandardScaler,
+    Binarizer,
+    MaxAbsScaler,
+    MinMaxScaler,
+    Normalizer,
+    RobustScaler,
+    StandardScaler,
+    TargetStandardScaler,
+)
+
+__all__ = [
+    "AdaptiveStandardScaler",
+    "Binarizer",
+    "FeatureHasher",
+    "LDA",
+    "MaxAbsScaler",
+    "MinMaxScaler",
+    "Normalizer",
+    "OneHotEncoder",
+    "PredClipper",
+    "PreviousImputer",
+    "RobustScaler",
+    "StandardScaler",
+    "StatImputer",
+    "TargetStandardScaler",
+]
```

### Comparing `river-0.8.0/river/preprocessing/feature_hasher.py` & `river-0.9.0/river/preprocessing/feature_hasher.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,71 +1,71 @@
-import collections
-import hashlib
-
-import numpy as np
-
-from river import base
-
-__all__ = ["FeatureHasher"]
-
-
-class FeatureHasher(base.Transformer):
-    """Implements the hashing trick.
-
-    Each pair of (name, value) features is hashed into a random integer. A module operator is then
-    used to make sure the hash is in a certain range. We use the Murmurhash implementation from
-    scikit-learn.
-
-    Parameters
-    ----------
-    n_features
-        The number by which each hash will be moduloed by.
-    alternate_sign
-        Whether or not half of the hashes will be negated.
-    seed
-        Set the seed to produce identical results.
-
-    Examples
-    --------
-
-    >>> import river
-
-    >>> hasher = river.preprocessing.FeatureHasher(n_features=10, seed=42)
-
-    >>> X = [
-    ...     {'dog': 1, 'cat': 2, 'elephant': 4},
-    ...     {'dog': 2, 'run': 5}
-    ... ]
-    >>> for x in X:
-    ...     print(hasher.transform_one(x))
-    Counter({1: 4, 9: 2, 8: 1})
-    Counter({4: 5, 8: 2})
-
-    References
-    ----------
-    [^1]: [Wikipedia article on feature vectorization using the hashing trick](https://www.wikiwand.com/en/Feature_hashing#/Feature_vectorization_using_hashing_trick)
-
-    """
-
-    def __init__(self, n_features=1048576, seed: int = None):
-        self.n_features = n_features
-        self.seed = seed
-        self._salt = np.random.RandomState(seed).bytes(hashlib.blake2s.SALT_SIZE)
-
-    def _hash(self, x):
-        hexa = hashlib.blake2s(bytes(x, encoding="utf8"), salt=self._salt).hexdigest()
-        return int(hexa, 16)
-
-    def transform_one(self, x):
-
-        x_hashed = collections.Counter()
-
-        for feature, value in x.items():
-
-            if isinstance(value, str):
-                feature = f"{feature}={value}"
-                value = 1
-
-            i = self._hash(feature) % self.n_features
-            x_hashed[i] += value
-
-        return x_hashed
+import collections
+import hashlib
+
+import numpy as np
+
+from river import base
+
+__all__ = ["FeatureHasher"]
+
+
+class FeatureHasher(base.Transformer):
+    """Implements the hashing trick.
+
+    Each pair of (name, value) features is hashed into a random integer. A module operator is then
+    used to make sure the hash is in a certain range. We use the Murmurhash implementation from
+    scikit-learn.
+
+    Parameters
+    ----------
+    n_features
+        The number by which each hash will be moduloed by.
+    alternate_sign
+        Whether or not half of the hashes will be negated.
+    seed
+        Set the seed to produce identical results.
+
+    Examples
+    --------
+
+    >>> import river
+
+    >>> hasher = river.preprocessing.FeatureHasher(n_features=10, seed=42)
+
+    >>> X = [
+    ...     {'dog': 1, 'cat': 2, 'elephant': 4},
+    ...     {'dog': 2, 'run': 5}
+    ... ]
+    >>> for x in X:
+    ...     print(hasher.transform_one(x))
+    Counter({1: 4, 9: 2, 8: 1})
+    Counter({4: 5, 8: 2})
+
+    References
+    ----------
+    [^1]: [Wikipedia article on feature vectorization using the hashing trick](https://www.wikiwand.com/en/Feature_hashing#/Feature_vectorization_using_hashing_trick)
+
+    """
+
+    def __init__(self, n_features=1048576, seed: int = None):
+        self.n_features = n_features
+        self.seed = seed
+        self._salt = np.random.RandomState(seed).bytes(hashlib.blake2s.SALT_SIZE)
+
+    def _hash(self, x):
+        hexa = hashlib.blake2s(bytes(x, encoding="utf8"), salt=self._salt).hexdigest()
+        return int(hexa, 16)
+
+    def transform_one(self, x):
+
+        x_hashed = collections.Counter()
+
+        for feature, value in x.items():
+
+            if isinstance(value, str):
+                feature = f"{feature}={value}"
+                value = 1
+
+            i = self._hash(feature) % self.n_features
+            x_hashed[i] += value
+
+        return x_hashed
```

### Comparing `river-0.8.0/river/preprocessing/lda.py` & `river-0.9.0/river/preprocessing/lda.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,422 +1,422 @@
-import functools
-import typing
-from collections import defaultdict
-
-import numpy as np
-from scipy import ndimage, special
-
-from river import base
-
-__all__ = ["LDA"]
-
-
-class LDA(base.Transformer):
-    """Online Latent Dirichlet Allocation with Infinite Vocabulary.
-
-    Latent Dirichlet allocation (LDA) is a probabilistic approach for exploring topics in document
-    collections. The key advantage of this variant is that it assumes an infinite vocabulary,
-    meaning that the set of tokens does not have to known in advance, as opposed to the
-    [implementation from sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)
-    The results produced by this implementation are identical to those from [the original
-    implementation](https://github.com/kzhai/PyInfVoc) proposed by the method's authors.
-
-    This class takes as input token counts. Therefore, it requires you to tokenize beforehand. You
-    can do so by using a `feature_extraction.BagOfWords` instance, as shown in the example below.
-
-    Parameters
-    ----------
-    n_components
-        Number of topics of the latent Drichlet allocation.
-    number_of_documents
-        Estimated number of documents.
-    alpha_theta
-        Hyper-parameter of the Dirichlet distribution of topics.
-    alpha_beta
-        Hyper-parameter of the Dirichlet process of distribution over words.
-    tau
-        Learning inertia to prevent premature convergence.
-    kappa
-        The learning rate kappa controls how quickly new parameters estimates replace the old ones.
-        kappa ∈ (0.5, 1] is required for convergence.
-    vocab_prune_interval
-        Interval at which to refresh the words topics distribution.
-    number_of_samples
-        Number of iteration to computes documents topics distribution.
-    ranking_smooth_factor
-    burn_in_sweeps
-        Number of iteration necessaries while analyzing a document before updating document topics
-        distribution.
-    maximum_size_vocabulary
-        Maximum size of the stored vocabulary.
-    seed
-        Random number seed used for reproducibility.
-
-    Attributes
-    ----------
-    counter : int
-        The current number of observed documents.
-    truncation_size_prime : int
-        Number of distincts words stored in the vocabulary. Updated before processing a document.
-    truncation_size : int
-        Number of distincts words stored in the vocabulary. Updated after processing a document.
-    word_to_index : dict
-        Words as keys and indexes as values.
-    index_to_word : dict
-        Indexes as keys and words as values.
-    nu_1 : dict
-        Weights of the words. Component of the variational inference.
-    nu_2 : dict
-        Weights of the words. Component of the variational inference.
-
-    Examples
-    --------
-
-    >>> from river import compose
-    >>> from river import feature_extraction
-    >>> from river import preprocessing
-
-    >>> X = [
-    ...    'weather cold',
-    ...    'weather hot dry',
-    ...    'weather cold rainy',
-    ...    'weather hot',
-    ...    'weather cold humid',
-    ... ]
-
-    >>> lda = compose.Pipeline(
-    ...     feature_extraction.BagOfWords(),
-    ...     preprocessing.LDA(
-    ...         n_components=2,
-    ...         number_of_documents=60,
-    ...         seed=42
-    ...     )
-    ... )
-
-    >>> for x in X:
-    ...     lda = lda.learn_one(x)
-    ...     topics = lda.transform_one(x)
-    ...     print(topics)
-    {0: 2.5, 1: 0.5}
-    {0: 2.5, 1: 1.5}
-    {0: 1.5, 1: 2.5}
-    {0: 1.5, 1: 1.5}
-    {0: 0.5, 1: 3.5}
-
-    References
-    ----------
-    [^1]: [Zhai, K. and Boyd-Graber, J., 2013, February. Online latent Dirichlet allocation with infinite vocabulary. In International Conference on Machine Learning (pp. 561-569).](http://proceedings.mlr.press/v28/zhai13.pdf)
-    [^2]: [PyInfVoc on GitHub](https://github.com/kzhai/PyInfVoc)
-
-    """
-
-    def __init__(
-        self,
-        n_components=10,
-        number_of_documents=1e6,
-        alpha_theta=0.5,
-        alpha_beta=100.0,
-        tau=64.0,
-        kappa=0.75,
-        vocab_prune_interval=10,
-        number_of_samples=10,
-        ranking_smooth_factor=1e-12,
-        burn_in_sweeps=5,
-        maximum_size_vocabulary=4000,
-        seed: int = None,
-    ):
-
-        self.n_components = n_components
-        self.number_of_documents = number_of_documents
-        self.alpha_theta = alpha_theta
-        self.alpha_beta = alpha_beta
-        self.tau = tau
-        self.kappa = kappa
-        self.vocab_prune_interval = vocab_prune_interval
-        self.number_of_samples = number_of_samples
-        self.ranking_smooth_factor = ranking_smooth_factor
-        self.burn_in_sweeps = burn_in_sweeps
-        self.maximum_size_vocabulary = maximum_size_vocabulary
-        self.seed = seed
-        self.rng = np.random.RandomState(seed)
-
-        self.counter = 0
-        self.truncation_size_prime = 1
-        self.truncation_size = 1
-
-        self.word_to_index = {}
-        self.index_to_word = {}
-
-        self.nu_1 = defaultdict(functools.partial(np.ones, 1))
-        self.nu_2 = defaultdict(functools.partial(np.array, [self.alpha_beta]))
-
-        for topic in range(self.n_components):
-            self.nu_1[topic] = np.ones(1)
-            self.nu_2[topic] = np.array([self.alpha_beta])
-
-    def learn_transform_one(self, x: dict) -> dict:
-        """Equivalent to `lda.learn_one(x).transform_one(x)`s, but faster.
-
-        Parameters
-        ----------
-        x: A document.
-
-        Returns
-        -------
-        Component attributions for the input document.
-
-        """
-
-        # Updates number of documents:
-        self.counter += 1
-
-        # Extracts words of the document as a list of words:
-        word_list = x.keys()
-
-        # Update words indexes:
-        self._update_indexes(word_list=word_list)
-
-        # Replace the words by their index:
-        words_indexes_list = [self.word_to_index[word] for word in word_list]
-
-        # Sample empirical topic assignment:
-        (
-            statistics,
-            batch_document_topic_distribution,
-        ) = self._compute_statistics_components(words_indexes_list)
-
-        # Online variational inference
-        self._update_weights(statistics=statistics)
-
-        if self.counter % self.vocab_prune_interval == 0:
-            self._prune_vocabulary()
-
-        return dict(enumerate(batch_document_topic_distribution))
-
-    def learn_one(self, x):
-        self.learn_transform_one(x)
-        return self
-
-    def transform_one(self, x):
-
-        # Extracts words of the document as a list of words:
-        word_list = x.keys()
-
-        # Update words indexes:
-        self._update_indexes(word_list=word_list)
-
-        # Replace the words by their index:
-        words_indexes_list = [self.word_to_index[word] for word in word_list]
-
-        # Sample empirical topic assignment:
-        _, components = self._compute_statistics_components(words_indexes_list)
-
-        return dict(enumerate(components))
-
-    def _update_indexes(self, word_list: list):
-        """
-        Adds the words of the document to the index if they are not part of the current vocabulary.
-        Updates of the number of distinct words seen.
-
-        Parameters
-        ----------
-        word_list
-            Content of the document as a list of words.
-
-        """
-        for word in word_list:
-            if word not in self.word_to_index:
-                new_index = len(self.word_to_index) + 1
-                self.word_to_index[word] = new_index
-                self.index_to_word[new_index] = word
-                self.truncation_size_prime += 1
-
-    @classmethod
-    def _compute_weights(
-        cls, n_components: int, nu_1: dict, nu_2: dict
-    ) -> typing.Tuple[dict, dict]:
-        """Calculates the vocabulary weighting according to the word distribution present in the
-        vocabulary.
-
-        The Psi function is the logarithmic derivative of the gamma function.
-
-        Parameters
-        ----------
-        n_components
-            Number of topics.
-        nu_1
-            Weights of the words of the vocabulary.
-        nu_2
-            Weights of the words of the vocabulary.
-
-        Returns
-        -------
-        Weights of the words of the current vocabulary.
-
-        """
-        exp_weights = {}
-        exp_oov_weights = {}
-
-        for topic in range(n_components):
-
-            psi_nu_1 = special.psi(nu_1[topic])
-            psi_nu_2 = special.psi(nu_2[topic])
-
-            psi_nu_1_nu_2 = special.psi(nu_1[topic] + nu_2[topic])
-
-            psi_nu_1_nu_2_minus_psi_nu_2 = np.cumsum([psi_nu_2 - psi_nu_1_nu_2], axis=1)
-
-            exp_oov_weights[topic] = np.exp(psi_nu_1_nu_2_minus_psi_nu_2[0][-1])
-
-            psi_nu_1_nu_2_minus_psi_nu_2 = ndimage.interpolation.shift(
-                input=psi_nu_1_nu_2_minus_psi_nu_2[0], shift=1, cval=0
-            )
-
-            exp_weights[topic] = np.exp(
-                psi_nu_1 - psi_nu_1_nu_2 + psi_nu_1_nu_2_minus_psi_nu_2
-            )
-
-        return exp_weights, exp_oov_weights
-
-    def _update_weights(self, statistics):
-        """Learn documents and word representations. Calculate the variational approximation.
-
-        Parameters
-        ----------
-        statistics
-            Weights associated to the words.
-
-        """
-        reverse_cumulated_phi = {}
-
-        for k in range(self.n_components):
-
-            reverse_cumulated_phi[k] = ndimage.interpolation.shift(
-                input=statistics[k], shift=-1, cval=0
-            )
-
-            reverse_cumulated_phi[k] = np.flip(reverse_cumulated_phi[k])
-            reverse_cumulated_phi[k] = np.cumsum(reverse_cumulated_phi[k])
-            reverse_cumulated_phi[k] = np.flip(reverse_cumulated_phi[k])
-
-        # Epsilon will be between 0 and 1.
-        # Epsilon value says how much to weight the information we got from this document.
-        self.epsilon = (self.tau + self.counter) ** -self.kappa
-
-        for k in range(self.n_components):
-
-            if self.truncation_size < self.truncation_size_prime:
-
-                difference_truncation = (
-                    self.truncation_size_prime - self.truncation_size
-                )
-
-                self.nu_1[k] = np.append(self.nu_1[k], np.ones(difference_truncation))
-                self.nu_2[k] = np.append(self.nu_2[k], np.ones(difference_truncation))
-
-            # Variational Approximation
-            self.nu_1[k] += self.epsilon * (
-                self.number_of_documents * np.array(statistics[k]) + 1 - self.nu_1[k]
-            )
-
-            self.nu_2[k] += self.epsilon * (
-                self.alpha_beta
-                + self.number_of_documents * np.array(reverse_cumulated_phi[k])
-                - self.nu_2[k]
-            )
-
-        self.truncation_size = self.truncation_size_prime
-
-    def _compute_statistics_components(
-        self, words_indexes_list: list
-    ) -> typing.Tuple[dict, dict]:
-        """Extract latent variables from the document and words.
-
-        Parameters
-        ----------
-        words_indexes_list
-            Ids of the words of the input document.
-
-        Returns
-        -------
-        Computed statistics over the words. Document reprensetation across topics.
-
-        """
-        statistics = defaultdict(lambda: np.zeros(self.truncation_size_prime))
-
-        exp_weights, exp_oov_weights = self._compute_weights(
-            n_components=self.n_components, nu_1=self.nu_1, nu_2=self.nu_2
-        )
-
-        size_vocab = len(words_indexes_list)
-
-        phi = self.rng.random((self.n_components, size_vocab))
-
-        phi = phi / np.sum(phi, axis=0)
-
-        phi_sum = np.sum(phi, axis=1)
-
-        for sample_index in range(self.number_of_samples):
-
-            for word_index in range(size_vocab):
-
-                phi_sum -= phi[:, word_index]
-                phi_sum = phi_sum.clip(min=0)
-                temp_phi = phi_sum + self.alpha_theta
-
-                for k in range(self.n_components):
-
-                    if words_indexes_list[word_index] >= self.truncation_size:
-
-                        temp_phi[k] *= exp_oov_weights[k]
-                    else:
-                        temp_phi[k] *= exp_weights[k][words_indexes_list[word_index]]
-
-                # Normalize document topic distribution before applying multinomial distribution:
-                temp_phi /= temp_phi.sum()
-
-                # Sample a topic based a given probability distribution:
-                temp_phi = self.rng.multinomial(1, temp_phi)
-
-                phi[:, word_index] = temp_phi
-
-                phi_sum += temp_phi
-
-                if sample_index >= self.burn_in_sweeps:
-
-                    for k in range(self.n_components):
-
-                        index = words_indexes_list[word_index]
-
-                        statistics[k][index] += temp_phi[k]
-
-        document_topic_distribution = self.alpha_theta + phi_sum
-
-        for k in range(self.n_components):
-
-            statistics[k] /= self.number_of_samples - self.burn_in_sweeps
-
-        return statistics, document_topic_distribution
-
-    def _prune_vocabulary(self):
-        """Reduce the size of the index exceeds the maximum size."""
-        if self.nu_1[0].shape[0] > self.maximum_size_vocabulary:
-
-            for topic in range(self.n_components):
-                # Updates words latent variables
-                self.nu_1[topic] = self.nu_1[topic][: self.maximum_size_vocabulary]
-                self.nu_2[topic] = self.nu_2[topic][: self.maximum_size_vocabulary]
-
-            new_word_to_index = {}
-            new_index_to_word = {}
-
-            for index in range(1, self.maximum_size_vocabulary):
-                # Updates words indexes
-                word = self.index_to_word[index]
-                new_word_to_index[word] = index
-                new_index_to_word[index] = word
-
-            self.word_to_index = new_word_to_index
-            self.index_to_word = new_index_to_word
-
-            self.truncation_size = self.nu_1[0].shape[0]
-            self.truncation_size_prime = self.truncation_size
+import functools
+import typing
+from collections import defaultdict
+
+import numpy as np
+from scipy import ndimage, special
+
+from river import base
+
+__all__ = ["LDA"]
+
+
+class LDA(base.Transformer):
+    """Online Latent Dirichlet Allocation with Infinite Vocabulary.
+
+    Latent Dirichlet allocation (LDA) is a probabilistic approach for exploring topics in document
+    collections. The key advantage of this variant is that it assumes an infinite vocabulary,
+    meaning that the set of tokens does not have to known in advance, as opposed to the
+    [implementation from sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)
+    The results produced by this implementation are identical to those from [the original
+    implementation](https://github.com/kzhai/PyInfVoc) proposed by the method's authors.
+
+    This class takes as input token counts. Therefore, it requires you to tokenize beforehand. You
+    can do so by using a `feature_extraction.BagOfWords` instance, as shown in the example below.
+
+    Parameters
+    ----------
+    n_components
+        Number of topics of the latent Drichlet allocation.
+    number_of_documents
+        Estimated number of documents.
+    alpha_theta
+        Hyper-parameter of the Dirichlet distribution of topics.
+    alpha_beta
+        Hyper-parameter of the Dirichlet process of distribution over words.
+    tau
+        Learning inertia to prevent premature convergence.
+    kappa
+        The learning rate kappa controls how quickly new parameters estimates replace the old ones.
+        kappa ∈ (0.5, 1] is required for convergence.
+    vocab_prune_interval
+        Interval at which to refresh the words topics distribution.
+    number_of_samples
+        Number of iteration to computes documents topics distribution.
+    ranking_smooth_factor
+    burn_in_sweeps
+        Number of iteration necessaries while analyzing a document before updating document topics
+        distribution.
+    maximum_size_vocabulary
+        Maximum size of the stored vocabulary.
+    seed
+        Random number seed used for reproducibility.
+
+    Attributes
+    ----------
+    counter : int
+        The current number of observed documents.
+    truncation_size_prime : int
+        Number of distincts words stored in the vocabulary. Updated before processing a document.
+    truncation_size : int
+        Number of distincts words stored in the vocabulary. Updated after processing a document.
+    word_to_index : dict
+        Words as keys and indexes as values.
+    index_to_word : dict
+        Indexes as keys and words as values.
+    nu_1 : dict
+        Weights of the words. Component of the variational inference.
+    nu_2 : dict
+        Weights of the words. Component of the variational inference.
+
+    Examples
+    --------
+
+    >>> from river import compose
+    >>> from river import feature_extraction
+    >>> from river import preprocessing
+
+    >>> X = [
+    ...    'weather cold',
+    ...    'weather hot dry',
+    ...    'weather cold rainy',
+    ...    'weather hot',
+    ...    'weather cold humid',
+    ... ]
+
+    >>> lda = compose.Pipeline(
+    ...     feature_extraction.BagOfWords(),
+    ...     preprocessing.LDA(
+    ...         n_components=2,
+    ...         number_of_documents=60,
+    ...         seed=42
+    ...     )
+    ... )
+
+    >>> for x in X:
+    ...     lda = lda.learn_one(x)
+    ...     topics = lda.transform_one(x)
+    ...     print(topics)
+    {0: 0.5, 1: 2.5}
+    {0: 1.5, 1: 2.5}
+    {0: 3.5, 1: 0.5}
+    {0: 1.5, 1: 1.5}
+    {0: 2.5, 1: 1.5}
+
+    References
+    ----------
+    [^1]: [Zhai, K. and Boyd-Graber, J., 2013, February. Online latent Dirichlet allocation with infinite vocabulary. In International Conference on Machine Learning (pp. 561-569).](http://proceedings.mlr.press/v28/zhai13.pdf)
+    [^2]: [PyInfVoc on GitHub](https://github.com/kzhai/PyInfVoc)
+
+    """
+
+    def __init__(
+        self,
+        n_components=10,
+        number_of_documents=1e6,
+        alpha_theta=0.5,
+        alpha_beta=100.0,
+        tau=64.0,
+        kappa=0.75,
+        vocab_prune_interval=10,
+        number_of_samples=10,
+        ranking_smooth_factor=1e-12,
+        burn_in_sweeps=5,
+        maximum_size_vocabulary=4000,
+        seed: int = None,
+    ):
+
+        self.n_components = n_components
+        self.number_of_documents = number_of_documents
+        self.alpha_theta = alpha_theta
+        self.alpha_beta = alpha_beta
+        self.tau = tau
+        self.kappa = kappa
+        self.vocab_prune_interval = vocab_prune_interval
+        self.number_of_samples = number_of_samples
+        self.ranking_smooth_factor = ranking_smooth_factor
+        self.burn_in_sweeps = burn_in_sweeps
+        self.maximum_size_vocabulary = maximum_size_vocabulary
+        self.seed = seed
+        self.rng = np.random.RandomState(seed)
+
+        self.counter = 0
+        self.truncation_size_prime = 1
+        self.truncation_size = 1
+
+        self.word_to_index = {}
+        self.index_to_word = {}
+
+        self.nu_1 = defaultdict(functools.partial(np.ones, 1))
+        self.nu_2 = defaultdict(functools.partial(np.array, [self.alpha_beta]))
+
+        for topic in range(self.n_components):
+            self.nu_1[topic] = np.ones(1)
+            self.nu_2[topic] = np.array([self.alpha_beta])
+
+    def learn_transform_one(self, x: dict) -> dict:
+        """Equivalent to `lda.learn_one(x).transform_one(x)`s, but faster.
+
+        Parameters
+        ----------
+        x: A document.
+
+        Returns
+        -------
+        Component attributions for the input document.
+
+        """
+
+        # Updates number of documents:
+        self.counter += 1
+
+        # Extracts words of the document as a list of words:
+        word_list = x.keys()
+
+        # Update words indexes:
+        self._update_indexes(word_list=word_list)
+
+        # Replace the words by their index:
+        words_indexes_list = [self.word_to_index[word] for word in word_list]
+
+        # Sample empirical topic assignment:
+        (
+            statistics,
+            batch_document_topic_distribution,
+        ) = self._compute_statistics_components(words_indexes_list)
+
+        # Online variational inference
+        self._update_weights(statistics=statistics)
+
+        if self.counter % self.vocab_prune_interval == 0:
+            self._prune_vocabulary()
+
+        return dict(enumerate(batch_document_topic_distribution))
+
+    def learn_one(self, x):
+        self.learn_transform_one(x)
+        return self
+
+    def transform_one(self, x):
+
+        # Extracts words of the document as a list of words:
+        word_list = x.keys()
+
+        # Update words indexes:
+        self._update_indexes(word_list=word_list)
+
+        # Replace the words by their index:
+        words_indexes_list = [self.word_to_index[word] for word in word_list]
+
+        # Sample empirical topic assignment:
+        _, components = self._compute_statistics_components(words_indexes_list)
+
+        return dict(enumerate(components))
+
+    def _update_indexes(self, word_list: list):
+        """
+        Adds the words of the document to the index if they are not part of the current vocabulary.
+        Updates of the number of distinct words seen.
+
+        Parameters
+        ----------
+        word_list
+            Content of the document as a list of words.
+
+        """
+        for word in word_list:
+            if word not in self.word_to_index:
+                new_index = len(self.word_to_index) + 1
+                self.word_to_index[word] = new_index
+                self.index_to_word[new_index] = word
+                self.truncation_size_prime += 1
+
+    @classmethod
+    def _compute_weights(
+        cls, n_components: int, nu_1: dict, nu_2: dict
+    ) -> typing.Tuple[dict, dict]:
+        """Calculates the vocabulary weighting according to the word distribution present in the
+        vocabulary.
+
+        The Psi function is the logarithmic derivative of the gamma function.
+
+        Parameters
+        ----------
+        n_components
+            Number of topics.
+        nu_1
+            Weights of the words of the vocabulary.
+        nu_2
+            Weights of the words of the vocabulary.
+
+        Returns
+        -------
+        Weights of the words of the current vocabulary.
+
+        """
+        exp_weights = {}
+        exp_oov_weights = {}
+
+        for topic in range(n_components):
+
+            psi_nu_1 = special.psi(nu_1[topic])
+            psi_nu_2 = special.psi(nu_2[topic])
+
+            psi_nu_1_nu_2 = special.psi(nu_1[topic] + nu_2[topic])
+
+            psi_nu_1_nu_2_minus_psi_nu_2 = np.cumsum([psi_nu_2 - psi_nu_1_nu_2], axis=1)
+
+            exp_oov_weights[topic] = np.exp(psi_nu_1_nu_2_minus_psi_nu_2[0][-1])
+
+            psi_nu_1_nu_2_minus_psi_nu_2 = ndimage.interpolation.shift(
+                input=psi_nu_1_nu_2_minus_psi_nu_2[0], shift=1, cval=0
+            )
+
+            exp_weights[topic] = np.exp(
+                psi_nu_1 - psi_nu_1_nu_2 + psi_nu_1_nu_2_minus_psi_nu_2
+            )
+
+        return exp_weights, exp_oov_weights
+
+    def _update_weights(self, statistics):
+        """Learn documents and word representations. Calculate the variational approximation.
+
+        Parameters
+        ----------
+        statistics
+            Weights associated to the words.
+
+        """
+        reverse_cumulated_phi = {}
+
+        for k in range(self.n_components):
+
+            reverse_cumulated_phi[k] = ndimage.interpolation.shift(
+                input=statistics[k], shift=-1, cval=0
+            )
+
+            reverse_cumulated_phi[k] = np.flip(reverse_cumulated_phi[k])
+            reverse_cumulated_phi[k] = np.cumsum(reverse_cumulated_phi[k])
+            reverse_cumulated_phi[k] = np.flip(reverse_cumulated_phi[k])
+
+        # Epsilon will be between 0 and 1.
+        # Epsilon value says how much to weight the information we got from this document.
+        self.epsilon = (self.tau + self.counter) ** -self.kappa
+
+        for k in range(self.n_components):
+
+            if self.truncation_size < self.truncation_size_prime:
+
+                difference_truncation = (
+                    self.truncation_size_prime - self.truncation_size
+                )
+
+                self.nu_1[k] = np.append(self.nu_1[k], np.ones(difference_truncation))
+                self.nu_2[k] = np.append(self.nu_2[k], np.ones(difference_truncation))
+
+            # Variational Approximation
+            self.nu_1[k] += self.epsilon * (
+                self.number_of_documents * np.array(statistics[k]) + 1 - self.nu_1[k]
+            )
+
+            self.nu_2[k] += self.epsilon * (
+                self.alpha_beta
+                + self.number_of_documents * np.array(reverse_cumulated_phi[k])
+                - self.nu_2[k]
+            )
+
+        self.truncation_size = self.truncation_size_prime
+
+    def _compute_statistics_components(
+        self, words_indexes_list: list
+    ) -> typing.Tuple[dict, dict]:
+        """Extract latent variables from the document and words.
+
+        Parameters
+        ----------
+        words_indexes_list
+            Ids of the words of the input document.
+
+        Returns
+        -------
+        Computed statistics over the words. Document reprensetation across topics.
+
+        """
+        statistics = defaultdict(lambda: np.zeros(self.truncation_size_prime))
+
+        exp_weights, exp_oov_weights = self._compute_weights(
+            n_components=self.n_components, nu_1=self.nu_1, nu_2=self.nu_2
+        )
+
+        size_vocab = len(words_indexes_list)
+
+        phi = self.rng.random((self.n_components, size_vocab))
+
+        phi /= np.sum(phi, axis=0)
+
+        phi_sum = np.sum(phi, axis=1)
+
+        for sample_index in range(self.number_of_samples):
+
+            for word_index in range(size_vocab):
+
+                phi_sum -= phi[:, word_index]
+                phi_sum = phi_sum.clip(min=0)
+                temp_phi = phi_sum + self.alpha_theta
+
+                for k in range(self.n_components):
+
+                    if words_indexes_list[word_index] >= self.truncation_size:
+
+                        temp_phi[k] *= exp_oov_weights[k]
+                    else:
+                        temp_phi[k] *= exp_weights[k][words_indexes_list[word_index]]
+
+                # Normalize document topic distribution before applying multinomial distribution:
+                temp_phi /= temp_phi.sum()
+
+                # Sample a topic based a given probability distribution:
+                temp_phi = self.rng.multinomial(1, temp_phi)
+
+                phi[:, word_index] = temp_phi
+
+                phi_sum += temp_phi
+
+                if sample_index >= self.burn_in_sweeps:
+
+                    for k in range(self.n_components):
+
+                        index = words_indexes_list[word_index]
+
+                        statistics[k][index] += temp_phi[k]
+
+        document_topic_distribution = self.alpha_theta + phi_sum
+
+        for k in range(self.n_components):
+
+            statistics[k] /= self.number_of_samples - self.burn_in_sweeps
+
+        return statistics, document_topic_distribution
+
+    def _prune_vocabulary(self):
+        """Reduce the size of the index exceeds the maximum size."""
+        if self.nu_1[0].shape[0] > self.maximum_size_vocabulary:
+
+            for topic in range(self.n_components):
+                # Updates words latent variables
+                self.nu_1[topic] = self.nu_1[topic][: self.maximum_size_vocabulary]
+                self.nu_2[topic] = self.nu_2[topic][: self.maximum_size_vocabulary]
+
+            new_word_to_index = {}
+            new_index_to_word = {}
+
+            for index in range(1, self.maximum_size_vocabulary):
+                # Updates words indexes
+                word = self.index_to_word[index]
+                new_word_to_index[word] = index
+                new_index_to_word[index] = word
+
+            self.word_to_index = new_word_to_index
+            self.index_to_word = new_index_to_word
+
+            self.truncation_size = self.nu_1[0].shape[0]
+            self.truncation_size_prime = self.truncation_size
```

### Comparing `river-0.8.0/river/preprocessing/scale.py` & `river-0.9.0/river/preprocessing/scale.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,554 +1,606 @@
-import collections
-import functools
-import itertools
-import numbers
-
-import numpy as np
-import pandas as pd
-
-from river import base, stats, utils
-
-__all__ = [
-    "AdaptiveStandardScaler",
-    "Binarizer",
-    "MaxAbsScaler",
-    "MinMaxScaler",
-    "Normalizer",
-    "RobustScaler",
-    "StandardScaler",
-]
-
-
-def safe_div(a, b):
-    """Returns a if b is nil, else divides a by b.
-
-    When scaling, sometimes a denominator might be nil. For instance, during standard scaling
-    the denominator can be nil if a feature has no variance.
-
-    """
-    return a / b if b else 0.0
-
-
-class Binarizer(base.Transformer):
-    """Binarizes the data to 0 or 1 according to a threshold.
-
-    Parameters
-    ----------
-    threshold
-        Values above this are replaced by 1 and the others by 0.
-    dtype
-        The desired data type to apply.
-
-    Examples
-    --------
-
-    >>> import river
-    >>> import numpy as np
-
-    >>> rng = np.random.RandomState(42)
-    >>> X = [{'x1': v, 'x2': int(v)} for v in rng.uniform(low=-4, high=4, size=6)]
-
-    >>> binarizer = river.preprocessing.Binarizer()
-    >>> for x in X:
-    ...     print(binarizer.learn_one(x).transform_one(x))
-    {'x1': False, 'x2': False}
-    {'x1': True, 'x2': True}
-    {'x1': True, 'x2': True}
-    {'x1': True, 'x2': False}
-    {'x1': False, 'x2': False}
-    {'x1': False, 'x2': False}
-
-    """
-
-    def __init__(self, threshold=0.0, dtype=bool):
-        self.threshold = threshold
-        self.dtype = dtype
-
-    def transform_one(self, x):
-        x_tf = x.copy()
-
-        for i, xi in x_tf.items():
-            if isinstance(xi, numbers.Number):
-                x_tf[i] = self.dtype(xi > self.threshold)
-
-        return x_tf
-
-
-class StandardScaler(base.Transformer):
-    """Scales the data so that it has zero mean and unit variance.
-
-    Under the hood, a running mean and a running variance are maintained. The scaling is slightly
-    different than when scaling the data in batch because the exact means and variances are not
-    known in advance. However, this doesn't have a detrimental impact on performance in the long
-    run.
-
-    This transformer supports mini-batches as well as single instances. In the mini-batch case, the
-    number of columns and the ordering of the columns are allowed to change between subsequent
-    calls. In other words, this transformer will keep working even if you add and/or remove
-    features every time you call `learn_many` and `transform_many`.
-
-    Parameters
-    ----------
-    with_std
-        Whether or not each feature should be divided by its standard deviation.
-
-    Examples
-    --------
-
-    >>> import random
-    >>> from river import preprocessing
-
-    >>> random.seed(42)
-    >>> X = [{'x': random.uniform(8, 12), 'y': random.uniform(8, 12)} for _ in range(6)]
-    >>> for x in X:
-    ...     print(x)
-    {'x': 10.557, 'y': 8.100}
-    {'x': 9.100, 'y': 8.892}
-    {'x': 10.945, 'y': 10.706}
-    {'x': 11.568, 'y': 8.347}
-    {'x': 9.687, 'y': 8.119}
-    {'x': 8.874, 'y': 10.021}
-
-    >>> scaler = preprocessing.StandardScaler()
-
-    >>> for x in X:
-    ...     print(scaler.learn_one(x).transform_one(x))
-    {'x': 0.0, 'y': 0.0}
-    {'x': -0.999, 'y': 0.999}
-    {'x': 0.937, 'y': 1.350}
-    {'x': 1.129, 'y': -0.651}
-    {'x': -0.776, 'y': -0.729}
-    {'x': -1.274, 'y': 0.992}
-
-    This transformer also supports mini-batch updates. You can call `learn_many` and provide a
-    `pandas.DataFrame`:
-
-    >>> import pandas as pd
-    >>> X = pd.DataFrame.from_dict(X)
-
-    >>> scaler = preprocessing.StandardScaler()
-    >>> scaler = scaler.learn_many(X[:3])
-    >>> scaler = scaler.learn_many(X[3:])
-
-    You can then call `transform_many` to scale a mini-batch of features:
-
-    >>> scaler.transform_many(X)
-        x         y
-    0  0.444600 -0.933384
-    1 -1.044259 -0.138809
-    2  0.841106  1.679208
-    3  1.477301 -0.685117
-    4 -0.444084 -0.914195
-    5 -1.274664  0.992296
-
-    References
-    ----------
-    [^1]: [Welford's Method (and Friends)](https://www.embeddedrelated.com/showarticle/785.php)
-    [^2]: [Batch updates for simple statistics](https://notmatthancock.github.io/2017/03/23/simple-batch-stat-updates.html)
-
-    """
-
-    def __init__(self, with_std=True):
-        self.with_std = with_std
-        self.counts = collections.Counter()
-        self.means = collections.defaultdict(float)
-        self.vars = collections.defaultdict(float)
-
-    def learn_one(self, x):
-
-        for i, xi in x.items():
-            self.counts[i] += 1
-            old_mean = self.means[i]
-            self.means[i] += (xi - old_mean) / self.counts[i]
-            if self.with_std:
-                self.vars[i] += (
-                    (xi - old_mean) * (xi - self.means[i]) - self.vars[i]
-                ) / self.counts[i]
-
-        return self
-
-    def transform_one(self, x):
-        if self.with_std:
-            return {
-                i: safe_div(xi - self.means[i], self.vars[i] ** 0.5)
-                for i, xi in x.items()
-            }
-        return {i: xi - self.means[i] for i, xi in x.items()}
-
-    def learn_many(self, X: pd.DataFrame):
-        """Update with a mini-batch of features.
-
-        Note that the update formulas for mean and variance are slightly different than in the
-        single instance case, but they produce exactly the same result.
-
-        Parameters
-        ----------
-        X
-            A dataframe where each column is a feature.
-
-        """
-
-        # Operating on X.values, which is a view to the underlying numpy array, is slightly faster
-        # than operating on X
-        columns = X.columns
-        X = X.values
-
-        # In the rest of this method, old_* refers to the existing statistics, whilst new_* refers
-        # to the statistics of the current mini-batch.
-
-        new_means = np.nanmean(X, axis=0)
-        # We could call np.var, but we already have the mean so we can be smart
-        if self.with_std:
-            new_vars = np.einsum("ij,ij->j", X, X) / len(X) - new_means ** 2
-        else:
-            new_vars = []
-        new_counts = np.sum(~np.isnan(X), axis=0)
-
-        for col, new_mean, new_var, new_count in itertools.zip_longest(
-            columns, new_means, new_vars, new_counts
-        ):
-
-            old_mean = self.means[col]
-            old_var = self.vars[col]
-            old_count = self.counts[col]
-
-            a = old_count / (old_count + new_count)
-            b = new_count / (old_count + new_count)
-
-            self.means[col] = a * old_mean + b * new_mean
-            if self.with_std:
-                self.vars[col] = (
-                    a * old_var + b * new_var + a * b * (old_mean - new_mean) ** 2
-                )
-            self.counts[col] += new_count
-
-        return self
-
-    def transform_many(self, X: pd.DataFrame):
-        """Scale a mini-batch of features.
-
-        Parameters
-        ----------
-        X
-            A dataframe where each column is a feature. An exception will be raised if any of
-            the features has not been seen during a previous call to `learn_many`.
-
-        """
-
-        means = np.array([self.means[c] for c in X.columns])
-        Xt = X.values - means
-
-        if self.with_std:
-            stds = np.array([self.vars[c] ** 0.5 for c in X.columns])
-            np.divide(Xt, stds, where=stds > 0, out=Xt)
-
-        return pd.DataFrame(Xt, index=X.index, columns=X.columns, copy=False)
-
-
-class MinMaxScaler(base.Transformer):
-    """Scales the data to a fixed range from 0 to 1.
-
-    Under the hood a running min and a running peak to peak (max - min) are maintained.
-
-    Attributes
-    ----------
-    min : dict
-        Mapping between features and instances of `stats.Min`.
-    max : dict
-        Mapping between features and instances of `stats.Max`.
-
-    Examples
-    --------
-
-    >>> import random
-    >>> from river import preprocessing
-
-    >>> random.seed(42)
-    >>> X = [{'x': random.uniform(8, 12)} for _ in range(5)]
-    >>> for x in X:
-    ...     print(x)
-    {'x': 10.557707}
-    {'x': 8.100043}
-    {'x': 9.100117}
-    {'x': 8.892842}
-    {'x': 10.945884}
-
-    >>> scaler = preprocessing.MinMaxScaler()
-
-    >>> for x in X:
-    ...     print(scaler.learn_one(x).transform_one(x))
-    {'x': 0.0}
-    {'x': 0.0}
-    {'x': 0.406920}
-    {'x': 0.322582}
-    {'x': 1.0}
-
-    """
-
-    def __init__(self):
-        self.min = collections.defaultdict(stats.Min)
-        self.max = collections.defaultdict(stats.Max)
-
-    def learn_one(self, x):
-
-        for i, xi in x.items():
-            self.min[i].update(xi)
-            self.max[i].update(xi)
-
-        return self
-
-    def transform_one(self, x):
-        return {
-            i: safe_div(xi - self.min[i].get(), self.max[i].get() - self.min[i].get())
-            for i, xi in x.items()
-        }
-
-
-class MaxAbsScaler(base.Transformer):
-    """Scales the data to a [-1, 1] range based on absolute maximum.
-
-    Under the hood a running absolute max is maintained. This scaler is meant for
-    data that is already centered at zero or sparse data. It does not shift/center
-    the data, and thus does not destroy any sparsity.
-
-    Attributes
-    ----------
-    abs_max : dict
-        Mapping between features and instances of `stats.AbsMax`.
-
-    Examples
-    --------
-
-    >>> import random
-    >>> from river import preprocessing
-
-    >>> random.seed(42)
-    >>> X = [{'x': random.uniform(8, 12)} for _ in range(5)]
-    >>> for x in X:
-    ...     print(x)
-    {'x': 10.557707}
-    {'x': 8.100043}
-    {'x': 9.100117}
-    {'x': 8.892842}
-    {'x': 10.945884}
-
-    >>> scaler = preprocessing.MaxAbsScaler()
-
-    >>> for x in X:
-    ...     print(scaler.learn_one(x).transform_one(x))
-    {'x': 1.0}
-    {'x': 0.767216}
-    {'x': 0.861940}
-    {'x': 0.842308}
-    {'x': 1.0}
-
-    """
-
-    def __init__(self):
-        self.abs_max = collections.defaultdict(stats.AbsMax)
-
-    def learn_one(self, x):
-
-        for i, xi in x.items():
-            self.abs_max[i].update(xi)
-
-        return self
-
-    def transform_one(self, x):
-        return {i: safe_div(xi, self.abs_max[i].get()) for i, xi in x.items()}
-
-
-class RobustScaler(base.Transformer):
-    """Scale features using statistics that are robust to outliers.
-
-    This Scaler removes the median and scales the data according to the
-    interquantile range.
-
-    Parameters
-    ----------
-    with_centering
-        Whether to centre the data before scaling.
-    with_scaling
-        Whether to scale data to IQR.
-    q_inf
-        Desired inferior quantile, must be between 0 and 1.
-    q_sup
-        Desired superior quantile, must be between 0 and 1.
-
-    Attributes
-    ----------
-    median : dict
-        Mapping between features and instances of `stats.Quantile(0.5)`.
-    iqr : dict
-        Mapping between features and instances of `stats.IQR`.
-
-    Examples
-    --------
-
-    >>> from pprint import pprint
-    >>> import random
-    >>> from river import preprocessing
-
-    >>> random.seed(42)
-    >>> X = [{'x': random.uniform(8, 12)} for _ in range(5)]
-    >>> pprint(X)
-    [{'x': 10.557707},
-        {'x': 8.100043},
-        {'x': 9.100117},
-        {'x': 8.892842},
-        {'x': 10.945884}]
-
-    >>> scaler = preprocessing.RobustScaler()
-
-    >>> for x in X:
-    ...     print(scaler.learn_one(x).transform_one(x))
-    {'x': 0.0}
-    {'x': -1.0}
-    {'x': 0.0}
-    {'x': -0.124499}
-    {'x': 1.108659}
-
-    """
-
-    def __init__(self, with_centering=True, with_scaling=True, q_inf=0.25, q_sup=0.75):
-        self.with_centering = with_centering
-        self.with_scaling = with_scaling
-        self.q_inf = q_inf
-        self.q_sup = q_sup
-        self.median = collections.defaultdict(functools.partial(stats.Quantile, 0.5))
-        self.iqr = collections.defaultdict(
-            functools.partial(stats.IQR, self.q_inf, self.q_sup)
-        )
-
-    def learn_one(self, x):
-
-        for i, xi in x.items():
-            if self.with_centering:
-                self.median[i].update(xi)
-            if self.with_scaling:
-                self.iqr[i].update(xi)
-
-        return self
-
-    def transform_one(self, x):
-        x_tf = {}
-
-        for i, xi in x.items():
-            x_tf[i] = xi
-            if self.with_centering:
-                x_tf[i] -= self.median[i].get()
-            if self.with_scaling:
-                x_tf[i] = safe_div(x_tf[i], self.iqr[i].get())
-
-        return x_tf
-
-
-class Normalizer(base.Transformer):
-    """Scales a set of features so that it has unit norm.
-
-    This is particularly useful when used after a `feature_extraction.TFIDF`.
-
-    Parameters
-    ----------
-    order
-        Order of the norm (e.g. 2 corresponds to the $L^2$ norm).
-
-    Examples
-    --------
-
-    >>> from river import preprocessing
-    >>> from river import stream
-
-    >>> scaler = preprocessing.Normalizer(order=2)
-
-    >>> X = [[4, 1, 2, 2],
-    ...      [1, 3, 9, 3],
-    ...      [5, 7, 5, 1]]
-
-    >>> for x, _ in stream.iter_array(X):
-    ...     print(scaler.transform_one(x))
-    {0: 0.8, 1: 0.2, 2: 0.4, 3: 0.4}
-    {0: 0.1, 1: 0.3, 2: 0.9, 3: 0.3}
-    {0: 0.5, 1: 0.7, 2: 0.5, 3: 0.1}
-
-    """
-
-    def __init__(self, order=2):
-        self.order = order
-
-    def transform_one(self, x):
-        norm = utils.math.norm(x, order=self.order)
-        return {i: xi / norm for i, xi in x.items()}
-
-
-class AdaptiveStandardScaler(base.Transformer):
-    """Scales data using exponentially weighted moving average and variance.
-
-    Under the hood, a exponentially weighted running mean and variance are maintained for each
-    feature. This can potentially provide better results for drifting data in comparison to
-    `preprocessing.StandardScaler`. Indeed, the latter computes a global mean and variance for each
-    feature, whereas this scaler weights data in proportion to their recency.
-
-    Parameters
-    ----------
-    alpha
-        This parameter is passed to `stats.EWVar`. It is expected to be in [0, 1]. More weight is
-        assigned to recent samples the closer `alpha` is to 1.
-
-    Examples
-    --------
-
-    Consider the following series which contains a positive trend.
-
-    >>> import random
-
-    >>> random.seed(42)
-    >>> X = [
-    ...     {'x': random.uniform(4 + i, 6 + i)}
-    ...     for i in range(8)
-    ... ]
-    >>> for x in X:
-    ...     print(x)
-    {'x': 5.278}
-    {'x': 5.050}
-    {'x': 6.550}
-    {'x': 7.446}
-    {'x': 9.472}
-    {'x': 10.353}
-    {'x': 11.784}
-    {'x': 11.173}
-
-    This scaler works well with this kind of data because it uses statistics that assign higher
-    weight to more recent data.
-
-    >>> from river import preprocessing
-
-    >>> scaler = preprocessing.AdaptiveStandardScaler(alpha=.6)
-
-    >>> for x in X:
-    ...     print(scaler.learn_one(x).transform_one(x))
-    {'x': 0.0}
-    {'x': -0.816}
-    {'x': 0.812}
-    {'x': 0.695}
-    {'x': 0.754}
-    {'x': 0.598}
-    {'x': 0.651}
-    {'x': 0.124}
-
-    """
-
-    def __init__(self, alpha=0.3):
-        self.alpha = alpha
-        self.vars = collections.defaultdict(functools.partial(stats.EWVar, self.alpha))
-
-    def learn_one(self, x):
-        for i, xi in x.items():
-            self.vars[i].update(xi)
-        return self
-
-    def transform_one(self, x):
-        return {
-            i: safe_div(xi - self.vars[i].mean.get(), self.vars[i].get() ** 0.5)
-            for i, xi in x.items()
-        }
+import collections
+import functools
+import itertools
+import numbers
+
+import numpy as np
+import pandas as pd
+
+from river import base, compose, stats, utils
+
+__all__ = [
+    "AdaptiveStandardScaler",
+    "Binarizer",
+    "MaxAbsScaler",
+    "MinMaxScaler",
+    "Normalizer",
+    "RobustScaler",
+    "StandardScaler",
+    "TargetStandardScaler",
+]
+
+
+def safe_div(a, b):
+    """Returns a if b is nil, else divides a by b.
+
+    When scaling, sometimes a denominator might be nil. For instance, during standard scaling
+    the denominator can be nil if a feature has no variance.
+
+    """
+    return a / b if b else 0.0
+
+
+class Binarizer(base.Transformer):
+    """Binarizes the data to 0 or 1 according to a threshold.
+
+    Parameters
+    ----------
+    threshold
+        Values above this are replaced by 1 and the others by 0.
+    dtype
+        The desired data type to apply.
+
+    Examples
+    --------
+
+    >>> import river
+    >>> import numpy as np
+
+    >>> rng = np.random.RandomState(42)
+    >>> X = [{'x1': v, 'x2': int(v)} for v in rng.uniform(low=-4, high=4, size=6)]
+
+    >>> binarizer = river.preprocessing.Binarizer()
+    >>> for x in X:
+    ...     print(binarizer.learn_one(x).transform_one(x))
+    {'x1': False, 'x2': False}
+    {'x1': True, 'x2': True}
+    {'x1': True, 'x2': True}
+    {'x1': True, 'x2': False}
+    {'x1': False, 'x2': False}
+    {'x1': False, 'x2': False}
+
+    """
+
+    def __init__(self, threshold=0.0, dtype=bool):
+        self.threshold = threshold
+        self.dtype = dtype
+
+    def transform_one(self, x):
+        x_tf = x.copy()
+
+        for i, xi in x_tf.items():
+            if isinstance(xi, numbers.Number):
+                x_tf[i] = self.dtype(xi > self.threshold)
+
+        return x_tf
+
+
+class StandardScaler(base.Transformer):
+    """Scales the data so that it has zero mean and unit variance.
+
+    Under the hood, a running mean and a running variance are maintained. The scaling is slightly
+    different than when scaling the data in batch because the exact means and variances are not
+    known in advance. However, this doesn't have a detrimental impact on performance in the long
+    run.
+
+    This transformer supports mini-batches as well as single instances. In the mini-batch case, the
+    number of columns and the ordering of the columns are allowed to change between subsequent
+    calls. In other words, this transformer will keep working even if you add and/or remove
+    features every time you call `learn_many` and `transform_many`.
+
+    Parameters
+    ----------
+    with_std
+        Whether or not each feature should be divided by its standard deviation.
+
+    Examples
+    --------
+
+    >>> import random
+    >>> from river import preprocessing
+
+    >>> random.seed(42)
+    >>> X = [{'x': random.uniform(8, 12), 'y': random.uniform(8, 12)} for _ in range(6)]
+    >>> for x in X:
+    ...     print(x)
+    {'x': 10.557, 'y': 8.100}
+    {'x': 9.100, 'y': 8.892}
+    {'x': 10.945, 'y': 10.706}
+    {'x': 11.568, 'y': 8.347}
+    {'x': 9.687, 'y': 8.119}
+    {'x': 8.874, 'y': 10.021}
+
+    >>> scaler = preprocessing.StandardScaler()
+
+    >>> for x in X:
+    ...     print(scaler.learn_one(x).transform_one(x))
+    {'x': 0.0, 'y': 0.0}
+    {'x': -0.999, 'y': 0.999}
+    {'x': 0.937, 'y': 1.350}
+    {'x': 1.129, 'y': -0.651}
+    {'x': -0.776, 'y': -0.729}
+    {'x': -1.274, 'y': 0.992}
+
+    This transformer also supports mini-batch updates. You can call `learn_many` and provide a
+    `pandas.DataFrame`:
+
+    >>> import pandas as pd
+    >>> X = pd.DataFrame.from_dict(X)
+
+    >>> scaler = preprocessing.StandardScaler()
+    >>> scaler = scaler.learn_many(X[:3])
+    >>> scaler = scaler.learn_many(X[3:])
+
+    You can then call `transform_many` to scale a mini-batch of features:
+
+    >>> scaler.transform_many(X)
+        x         y
+    0  0.444600 -0.933384
+    1 -1.044259 -0.138809
+    2  0.841106  1.679208
+    3  1.477301 -0.685117
+    4 -0.444084 -0.914195
+    5 -1.274664  0.992296
+
+    References
+    ----------
+    [^1]: [Welford's Method (and Friends)](https://www.embeddedrelated.com/showarticle/785.php)
+    [^2]: [Batch updates for simple statistics](https://notmatthancock.github.io/2017/03/23/simple-batch-stat-updates.html)
+
+    """
+
+    def __init__(self, with_std=True):
+        self.with_std = with_std
+        self.counts = collections.Counter()
+        self.means = collections.defaultdict(float)
+        self.vars = collections.defaultdict(float)
+
+    def learn_one(self, x):
+
+        for i, xi in x.items():
+            self.counts[i] += 1
+            old_mean = self.means[i]
+            self.means[i] += (xi - old_mean) / self.counts[i]
+            if self.with_std:
+                self.vars[i] += (
+                    (xi - old_mean) * (xi - self.means[i]) - self.vars[i]
+                ) / self.counts[i]
+
+        return self
+
+    def transform_one(self, x):
+        if self.with_std:
+            return {
+                i: safe_div(xi - self.means[i], self.vars[i] ** 0.5)
+                for i, xi in x.items()
+            }
+        return {i: xi - self.means[i] for i, xi in x.items()}
+
+    def learn_many(self, X: pd.DataFrame):
+        """Update with a mini-batch of features.
+
+        Note that the update formulas for mean and variance are slightly different than in the
+        single instance case, but they produce exactly the same result.
+
+        Parameters
+        ----------
+        X
+            A dataframe where each column is a feature.
+
+        """
+
+        # Operating on X.values, which is a view to the underlying numpy array, is slightly faster
+        # than operating on X
+        columns = X.columns
+        X = X.values
+
+        # In the rest of this method, old_* refers to the existing statistics, whilst new_* refers
+        # to the statistics of the current mini-batch.
+
+        new_means = np.nanmean(X, axis=0)
+        # We could call np.var, but we already have the mean so we can be smart
+        if self.with_std:
+            new_vars = np.einsum("ij,ij->j", X, X) / len(X) - new_means ** 2
+        else:
+            new_vars = []
+        new_counts = np.sum(~np.isnan(X), axis=0)
+
+        for col, new_mean, new_var, new_count in itertools.zip_longest(
+            columns, new_means, new_vars, new_counts
+        ):
+
+            old_mean = self.means[col]
+            old_var = self.vars[col]
+            old_count = self.counts[col]
+
+            a = old_count / (old_count + new_count)
+            b = new_count / (old_count + new_count)
+
+            self.means[col] = a * old_mean + b * new_mean
+            if self.with_std:
+                self.vars[col] = (
+                    a * old_var + b * new_var + a * b * (old_mean - new_mean) ** 2
+                )
+            self.counts[col] += new_count
+
+        return self
+
+    def transform_many(self, X: pd.DataFrame):
+        """Scale a mini-batch of features.
+
+        Parameters
+        ----------
+        X
+            A dataframe where each column is a feature. An exception will be raised if any of
+            the features has not been seen during a previous call to `learn_many`.
+
+        """
+
+        means = np.array([self.means[c] for c in X.columns])
+        Xt = X.values - means
+
+        if self.with_std:
+            stds = np.array([self.vars[c] ** 0.5 for c in X.columns])
+            np.divide(Xt, stds, where=stds > 0, out=Xt)
+
+        return pd.DataFrame(Xt, index=X.index, columns=X.columns, copy=False)
+
+
+class MinMaxScaler(base.Transformer):
+    """Scales the data to a fixed range from 0 to 1.
+
+    Under the hood a running min and a running peak to peak (max - min) are maintained.
+
+    Attributes
+    ----------
+    min : dict
+        Mapping between features and instances of `stats.Min`.
+    max : dict
+        Mapping between features and instances of `stats.Max`.
+
+    Examples
+    --------
+
+    >>> import random
+    >>> from river import preprocessing
+
+    >>> random.seed(42)
+    >>> X = [{'x': random.uniform(8, 12)} for _ in range(5)]
+    >>> for x in X:
+    ...     print(x)
+    {'x': 10.557707}
+    {'x': 8.100043}
+    {'x': 9.100117}
+    {'x': 8.892842}
+    {'x': 10.945884}
+
+    >>> scaler = preprocessing.MinMaxScaler()
+
+    >>> for x in X:
+    ...     print(scaler.learn_one(x).transform_one(x))
+    {'x': 0.0}
+    {'x': 0.0}
+    {'x': 0.406920}
+    {'x': 0.322582}
+    {'x': 1.0}
+
+    """
+
+    def __init__(self):
+        self.min = collections.defaultdict(stats.Min)
+        self.max = collections.defaultdict(stats.Max)
+
+    def learn_one(self, x):
+
+        for i, xi in x.items():
+            self.min[i].update(xi)
+            self.max[i].update(xi)
+
+        return self
+
+    def transform_one(self, x):
+        return {
+            i: safe_div(xi - self.min[i].get(), self.max[i].get() - self.min[i].get())
+            for i, xi in x.items()
+        }
+
+
+class MaxAbsScaler(base.Transformer):
+    """Scales the data to a [-1, 1] range based on absolute maximum.
+
+    Under the hood a running absolute max is maintained. This scaler is meant for
+    data that is already centered at zero or sparse data. It does not shift/center
+    the data, and thus does not destroy any sparsity.
+
+    Attributes
+    ----------
+    abs_max : dict
+        Mapping between features and instances of `stats.AbsMax`.
+
+    Examples
+    --------
+
+    >>> import random
+    >>> from river import preprocessing
+
+    >>> random.seed(42)
+    >>> X = [{'x': random.uniform(8, 12)} for _ in range(5)]
+    >>> for x in X:
+    ...     print(x)
+    {'x': 10.557707}
+    {'x': 8.100043}
+    {'x': 9.100117}
+    {'x': 8.892842}
+    {'x': 10.945884}
+
+    >>> scaler = preprocessing.MaxAbsScaler()
+
+    >>> for x in X:
+    ...     print(scaler.learn_one(x).transform_one(x))
+    {'x': 1.0}
+    {'x': 0.767216}
+    {'x': 0.861940}
+    {'x': 0.842308}
+    {'x': 1.0}
+
+    """
+
+    def __init__(self):
+        self.abs_max = collections.defaultdict(stats.AbsMax)
+
+    def learn_one(self, x):
+
+        for i, xi in x.items():
+            self.abs_max[i].update(xi)
+
+        return self
+
+    def transform_one(self, x):
+        return {i: safe_div(xi, self.abs_max[i].get()) for i, xi in x.items()}
+
+
+class RobustScaler(base.Transformer):
+    """Scale features using statistics that are robust to outliers.
+
+    This Scaler removes the median and scales the data according to the
+    interquantile range.
+
+    Parameters
+    ----------
+    with_centering
+        Whether to centre the data before scaling.
+    with_scaling
+        Whether to scale data to IQR.
+    q_inf
+        Desired inferior quantile, must be between 0 and 1.
+    q_sup
+        Desired superior quantile, must be between 0 and 1.
+
+    Attributes
+    ----------
+    median : dict
+        Mapping between features and instances of `stats.Quantile(0.5)`.
+    iqr : dict
+        Mapping between features and instances of `stats.IQR`.
+
+    Examples
+    --------
+
+    >>> from pprint import pprint
+    >>> import random
+    >>> from river import preprocessing
+
+    >>> random.seed(42)
+    >>> X = [{'x': random.uniform(8, 12)} for _ in range(5)]
+    >>> pprint(X)
+    [{'x': 10.557707},
+        {'x': 8.100043},
+        {'x': 9.100117},
+        {'x': 8.892842},
+        {'x': 10.945884}]
+
+    >>> scaler = preprocessing.RobustScaler()
+
+    >>> for x in X:
+    ...     print(scaler.learn_one(x).transform_one(x))
+    {'x': 0.0}
+    {'x': -1.0}
+    {'x': 0.0}
+    {'x': -0.124499}
+    {'x': 1.108659}
+
+    """
+
+    def __init__(self, with_centering=True, with_scaling=True, q_inf=0.25, q_sup=0.75):
+        self.with_centering = with_centering
+        self.with_scaling = with_scaling
+        self.q_inf = q_inf
+        self.q_sup = q_sup
+        self.median = collections.defaultdict(functools.partial(stats.Quantile, 0.5))
+        self.iqr = collections.defaultdict(
+            functools.partial(stats.IQR, self.q_inf, self.q_sup)
+        )
+
+    def learn_one(self, x):
+
+        for i, xi in x.items():
+            if self.with_centering:
+                self.median[i].update(xi)
+            if self.with_scaling:
+                self.iqr[i].update(xi)
+
+        return self
+
+    def transform_one(self, x):
+        x_tf = {}
+
+        for i, xi in x.items():
+            x_tf[i] = xi
+            if self.with_centering:
+                x_tf[i] -= self.median[i].get()
+            if self.with_scaling:
+                x_tf[i] = safe_div(x_tf[i], self.iqr[i].get())
+
+        return x_tf
+
+
+class Normalizer(base.Transformer):
+    """Scales a set of features so that it has unit norm.
+
+    This is particularly useful when used after a `feature_extraction.TFIDF`.
+
+    Parameters
+    ----------
+    order
+        Order of the norm (e.g. 2 corresponds to the $L^2$ norm).
+
+    Examples
+    --------
+
+    >>> from river import preprocessing
+    >>> from river import stream
+
+    >>> scaler = preprocessing.Normalizer(order=2)
+
+    >>> X = [[4, 1, 2, 2],
+    ...      [1, 3, 9, 3],
+    ...      [5, 7, 5, 1]]
+
+    >>> for x, _ in stream.iter_array(X):
+    ...     print(scaler.transform_one(x))
+    {0: 0.8, 1: 0.2, 2: 0.4, 3: 0.4}
+    {0: 0.1, 1: 0.3, 2: 0.9, 3: 0.3}
+    {0: 0.5, 1: 0.7, 2: 0.5, 3: 0.1}
+
+    """
+
+    def __init__(self, order=2):
+        self.order = order
+
+    def transform_one(self, x):
+        norm = utils.math.norm(x, order=self.order)
+        return {i: xi / norm for i, xi in x.items()}
+
+
+class AdaptiveStandardScaler(base.Transformer):
+    """Scales data using exponentially weighted moving average and variance.
+
+    Under the hood, a exponentially weighted running mean and variance are maintained for each
+    feature. This can potentially provide better results for drifting data in comparison to
+    `preprocessing.StandardScaler`. Indeed, the latter computes a global mean and variance for each
+    feature, whereas this scaler weights data in proportion to their recency.
+
+    Parameters
+    ----------
+    alpha
+        This parameter is passed to `stats.EWVar`. It is expected to be in [0, 1]. More weight is
+        assigned to recent samples the closer `alpha` is to 1.
+
+    Examples
+    --------
+
+    Consider the following series which contains a positive trend.
+
+    >>> import random
+
+    >>> random.seed(42)
+    >>> X = [
+    ...     {'x': random.uniform(4 + i, 6 + i)}
+    ...     for i in range(8)
+    ... ]
+    >>> for x in X:
+    ...     print(x)
+    {'x': 5.278}
+    {'x': 5.050}
+    {'x': 6.550}
+    {'x': 7.446}
+    {'x': 9.472}
+    {'x': 10.353}
+    {'x': 11.784}
+    {'x': 11.173}
+
+    This scaler works well with this kind of data because it uses statistics that assign higher
+    weight to more recent data.
+
+    >>> from river import preprocessing
+
+    >>> scaler = preprocessing.AdaptiveStandardScaler(alpha=.6)
+
+    >>> for x in X:
+    ...     print(scaler.learn_one(x).transform_one(x))
+    {'x': 0.0}
+    {'x': -0.816}
+    {'x': 0.812}
+    {'x': 0.695}
+    {'x': 0.754}
+    {'x': 0.598}
+    {'x': 0.651}
+    {'x': 0.124}
+
+    """
+
+    def __init__(self, alpha=0.3):
+        self.alpha = alpha
+        self.vars = collections.defaultdict(functools.partial(stats.EWVar, self.alpha))
+
+    def learn_one(self, x):
+        for i, xi in x.items():
+            self.vars[i].update(xi)
+        return self
+
+    def transform_one(self, x):
+        return {
+            i: safe_div(xi - self.vars[i].mean.get(), self.vars[i].get() ** 0.5)
+            for i, xi in x.items()
+        }
+
+
+class TargetStandardScaler(compose.TargetTransformRegressor):
+    """Applies standard scaling to the target.
+
+    Parameters
+    ----------
+    regressor
+        Regression model to wrap.
+
+    Examples
+    --------
+
+    >>> from river import datasets
+    >>> from river import evaluate
+    >>> from river import linear_model
+    >>> from river import metrics
+    >>> from river import preprocessing
+
+    >>> dataset = datasets.TrumpApproval()
+    >>> model = (
+    ...     preprocessing.StandardScaler() |
+    ...     preprocessing.TargetStandardScaler(
+    ...         regressor=linear_model.LinearRegression(intercept_lr=0.15)
+    ...     )
+    ... )
+    >>> metric = metrics.MSE()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    MSE: 2.003724
+
+    """
+
+    def __init__(self, regressor: base.Regressor):
+        self.var = stats.Var()
+        super().__init__(
+            regressor=regressor, func=self._scale, inverse_func=self._unscale
+        )
+
+    def learn_one(self, x, y):
+        self.var.update(y)
+        return super().learn_one(x, y)
+
+    def _scale(self, y):
+        try:
+            return (y - self.var.mean.get()) / self.var.get() ** 0.5
+        except ZeroDivisionError:
+            return 0.0
+
+    def _unscale(self, y):
+        return y * self.var.get() ** 0.5 + self.var.mean.get()
```

### Comparing `river-0.8.0/river/preprocessing/test_lda.py` & `river-0.9.0/river/preprocessing/test_lda.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,281 +1,281 @@
-"""
-The tests performed here confirm that the outputs of the online preprocessing.LDA are exactly the
-same as those of the original with a batch_size of size 1. Coverage is 100%.
-
-References
-----------
-[^1]: Jordan Boyd-Graber, Ke Zhai, Online Latent Dirichlet Allocation with Infinite Vocabulary.
-    http://proceedings.mlr.press/v28/zhai13.pdf
-[^2]: river's LDA implementation reproduces exactly the same results as the original one from
-    PyInfVov (https://github.com/kzhai/PyInfVoc) with a batch size of 1.
-
-"""
-import numpy as np
-
-from river import preprocessing
-
-DOC_SET = [
-    "weather cold",
-    "weather hot dry",
-    "weather cold rainny",
-    "weather hot",
-    "weather cold humid",
-]
-
-REFERENCE_STATISTICS_TWO_COMPONENTS = [
-    {0: np.array([0.0, 0.0, 0.0]), 1: np.array([0.0, 1.0, 1.0])},
-    {0: np.array([0.0, 0.6, 0.0, 0.8, 0.6]), 1: np.array([0.0, 0.4, 0.0, 0.2, 0.4])},
-    {
-        0: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),
-        1: np.array([0.0, 1.0, 1.0, 0.0, 0.0, 1.0]),
-    },
-    {
-        0: np.array([0.0, 0.2, 0.0, 0.6, 0.0, 0.0]),
-        1: np.array([0.0, 0.8, 0.0, 0.4, 0.0, 0.0]),
-    },
-    {
-        0: np.array([0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2]),
-        1: np.array([0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.8]),
-    },
-]
-
-
-REFERENCE_STATISTICS_FIVE_COMPONENTS = [
-    {
-        0: np.array([0.0, 0.4, 0.2]),
-        1: np.array([0.0, 0.2, 0.6]),
-        2: np.array([0.0, 0.4, 0.0]),
-        3: np.array([0.0, 0.0, 0.0]),
-        4: np.array([0.0, 0.0, 0.2]),
-    },
-    {
-        0: np.array([0.0, 0.8, 0.0, 0.4, 0.6]),
-        1: np.array([0.0, 0.0, 0.0, 0.2, 0.4]),
-        2: np.array([0.0, 0.0, 0.0, 0.0, 0.0]),
-        3: np.array([0.0, 0.0, 0.0, 0.2, 0.0]),
-        4: np.array([0.0, 0.2, 0.0, 0.2, 0.0]),
-    },
-    {
-        0: np.array([0.0, 0.4, 0.2, 0.0, 0.0, 0.0]),
-        1: np.array([0.0, 0.2, 0.6, 0.0, 0.0, 0.6]),
-        2: np.array([0.0, 0.4, 0.2, 0.0, 0.0, 0.4]),
-        3: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),
-        4: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),
-    },
-    {
-        0: np.array([0.0, 0.2, 0.0, 0.4, 0.0, 0.0]),
-        1: np.array([0.0, 0.2, 0.0, 0.2, 0.0, 0.0]),
-        2: np.array([0.0, 0.4, 0.0, 0.2, 0.0, 0.0]),
-        3: np.array([0.0, 0.2, 0.0, 0.2, 0.0, 0.0]),
-        4: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),
-    },
-    {
-        0: np.array([0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2]),
-        1: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),
-        2: np.array([0.0, 0.8, 0.8, 0.0, 0.0, 0.0, 0.6]),
-        3: np.array([0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2]),
-        4: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),
-    },
-]
-
-REFERENCE_FIVE_COMPONENTS = [
-    np.array([1.5, 0.5, 0.5, 0.5, 1.5]),
-    np.array([1.5, 1.5, 0.5, 0.5, 1.5]),
-    np.array([0.5, 0.5, 3.5, 0.5, 0.5]),
-    np.array([0.5, 0.5, 0.5, 2.5, 0.5]),
-    np.array([2.5, 0.5, 0.5, 1.5, 0.5]),
-]
-
-REFERENCE_COMPONENTS_WITH_PRUNNING = [
-    np.array([0.5, 2.5]),
-    np.array([3.5, 0.5]),
-    np.array([0.5, 3.5]),
-    np.array([2.5, 0.5]),
-    np.array([2.5, 1.5]),
-]
-
-REFERENCE_LEARN_ONE_PREDICT_ONE = [
-    np.array([2.5, 0.5]),
-    np.array([2.5, 1.5]),
-    np.array([1.5, 2.5]),
-    np.array([0.5, 2.5]),
-    np.array([0.5, 3.5]),
-]
-
-
-def test_extraction_words_ids():
-    """
-    Assert that input words are split.
-    Assert that indexes are updated and extractable.
-    """
-
-    lda = preprocessing.LDA(2, number_of_documents=5, seed=42)
-
-    word_indexes_list = []
-
-    for doc in DOC_SET:
-
-        words = doc.split(" ")
-
-        lda._update_indexes(word_list=words)
-
-        word_indexes_list.append([lda.word_to_index[word] for word in words])
-
-    assert word_indexes_list == [[1, 2], [1, 3, 4], [1, 2, 5], [1, 3], [1, 2, 6]]
-
-
-def test_statistics_two_components():
-    """
-    Assert that online lda extracts waited statistics on current document.
-    """
-    n_components = 2
-
-    lda = preprocessing.LDA(n_components, number_of_documents=60, seed=42)
-
-    statistics_list = []
-
-    for doc in DOC_SET:
-
-        word_list = doc.split(" ")
-
-        lda._update_indexes(word_list=word_list)
-
-        word_indexes = [lda.word_to_index[word] for word in word_list]
-
-        statistics, _ = lda._compute_statistics_components(
-            words_indexes_list=word_indexes
-        )
-
-        statistics_list.append(statistics)
-
-        lda._update_weights(statistics=statistics)
-
-    for index, statistics in enumerate(statistics_list):
-        for component in range(n_components):
-            assert np.array_equal(
-                a1=statistics[component],
-                a2=REFERENCE_STATISTICS_TWO_COMPONENTS[index][component],
-            )
-
-
-def test_statistics_five_components():
-    """
-    Assert that online lda extracts waited statistics on current document.
-    """
-
-    n_components = 5
-
-    lda = preprocessing.LDA(
-        n_components=n_components,
-        number_of_documents=60,
-        maximum_size_vocabulary=100,
-        alpha_beta=100,
-        alpha_theta=0.5,
-        seed=42,
-    )
-
-    statistics_list = []
-
-    for doc in DOC_SET:
-
-        word_list = doc.split(" ")
-
-        lda._update_indexes(word_list=word_list)
-
-        word_indexes = [lda.word_to_index[word] for word in word_list]
-
-        statistics, _ = lda._compute_statistics_components(
-            words_indexes_list=word_indexes
-        )
-
-        statistics_list.append(statistics)
-
-        lda._update_weights(statistics=statistics)
-
-    for index, statistics in enumerate(statistics_list):
-        for component in range(n_components):
-            assert np.array_equal(
-                a1=statistics[component],
-                a2=REFERENCE_STATISTICS_FIVE_COMPONENTS[index][component],
-            )
-
-
-def test_five_components():
-    """
-    Assert that components computed are identical to the original version for n dimensions.
-    """
-
-    n_components = 5
-
-    lda = preprocessing.LDA(
-        n_components=n_components,
-        number_of_documents=60,
-        maximum_size_vocabulary=100,
-        alpha_beta=100,
-        alpha_theta=0.5,
-        seed=42,
-    )
-
-    components_list = []
-
-    for document in DOC_SET:
-        tokens = {token: 1 for token in document.split(" ")}
-        components_list.append(lda.learn_transform_one(tokens))
-
-    for index, component in enumerate(components_list):
-        assert np.array_equal(
-            a1=list(component.values()), a2=REFERENCE_FIVE_COMPONENTS[index]
-        )
-
-
-def test_prunning_vocabulary():
-    """
-    Vocabulary prunning is available to improve accuracy and limit memory usage.
-    You can perform vocabulary prunning with parameters vocab_prune_interval (int) and
-    maximum_size_vocabulary (int).
-    """
-
-    lda = preprocessing.LDA(
-        n_components=2,
-        number_of_documents=60,
-        vocab_prune_interval=2,
-        maximum_size_vocabulary=3,
-        seed=42,
-    )
-
-    components_list = []
-
-    for document in DOC_SET:
-        tokens = {token: 1 for token in document.split(" ")}
-        components_list.append(lda.learn_transform_one(tokens))
-
-    for index, component in enumerate(components_list):
-        assert np.array_equal(
-            a1=list(component.values()), a2=REFERENCE_COMPONENTS_WITH_PRUNNING[index]
-        )
-
-
-def test_learn_transform():
-    """
-    Assert that learn_one and transform_one methods returns waited output.
-    """
-
-    lda = preprocessing.LDA(
-        n_components=2,
-        number_of_documents=60,
-        vocab_prune_interval=2,
-        maximum_size_vocabulary=3,
-        seed=42,
-    )
-    components_list = []
-
-    for document in DOC_SET:
-        tokens = {token: 1 for token in document.split(" ")}
-        lda = lda.learn_one(x=tokens)
-
-        components_list.append(lda.transform_one(x=tokens))
-
-    for index, component in enumerate(components_list):
-        assert np.array_equal(
-            a1=list(component.values()), a2=REFERENCE_LEARN_ONE_PREDICT_ONE[index]
-        )
+"""
+The tests performed here confirm that the outputs of the online preprocessing.LDA are exactly the
+same as those of the original with a batch_size of size 1. Coverage is 100%.
+
+References
+----------
+[^1]: Jordan Boyd-Graber, Ke Zhai, Online Latent Dirichlet Allocation with Infinite Vocabulary.
+    http://proceedings.mlr.press/v28/zhai13.pdf
+[^2]: river's LDA implementation reproduces exactly the same results as the original one from
+    PyInfVov (https://github.com/kzhai/PyInfVoc) with a batch size of 1.
+
+"""
+import numpy as np
+
+from river import preprocessing
+
+DOC_SET = [
+    "weather cold",
+    "weather hot dry",
+    "weather cold rainny",
+    "weather hot",
+    "weather cold humid",
+]
+
+REFERENCE_STATISTICS_TWO_COMPONENTS = [
+    {0: np.array([0.0, 0.0, 0.0]), 1: np.array([0.0, 1.0, 1.0])},
+    {0: np.array([0.0, 0.6, 0.0, 0.8, 0.6]), 1: np.array([0.0, 0.4, 0.0, 0.2, 0.4])},
+    {
+        0: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),
+        1: np.array([0.0, 1.0, 1.0, 0.0, 0.0, 1.0]),
+    },
+    {
+        0: np.array([0.0, 0.2, 0.0, 0.6, 0.0, 0.0]),
+        1: np.array([0.0, 0.8, 0.0, 0.4, 0.0, 0.0]),
+    },
+    {
+        0: np.array([0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2]),
+        1: np.array([0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.8]),
+    },
+]
+
+
+REFERENCE_STATISTICS_FIVE_COMPONENTS = [
+    {
+        0: np.array([0.0, 0.4, 0.2]),
+        1: np.array([0.0, 0.2, 0.6]),
+        2: np.array([0.0, 0.4, 0.0]),
+        3: np.array([0.0, 0.0, 0.0]),
+        4: np.array([0.0, 0.0, 0.2]),
+    },
+    {
+        0: np.array([0.0, 0.8, 0.0, 0.4, 0.6]),
+        1: np.array([0.0, 0.0, 0.0, 0.2, 0.4]),
+        2: np.array([0.0, 0.0, 0.0, 0.0, 0.0]),
+        3: np.array([0.0, 0.0, 0.0, 0.2, 0.0]),
+        4: np.array([0.0, 0.2, 0.0, 0.2, 0.0]),
+    },
+    {
+        0: np.array([0.0, 0.4, 0.2, 0.0, 0.0, 0.0]),
+        1: np.array([0.0, 0.2, 0.6, 0.0, 0.0, 0.6]),
+        2: np.array([0.0, 0.4, 0.2, 0.0, 0.0, 0.4]),
+        3: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),
+        4: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),
+    },
+    {
+        0: np.array([0.0, 0.2, 0.0, 0.4, 0.0, 0.0]),
+        1: np.array([0.0, 0.2, 0.0, 0.2, 0.0, 0.0]),
+        2: np.array([0.0, 0.4, 0.0, 0.2, 0.0, 0.0]),
+        3: np.array([0.0, 0.2, 0.0, 0.2, 0.0, 0.0]),
+        4: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),
+    },
+    {
+        0: np.array([0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.2]),
+        1: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),
+        2: np.array([0.0, 0.8, 0.8, 0.0, 0.0, 0.0, 0.6]),
+        3: np.array([0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2]),
+        4: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),
+    },
+]
+
+REFERENCE_FIVE_COMPONENTS = [
+    np.array([1.5, 0.5, 0.5, 0.5, 1.5]),
+    np.array([1.5, 1.5, 0.5, 0.5, 1.5]),
+    np.array([0.5, 0.5, 3.5, 0.5, 0.5]),
+    np.array([0.5, 0.5, 0.5, 2.5, 0.5]),
+    np.array([2.5, 0.5, 0.5, 1.5, 0.5]),
+]
+
+REFERENCE_COMPONENTS_WITH_PRUNNING = [
+    np.array([0.5, 2.5]),
+    np.array([3.5, 0.5]),
+    np.array([0.5, 3.5]),
+    np.array([2.5, 0.5]),
+    np.array([2.5, 1.5]),
+]
+
+REFERENCE_LEARN_ONE_PREDICT_ONE = [
+    np.array([2.5, 0.5]),
+    np.array([2.5, 1.5]),
+    np.array([1.5, 2.5]),
+    np.array([0.5, 2.5]),
+    np.array([0.5, 3.5]),
+]
+
+
+def test_extraction_words_ids():
+    """
+    Assert that input words are split.
+    Assert that indexes are updated and extractable.
+    """
+
+    lda = preprocessing.LDA(2, number_of_documents=5, seed=42)
+
+    word_indexes_list = []
+
+    for doc in DOC_SET:
+
+        words = doc.split(" ")
+
+        lda._update_indexes(word_list=words)
+
+        word_indexes_list.append([lda.word_to_index[word] for word in words])
+
+    assert word_indexes_list == [[1, 2], [1, 3, 4], [1, 2, 5], [1, 3], [1, 2, 6]]
+
+
+def test_statistics_two_components():
+    """
+    Assert that online lda extracts waited statistics on current document.
+    """
+    n_components = 2
+
+    lda = preprocessing.LDA(n_components, number_of_documents=60, seed=42)
+
+    statistics_list = []
+
+    for doc in DOC_SET:
+
+        word_list = doc.split(" ")
+
+        lda._update_indexes(word_list=word_list)
+
+        word_indexes = [lda.word_to_index[word] for word in word_list]
+
+        statistics, _ = lda._compute_statistics_components(
+            words_indexes_list=word_indexes
+        )
+
+        statistics_list.append(statistics)
+
+        lda._update_weights(statistics=statistics)
+
+    for index, statistics in enumerate(statistics_list):
+        for component in range(n_components):
+            assert np.array_equal(
+                a1=statistics[component],
+                a2=REFERENCE_STATISTICS_TWO_COMPONENTS[index][component],
+            )
+
+
+def test_statistics_five_components():
+    """
+    Assert that online lda extracts waited statistics on current document.
+    """
+
+    n_components = 5
+
+    lda = preprocessing.LDA(
+        n_components=n_components,
+        number_of_documents=60,
+        maximum_size_vocabulary=100,
+        alpha_beta=100,
+        alpha_theta=0.5,
+        seed=42,
+    )
+
+    statistics_list = []
+
+    for doc in DOC_SET:
+
+        word_list = doc.split(" ")
+
+        lda._update_indexes(word_list=word_list)
+
+        word_indexes = [lda.word_to_index[word] for word in word_list]
+
+        statistics, _ = lda._compute_statistics_components(
+            words_indexes_list=word_indexes
+        )
+
+        statistics_list.append(statistics)
+
+        lda._update_weights(statistics=statistics)
+
+    for index, statistics in enumerate(statistics_list):
+        for component in range(n_components):
+            assert np.array_equal(
+                a1=statistics[component],
+                a2=REFERENCE_STATISTICS_FIVE_COMPONENTS[index][component],
+            )
+
+
+def test_five_components():
+    """
+    Assert that components computed are identical to the original version for n dimensions.
+    """
+
+    n_components = 5
+
+    lda = preprocessing.LDA(
+        n_components=n_components,
+        number_of_documents=60,
+        maximum_size_vocabulary=100,
+        alpha_beta=100,
+        alpha_theta=0.5,
+        seed=42,
+    )
+
+    components_list = []
+
+    for document in DOC_SET:
+        tokens = {token: 1 for token in document.split(" ")}
+        components_list.append(lda.learn_transform_one(tokens))
+
+    for index, component in enumerate(components_list):
+        assert np.array_equal(
+            a1=list(component.values()), a2=REFERENCE_FIVE_COMPONENTS[index]
+        )
+
+
+def test_prunning_vocabulary():
+    """
+    Vocabulary prunning is available to improve accuracy and limit memory usage.
+    You can perform vocabulary prunning with parameters vocab_prune_interval (int) and
+    maximum_size_vocabulary (int).
+    """
+
+    lda = preprocessing.LDA(
+        n_components=2,
+        number_of_documents=60,
+        vocab_prune_interval=2,
+        maximum_size_vocabulary=3,
+        seed=42,
+    )
+
+    components_list = []
+
+    for document in DOC_SET:
+        tokens = {token: 1 for token in document.split(" ")}
+        components_list.append(lda.learn_transform_one(tokens))
+
+    for index, component in enumerate(components_list):
+        assert np.array_equal(
+            a1=list(component.values()), a2=REFERENCE_COMPONENTS_WITH_PRUNNING[index]
+        )
+
+
+def test_learn_transform():
+    """
+    Assert that learn_one and transform_one methods returns waited output.
+    """
+
+    lda = preprocessing.LDA(
+        n_components=2,
+        number_of_documents=60,
+        vocab_prune_interval=2,
+        maximum_size_vocabulary=3,
+        seed=42,
+    )
+    components_list = []
+
+    for document in DOC_SET:
+        tokens = {token: 1 for token in document.split(" ")}
+        lda = lda.learn_one(x=tokens)
+
+        components_list.append(lda.transform_one(x=tokens))
+
+    for index, component in enumerate(components_list):
+        assert np.array_equal(
+            a1=list(component.values()), a2=REFERENCE_LEARN_ONE_PREDICT_ONE[index]
+        )
```

### Comparing `river-0.8.0/river/proba/base.py` & `river-0.9.0/river/proba/base.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,38 +1,40 @@
-import abc
-
-
-class Distribution(abc.ABC):
-    @abc.abstractmethod
-    def update(self, x):
-        """Updates the parameters of the distribution given a new observation."""
-
-    @abc.abstractproperty
-    def n_samples(self):
-        """The number of observed samples."""
-
-    def __repr__(self):
-        return str(self)
-
-
-class DiscreteDistribution(Distribution):
-    """A probability distribution for discrete values."""
-
-    @abc.abstractmethod
-    def pmf(self, x):
-        """Probability mass function."""
-
-
-class ContinuousDistribution(Distribution):
-    """A probability distribution for continuous values."""
-
-    @abc.abstractproperty
-    def mode(self):
-        """Most likely value."""
-
-    @abc.abstractmethod
-    def pdf(self, x):
-        """Probability density function, i.e. P(x <= X < x+dx) / dx."""
-
-    @abc.abstractmethod
-    def cdf(self, x):
-        """Cumulative density function, i.e. P(X <= x)."""
+import abc
+
+
+class Distribution(abc.ABC):
+    @abc.abstractmethod
+    def update(self, x):
+        """Updates the parameters of the distribution given a new observation."""
+
+    @property
+    @abc.abstractmethod
+    def n_samples(self):
+        """The number of observed samples."""
+
+    def __repr__(self):
+        return str(self)
+
+
+class DiscreteDistribution(Distribution):
+    """A probability distribution for discrete values."""
+
+    @abc.abstractmethod
+    def pmf(self, x):
+        """Probability mass function."""
+
+
+class ContinuousDistribution(Distribution):
+    """A probability distribution for continuous values."""
+
+    @property
+    @abc.abstractmethod
+    def mode(self):
+        """Most likely value."""
+
+    @abc.abstractmethod
+    def pdf(self, x):
+        """Probability density function, i.e. P(x <= X < x+dx) / dx."""
+
+    @abc.abstractmethod
+    def cdf(self, x):
+        """Cumulative density function, i.e. P(X <= x)."""
```

### Comparing `river-0.8.0/river/proba/multinomial.py` & `river-0.9.0/river/proba/multinomial.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,65 +1,65 @@
-import collections
-import typing
-
-from . import base
-
-__all__ = ["Multinomial"]
-
-
-class Multinomial(base.DiscreteDistribution):
-    """Multinomial distribution for categorical data.
-
-    Parameters
-    ----------
-    events
-        An optional list of events that already occurred.
-
-    Examples
-    --------
-
-    >>> from river import proba
-
-    >>> p = proba.Multinomial(['green'] * 3)
-    >>> p = p.update('red')
-
-    >>> p.pmf('red')
-    0.25
-
-    >>> p.update('red').update('red').pmf('green')
-    0.5
-
-    """
-
-    def __init__(self, events: typing.Union[dict, list] = None):
-        self.counts: typing.Counter[typing.Any] = collections.Counter(events)
-        self._n = sum(self.counts.values())
-
-    @property
-    def n_samples(self):
-        return self._n
-
-    def __iter__(self):
-        return iter(self.counts)
-
-    def __len__(self):
-        return len(self.counts)
-
-    @property
-    def mode(self):
-        return self.counts.most_common(1)[0][0]
-
-    def update(self, x):
-        self.counts.update([x])
-        self._n += 1
-        return self
-
-    def pmf(self, x):
-        try:
-            return self.counts[x] / self._n
-        except ZeroDivisionError:
-            return 0.0
-
-    def __str__(self):
-        return "\n".join(
-            f"P({c}) = {self.pmf(c):.3f}" for c in self.counts.most_common()
-        )
+import collections
+import typing
+
+from . import base
+
+__all__ = ["Multinomial"]
+
+
+class Multinomial(base.DiscreteDistribution):
+    """Multinomial distribution for categorical data.
+
+    Parameters
+    ----------
+    events
+        An optional list of events that already occurred.
+
+    Examples
+    --------
+
+    >>> from river import proba
+
+    >>> p = proba.Multinomial(['green'] * 3)
+    >>> p = p.update('red')
+
+    >>> p.pmf('red')
+    0.25
+
+    >>> p.update('red').update('red').pmf('green')
+    0.5
+
+    """
+
+    def __init__(self, events: typing.Union[dict, list] = None):
+        self.counts: typing.Counter[typing.Any] = collections.Counter(events)
+        self._n = sum(self.counts.values())
+
+    @property
+    def n_samples(self):
+        return self._n
+
+    def __iter__(self):
+        return iter(self.counts)
+
+    def __len__(self):
+        return len(self.counts)
+
+    @property
+    def mode(self):
+        return self.counts.most_common(1)[0][0]
+
+    def update(self, x):
+        self.counts.update([x])
+        self._n += 1
+        return self
+
+    def pmf(self, x):
+        try:
+            return self.counts[x] / self._n
+        except ZeroDivisionError:
+            return 0.0
+
+    def __str__(self):
+        return "\n".join(
+            f"P({c}) = {self.pmf(c):.3f}" for c in self.counts.most_common()
+        )
```

### Comparing `river-0.8.0/river/reco/baseline.py` & `river-0.9.0/river/reco/baseline.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,137 +1,137 @@
-import collections
-import copy
-import typing
-
-from river import optim, stats, utils
-
-from . import base
-
-__all__ = ["Baseline"]
-
-
-class Baseline(base.Recommender):
-    """Baseline for recommender systems.
-
-    A first-order approximation of the bias involved in target. The model equation is defined as:
-
-    $$\\hat{y}(x) = \\bar{y} + bu_{u} + bi_{i}$$
-
-    Where $bu_{u}$ and $bi_{i}$ are respectively the user and item biases.
-
-    This model expects a dict input with a `user` and an `item` entries without any type constraint
-    on their values (i.e. can be strings or numbers). Other entries are ignored.
-
-    Parameters
-    ----------
-    optimizer
-        The sequential optimizer used for updating the weights.
-    loss
-        The loss function to optimize for.
-    l2
-        regularization amount used to push weights towards 0.
-    initializer
-        Weights initialization scheme.
-    clip_gradient
-        Clips the absolute value of each gradient value.
-
-    Attributes
-    ----------
-    global_mean : stats.Mean
-        The target arithmetic mean.
-    u_biases : collections.defaultdict
-        The user bias weights.
-    i_biases : collections.defaultdict
-        The item bias weights.
-    u_optimizer : optim.Optimizer
-        The sequential optimizer used for updating the user bias weights.
-    i_optimizer : optim.Optimizer
-        The sequential optimizer used for updating the item bias weights.
-
-    Examples
-    --------
-
-    >>> from river import optim
-    >>> from river import reco
-
-    >>> dataset = (
-    ...     ({'user': 'Alice', 'item': 'Superman'}, 8),
-    ...     ({'user': 'Alice', 'item': 'Terminator'}, 9),
-    ...     ({'user': 'Alice', 'item': 'Star Wars'}, 8),
-    ...     ({'user': 'Alice', 'item': 'Notting Hill'}, 2),
-    ...     ({'user': 'Alice', 'item': 'Harry Potter'}, 5),
-    ...     ({'user': 'Bob', 'item': 'Superman'}, 8),
-    ...     ({'user': 'Bob', 'item': 'Terminator'}, 9),
-    ...     ({'user': 'Bob', 'item': 'Star Wars'}, 8),
-    ...     ({'user': 'Bob', 'item': 'Notting Hill'}, 2)
-    ... )
-
-    >>> model = reco.Baseline(optimizer=optim.SGD(0.005))
-
-    >>> for x, y in dataset:
-    ...     _ = model.learn_one(x, y)
-
-    >>> model.predict_one({'user': 'Bob', 'item': 'Harry Potter'})
-    6.538120
-
-    References
-    ----------
-    [^1]: [Matrix factorization techniques for recommender systems](https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf)
-
-    """
-
-    def __init__(
-        self,
-        optimizer: optim.Optimizer = None,
-        loss: optim.losses.Loss = None,
-        l2=0.0,
-        initializer: optim.initializers.Initializer = None,
-        clip_gradient=1e12,
-    ):
-        self.optimizer = optim.SGD() if optimizer is None else copy.deepcopy(optimizer)
-        self.u_optimizer = (
-            optim.SGD() if optimizer is None else copy.deepcopy(optimizer)
-        )
-        self.i_optimizer = (
-            optim.SGD() if optimizer is None else copy.deepcopy(optimizer)
-        )
-        self.loss = optim.losses.Squared() if loss is None else loss
-        self.l2 = l2
-
-        if initializer is None:
-            initializer = optim.initializers.Zeros()
-        self.initializer = initializer
-
-        self.clip_gradient = clip_gradient
-        self.global_mean = stats.Mean()
-        self.u_biases: typing.DefaultDict[
-            int, optim.initializers.Initializer
-        ] = collections.defaultdict(initializer)
-        self.i_biases: typing.DefaultDict[
-            int, optim.initializers.Initializer
-        ] = collections.defaultdict(initializer)
-
-    def _predict_one(self, user, item):
-        return self.global_mean.get() + self.u_biases[user] + self.i_biases[item]
-
-    def _learn_one(self, user, item, y):
-
-        # Update the global mean
-        self.global_mean.update(y)
-
-        # Calculate the gradient of the loss with respect to the prediction
-        g_loss = self.loss.gradient(y, self._predict_one(user, item))
-
-        # Clamp the gradient to avoid numerical instability
-        g_loss = utils.math.clamp(
-            g_loss, minimum=-self.clip_gradient, maximum=self.clip_gradient
-        )
-
-        # Calculate bias gradients
-        u_grad_bias = {user: g_loss + self.l2 * self.u_biases[user]}
-        i_grad_bias = {item: g_loss + self.l2 * self.i_biases[item]}
-
-        # Update biases
-        self.u_biases = self.u_optimizer.step(self.u_biases, u_grad_bias)
-        self.i_biases = self.i_optimizer.step(self.i_biases, i_grad_bias)
-
-        return self
+import collections
+import copy
+import typing
+
+from river import optim, stats, utils
+
+from . import base
+
+__all__ = ["Baseline"]
+
+
+class Baseline(base.Recommender):
+    """Baseline for recommender systems.
+
+    A first-order approximation of the bias involved in target. The model equation is defined as:
+
+    $$\\hat{y}(x) = \\bar{y} + bu_{u} + bi_{i}$$
+
+    Where $bu_{u}$ and $bi_{i}$ are respectively the user and item biases.
+
+    This model expects a dict input with a `user` and an `item` entries without any type constraint
+    on their values (i.e. can be strings or numbers). Other entries are ignored.
+
+    Parameters
+    ----------
+    optimizer
+        The sequential optimizer used for updating the weights.
+    loss
+        The loss function to optimize for.
+    l2
+        regularization amount used to push weights towards 0.
+    initializer
+        Weights initialization scheme.
+    clip_gradient
+        Clips the absolute value of each gradient value.
+
+    Attributes
+    ----------
+    global_mean : stats.Mean
+        The target arithmetic mean.
+    u_biases : collections.defaultdict
+        The user bias weights.
+    i_biases : collections.defaultdict
+        The item bias weights.
+    u_optimizer : optim.Optimizer
+        The sequential optimizer used for updating the user bias weights.
+    i_optimizer : optim.Optimizer
+        The sequential optimizer used for updating the item bias weights.
+
+    Examples
+    --------
+
+    >>> from river import optim
+    >>> from river import reco
+
+    >>> dataset = (
+    ...     ({'user': 'Alice', 'item': 'Superman'}, 8),
+    ...     ({'user': 'Alice', 'item': 'Terminator'}, 9),
+    ...     ({'user': 'Alice', 'item': 'Star Wars'}, 8),
+    ...     ({'user': 'Alice', 'item': 'Notting Hill'}, 2),
+    ...     ({'user': 'Alice', 'item': 'Harry Potter'}, 5),
+    ...     ({'user': 'Bob', 'item': 'Superman'}, 8),
+    ...     ({'user': 'Bob', 'item': 'Terminator'}, 9),
+    ...     ({'user': 'Bob', 'item': 'Star Wars'}, 8),
+    ...     ({'user': 'Bob', 'item': 'Notting Hill'}, 2)
+    ... )
+
+    >>> model = reco.Baseline(optimizer=optim.SGD(0.005))
+
+    >>> for x, y in dataset:
+    ...     _ = model.learn_one(x, y)
+
+    >>> model.predict_one({'user': 'Bob', 'item': 'Harry Potter'})
+    6.538120
+
+    References
+    ----------
+    [^1]: [Matrix factorization techniques for recommender systems](https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf)
+
+    """
+
+    def __init__(
+        self,
+        optimizer: optim.Optimizer = None,
+        loss: optim.losses.Loss = None,
+        l2=0.0,
+        initializer: optim.initializers.Initializer = None,
+        clip_gradient=1e12,
+    ):
+        self.optimizer = optim.SGD() if optimizer is None else copy.deepcopy(optimizer)
+        self.u_optimizer = (
+            optim.SGD() if optimizer is None else copy.deepcopy(optimizer)
+        )
+        self.i_optimizer = (
+            optim.SGD() if optimizer is None else copy.deepcopy(optimizer)
+        )
+        self.loss = optim.losses.Squared() if loss is None else loss
+        self.l2 = l2
+
+        if initializer is None:
+            initializer = optim.initializers.Zeros()
+        self.initializer = initializer
+
+        self.clip_gradient = clip_gradient
+        self.global_mean = stats.Mean()
+        self.u_biases: typing.DefaultDict[
+            int, optim.initializers.Initializer
+        ] = collections.defaultdict(initializer)
+        self.i_biases: typing.DefaultDict[
+            int, optim.initializers.Initializer
+        ] = collections.defaultdict(initializer)
+
+    def _predict_one(self, user, item):
+        return self.global_mean.get() + self.u_biases[user] + self.i_biases[item]
+
+    def _learn_one(self, user, item, y):
+
+        # Update the global mean
+        self.global_mean.update(y)
+
+        # Calculate the gradient of the loss with respect to the prediction
+        g_loss = self.loss.gradient(y, self._predict_one(user, item))
+
+        # Clamp the gradient to avoid numerical instability
+        g_loss = utils.math.clamp(
+            g_loss, minimum=-self.clip_gradient, maximum=self.clip_gradient
+        )
+
+        # Calculate bias gradients
+        u_grad_bias = {user: g_loss + self.l2 * self.u_biases[user]}
+        i_grad_bias = {item: g_loss + self.l2 * self.i_biases[item]}
+
+        # Update biases
+        self.u_biases = self.u_optimizer.step(self.u_biases, u_grad_bias)
+        self.i_biases = self.i_optimizer.step(self.i_biases, i_grad_bias)
+
+        return self
```

### Comparing `river-0.8.0/river/reco/biased_mf.py` & `river-0.9.0/river/reco/biased_mf.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,219 +1,219 @@
-import collections
-import copy
-import functools
-import typing
-
-import numpy as np
-
-from river import optim, stats, utils
-
-from . import base
-
-__all__ = ["BiasedMF"]
-
-
-class BiasedMF(base.Recommender):
-    """Biased Matrix Factorization for recommender systems.
-
-    The model equation is defined as:
-
-    $$\\hat{y}(x) = \\bar{y} + bu_{u} + bi_{i} + \\langle \\mathbf{v}_u, \\mathbf{v}_i \\rangle$$
-
-    Where $bu_{u}$ and $bi_{i}$ are respectively the user and item biases. The last term being
-    simply the dot product between the latent vectors of the given user-item pair:
-
-    $$\\langle \\mathbf{v}_u, \\mathbf{v}_i \\rangle = \\sum_{f=1}^{k} \\mathbf{v}_{u, f} \\cdot \\mathbf{v}_{i, f}$$
-
-    where $k$ is the number of latent factors.
-
-    This model expects a dict input with a `user` and an `item` entries without any type constraint
-    on their values (i.e. can be strings or numbers). Other entries are ignored.
-
-    Parameters
-    ----------
-    n_factors
-        Dimensionality of the factorization or number of latent factors.
-    bias_optimizer
-        The sequential optimizer used for updating the bias weights.
-    latent_optimizer
-        The sequential optimizer used for updating the latent weights.
-    loss
-        The loss function to optimize for.
-    l2_bias
-        Amount of L2 regularization used to push bias weights towards 0.
-    l2_latent
-        Amount of L2 regularization used to push latent weights towards 0.
-    weight_initializer
-        Weights initialization scheme.
-    latent_initializer
-        Latent factors initialization scheme.
-    clip_gradient
-        Clips the absolute value of each gradient value.
-    seed
-        Randomization seed used for reproducibility.
-
-    Attributes
-    ----------
-    global_mean : stats.Mean
-        The target arithmetic mean.
-    u_biases : collections.defaultdict
-        The user bias weights.
-    i_biases : collections.defaultdict
-        The item bias weights.
-    u_latents : collections.defaultdict
-        The user latent vectors randomly initialized.
-    i_latents : collections.defaultdict
-        The item latent vectors randomly initialized.
-    u_bias_optimizer : optim.Optimizer
-        The sequential optimizer used for updating the user bias weights.
-    i_bias_optimizer : optim.Optimizer
-        The sequential optimizer used for updating the item bias weights.
-    u_latent_optimizer : optim.Optimizer
-        The sequential optimizer used for updating the user latent weights.
-    i_latent_optimizer : optim.Optimizer
-        The sequential optimizer used for updating the item latent weights.
-
-    Examples
-    --------
-
-    >>> from river import optim
-    >>> from river import reco
-
-    >>> dataset = (
-    ...     ({'user': 'Alice', 'item': 'Superman'}, 8),
-    ...     ({'user': 'Alice', 'item': 'Terminator'}, 9),
-    ...     ({'user': 'Alice', 'item': 'Star Wars'}, 8),
-    ...     ({'user': 'Alice', 'item': 'Notting Hill'}, 2),
-    ...     ({'user': 'Alice', 'item': 'Harry Potter'}, 5),
-    ...     ({'user': 'Bob', 'item': 'Superman'}, 8),
-    ...     ({'user': 'Bob', 'item': 'Terminator'}, 9),
-    ...     ({'user': 'Bob', 'item': 'Star Wars'}, 8),
-    ...     ({'user': 'Bob', 'item': 'Notting Hill'}, 2)
-    ... )
-
-    >>> model = reco.BiasedMF(
-    ...     n_factors=10,
-    ...     bias_optimizer=optim.SGD(0.025),
-    ...     latent_optimizer=optim.SGD(0.025),
-    ...     latent_initializer=optim.initializers.Normal(mu=0., sigma=0.1, seed=71)
-    ... )
-
-    >>> for x, y in dataset:
-    ...     _ = model.learn_one(x, y)
-
-    >>> model.predict_one({'user': 'Bob', 'item': 'Harry Potter'})
-    6.489025
-
-    References
-    ----------
-    [^1]: [Paterek, A., 2007, August. Improving regularized singular value decomposition for collaborative filtering. In Proceedings of KDD cup and workshop (Vol. 2007, pp. 5-8)](https://www.cs.uic.edu/~liub/KDD-cup-2007/proceedings/Regular-Paterek.pdf)
-    [^2]: [Matrix factorization techniques for recommender systems](https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf)
-
-    """
-
-    def __init__(
-        self,
-        n_factors=10,
-        bias_optimizer: optim.Optimizer = None,
-        latent_optimizer: optim.Optimizer = None,
-        loss: optim.losses.Loss = None,
-        l2_bias=0.0,
-        l2_latent=0.0,
-        weight_initializer: optim.initializers.Initializer = None,
-        latent_initializer: optim.initializers.Initializer = None,
-        clip_gradient=1e12,
-        seed: int = None,
-    ):
-
-        self.n_factors = n_factors
-        self.u_bias_optimizer = (
-            optim.SGD() if bias_optimizer is None else copy.deepcopy(bias_optimizer)
-        )
-        self.i_bias_optimizer = (
-            optim.SGD() if bias_optimizer is None else copy.deepcopy(bias_optimizer)
-        )
-        self.u_latent_optimizer = (
-            optim.SGD() if latent_optimizer is None else copy.deepcopy(latent_optimizer)
-        )
-        self.i_latent_optimizer = (
-            optim.SGD() if latent_optimizer is None else copy.deepcopy(latent_optimizer)
-        )
-        self.loss = optim.losses.Squared() if loss is None else loss
-        self.l2_bias = l2_bias
-        self.l2_latent = l2_latent
-
-        if weight_initializer is None:
-            weight_initializer = optim.initializers.Zeros()
-        self.weight_initializer = weight_initializer
-
-        if latent_initializer is None:
-            latent_initializer = optim.initializers.Normal(sigma=0.1, seed=seed)
-        self.latent_initializer = latent_initializer
-
-        self.clip_gradient = clip_gradient
-        self.seed = seed
-        self.global_mean = stats.Mean()
-
-        self.u_biases: typing.DefaultDict[
-            int, optim.initializers.Initializer
-        ] = collections.defaultdict(weight_initializer)
-        self.i_biases: typing.DefaultDict[
-            int, optim.initializers.Initializer
-        ] = collections.defaultdict(weight_initializer)
-
-        random_latents = functools.partial(
-            self.latent_initializer, shape=self.n_factors
-        )
-        self.u_latents: typing.DefaultDict[
-            int, optim.initializers.Initializer
-        ] = collections.defaultdict(random_latents)
-        self.i_latents: typing.DefaultDict[
-            int, optim.initializers.Initializer
-        ] = collections.defaultdict(random_latents)
-
-    def _predict_one(self, user, item):
-
-        # Initialize the prediction to the mean
-        y_pred = self.global_mean.get()
-
-        # Add the user bias
-        y_pred += self.u_biases[user]
-
-        # Add the item bias
-        y_pred += self.i_biases[item]
-
-        # Add the dot product of the user and the item latent vectors
-        y_pred += np.dot(self.u_latents[user], self.i_latents[item])
-
-        return y_pred
-
-    def _learn_one(self, user, item, y):
-
-        # Update the global mean
-        self.global_mean.update(y)
-
-        # Calculate the gradient of the loss with respect to the prediction
-        g_loss = self.loss.gradient(y, self._predict_one(user, item))
-
-        # Clamp the gradient to avoid numerical instability
-        g_loss = utils.math.clamp(
-            g_loss, minimum=-self.clip_gradient, maximum=self.clip_gradient
-        )
-
-        # Calculate weights gradients
-        u_grad_bias = {user: g_loss + self.l2_bias * self.u_biases[user]}
-        i_grad_bias = {item: g_loss + self.l2_bias * self.i_biases[item]}
-        u_latent_grad = {
-            user: g_loss * self.i_latents[item] + self.l2_latent * self.u_latents[user]
-        }
-        i_latent_grad = {
-            item: g_loss * self.u_latents[user] + self.l2_latent * self.i_latents[item]
-        }
-
-        # Update weights
-        self.u_biases = self.u_bias_optimizer.step(self.u_biases, u_grad_bias)
-        self.i_biases = self.i_bias_optimizer.step(self.i_biases, i_grad_bias)
-        self.u_latents = self.u_latent_optimizer.step(self.u_latents, u_latent_grad)
-        self.i_latents = self.i_latent_optimizer.step(self.i_latents, i_latent_grad)
-
-        return self
+import collections
+import copy
+import functools
+import typing
+
+import numpy as np
+
+from river import optim, stats, utils
+
+from . import base
+
+__all__ = ["BiasedMF"]
+
+
+class BiasedMF(base.Recommender):
+    """Biased Matrix Factorization for recommender systems.
+
+    The model equation is defined as:
+
+    $$\\hat{y}(x) = \\bar{y} + bu_{u} + bi_{i} + \\langle \\mathbf{v}_u, \\mathbf{v}_i \\rangle$$
+
+    Where $bu_{u}$ and $bi_{i}$ are respectively the user and item biases. The last term being
+    simply the dot product between the latent vectors of the given user-item pair:
+
+    $$\\langle \\mathbf{v}_u, \\mathbf{v}_i \\rangle = \\sum_{f=1}^{k} \\mathbf{v}_{u, f} \\cdot \\mathbf{v}_{i, f}$$
+
+    where $k$ is the number of latent factors.
+
+    This model expects a dict input with a `user` and an `item` entries without any type constraint
+    on their values (i.e. can be strings or numbers). Other entries are ignored.
+
+    Parameters
+    ----------
+    n_factors
+        Dimensionality of the factorization or number of latent factors.
+    bias_optimizer
+        The sequential optimizer used for updating the bias weights.
+    latent_optimizer
+        The sequential optimizer used for updating the latent weights.
+    loss
+        The loss function to optimize for.
+    l2_bias
+        Amount of L2 regularization used to push bias weights towards 0.
+    l2_latent
+        Amount of L2 regularization used to push latent weights towards 0.
+    weight_initializer
+        Weights initialization scheme.
+    latent_initializer
+        Latent factors initialization scheme.
+    clip_gradient
+        Clips the absolute value of each gradient value.
+    seed
+        Randomization seed used for reproducibility.
+
+    Attributes
+    ----------
+    global_mean : stats.Mean
+        The target arithmetic mean.
+    u_biases : collections.defaultdict
+        The user bias weights.
+    i_biases : collections.defaultdict
+        The item bias weights.
+    u_latents : collections.defaultdict
+        The user latent vectors randomly initialized.
+    i_latents : collections.defaultdict
+        The item latent vectors randomly initialized.
+    u_bias_optimizer : optim.Optimizer
+        The sequential optimizer used for updating the user bias weights.
+    i_bias_optimizer : optim.Optimizer
+        The sequential optimizer used for updating the item bias weights.
+    u_latent_optimizer : optim.Optimizer
+        The sequential optimizer used for updating the user latent weights.
+    i_latent_optimizer : optim.Optimizer
+        The sequential optimizer used for updating the item latent weights.
+
+    Examples
+    --------
+
+    >>> from river import optim
+    >>> from river import reco
+
+    >>> dataset = (
+    ...     ({'user': 'Alice', 'item': 'Superman'}, 8),
+    ...     ({'user': 'Alice', 'item': 'Terminator'}, 9),
+    ...     ({'user': 'Alice', 'item': 'Star Wars'}, 8),
+    ...     ({'user': 'Alice', 'item': 'Notting Hill'}, 2),
+    ...     ({'user': 'Alice', 'item': 'Harry Potter'}, 5),
+    ...     ({'user': 'Bob', 'item': 'Superman'}, 8),
+    ...     ({'user': 'Bob', 'item': 'Terminator'}, 9),
+    ...     ({'user': 'Bob', 'item': 'Star Wars'}, 8),
+    ...     ({'user': 'Bob', 'item': 'Notting Hill'}, 2)
+    ... )
+
+    >>> model = reco.BiasedMF(
+    ...     n_factors=10,
+    ...     bias_optimizer=optim.SGD(0.025),
+    ...     latent_optimizer=optim.SGD(0.025),
+    ...     latent_initializer=optim.initializers.Normal(mu=0., sigma=0.1, seed=71)
+    ... )
+
+    >>> for x, y in dataset:
+    ...     _ = model.learn_one(x, y)
+
+    >>> model.predict_one({'user': 'Bob', 'item': 'Harry Potter'})
+    6.489025
+
+    References
+    ----------
+    [^1]: [Paterek, A., 2007, August. Improving regularized singular value decomposition for collaborative filtering. In Proceedings of KDD cup and workshop (Vol. 2007, pp. 5-8)](https://www.cs.uic.edu/~liub/KDD-cup-2007/proceedings/Regular-Paterek.pdf)
+    [^2]: [Matrix factorization techniques for recommender systems](https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf)
+
+    """
+
+    def __init__(
+        self,
+        n_factors=10,
+        bias_optimizer: optim.Optimizer = None,
+        latent_optimizer: optim.Optimizer = None,
+        loss: optim.losses.Loss = None,
+        l2_bias=0.0,
+        l2_latent=0.0,
+        weight_initializer: optim.initializers.Initializer = None,
+        latent_initializer: optim.initializers.Initializer = None,
+        clip_gradient=1e12,
+        seed: int = None,
+    ):
+
+        self.n_factors = n_factors
+        self.u_bias_optimizer = (
+            optim.SGD() if bias_optimizer is None else copy.deepcopy(bias_optimizer)
+        )
+        self.i_bias_optimizer = (
+            optim.SGD() if bias_optimizer is None else copy.deepcopy(bias_optimizer)
+        )
+        self.u_latent_optimizer = (
+            optim.SGD() if latent_optimizer is None else copy.deepcopy(latent_optimizer)
+        )
+        self.i_latent_optimizer = (
+            optim.SGD() if latent_optimizer is None else copy.deepcopy(latent_optimizer)
+        )
+        self.loss = optim.losses.Squared() if loss is None else loss
+        self.l2_bias = l2_bias
+        self.l2_latent = l2_latent
+
+        if weight_initializer is None:
+            weight_initializer = optim.initializers.Zeros()
+        self.weight_initializer = weight_initializer
+
+        if latent_initializer is None:
+            latent_initializer = optim.initializers.Normal(sigma=0.1, seed=seed)
+        self.latent_initializer = latent_initializer
+
+        self.clip_gradient = clip_gradient
+        self.seed = seed
+        self.global_mean = stats.Mean()
+
+        self.u_biases: typing.DefaultDict[
+            int, optim.initializers.Initializer
+        ] = collections.defaultdict(weight_initializer)
+        self.i_biases: typing.DefaultDict[
+            int, optim.initializers.Initializer
+        ] = collections.defaultdict(weight_initializer)
+
+        random_latents = functools.partial(
+            self.latent_initializer, shape=self.n_factors
+        )
+        self.u_latents: typing.DefaultDict[
+            int, optim.initializers.Initializer
+        ] = collections.defaultdict(random_latents)
+        self.i_latents: typing.DefaultDict[
+            int, optim.initializers.Initializer
+        ] = collections.defaultdict(random_latents)
+
+    def _predict_one(self, user, item):
+
+        # Initialize the prediction to the mean
+        y_pred = self.global_mean.get()
+
+        # Add the user bias
+        y_pred += self.u_biases[user]
+
+        # Add the item bias
+        y_pred += self.i_biases[item]
+
+        # Add the dot product of the user and the item latent vectors
+        y_pred += np.dot(self.u_latents[user], self.i_latents[item])
+
+        return y_pred
+
+    def _learn_one(self, user, item, y):
+
+        # Update the global mean
+        self.global_mean.update(y)
+
+        # Calculate the gradient of the loss with respect to the prediction
+        g_loss = self.loss.gradient(y, self._predict_one(user, item))
+
+        # Clamp the gradient to avoid numerical instability
+        g_loss = utils.math.clamp(
+            g_loss, minimum=-self.clip_gradient, maximum=self.clip_gradient
+        )
+
+        # Calculate weights gradients
+        u_grad_bias = {user: g_loss + self.l2_bias * self.u_biases[user]}
+        i_grad_bias = {item: g_loss + self.l2_bias * self.i_biases[item]}
+        u_latent_grad = {
+            user: g_loss * self.i_latents[item] + self.l2_latent * self.u_latents[user]
+        }
+        i_latent_grad = {
+            item: g_loss * self.u_latents[user] + self.l2_latent * self.i_latents[item]
+        }
+
+        # Update weights
+        self.u_biases = self.u_bias_optimizer.step(self.u_biases, u_grad_bias)
+        self.i_biases = self.i_bias_optimizer.step(self.i_biases, i_grad_bias)
+        self.u_latents = self.u_latent_optimizer.step(self.u_latents, u_latent_grad)
+        self.i_latents = self.i_latent_optimizer.step(self.i_latents, i_latent_grad)
+
+        return self
```

### Comparing `river-0.8.0/river/reco/funk_mf.py` & `river-0.9.0/river/reco/funk_mf.py`

 * *Ordering differences only*

 * *Files 27% similar despite different names*

```diff
@@ -1,153 +1,153 @@
-import collections
-import copy
-import functools
-import typing
-
-import numpy as np
-
-from river import optim, utils
-
-from . import base
-
-__all__ = ["FunkMF"]
-
-
-class FunkMF(base.Recommender):
-    """Funk Matrix Factorization for recommender systems.
-
-    The model equation is defined as:
-
-    $$\\hat{y}(x) = \\langle \\mathbf{v}_u, \\mathbf{v}_i \\rangle = \\sum_{f=1}^{k} \\mathbf{v}_{u, f} \\cdot \\mathbf{v}_{i, f}$$
-
-    where $k$ is the number of latent factors.
-
-    This model expects a dict input with a `user` and an `item` entries without any type
-    constraint on their values (i.e. can be strings or numbers). Other entries are ignored.
-
-    Parameters
-    ----------
-    n_factors
-        Dimensionality of the factorization or number of latent factors.
-    optimizer
-        The sequential optimizer used for updating the latent factors.
-    loss
-        The loss function to optimize for.
-    l2
-        Amount of L2 regularization used to push weights towards 0.
-    initializer
-        Latent factors initialization scheme.
-    clip_gradient
-        Clips the absolute value of each gradient value.
-    seed
-        Randomization seed used for reproducibility.
-
-    Attributes
-    ----------
-    u_latents : collections.defaultdict
-        The user latent vectors randomly initialized.
-    i_latents : collections.defaultdict
-        The item latent vectors randomly initialized.
-    u_optimizer : optim.Optimizer
-        The sequential optimizer used for updating the user latent weights.
-    i_optimizer : optim.Optimizer
-        The sequential optimizer used for updating the item latent weights.
-
-    Examples
-    --------
-
-    >>> from river import optim
-    >>> from river import reco
-
-    >>> dataset = (
-    ...     ({'user': 'Alice', 'item': 'Superman'}, 8),
-    ...     ({'user': 'Alice', 'item': 'Terminator'}, 9),
-    ...     ({'user': 'Alice', 'item': 'Star Wars'}, 8),
-    ...     ({'user': 'Alice', 'item': 'Notting Hill'}, 2),
-    ...     ({'user': 'Alice', 'item': 'Harry Potter'}, 5),
-    ...     ({'user': 'Bob', 'item': 'Superman'}, 8),
-    ...     ({'user': 'Bob', 'item': 'Terminator'}, 9),
-    ...     ({'user': 'Bob', 'item': 'Star Wars'}, 8),
-    ...     ({'user': 'Bob', 'item': 'Notting Hill'}, 2)
-    ... )
-
-    >>> model = reco.FunkMF(
-    ...     n_factors=10,
-    ...     optimizer=optim.SGD(0.1),
-    ...     initializer=optim.initializers.Normal(mu=0., sigma=0.1, seed=11),
-    ... )
-
-    >>> for x, y in dataset:
-    ...     _ = model.learn_one(x, y)
-
-    >>> model.predict_one({'user': 'Bob', 'item': 'Harry Potter'})
-    1.866272
-
-    References
-    ----------
-    [^1]: [Netflix update: Try this at home](https://sifter.org/simon/journal/20061211.html)
-    [^2]: [Matrix factorization techniques for recommender systems](https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf)
-
-    """
-
-    def __init__(
-        self,
-        n_factors=10,
-        optimizer: optim.Optimizer = None,
-        loss: optim.losses.Loss = None,
-        l2=0.0,
-        initializer: optim.initializers.Initializer = None,
-        clip_gradient=1e12,
-        seed: int = None,
-    ):
-
-        self.n_factors = n_factors
-        self.u_optimizer = (
-            optim.SGD() if optimizer is None else copy.deepcopy(optimizer)
-        )
-        self.i_optimizer = (
-            optim.SGD() if optimizer is None else copy.deepcopy(optimizer)
-        )
-        self.loss = optim.losses.Squared() if loss is None else loss
-        self.l2 = l2
-
-        if initializer is None:
-            initializer = optim.initializers.Normal(mu=0.0, sigma=0.1, seed=seed)
-        self.initializer = initializer
-
-        self.clip_gradient = clip_gradient
-        self.seed = seed
-
-        random_latents = functools.partial(self.initializer, shape=self.n_factors)
-        self.u_latents: typing.DefaultDict[
-            int, optim.initializers.Initializer
-        ] = collections.defaultdict(random_latents)
-        self.i_latents: typing.DefaultDict[
-            int, optim.initializers.Initializer
-        ] = collections.defaultdict(random_latents)
-
-    def _predict_one(self, user, item):
-        return np.dot(self.u_latents[user], self.i_latents[item])
-
-    def _learn_one(self, user, item, y):
-
-        # Calculate the gradient of the loss with respect to the prediction
-        g_loss = self.loss.gradient(y, self._predict_one(user, item))
-
-        # Clamp the gradient to avoid numerical instability
-        g_loss = utils.math.clamp(
-            g_loss, minimum=-self.clip_gradient, maximum=self.clip_gradient
-        )
-
-        # Calculate latent gradients
-        u_latent_grad = {
-            user: g_loss * self.i_latents[item] + self.l2 * self.u_latents[user]
-        }
-        i_latent_grad = {
-            item: g_loss * self.u_latents[user] + self.l2 * self.i_latents[item]
-        }
-
-        # Update latent weights
-        self.u_latents = self.u_optimizer.step(self.u_latents, u_latent_grad)
-        self.i_latents = self.i_optimizer.step(self.i_latents, i_latent_grad)
-
-        return self
+import collections
+import copy
+import functools
+import typing
+
+import numpy as np
+
+from river import optim, utils
+
+from . import base
+
+__all__ = ["FunkMF"]
+
+
+class FunkMF(base.Recommender):
+    """Funk Matrix Factorization for recommender systems.
+
+    The model equation is defined as:
+
+    $$\\hat{y}(x) = \\langle \\mathbf{v}_u, \\mathbf{v}_i \\rangle = \\sum_{f=1}^{k} \\mathbf{v}_{u, f} \\cdot \\mathbf{v}_{i, f}$$
+
+    where $k$ is the number of latent factors.
+
+    This model expects a dict input with a `user` and an `item` entries without any type
+    constraint on their values (i.e. can be strings or numbers). Other entries are ignored.
+
+    Parameters
+    ----------
+    n_factors
+        Dimensionality of the factorization or number of latent factors.
+    optimizer
+        The sequential optimizer used for updating the latent factors.
+    loss
+        The loss function to optimize for.
+    l2
+        Amount of L2 regularization used to push weights towards 0.
+    initializer
+        Latent factors initialization scheme.
+    clip_gradient
+        Clips the absolute value of each gradient value.
+    seed
+        Randomization seed used for reproducibility.
+
+    Attributes
+    ----------
+    u_latents : collections.defaultdict
+        The user latent vectors randomly initialized.
+    i_latents : collections.defaultdict
+        The item latent vectors randomly initialized.
+    u_optimizer : optim.Optimizer
+        The sequential optimizer used for updating the user latent weights.
+    i_optimizer : optim.Optimizer
+        The sequential optimizer used for updating the item latent weights.
+
+    Examples
+    --------
+
+    >>> from river import optim
+    >>> from river import reco
+
+    >>> dataset = (
+    ...     ({'user': 'Alice', 'item': 'Superman'}, 8),
+    ...     ({'user': 'Alice', 'item': 'Terminator'}, 9),
+    ...     ({'user': 'Alice', 'item': 'Star Wars'}, 8),
+    ...     ({'user': 'Alice', 'item': 'Notting Hill'}, 2),
+    ...     ({'user': 'Alice', 'item': 'Harry Potter'}, 5),
+    ...     ({'user': 'Bob', 'item': 'Superman'}, 8),
+    ...     ({'user': 'Bob', 'item': 'Terminator'}, 9),
+    ...     ({'user': 'Bob', 'item': 'Star Wars'}, 8),
+    ...     ({'user': 'Bob', 'item': 'Notting Hill'}, 2)
+    ... )
+
+    >>> model = reco.FunkMF(
+    ...     n_factors=10,
+    ...     optimizer=optim.SGD(0.1),
+    ...     initializer=optim.initializers.Normal(mu=0., sigma=0.1, seed=11),
+    ... )
+
+    >>> for x, y in dataset:
+    ...     _ = model.learn_one(x, y)
+
+    >>> model.predict_one({'user': 'Bob', 'item': 'Harry Potter'})
+    1.866272
+
+    References
+    ----------
+    [^1]: [Netflix update: Try this at home](https://sifter.org/simon/journal/20061211.html)
+    [^2]: [Matrix factorization techniques for recommender systems](https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf)
+
+    """
+
+    def __init__(
+        self,
+        n_factors=10,
+        optimizer: optim.Optimizer = None,
+        loss: optim.losses.Loss = None,
+        l2=0.0,
+        initializer: optim.initializers.Initializer = None,
+        clip_gradient=1e12,
+        seed: int = None,
+    ):
+
+        self.n_factors = n_factors
+        self.u_optimizer = (
+            optim.SGD() if optimizer is None else copy.deepcopy(optimizer)
+        )
+        self.i_optimizer = (
+            optim.SGD() if optimizer is None else copy.deepcopy(optimizer)
+        )
+        self.loss = optim.losses.Squared() if loss is None else loss
+        self.l2 = l2
+
+        if initializer is None:
+            initializer = optim.initializers.Normal(mu=0.0, sigma=0.1, seed=seed)
+        self.initializer = initializer
+
+        self.clip_gradient = clip_gradient
+        self.seed = seed
+
+        random_latents = functools.partial(self.initializer, shape=self.n_factors)
+        self.u_latents: typing.DefaultDict[
+            int, optim.initializers.Initializer
+        ] = collections.defaultdict(random_latents)
+        self.i_latents: typing.DefaultDict[
+            int, optim.initializers.Initializer
+        ] = collections.defaultdict(random_latents)
+
+    def _predict_one(self, user, item):
+        return np.dot(self.u_latents[user], self.i_latents[item])
+
+    def _learn_one(self, user, item, y):
+
+        # Calculate the gradient of the loss with respect to the prediction
+        g_loss = self.loss.gradient(y, self._predict_one(user, item))
+
+        # Clamp the gradient to avoid numerical instability
+        g_loss = utils.math.clamp(
+            g_loss, minimum=-self.clip_gradient, maximum=self.clip_gradient
+        )
+
+        # Calculate latent gradients
+        u_latent_grad = {
+            user: g_loss * self.i_latents[item] + self.l2 * self.u_latents[user]
+        }
+        i_latent_grad = {
+            item: g_loss * self.u_latents[user] + self.l2 * self.i_latents[item]
+        }
+
+        # Update latent weights
+        self.u_latents = self.u_optimizer.step(self.u_latents, u_latent_grad)
+        self.i_latents = self.i_optimizer.step(self.i_latents, i_latent_grad)
+
+        return self
```

### Comparing `river-0.8.0/river/rules/amrules.py` & `river-0.9.0/river/rules/amrules.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,536 +1,537 @@
-import collections
-import functools
-import io
-import math
-import typing
-
-from river import base, drift, linear_model, stats, tree
-
-from ..tree.split_criterion import VarianceRatioSplitCriterion
-from ..tree.splitter.base import Splitter
-from ..tree.splitter.nominal_splitter_reg import NominalSplitterReg
-from .base import HoeffdingRule
-
-
-class MeanRegressor(base.Regressor):
-    def __init__(self):
-        self.mean = stats.Mean()
-
-    def learn_one(self, x: dict, y: base.typing.RegTarget, w: int = 1):
-        self.mean.update(y, w)
-        return self
-
-    def predict_one(self, x: dict):
-        return self.mean.get()
-
-
-class AdaptiveRegressor(base.Regressor):
-    """This predictor selects between the target mean and the the user-selected regression model
-     automatically. Faded error metrics are kept for both the predictors, and the most
-    accurate one is selected automatically for each incoming instance.
-    """
-
-    def __init__(self, model_predictor: base.Regressor, alpha: float):
-        self.model_predictor = model_predictor
-        self.mean_predictor = MeanRegressor()
-        self.alpha = alpha
-
-        self._mae_mean = 0.0
-        self._mae_model = 0.0
-
-    def learn_one(self, x: dict, y: base.typing.RegTarget, w: int = 1):
-        abs_error_mean = abs(y - self.mean_predictor.predict_one(x))  # noqa
-        abs_error_model = abs(y - self.model_predictor.predict_one(x))  # noqa
-
-        self._mae_mean = self.alpha * self._mae_mean + abs_error_mean
-        self._mae_model = self.alpha * self._mae_model + abs_error_model
-
-        self.mean_predictor.learn_one(x, y, w)
-
-        try:
-            self.model_predictor.learn_one(x, y, w)  # noqa
-        except TypeError:
-            for _ in range(int(w)):
-                self.model_predictor.learn_one(x, y)
-
-        return self
-
-    def predict_one(self, x: dict):
-        if self._mae_mean <= self._mae_model:
-            return self.mean_predictor.predict_one(x)
-        else:
-            return self.model_predictor.predict_one(x)
-
-
-class RegRule(HoeffdingRule, base.Regressor, base.AnomalyDetector):
-    def __init__(
-        self, template_splitter, split_criterion, pred_model, drift_detector,
-    ):
-        super().__init__(
-            template_splitter=template_splitter, split_criterion=split_criterion,
-        )
-        self.pred_model = pred_model
-        self.drift_detector = drift_detector
-
-        self._target_stats = stats.Var()
-        self._feat_stats = collections.defaultdict(functools.partial(stats.Var))
-
-    def new_nominal_splitter(self):
-        return NominalSplitterReg()
-
-    @property
-    def statistics(self):
-        return self._target_stats
-
-    @statistics.setter
-    def statistics(self, target_stats):
-        self._target_stats = target_stats
-
-    def _update_target_stats(self, y, w):
-        self._target_stats.update(y, w)
-
-    def _update_feature_stats(self, feat_name, feat_val, w):
-        if feat_name not in self.nominal_features:
-            self._feat_stats[feat_name].update(feat_val, w)
-
-    def drift_test(self, y, y_pred):
-        abs_error = abs(y - y_pred)
-        in_drift, _ = self.drift_detector.update(abs_error)
-
-        return in_drift
-
-    def score_one(self, x) -> float:
-        """Rule anomaly score.
-
-        The more negative the score, the more anomalous is the instance regarding the subspace
-        covered by the rule. Instances whose score is greater than zero are considered normal.
-        Different threshold values can be used to decide upon discarding or using the instance
-        for training. A small negative threshold is considered to be a conservative choice,
-        whereas cutting at zero is a more aggressive choice.
-
-        Parameters
-        ----------
-        x
-            A dictionary of features.
-
-        Returns
-        -------
-        An anomaly score. The more negative the score, the more anomalous is the instance.
-
-
-        """
-        score = 0.0
-        hits = 0
-
-        for feat_name, feat_val in x.items():
-            # Skip nominal features
-            if feat_name in self.nominal_features:
-                continue
-
-            mean = self._feat_stats[feat_name].mean.get()
-            var = self._feat_stats[feat_name].get()
-
-            if var > 0:
-                # One tailed variant of Chebyshev's inequality: ported from the MOA code
-                proba = (2 * var) / (var + (feat_val - mean) ** 2)
-
-                if 0 < proba < 1:
-                    score += math.log(proba) - math.log(1 - proba)
-                    hits += 1
-
-        return score / hits if hits > 0 else 0.0
-
-    def learn_one(self, x: dict, y: base.typing.RegTarget, w: int = 1):
-        self.update(x, y, w)
-        self.pred_model.learn_one(x, y, w)
-
-        return self
-
-    def predict_one(self, x: dict):
-        return self.pred_model.predict_one(x)
-
-
-class AMRules(base.Regressor):
-    """Adaptive Model Rules.
-
-    AMRules[^1] is a rule-based algorithm for incremental regression tasks. AMRules relies on the
-    Hoeffding bound to build its rule set, similarly to Hoeffding Trees. The Variance-Ratio
-    heuristic is used to evaluate rules' splits. Moreover, this rule-based regressor has
-    additional capacities not usually found in decision trees.
-
-    Firstly, each created decision rule has a built-in drift detection mechanism. Every time a
-    drift is detected, the affected decision rule is removed. In addition, AMRules' rules also
-    have anomaly detection capabilities. After a warm-up period, each rule tests whether or not
-    the incoming instances are anomalies. Anomalous instances are not used for training.
-
-    Every time no rule is covering an incoming example, a default rule is used to learn
-    from it. A rule covers an instance when all of the rule's literals (tests joined by the
-    logical operation `and`) match the input case. The default rule is also applied for predicting
-    examples not covered by any rules from the rule set.
-
-    Parameters
-    ----------
-    n_min
-        The total weight that must be observed by a rule between expansion attempts.
-    delta
-        The split test significance. The split confidence is given by `1 - delta`.
-    tau
-        The tie-breaking threshold.
-    pred_type
-        The prediction strategy used by the decision rules. Can be either:</br>
-        - `"mean"`: outputs the target mean within the partitions defined by the decision
-        rules.</br>
-        - `"model"`: always use instances of the model passed `pred_model` to make
-        predictions.</br>
-        - `"adaptive"`: dynamically selects between "mean" and "model" for each incoming example.
-        The most accurate option at the moment will be used.
-    pred_model
-        The regression model that will be replicated for every rule when `pred_type` is either
-        `"model"` or `"adaptive"`.
-    splitter
-        The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric
-        features and perform splits. Splitters are available in the `tree.splitter` module.
-        Different splitters are available for classification and regression tasks. Classification
-        and regression splitters can be distinguished by their property `is_target_class`.
-        This is an advanced option. Special care must be taken when choosing different splitters.
-        By default, `tree.splitter.EBSTSplitter` is used if `splitter` is `None`.
-    drift_detector
-        The drift detection model that is used by each rule. Care must be taken to avoid the
-        triggering of too many false alarms or delaying too much the concept drift detection.
-        By default, `drift.PageHinckley` is used if `drift_detector` is `None`.
-    alpha
-        The exponential decaying factor applied to the learning models' absolute errors, that
-        are monitored if `pred_type='adaptive'`. Must be between `0` and `1`. The closer
-        to `1`, the more importance is going to be given to past observations. On the other hand,
-        if its value approaches `0`, the recent observed errors are going to have more influence
-        on the final decision.
-    anomaly_threshold
-        The threshold below which instances will be considered anomalies by the rules.
-    m_min
-        The minimum total weight a rule must observe before it starts to skip anomalous
-        instances during training.
-    ordered_rule_set
-        If `True`, only the first rule that covers an instance will be used for training or
-        prediction. If `False`, all the rules covering an instance will be updated during
-        training, and the predictions for an instance will be the average prediction of all
-        rules covering that example.
-    min_samples_split
-        The minimum number of samples each partition of a binary split candidate must have
-        to be considered valid.
-
-    Notes
-    -----
-    AMRules treats all the non-numerical inputs as nominal features. All instances of
-    `numbers.Number` will be treated as continuous, even if they represent integer categories.
-    When using nominal features, `pred_type` should be set to "mean", otherwise errors will be
-    thrown while trying to update the underlying rules' prediction models. Prediction strategies
-    other than "mean" can be used, as long as the prediction model passed to `pred_model` supports
-    nominal features.
-
-    Raises
-    ------
-    TypeError
-        If one or more input features are non-numeric and the selected `pred_model` is either
-        "model" or "adaptive".
-
-    Examples
-    --------
-    >>> from river import datasets
-    >>> from river import drift
-    >>> from river import evaluate
-    >>> from river import metrics
-    >>> from river import preprocessing
-    >>> from river import rules
-
-    >>> dataset = datasets.TrumpApproval()
-
-    >>> model = (
-    ...     preprocessing.StandardScaler() |
-    ...     rules.AMRules(
-    ...         delta=0.00001,
-    ...         n_min=50,
-    ...         drift_detector=drift.ADWIN()
-    ...     )
-    ... )
-
-    >>> metric = metrics.MAE()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    MAE: 1.079129
-
-    References
-    ----------
-    [^1]: Duarte, J., Gama, J. and Bifet, A., 2016. Adaptive model rules from high-speed data
-    streams. ACM Transactions on Knowledge Discovery from Data (TKDD), 10(3), pp.1-22.
-
-    """
-
-    _PRED_MEAN = "mean"
-    _PRED_MODEL = "model"
-    _PRED_ADAPTIVE = "adaptive"
-    _VALID_PRED = [_PRED_MEAN, _PRED_MODEL, _PRED_ADAPTIVE]
-
-    def __init__(
-        self,
-        n_min: int = 200,
-        delta: float = 1e-7,
-        tau: float = 0.05,
-        pred_type: str = "adaptive",
-        pred_model: base.Regressor = None,
-        splitter: Splitter = None,
-        drift_detector: base.DriftDetector = None,
-        alpha: float = 0.99,
-        anomaly_threshold: float = -0.75,
-        m_min: int = 30,
-        ordered_rule_set: bool = True,
-        min_samples_split: int = 5,
-    ):
-        self.n_min = n_min
-        self.delta = delta
-        self.tau = tau
-
-        if pred_type not in self._VALID_PRED:
-            raise ValueError(f"Invalid 'pred_type': {pred_type}")
-        self.pred_type = pred_type
-        self.pred_model = pred_model if pred_model else linear_model.LinearRegression()
-
-        if splitter is None:
-            self.splitter = tree.splitter.EBSTSplitter()
-        else:
-            self.splitter = splitter
-
-        self.drift_detector = (
-            drift_detector if drift_detector is not None else drift.PageHinkley()
-        )
-
-        self.alpha = alpha
-        self.anomaly_threshold = anomaly_threshold
-        self.m_min = m_min
-        self.ordered_rule_set = ordered_rule_set
-        self.min_samples_split = min_samples_split
-
-        self._default_rule = self._new_rule()
-        self._rules: typing.Dict[typing.Hashable, RegRule] = {}
-
-        self._n_drifts_detected: int = 0
-
-    def __len__(self):
-        return len(self._rules) + 1
-
-    def __getitem__(self, item):
-        return list(self._rules.values())[item]
-
-    @property
-    def n_drifts_detected(self) -> int:
-        """The number of detected concept drifts."""
-        return self._n_drifts_detected
-
-    def _new_rule(self) -> RegRule:
-        if self.pred_type == self._PRED_MEAN:
-            predictor = MeanRegressor()
-        elif self.pred_type == self._PRED_MODEL:
-            predictor = self.pred_model.clone()
-        else:  # adaptive predictor
-            predictor = AdaptiveRegressor(
-                model_predictor=self.pred_model.clone(), alpha=self.alpha,
-            )
-
-        return RegRule(
-            template_splitter=self.splitter,
-            split_criterion=VarianceRatioSplitCriterion(self.min_samples_split),
-            pred_model=predictor,
-            drift_detector=self.drift_detector.clone(),
-        )
-
-    def learn_one(self, x: dict, y: base.typing.RegTarget, w: int = 1) -> "AMRules":
-        any_covered = False
-        to_del = set()
-
-        for rule_id, rule in self._rules.items():
-            if not rule.covers(x):
-                continue
-
-            # Anomaly detected skip training
-            if (
-                rule.total_weight > self.m_min
-                and rule.score_one(x) < self.anomaly_threshold
-            ):
-                continue
-
-            y_pred = rule.predict_one(x)
-
-            in_drift = rule.drift_test(y, y_pred)  # noqa
-            if in_drift:
-                to_del.add(rule_id)
-                self._n_drifts_detected += 1
-                continue
-
-            any_covered = True
-            rule.learn_one(x, y, w)
-
-            if rule.total_weight - rule.last_expansion_attempt_at >= self.n_min:
-                updated_rule, expanded = rule.expand(self.delta, self.tau)
-
-                if expanded:
-                    updated_rule.pred_model = rule.pred_model
-                    self._rules[rule_id] = updated_rule
-
-            # Only the first matching rule is updated
-            if self.ordered_rule_set:
-                break
-
-        if not any_covered:
-            self._default_rule.learn_one(x, y, w)
-
-            expanded = False
-            if (
-                self._default_rule.total_weight
-                - self._default_rule.last_expansion_attempt_at
-                >= self.n_min
-            ):
-                updated_rule, expanded = self._default_rule.expand(self.delta, self.tau)
-
-            if expanded:
-                updated_rule.pred_model = self._default_rule.pred_model  # noqa
-                code = hash(updated_rule)
-                self._rules[code] = updated_rule
-
-                self._default_rule = self._new_rule()
-
-        # Drop the rules affected by concept drift
-        for rule_id in to_del:
-            del self._rules[rule_id]
-
-        return self
-
-    def predict_one(self, x: dict) -> base.typing.RegTarget:
-        y_pred = 0
-        hits = 0
-
-        for rule in self._rules.values():
-            if rule.covers(x):
-                y_pred += rule.predict_one(x)
-                hits += 1
-
-                if self.ordered_rule_set:
-                    break
-
-        if hits > 0:
-            return y_pred / hits
-        else:
-            return self._default_rule.predict_one(x)
-
-    def anomaly_score(self, x) -> typing.Tuple[float, float, float]:
-        """Aggregated anomaly score computed using all the rules that cover the input instance.
-
-        Returns the mean anomaly score, the standard deviation of the score, and the proportion
-        of rules that cover the instance (support). If the support is zero, it means that the
-        default rule was used (not other rule covered `x`).
-
-        Parameters
-        ----------
-        x
-            The input instance.
-
-        Returns
-        -------
-        mean_anomaly_score, std_anomaly_score, support
-
-        Examples
-        --------
-        >>> from river import drift
-        >>> from river import rules
-        >>> from river import tree
-        >>> from river import synth
-
-        >>> dataset = synth.Friedman(seed=42).take(1001)
-
-        >>> model = rules.AMRules(
-        ...     n_min=50,
-        ...     delta=0.1,
-        ...     drift_detector=drift.ADWIN(),
-        ...     splitter=tree.splitter.QOSplitter()
-        ... )
-
-        >>> for i, (x, y) in enumerate(dataset):
-        ...     if i == 1000:
-        ...         # Skip the last example
-        ...         break
-        ...     model = model.learn_one(x, y)
-
-        >>> model.anomaly_score(x)
-        (1.0168907243483924, 0.13045786430817474, 1.0)
-
-        """
-        var = stats.Var()
-
-        for rule in self._rules.values():
-            if rule.covers(x):
-                var.update(rule.score_one(x))
-
-        if var.mean.n > 0:
-            return var.mean.get(), math.sqrt(var.get()), var.mean.n / len(self._rules)
-
-        # No rule covers the instance. Use the default rule
-        return self._default_rule.score_one(x), 0.0, 0
-
-    def debug_one(self, x) -> str:
-        """Return an explanation of how `x` is predicted
-
-        Parameters
-        ----------
-        x
-            The input instance.
-
-        Returns
-        -------
-        A representation of the rules that cover the input and their prediction.
-
-        Examples
-        --------
-        >>> from river import drift
-        >>> from river import rules
-        >>> from river import tree
-        >>> from river import synth
-
-        >>> dataset = synth.Friedman(seed=42).take(1001)
-
-        >>> model = rules.AMRules(
-        ...     n_min=50,
-        ...     delta=0.1,
-        ...     drift_detector=drift.ADWIN(),
-        ...     splitter=tree.splitter.QOSplitter()
-        ... )
-
-        >>> for i, (x, y) in enumerate(dataset):
-        ...     if i == 1000:
-        ...         # Skip the last example
-        ...         break
-        ...     model = model.learn_one(x, y)
-
-        >>> print(model.debug_one(x))
-        Rule 0: 3 > 0.5060027751338432 and 0 > 0.25379161985850063
-            Prediction: 18.72169583862947
-        <BLANKLINE>
-
-        """
-        buffer = io.StringIO()
-        _print = functools.partial(print, file=buffer)
-
-        any_covered = False
-        for i, rule in enumerate(self._rules.values()):
-            if rule.covers(x):
-                any_covered = True
-                _print(f"Rule {i}: {repr(rule)}")
-                _print(f"\tPrediction: {rule.predict_one(x)}")
-            if self.ordered_rule_set:
-                break
-
-        if any_covered:
-            if not self.ordered_rule_set:
-                _print(f"Final prediction: {self.predict_one(x)}")
-        else:
-            _print("Default rule triggered:")
-            _print(f"\tPrediction: {self._default_rule.predict_one(x)}")
-
-        return buffer.getvalue()
+import collections
+import functools
+import io
+import math
+import typing
+
+from river import base, drift, linear_model, stats, tree
+from river.anomaly import AnomalyDetector
+
+from ..tree.split_criterion import VarianceRatioSplitCriterion
+from ..tree.splitter.base import Splitter
+from ..tree.splitter.nominal_splitter_reg import NominalSplitterReg
+from .base import HoeffdingRule
+
+
+class MeanRegressor(base.Regressor):
+    def __init__(self):
+        self.mean = stats.Mean()
+
+    def learn_one(self, x: dict, y: base.typing.RegTarget, w: int = 1):
+        self.mean.update(y, w)
+        return self
+
+    def predict_one(self, x: dict):
+        return self.mean.get()
+
+
+class AdaptiveRegressor(base.Regressor):
+    """This predictor selects between the target mean and the the user-selected regression model
+     automatically. Faded error metrics are kept for both the predictors, and the most
+    accurate one is selected automatically for each incoming instance.
+    """
+
+    def __init__(self, model_predictor: base.Regressor, alpha: float):
+        self.model_predictor = model_predictor
+        self.mean_predictor = MeanRegressor()
+        self.alpha = alpha
+
+        self._mae_mean = 0.0
+        self._mae_model = 0.0
+
+    def learn_one(self, x: dict, y: base.typing.RegTarget, w: int = 1):
+        abs_error_mean = abs(y - self.mean_predictor.predict_one(x))  # noqa
+        abs_error_model = abs(y - self.model_predictor.predict_one(x))  # noqa
+
+        self._mae_mean = self.alpha * self._mae_mean + abs_error_mean
+        self._mae_model = self.alpha * self._mae_model + abs_error_model
+
+        self.mean_predictor.learn_one(x, y, w)
+
+        try:
+            self.model_predictor.learn_one(x, y, w)  # noqa
+        except TypeError:
+            for _ in range(int(w)):
+                self.model_predictor.learn_one(x, y)
+
+        return self
+
+    def predict_one(self, x: dict):
+        if self._mae_mean <= self._mae_model:
+            return self.mean_predictor.predict_one(x)
+        else:
+            return self.model_predictor.predict_one(x)
+
+
+class RegRule(HoeffdingRule, base.Regressor, AnomalyDetector):
+    def __init__(
+        self, template_splitter, split_criterion, pred_model, drift_detector,
+    ):
+        super().__init__(
+            template_splitter=template_splitter, split_criterion=split_criterion,
+        )
+        self.pred_model = pred_model
+        self.drift_detector = drift_detector
+
+        self._target_stats = stats.Var()
+        self._feat_stats = collections.defaultdict(functools.partial(stats.Var))
+
+    def new_nominal_splitter(self):
+        return NominalSplitterReg()
+
+    @property
+    def statistics(self):
+        return self._target_stats
+
+    @statistics.setter
+    def statistics(self, target_stats):
+        self._target_stats = target_stats
+
+    def _update_target_stats(self, y, w):
+        self._target_stats.update(y, w)
+
+    def _update_feature_stats(self, feat_name, feat_val, w):
+        if feat_name not in self.nominal_features:
+            self._feat_stats[feat_name].update(feat_val, w)
+
+    def drift_test(self, y, y_pred):
+        abs_error = abs(y - y_pred)
+        in_drift, _ = self.drift_detector.update(abs_error)
+
+        return in_drift
+
+    def score_one(self, x) -> float:
+        """Rule anomaly score.
+
+        The more negative the score, the more anomalous is the instance regarding the subspace
+        covered by the rule. Instances whose score is greater than zero are considered normal.
+        Different threshold values can be used to decide upon discarding or using the instance
+        for training. A small negative threshold is considered to be a conservative choice,
+        whereas cutting at zero is a more aggressive choice.
+
+        Parameters
+        ----------
+        x
+            A dictionary of features.
+
+        Returns
+        -------
+        An anomaly score. The more negative the score, the more anomalous is the instance.
+
+
+        """
+        score = 0.0
+        hits = 0
+
+        for feat_name, feat_val in x.items():
+            # Skip nominal features
+            if feat_name in self.nominal_features:
+                continue
+
+            mean = self._feat_stats[feat_name].mean.get()
+            var = self._feat_stats[feat_name].get()
+
+            if var > 0:
+                # One tailed variant of Chebyshev's inequality: ported from the MOA code
+                proba = (2 * var) / (var + (feat_val - mean) ** 2)
+
+                if 0 < proba < 1:
+                    score += math.log(proba) - math.log(1 - proba)
+                    hits += 1
+
+        return score / hits if hits > 0 else 0.0
+
+    def learn_one(self, x: dict, y: base.typing.RegTarget, w: int = 1):
+        self.update(x, y, w)
+        self.pred_model.learn_one(x, y, w)
+
+        return self
+
+    def predict_one(self, x: dict):
+        return self.pred_model.predict_one(x)
+
+
+class AMRules(base.Regressor):
+    """Adaptive Model Rules.
+
+    AMRules[^1] is a rule-based algorithm for incremental regression tasks. AMRules relies on the
+    Hoeffding bound to build its rule set, similarly to Hoeffding Trees. The Variance-Ratio
+    heuristic is used to evaluate rules' splits. Moreover, this rule-based regressor has
+    additional capacities not usually found in decision trees.
+
+    Firstly, each created decision rule has a built-in drift detection mechanism. Every time a
+    drift is detected, the affected decision rule is removed. In addition, AMRules' rules also
+    have anomaly detection capabilities. After a warm-up period, each rule tests whether or not
+    the incoming instances are anomalies. Anomalous instances are not used for training.
+
+    Every time no rule is covering an incoming example, a default rule is used to learn
+    from it. A rule covers an instance when all of the rule's literals (tests joined by the
+    logical operation `and`) match the input case. The default rule is also applied for predicting
+    examples not covered by any rules from the rule set.
+
+    Parameters
+    ----------
+    n_min
+        The total weight that must be observed by a rule between expansion attempts.
+    delta
+        The split test significance. The split confidence is given by `1 - delta`.
+    tau
+        The tie-breaking threshold.
+    pred_type
+        The prediction strategy used by the decision rules. Can be either:</br>
+        - `"mean"`: outputs the target mean within the partitions defined by the decision
+        rules.</br>
+        - `"model"`: always use instances of the model passed `pred_model` to make
+        predictions.</br>
+        - `"adaptive"`: dynamically selects between "mean" and "model" for each incoming example.
+        The most accurate option at the moment will be used.
+    pred_model
+        The regression model that will be replicated for every rule when `pred_type` is either
+        `"model"` or `"adaptive"`.
+    splitter
+        The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric
+        features and perform splits. Splitters are available in the `tree.splitter` module.
+        Different splitters are available for classification and regression tasks. Classification
+        and regression splitters can be distinguished by their property `is_target_class`.
+        This is an advanced option. Special care must be taken when choosing different splitters.
+        By default, `tree.splitter.EBSTSplitter` is used if `splitter` is `None`.
+    drift_detector
+        The drift detection model that is used by each rule. Care must be taken to avoid the
+        triggering of too many false alarms or delaying too much the concept drift detection.
+        By default, `drift.PageHinckley` is used if `drift_detector` is `None`.
+    alpha
+        The exponential decaying factor applied to the learning models' absolute errors, that
+        are monitored if `pred_type='adaptive'`. Must be between `0` and `1`. The closer
+        to `1`, the more importance is going to be given to past observations. On the other hand,
+        if its value approaches `0`, the recent observed errors are going to have more influence
+        on the final decision.
+    anomaly_threshold
+        The threshold below which instances will be considered anomalies by the rules.
+    m_min
+        The minimum total weight a rule must observe before it starts to skip anomalous
+        instances during training.
+    ordered_rule_set
+        If `True`, only the first rule that covers an instance will be used for training or
+        prediction. If `False`, all the rules covering an instance will be updated during
+        training, and the predictions for an instance will be the average prediction of all
+        rules covering that example.
+    min_samples_split
+        The minimum number of samples each partition of a binary split candidate must have
+        to be considered valid.
+
+    Notes
+    -----
+    AMRules treats all the non-numerical inputs as nominal features. All instances of
+    `numbers.Number` will be treated as continuous, even if they represent integer categories.
+    When using nominal features, `pred_type` should be set to "mean", otherwise errors will be
+    thrown while trying to update the underlying rules' prediction models. Prediction strategies
+    other than "mean" can be used, as long as the prediction model passed to `pred_model` supports
+    nominal features.
+
+    Raises
+    ------
+    TypeError
+        If one or more input features are non-numeric and the selected `pred_model` is either
+        "model" or "adaptive".
+
+    Examples
+    --------
+    >>> from river import datasets
+    >>> from river import drift
+    >>> from river import evaluate
+    >>> from river import metrics
+    >>> from river import preprocessing
+    >>> from river import rules
+
+    >>> dataset = datasets.TrumpApproval()
+
+    >>> model = (
+    ...     preprocessing.StandardScaler() |
+    ...     rules.AMRules(
+    ...         delta=0.01,
+    ...         n_min=50,
+    ...         drift_detector=drift.ADWIN()
+    ...     )
+    ... )
+
+    >>> metric = metrics.MAE()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    MAE: 0.930966
+
+    References
+    ----------
+    [^1]: Duarte, J., Gama, J. and Bifet, A., 2016. Adaptive model rules from high-speed data
+    streams. ACM Transactions on Knowledge Discovery from Data (TKDD), 10(3), pp.1-22.
+
+    """
+
+    _PRED_MEAN = "mean"
+    _PRED_MODEL = "model"
+    _PRED_ADAPTIVE = "adaptive"
+    _VALID_PRED = [_PRED_MEAN, _PRED_MODEL, _PRED_ADAPTIVE]
+
+    def __init__(
+        self,
+        n_min: int = 200,
+        delta: float = 1e-7,
+        tau: float = 0.05,
+        pred_type: str = "adaptive",
+        pred_model: base.Regressor = None,
+        splitter: Splitter = None,
+        drift_detector: base.DriftDetector = None,
+        alpha: float = 0.99,
+        anomaly_threshold: float = -0.75,
+        m_min: int = 30,
+        ordered_rule_set: bool = True,
+        min_samples_split: int = 5,
+    ):
+        self.n_min = n_min
+        self.delta = delta
+        self.tau = tau
+
+        if pred_type not in self._VALID_PRED:
+            raise ValueError(f"Invalid 'pred_type': {pred_type}")
+        self.pred_type = pred_type
+        self.pred_model = pred_model if pred_model else linear_model.LinearRegression()
+
+        if splitter is None:
+            self.splitter = tree.splitter.EBSTSplitter()
+        else:
+            self.splitter = splitter
+
+        self.drift_detector = (
+            drift_detector if drift_detector is not None else drift.PageHinkley()
+        )
+
+        self.alpha = alpha
+        self.anomaly_threshold = anomaly_threshold
+        self.m_min = m_min
+        self.ordered_rule_set = ordered_rule_set
+        self.min_samples_split = min_samples_split
+
+        self._default_rule = self._new_rule()
+        self._rules: typing.Dict[typing.Hashable, RegRule] = {}
+
+        self._n_drifts_detected: int = 0
+
+    def __len__(self):
+        return len(self._rules) + 1
+
+    def __getitem__(self, item):
+        return list(self._rules.values())[item]
+
+    @property
+    def n_drifts_detected(self) -> int:
+        """The number of detected concept drifts."""
+        return self._n_drifts_detected
+
+    def _new_rule(self) -> RegRule:
+        if self.pred_type == self._PRED_MEAN:
+            predictor = MeanRegressor()
+        elif self.pred_type == self._PRED_MODEL:
+            predictor = self.pred_model.clone()
+        else:  # adaptive predictor
+            predictor = AdaptiveRegressor(
+                model_predictor=self.pred_model.clone(), alpha=self.alpha,
+            )
+
+        return RegRule(
+            template_splitter=self.splitter,
+            split_criterion=VarianceRatioSplitCriterion(self.min_samples_split),
+            pred_model=predictor,
+            drift_detector=self.drift_detector.clone(),
+        )
+
+    def learn_one(self, x: dict, y: base.typing.RegTarget, w: int = 1) -> "AMRules":
+        any_covered = False
+        to_del = set()
+
+        for rule_id, rule in self._rules.items():
+            if not rule.covers(x):
+                continue
+
+            # Anomaly detected skip training
+            if (
+                rule.total_weight > self.m_min
+                and rule.score_one(x) < self.anomaly_threshold
+            ):
+                continue
+
+            y_pred = rule.predict_one(x)
+
+            in_drift = rule.drift_test(y, y_pred)  # noqa
+            if in_drift:
+                to_del.add(rule_id)
+                self._n_drifts_detected += 1
+                continue
+
+            any_covered = True
+            rule.learn_one(x, y, w)
+
+            if rule.total_weight - rule.last_expansion_attempt_at >= self.n_min:
+                updated_rule, expanded = rule.expand(self.delta, self.tau)
+
+                if expanded:
+                    updated_rule.pred_model = rule.pred_model
+                    self._rules[rule_id] = updated_rule
+
+            # Only the first matching rule is updated
+            if self.ordered_rule_set:
+                break
+
+        if not any_covered:
+            self._default_rule.learn_one(x, y, w)
+
+            expanded = False
+            if (
+                self._default_rule.total_weight
+                - self._default_rule.last_expansion_attempt_at
+                >= self.n_min
+            ):
+                updated_rule, expanded = self._default_rule.expand(self.delta, self.tau)
+
+            if expanded:
+                updated_rule.pred_model = self._default_rule.pred_model  # noqa
+                code = hash(updated_rule)
+                self._rules[code] = updated_rule
+
+                self._default_rule = self._new_rule()
+
+        # Drop the rules affected by concept drift
+        for rule_id in to_del:
+            del self._rules[rule_id]
+
+        return self
+
+    def predict_one(self, x: dict) -> base.typing.RegTarget:
+        y_pred = 0
+        hits = 0
+
+        for rule in self._rules.values():
+            if rule.covers(x):
+                y_pred += rule.predict_one(x)
+                hits += 1
+
+                if self.ordered_rule_set:
+                    break
+
+        if hits > 0:
+            return y_pred / hits
+        else:
+            return self._default_rule.predict_one(x)
+
+    def anomaly_score(self, x) -> typing.Tuple[float, float, float]:
+        """Aggregated anomaly score computed using all the rules that cover the input instance.
+
+        Returns the mean anomaly score, the standard deviation of the score, and the proportion
+        of rules that cover the instance (support). If the support is zero, it means that the
+        default rule was used (not other rule covered `x`).
+
+        Parameters
+        ----------
+        x
+            The input instance.
+
+        Returns
+        -------
+        mean_anomaly_score, std_anomaly_score, support
+
+        Examples
+        --------
+        >>> from river import drift
+        >>> from river import rules
+        >>> from river import tree
+        >>> from river import synth
+
+        >>> dataset = synth.Friedman(seed=42).take(1001)
+
+        >>> model = rules.AMRules(
+        ...     n_min=50,
+        ...     delta=0.1,
+        ...     drift_detector=drift.ADWIN(),
+        ...     splitter=tree.splitter.QOSplitter()
+        ... )
+
+        >>> for i, (x, y) in enumerate(dataset):
+        ...     if i == 1000:
+        ...         # Skip the last example
+        ...         break
+        ...     model = model.learn_one(x, y)
+
+        >>> model.anomaly_score(x)
+        (1.0168907243483933, 0.13045786430817402, 1.0)
+
+        """
+        var = stats.Var()
+
+        for rule in self._rules.values():
+            if rule.covers(x):
+                var.update(rule.score_one(x))
+
+        if var.mean.n > 0:
+            return var.mean.get(), math.sqrt(var.get()), var.mean.n / len(self._rules)
+
+        # No rule covers the instance. Use the default rule
+        return self._default_rule.score_one(x), 0.0, 0
+
+    def debug_one(self, x) -> str:
+        """Return an explanation of how `x` is predicted
+
+        Parameters
+        ----------
+        x
+            The input instance.
+
+        Returns
+        -------
+        A representation of the rules that cover the input and their prediction.
+
+        Examples
+        --------
+        >>> from river import drift
+        >>> from river import rules
+        >>> from river import tree
+        >>> from river import synth
+
+        >>> dataset = synth.Friedman(seed=42).take(1001)
+
+        >>> model = rules.AMRules(
+        ...     n_min=50,
+        ...     delta=0.1,
+        ...     drift_detector=drift.ADWIN(),
+        ...     splitter=tree.splitter.QOSplitter()
+        ... )
+
+        >>> for i, (x, y) in enumerate(dataset):
+        ...     if i == 1000:
+        ...         # Skip the last example
+        ...         break
+        ...     model = model.learn_one(x, y)
+
+        >>> print(model.debug_one(x))
+        Rule 0: 3 > 0.5060027751338432 and 0 > 0.25379161985850063
+            Prediction: 18.72169583862947
+        <BLANKLINE>
+
+        """
+        buffer = io.StringIO()
+        _print = functools.partial(print, file=buffer)
+
+        any_covered = False
+        for i, rule in enumerate(self._rules.values()):
+            if rule.covers(x):
+                any_covered = True
+                _print(f"Rule {i}: {repr(rule)}")
+                _print(f"\tPrediction: {rule.predict_one(x)}")
+            if self.ordered_rule_set:
+                break
+
+        if any_covered:
+            if not self.ordered_rule_set:
+                _print(f"Final prediction: {self.predict_one(x)}")
+        else:
+            _print("Default rule triggered:")
+            _print(f"\tPrediction: {self._default_rule.predict_one(x)}")
+
+        return buffer.getvalue()
```

### Comparing `river-0.8.0/river/rules/base.py` & `river-0.9.0/river/rules/base.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,299 +1,299 @@
-import abc
-import copy
-import math
-import numbers
-import typing
-
-from river import base, tree
-
-
-class Literal(base.Base):
-    __slots__ = "on", "at", "neg"
-
-    def __init__(self, on, at, neg=False):
-        self.on = on
-        self.at = at
-        self.neg = neg
-
-    @abc.abstractmethod
-    def __call__(self, x):
-        pass
-
-    @abc.abstractmethod
-    def describe(self):
-        pass
-
-
-class NumericLiteral(Literal):
-    def __init__(self, on, at, neg):
-        super().__init__(on, at, neg)
-
-    def __call__(self, x):
-        if self.on in x:
-            if not self.neg:
-                return x[self.on] <= self.at
-            else:
-                return x[self.on] > self.at
-
-        return False
-
-    def describe(self):
-        if not self.neg:
-            return f"{self.on} ≤ {self.at}"
-        else:
-            return f"{self.on} > {self.at}"
-
-
-class NominalLiteral(Literal):
-    def __init__(self, on, at, neg):
-        super().__init__(on, at, neg)
-
-    def __call__(self, x):
-        if self.on in x:
-            if not self.neg:
-                return x[self.on] == self.at
-            else:
-                return x[self.on] != self.at
-
-        return False
-
-    def describe(self):
-        if not self.neg:
-            return f"{self.on} = {self.at}"
-        else:
-            return f"{self.on} ≠ {self.at}"
-
-
-class HoeffdingRule(base.Estimator, metaclass=abc.ABCMeta):
-    """Base class for the decision rules based on the Hoeffding bound.
-
-    It defines properties and operations shared by all the rule-based systems, for instance, how
-    to perform rule expansion, how to monitor inputs, and whether or not a rule is currently
-    covering an input datum.
-
-    Parameters
-    ----------
-    template_splitter
-        The attribute observer algorithm used to monitor and perform split attempts in numerical
-        features. This splitter is be deep-copied to all the decision rules. The same attribute
-        observer algorithms used by the Hoeffding Trees can be used with Hoeffding rules.
-    split_criterion
-        The criterion used to rank the split candidates. The same split criteria used by the
-        Hoeffding Trees can be applied to the Hoeffding rules.
-    attributes
-        Other parameters passed to the rules via `**kwargs`.
-    """
-
-    def __init__(
-        self,
-        template_splitter: tree.splitter.base.Splitter,
-        split_criterion: tree.split_criterion.base.SplitCriterion,
-        **attributes,
-    ):
-        self.template_splitter = template_splitter
-        self.split_criterion = split_criterion
-        self.literals = []
-        self.splitters = {}
-
-        self._total_weight = 0
-        self._last_expansion_attempt_at = 0
-
-        self.nominal_features = set()
-
-        self.__dict__.update(attributes)
-
-    def _hoeffding_bound(self, r_heur, delta):
-        r"""Compute the Hoeffding bound, used to decide how many samples are necessary to expand
-        a decision rule.
-
-        Notes
-        -----
-        The Hoeffding bound is defined as:
-
-        $\\epsilon = \\sqrt{\\frac{R^2\\ln(1/\\delta))}{2n}}$
-
-        where:
-
-        $\\epsilon$: Hoeffding bound.
-        $R$: Range of a random variable.
-        $\\delta$: significance level.
-        $n$: Number of samples.
-
-        Parameters
-        ----------
-        r_heur
-            Range of the split heuristic.
-        delta
-            The significance level.
-        """
-        return math.sqrt(
-            (r_heur * r_heur * math.log(1.0 / delta)) / (2.0 * self.total_weight)
-        )
-
-    @property
-    @abc.abstractmethod
-    def statistics(self):
-        pass
-
-    @statistics.setter
-    @abc.abstractmethod
-    def statistics(self, target_stats):
-        pass
-
-    @property
-    def total_weight(self):
-        return self._total_weight
-
-    @property
-    def last_expansion_attempt_at(self):
-        return self._last_expansion_attempt_at
-
-    def expand(self, delta, tau):
-        """
-        Attempt to expand a decision rule.
-
-        If the expansion succeeds, a new rule with all attributes reset and updated literals is
-        returned along a boolean flag set to `True` to indicate the expansion. If the expansion
-        fails, `self` is returned along with the boolean flag set to `False`.
-
-        Parameters
-        ----------
-        delta
-            The split test significance.
-        tau
-            The tie-breaking threshold.
-
-        Returns
-        -------
-        rule, expanded
-            The (potentially expanded) rule and an indicator of whether or not an expansion was
-            successfully performed.
-
-        """
-        suggestions = []
-        for att_id, splitter in self.splitters.items():
-            suggestions.append(
-                splitter.best_evaluated_split_suggestion(
-                    criterion=self.split_criterion,
-                    pre_split_dist=self.statistics,
-                    att_idx=att_id,
-                    binary_only=True,
-                )
-            )
-        suggestions.sort()
-
-        should_expand = False
-        if len(suggestions) < 2:
-            should_expand = True
-        else:
-            b_split = suggestions[-1]
-            sb_split = suggestions[-2]
-
-            hb = self._hoeffding_bound(
-                self.split_criterion.range_of_merit(self.statistics), delta
-            )
-
-            if b_split.merit > 0 and (b_split.merit - sb_split.merit > hb or hb < tau):
-                should_expand = True
-
-        if should_expand:
-            b_split = suggestions[-1]
-            is_numerical = b_split.numerical_feature
-            branch_no = self.split_criterion.select_best_branch(b_split.children_stats)
-
-            if is_numerical:
-                lit = NumericLiteral(
-                    b_split.feature, b_split.split_info, branch_no != 0
-                )
-
-                literal_updated = False
-                for literal in self.literals:
-                    if lit.on == literal.on and lit.neg == literal.neg:
-                        # Update thresholds rather than adding a new literal
-                        if not literal.neg and lit.at < literal.at:
-                            literal.at = lit.at
-                            literal_updated = True
-                            break
-                        elif literal.neg and lit.at > literal.at:
-                            literal.at = lit.at
-                            literal_updated = True
-                            break
-
-                # No threshold was updated, thus a new literal is added
-                if not literal_updated:
-                    self.literals.append(lit)
-            else:
-                lit = NominalLiteral(
-                    b_split.feature, b_split.split_info, branch_no != 0
-                )
-                # Add a new literal
-                self.literals.append(lit)
-
-            # Reset all the statistics stored in the the decision rule
-            updated_rule = self.clone()
-            # Keep the literals
-            updated_rule.literals.extend(self.literals)
-
-            return updated_rule, True
-
-        # If the expansion attempt failed, update the expansion tracker
-        self._last_expansion_attempt_at = self.total_weight
-        return self, False
-
-    def covers(self, x: dict) -> bool:
-        """Check if all the rule's conditions are fulfilled.
-
-        Parameters
-        ----------
-        x
-            Input instance.
-        """
-        return all(map(lambda lit: lit(x), self.literals))
-
-    @abc.abstractmethod
-    def new_nominal_splitter(self):
-        """Define the attribute observer used when dealing with nominal features.
-
-        Must be defined by classification and regression decision rule-based algorithms to match
-        the target type.
-        """
-        pass
-
-    def _iter_features(self, x: dict) -> typing.Iterable:
-        """Determine how the input instance is looped through when updating the splitters.
-
-        Parameters
-        ----------
-        x
-            The input instance.
-        """
-        for att_id, att_val in x.items():
-            yield att_id, att_val
-
-    @abc.abstractmethod
-    def _update_target_stats(self, y, w):
-        pass
-
-    def _update_feature_stats(self, feat_name, feat_val, w):
-        # By default, no feature stats are kept
-        pass
-
-    def update(self, x, y, w):
-        self._total_weight += w
-        self._update_target_stats(y, w)
-
-        for feat_name, feat_val in self._iter_features(x):
-            try:
-                splt = self.splitters[feat_name]
-            except KeyError:
-                if isinstance(feat_val, numbers.Number):
-                    self.splitters[feat_name] = copy.deepcopy(self.template_splitter)
-                else:
-                    self.nominal_features.add(feat_name)
-                    self.splitters[feat_name] = self.new_nominal_splitter()
-                splt = self.splitters[feat_name]
-            splt.update(feat_val, y, w)
-            self._update_feature_stats(feat_name, feat_val, w)
-
-    def __repr__(self):
-        return f"{' and '.join([lit.describe() for lit in self.literals])}"
+import abc
+import copy
+import math
+import numbers
+import typing
+
+from river import base, tree
+
+
+class Literal(base.Base):
+    __slots__ = "on", "at", "neg"
+
+    def __init__(self, on, at, neg=False):
+        self.on = on
+        self.at = at
+        self.neg = neg
+
+    @abc.abstractmethod
+    def __call__(self, x):
+        pass
+
+    @abc.abstractmethod
+    def describe(self):
+        pass
+
+
+class NumericLiteral(Literal):
+    def __init__(self, on, at, neg):
+        super().__init__(on, at, neg)
+
+    def __call__(self, x):
+        if self.on in x:
+            if not self.neg:
+                return x[self.on] <= self.at
+            else:
+                return x[self.on] > self.at
+
+        return False
+
+    def describe(self):
+        if not self.neg:
+            return f"{self.on} ≤ {self.at}"
+        else:
+            return f"{self.on} > {self.at}"
+
+
+class NominalLiteral(Literal):
+    def __init__(self, on, at, neg):
+        super().__init__(on, at, neg)
+
+    def __call__(self, x):
+        if self.on in x:
+            if not self.neg:
+                return x[self.on] == self.at
+            else:
+                return x[self.on] != self.at
+
+        return False
+
+    def describe(self):
+        if not self.neg:
+            return f"{self.on} = {self.at}"
+        else:
+            return f"{self.on} ≠ {self.at}"
+
+
+class HoeffdingRule(base.Estimator, metaclass=abc.ABCMeta):
+    """Base class for the decision rules based on the Hoeffding bound.
+
+    It defines properties and operations shared by all the rule-based systems, for instance, how
+    to perform rule expansion, how to monitor inputs, and whether or not a rule is currently
+    covering an input datum.
+
+    Parameters
+    ----------
+    template_splitter
+        The attribute observer algorithm used to monitor and perform split attempts in numerical
+        features. This splitter is be deep-copied to all the decision rules. The same attribute
+        observer algorithms used by the Hoeffding Trees can be used with Hoeffding rules.
+    split_criterion
+        The criterion used to rank the split candidates. The same split criteria used by the
+        Hoeffding Trees can be applied to the Hoeffding rules.
+    attributes
+        Other parameters passed to the rules via `**kwargs`.
+    """
+
+    def __init__(
+        self,
+        template_splitter: tree.splitter.base.Splitter,
+        split_criterion: tree.split_criterion.base.SplitCriterion,
+        **attributes,
+    ):
+        self.template_splitter = template_splitter
+        self.split_criterion = split_criterion
+        self.literals = []
+        self.splitters = {}
+
+        self._total_weight = 0
+        self._last_expansion_attempt_at = 0
+
+        self.nominal_features = set()
+
+        self.__dict__.update(attributes)
+
+    def _hoeffding_bound(self, r_heur, delta):
+        r"""Compute the Hoeffding bound, used to decide how many samples are necessary to expand
+        a decision rule.
+
+        Notes
+        -----
+        The Hoeffding bound is defined as:
+
+        $\\epsilon = \\sqrt{\\frac{R^2\\ln(1/\\delta))}{2n}}$
+
+        where:
+
+        $\\epsilon$: Hoeffding bound.
+        $R$: Range of a random variable.
+        $\\delta$: significance level.
+        $n$: Number of samples.
+
+        Parameters
+        ----------
+        r_heur
+            Range of the split heuristic.
+        delta
+            The significance level.
+        """
+        return math.sqrt(
+            (r_heur * r_heur * math.log(1.0 / delta)) / (2.0 * self.total_weight)
+        )
+
+    @property
+    @abc.abstractmethod
+    def statistics(self):
+        pass
+
+    @statistics.setter
+    @abc.abstractmethod
+    def statistics(self, target_stats):
+        pass
+
+    @property
+    def total_weight(self):
+        return self._total_weight
+
+    @property
+    def last_expansion_attempt_at(self):
+        return self._last_expansion_attempt_at
+
+    def expand(self, delta, tau):
+        """
+        Attempt to expand a decision rule.
+
+        If the expansion succeeds, a new rule with all attributes reset and updated literals is
+        returned along a boolean flag set to `True` to indicate the expansion. If the expansion
+        fails, `self` is returned along with the boolean flag set to `False`.
+
+        Parameters
+        ----------
+        delta
+            The split test significance.
+        tau
+            The tie-breaking threshold.
+
+        Returns
+        -------
+        rule, expanded
+            The (potentially expanded) rule and an indicator of whether or not an expansion was
+            successfully performed.
+
+        """
+        suggestions = []
+        for att_id, splitter in self.splitters.items():
+            suggestions.append(
+                splitter.best_evaluated_split_suggestion(
+                    criterion=self.split_criterion,
+                    pre_split_dist=self.statistics,
+                    att_idx=att_id,
+                    binary_only=True,
+                )
+            )
+        suggestions.sort()
+
+        should_expand = False
+        if len(suggestions) < 2:
+            should_expand = True
+        else:
+            b_split = suggestions[-1]
+            sb_split = suggestions[-2]
+
+            hb = self._hoeffding_bound(
+                self.split_criterion.range_of_merit(self.statistics), delta
+            )
+
+            if b_split.merit > 0 and (b_split.merit - sb_split.merit > hb or hb < tau):
+                should_expand = True
+
+        if should_expand:
+            b_split = suggestions[-1]
+            is_numerical = b_split.numerical_feature
+            branch_no = self.split_criterion.select_best_branch(b_split.children_stats)
+
+            if is_numerical:
+                lit = NumericLiteral(
+                    b_split.feature, b_split.split_info, branch_no != 0
+                )
+
+                literal_updated = False
+                for literal in self.literals:
+                    if lit.on == literal.on and lit.neg == literal.neg:
+                        # Update thresholds rather than adding a new literal
+                        if not literal.neg and lit.at < literal.at:
+                            literal.at = lit.at
+                            literal_updated = True
+                            break
+                        elif literal.neg and lit.at > literal.at:
+                            literal.at = lit.at
+                            literal_updated = True
+                            break
+
+                # No threshold was updated, thus a new literal is added
+                if not literal_updated:
+                    self.literals.append(lit)
+            else:
+                lit = NominalLiteral(
+                    b_split.feature, b_split.split_info, branch_no != 0
+                )
+                # Add a new literal
+                self.literals.append(lit)
+
+            # Reset all the statistics stored in the the decision rule
+            updated_rule = self.clone()
+            # Keep the literals
+            updated_rule.literals.extend(self.literals)
+
+            return updated_rule, True
+
+        # If the expansion attempt failed, update the expansion tracker
+        self._last_expansion_attempt_at = self.total_weight
+        return self, False
+
+    def covers(self, x: dict) -> bool:
+        """Check if all the rule's conditions are fulfilled.
+
+        Parameters
+        ----------
+        x
+            Input instance.
+        """
+        return all(map(lambda lit: lit(x), self.literals))
+
+    @abc.abstractmethod
+    def new_nominal_splitter(self):
+        """Define the attribute observer used when dealing with nominal features.
+
+        Must be defined by classification and regression decision rule-based algorithms to match
+        the target type.
+        """
+        pass
+
+    def _iter_features(self, x: dict) -> typing.Iterable:
+        """Determine how the input instance is looped through when updating the splitters.
+
+        Parameters
+        ----------
+        x
+            The input instance.
+        """
+        for att_id, att_val in x.items():
+            yield att_id, att_val
+
+    @abc.abstractmethod
+    def _update_target_stats(self, y, w):
+        pass
+
+    def _update_feature_stats(self, feat_name, feat_val, w):
+        # By default, no feature stats are kept
+        pass
+
+    def update(self, x, y, w):
+        self._total_weight += w
+        self._update_target_stats(y, w)
+
+        for feat_name, feat_val in self._iter_features(x):
+            try:
+                splt = self.splitters[feat_name]
+            except KeyError:
+                if isinstance(feat_val, numbers.Number):
+                    self.splitters[feat_name] = copy.deepcopy(self.template_splitter)
+                else:
+                    self.nominal_features.add(feat_name)
+                    self.splitters[feat_name] = self.new_nominal_splitter()
+                splt = self.splitters[feat_name]
+            splt.update(feat_val, y, w)
+            self._update_feature_stats(feat_name, feat_val, w)
+
+    def __repr__(self):
+        return f"{' and '.join([lit.describe() for lit in self.literals])}"
```

### Comparing `river-0.8.0/river/stats/__init__.py` & `river-0.9.0/river/stats/__init__.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,66 +1,68 @@
-"""Running statistics"""
-from .auto_corr import AutoCorr
-from .base import Bivariate, Univariate
-from .count import Count
-from .cov import Cov, RollingCov
-from .entropy import Entropy
-from .ewmean import EWMean
-from .ewvar import EWVar
-from .iqr import IQR, RollingIQR
-from .kurtosis import Kurtosis
-from .link import Link
-from .maximum import AbsMax, Max, RollingAbsMax, RollingMax
-from .mean import BayesianMean, Mean, RollingMean
-from .minimum import Min, RollingMin
-from .mode import Mode, RollingMode
-from .n_unique import NUnique
-from .pearson import PearsonCorr, RollingPearsonCorr
-from .ptp import PeakToPeak, RollingPeakToPeak
-from .quantile import Quantile, RollingQuantile
-from .sem import SEM, RollingSEM
-from .shift import Shift
-from .skew import Skew
-from .summing import RollingSum, Sum
-from .var import RollingVar, Var
-
-__all__ = [
-    "AbsMax",
-    "AutoCorr",
-    "BayesianMean",
-    "Bivariate",
-    "Count",
-    "Cov",
-    "Entropy",
-    "EWMean",
-    "EWVar",
-    "IQR",
-    "Kurtosis",
-    "Link",
-    "Max",
-    "Mean",
-    "Min",
-    "Mode",
-    "NUnique",
-    "PeakToPeak",
-    "PearsonCorr",
-    "Quantile",
-    "RollingAbsMax",
-    "RollingCov",
-    "RollingIQR",
-    "RollingMax",
-    "RollingMean",
-    "RollingMin",
-    "RollingMode",
-    "RollingPeakToPeak",
-    "RollingPearsonCorr",
-    "RollingQuantile",
-    "RollingSEM",
-    "RollingSum",
-    "RollingVar",
-    "SEM",
-    "Shift",
-    "Skew",
-    "Sum",
-    "Univariate",
-    "Var",
-]
+"""Running statistics"""
+from .auto_corr import AutoCorr
+from .base import Bivariate, Univariate
+from .count import Count
+from .cov import Cov, RollingCov
+from .entropy import Entropy
+from .ewmean import EWMean
+from .ewvar import EWVar
+from .iqr import IQR, RollingIQR
+from .kurtosis import Kurtosis
+from .link import Link
+from .mad import MAD
+from .maximum import AbsMax, Max, RollingAbsMax, RollingMax
+from .mean import BayesianMean, Mean, RollingMean
+from .minimum import Min, RollingMin
+from .mode import Mode, RollingMode
+from .n_unique import NUnique
+from .pearson import PearsonCorr, RollingPearsonCorr
+from .ptp import PeakToPeak, RollingPeakToPeak
+from .quantile import Quantile, RollingQuantile
+from .sem import SEM, RollingSEM
+from .shift import Shift
+from .skew import Skew
+from .summing import RollingSum, Sum
+from .var import RollingVar, Var
+
+__all__ = [
+    "AbsMax",
+    "AutoCorr",
+    "BayesianMean",
+    "Bivariate",
+    "Count",
+    "Cov",
+    "Entropy",
+    "EWMean",
+    "EWVar",
+    "IQR",
+    "Kurtosis",
+    "Link",
+    "MAD",
+    "Max",
+    "Mean",
+    "Min",
+    "Mode",
+    "NUnique",
+    "PeakToPeak",
+    "PearsonCorr",
+    "Quantile",
+    "RollingAbsMax",
+    "RollingCov",
+    "RollingIQR",
+    "RollingMax",
+    "RollingMean",
+    "RollingMin",
+    "RollingMode",
+    "RollingPeakToPeak",
+    "RollingPearsonCorr",
+    "RollingQuantile",
+    "RollingSEM",
+    "RollingSum",
+    "RollingVar",
+    "SEM",
+    "Shift",
+    "Skew",
+    "Sum",
+    "Univariate",
+    "Var",
+]
```

### Comparing `river-0.8.0/river/stats/auto_corr.py` & `river-0.9.0/river/stats/minimum.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,71 +1,71 @@
-import collections
-import typing
-
-from . import base, pearson
-
-
-class AutoCorr(base.Univariate):
-    """Measures the serial correlation.
-
-    This method computes the Pearson correlation between the current value and the value seen `n`
-    steps before.
-
-    Parameters
-    ----------
-    lag
-
-    Examples
-    --------
-
-    The following examples are taken from the [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.autocorr.html).
-
-    >>> from river import stats
-
-    >>> auto_corr = stats.AutoCorr(lag=1)
-    >>> for x in [0.25, 0.5, 0.2, -0.05]:
-    ...     print(auto_corr.update(x).get())
-    0
-    0
-    -1.0
-    0.103552
-
-    >>> auto_corr = stats.AutoCorr(lag=2)
-    >>> for x in [0.25, 0.5, 0.2, -0.05]:
-    ...     print(auto_corr.update(x).get())
-    0
-    0
-    0
-    -1.0
-
-    >>> auto_corr = stats.AutoCorr(lag=1)
-    >>> for x in [1, 0, 0, 0]:
-    ...     print(auto_corr.update(x).get())
-    0
-    0
-    0
-    0
-
-    """
-
-    def __init__(self, lag: int):
-        self.window: typing.Deque[float] = collections.deque(maxlen=lag)
-        self.lag = lag
-        self.pearson = pearson.PearsonCorr(ddof=1)
-
-    @property
-    def name(self):
-        return f"autocorr_{self.lag}"
-
-    def update(self, x):
-
-        # The correlation can be update once enough elements have been seen
-        if len(self.window) == self.lag:
-            self.pearson.update(x, self.window[0])
-
-        # Add x to the window
-        self.window.append(x)
-
-        return self
-
-    def get(self):
-        return self.pearson.get()
+import math
+
+from river import utils
+
+from . import base
+
+
+class Min(base.Univariate):
+    """Running min.
+
+    Attributes
+    ----------
+    min : float
+        The current min.
+
+    """
+
+    def __init__(self):
+        self.min = math.inf
+
+    def update(self, x):
+        if x < self.min:
+            self.min = x
+        return self
+
+    def get(self):
+        return self.min
+
+
+class RollingMin(base.RollingUnivariate, utils.SortedWindow):
+    """Running min over a window.
+
+    Parameters
+    ----------
+    window_size
+        Size of the rolling window.
+
+    Examples
+    --------
+
+    >>> from river import stats
+
+    >>> X = [1, -4, 3, -2, 2, 1]
+    >>> rolling_min = stats.RollingMin(2)
+    >>> for x in X:
+    ...     print(rolling_min.update(x).get())
+    1
+    -4
+    -4
+    -2
+    -2
+    1
+
+    """
+
+    def __init__(self, window_size: int):
+        super().__init__(size=window_size)
+
+    @property
+    def window_size(self):
+        return self.size
+
+    def update(self, x):
+        self.append(x)
+        return self
+
+    def get(self):
+        try:
+            return self[0]
+        except IndexError:
+            return None
```

### Comparing `river-0.8.0/river/stats/cov.py` & `river-0.9.0/river/stats/cov.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,206 +1,209 @@
-import copy
-
-from . import base, mean, summing
-
-
-class Cov(base.Bivariate):
-    """Covariance.
-
-    Parameters
-    ----------
-    ddof
-        Delta Degrees of Freedom.
-
-    Examples
-    --------
-
-    >>> from river import stats
-
-    >>> x = [-2.1,  -1,  4.3]
-    >>> y = [   3, 1.1, 0.12]
-
-    >>> cov = stats.Cov()
-
-    >>> for xi, yi in zip(x, y):
-    ...     print(cov.update(xi, yi).get())
-    0.0
-    -1.044999
-    -4.286
-
-    Notes
-    -----
-    The outcomes of the incremental and parallel updates are consistent with numpy's
-    batch processing when $\\text{ddof} \\le 1$.
-
-    References
-    ----------
-    [^1]: [Wikipedia article on algorithms for calculating variance](https://www.wikiwand.com/en/Algorithms_for_calculating_variance#/Covariance)
-    [^2]: Schubert, E. and Gertz, M., 2018, July. Numerically stable parallel computation of
-    (co-) variance. In Proceedings of the 30th International Conference on Scientific and
-    Statistical Database Management (pp. 1-12).
-
-    """
-
-    def __init__(self, ddof=1):
-        self.ddof = ddof
-        self.mean_x = mean.Mean()
-        self.mean_y = mean.Mean()
-        self.cov = 0
-
-    def update(self, x, y, w=1.0):
-        dx = x - self.mean_x.get()
-        self.mean_x.update(x, w)
-        self.mean_y.update(y, w)
-        dy = y - self.mean_y.get()
-        self.cov += w * (dx * dy - self.cov) / max(1, self.mean_x.n - self.ddof)
-        return self
-
-    def get(self):
-        return self.cov
-
-    def __iadd__(self, other):
-        old_mean_x = self.mean_x.get()
-        old_mean_y = self.mean_y.get()
-        old_n = self.mean_x.n
-
-        # Update mean estimates
-        self.mean_x += other.mean_x
-        self.mean_y += other.mean_y
-
-        if self.mean_x.n <= self.ddof:
-            return self
-
-        # Scale factors
-        scale_a = old_n - self.ddof
-        scale_b = other.mean_x.n - other.ddof
-
-        # Scale the covariances
-        self.cov = scale_a * self.cov + scale_b * other.cov
-        # Apply correction factor
-        self.cov += (
-            (old_mean_x - other.mean_x.get())
-            * (old_mean_y - other.mean_y.get())
-            * ((old_n * other.mean_x.n) / self.mean_x.n)
-        )
-        # Reapply scale
-        self.cov /= self.mean_x.n - self.ddof
-
-        return self
-
-    def __add__(self, other):
-        result = copy.deepcopy(self)
-        result += other
-
-        return result
-
-    def __isub__(self, other):
-        if self.mean_x.n <= self.ddof:
-            return self
-
-        old_n = self.mean_x.n
-
-        # Update mean estimates
-        self.mean_x -= other.mean_x
-        self.mean_y -= other.mean_y
-
-        if self.mean_x.n <= self.ddof:
-            self.cov = 0
-            return self
-
-        # Scale factors
-        scale_x = old_n - self.ddof
-        scale_b = other.mean_x.n - other.ddof
-
-        # Scale the covariances
-        self.cov = scale_x * self.cov - scale_b * other.cov
-        # Apply correction
-        self.cov -= (
-            (self.mean_x.get() - other.mean_x.get())
-            * (self.mean_y.get() - other.mean_y.get())
-            * ((self.mean_x.n * other.mean_x.n) / old_n)
-        )
-        # Re-apply scale factor
-        self.cov /= self.mean_x.n - self.ddof
-
-        return self
-
-    def __sub__(self, other):
-        result = copy.deepcopy(self)
-        result -= other
-
-        return result
-
-
-class RollingCov(base.Bivariate):
-    """Rolling covariance.
-
-    Parameters
-    ----------
-    window_size
-        Size of the window over which to compute the covariance.
-    ddof
-        Delta Degrees of Freedom.
-
-    Here is the derivation, where $C$ denotes the covariance and $d$ is the amount of degrees of
-    freedom:
-
-    $$C = \\frac{1}{n - d} \\sum_{i=1}^n (x_i - \\bar{x}) (y_i - \\bar{y})$$
-
-    $$C = \\frac{1}{n - d} \\sum_{i=1}^n x_i y_i - x_i \\bar{y} - \\bar{x} y_i + \\bar{x} \\bar{y}$$
-
-    $$C = \\frac{1}{n - d} (\\sum_{i=1}^n x_i y_i - \\bar{y} \\sum_{i=1}^n x_i - \\bar{x} \\sum_{i=1}^n y_i + \\sum_{i=1}^n \\bar{x}\\bar{y})$$
-
-    $$C = \\frac{1}{n - d} (\\sum_{i=1}^n x_i y_i - \\bar{y} n \\bar{x} - \\bar{x} n \\bar{y} + n \\bar{x}\\bar{y})$$
-
-    $$C = \\frac{1}{n - d} (\\sum_{i=1}^n x_i y_i - n \\bar{x} \\bar{y})$$
-
-    $$C = \\frac{1}{n - d} (\\sum_{i=1}^n x_i y_i - \\frac{\\sum_{i=1}^n x_i \\sum_{i=1}^n y_i}{n})$$
-
-    The derivation is straightforward and somewhat trivial, but is a nice example of reformulating
-    an equation so that it can be updated online. Note that we cannot apply this derivation to the
-    non-rolling version of covariance because that would result in sums that grow infinitely, which
-    can potentially cause numeric overflow.
-
-    Examples
-    --------
-
-    >>> from river import stats
-
-    >>> x = [-2.1,  -1, 4.3, 1, -2.1,  -1, 4.3]
-    >>> y = [   3, 1.1, .12, 1,    3, 1.1, .12]
-
-    >>> rcov = stats.RollingCov(3)
-
-    >>> for xi, yi in zip(x, y):
-    ...     print(rcov.update(xi, yi).get())
-    0.0
-    -1.045
-    -4.286
-    -1.382
-    -4.589
-    -1.415
-    -4.286
-
-    """
-
-    def __init__(self, window_size, ddof=1):
-        self.ddof = ddof
-        self.sx = summing.RollingSum(window_size)
-        self.sy = summing.RollingSum(window_size)
-        self.sxy = summing.RollingSum(window_size)
-
-    @property
-    def window_size(self):
-        return self.sxy.window_size
-
-    def update(self, x, y):
-        self.sx.update(x)
-        self.sy.update(y)
-        self.sxy.update(x * y)
-        return self
-
-    def get(self):
-        n = len(self.sx)  # current window size
-        return (self.sxy.get() - self.sx.get() * self.sy.get() / n) / max(
-            1, n - self.ddof
-        )
+import copy
+
+from . import base, mean, summing
+
+
+class Cov(base.Bivariate):
+    """Covariance.
+
+    Parameters
+    ----------
+    ddof
+        Delta Degrees of Freedom.
+
+    Examples
+    --------
+
+    >>> from river import stats
+
+    >>> x = [-2.1,  -1,  4.3]
+    >>> y = [   3, 1.1, 0.12]
+
+    >>> cov = stats.Cov()
+
+    >>> for xi, yi in zip(x, y):
+    ...     print(cov.update(xi, yi).get())
+    0.0
+    -1.044999
+    -4.286
+
+    Notes
+    -----
+    The outcomes of the incremental and parallel updates are consistent with numpy's
+    batch processing when $\\text{ddof} \\le 1$.
+
+    References
+    ----------
+    [^1]: [Wikipedia article on algorithms for calculating variance](https://www.wikiwand.com/en/Algorithms_for_calculating_variance#/Covariance)
+    [^2]: Schubert, E. and Gertz, M., 2018, July. Numerically stable parallel computation of
+    (co-) variance. In Proceedings of the 30th International Conference on Scientific and
+    Statistical Database Management (pp. 1-12).
+
+    """
+
+    def __init__(self, ddof=1):
+        self.ddof = ddof
+        self.mean_x = mean.Mean()
+        self.mean_y = mean.Mean()
+        self.cov = 0
+
+    def update(self, x, y, w=1.0):
+        dx = x - self.mean_x.get()
+        self.mean_x.update(x, w)
+        self.mean_y.update(y, w)
+        dy = y - self.mean_y.get()
+        self.cov += w * (dx * dy - self.cov) / max(1, self.mean_x.n - self.ddof)
+        return self
+
+    def get(self):
+        return self.cov
+
+    def __iadd__(self, other):
+        old_mean_x = self.mean_x.get()
+        old_mean_y = self.mean_y.get()
+        old_n = self.mean_x.n
+
+        # Update mean estimates
+        self.mean_x += other.mean_x
+        self.mean_y += other.mean_y
+
+        if self.mean_x.n <= self.ddof:
+            return self
+
+        # Scale factors
+        scale_a = old_n - self.ddof
+        scale_b = other.mean_x.n - other.ddof
+
+        # Scale the covariances
+        self.cov = scale_a * self.cov + scale_b * other.cov
+        # Apply correction factor
+        self.cov += (
+            (old_mean_x - other.mean_x.get())
+            * (old_mean_y - other.mean_y.get())
+            * ((old_n * other.mean_x.n) / self.mean_x.n)
+        )
+        # Reapply scale
+        self.cov /= self.mean_x.n - self.ddof
+
+        return self
+
+    def __add__(self, other):
+        result = copy.deepcopy(self)
+        result += other
+
+        return result
+
+    def __isub__(self, other):
+        if self.mean_x.n <= self.ddof:
+            return self
+
+        old_n = self.mean_x.n
+
+        # Update mean estimates
+        self.mean_x -= other.mean_x
+        self.mean_y -= other.mean_y
+
+        if self.mean_x.n <= self.ddof:
+            self.cov = 0
+            return self
+
+        # Scale factors
+        scale_x = old_n - self.ddof
+        scale_b = other.mean_x.n - other.ddof
+
+        # Scale the covariances
+        self.cov = scale_x * self.cov - scale_b * other.cov
+        # Apply correction
+        self.cov -= (
+            (self.mean_x.get() - other.mean_x.get())
+            * (self.mean_y.get() - other.mean_y.get())
+            * ((self.mean_x.n * other.mean_x.n) / old_n)
+        )
+        # Re-apply scale factor
+        self.cov /= self.mean_x.n - self.ddof
+
+        return self
+
+    def __sub__(self, other):
+        result = copy.deepcopy(self)
+        result -= other
+
+        return result
+
+
+class RollingCov(base.Bivariate):
+    """Rolling covariance.
+
+    Parameters
+    ----------
+    window_size
+        Size of the window over which to compute the covariance.
+    ddof
+        Delta Degrees of Freedom.
+
+    Here is the derivation, where $C$ denotes the covariance and $d$ is the amount of degrees of
+    freedom:
+
+    $$C = \\frac{1}{n - d} \\sum_{i=1}^n (x_i - \\bar{x}) (y_i - \\bar{y})$$
+
+    $$C = \\frac{1}{n - d} \\sum_{i=1}^n x_i y_i - x_i \\bar{y} - \\bar{x} y_i + \\bar{x} \\bar{y}$$
+
+    $$C = \\frac{1}{n - d} (\\sum_{i=1}^n x_i y_i - \\bar{y} \\sum_{i=1}^n x_i - \\bar{x} \\sum_{i=1}^n y_i + \\sum_{i=1}^n \\bar{x}\\bar{y})$$
+
+    $$C = \\frac{1}{n - d} (\\sum_{i=1}^n x_i y_i - \\bar{y} n \\bar{x} - \\bar{x} n \\bar{y} + n \\bar{x}\\bar{y})$$
+
+    $$C = \\frac{1}{n - d} (\\sum_{i=1}^n x_i y_i - n \\bar{x} \\bar{y})$$
+
+    $$C = \\frac{1}{n - d} (\\sum_{i=1}^n x_i y_i - \\frac{\\sum_{i=1}^n x_i \\sum_{i=1}^n y_i}{n})$$
+
+    The derivation is straightforward and somewhat trivial, but is a nice example of reformulating
+    an equation so that it can be updated online. Note that we cannot apply this derivation to the
+    non-rolling version of covariance because that would result in sums that grow infinitely, which
+    can potentially cause numeric overflow.
+
+    Examples
+    --------
+
+    >>> from river import stats
+
+    >>> x = [-2.1,  -1, 4.3, 1, -2.1,  -1, 4.3]
+    >>> y = [   3, 1.1, .12, 1,    3, 1.1, .12]
+
+    >>> rcov = stats.RollingCov(3)
+
+    >>> for xi, yi in zip(x, y):
+    ...     print(rcov.update(xi, yi).get())
+    0.0
+    -1.045
+    -4.286
+    -1.382
+    -4.589
+    -1.415
+    -4.286
+
+    """
+
+    def __init__(self, window_size, ddof=1):
+        self.ddof = ddof
+        self.sx = summing.RollingSum(window_size)
+        self.sy = summing.RollingSum(window_size)
+        self.sxy = summing.RollingSum(window_size)
+
+    @property
+    def window_size(self):
+        return self.sxy.window_size
+
+    def update(self, x, y):
+        self.sx.update(x)
+        self.sy.update(y)
+        self.sxy.update(x * y)
+        return self
+
+    def get(self):
+        n = len(self.sx)  # current window size
+        try:
+            return (self.sxy.get() - self.sx.get() * self.sy.get() / n) / max(
+                1, n - self.ddof
+            )
+        except ZeroDivisionError:
+            return None
```

### Comparing `river-0.8.0/river/stats/entropy.py` & `river-0.9.0/river/stats/entropy.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,99 +1,99 @@
-import collections
-import math
-
-from . import base
-
-
-class Entropy(base.Univariate):
-    """Running entropy.
-
-    Parameters
-    ----------
-    alpha
-        Fading factor.
-    eps
-        Small value that will be added to the denominator to avoid division by zero.
-
-    Attributes
-    ----------
-    entropy : float
-        The running entropy.
-    n : int
-        The current number of observations.
-    counter : collections.Counter
-        Count the number of times the values have occurred
-
-    Examples
-    --------
-
-    >>> import math
-    >>> import random
-    >>> import numpy as np
-    >>> from scipy.stats import entropy
-    >>> from river import stats
-
-    >>> def entropy_list(labels, base=None):
-    ...   value,counts = np.unique(labels, return_counts=True)
-    ...   return entropy(counts, base=base)
-
-    >>> SEED = 42 * 1337
-    >>> random.seed(SEED)
-
-    >>> entro = stats.Entropy(alpha=1)
-
-    >>> list_animal = []
-    >>> for animal, num_val in zip(['cat', 'dog', 'bird'],[301, 401, 601]):
-    ...     list_animal += [animal for i in range(num_val)]
-    >>> random.shuffle(list_animal)
-
-    >>> for animal in list_animal:
-    ...     _ = entro.update(animal)
-
-    >>> print(f'{entro.get():.6f}')
-    1.058093
-    >>> print(f'{entropy_list(list_animal):.6f}')
-    1.058093
-
-    References
-    ----------
-    [^1]: [Sovdat, B., 2014. Updating Formulas and Algorithms for Computing Entropy and Gini Index from Time-Changing Data Streams. arXiv preprint arXiv:1403.6348.](https://arxiv.org/pdf/1403.6348.pdf)
-
-    """
-
-    def __init__(self, alpha=1, eps=1e-8):
-
-        if 0 < alpha <= 1:
-            self.alpha = alpha
-        else:
-            raise ValueError("alpha must be between 0 excluded and 1")
-        self.eps = eps
-        self.entropy = 0
-        self.n = 0
-        self.counter = collections.Counter()
-
-    @property
-    def name(self):
-        return "entropy"
-
-    def update(self, x):
-
-        cx = self.counter.get(x, 0)
-        n = self.n
-        eps = self.eps
-        alpha = self.alpha
-
-        entropy = self.entropy
-        entropy = (
-            (n + eps) / (n + 1) * (alpha * entropy - math.log((n + eps) / (n + 1)))
-        )
-        entropy -= (cx + 1) / (n + 1) * math.log((cx + 1) / (n + 1))
-        entropy += (cx + eps) / (n + 1) * math.log((cx + eps) / (n + 1))
-        self.entropy = entropy
-
-        self.n += 1
-        self.counter.update([x])
-
-        return self
-
-    def get(self):
-        return self.entropy
+import collections
+import math
+
+from . import base
+
+
+class Entropy(base.Univariate):
+    """Running entropy.
+
+    Parameters
+    ----------
+    alpha
+        Fading factor.
+    eps
+        Small value that will be added to the denominator to avoid division by zero.
+
+    Attributes
+    ----------
+    entropy : float
+        The running entropy.
+    n : int
+        The current number of observations.
+    counter : collections.Counter
+        Count the number of times the values have occurred
+
+    Examples
+    --------
+
+    >>> import math
+    >>> import random
+    >>> import numpy as np
+    >>> from scipy.stats import entropy
+    >>> from river import stats
+
+    >>> def entropy_list(labels, base=None):
+    ...   value,counts = np.unique(labels, return_counts=True)
+    ...   return entropy(counts, base=base)
+
+    >>> SEED = 42 * 1337
+    >>> random.seed(SEED)
+
+    >>> entro = stats.Entropy(alpha=1)
+
+    >>> list_animal = []
+    >>> for animal, num_val in zip(['cat', 'dog', 'bird'],[301, 401, 601]):
+    ...     list_animal += [animal for i in range(num_val)]
+    >>> random.shuffle(list_animal)
+
+    >>> for animal in list_animal:
+    ...     _ = entro.update(animal)
+
+    >>> print(f'{entro.get():.6f}')
+    1.058093
+    >>> print(f'{entropy_list(list_animal):.6f}')
+    1.058093
+
+    References
+    ----------
+    [^1]: [Sovdat, B., 2014. Updating Formulas and Algorithms for Computing Entropy and Gini Index from Time-Changing Data Streams. arXiv preprint arXiv:1403.6348.](https://arxiv.org/pdf/1403.6348.pdf)
+
+    """
+
+    def __init__(self, alpha=1, eps=1e-8):
+
+        if 0 < alpha <= 1:
+            self.alpha = alpha
+        else:
+            raise ValueError("alpha must be between 0 excluded and 1")
+        self.eps = eps
+        self.entropy = 0
+        self.n = 0
+        self.counter = collections.Counter()
+
+    @property
+    def name(self):
+        return "entropy"
+
+    def update(self, x):
+
+        cx = self.counter.get(x, 0)
+        n = self.n
+        eps = self.eps
+        alpha = self.alpha
+
+        entropy = self.entropy
+        entropy = (
+            (n + eps) / (n + 1) * (alpha * entropy - math.log((n + eps) / (n + 1)))
+        )
+        entropy -= (cx + 1) / (n + 1) * math.log((cx + 1) / (n + 1))
+        entropy += (cx + eps) / (n + 1) * math.log((cx + eps) / (n + 1))
+        self.entropy = entropy
+
+        self.n += 1
+        self.counter.update([x])
+
+        return self
+
+    def get(self):
+        return self.entropy
```

### Comparing `river-0.8.0/river/stats/ewmean.py` & `river-0.9.0/river/stats/ewmean.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,56 +1,56 @@
-from . import base
-
-
-class EWMean(base.Univariate):
-    """Exponentially weighted mean.
-
-    Parameters
-    ----------
-    alpha
-        The closer `alpha` is to 1 the more the statistic will adapt to recent values.
-
-    Attributes
-    ----------
-    mean : float
-        The running exponentially weighted mean.
-
-    Examples
-    --------
-
-    >>> from river import stats
-
-    >>> X = [1, 3, 5, 4, 6, 8, 7, 9, 11]
-    >>> ewm = stats.EWMean(alpha=0.5)
-    >>> for x in X:
-    ...     print(ewm.update(x).get())
-    1
-    2.0
-    3.5
-    3.75
-    4.875
-    6.4375
-    6.71875
-    7.859375
-    9.4296875
-
-    References
-    ----------
-    [^1]: [Finch, T., 2009. Incremental calculation of weighted mean and variance. University of Cambridge, 4(11-5), pp.41-42.](https://fanf2.user.srcf.net/hermes/doc/antiforgery/stats.pdf)
-    [^2]: [Exponential Moving Average on Streaming Data](https://dev.to/nestedsoftware/exponential-moving-average-on-streaming-data-4hhl)
-
-    """
-
-    def __init__(self, alpha=0.5):
-        self.alpha = alpha
-        self.mean = 0
-
-    @property
-    def name(self):
-        return f"ewm_{self.alpha}"
-
-    def update(self, x):
-        self.mean = self.alpha * x + (1.0 - self.alpha) * self.mean if self.mean else x
-        return self
-
-    def get(self):
-        return self.mean
+from . import base
+
+
+class EWMean(base.Univariate):
+    """Exponentially weighted mean.
+
+    Parameters
+    ----------
+    alpha
+        The closer `alpha` is to 1 the more the statistic will adapt to recent values.
+
+    Attributes
+    ----------
+    mean : float
+        The running exponentially weighted mean.
+
+    Examples
+    --------
+
+    >>> from river import stats
+
+    >>> X = [1, 3, 5, 4, 6, 8, 7, 9, 11]
+    >>> ewm = stats.EWMean(alpha=0.5)
+    >>> for x in X:
+    ...     print(ewm.update(x).get())
+    1
+    2.0
+    3.5
+    3.75
+    4.875
+    6.4375
+    6.71875
+    7.859375
+    9.4296875
+
+    References
+    ----------
+    [^1]: [Finch, T., 2009. Incremental calculation of weighted mean and variance. University of Cambridge, 4(11-5), pp.41-42.](https://fanf2.user.srcf.net/hermes/doc/antiforgery/stats.pdf)
+    [^2]: [Exponential Moving Average on Streaming Data](https://dev.to/nestedsoftware/exponential-moving-average-on-streaming-data-4hhl)
+
+    """
+
+    def __init__(self, alpha=0.5):
+        self.alpha = alpha
+        self.mean = 0
+
+    @property
+    def name(self):
+        return f"ewm_{self.alpha}"
+
+    def update(self, x):
+        self.mean = self.alpha * x + (1.0 - self.alpha) * self.mean if self.mean else x
+        return self
+
+    def get(self):
+        return self.mean
```

### Comparing `river-0.8.0/river/stats/ewvar.py` & `river-0.9.0/river/stats/ewvar.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,61 +1,61 @@
-from . import base, ewmean
-
-
-class EWVar(base.Univariate):
-    """Exponentially weighted variance.
-
-    To calculate the variance we use the fact that Var(X) = Mean(x^2) - Mean(x)^2 and internally
-    we use the exponentially weighted mean of x/x^2 to calculate this.
-
-    Parameters
-    ----------
-    alpha
-        The closer `alpha` is to 1 the more the statistic will adapt to recent values.
-
-    Attributes
-    ----------
-    variance : float
-        The running exponentially weighted variance.
-
-    Examples
-    --------
-
-    >>> from river import stats
-
-    >>> X = [1, 3, 5, 4, 6, 8, 7, 9, 11]
-    >>> ewv = stats.EWVar(alpha=0.5)
-    >>> for x in X:
-    ...     print(ewv.update(x).get())
-    0
-    1.0
-    2.75
-    1.4375
-    1.984375
-    3.43359375
-    1.7958984375
-    2.198974609375
-    3.56536865234375
-
-    References
-    ----------
-    [^1]: [Finch, T., 2009. Incremental calculation of weighted mean and variance. University of Cambridge, 4(11-5), pp.41-42.](https://fanf2.user.srcf.net/hermes/doc/antiforgery/stats.pdf)
-    [^2]: [Exponential Moving Average on Streaming Data](https://dev.to/nestedsoftware/exponential-moving-average-on-streaming-data-4hhl)
-
-    """
-
-    def __init__(self, alpha=0.5):
-        self.alpha = alpha
-        self.mean = ewmean.EWMean(alpha=alpha)
-        self.sq_mean = ewmean.EWMean(alpha=alpha)
-
-    @property
-    def name(self):
-        return f"ewv_{self.alpha}"
-
-    def update(self, x):
-        self.mean.update(x)
-        self.sq_mean.update(x ** 2)
-        return self
-
-    def get(self):
-        return self.sq_mean.get() - self.mean.get() ** 2
+from . import base, ewmean
+
+
+class EWVar(base.Univariate):
+    """Exponentially weighted variance.
+
+    To calculate the variance we use the fact that Var(X) = Mean(x^2) - Mean(x)^2 and internally
+    we use the exponentially weighted mean of x/x^2 to calculate this.
+
+    Parameters
+    ----------
+    alpha
+        The closer `alpha` is to 1 the more the statistic will adapt to recent values.
+
+    Attributes
+    ----------
+    variance : float
+        The running exponentially weighted variance.
+
+    Examples
+    --------
+
+    >>> from river import stats
+
+    >>> X = [1, 3, 5, 4, 6, 8, 7, 9, 11]
+    >>> ewv = stats.EWVar(alpha=0.5)
+    >>> for x in X:
+    ...     print(ewv.update(x).get())
+    0
+    1.0
+    2.75
+    1.4375
+    1.984375
+    3.43359375
+    1.7958984375
+    2.198974609375
+    3.56536865234375
+
+    References
+    ----------
+    [^1]: [Finch, T., 2009. Incremental calculation of weighted mean and variance. University of Cambridge, 4(11-5), pp.41-42.](https://fanf2.user.srcf.net/hermes/doc/antiforgery/stats.pdf)
+    [^2]: [Exponential Moving Average on Streaming Data](https://dev.to/nestedsoftware/exponential-moving-average-on-streaming-data-4hhl)
+
+    """
+
+    def __init__(self, alpha=0.5):
+        self.alpha = alpha
+        self.mean = ewmean.EWMean(alpha=alpha)
+        self.sq_mean = ewmean.EWMean(alpha=alpha)
+
+    @property
+    def name(self):
+        return f"ewv_{self.alpha}"
+
+    def update(self, x):
+        self.mean.update(x)
+        self.sq_mean.update(x ** 2)
+        return self
+
+    def get(self):
+        return self.sq_mean.get() - self.mean.get() ** 2
```

### Comparing `river-0.8.0/river/stats/link.py` & `river-0.9.0/river/stats/link.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,84 +1,84 @@
-from river import stats
-
-
-class Link(stats.Univariate):
-    """A link joins two univariate statistics as a sequence.
-
-    This can be used to pipe the output of one statistic to the input of another. This can be used,
-    for instance, to calculate the mean of the variance of a variable. It can also be used to
-    compute shifted statistics by piping statistics with an instance of `stats.Shift`.
-
-    Note that a link is not meant to be instantiated via this class definition. Instead, users can
-    link statistics together via the `|` operator.
-
-    Parameters
-    ----------
-    left
-    right
-        The output from `left`'s `get` method is passed to `right`'s `update` method if `left`'s
-        `get` method doesn't produce `None.`
-
-    Examples
-    --------
-
-    >>> from river import stats
-    >>> stat = stats.Shift(1) | stats.Mean()
-
-    No values have been seen, therefore `get` defaults to the initial value of `stats.Mean`,
-    which is 0.
-
-    >>> stat.get()
-    0.
-
-    Let us now call `update`.
-
-    >>> stat = stat.update(1)
-
-    The output from `get` will still be 0. The reason is that `stats.Shift` has not enough
-    values, and therefore outputs it's default value, which is `None`. The `stats.Mean`
-    instance is therefore not updated.
-
-    >>> stat.get()
-    0.0
-
-    On the next call to `update`, the `stats.Shift` instance has seen enough values, and
-    therefore the mean can be updated. The mean is therefore equal to 1, because that's the
-    only value from the past.
-
-    >>> stat = stat.update(3)
-    >>> stat.get()
-    1.0
-
-    On the subsequent call to update, the mean will be updated with the value 3.
-
-    >>> stat = stat.update(4)
-    >>> stat.get()
-    2.0
-
-    Note that composing statistics returns a new statistic with it's own name.
-
-    >>> stat.name
-    'mean_of_shift_1'
-
-    """
-
-    def __init__(self, left: stats.Univariate, right: stats.Univariate):
-        self.left = left
-        self.right = right
-
-    def update(self, x):
-        self.left.update(x)
-        y = self.left.get()
-        if y is not None:
-            self.right.update(y)
-        return self
-
-    def get(self):
-        return self.right.get()
-
-    @property
-    def name(self):
-        return f"{self.right.name}_of_{self.left.name}"
-
-    def __repr__(self):
-        return repr(self.right)
+from river import stats
+
+
+class Link(stats.Univariate):
+    """A link joins two univariate statistics as a sequence.
+
+    This can be used to pipe the output of one statistic to the input of another. This can be used,
+    for instance, to calculate the mean of the variance of a variable. It can also be used to
+    compute shifted statistics by piping statistics with an instance of `stats.Shift`.
+
+    Note that a link is not meant to be instantiated via this class definition. Instead, users can
+    link statistics together via the `|` operator.
+
+    Parameters
+    ----------
+    left
+    right
+        The output from `left`'s `get` method is passed to `right`'s `update` method if `left`'s
+        `get` method doesn't produce `None.`
+
+    Examples
+    --------
+
+    >>> from river import stats
+    >>> stat = stats.Shift(1) | stats.Mean()
+
+    No values have been seen, therefore `get` defaults to the initial value of `stats.Mean`,
+    which is 0.
+
+    >>> stat.get()
+    0.
+
+    Let us now call `update`.
+
+    >>> stat = stat.update(1)
+
+    The output from `get` will still be 0. The reason is that `stats.Shift` has not enough
+    values, and therefore outputs it's default value, which is `None`. The `stats.Mean`
+    instance is therefore not updated.
+
+    >>> stat.get()
+    0.0
+
+    On the next call to `update`, the `stats.Shift` instance has seen enough values, and
+    therefore the mean can be updated. The mean is therefore equal to 1, because that's the
+    only value from the past.
+
+    >>> stat = stat.update(3)
+    >>> stat.get()
+    1.0
+
+    On the subsequent call to update, the mean will be updated with the value 3.
+
+    >>> stat = stat.update(4)
+    >>> stat.get()
+    2.0
+
+    Note that composing statistics returns a new statistic with it's own name.
+
+    >>> stat.name
+    'mean_of_shift_1'
+
+    """
+
+    def __init__(self, left: stats.Univariate, right: stats.Univariate):
+        self.left = left
+        self.right = right
+
+    def update(self, x):
+        self.left.update(x)
+        y = self.left.get()
+        if y is not None:
+            self.right.update(y)
+        return self
+
+    def get(self):
+        return self.right.get()
+
+    @property
+    def name(self):
+        return f"{self.right.name}_of_{self.left.name}"
+
+    def __repr__(self):
+        return repr(self.right)
```

### Comparing `river-0.8.0/river/stats/maximum.py` & `river-0.9.0/river/stats/maximum.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,163 +1,169 @@
-import math
-
-from river import utils
-
-from . import base
-
-
-class Max(base.Univariate):
-    """Running max.
-
-    Attributes
-    ----------
-    max : float
-        The current max.
-
-    Examples
-    --------
-
-    >>> from river import stats
-
-    >>> X = [1, -4, 3, -2, 5, -6]
-    >>> _max = stats.Max()
-    >>> for x in X:
-    ...     print(_max.update(x).get())
-    1
-    1
-    3
-    3
-    5
-    5
-
-    """
-
-    def __init__(self):
-        self.max = -math.inf
-
-    def update(self, x):
-        if x > self.max:
-            self.max = x
-        return self
-
-    def get(self):
-        return self.max
-
-
-class RollingMax(base.RollingUnivariate, utils.SortedWindow):
-    """Running max over a window.
-
-    Parameters
-    ----------
-    window_size
-        Size of the rolling window.
-
-    Examples
-    --------
-
-    >>> from river import stats
-
-    >>> X = [1, -4, 3, -2, 2, 1]
-    >>> rolling_max = stats.RollingMax(window_size=2)
-    >>> for x in X:
-    ...     print(rolling_max.update(x).get())
-    1
-    1
-    3
-    3
-    2
-    2
-
-    """
-
-    def __init__(self, window_size: int):
-        super().__init__(size=window_size)
-
-    @property
-    def window_size(self):
-        return self.size
-
-    def update(self, x):
-        self.append(x)
-        return self
-
-    def get(self):
-        return self[-1]
-
-
-class AbsMax(base.Univariate):
-    """Running absolute max.
-
-    Attributes
-    ----------
-    abs_max : float
-        The current absolute max.
-
-    Examples
-    --------
-
-    >>> from river import stats
-
-    >>> X = [1, -4, 3, -2, 5, -6]
-    >>> abs_max = stats.AbsMax()
-    >>> for x in X:
-    ...     print(abs_max.update(x).get())
-    1
-    4
-    4
-    4
-    5
-    6
-
-    """
-
-    def __init__(self):
-        self.abs_max = 0.0
-
-    def update(self, x):
-        if abs(x) > self.abs_max:
-            self.abs_max = abs(x)
-        return self
-
-    def get(self):
-        return self.abs_max
-
-
-class RollingAbsMax(base.RollingUnivariate, utils.SortedWindow):
-    """Running absolute max over a window.
-
-    Parameters
-    ----------
-    window_size
-        Size of the rolling window.
-
-    Examples
-    --------
-
-    >>> from river import stats
-
-    >>> X = [1, -4, 3, -2, 2, 1]
-    >>> rolling_absmax = stats.RollingAbsMax(window_size=2)
-    >>> for x in X:
-    ...     print(rolling_absmax.update(x).get())
-    1
-    4
-    4
-    3
-    2
-    2
-
-    """
-
-    def __init__(self, window_size: int):
-        super().__init__(size=window_size)
-
-    @property
-    def window_size(self):
-        return self.size
-
-    def update(self, x):
-        self.append(abs(x))
-        return self
-
-    def get(self):
-        return self[-1]
+import math
+
+from river import utils
+
+from . import base
+
+
+class Max(base.Univariate):
+    """Running max.
+
+    Attributes
+    ----------
+    max : float
+        The current max.
+
+    Examples
+    --------
+
+    >>> from river import stats
+
+    >>> X = [1, -4, 3, -2, 5, -6]
+    >>> _max = stats.Max()
+    >>> for x in X:
+    ...     print(_max.update(x).get())
+    1
+    1
+    3
+    3
+    5
+    5
+
+    """
+
+    def __init__(self):
+        self.max = -math.inf
+
+    def update(self, x):
+        if x > self.max:
+            self.max = x
+        return self
+
+    def get(self):
+        return self.max
+
+
+class RollingMax(base.RollingUnivariate, utils.SortedWindow):
+    """Running max over a window.
+
+    Parameters
+    ----------
+    window_size
+        Size of the rolling window.
+
+    Examples
+    --------
+
+    >>> from river import stats
+
+    >>> X = [1, -4, 3, -2, 2, 1]
+    >>> rolling_max = stats.RollingMax(window_size=2)
+    >>> for x in X:
+    ...     print(rolling_max.update(x).get())
+    1
+    1
+    3
+    3
+    2
+    2
+
+    """
+
+    def __init__(self, window_size: int):
+        super().__init__(size=window_size)
+
+    @property
+    def window_size(self):
+        return self.size
+
+    def update(self, x):
+        self.append(x)
+        return self
+
+    def get(self):
+        try:
+            return self[-1]
+        except IndexError:
+            return None
+
+
+class AbsMax(base.Univariate):
+    """Running absolute max.
+
+    Attributes
+    ----------
+    abs_max : float
+        The current absolute max.
+
+    Examples
+    --------
+
+    >>> from river import stats
+
+    >>> X = [1, -4, 3, -2, 5, -6]
+    >>> abs_max = stats.AbsMax()
+    >>> for x in X:
+    ...     print(abs_max.update(x).get())
+    1
+    4
+    4
+    4
+    5
+    6
+
+    """
+
+    def __init__(self):
+        self.abs_max = 0.0
+
+    def update(self, x):
+        if abs(x) > self.abs_max:
+            self.abs_max = abs(x)
+        return self
+
+    def get(self):
+        return self.abs_max
+
+
+class RollingAbsMax(base.RollingUnivariate, utils.SortedWindow):
+    """Running absolute max over a window.
+
+    Parameters
+    ----------
+    window_size
+        Size of the rolling window.
+
+    Examples
+    --------
+
+    >>> from river import stats
+
+    >>> X = [1, -4, 3, -2, 2, 1]
+    >>> rolling_absmax = stats.RollingAbsMax(window_size=2)
+    >>> for x in X:
+    ...     print(rolling_absmax.update(x).get())
+    1
+    4
+    4
+    3
+    2
+    2
+
+    """
+
+    def __init__(self, window_size: int):
+        super().__init__(size=window_size)
+
+    @property
+    def window_size(self):
+        return self.size
+
+    def update(self, x):
+        self.append(abs(x))
+        return self
+
+    def get(self):
+        try:
+            return self[-1]
+        except IndexError:
+            return None
```

### Comparing `river-0.8.0/river/stats/mode.py` & `river-0.9.0/river/stats/mode.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,137 +1,137 @@
-import collections
-import typing
-
-from river import utils
-
-from . import base
-
-__all__ = ["Mode"]
-
-
-class Mode(base.Univariate):
-    """Running mode.
-
-    The mode is simply the most common value. An approximate mode can be computed by setting the
-    number of first unique values to count.
-
-    Parameters
-    ----------
-    k
-        Only the first `k` unique values will be included. If `k` equals -1, the exact mode is
-        computed.
-
-    Examples
-    --------
-
-    >>> from river import stats
-
-    >>> X = ['sunny', 'cloudy', 'cloudy', 'rainy', 'rainy', 'rainy']
-    >>> mode = stats.Mode(k=2)
-    >>> for x in X:
-    ...     print(mode.update(x).get())
-    sunny
-    sunny
-    cloudy
-    cloudy
-    cloudy
-    cloudy
-
-    >>> mode = stats.Mode(k=-1)
-    >>> for x in X:
-    ...     print(mode.update(x).get())
-    sunny
-    sunny
-    cloudy
-    cloudy
-    cloudy
-    rainy
-
-    """
-
-    def __init__(self, k=25):
-        self.k = k
-        self.counts = collections.defaultdict(int)
-
-    @property
-    def name(self):
-        return "mode"
-
-    def update(self, x):
-        if self.k == -1 or x in self.counts or len(self.counts) < self.k:
-            self.counts[x] += 1
-        return self
-
-    def get(self):
-        return max(self.counts, key=self.counts.get)
-
-
-class RollingMode(base.RollingUnivariate, utils.Window):
-    """Running mode over a window.
-
-    The mode is the most common value.
-
-    Parameters
-    ----------
-    window_size
-        Size of the rolling window.
-
-    Attributes
-    ----------
-    counts : collections.defaultdict
-        Value counts.
-
-    Examples
-    --------
-
-    >>> from river import stats
-
-    >>> X = ['sunny', 'sunny', 'sunny', 'rainy', 'rainy', 'rainy', 'rainy']
-    >>> rolling_mode = stats.RollingMode(window_size=2)
-    >>> for x in X:
-    ...     print(rolling_mode.update(x).get())
-    sunny
-    sunny
-    sunny
-    sunny
-    rainy
-    rainy
-    rainy
-
-    >>> rolling_mode = stats.RollingMode(window_size=5)
-    >>> for x in X:
-    ...     print(rolling_mode.update(x).get())
-    sunny
-    sunny
-    sunny
-    sunny
-    sunny
-    rainy
-    rainy
-
-    """
-
-    def __init__(self, window_size: int):
-        super().__init__(size=window_size)
-        self.counts: typing.DefaultDict[typing.Any, int] = collections.defaultdict(int)
-
-    @property
-    def window_size(self):
-        return self.size
-
-    def update(self, x):
-        if len(self) >= self.size:
-
-            # Subtract the counter of the last element
-            first_in = self[0]
-            self.counts[first_in] -= 1
-
-            # No need to store the value if it's counter is 0
-            if self.counts[first_in] == 0:
-                self.counts.pop(first_in)
-
-        self.counts[x] += 1
-        super().append(x)
-        return self
-
-    def get(self):
-        return max(self.counts, key=self.counts.get)
+import collections
+import typing
+
+from river import utils
+
+from . import base
+
+__all__ = ["Mode"]
+
+
+class Mode(base.Univariate):
+    """Running mode.
+
+    The mode is simply the most common value. An approximate mode can be computed by setting the
+    number of first unique values to count.
+
+    Parameters
+    ----------
+    k
+        Only the first `k` unique values will be included. If `k` equals -1, the exact mode is
+        computed.
+
+    Examples
+    --------
+
+    >>> from river import stats
+
+    >>> X = ['sunny', 'cloudy', 'cloudy', 'rainy', 'rainy', 'rainy']
+    >>> mode = stats.Mode(k=2)
+    >>> for x in X:
+    ...     print(mode.update(x).get())
+    sunny
+    sunny
+    cloudy
+    cloudy
+    cloudy
+    cloudy
+
+    >>> mode = stats.Mode(k=-1)
+    >>> for x in X:
+    ...     print(mode.update(x).get())
+    sunny
+    sunny
+    cloudy
+    cloudy
+    cloudy
+    rainy
+
+    """
+
+    def __init__(self, k=25):
+        self.k = k
+        self.counts = collections.defaultdict(int)
+
+    @property
+    def name(self):
+        return "mode"
+
+    def update(self, x):
+        if self.k == -1 or x in self.counts or len(self.counts) < self.k:
+            self.counts[x] += 1
+        return self
+
+    def get(self):
+        return max(self.counts, key=self.counts.get, default=None)
+
+
+class RollingMode(base.RollingUnivariate, utils.Window):
+    """Running mode over a window.
+
+    The mode is the most common value.
+
+    Parameters
+    ----------
+    window_size
+        Size of the rolling window.
+
+    Attributes
+    ----------
+    counts : collections.defaultdict
+        Value counts.
+
+    Examples
+    --------
+
+    >>> from river import stats
+
+    >>> X = ['sunny', 'sunny', 'sunny', 'rainy', 'rainy', 'rainy', 'rainy']
+    >>> rolling_mode = stats.RollingMode(window_size=2)
+    >>> for x in X:
+    ...     print(rolling_mode.update(x).get())
+    sunny
+    sunny
+    sunny
+    sunny
+    rainy
+    rainy
+    rainy
+
+    >>> rolling_mode = stats.RollingMode(window_size=5)
+    >>> for x in X:
+    ...     print(rolling_mode.update(x).get())
+    sunny
+    sunny
+    sunny
+    sunny
+    sunny
+    rainy
+    rainy
+
+    """
+
+    def __init__(self, window_size: int):
+        super().__init__(size=window_size)
+        self.counts: typing.DefaultDict[typing.Any, int] = collections.defaultdict(int)
+
+    @property
+    def window_size(self):
+        return self.size
+
+    def update(self, x):
+        if len(self) >= self.size:
+
+            # Subtract the counter of the last element
+            first_in = self[0]
+            self.counts[first_in] -= 1
+
+            # No need to store the value if it's counter is 0
+            if self.counts[first_in] == 0:
+                self.counts.pop(first_in)
+
+        self.counts[x] += 1
+        super().append(x)
+        return self
+
+    def get(self):
+        return max(self.counts, key=self.counts.get, default=None)
```

### Comparing `river-0.8.0/river/stats/moments.py` & `river-0.9.0/river/stats/moments.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,68 +1,68 @@
-from . import base, count
-
-
-class CentralMoments(base.Univariate):
-    """Computes central moments using Welford's algorithm.
-
-    Attributes
-    ----------
-    count : stats.Count
-    delta: float
-    sum_delta : float
-        Mean of sum of differences.
-    M1 : float
-        Sums of powers of differences from the mean order 1.
-    M2 : float)
-        Sums of powers of differences from the mean order 2.
-    M3 : float
-        Sums of powers of differences from the mean order 3.
-    M4 : float
-        Sums of powers of differences from the mean order 4.
-
-    References
-    ----------
-    [^1]: [Wikipedia article on algorithms for calculating variance](https://www.wikiwand.com/en/Algorithms_for_calculating_variance#/Covariance)
-
-    """
-
-    def __init__(self):
-        self.count = count.Count()
-
-        self.delta = 0
-        self.sum_delta = 0
-
-        self.M1 = 0
-        self.M2 = 0
-        self.M3 = 0
-        self.M4 = 0
-
-    def _update_delta(self, x):
-        self.delta = (x - self.sum_delta) / self.count.get()
-        return self
-
-    def _update_sum_delta(self):
-        self.sum_delta += self.delta
-        return self
-
-    def _update_m1(self, x):
-        self.M1 = (x - self.sum_delta) * self.delta * (self.count.get() - 1)
-        return self
-
-    def _update_m2(self):
-        self.M2 += self.M1
-        return self
-
-    def _update_m3(self):
-        self.M3 += (
-            self.M1 * self.delta * (self.count.get() - 2) - 3 * self.delta * self.M2
-        )
-        return self
-
-    def _update_m4(self):
-        delta_square = self.delta ** 2
-        self.M4 += (
-            self.M1 * delta_square * (self.count.get() ** 2 - 3 * self.count.get() + 3)
-            + 6 * delta_square * self.M2
-            - 4 * self.delta * self.M3
-        )
-        return self
+from . import base, count
+
+
+class CentralMoments(base.Univariate):
+    """Computes central moments using Welford's algorithm.
+
+    Attributes
+    ----------
+    count : stats.Count
+    delta: float
+    sum_delta : float
+        Mean of sum of differences.
+    M1 : float
+        Sums of powers of differences from the mean order 1.
+    M2 : float)
+        Sums of powers of differences from the mean order 2.
+    M3 : float
+        Sums of powers of differences from the mean order 3.
+    M4 : float
+        Sums of powers of differences from the mean order 4.
+
+    References
+    ----------
+    [^1]: [Wikipedia article on algorithms for calculating variance](https://www.wikiwand.com/en/Algorithms_for_calculating_variance#/Covariance)
+
+    """
+
+    def __init__(self):
+        self.count = count.Count()
+
+        self.delta = 0
+        self.sum_delta = 0
+
+        self.M1 = 0
+        self.M2 = 0
+        self.M3 = 0
+        self.M4 = 0
+
+    def _update_delta(self, x):
+        self.delta = (x - self.sum_delta) / self.count.get()
+        return self
+
+    def _update_sum_delta(self):
+        self.sum_delta += self.delta
+        return self
+
+    def _update_m1(self, x):
+        self.M1 = (x - self.sum_delta) * self.delta * (self.count.get() - 1)
+        return self
+
+    def _update_m2(self):
+        self.M2 += self.M1
+        return self
+
+    def _update_m3(self):
+        self.M3 += (
+            self.M1 * self.delta * (self.count.get() - 2) - 3 * self.delta * self.M2
+        )
+        return self
+
+    def _update_m4(self):
+        delta_square = self.delta ** 2
+        self.M4 += (
+            self.M1 * delta_square * (self.count.get() ** 2 - 3 * self.count.get() + 3)
+            + 6 * delta_square * self.M2
+            - 4 * self.delta * self.M3
+        )
+        return self
```

### Comparing `river-0.8.0/river/stats/n_unique.py` & `river-0.9.0/river/stats/n_unique.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,107 +1,107 @@
-import hashlib
-import math
-
-import numpy as np
-
-from . import base
-
-
-class NUnique(base.Univariate):
-    """Approximate number of unique values counter.
-
-    This is basically an implementation of the HyperLogLog algorithm. Adapted from
-    [`hypy`](https://github.com/clarkduvall/hypy). The code is a bit too terse but it will do for
-    now.
-
-    Parameters
-    ----------
-    error_rate
-        Desired error rate. Memory usage is inversely proportional to this value.
-    seed
-        Set the seed to produce identical results.
-
-    Attributes
-    ----------
-    n_bits : int
-    n_buckets : int
-    buckets : list
-
-    Examples
-    --------
-
-    >>> import string
-    >>> from river import stats
-
-    >>> alphabet = string.ascii_lowercase
-    >>> n_unique = stats.NUnique(error_rate=0.2, seed=42)
-
-    >>> n_unique.update('a').get()
-    1
-
-    >>> n_unique.update('b').get()
-    2
-
-    >>> for letter in alphabet:
-    ...     n_unique = n_unique.update(letter)
-    >>> n_unique.get()
-    31
-
-    Lowering the `error_rate` parameter will increase the precision.
-
-    >>> n_unique = stats.NUnique(error_rate=0.01, seed=42)
-    >>> for letter in alphabet:
-    ...     n_unique = n_unique.update(letter)
-    >>> n_unique.get()
-    26
-
-    References
-    ----------
-    [^1]: [My favorite algorithm (and data structure): HyperLogLog](https://odino.org/my-favorite-data-structure-hyperloglog/)
-    [^2]: [Flajolet, P., Fusy, É., Gandouet, O. and Meunier, F., 2007, June. Hyperloglog: the analysis of a near-optimal cardinality estimation algorithm.](http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf)
-
-    """
-
-    P32 = 2 ** 32
-
-    def __init__(self, error_rate=0.01, seed: int = None):
-        self.error_rate = error_rate
-        self.seed = seed
-
-        self.n_bits = int(math.ceil(math.log((1.04 / error_rate) ** 2, 2)))
-        self.n_buckets = 1 << self.n_bits
-        self.buckets = [0] * self.n_buckets
-        self._salt = np.random.RandomState(seed).bytes(hashlib.blake2s.SALT_SIZE)
-
-    @property
-    def name(self):
-        return "n_unique"
-
-    def _hash(self, x):
-        hexa = hashlib.blake2s(bytes(x, encoding="utf8"), salt=self._salt).hexdigest()
-        return int(hexa, 16)
-
-    def update(self, x):
-        x = self._hash(x)
-        i = x & NUnique.P32 - 1 >> 32 - self.n_bits
-        z = 35 - len(bin(NUnique.P32 - 1 & x << self.n_bits | 1 << self.n_bits - 1))
-        self.buckets[i] = max(self.buckets[i], z)
-        return self
-
-    def get(self):
-        a = (
-            {16: 0.673, 32: 0.697, 64: 0.709}[self.n_buckets]
-            if self.n_buckets <= 64
-            else 0.7213 / (1 + 1.079 / self.n_buckets)
-        )
-        e = (
-            a
-            * self.n_buckets
-            * self.n_buckets
-            / sum(1.0 / (1 << x) for x in self.buckets)
-        )
-        if e <= self.n_buckets * 2.5:
-            z = len([r for r in self.buckets if not r])
-            return int(self.n_buckets * math.log(float(self.n_buckets) / z) if z else e)
-        return int(
-            e if e < NUnique.P32 / 30 else -NUnique.P32 * math.log(1 - e / NUnique.P32)
-        )
+import hashlib
+import math
+
+import numpy as np
+
+from . import base
+
+
+class NUnique(base.Univariate):
+    """Approximate number of unique values counter.
+
+    This is basically an implementation of the HyperLogLog algorithm. Adapted from
+    [`hypy`](https://github.com/clarkduvall/hypy). The code is a bit too terse but it will do for
+    now.
+
+    Parameters
+    ----------
+    error_rate
+        Desired error rate. Memory usage is inversely proportional to this value.
+    seed
+        Set the seed to produce identical results.
+
+    Attributes
+    ----------
+    n_bits : int
+    n_buckets : int
+    buckets : list
+
+    Examples
+    --------
+
+    >>> import string
+    >>> from river import stats
+
+    >>> alphabet = string.ascii_lowercase
+    >>> n_unique = stats.NUnique(error_rate=0.2, seed=42)
+
+    >>> n_unique.update('a').get()
+    1
+
+    >>> n_unique.update('b').get()
+    2
+
+    >>> for letter in alphabet:
+    ...     n_unique = n_unique.update(letter)
+    >>> n_unique.get()
+    31
+
+    Lowering the `error_rate` parameter will increase the precision.
+
+    >>> n_unique = stats.NUnique(error_rate=0.01, seed=42)
+    >>> for letter in alphabet:
+    ...     n_unique = n_unique.update(letter)
+    >>> n_unique.get()
+    26
+
+    References
+    ----------
+    [^1]: [My favorite algorithm (and data structure): HyperLogLog](https://odino.org/my-favorite-data-structure-hyperloglog/)
+    [^2]: [Flajolet, P., Fusy, É., Gandouet, O. and Meunier, F., 2007, June. Hyperloglog: the analysis of a near-optimal cardinality estimation algorithm.](http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf)
+
+    """
+
+    P32 = 2 ** 32
+
+    def __init__(self, error_rate=0.01, seed: int = None):
+        self.error_rate = error_rate
+        self.seed = seed
+
+        self.n_bits = int(math.ceil(math.log((1.04 / error_rate) ** 2, 2)))
+        self.n_buckets = 1 << self.n_bits
+        self.buckets = [0] * self.n_buckets
+        self._salt = np.random.RandomState(seed).bytes(hashlib.blake2s.SALT_SIZE)
+
+    @property
+    def name(self):
+        return "n_unique"
+
+    def _hash(self, x):
+        hexa = hashlib.blake2s(bytes(x, encoding="utf8"), salt=self._salt).hexdigest()
+        return int(hexa, 16)
+
+    def update(self, x):
+        x = self._hash(x)
+        i = x & NUnique.P32 - 1 >> 32 - self.n_bits
+        z = 35 - len(bin(NUnique.P32 - 1 & x << self.n_bits | 1 << self.n_bits - 1))
+        self.buckets[i] = max(self.buckets[i], z)
+        return self
+
+    def get(self):
+        a = (
+            {16: 0.673, 32: 0.697, 64: 0.709}[self.n_buckets]
+            if self.n_buckets <= 64
+            else 0.7213 / (1 + 1.079 / self.n_buckets)
+        )
+        e = (
+            a
+            * self.n_buckets
+            * self.n_buckets
+            / sum(1.0 / (1 << x) for x in self.buckets)
+        )
+        if e <= self.n_buckets * 2.5:
+            z = len([r for r in self.buckets if not r])
+            return int(self.n_buckets * math.log(float(self.n_buckets) / z) if z else e)
+        return int(
+            e if e < NUnique.P32 / 30 else -NUnique.P32 * math.log(1 - e / NUnique.P32)
+        )
```

### Comparing `river-0.8.0/river/stats/pearson.py` & `river-0.9.0/river/stats/ptp.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,127 +1,104 @@
-from . import base, cov, var
-
-
-class PearsonCorr(base.Bivariate):
-    """Online Pearson correlation.
-
-    Parameters
-    ----------
-    ddof
-        Delta Degrees of Freedom.
-
-    Attributes
-    ----------
-    var_x : stats.Var
-        Running variance of `x`.
-    var_y : stats.Var
-        Running variance of `y`.
-    cov_xy : stats.Cov
-        Running covariance of `x` and `y`.
-
-    Examples
-    --------
-
-    >>> from river import stats
-
-    >>> x = [0, 0, 0, 1, 1, 1, 1]
-    >>> y = [0, 1, 2, 3, 4, 5, 6]
-
-    >>> pearson = stats.PearsonCorr()
-
-    >>> for xi, yi in zip(x, y):
-    ...     print(pearson.update(xi, yi).get())
-    0
-    0
-    0
-    0.774596
-    0.866025
-    0.878310
-    0.866025
-
-    """
-
-    def __init__(self, ddof=1):
-        self.var_x = var.Var(ddof=ddof)
-        self.var_y = var.Var(ddof=ddof)
-        self.cov_xy = cov.Cov(ddof=ddof)
-
-    @property
-    def ddof(self):
-        return self.cov_xy.ddof
-
-    def update(self, x, y):
-        self.var_x.update(x)
-        self.var_y.update(y)
-        self.cov_xy.update(x, y)
-        return self
-
-    def get(self):
-        var_x = self.var_x.get()
-        var_y = self.var_y.get()
-        if var_x and var_y:
-            return self.cov_xy.get() / (var_x * var_y) ** 0.5
-        return 0
-
-
-class RollingPearsonCorr(base.Bivariate):
-    """Rolling Pearson correlation.
-
-    Parameters
-    ----------
-    window_size
-        Amount of samples over which to compute the correlation.
-    ddof
-        Delta Degrees of Freedom.
-
-    Attributes
-    ----------
-    var_x : stats.Var
-        Running variance of `x`.
-    var_y : stats.Var
-        Running variance of `y`.
-    cov_xy : stats.Cov
-        Running covariance of `x` and `y`.
-
-    Examples
-    --------
-
-    >>> from river import stats
-
-    >>> x = [0, 0, 0, 1, 1, 1, 1]
-    >>> y = [0, 1, 2, 3, 4, 5, 6]
-
-    >>> pearson = stats.RollingPearsonCorr(window_size=4)
-
-    >>> for xi, yi in zip(x, y):
-    ...     print(pearson.update(xi, yi).get())
-    0
-    0
-    0
-    0.7745966692414834
-    0.894427190999916
-    0.7745966692414834
-    0
-
-    """
-
-    def __init__(self, window_size, ddof=1):
-        self.var_x = var.RollingVar(window_size=window_size, ddof=ddof)
-        self.var_y = var.RollingVar(window_size=window_size, ddof=ddof)
-        self.cov_xy = cov.RollingCov(window_size=window_size, ddof=ddof)
-
-    @property
-    def window_size(self):
-        return self.cov_xy.window_size
-
-    def update(self, x, y):
-        self.var_x.update(x)
-        self.var_y.update(y)
-        self.cov_xy.update(x, y)
-        return self
-
-    def get(self):
-        var_x = self.var_x.get()
-        var_y = self.var_y.get()
-        if var_x and var_y:
-            return self.cov_xy.get() / (var_x * var_y) ** 0.5
-        return 0
+from . import base, maximum, minimum
+
+
+class PeakToPeak(base.Univariate):
+    """Running peak to peak (max - min).
+
+    Attributes
+    ----------
+    max : stats.Max
+        The running max.
+    min : stats.Min
+        The running min.
+    p2p : float
+        The running peak to peak.
+
+    Examples
+    --------
+
+    >>> from river import stats
+
+    >>> X = [1, -4, 3, -2, 2, 4]
+    >>> ptp = stats.PeakToPeak()
+    >>> for x in X:
+    ...     print(ptp.update(x).get())
+    0
+    5
+    7
+    7
+    7
+    8
+
+    """
+
+    def __init__(self):
+        self.max = maximum.Max()
+        self.min = minimum.Min()
+
+    @property
+    def name(self):
+        return "ptp"
+
+    def update(self, x):
+        self.max.update(x)
+        self.min.update(x)
+        return self
+
+    def get(self):
+        return self.max.get() - self.min.get()
+
+
+class RollingPeakToPeak(base.RollingUnivariate):
+    """Running peak to peak (max - min) over a window.
+
+    Parameters
+    ----------
+    window_size
+        Size of the rolling window.
+
+    Attributes
+    ----------
+    max : stats.RollingMax
+        The running rolling max.
+    min : stats.RollingMin
+        The running rolling min.
+
+    Examples
+    --------
+
+    >>> from river import stats
+
+    >>> X = [1, -4, 3, -2, 2, 1]
+    >>> ptp = stats.RollingPeakToPeak(window_size=2)
+    >>> for x in X:
+    ...     print(ptp.update(x).get())
+    0
+    5
+    7
+    5
+    4
+    1
+
+    """
+
+    def __init__(self, window_size: int):
+        self.max = maximum.RollingMax(window_size)
+        self.min = minimum.RollingMin(window_size)
+
+    @property
+    def window_size(self):
+        return self.max.window_size
+
+    def update(self, x):
+        self.max.update(x)
+        self.min.update(x)
+        return self
+
+    def get(self):
+        maximum = self.max.get()
+        if maximum is None:
+            return None
+        minimum = self.min.get()
+        if minimum is None:
+            return None
+        return maximum - minimum
```

### Comparing `river-0.8.0/river/stats/sem.py` & `river-0.9.0/river/stats/sem.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,85 +1,91 @@
-from . import var
-
-
-class SEM(var.Var):
-    """Running standard error of the mean using Welford's algorithm.
-
-    Parameters
-    ----------
-    ddof
-        Delta Degrees of Freedom. The divisor used in calculations is `n - ddof`, where `n` is the
-        number of seen elements.
-
-    Attributes
-    ----------
-    n : int
-        Number of observations.
-
-    Examples
-    --------
-
-    >>> import river.stats
-
-    >>> X = [3, 5, 4, 7, 10, 12]
-
-    >>> sem = river.stats.SEM()
-    >>> for x in X:
-    ...     print(sem.update(x).get())
-    0.0
-    1.0
-    0.577350
-    0.853912
-    1.240967
-    1.447219
-
-    References
-    ----------
-    [^1]: [Wikipedia article on algorithms for calculating variance](https://www.wikiwand.com/en/Algorithms_for_calculating_variance#/Covariance)
-
-    """
-
-    def get(self):
-        return (super().get() / self.mean.n) ** 0.5
-
-
-class RollingSEM(var.RollingVar):
-    """Running standard error of the mean over a window.
-
-    Parameters
-    ----------
-    window_size
-        Size of the rolling window.
-    ddof
-        Delta Degrees of Freedom for the variance.
-
-    Examples
-    --------
-
-    >>> import river
-
-    >>> X = [1, 4, 2, -4, -8, 0]
-
-    >>> rolling_sem = river.stats.RollingSEM(ddof=1, window_size=2)
-    >>> for x in X:
-    ...     print(rolling_sem.update(x).get())
-    0.0
-    1.5
-    1.0
-    3.0
-    2.0
-    4.0
-
-    >>> rolling_sem = river.stats.RollingSEM(ddof=1, window_size=3)
-    >>> for x in X:
-    ...     print(rolling_sem.update(x).get())
-    0.0
-    1.5
-    0.881917
-    2.403700
-    2.905932
-    2.309401
-
-    """
-
-    def get(self):
-        return (super().get() / len(self.rolling_mean)) ** 0.5
+from . import var
+
+
+class SEM(var.Var):
+    """Running standard error of the mean using Welford's algorithm.
+
+    Parameters
+    ----------
+    ddof
+        Delta Degrees of Freedom. The divisor used in calculations is `n - ddof`, where `n` is the
+        number of seen elements.
+
+    Attributes
+    ----------
+    n : int
+        Number of observations.
+
+    Examples
+    --------
+
+    >>> import river.stats
+
+    >>> X = [3, 5, 4, 7, 10, 12]
+
+    >>> sem = river.stats.SEM()
+    >>> for x in X:
+    ...     print(sem.update(x).get())
+    0.0
+    1.0
+    0.577350
+    0.853912
+    1.240967
+    1.447219
+
+    References
+    ----------
+    [^1]: [Wikipedia article on algorithms for calculating variance](https://www.wikiwand.com/en/Algorithms_for_calculating_variance#/Covariance)
+
+    """
+
+    def get(self):
+        try:
+            return (super().get() / self.mean.n) ** 0.5
+        except ZeroDivisionError:
+            return None
+
+
+class RollingSEM(var.RollingVar):
+    """Running standard error of the mean over a window.
+
+    Parameters
+    ----------
+    window_size
+        Size of the rolling window.
+    ddof
+        Delta Degrees of Freedom for the variance.
+
+    Examples
+    --------
+
+    >>> import river
+
+    >>> X = [1, 4, 2, -4, -8, 0]
+
+    >>> rolling_sem = river.stats.RollingSEM(ddof=1, window_size=2)
+    >>> for x in X:
+    ...     print(rolling_sem.update(x).get())
+    0.0
+    1.5
+    1.0
+    3.0
+    2.0
+    4.0
+
+    >>> rolling_sem = river.stats.RollingSEM(ddof=1, window_size=3)
+    >>> for x in X:
+    ...     print(rolling_sem.update(x).get())
+    0.0
+    1.5
+    0.881917
+    2.403700
+    2.905932
+    2.309401
+
+    """
+
+    def get(self):
+        try:
+            return (super().get() / len(self._rolling_mean)) ** 0.5
+        except ZeroDivisionError:
+            return None
```

### Comparing `river-0.8.0/river/stats/shift.py` & `river-0.9.0/river/stats/shift.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,107 +1,107 @@
-import collections
-
-from river import stats
-
-
-class Shift(stats.Univariate):
-    """Shifts a data stream by returning past values.
-
-    This can be used to compute statistics over past data. For instance, if you're computing daily
-    averages, then shifting by 7 will be equivalent to computing averages from a week ago.
-
-    Shifting values is useful when you're calculating an average over a target value. Indeed,
-    in this case it's important to shift the values in order not to introduce leakage. The
-    recommended way to do this is to `feature_extraction.TargetAgg`, which already takes care
-    of shifting the target values once.
-
-    Parameters
-    ----------
-    amount
-        Shift amount. The `get` method will return the `t - amount` value, where `t` is the
-        current moment.
-    fill_value
-        This value will be returned by the `get` method if not enough values have been observed.
-
-    Examples
-    --------
-
-    It is rare to have to use `Shift` by itself. A more common usage is to compose it with
-    other statistics. This can be done via the `|` operator.
-
-    >>> from river import stats
-
-    >>> stat = stats.Shift(1) | stats.Mean()
-
-    >>> for i in range(5):
-    ...     stat = stat.update(i)
-    ...     print(stat.get())
-    0.0
-    0.0
-    0.5
-    1.0
-    1.5
-
-    A common usecase for using `Shift` is when computing statistics on shifted data. For
-    instance, say you have a dataset which records the amount of sales for a set of shops. You
-    might then have a `shop` field and a `sales` field. Let's say you want to look at the
-    average amount of sales per shop. You can do this by using a `feature_extraction.Agg`. When
-    you call `transform_one`, you're expecting it to return the average amount of sales,
-    *without* including today's sales. You can do this by prepending an instance of
-    `stats.Mean` with an instance of `stats.Shift`.
-
-    >>> from river import feature_extraction
-
-    >>> agg = feature_extraction.Agg(
-    ...     on='sales',
-    ...     how=stats.Shift(1) | stats.Mean(),
-    ...     by='shop'
-    ... )
-
-    Let's define a little example dataset.
-
-    >>> X = iter([
-    ...     {'shop': 'Ikea', 'sales': 10},
-    ...     {'shop': 'Ikea', 'sales': 15},
-    ...     {'shop': 'Ikea', 'sales': 20}
-    ... ])
-
-    Now let's call the `learn_one` method to update our feature extractor.
-
-    >>> x = next(X)
-    >>> agg = agg.learn_one(x)
-
-    At this point, the average defaults to the initial value of `stats.Mean`, which is 0.
-
-    >>> agg.transform_one(x)
-    {'sales_mean_of_shift_1_by_shop': 0.0}
-
-    We can now update our feature extractor with the next data point and check the output.
-
-    >>> agg = agg.learn_one(next(X))
-    >>> agg.transform_one(x)
-    {'sales_mean_of_shift_1_by_shop': 10.0}
-
-    >>> agg = agg.learn_one(next(X))
-    >>> agg.transform_one(x)
-    {'sales_mean_of_shift_1_by_shop': 12.5}
-
-    """
-
-    def __init__(self, amount=1, fill_value=None):
-        self.amount = amount
-        self.fill_value = fill_value
-        self.buffer = collections.deque(maxlen=self.amount + 1)
-
-    def update(self, x):
-        self.buffer.append(x)
-        return self
-
-    def get(self):
-        try:
-            return self.buffer[-self.amount - 1]
-        except IndexError:
-            return self.fill_value
-
-    @property
-    def name(self):
-        return f"shift_{self.amount}"
+import collections
+
+from river import stats
+
+
+class Shift(stats.Univariate):
+    """Shifts a data stream by returning past values.
+
+    This can be used to compute statistics over past data. For instance, if you're computing daily
+    averages, then shifting by 7 will be equivalent to computing averages from a week ago.
+
+    Shifting values is useful when you're calculating an average over a target value. Indeed,
+    in this case it's important to shift the values in order not to introduce leakage. The
+    recommended way to do this is to `feature_extraction.TargetAgg`, which already takes care
+    of shifting the target values once.
+
+    Parameters
+    ----------
+    amount
+        Shift amount. The `get` method will return the `t - amount` value, where `t` is the
+        current moment.
+    fill_value
+        This value will be returned by the `get` method if not enough values have been observed.
+
+    Examples
+    --------
+
+    It is rare to have to use `Shift` by itself. A more common usage is to compose it with
+    other statistics. This can be done via the `|` operator.
+
+    >>> from river import stats
+
+    >>> stat = stats.Shift(1) | stats.Mean()
+
+    >>> for i in range(5):
+    ...     stat = stat.update(i)
+    ...     print(stat.get())
+    0.0
+    0.0
+    0.5
+    1.0
+    1.5
+
+    A common usecase for using `Shift` is when computing statistics on shifted data. For
+    instance, say you have a dataset which records the amount of sales for a set of shops. You
+    might then have a `shop` field and a `sales` field. Let's say you want to look at the
+    average amount of sales per shop. You can do this by using a `feature_extraction.Agg`. When
+    you call `transform_one`, you're expecting it to return the average amount of sales,
+    *without* including today's sales. You can do this by prepending an instance of
+    `stats.Mean` with an instance of `stats.Shift`.
+
+    >>> from river import feature_extraction
+
+    >>> agg = feature_extraction.Agg(
+    ...     on='sales',
+    ...     how=stats.Shift(1) | stats.Mean(),
+    ...     by='shop'
+    ... )
+
+    Let's define a little example dataset.
+
+    >>> X = iter([
+    ...     {'shop': 'Ikea', 'sales': 10},
+    ...     {'shop': 'Ikea', 'sales': 15},
+    ...     {'shop': 'Ikea', 'sales': 20}
+    ... ])
+
+    Now let's call the `learn_one` method to update our feature extractor.
+
+    >>> x = next(X)
+    >>> agg = agg.learn_one(x)
+
+    At this point, the average defaults to the initial value of `stats.Mean`, which is 0.
+
+    >>> agg.transform_one(x)
+    {'sales_mean_of_shift_1_by_shop': 0.0}
+
+    We can now update our feature extractor with the next data point and check the output.
+
+    >>> agg = agg.learn_one(next(X))
+    >>> agg.transform_one(x)
+    {'sales_mean_of_shift_1_by_shop': 10.0}
+
+    >>> agg = agg.learn_one(next(X))
+    >>> agg.transform_one(x)
+    {'sales_mean_of_shift_1_by_shop': 12.5}
+
+    """
+
+    def __init__(self, amount=1, fill_value=None):
+        self.amount = amount
+        self.fill_value = fill_value
+        self.buffer = collections.deque(maxlen=self.amount + 1)
+
+    def update(self, x):
+        self.buffer.append(x)
+        return self
+
+    def get(self):
+        try:
+            return self.buffer[-self.amount - 1]
+        except IndexError:
+            return self.fill_value
+
+    @property
+    def name(self):
+        return f"shift_{self.amount}"
```

### Comparing `river-0.8.0/river/stats/skew.py` & `river-0.9.0/river/stats/skew.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,104 +1,104 @@
-from . import moments
-
-
-class Skew(moments.CentralMoments):
-    """Running skew using Welford's algorithm.
-
-    Parameters
-    ----------
-    bias
-        If `False`, then the calculations are corrected for statistical bias.
-
-    Examples
-    --------
-
-    >>> import river.stats
-    >>> import scipy.stats
-    >>> import numpy as np
-
-    >>> np.random.seed(42)
-    >>> X = np.random.normal(loc=0, scale=1, size=10)
-
-    >>> skew = river.stats.Skew(bias=False)
-    >>> for x in X:
-    ...     print(skew.update(x).get())
-    0
-    0.0
-    -1.4802398132849872
-    0.5127437186677888
-    0.7803466510704751
-    1.056115628922055
-    0.5057840774320389
-    0.3478402420400934
-    0.4536710660918704
-    0.4123070197493227
-
-    >>> for i in range(1, len(X)+1):
-    ...     print(scipy.stats.skew(X[:i], bias=False))
-    0.0
-    0.0
-    -1.4802398132849874
-    0.5127437186677893
-    0.7803466510704746
-    1.056115628922055
-    0.5057840774320389
-    0.3478402420400927
-    0.4536710660918703
-    0.4123070197493223
-
-    >>> skew = river.stats.Skew(bias=True)
-    >>> for x in X:
-    ...     print(skew.update(x).get())
-    0
-    0.0
-    -0.6043053732501439
-    0.2960327239981376
-    0.5234724473423674
-    0.7712778043924866
-    0.39022088752624845
-    0.278892645224261
-    0.37425953513864063
-    0.3476878073823696
-
-    >>> for i in range(1, len(X)+1):
-    ...     print(scipy.stats.skew(X[:i], bias=True))
-    0.0
-    0.0
-    -0.604305373250144
-    0.29603272399813796
-    0.5234724473423671
-    0.7712778043924865
-    0.39022088752624845
-    0.2788926452242604
-    0.3742595351386406
-    0.34768780738236926
-
-    References
-    ----------
-    [^1]: [Wikipedia article on algorithms for calculating variance](https://www.wikiwand.com/en/Algorithms_for_calculating_variance#/Covariance)
-
-    """
-
-    def __init__(self, bias=False):
-        super().__init__()
-        self.bias = bias
-
-    @property
-    def name(self):
-        return "skew"
-
-    def update(self, x):
-        self.count.update()
-        self._update_delta(x)
-        self._update_m1(x)
-        self._update_sum_delta()
-        self._update_m3()
-        self._update_m2()
-        return self
-
-    def get(self):
-        n = self.count.get()
-        skew = n ** 0.5 * self.M3 / self.M2 ** 1.5 if self.M2 != 0 else 0
-        if not self.bias and n > 2:
-            return ((n - 1.0) * n) ** 0.5 / (n - 2.0) * skew
-        return skew
+from . import moments
+
+
+class Skew(moments.CentralMoments):
+    """Running skew using Welford's algorithm.
+
+    Parameters
+    ----------
+    bias
+        If `False`, then the calculations are corrected for statistical bias.
+
+    Examples
+    --------
+
+    >>> import river.stats
+    >>> import scipy.stats
+    >>> import numpy as np
+
+    >>> np.random.seed(42)
+    >>> X = np.random.normal(loc=0, scale=1, size=10)
+
+    >>> skew = river.stats.Skew(bias=False)
+    >>> for x in X:
+    ...     print(skew.update(x).get())
+    0
+    0.0
+    -1.4802398132849872
+    0.5127437186677888
+    0.7803466510704751
+    1.056115628922055
+    0.5057840774320389
+    0.3478402420400934
+    0.4536710660918704
+    0.4123070197493227
+
+    >>> for i in range(1, len(X)+1):
+    ...     print(scipy.stats.skew(X[:i], bias=False))
+    0.0
+    0.0
+    -1.4802398132849874
+    0.5127437186677893
+    0.7803466510704746
+    1.056115628922055
+    0.5057840774320389
+    0.3478402420400927
+    0.4536710660918703
+    0.4123070197493223
+
+    >>> skew = river.stats.Skew(bias=True)
+    >>> for x in X:
+    ...     print(skew.update(x).get())
+    0
+    0.0
+    -0.6043053732501439
+    0.2960327239981376
+    0.5234724473423674
+    0.7712778043924866
+    0.39022088752624845
+    0.278892645224261
+    0.37425953513864063
+    0.3476878073823696
+
+    >>> for i in range(1, len(X)+1):
+    ...     print(scipy.stats.skew(X[:i], bias=True))
+    0.0
+    0.0
+    -0.604305373250144
+    0.29603272399813796
+    0.5234724473423671
+    0.7712778043924865
+    0.39022088752624845
+    0.2788926452242604
+    0.3742595351386406
+    0.34768780738236926
+
+    References
+    ----------
+    [^1]: [Wikipedia article on algorithms for calculating variance](https://www.wikiwand.com/en/Algorithms_for_calculating_variance#/Covariance)
+
+    """
+
+    def __init__(self, bias=False):
+        super().__init__()
+        self.bias = bias
+
+    @property
+    def name(self):
+        return "skew"
+
+    def update(self, x):
+        self.count.update()
+        self._update_delta(x)
+        self._update_m1(x)
+        self._update_sum_delta()
+        self._update_m3()
+        self._update_m2()
+        return self
+
+    def get(self):
+        n = self.count.get()
+        skew = n ** 0.5 * self.M3 / self.M2 ** 1.5 if self.M2 != 0 else 0
+        if not self.bias and n > 2:
+            return ((n - 1.0) * n) ** 0.5 / (n - 2.0) * skew
+        return skew
```

### Comparing `river-0.8.0/river/stats/test_mean_var_cov.py` & `river-0.9.0/river/stats/test_parallel.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,120 +1,145 @@
-import functools
-import math
-import random
-import statistics
-
-import numpy as np
-import pytest
-
-from river import stats
-
-
-@pytest.mark.parametrize(
-    "stat1, stat2, func",
-    [
-        (stats.Mean(), stats.Mean(), statistics.mean),
-        (stats.Var(ddof=0), stats.Var(ddof=0), np.var),
-        (stats.Var(), stats.Var(), functools.partial(np.var, ddof=1)),
-    ],
-)
-def test_add_mean_var(stat1, stat2, func):
-    X = [random.random() for _ in range(30)]
-    Y = [random.random() for _ in range(30)]
-
-    for i, (x, y) in enumerate(zip(X, Y)):
-        stat1.update(x)
-        stat2.update(y)
-        if i >= 1:
-            assert math.isclose(
-                (stat1 + stat2).get(), func(X[: i + 1] + Y[: i + 1]), abs_tol=1e-10
-            )
-
-    stat1 += stat2
-    assert math.isclose(stat1.get(), func(X + Y), abs_tol=1e-10)
-
-
-@pytest.mark.parametrize(
-    "stat1, stat2, func",
-    [
-        (stats.Mean(), stats.Mean(), statistics.mean),
-        (stats.Var(ddof=0), stats.Var(ddof=0), np.var),
-        (stats.Var(), stats.Var(), functools.partial(np.var, ddof=1)),
-    ],
-)
-def test_sub_mean_var(stat1, stat2, func):
-    X = [random.random() for _ in range(30)]
-
-    for x in X:
-        stat1.update(x)
-
-    for i, x in enumerate(X):
-        stat2.update(x)
-        if i < len(X) - 2:
-            assert math.isclose((stat1 - stat2).get(), func(X[i + 1 :]), abs_tol=1e-10)
-
-    # Test inplace subtraction
-    X.extend(random.random() for _ in range(3))
-    for i in range(30, 33):
-        stat1.update(X[i])
-
-    stat1 -= stat2
-    assert math.isclose(stat1.get(), func(X[30:33]), abs_tol=1e-10)
-
-
-@pytest.mark.parametrize(
-    "stat1, stat2, func",
-    [
-        (stats.Cov(ddof=0), stats.Cov(ddof=0), functools.partial(np.cov, ddof=0)),
-        (stats.Cov(ddof=1), stats.Cov(ddof=1), functools.partial(np.cov, ddof=1)),
-    ],
-)
-def test_add_cov(stat1, stat2, func):
-    X = [random.random() for _ in range(30)]
-    Y = [random.random() for _ in range(30)]
-
-    W = [random.random() for _ in range(30)]
-    Z = [random.random() for _ in range(30)]
-
-    for i, (x, y, w, z) in enumerate(zip(X, Y, W, Z)):
-        stat1.update(x, y)
-        stat2.update(w, z)
-        if i >= 1:
-            assert math.isclose(
-                (stat1 + stat2).get(),
-                func(X[: i + 1] + W[: i + 1], Y[: i + 1] + Z[: i + 1])[0, 1],
-                abs_tol=1e-10,
-            )
-
-    stat1 += stat2
-    assert math.isclose(stat1.get(), func(X + W, Y + Z)[0, 1], abs_tol=1e-10)
-
-
-@pytest.mark.parametrize(
-    "stat1, stat2, func",
-    [
-        (stats.Cov(ddof=0), stats.Cov(ddof=0), functools.partial(np.cov, ddof=0)),
-        (stats.Cov(ddof=1), stats.Cov(ddof=1), functools.partial(np.cov, ddof=1)),
-    ],
-)
-def test_sub_cov(stat1, stat2, func):
-    X = [random.random() for _ in range(30)]
-    Y = [random.random() for _ in range(30)]
-
-    for x, y in zip(X, Y):
-        stat1.update(x, y)
-
-    for i, (x, y) in enumerate(zip(X, Y)):
-        stat2.update(x, y)
-        if i < len(X) - 2:
-            assert math.isclose(
-                (stat1 - stat2).get(), func(X[i + 1 :], Y[i + 1 :])[0, 1], abs_tol=1e-10
-            )
-
-    # Test inplace subtraction
-    X.extend(random.random() for _ in range(3))
-    Y.extend(random.random() for _ in range(3))
-    for i in range(30, 33):
-        stat1.update(X[i], Y[i])
-
-    stat1 -= stat2
-    assert math.isclose(stat1.get(), func(X[30:33], Y[30:33])[0, 1], abs_tol=1e-10)
+import copy
+import functools
+import math
+import random
+
+import numpy as np
+import pytest
+
+from river import stats
+
+
+@pytest.mark.parametrize(
+    "stat",
+    [
+        pytest.param(stat, id=stat.__class__.__name__)
+        for stat in [stats.Mean(), stats.Var(ddof=0), stats.Var(ddof=1)]
+    ],
+)
+def test_add(stat):
+    A = copy.deepcopy(stat)
+    B = copy.deepcopy(stat)
+    C = copy.deepcopy(stat)
+
+    X = [random.random() for _ in range(30)]
+    Y = [random.random() for _ in range(30)]
+    W = [random.random() for _ in range(30)]
+
+    for x, y, w in zip(X, Y, W):
+        A.update(x, w)
+        B.update(y, w)
+        C.update(x, w).update(y, w)
+
+    D = A + B
+    assert math.isclose(C.get(), D.get())
+
+    A += B
+    assert math.isclose(C.get(), A.get())
+
+
+@pytest.mark.parametrize(
+    "stat",
+    [
+        pytest.param(stat, id=stat.__class__.__name__)
+        for stat in [stats.Mean(), stats.Var(ddof=0), stats.Var(ddof=1)]
+    ],
+)
+def test_sub(stat):
+    A = copy.deepcopy(stat)
+    B = copy.deepcopy(stat)
+    C = copy.deepcopy(stat)
+
+    X = [random.random() for _ in range(30)]
+    Y = [random.random() for _ in range(30)]
+    W = [random.random() for _ in range(30)]
+
+    for x, y, w in zip(X, Y, W):
+        A.update(x, w)
+        B.update(y, w)
+        C.update(x, w).update(y, w)
+
+    D = C - B
+    assert math.isclose(D.get(), A.get())
+
+    C -= B
+    assert math.isclose(C.get(), A.get())
+
+
+@pytest.mark.parametrize(
+    "stat",
+    [
+        pytest.param(stat, id=stat.__class__.__name__)
+        for stat in [stats.Mean(), stats.Var(ddof=0), stats.Var(ddof=1)]
+    ],
+)
+def test_sub_back_to_zero(stat):
+
+    A = copy.deepcopy(stat)
+    B = copy.deepcopy(stat)
+    C = copy.deepcopy(stat)
+
+    x = random.random()
+    A.update(x)
+    B.update(x)
+
+    D = A - B
+    assert math.isclose(D.get(), C.get())
+
+
+@pytest.mark.parametrize(
+    "stat1, stat2, func",
+    [
+        (stats.Cov(ddof=0), stats.Cov(ddof=0), functools.partial(np.cov, ddof=0)),
+        (stats.Cov(ddof=1), stats.Cov(ddof=1), functools.partial(np.cov, ddof=1)),
+    ],
+)
+def test_add_cov(stat1, stat2, func):
+    X = [random.random() for _ in range(30)]
+    Y = [random.random() for _ in range(30)]
+
+    W = [random.random() for _ in range(30)]
+    Z = [random.random() for _ in range(30)]
+
+    for i, (x, y, w, z) in enumerate(zip(X, Y, W, Z)):
+        stat1.update(x, y)
+        stat2.update(w, z)
+        if i >= 1:
+            assert math.isclose(
+                (stat1 + stat2).get(),
+                func(X[: i + 1] + W[: i + 1], Y[: i + 1] + Z[: i + 1])[0, 1],
+                abs_tol=1e-10,
+            )
+
+    stat1 += stat2
+    assert math.isclose(stat1.get(), func(X + W, Y + Z)[0, 1], abs_tol=1e-10)
+
+
+@pytest.mark.parametrize(
+    "stat1, stat2, func",
+    [
+        (stats.Cov(ddof=0), stats.Cov(ddof=0), functools.partial(np.cov, ddof=0)),
+        (stats.Cov(ddof=1), stats.Cov(ddof=1), functools.partial(np.cov, ddof=1)),
+    ],
+)
+def test_sub_cov(stat1, stat2, func):
+    X = [random.random() for _ in range(30)]
+    Y = [random.random() for _ in range(30)]
+
+    for x, y in zip(X, Y):
+        stat1.update(x, y)
+
+    for i, (x, y) in enumerate(zip(X, Y)):
+        stat2.update(x, y)
+        if i < len(X) - 2:
+            assert math.isclose(
+                (stat1 - stat2).get(), func(X[i + 1 :], Y[i + 1 :])[0, 1], abs_tol=1e-10
+            )
+
+    # Test inplace subtraction
+    X.extend(random.random() for _ in range(3))
+    Y.extend(random.random() for _ in range(3))
+    for i in range(30, 33):
+        stat1.update(X[i], Y[i])
+
+    stat1 -= stat2
+    assert math.isclose(stat1.get(), func(X[30:33], Y[30:33])[0, 1], abs_tol=1e-10)
```

### Comparing `river-0.8.0/river/stream/cache.py` & `river-0.9.0/river/stream/cache.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,161 +1,161 @@
-import glob
-import inspect
-import os
-import pickle
-import platform
-
-from river import utils
-
-
-class Cache:
-    """Utility for caching iterables.
-
-    This can be used to save a stream of data to the disk in order to iterate over it faster the
-    following time. This can save time depending on the nature of stream. The more processing
-    happens in a stream, the more time will be saved. Even in the case where no processing is done
-    apart from reading the data, the cache will save some time because it is using the pickle
-    binary protocol. It can thus improve the speed in common cases such as reading from a CSV file.
-
-    Parameters
-    ----------
-    directory
-        The path where to store the pickled data streams. If not provided, then it will be
-        automatically inferred whenever possible, if not an exception will be raised.
-
-    Attributes
-    ----------
-    keys : set
-        The set of keys that are being cached.
-
-    Examples
-    --------
-
-    >>> import time
-    >>> from river import datasets
-    >>> from river import stream
-
-    >>> dataset = datasets.Phishing()
-    >>> cache = stream.Cache()
-
-    The cache can be used by wrapping it around an iterable. Because this is the first time
-    are iterating over the data, nothing is cached.
-
-    >>> tic = time.time()
-    >>> for x, y in cache(dataset, key='phishing'):
-    ...     pass
-    >>> toc = time.time()
-    >>> print(toc - tic)  # doctest: +SKIP
-    0.012813
-
-    If we do the same thing again, we can see the loop is now faster.
-
-    >>> tic = time.time()
-    >>> for x, y in cache(dataset, key='phishing'):
-    ...     pass
-    >>> toc = time.time()
-    >>> print(toc - tic)  # doctest: +SKIP
-    0.001927
-
-    We can see an overview of the cache. The first line indicates the location of the
-    cache.
-
-    >>> cache  # doctest: +SKIP
-    /tmp
-    phishing - 125.2KiB
-
-    Finally, we can clear the stream from the cache.
-
-    >>> cache.clear('phishing')
-    >>> cache  # doctest: +SKIP
-    /tmp
-
-    There is also a `clear_all` method to remove all the items in the cache.
-
-    >>> cache.clear_all()
-
-    """
-
-    def __init__(self, directory=None):
-
-        # Guess the directory from the system
-        system = platform.system()
-        if directory is None:
-            directory = {"Linux": "/tmp", "Darwin": "/tmp", "Windows": "C:\\TEMP"}.get(
-                system
-            )
-
-        if directory is None:
-            raise ValueError(
-                f"There is no default directory defined for {system} systems, "
-                "please provide one manually"
-            )
-
-        self.directory = directory
-        self.keys = set()
-
-        # Check if there is anything already in the cache
-        for f in glob.glob(os.path.join(self.directory, "*.river_cache.pkl")):
-            key = os.path.basename(f).split(".")[0]
-            self.keys.add(key)
-
-    def _get_path(self, key):
-        return os.path.join(self.directory, f"{key}.river_cache.pkl")
-
-    def __call__(self, stream, key=None):
-
-        # Try to guess a key from the stream object
-        if key is None:
-            if inspect.isfunction(stream):
-                key = stream.__name__
-
-        if key is None:
-            raise ValueError(
-                "No default key could be guessed for the given stream, "
-                "please provide one"
-            )
-
-        path = self._get_path(key)
-
-        if os.path.exists(path):
-            yield from self[key]
-            return
-
-        with open(path, "wb") as f:
-            pickler = pickle.Pickler(f)
-            for el in stream:
-                pickler.dump(el)
-                yield el
-            self.keys.add(key)
-
-    def __getitem__(self, key):
-        """Iterates over the stream associated with the given key."""
-        with open(self._get_path(key), "rb") as f:
-            unpickler = pickle.Unpickler(f)
-            while f.peek(1):
-                yield unpickler.load()
-
-    def clear(self, key: str):
-        """Delete the cached stream associated with the given key.
-
-        Parameters
-        ----------
-        key
-
-        """
-        os.remove(self._get_path(key))
-        self.keys.remove(key)
-
-    def clear_all(self):
-        """Delete all the cached streams."""
-        for key in list(self.keys):
-            os.remove(self._get_path(key))
-            self.keys.remove(key)
-
-    def __repr__(self):
-        return "\n".join(
-            [self.directory]
-            + [
-                f"{key} - {utils.pretty.humanize_bytes(os.path.getsize(self._get_path(key)))}"
-                for key in self.keys
-            ]
-        )
+import glob
+import inspect
+import os
+import pickle
+import platform
+
+from river import utils
+
+
+class Cache:
+    """Utility for caching iterables.
+
+    This can be used to save a stream of data to the disk in order to iterate over it faster the
+    following time. This can save time depending on the nature of stream. The more processing
+    happens in a stream, the more time will be saved. Even in the case where no processing is done
+    apart from reading the data, the cache will save some time because it is using the pickle
+    binary protocol. It can thus improve the speed in common cases such as reading from a CSV file.
+
+    Parameters
+    ----------
+    directory
+        The path where to store the pickled data streams. If not provided, then it will be
+        automatically inferred whenever possible, if not an exception will be raised.
+
+    Attributes
+    ----------
+    keys : set
+        The set of keys that are being cached.
+
+    Examples
+    --------
+
+    >>> import time
+    >>> from river import datasets
+    >>> from river import stream
+
+    >>> dataset = datasets.Phishing()
+    >>> cache = stream.Cache()
+
+    The cache can be used by wrapping it around an iterable. Because this is the first time
+    are iterating over the data, nothing is cached.
+
+    >>> tic = time.time()
+    >>> for x, y in cache(dataset, key='phishing'):
+    ...     pass
+    >>> toc = time.time()
+    >>> print(toc - tic)  # doctest: +SKIP
+    0.012813
+
+    If we do the same thing again, we can see the loop is now faster.
+
+    >>> tic = time.time()
+    >>> for x, y in cache(dataset, key='phishing'):
+    ...     pass
+    >>> toc = time.time()
+    >>> print(toc - tic)  # doctest: +SKIP
+    0.001927
+
+    We can see an overview of the cache. The first line indicates the location of the
+    cache.
+
+    >>> cache  # doctest: +SKIP
+    /tmp
+    phishing - 125.2KiB
+
+    Finally, we can clear the stream from the cache.
+
+    >>> cache.clear('phishing')
+    >>> cache  # doctest: +SKIP
+    /tmp
+
+    There is also a `clear_all` method to remove all the items in the cache.
+
+    >>> cache.clear_all()
+
+    """
+
+    def __init__(self, directory=None):
+
+        # Guess the directory from the system
+        system = platform.system()
+        if directory is None:
+            directory = {"Linux": "/tmp", "Darwin": "/tmp", "Windows": "C:\\TEMP"}.get(
+                system
+            )
+
+        if directory is None:
+            raise ValueError(
+                f"There is no default directory defined for {system} systems, "
+                "please provide one manually"
+            )
+
+        self.directory = directory
+        self.keys = set()
+
+        # Check if there is anything already in the cache
+        for f in glob.glob(os.path.join(self.directory, "*.river_cache.pkl")):
+            key = os.path.basename(f).split(".")[0]
+            self.keys.add(key)
+
+    def _get_path(self, key):
+        return os.path.join(self.directory, f"{key}.river_cache.pkl")
+
+    def __call__(self, stream, key=None):
+
+        # Try to guess a key from the stream object
+        if key is None:
+            if inspect.isfunction(stream):
+                key = stream.__name__
+
+        if key is None:
+            raise ValueError(
+                "No default key could be guessed for the given stream, "
+                "please provide one"
+            )
+
+        path = self._get_path(key)
+
+        if os.path.exists(path):
+            yield from self[key]
+            return
+
+        with open(path, "wb") as f:
+            pickler = pickle.Pickler(f)
+            for el in stream:
+                pickler.dump(el)
+                yield el
+            self.keys.add(key)
+
+    def __getitem__(self, key):
+        """Iterates over the stream associated with the given key."""
+        with open(self._get_path(key), "rb") as f:
+            unpickler = pickle.Unpickler(f)
+            while f.peek(1):
+                yield unpickler.load()
+
+    def clear(self, key: str):
+        """Delete the cached stream associated with the given key.
+
+        Parameters
+        ----------
+        key
+
+        """
+        os.remove(self._get_path(key))
+        self.keys.remove(key)
+
+    def clear_all(self):
+        """Delete all the cached streams."""
+        for key in list(self.keys):
+            os.remove(self._get_path(key))
+            self.keys.remove(key)
+
+    def __repr__(self):
+        return "\n".join(
+            [self.directory]
+            + [
+                f"{key} - {utils.pretty.humanize_bytes(os.path.getsize(self._get_path(key)))}"
+                for key in self.keys
+            ]
+        )
```

### Comparing `river-0.8.0/river/stream/iter_arff.py` & `river-0.9.0/river/stream/iter_arff.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,58 +1,58 @@
-from scipy.io.arff import arffread
-
-from river import base
-
-from . import utils
-
-
-def iter_arff(
-    filepath_or_buffer, target: str = None, compression="infer"
-) -> base.typing.Stream:
-    """Iterates over rows from an ARFF file.
-
-    Parameters
-    ----------
-    filepath_or_buffer
-        Either a string indicating the location of a CSV file, or a buffer object that has a
-        `read` method.
-    target
-        Name of the target field.
-    compression
-        For on-the-fly decompression of on-disk data. If this is set to 'infer' and
-        `filepath_or_buffer` is a path, then the decompression method is inferred for the
-        following extensions: '.gz', '.zip'.
-
-    """
-
-    # If a file is not opened, then we open it
-    buffer = filepath_or_buffer
-    if not hasattr(buffer, "read"):
-        buffer = utils.open_filepath(buffer, compression)
-
-    try:
-        rel, attrs = arffread.read_header(buffer)
-    except ValueError as e:
-        msg = f"Error while parsing header, error was: {e}"
-        raise arffread.ParseArffError(msg)
-
-    names = [attr.name for attr in attrs]
-    types = [
-        float if isinstance(attr, arffread.NumericAttribute) else None for attr in attrs
-    ]
-
-    for r in buffer:
-        x = {
-            name: typ(val) if typ else val
-            for name, typ, val in zip(names, types, r.rstrip().split(","))
-        }
-        try:
-            y = x.pop(target) if target else None
-        except KeyError as e:
-            print(r)
-            raise e
-
-        yield x, y
-
-    # Close the file if we opened it
-    if buffer is not filepath_or_buffer:
-        buffer.close()
+from scipy.io.arff import arffread
+
+from river import base
+
+from . import utils
+
+
+def iter_arff(
+    filepath_or_buffer, target: str = None, compression="infer"
+) -> base.typing.Stream:
+    """Iterates over rows from an ARFF file.
+
+    Parameters
+    ----------
+    filepath_or_buffer
+        Either a string indicating the location of a file, or a buffer object that has a
+        `read` method.
+    target
+        Name of the target field.
+    compression
+        For on-the-fly decompression of on-disk data. If this is set to 'infer' and
+        `filepath_or_buffer` is a path, then the decompression method is inferred for the
+        following extensions: '.gz', '.zip'.
+
+    """
+
+    # If a file is not opened, then we open it
+    buffer = filepath_or_buffer
+    if not hasattr(buffer, "read"):
+        buffer = utils.open_filepath(buffer, compression)
+
+    try:
+        rel, attrs = arffread.read_header(buffer)
+    except ValueError as e:
+        msg = f"Error while parsing header, error was: {e}"
+        raise arffread.ParseArffError(msg)
+
+    names = [attr.name for attr in attrs]
+    types = [
+        float if isinstance(attr, arffread.NumericAttribute) else None for attr in attrs
+    ]
+
+    for r in buffer:
+        x = {
+            name: typ(val) if typ else val
+            for name, typ, val in zip(names, types, r.rstrip().split(","))
+        }
+        try:
+            y = x.pop(target) if target else None
+        except KeyError as e:
+            print(r)
+            raise e
+
+        yield x, y
+
+    # Close the file if we opened it
+    if buffer is not filepath_or_buffer:
+        buffer.close()
```

### Comparing `river-0.8.0/river/stream/iter_csv.py` & `river-0.9.0/river/stream/iter_csv.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,185 +1,185 @@
-import csv
-import datetime as dt
-import random
-import typing
-
-from .. import base
-from . import utils
-
-__all__ = ["iter_csv"]
-
-
-class DictReader(csv.DictReader):
-    """Overlay on top of `csv.DictReader` which allows sampling."""
-
-    def __init__(self, fraction, rng, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        self.fraction = fraction
-        self.rng = rng
-
-    def __next__(self):
-
-        if self.line_num == 0:
-            self.fieldnames
-
-        row = next(self.reader)
-
-        if self.fraction < 1:
-            while self.rng.random() > self.fraction:
-                row = next(self.reader)
-
-        return dict(zip(self.fieldnames, row))
-
-
-def iter_csv(
-    filepath_or_buffer,
-    target: typing.Union[str, typing.List[str]] = None,
-    converters: dict = None,
-    parse_dates: dict = None,
-    drop: typing.List[str] = None,
-    drop_nones=False,
-    fraction=1.0,
-    compression="infer",
-    seed: int = None,
-    field_size_limit: int = None,
-    **kwargs
-) -> base.typing.Stream:
-    """Iterates over rows from a CSV file.
-
-    Reading CSV files can be quite slow. If, for whatever reason, you're going to loop through
-    the same file multiple times, then we recommend that you to use the `stream.Cache` utility.
-
-    Parameters
-    ----------
-    filepath_or_buffer
-        Either a string indicating the location of a CSV file, or a buffer object that has a
-        `read` method.
-    target
-        A single target column is assumed if a string is passed. A multiple output scenario
-        is assumed if a list of strings is passed. A `None` value will be assigned to each `y`
-        if this parameter is omitted.
-    converters
-        A `dict` mapping feature names to callables used to parse their associated values.
-    parse_dates
-        A `dict` mapping feature names to a format passed to the `datetime.datetime.strptime`
-        method.
-    drop
-        Fields to ignore.
-    drop_nones
-        Whether or not to drop fields where the value is a `None`.
-    fraction
-        Sampling fraction.
-    compression
-        For on-the-fly decompression of on-disk data. If this is set to 'infer' and
-        `filepath_or_buffer` is a path, then the decompression method is inferred for the
-        following extensions: '.gz', '.zip'.
-    seed
-        If specified, the sampling will be deterministic.
-    field_size_limit
-        If not `None`, this will be passed to the `csv.field_size_limit` function.
-    kwargs
-        All other keyword arguments are passed to the underlying `csv.DictReader`.
-
-    Returns:
-        By default each feature value will be of type `str`. You can use the `converters` and
-        `parse_dates` parameters to convert them as you see fit.
-
-    Examples
-    --------
-
-    Although this function is designed to handle different kinds of inputs, the most common
-    use case is to read a file on the disk. We'll first create a little CSV file to illustrate.
-
-    >>> tv_shows = '''name,year,rating
-    ... Planet Earth II,2016,9.5
-    ... Planet Earth,2006,9.4
-    ... Band of Brothers,2001,9.4
-    ... Breaking Bad,2008,9.4
-    ... Chernobyl,2019,9.4
-    ... '''
-    >>> with open('tv_shows.csv', mode='w') as f:
-    ...     _ = f.write(tv_shows)
-
-    We can now go through the rows one by one. We can use the `converters` parameter to cast
-    the `rating` field value as a `float`. We can also convert the `year` to a `datetime` via
-    the `parse_dates` parameter.
-
-    >>> from river import stream
-
-    >>> params = {
-    ...     'converters': {'rating': float},
-    ...     'parse_dates': {'year': '%Y'}
-    ... }
-    >>> for x, y in stream.iter_csv('tv_shows.csv', **params):
-    ...     print(x, y)
-    {'name': 'Planet Earth II', 'year': datetime.datetime(2016, 1, 1, 0, 0), 'rating': 9.5} None
-    {'name': 'Planet Earth', 'year': datetime.datetime(2006, 1, 1, 0, 0), 'rating': 9.4} None
-    {'name': 'Band of Brothers', 'year': datetime.datetime(2001, 1, 1, 0, 0), 'rating': 9.4} None
-    {'name': 'Breaking Bad', 'year': datetime.datetime(2008, 1, 1, 0, 0), 'rating': 9.4} None
-    {'name': 'Chernobyl', 'year': datetime.datetime(2019, 1, 1, 0, 0), 'rating': 9.4} None
-
-    The value of `y` is always `None` because we haven't provided a value for the `target`
-    parameter. Here is an example where a `target` is provided:
-
-    >>> dataset = stream.iter_csv('tv_shows.csv', target='rating', **params)
-    >>> for x, y in dataset:
-    ...     print(x, y)
-    {'name': 'Planet Earth II', 'year': datetime.datetime(2016, 1, 1, 0, 0)} 9.5
-    {'name': 'Planet Earth', 'year': datetime.datetime(2006, 1, 1, 0, 0)} 9.4
-    {'name': 'Band of Brothers', 'year': datetime.datetime(2001, 1, 1, 0, 0)} 9.4
-    {'name': 'Breaking Bad', 'year': datetime.datetime(2008, 1, 1, 0, 0)} 9.4
-    {'name': 'Chernobyl', 'year': datetime.datetime(2019, 1, 1, 0, 0)} 9.4
-
-    Finally, let's delete the example file.
-
-    >>> import os; os.remove('tv_shows.csv')
-
-    """
-
-    # Set the field size limit
-    limit = csv.field_size_limit()
-    if field_size_limit is not None:
-        csv.field_size_limit(field_size_limit)
-
-    # If a file is not opened, then we open it
-    buffer = filepath_or_buffer
-    if not hasattr(buffer, "read"):
-        buffer = utils.open_filepath(buffer, compression)
-
-    for x in DictReader(fraction=fraction, rng=random.Random(seed), f=buffer, **kwargs):
-
-        if drop:
-            for i in drop:
-                del x[i]
-
-        # Cast the values to the given types
-        if converters is not None:
-            for i, t in converters.items():
-                x[i] = t(x[i])
-
-        # Drop Nones
-        if drop_nones:
-            for i in list(x):
-                if x[i] is None:
-                    del x[i]
-
-        # Parse the dates
-        if parse_dates is not None:
-            for i, fmt in parse_dates.items():
-                x[i] = dt.datetime.strptime(x[i], fmt)
-
-        # Separate the target from the features
-        y = None
-        if isinstance(target, list):
-            y = {name: x.pop(name) for name in target}
-        elif target is not None:
-            y = x.pop(target)
-
-        yield x, y
-
-    # Close the file if we opened it
-    if buffer is not filepath_or_buffer:
-        buffer.close()
-
-    # Reset the file size limit to it's original value
-    csv.field_size_limit(limit)
+import csv
+import datetime as dt
+import random
+import typing
+
+from .. import base
+from . import utils
+
+__all__ = ["iter_csv"]
+
+
+class DictReader(csv.DictReader):
+    """Overlay on top of `csv.DictReader` which allows sampling."""
+
+    def __init__(self, fraction, rng, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self.fraction = fraction
+        self.rng = rng
+
+    def __next__(self):
+
+        if self.line_num == 0:
+            self.fieldnames
+
+        row = next(self.reader)
+
+        if self.fraction < 1:
+            while self.rng.random() > self.fraction:
+                row = next(self.reader)
+
+        return dict(zip(self.fieldnames, row))
+
+
+def iter_csv(
+    filepath_or_buffer,
+    target: typing.Union[str, typing.List[str]] = None,
+    converters: dict = None,
+    parse_dates: dict = None,
+    drop: typing.List[str] = None,
+    drop_nones=False,
+    fraction=1.0,
+    compression="infer",
+    seed: int = None,
+    field_size_limit: int = None,
+    **kwargs
+) -> base.typing.Stream:
+    """Iterates over rows from a CSV file.
+
+    Reading CSV files can be quite slow. If, for whatever reason, you're going to loop through
+    the same file multiple times, then we recommend that you to use the `stream.Cache` utility.
+
+    Parameters
+    ----------
+    filepath_or_buffer
+        Either a string indicating the location of a file, or a buffer object that has a
+        `read` method.
+    target
+        A single target column is assumed if a string is passed. A multiple output scenario
+        is assumed if a list of strings is passed. A `None` value will be assigned to each `y`
+        if this parameter is omitted.
+    converters
+        A `dict` mapping feature names to callables used to parse their associated values.
+    parse_dates
+        A `dict` mapping feature names to a format passed to the `datetime.datetime.strptime`
+        method.
+    drop
+        Fields to ignore.
+    drop_nones
+        Whether or not to drop fields where the value is a `None`.
+    fraction
+        Sampling fraction.
+    compression
+        For on-the-fly decompression of on-disk data. If this is set to 'infer' and
+        `filepath_or_buffer` is a path, then the decompression method is inferred for the
+        following extensions: '.gz', '.zip'.
+    seed
+        If specified, the sampling will be deterministic.
+    field_size_limit
+        If not `None`, this will be passed to the `csv.field_size_limit` function.
+    kwargs
+        All other keyword arguments are passed to the underlying `csv.DictReader`.
+
+    Returns:
+        By default each feature value will be of type `str`. You can use the `converters` and
+        `parse_dates` parameters to convert them as you see fit.
+
+    Examples
+    --------
+
+    Although this function is designed to handle different kinds of inputs, the most common
+    use case is to read a file on the disk. We'll first create a little CSV file to illustrate.
+
+    >>> tv_shows = '''name,year,rating
+    ... Planet Earth II,2016,9.5
+    ... Planet Earth,2006,9.4
+    ... Band of Brothers,2001,9.4
+    ... Breaking Bad,2008,9.4
+    ... Chernobyl,2019,9.4
+    ... '''
+    >>> with open('tv_shows.csv', mode='w') as f:
+    ...     _ = f.write(tv_shows)
+
+    We can now go through the rows one by one. We can use the `converters` parameter to cast
+    the `rating` field value as a `float`. We can also convert the `year` to a `datetime` via
+    the `parse_dates` parameter.
+
+    >>> from river import stream
+
+    >>> params = {
+    ...     'converters': {'rating': float},
+    ...     'parse_dates': {'year': '%Y'}
+    ... }
+    >>> for x, y in stream.iter_csv('tv_shows.csv', **params):
+    ...     print(x, y)
+    {'name': 'Planet Earth II', 'year': datetime.datetime(2016, 1, 1, 0, 0), 'rating': 9.5} None
+    {'name': 'Planet Earth', 'year': datetime.datetime(2006, 1, 1, 0, 0), 'rating': 9.4} None
+    {'name': 'Band of Brothers', 'year': datetime.datetime(2001, 1, 1, 0, 0), 'rating': 9.4} None
+    {'name': 'Breaking Bad', 'year': datetime.datetime(2008, 1, 1, 0, 0), 'rating': 9.4} None
+    {'name': 'Chernobyl', 'year': datetime.datetime(2019, 1, 1, 0, 0), 'rating': 9.4} None
+
+    The value of `y` is always `None` because we haven't provided a value for the `target`
+    parameter. Here is an example where a `target` is provided:
+
+    >>> dataset = stream.iter_csv('tv_shows.csv', target='rating', **params)
+    >>> for x, y in dataset:
+    ...     print(x, y)
+    {'name': 'Planet Earth II', 'year': datetime.datetime(2016, 1, 1, 0, 0)} 9.5
+    {'name': 'Planet Earth', 'year': datetime.datetime(2006, 1, 1, 0, 0)} 9.4
+    {'name': 'Band of Brothers', 'year': datetime.datetime(2001, 1, 1, 0, 0)} 9.4
+    {'name': 'Breaking Bad', 'year': datetime.datetime(2008, 1, 1, 0, 0)} 9.4
+    {'name': 'Chernobyl', 'year': datetime.datetime(2019, 1, 1, 0, 0)} 9.4
+
+    Finally, let's delete the example file.
+
+    >>> import os; os.remove('tv_shows.csv')
+
+    """
+
+    # Set the field size limit
+    limit = csv.field_size_limit()
+    if field_size_limit is not None:
+        csv.field_size_limit(field_size_limit)
+
+    # If a file is not opened, then we open it
+    buffer = filepath_or_buffer
+    if not hasattr(buffer, "read"):
+        buffer = utils.open_filepath(buffer, compression)
+
+    for x in DictReader(fraction=fraction, rng=random.Random(seed), f=buffer, **kwargs):
+
+        if drop:
+            for i in drop:
+                del x[i]
+
+        # Cast the values to the given types
+        if converters is not None:
+            for i, t in converters.items():
+                x[i] = t(x[i])
+
+        # Drop Nones
+        if drop_nones:
+            for i in list(x):
+                if x[i] is None:
+                    del x[i]
+
+        # Parse the dates
+        if parse_dates is not None:
+            for i, fmt in parse_dates.items():
+                x[i] = dt.datetime.strptime(x[i], fmt)
+
+        # Separate the target from the features
+        y = None
+        if isinstance(target, list):
+            y = {name: x.pop(name) for name in target}
+        elif target is not None:
+            y = x.pop(target)
+
+        yield x, y
+
+    # Close the file if we opened it
+    if buffer is not filepath_or_buffer:
+        buffer.close()
+
+    # Reset the file size limit to it's original value
+    csv.field_size_limit(limit)
```

### Comparing `river-0.8.0/river/stream/iter_libsvm.py` & `river-0.9.0/river/stream/iter_libsvm.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,73 +1,73 @@
-from river import base
-
-from . import utils
-
-
-def iter_libsvm(
-    filepath_or_buffer: str, target_type=float, compression="infer"
-) -> base.typing.Stream:
-    """Iterates over a dataset in LIBSVM format.
-
-    The LIBSVM format is a popular way in the machine learning community to store sparse datasets.
-    Only numerical feature values are supported. The feature names will be considered as strings.
-
-    Parameters
-    ----------
-    filepath_or_buffer
-        Either a string indicating the location of a CSV file, or a buffer object that has a `read`
-        method.
-    target_type
-        The type of the target value.
-    compression
-        For on-the-fly decompression of on-disk data. If this is set to 'infer' and
-        `filepath_or_buffer` is a path, then the decompression method is inferred for the
-        following extensions: '.gz', '.zip'.
-
-    Examples
-    --------
-
-    >>> import io
-    >>> from river import stream
-
-    >>> data = io.StringIO('''+1 x:-134.26 y:0.2563
-    ... 1 x:-12 z:0.3
-    ... -1 y:.25
-    ... ''')
-
-    >>> for x, y in stream.iter_libsvm(data, target_type=int):
-    ...     print(y, x)
-    1 {'x': -134.26, 'y': 0.2563}
-    1 {'x': -12.0, 'z': 0.3}
-    -1 {'y': 0.25}
-
-    References
-    ----------
-    [^1]: [LIBSVM documentation](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)
-
-    """
-
-    # If a file is not opened, then we open it
-    buffer = filepath_or_buffer
-    if not hasattr(buffer, "read"):
-        buffer = utils.open_filepath(buffer, compression)
-
-    def split_pair(pair):
-        name, value = pair.split(":")
-        value = float(value)
-        return name, value
-
-    for line in buffer:
-
-        # Remove carriage return and whitespace
-        line = line.rstrip()
-        # Remove potential end of line comments
-        line = line.split("#")[0]
-
-        y, x = line.split(" ", maxsplit=1)
-        y = target_type(y)
-        x = dict([split_pair(pair) for pair in x.split(" ")])
-        yield x, y
-
-    # Close the file if we opened it
-    if buffer is not filepath_or_buffer:
-        buffer.close()
+from river import base
+
+from . import utils
+
+
+def iter_libsvm(
+    filepath_or_buffer: str, target_type=float, compression="infer"
+) -> base.typing.Stream:
+    """Iterates over a dataset in LIBSVM format.
+
+    The LIBSVM format is a popular way in the machine learning community to store sparse datasets.
+    Only numerical feature values are supported. The feature names will be considered as strings.
+
+    Parameters
+    ----------
+    filepath_or_buffer
+        Either a string indicating the location of a file, or a buffer object that has a `read`
+        method.
+    target_type
+        The type of the target value.
+    compression
+        For on-the-fly decompression of on-disk data. If this is set to 'infer' and
+        `filepath_or_buffer` is a path, then the decompression method is inferred for the
+        following extensions: '.gz', '.zip'.
+
+    Examples
+    --------
+
+    >>> import io
+    >>> from river import stream
+
+    >>> data = io.StringIO('''+1 x:-134.26 y:0.2563
+    ... 1 x:-12 z:0.3
+    ... -1 y:.25
+    ... ''')
+
+    >>> for x, y in stream.iter_libsvm(data, target_type=int):
+    ...     print(y, x)
+    1 {'x': -134.26, 'y': 0.2563}
+    1 {'x': -12.0, 'z': 0.3}
+    -1 {'y': 0.25}
+
+    References
+    ----------
+    [^1]: [LIBSVM documentation](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)
+
+    """
+
+    # If a file is not opened, then we open it
+    buffer = filepath_or_buffer
+    if not hasattr(buffer, "read"):
+        buffer = utils.open_filepath(buffer, compression)
+
+    def split_pair(pair):
+        name, value = pair.split(":")
+        value = float(value)
+        return name, value
+
+    for line in buffer:
+
+        # Remove carriage return and whitespace
+        line = line.rstrip()
+        # Remove potential end of line comments
+        line = line.split("#")[0]
+
+        y, x = line.split(" ", maxsplit=1)
+        y = target_type(y)
+        x = dict([split_pair(pair) for pair in x.split(" ")])
+        yield x, y
+
+    # Close the file if we opened it
+    if buffer is not filepath_or_buffer:
+        buffer.close()
```

### Comparing `river-0.8.0/river/stream/iter_sklearn.py` & `river-0.9.0/river/stream/iter_sklearn.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,61 +1,58 @@
-import pandas as pd
-import sklearn.utils
-
-from river import base, stream
-
-
-def iter_sklearn_dataset(
-    dataset: "sklearn.utils.Bunch", **kwargs
-) -> base.typing.Stream:
-    """Iterates rows from one of the datasets provided by scikit-learn.
-
-    This allows you to use any dataset from [scikit-learn's `datasets` module](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets). For instance, you can use the `fetch_openml` function to get access to all of the
-    datasets from the OpenML website.
-
-    Parameters
-    ----------
-    dataset
-        A scikit-learn dataset.
-    kwargs
-        Extra keyword arguments are passed to the underlying call to `stream.iter_array`.
-
-    Examples
-    --------
-
-    >>> import pprint
-    >>> from sklearn import datasets
-    >>> from river import stream
-
-    >>> dataset = datasets.load_boston()
-
-    >>> for xi, yi in stream.iter_sklearn_dataset(dataset):
-    ...     pprint.pprint(xi)
-    ...     print(yi)
-    ...     break
-    {'AGE': 65.2,
-        'B': 396.9,
-        'CHAS': 0.0,
-        'CRIM': 0.00632,
-        'DIS': 4.09,
-        'INDUS': 2.31,
-        'LSTAT': 4.98,
-        'NOX': 0.538,
-        'PTRATIO': 15.3,
-        'RAD': 1.0,
-        'RM': 6.575,
-        'TAX': 296.0,
-        'ZN': 18.0}
-    24.0
-
-    """
-    kwargs["X"] = dataset.data
-    kwargs["y"] = dataset.target
-    try:
-        kwargs["feature_names"] = dataset.feature_names
-    except AttributeError:
-        pass
-
-    if isinstance(kwargs["X"], pd.DataFrame):
-        yield from stream.iter_pandas(**kwargs)
-    else:
-        yield from stream.iter_array(**kwargs)
+import pandas as pd
+import sklearn.utils
+
+from river import base, stream
+
+
+def iter_sklearn_dataset(
+    dataset: "sklearn.utils.Bunch", **kwargs
+) -> base.typing.Stream:
+    """Iterates rows from one of the datasets provided by scikit-learn.
+
+    This allows you to use any dataset from [scikit-learn's `datasets` module](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets). For instance, you can use the `fetch_openml` function to get access to all of the
+    datasets from the OpenML website.
+
+    Parameters
+    ----------
+    dataset
+        A scikit-learn dataset.
+    kwargs
+        Extra keyword arguments are passed to the underlying call to `stream.iter_array`.
+
+    Examples
+    --------
+
+    >>> import pprint
+    >>> from sklearn import datasets
+    >>> from river import stream
+
+    >>> dataset = datasets.load_diabetes()
+
+    >>> for xi, yi in stream.iter_sklearn_dataset(dataset):
+    ...     pprint.pprint(xi)
+    ...     print(yi)
+    ...     break
+    {'age': 0.0380759064334241,
+     'bmi': 0.0616962065186885,
+     'bp': 0.0218723549949558,
+     's1': -0.0442234984244464,
+     's2': -0.0348207628376986,
+     's3': -0.0434008456520269,
+     's4': -0.00259226199818282,
+     's5': 0.0199084208763183,
+     's6': -0.0176461251598052,
+     'sex': 0.0506801187398187}
+    151.0
+
+    """
+    kwargs["X"] = dataset.data
+    kwargs["y"] = dataset.target
+    try:
+        kwargs["feature_names"] = dataset.feature_names
+    except AttributeError:
+        pass
+
+    if isinstance(kwargs["X"], pd.DataFrame):
+        yield from stream.iter_pandas(**kwargs)
+    else:
+        yield from stream.iter_array(**kwargs)
```

### Comparing `river-0.8.0/river/stream/iter_sql.py` & `river-0.9.0/river/stream/iter_sql.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,93 +1,93 @@
-import typing
-
-import sqlalchemy
-
-from river import base
-
-__all__ = ["iter_sql"]
-
-
-def iter_sql(
-    query: typing.Union[str, sqlalchemy.sql.expression.Selectable],
-    conn: sqlalchemy.engine.Connectable,
-    target_name: str = None,
-) -> base.typing.Stream:
-    """Iterates over the results from an SQL query.
-
-    By default, SQLAlchemy prefetches results. Therefore, even though you can iterate over the
-    resulting rows one by one, the results are in fact loaded in batch. You can modify this
-    behavior by configuring the connection you pass to `iter_sql`. For instance, you can set
-    the `stream_results` parameter to `True`, as [explained in SQLAlchemy's documentation](https://docs.sqlalchemy.org/en/13/core/connections.html#sqlalchemy.engine.Connection.execution_options). Note, however,
-    that this isn't available for all database engines.
-
-    Parameters
-    ----------
-    query
-        SQL query to be executed.
-    conn
-        An SQLAlchemy construct which has an `execute` method. In other words you can pass an
-        engine, a connection, or a session.
-    target_name
-        The name of the target field. If this is `None`, then `y` will also be `None`.
-
-    Examples
-    --------
-
-    As an example we'll create an in-memory database with SQLAlchemy.
-
-    >>> import datetime as dt
-    >>> import sqlalchemy
-
-    >>> engine = sqlalchemy.create_engine('sqlite://')
-
-    >>> metadata = sqlalchemy.MetaData()
-
-    >>> t_sales = sqlalchemy.Table('sales', metadata,
-    ...     sqlalchemy.Column('shop', sqlalchemy.String, primary_key=True),
-    ...     sqlalchemy.Column('date', sqlalchemy.Date, primary_key=True),
-    ...     sqlalchemy.Column('amount', sqlalchemy.Integer)
-    ... )
-
-    >>> metadata.create_all(engine)
-
-    >>> sales = [
-    ...     {'shop': 'Hema', 'date': dt.date(2016, 8, 2), 'amount': 20},
-    ...     {'shop': 'Ikea', 'date': dt.date(2016, 8, 2), 'amount': 18},
-    ...     {'shop': 'Hema', 'date': dt.date(2016, 8, 3), 'amount': 22},
-    ...     {'shop': 'Ikea', 'date': dt.date(2016, 8, 3), 'amount': 14},
-    ...     {'shop': 'Hema', 'date': dt.date(2016, 8, 4), 'amount': 12},
-    ...     {'shop': 'Ikea', 'date': dt.date(2016, 8, 4), 'amount': 16}
-    ... ]
-
-    >>> with engine.connect() as conn:
-    ...     _ = conn.execute(t_sales.insert(), sales)
-
-    We can now query the database. We will set `amount` to be the target field.
-
-    >>> from river import stream
-
-    >>> with engine.connect() as conn:
-    ...     query = 'SELECT * FROM sales;'
-    ...     dataset = stream.iter_sql(query, conn, target_name='amount')
-    ...     for x, y in dataset:
-    ...         print(x, y)
-    {'shop': 'Hema', 'date': '2016-08-02'} 20
-    {'shop': 'Ikea', 'date': '2016-08-02'} 18
-    {'shop': 'Hema', 'date': '2016-08-03'} 22
-    {'shop': 'Ikea', 'date': '2016-08-03'} 14
-    {'shop': 'Hema', 'date': '2016-08-04'} 12
-    {'shop': 'Ikea', 'date': '2016-08-04'} 16
-
-    """
-
-    result_proxy = conn.execute(query)
-
-    if target_name is None:
-        for row in result_proxy:
-            yield dict(row._mapping.items()), None
-        return
-
-    for row in result_proxy:
-        x = dict(row._mapping.items())
-        y = x.pop(target_name)
-        yield x, y
+import typing
+
+import sqlalchemy
+
+from river import base
+
+__all__ = ["iter_sql"]
+
+
+def iter_sql(
+    query: typing.Union[str, sqlalchemy.sql.expression.Selectable],
+    conn: sqlalchemy.engine.Connectable,
+    target_name: str = None,
+) -> base.typing.Stream:
+    """Iterates over the results from an SQL query.
+
+    By default, SQLAlchemy prefetches results. Therefore, even though you can iterate over the
+    resulting rows one by one, the results are in fact loaded in batch. You can modify this
+    behavior by configuring the connection you pass to `iter_sql`. For instance, you can set
+    the `stream_results` parameter to `True`, as [explained in SQLAlchemy's documentation](https://docs.sqlalchemy.org/en/13/core/connections.html#sqlalchemy.engine.Connection.execution_options). Note, however,
+    that this isn't available for all database engines.
+
+    Parameters
+    ----------
+    query
+        SQL query to be executed.
+    conn
+        An SQLAlchemy construct which has an `execute` method. In other words you can pass an
+        engine, a connection, or a session.
+    target_name
+        The name of the target field. If this is `None`, then `y` will also be `None`.
+
+    Examples
+    --------
+
+    As an example we'll create an in-memory database with SQLAlchemy.
+
+    >>> import datetime as dt
+    >>> import sqlalchemy
+
+    >>> engine = sqlalchemy.create_engine('sqlite://')
+
+    >>> metadata = sqlalchemy.MetaData()
+
+    >>> t_sales = sqlalchemy.Table('sales', metadata,
+    ...     sqlalchemy.Column('shop', sqlalchemy.String, primary_key=True),
+    ...     sqlalchemy.Column('date', sqlalchemy.Date, primary_key=True),
+    ...     sqlalchemy.Column('amount', sqlalchemy.Integer)
+    ... )
+
+    >>> metadata.create_all(engine)
+
+    >>> sales = [
+    ...     {'shop': 'Hema', 'date': dt.date(2016, 8, 2), 'amount': 20},
+    ...     {'shop': 'Ikea', 'date': dt.date(2016, 8, 2), 'amount': 18},
+    ...     {'shop': 'Hema', 'date': dt.date(2016, 8, 3), 'amount': 22},
+    ...     {'shop': 'Ikea', 'date': dt.date(2016, 8, 3), 'amount': 14},
+    ...     {'shop': 'Hema', 'date': dt.date(2016, 8, 4), 'amount': 12},
+    ...     {'shop': 'Ikea', 'date': dt.date(2016, 8, 4), 'amount': 16}
+    ... ]
+
+    >>> with engine.connect() as conn:
+    ...     _ = conn.execute(t_sales.insert(), sales)
+
+    We can now query the database. We will set `amount` to be the target field.
+
+    >>> from river import stream
+
+    >>> with engine.connect() as conn:
+    ...     query = 'SELECT * FROM sales;'
+    ...     dataset = stream.iter_sql(query, conn, target_name='amount')
+    ...     for x, y in dataset:
+    ...         print(x, y)
+    {'shop': 'Hema', 'date': '2016-08-02'} 20
+    {'shop': 'Ikea', 'date': '2016-08-02'} 18
+    {'shop': 'Hema', 'date': '2016-08-03'} 22
+    {'shop': 'Ikea', 'date': '2016-08-03'} 14
+    {'shop': 'Hema', 'date': '2016-08-04'} 12
+    {'shop': 'Ikea', 'date': '2016-08-04'} 16
+
+    """
+
+    result_proxy = conn.execute(query)
+
+    if target_name is None:
+        for row in result_proxy:
+            yield dict(row._mapping.items()), None
+        return
+
+    for row in result_proxy:
+        x = dict(row._mapping.items())
+        y = x.pop(target_name)
+        yield x, y
```

### Comparing `river-0.8.0/river/stream/iter_vaex.py` & `river-0.9.0/river/stream/iter_vaex.py`

 * *Ordering differences only*

 * *Files 25% similar despite different names*

```diff
@@ -1,51 +1,51 @@
-import typing
-
-import vaex
-from vaex.utils import _ensure_list, _ensure_strings_from_expressions
-
-from river import base
-
-
-def iter_vaex(
-    X: vaex.dataframe.DataFrame,
-    y: typing.Union[str, vaex.expression.Expression] = None,
-    features: typing.Union[typing.List[str], vaex.expression.Expression] = None,
-) -> base.typing.Stream:
-    """Yields rows from a ``vaex.DataFrame``.
-
-    Parameters
-    ----------
-    X
-        A vaex DataFrame housing the training featuers.
-    y
-        The column or expression containing the target variable.
-    features
-        A list of features used for training. If None, all columns in `X` will be used. Features
-        specifying in `y` are ignored.
-
-    """
-
-    features = _ensure_strings_from_expressions(features)
-    feature_names = features or X.get_column_names()
-
-    if y:
-        y = _ensure_strings_from_expressions(y)
-        y = _ensure_list(y)
-        feature_names = [feat for feat in feature_names if feat not in y]
-
-    multioutput = len(y) > 1
-
-    if multioutput:
-        for i in range(len(X)):
-            yield (
-                {key: X.evaluate(key, i, i + 1)[0] for key in feature_names},
-                {key: X.evaluate(key, i, i + 1)[0] for key in y},
-            )
-
-    else:
-
-        for i in range(len(X)):
-            yield (
-                {key: X.evaluate(key, i, i + 1)[0] for key in feature_names},
-                X.evaluate(y[0], i, i + 1)[0],
-            )
+import typing
+
+import vaex
+from vaex.utils import _ensure_list, _ensure_strings_from_expressions
+
+from river import base
+
+
+def iter_vaex(
+    X: vaex.dataframe.DataFrame,
+    y: typing.Union[str, vaex.expression.Expression] = None,
+    features: typing.Union[typing.List[str], vaex.expression.Expression] = None,
+) -> base.typing.Stream:
+    """Yields rows from a ``vaex.DataFrame``.
+
+    Parameters
+    ----------
+    X
+        A vaex DataFrame housing the training featuers.
+    y
+        The column or expression containing the target variable.
+    features
+        A list of features used for training. If None, all columns in `X` will be used. Features
+        specifying in `y` are ignored.
+
+    """
+
+    features = _ensure_strings_from_expressions(features)
+    feature_names = features or X.get_column_names()
+
+    if y:
+        y = _ensure_strings_from_expressions(y)
+        y = _ensure_list(y)
+        feature_names = [feat for feat in feature_names if feat not in y]
+
+    multioutput = len(y) > 1
+
+    if multioutput:
+        for i in range(len(X)):
+            yield (
+                {key: X.evaluate(key, i, i + 1)[0] for key in feature_names},
+                {key: X.evaluate(key, i, i + 1)[0] for key in y},
+            )
+
+    else:
+
+        for i in range(len(X)):
+            yield (
+                {key: X.evaluate(key, i, i + 1)[0] for key in feature_names},
+                X.evaluate(y[0], i, i + 1)[0],
+            )
```

### Comparing `river-0.8.0/river/stream/pokedb.zip` & `river-0.9.0/river/stream/pokedb.zip`

 * *Files identical despite different names*

### Comparing `river-0.8.0/river/stream/qa.py` & `river-0.9.0/river/stream/qa.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,184 +1,184 @@
-import bisect
-import collections
-import datetime as dt
-import typing
-from copy import deepcopy
-
-from river import base
-
-__all__ = ["simulate_qa"]
-
-
-class Memento(collections.namedtuple("Memento", "i x y t_expire")):
-    def __lt__(self, other):
-        return self.t_expire < other.t_expire
-
-
-def simulate_qa(
-    dataset: base.typing.Stream,
-    moment: typing.Union[str, typing.Callable],
-    delay: typing.Union[str, int, dt.timedelta, typing.Callable],
-    copy: bool = True,
-):
-    """Simulate a time-ordered question and answer session.
-
-    This method allows looping through a dataset in the order in which it arrived. Indeed, it
-    usually is the case that labels arrive after features. Being able to go through a dataset in
-    arrival order enables assessing a model's performance in a reliable manner. For instance, the
-    `evaluate.progressive_val_score` is a high-level method that can be used to score a model on a
-    dataset. Under the hood it uses this method to determine the correct arrival order.
-
-    Parameters
-    ----------
-    dataset
-        A stream of (features, target) tuples.
-    moment
-        The attribute used for measuring time. If a callable is passed, then it is expected
-        to take as input a `dict` of features. If `None`, then the observations are implicitly
-        timestamped in the order in which they arrive. If a `str` is passed, then it will be
-        used to obtain the time from the input features.
-    delay
-        The amount of time to wait before revealing the target associated with each
-        observation to the model. This value is expected to be able to sum with the `moment`
-        value. For instance, if `moment` is a `datetime.date`, then `delay` is expected to be a
-        `datetime.timedelta`. If a callable is passed, then it is expected to take as input a
-        `dict` of features and the target. If a `str` is passed, then it will be used to access
-        the relevant field from the features. If `None` is passed, then no delay will be used,
-        which leads to doing standard online validation. If a scalar is passed, such an `int`
-        or a `datetime.timedelta`, then the delay is constant.
-    copy
-        If `True`, then a separate copy of the features are yielded the second time
-        around. This ensures that inadvertent modifications in downstream code don't have any
-        effect.
-
-    Examples
-    --------
-
-    The arrival delay isn't usually indicated in a dataset, but it might be able to be inferred
-    from the features. As an example, we'll simulate the departure and arrival time of taxi
-    trips. Let's first create a time table which records the departure time and the duration of
-    seconds of several taxi trips.
-
-    >>> import datetime as dt
-    >>> time_table = [
-    ...     (dt.datetime(2020, 1, 1, 20,  0, 0),  900),
-    ...     (dt.datetime(2020, 1, 1, 20, 10, 0), 1800),
-    ...     (dt.datetime(2020, 1, 1, 20, 20, 0),  300),
-    ...     (dt.datetime(2020, 1, 1, 20, 45, 0),  400),
-    ...     (dt.datetime(2020, 1, 1, 20, 50, 0),  240),
-    ...     (dt.datetime(2020, 1, 1, 20, 55, 0),  450)
-    ... ]
-
-    We can now create a streaming dataset where the features are the departure dates and the
-    targets are the durations.
-
-    >>> dataset = (
-    ...     ({'date': date}, duration)
-    ...     for date, duration in time_table
-    ... )
-
-    Now, we can use `simulate_qa` to iterate over the events in the order in which they are
-    meant to occur.
-
-    >>> delay = lambda _, y: dt.timedelta(seconds=y)
-
-    >>> for i, x, y in simulate_qa(dataset, moment='date', delay=delay):
-    ...     if y is None:
-    ...         print(f'{x["date"]} - trip #{i} departs')
-    ...     else:
-    ...         arrival_date = x['date'] + dt.timedelta(seconds=y)
-    ...         print(f'{arrival_date} - trip #{i} arrives after {y} seconds')
-    2020-01-01 20:00:00 - trip #0 departs
-    2020-01-01 20:10:00 - trip #1 departs
-    2020-01-01 20:15:00 - trip #0 arrives after 900 seconds
-    2020-01-01 20:20:00 - trip #2 departs
-    2020-01-01 20:25:00 - trip #2 arrives after 300 seconds
-    2020-01-01 20:40:00 - trip #1 arrives after 1800 seconds
-    2020-01-01 20:45:00 - trip #3 departs
-    2020-01-01 20:50:00 - trip #4 departs
-    2020-01-01 20:51:40 - trip #3 arrives after 400 seconds
-    2020-01-01 20:54:00 - trip #4 arrives after 240 seconds
-    2020-01-01 20:55:00 - trip #5 departs
-    2020-01-01 21:02:30 - trip #5 arrives after 450 seconds
-
-    This function is extremely practical because it provides a reliable way to evaluate the
-    performance of a model in a real scenario. Indeed, it allows to make predictions and
-    perform model updates in exactly the same manner that would happen live. For instance, it
-    is used in `evaluate.progressive_val_score`, which is a higher level function for
-    evaluating models in an online manner.
-
-    """
-
-    # Determine how to insert mementos into the queue
-    if callable(delay) or isinstance(delay, str):
-
-        def queue(q, el):
-            bisect.insort(q, el)
-
-    else:
-
-        def queue(q, el):
-            q.append(el)
-
-    # Coerce moment to a function
-    if isinstance(moment, str):
-
-        def get_moment(_, x):
-            return x[moment]
-
-    elif callable(moment):
-
-        def get_moment(_, x):
-            return moment(x)
-
-    else:
-
-        def get_moment(i, _):
-            return i
-
-    # Coerce delay to a function
-    if delay is None:
-
-        def get_delay(i, _):
-            return 0
-
-    elif isinstance(delay, str):
-
-        def get_delay(x, _):
-            return x[delay]
-
-    elif not callable(delay):
-
-        def get_delay(_, __):
-            return delay
-
-    else:
-        get_delay = delay
-
-    mementos: typing.List[Memento] = []
-
-    for i, (x, y) in enumerate(dataset):
-
-        t = get_moment(i, x)
-        d = get_delay(x, y)
-
-        while mementos:
-
-            # Get the oldest answer
-            i_old, x_old, y_old, t_expire = mementos[0]
-
-            # If the oldest answer isn't old enough then stop
-            if t_expire > t:
-                break
-
-            # Reveal the duration and pop the trip from the queue
-            yield i_old, x_old, y_old
-            del mementos[0]
-
-        queue(mementos, Memento(i, x, y, t + d))
-        if copy:
-            x = deepcopy(x)
-        yield i, x, None
-
-    for memento in mementos:
-        yield memento.i, memento.x, memento.y
+import bisect
+import collections
+import datetime as dt
+import typing
+from copy import deepcopy
+
+from river import base
+
+__all__ = ["simulate_qa"]
+
+
+class Memento(collections.namedtuple("Memento", "i x y t_expire")):
+    def __lt__(self, other):
+        return self.t_expire < other.t_expire
+
+
+def simulate_qa(
+    dataset: base.typing.Dataset,
+    moment: typing.Union[str, typing.Callable],
+    delay: typing.Union[str, int, dt.timedelta, typing.Callable],
+    copy: bool = True,
+):
+    """Simulate a time-ordered question and answer session.
+
+    This method allows looping through a dataset in the order in which it arrived. Indeed, it
+    usually is the case that labels arrive after features. Being able to go through a dataset in
+    arrival order enables assessing a model's performance in a reliable manner. For instance, the
+    `evaluate.progressive_val_score` is a high-level method that can be used to score a model on a
+    dataset. Under the hood it uses this method to determine the correct arrival order.
+
+    Parameters
+    ----------
+    dataset
+        A stream of (features, target) tuples.
+    moment
+        The attribute used for measuring time. If a callable is passed, then it is expected
+        to take as input a `dict` of features. If `None`, then the observations are implicitly
+        timestamped in the order in which they arrive. If a `str` is passed, then it will be
+        used to obtain the time from the input features.
+    delay
+        The amount of time to wait before revealing the target associated with each
+        observation to the model. This value is expected to be able to sum with the `moment`
+        value. For instance, if `moment` is a `datetime.date`, then `delay` is expected to be a
+        `datetime.timedelta`. If a callable is passed, then it is expected to take as input a
+        `dict` of features and the target. If a `str` is passed, then it will be used to access
+        the relevant field from the features. If `None` is passed, then no delay will be used,
+        which leads to doing standard online validation. If a scalar is passed, such an `int`
+        or a `datetime.timedelta`, then the delay is constant.
+    copy
+        If `True`, then a separate copy of the features are yielded the second time
+        around. This ensures that inadvertent modifications in downstream code don't have any
+        effect.
+
+    Examples
+    --------
+
+    The arrival delay isn't usually indicated in a dataset, but it might be able to be inferred
+    from the features. As an example, we'll simulate the departure and arrival time of taxi
+    trips. Let's first create a time table which records the departure time and the duration of
+    seconds of several taxi trips.
+
+    >>> import datetime as dt
+    >>> time_table = [
+    ...     (dt.datetime(2020, 1, 1, 20,  0, 0),  900),
+    ...     (dt.datetime(2020, 1, 1, 20, 10, 0), 1800),
+    ...     (dt.datetime(2020, 1, 1, 20, 20, 0),  300),
+    ...     (dt.datetime(2020, 1, 1, 20, 45, 0),  400),
+    ...     (dt.datetime(2020, 1, 1, 20, 50, 0),  240),
+    ...     (dt.datetime(2020, 1, 1, 20, 55, 0),  450)
+    ... ]
+
+    We can now create a streaming dataset where the features are the departure dates and the
+    targets are the durations.
+
+    >>> dataset = (
+    ...     ({'date': date}, duration)
+    ...     for date, duration in time_table
+    ... )
+
+    Now, we can use `simulate_qa` to iterate over the events in the order in which they are
+    meant to occur.
+
+    >>> delay = lambda _, y: dt.timedelta(seconds=y)
+
+    >>> for i, x, y in simulate_qa(dataset, moment='date', delay=delay):
+    ...     if y is None:
+    ...         print(f'{x["date"]} - trip #{i} departs')
+    ...     else:
+    ...         arrival_date = x['date'] + dt.timedelta(seconds=y)
+    ...         print(f'{arrival_date} - trip #{i} arrives after {y} seconds')
+    2020-01-01 20:00:00 - trip #0 departs
+    2020-01-01 20:10:00 - trip #1 departs
+    2020-01-01 20:15:00 - trip #0 arrives after 900 seconds
+    2020-01-01 20:20:00 - trip #2 departs
+    2020-01-01 20:25:00 - trip #2 arrives after 300 seconds
+    2020-01-01 20:40:00 - trip #1 arrives after 1800 seconds
+    2020-01-01 20:45:00 - trip #3 departs
+    2020-01-01 20:50:00 - trip #4 departs
+    2020-01-01 20:51:40 - trip #3 arrives after 400 seconds
+    2020-01-01 20:54:00 - trip #4 arrives after 240 seconds
+    2020-01-01 20:55:00 - trip #5 departs
+    2020-01-01 21:02:30 - trip #5 arrives after 450 seconds
+
+    This function is extremely practical because it provides a reliable way to evaluate the
+    performance of a model in a real scenario. Indeed, it allows to make predictions and
+    perform model updates in exactly the same manner that would happen live. For instance, it
+    is used in `evaluate.progressive_val_score`, which is a higher level function for
+    evaluating models in an online manner.
+
+    """
+
+    # Determine how to insert mementos into the queue
+    if callable(delay) or isinstance(delay, str):
+
+        def queue(q, el):
+            bisect.insort(q, el)
+
+    else:
+
+        def queue(q, el):
+            q.append(el)
+
+    # Coerce moment to a function
+    if isinstance(moment, str):
+
+        def get_moment(_, x):
+            return x[moment]
+
+    elif callable(moment):
+
+        def get_moment(_, x):
+            return moment(x)
+
+    else:
+
+        def get_moment(i, _):
+            return i
+
+    # Coerce delay to a function
+    if delay is None:
+
+        def get_delay(i, _):
+            return 0
+
+    elif isinstance(delay, str):
+
+        def get_delay(x, _):
+            return x[delay]
+
+    elif not callable(delay):
+
+        def get_delay(_, __):
+            return delay
+
+    else:
+        get_delay = delay
+
+    mementos: typing.List[Memento] = []
+
+    for i, (x, y) in enumerate(dataset):
+
+        t = get_moment(i, x)
+        d = get_delay(x, y)
+
+        while mementos:
+
+            # Get the oldest answer
+            i_old, x_old, y_old, t_expire = mementos[0]
+
+            # If the oldest answer isn't old enough then stop
+            if t_expire > t:
+                break
+
+            # Reveal the duration and pop the trip from the queue
+            yield i_old, x_old, y_old
+            del mementos[0]
+
+        queue(mementos, Memento(i, x, y, t + d))
+        if copy:
+            x = deepcopy(x)
+        yield i, x, None
+
+    for memento in mementos:
+        yield memento.i, memento.x, memento.y
```

### Comparing `river-0.8.0/river/stream/shuffling.py` & `river-0.9.0/river/stream/shuffling.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,79 +1,79 @@
-import itertools
-import random
-import types
-import typing
-
-
-def shuffle(stream: typing.Iterator, buffer_size: int, seed: int = None):
-    """Shuffles a stream of data.
-
-    This works by maintaining a buffer of elements. The first `buffer_size` elements are stored in
-    memory. Once the buffer is full, a random element inside the buffer is yielded. Every time an
-    element is yielded, the next element in the stream replaces it and the buffer is sampled again.
-    Increasing `buffer_size` will improve the quality of the shuffling.
-
-    If you really want to stream over your dataset in a "good" random order, the best way is to
-    split your dataset into smaller datasets and loop over them in a round-robin fashion. You may
-    do this by using the ``roundrobin`` recipe from the `itertools` module.
-
-    Parameters
-    ----------
-    stream
-        The stream to shuffle.
-    buffer_size
-        The size of the buffer which contains the elements help in memory. Increasing this will
-        increase randomness but will incur more memory usage.
-    seed
-        Random seed used for sampling.
-
-    Examples
-    --------
-
-    >>> from river import stream
-
-    >>> for i in stream.shuffle(range(15), buffer_size=5, seed=42):
-    ...     print(i)
-    0
-    5
-    2
-    1
-    8
-    9
-    6
-    4
-    11
-    12
-    10
-    7
-    14
-    13
-    3
-
-    References
-    ----------
-    [^1]: [Visualizing TensorFlow's streaming shufflers](http://www.moderndescartes.com/essays/shuffle_viz/)
-
-    """
-
-    rng = random.Random(seed)
-
-    # If stream is not a generator, then we coerce it to one
-    if not isinstance(stream, types.GeneratorType):
-        stream = iter(stream)
-
-    # Initialize the buffer with the first buffer_size elements of the stream
-    buffer = list(itertools.islice(stream, buffer_size))
-
-    # Deplete the stream until it is empty
-    for element in stream:
-
-        # Pick a random element from the buffer and yield it
-        i = rng.randint(0, len(buffer) - 1)
-        yield buffer[i]
-
-        # Replace the yielded element from the buffer with the new element from the stream
-        buffer[i] = element
-
-    # Shuffle the remaining buffer elements and yield them one by one
-    rng.shuffle(buffer)
-    yield from buffer
+import itertools
+import random
+import types
+import typing
+
+
+def shuffle(stream: typing.Iterator, buffer_size: int, seed: int = None):
+    """Shuffles a stream of data.
+
+    This works by maintaining a buffer of elements. The first `buffer_size` elements are stored in
+    memory. Once the buffer is full, a random element inside the buffer is yielded. Every time an
+    element is yielded, the next element in the stream replaces it and the buffer is sampled again.
+    Increasing `buffer_size` will improve the quality of the shuffling.
+
+    If you really want to stream over your dataset in a "good" random order, the best way is to
+    split your dataset into smaller datasets and loop over them in a round-robin fashion. You may
+    do this by using the ``roundrobin`` recipe from the `itertools` module.
+
+    Parameters
+    ----------
+    stream
+        The stream to shuffle.
+    buffer_size
+        The size of the buffer which contains the elements help in memory. Increasing this will
+        increase randomness but will incur more memory usage.
+    seed
+        Random seed used for sampling.
+
+    Examples
+    --------
+
+    >>> from river import stream
+
+    >>> for i in stream.shuffle(range(15), buffer_size=5, seed=42):
+    ...     print(i)
+    0
+    5
+    2
+    1
+    8
+    9
+    6
+    4
+    11
+    12
+    10
+    7
+    14
+    13
+    3
+
+    References
+    ----------
+    [^1]: [Visualizing TensorFlow's streaming shufflers](http://www.moderndescartes.com/essays/shuffle_viz/)
+
+    """
+
+    rng = random.Random(seed)
+
+    # If stream is not a generator, then we coerce it to one
+    if not isinstance(stream, types.GeneratorType):
+        stream = iter(stream)
+
+    # Initialize the buffer with the first buffer_size elements of the stream
+    buffer = list(itertools.islice(stream, buffer_size))
+
+    # Deplete the stream until it is empty
+    for element in stream:
+
+        # Pick a random element from the buffer and yield it
+        i = rng.randint(0, len(buffer) - 1)
+        yield buffer[i]
+
+        # Replace the yielded element from the buffer with the new element from the stream
+        buffer[i] = element
+
+    # Shuffle the remaining buffer elements and yield them one by one
+    rng.shuffle(buffer)
+    yield from buffer
```

### Comparing `river-0.8.0/river/time_series/base.py` & `river-0.9.0/river/time_series/base.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,36 +1,36 @@
-import abc
-
-from .. import base
-
-
-class Forecaster(base.Estimator):
-    @property
-    def _supervised(self):
-        return True
-
-    def learn_one(self, y: float, x: dict = None) -> "Forecaster":
-        """Updates the model.
-
-        Parameters
-        ----------
-        y
-            In the literature this is called the endogenous variable.
-        x
-            Optional additional features to learn from. In the literature these are called the
-            exogenous variables.
-
-        """
-
-    @abc.abstractmethod
-    def forecast(self, horizon: int, xs: list = None) -> list:
-        """Makes forecast at each step of the given horizon.
-
-        Parameters
-        ----------
-        horizon
-            The number of steps ahead to forecast.
-        xs
-            The set of optional additional features. If given, then it's length should be equal to
-            the horizon.
-
-        """
+import abc
+
+from .. import base
+
+
+class Forecaster(base.Estimator):
+    @property
+    def _supervised(self):
+        return True
+
+    def learn_one(self, y: float, x: dict = None) -> "Forecaster":
+        """Updates the model.
+
+        Parameters
+        ----------
+        y
+            In the literature this is called the endogenous variable.
+        x
+            Optional additional features to learn from. In the literature these are called the
+            exogenous variables.
+
+        """
+
+    @abc.abstractmethod
+    def forecast(self, horizon: int, xs: list = None) -> list:
+        """Makes forecast at each step of the given horizon.
+
+        Parameters
+        ----------
+        horizon
+            The number of steps ahead to forecast.
+        xs
+            The set of optional additional features. If given, then it's length should be equal to
+            the horizon.
+
+        """
```

### Comparing `river-0.8.0/river/time_series/snarimax.py` & `river-0.9.0/river/time_series/snarimax.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,409 +1,391 @@
-import collections
-import itertools
-import math
-import typing
-
-import river.base
-from river import linear_model, preprocessing
-
-from . import base
-
-__all__ = ["SNARIMAX"]
-
-
-def make_coeffs(d, m):
-    """Precomputes the coefficients of the backshift operator.
-
-    Examples
-    --------
-
-    >>> make_coeffs(1, 1)
-    {0: -1}
-
-    >>> make_coeffs(2, 1)
-    {0: -2, 1: 1}
-
-    >>> make_coeffs(3, 1)
-    {0: -3, 1: 3, 2: -1}
-
-    >>> make_coeffs(2, 7)
-    {6: -2, 13: 1}
-
-    """
-
-    def n_choose_k(n, k):
-        f = math.factorial
-        return f(n) // f(k) // f(n - k)
-
-    return dict(
-        (k * m - 1, int(math.copysign(1, (k + 1) % 2 - 1)) * n_choose_k(n=d, k=k))
-        for k in range(1, d + 1)
-    )
-
-
-class Differencer:
-    """A time series differencer.
-
-    Examples
-    --------
-
-    >>> differencer = Differencer(2); differencer.coeffs
-    {0: -2, 1: 1}
-
-    >>> differencer.diff(7, [3, 1])
-    2
-
-    >>> differencer.undiff(2, [3, 1])
-    7
-
-    References
-    ----------
-    [^1]: [Stationarity and differencing](https://otexts.com/fpp2/stationarity.html)
-
-    """
-
-    def __init__(self, d, m=1):
-
-        if d < 0:
-            raise ValueError("d must be greater than or equal to 0")
-
-        if m < 1:
-            raise ValueError("m must be greater than or equal to 1")
-
-        self.coeffs = make_coeffs(d=d, m=m)
-
-    def __add__(self, other):
-        """Composes two differencers together.
-
-        Examples
-        --------
-
-        >>> differencer = Differencer(d=3, m=2) + Differencer(d=3, m=1)
-        >>> for t, c in sorted(differencer.coeffs.items()):
-        ...     print(t, c)
-        0 -3
-        2 8
-        3 -6
-        4 -6
-        5 8
-        7 -3
-        8 1
-
-        References
-        ----------
-        [^1]: [Backshift notation](https://otexts.com/fpp2/backshift.html)
-
-        """
-        coeffs = collections.Counter()
-        coeffs.update(self.coeffs)
-        coeffs.update(other.coeffs)
-
-        for (t1, c1), (t2, c2) in itertools.product(
-            self.coeffs.items(), other.coeffs.items()
-        ):
-            coeffs[t1 + t2 + 1] += c1 * c2
-
-        # Remove 0 coefficients
-        for t in list(coeffs.keys()):
-            if coeffs[t] == 0:
-                del coeffs[t]
-
-        differencer = Differencer(0, 1)
-        differencer.coeffs = dict(coeffs)
-        return differencer
-
-    def diff(self, y: float, y_previous: list):
-        """Differentiates a value.
-
-        Parameters
-        ----------
-        y
-            The value to differentiate.
-        y_previous
-            The window of previous values. The first element is assumed to be the most recent
-            value.
-
-        """
-        return y + sum(
-            c * y_previous[t] for t, c in self.coeffs.items() if t < len(y_previous)
-        )
-
-    def undiff(self, y: float, y_previous: typing.List[float]):
-        """Undifferentiates a value.
-
-        y
-            The value to differentiate.
-        y_previous
-            The window of previous values. The first element is assumed to be the most recent
-            value.
-
-        """
-        return y - sum(
-            c * y_previous[t] for t, c in self.coeffs.items() if t < len(y_previous)
-        )
-
-
-class SNARIMAX(base.Forecaster):
-    """SNARIMAX model.
-
-    SNARIMAX stands for (S)easonal (N)on-linear (A)uto(R)egressive (I)ntegrated (M)oving-(A)verage
-    with e(X)ogenous inputs model.
-
-    This model generalizes many established time series models in a single interface that can be
-    trained online. It assumes that the provided training data is ordered in time and is uniformly
-    spaced. It is made up of the following components:
-
-    - S (Seasonal)
-    - N (Non-linear): Any online regression model can be used, not necessarily a linear regression
-        as is done in textbooks.
-    - AR (Autoregressive): Lags of the target variable are used as features.
-    - I (Integrated): The model can be fitted on a differenced version of a time series. In this
-        context, integration is the reverse of differencing.
-    - MA (Moving average): Lags of the errors are used as features.
-    - X (Exogenous): Users can provide additional features. Care has to be taken to include
-        features that will be available both at training and prediction time.
-
-    Each of these components can be switched on and off by specifying the appropriate parameters.
-    Classical time series models such as AR, MA, ARMA, and ARIMA can thus be seen as special
-    parametrizations of the SNARIMAX model.
-
-    This model is tailored for time series that are homoskedastic. In other words, it might not
-    work well if the variance of the time series varies widely along time.
-
-    Parameters
-    ----------
-    p
-        Order of the autoregressive part. This is the number of past target values that will be
-        included as features.
-    d
-        Differencing order.
-    q
-        Order of the moving average part. This is the number of past error terms that will be
-        included as features.
-    m
-        Season length used for extracting seasonal features. If you believe your data has a
-        seasonal pattern, then set this accordingly. For instance, if the data seems to exhibit
-        a yearly seasonality, and that your data is spaced by month, then you should set this
-        to 12. Note that for this parameter to have any impact you should also set at least one
-        of the `p`, `d`, and `q` parameters.
-    sp
-        Seasonal order of the autoregressive part. This is the number of past target values
-        that will be included as features.
-    sd
-        Seasonal differencing order.
-    sq
-        Seasonal order of the moving average part. This is the number of past error terms that
-        will be included as features.
-    regressor
-        The online regression model to use. By default, a `preprocessing.StandardScaler`
-        piped with a `linear_model.LinearRegression` will be used.
-
-    Attributes
-    ----------
-    differencer : Differencer
-    y_trues : collections.deque
-        The `p` past target values.
-    errors : collections.deque
-        The `q` past error values.
-
-    Examples
-    --------
-
-    >>> import calendar
-    >>> import datetime as dt
-    >>> from river import compose
-    >>> from river import datasets
-    >>> from river import linear_model
-    >>> from river import metrics
-    >>> from river import optim
-    >>> from river import preprocessing
-    >>> from river import time_series
-
-    >>> def get_month_distances(x):
-    ...     return {
-    ...         calendar.month_name[month]: math.exp(-(x['month'].month - month) ** 2)
-    ...         for month in range(1, 13)
-    ...     }
-
-    >>> def get_ordinal_date(x):
-    ...     return {'ordinal_date': x['month'].toordinal()}
-
-    >>> extract_features = compose.TransformerUnion(
-    ...     get_ordinal_date,
-    ...     get_month_distances
-    ... )
-
-    >>> model = (
-    ...     extract_features |
-    ...     time_series.SNARIMAX(
-    ...         p=0,
-    ...         d=0,
-    ...         q=0,
-    ...         m=12,
-    ...         sp=3,
-    ...         sq=6,
-    ...         regressor=(
-    ...             preprocessing.StandardScaler() |
-    ...             linear_model.LinearRegression(
-    ...                 intercept_init=110,
-    ...                 optimizer=optim.SGD(0.01),
-    ...                 intercept_lr=0.3
-    ...             )
-    ...         )
-    ...     )
-    ... )
-
-    >>> metric = metrics.Rolling(metrics.MAE(), 12)
-
-    >>> for x, y in datasets.AirlinePassengers():
-    ...     y_pred = model.forecast(horizon=1, xs=[x])
-    ...     model = model.learn_one(x, y)
-    ...     metric = metric.update(y, y_pred[0])
-
-    >>> metric
-    MAE: 11.636563  (rolling 12)
-
-    >>> horizon = 12
-    >>> future = [
-    ...     {'month': dt.date(year=1961, month=m, day=1)}
-    ...     for m in range(1, horizon + 1)
-    ... ]
-    >>> forecast = model.forecast(horizon=horizon, xs=future)
-    >>> for x, y_pred in zip(future, forecast):
-    ...     print(x['month'], f'{y_pred:.3f}')
-    1961-01-01 442.554
-    1961-02-01 427.305
-    1961-03-01 471.861
-    1961-04-01 483.978
-    1961-05-01 489.995
-    1961-06-01 544.270
-    1961-07-01 632.882
-    1961-08-01 633.229
-    1961-09-01 531.349
-    1961-10-01 457.258
-    1961-11-01 405.978
-    1961-12-01 439.674
-
-    References
-    ----------
-    [^1]: [Wikipedia page on ARMA](https://www.wikiwand.com/en/Autoregressive%E2%80%93moving-average_model)
-    [^2]: [Wikipedia page on NARX](https://www.wikiwand.com/en/Nonlinear_autoregressive_exogenous_model)
-    [^3]: [ARIMA models](https://otexts.com/fpp2/arima.html)
-    [^4]: [Anava, O., Hazan, E., Mannor, S. and Shamir, O., 2013, June. Online learning for time series prediction. In Conference on learning theory (pp. 172-184)](https://arxiv.org/pdf/1302.6927.pdf)
-
-    """
-
-    def __init__(
-        self,
-        p: int,
-        d: int,
-        q: int,
-        m: int = 1,
-        sp: int = 0,
-        sd: int = 0,
-        sq: int = 0,
-        regressor: river.base.Regressor = None,
-    ):
-
-        self.p = p
-        self.d = d
-        self.q = q
-        self.m = m
-        self.sp = sp
-        self.sd = sd
-        self.sq = sq
-        self.regressor = (
-            regressor
-            if regressor is not None
-            else preprocessing.StandardScaler() | linear_model.LinearRegression()
-        )
-        self.differencer = Differencer(d=d, m=1) + Differencer(d=sd, m=1)
-        self.y_trues = collections.deque(maxlen=max(p, m * sp))
-        self.errors = collections.deque(maxlen=max(p, m * sq))
-
-    def _add_lag_features(self, x, y_trues, errors):
-
-        if x is None:
-            x = {}
-
-        # AR
-        for t in range(self.p):
-            try:
-                x[f"y-{t+1}"] = y_trues[t]
-            except IndexError:
-                break
-
-        # Seasonal AR
-        for t in range(self.m - 1, (self.m - 1) * self.sp, self.m):
-            try:
-                x[f"sy-{t+1}"] = y_trues[t]
-            except IndexError:
-                break
-
-        # MA
-        for t in range(self.q):
-            try:
-                x[f"e-{t+1}"] = errors[t]
-            except IndexError:
-                break
-
-        # Seasonal MA
-        for t in range(self.m - 1, (self.m - 1) * self.sq, self.m):
-            try:
-                x[f"se-{t+1}"] = errors[t]
-            except IndexError:
-                break
-
-        return x
-
-    def _learn_predict_one(self, y: float, x: dict = None):
-        """Updates the model and returns the prediction for the next time step.
-
-        Parameters
-        ----------
-        x
-            Optional additional features to learn from. In the literature these are called the
-            exogenous variables.
-        y
-            In the literature this is called the endogenous variable.
-
-        """
-
-        # Check there are enough observations so that differencing can happen
-        y = self.differencer.diff(y=y, y_previous=self.y_trues)
-        x = self._add_lag_features(x=x, y_trues=self.y_trues, errors=self.errors)
-        y_pred = self.regressor.predict_one(x)
-        self.regressor.learn_one(x, y)
-
-        self.y_trues.appendleft(y)
-        self.errors.appendleft(y - y_pred)
-
-        return y_pred
-
-    def learn_one(self, y, x=None):
-        self._learn_predict_one(y=y, x=x)
-        return self
-
-    def forecast(self, horizon, xs=None):
-
-        if xs is None:
-            xs = [{}] * horizon
-
-        if len(xs) != horizon:
-            raise ValueError(
-                "the length of xs should be equal to the specified horizon"
-            )
-
-        y_trues = collections.deque(self.y_trues)
-        errors = collections.deque(self.errors)
-        forecasts = [None] * horizon
-
-        for t, x in enumerate(xs):
-            x = self._add_lag_features(x=x, y_trues=y_trues, errors=errors)
-            y_pred = self.regressor.predict_one(x)
-            forecasts[t] = self.differencer.undiff(y=y_pred, y_previous=y_trues)
-
-            y_trues.appendleft(y_pred)
-            errors.appendleft(0)
-
-        return forecasts
+import collections
+import itertools
+import math
+import typing
+
+import river.base
+from river import linear_model, preprocessing
+
+from . import base
+
+__all__ = ["SNARIMAX"]
+
+
+def make_coeffs(d, m):
+    """Precomputes the coefficients of the backshift operator.
+
+    Examples
+    --------
+
+    >>> make_coeffs(1, 1)
+    {0: -1}
+
+    >>> make_coeffs(2, 1)
+    {0: -2, 1: 1}
+
+    >>> make_coeffs(3, 1)
+    {0: -3, 1: 3, 2: -1}
+
+    >>> make_coeffs(2, 7)
+    {6: -2, 13: 1}
+
+    """
+
+    def n_choose_k(n, k):
+        f = math.factorial
+        return f(n) // f(k) // f(n - k)
+
+    return dict(
+        (k * m - 1, int(math.copysign(1, (k + 1) % 2 - 1)) * n_choose_k(n=d, k=k))
+        for k in range(1, d + 1)
+    )
+
+
+class Differencer:
+    """A time series differencer.
+
+    Examples
+    --------
+
+    >>> differencer = Differencer(2); differencer.coeffs
+    {0: -2, 1: 1}
+
+    >>> differencer.diff(7, [3, 1])
+    2
+
+    >>> differencer.undiff(2, [3, 1])
+    7
+
+    References
+    ----------
+    [^1]: [Stationarity and differencing](https://otexts.com/fpp2/stationarity.html)
+
+    """
+
+    def __init__(self, d, m=1):
+
+        if d < 0:
+            raise ValueError("d must be greater than or equal to 0")
+
+        if m < 1:
+            raise ValueError("m must be greater than or equal to 1")
+
+        self.coeffs = make_coeffs(d=d, m=m)
+
+    def __add__(self, other):
+        """Composes two differencers together.
+
+        Examples
+        --------
+
+        >>> differencer = Differencer(d=3, m=2) + Differencer(d=3, m=1)
+        >>> for t, c in sorted(differencer.coeffs.items()):
+        ...     print(t, c)
+        0 -3
+        2 8
+        3 -6
+        4 -6
+        5 8
+        7 -3
+        8 1
+
+        References
+        ----------
+        [^1]: [Backshift notation](https://otexts.com/fpp2/backshift.html)
+
+        """
+        coeffs = collections.Counter()
+        coeffs.update(self.coeffs)
+        coeffs.update(other.coeffs)
+
+        for (t1, c1), (t2, c2) in itertools.product(
+            self.coeffs.items(), other.coeffs.items()
+        ):
+            coeffs[t1 + t2 + 1] += c1 * c2
+
+        # Remove 0 coefficients
+        for t in list(coeffs.keys()):
+            if coeffs[t] == 0:
+                del coeffs[t]
+
+        differencer = Differencer(0, 1)
+        differencer.coeffs = dict(coeffs)
+        return differencer
+
+    def diff(self, y: float, y_previous: list):
+        """Differentiates a value.
+
+        Parameters
+        ----------
+        y
+            The value to differentiate.
+        y_previous
+            The window of previous values. The first element is assumed to be the most recent
+            value.
+
+        """
+        return y + sum(
+            c * y_previous[t] for t, c in self.coeffs.items() if t < len(y_previous)
+        )
+
+    def undiff(self, y: float, y_previous: typing.List[float]):
+        """Undifferentiates a value.
+
+        y
+            The value to differentiate.
+        y_previous
+            The window of previous values. The first element is assumed to be the most recent
+            value.
+
+        """
+        return y - sum(
+            c * y_previous[t] for t, c in self.coeffs.items() if t < len(y_previous)
+        )
+
+
+class SNARIMAX(base.Forecaster):
+    """SNARIMAX model.
+
+    SNARIMAX stands for (S)easonal (N)on-linear (A)uto(R)egressive (I)ntegrated (M)oving-(A)verage
+    with e(X)ogenous inputs model.
+
+    This model generalizes many established time series models in a single interface that can be
+    trained online. It assumes that the provided training data is ordered in time and is uniformly
+    spaced. It is made up of the following components:
+
+    - S (Seasonal)
+    - N (Non-linear): Any online regression model can be used, not necessarily a linear regression
+        as is done in textbooks.
+    - AR (Autoregressive): Lags of the target variable are used as features.
+    - I (Integrated): The model can be fitted on a differenced version of a time series. In this
+        context, integration is the reverse of differencing.
+    - MA (Moving average): Lags of the errors are used as features.
+    - X (Exogenous): Users can provide additional features. Care has to be taken to include
+        features that will be available both at training and prediction time.
+
+    Each of these components can be switched on and off by specifying the appropriate parameters.
+    Classical time series models such as AR, MA, ARMA, and ARIMA can thus be seen as special
+    parametrizations of the SNARIMAX model.
+
+    This model is tailored for time series that are homoskedastic. In other words, it might not
+    work well if the variance of the time series varies widely along time.
+
+    Parameters
+    ----------
+    p
+        Order of the autoregressive part. This is the number of past target values that will be
+        included as features.
+    d
+        Differencing order.
+    q
+        Order of the moving average part. This is the number of past error terms that will be
+        included as features.
+    m
+        Season length used for extracting seasonal features. If you believe your data has a
+        seasonal pattern, then set this accordingly. For instance, if the data seems to exhibit
+        a yearly seasonality, and that your data is spaced by month, then you should set this
+        to 12. Note that for this parameter to have any impact you should also set at least one
+        of the `p`, `d`, and `q` parameters.
+    sp
+        Seasonal order of the autoregressive part. This is the number of past target values
+        that will be included as features.
+    sd
+        Seasonal differencing order.
+    sq
+        Seasonal order of the moving average part. This is the number of past error terms that
+        will be included as features.
+    regressor
+        The online regression model to use. By default, a `preprocessing.StandardScaler`
+        piped with a `linear_model.LinearRegression` will be used.
+
+    Attributes
+    ----------
+    differencer : Differencer
+    y_trues : collections.deque
+        The `p` past target values.
+    errors : collections.deque
+        The `q` past error values.
+
+    Examples
+    --------
+
+    >>> import calendar
+    >>> import datetime as dt
+    >>> import math
+    >>> from river import compose
+    >>> from river import datasets
+    >>> from river import linear_model
+    >>> from river import metrics
+    >>> from river import optim
+    >>> from river import preprocessing
+    >>> from river import time_series
+
+    >>> def get_month_distances(x):
+    ...     return {
+    ...         calendar.month_name[month]: math.exp(-(x['month'].month - month) ** 2)
+    ...         for month in range(1, 13)
+    ...     }
+
+    >>> def get_ordinal_date(x):
+    ...     return {'ordinal_date': x['month'].toordinal()}
+
+    >>> extract_features = compose.TransformerUnion(
+    ...     get_ordinal_date,
+    ...     get_month_distances
+    ... )
+
+    >>> model = (
+    ...     extract_features |
+    ...     time_series.SNARIMAX(
+    ...         p=0,
+    ...         d=0,
+    ...         q=0,
+    ...         m=12,
+    ...         sp=3,
+    ...         sq=6,
+    ...         regressor=(
+    ...             preprocessing.StandardScaler() |
+    ...             linear_model.LinearRegression(
+    ...                 intercept_init=110,
+    ...                 optimizer=optim.SGD(0.01),
+    ...                 intercept_lr=0.3
+    ...             )
+    ...         )
+    ...     )
+    ... )
+
+    >>> metric = metrics.Rolling(metrics.MAE(), 12)
+
+    >>> for x, y in datasets.AirlinePassengers():
+    ...     y_pred = model.forecast(horizon=1, xs=[x])
+    ...     model = model.learn_one(x, y)
+    ...     metric = metric.update(y, y_pred[0])
+
+    >>> metric
+    MAE: 11.636563  (rolling 12)
+
+    >>> horizon = 12
+    >>> future = [
+    ...     {'month': dt.date(year=1961, month=m, day=1)}
+    ...     for m in range(1, horizon + 1)
+    ... ]
+    >>> forecast = model.forecast(horizon=horizon, xs=future)
+    >>> for x, y_pred in zip(future, forecast):
+    ...     print(x['month'], f'{y_pred:.3f}')
+    1961-01-01 442.554
+    1961-02-01 427.305
+    1961-03-01 471.861
+    1961-04-01 483.978
+    1961-05-01 489.995
+    1961-06-01 544.270
+    1961-07-01 632.882
+    1961-08-01 633.229
+    1961-09-01 531.349
+    1961-10-01 457.258
+    1961-11-01 405.978
+    1961-12-01 439.674
+
+    References
+    ----------
+    [^1]: [ARMA - Wikipedia](https://www.wikiwand.com/en/Autoregressive%E2%80%93moving-average_model)
+    [^2]: [NARX - Wikipedia](https://www.wikiwand.com/en/Nonlinear_autoregressive_exogenous_model)
+    [^3]: [ARIMA - Forecasting: Principles and Practice](https://otexts.com/fpp2/arima.html)
+    [^4]: [Anava, O., Hazan, E., Mannor, S. and Shamir, O., 2013, June. Online learning for time series prediction. In Conference on learning theory (pp. 172-184)](https://arxiv.org/pdf/1302.6927.pdf)
+
+    """
+
+    def __init__(
+        self,
+        p: int,
+        d: int,
+        q: int,
+        m: int = 1,
+        sp: int = 0,
+        sd: int = 0,
+        sq: int = 0,
+        regressor: river.base.Regressor = None,
+    ):
+
+        self.p = p
+        self.d = d
+        self.q = q
+        self.m = m
+        self.sp = sp
+        self.sd = sd
+        self.sq = sq
+        self.regressor = (
+            regressor
+            if regressor is not None
+            else preprocessing.StandardScaler() | linear_model.LinearRegression()
+        )
+        self.differencer = Differencer(d=d, m=1) + Differencer(d=sd, m=1)
+        self.y_trues = collections.deque(maxlen=max(p, m * sp))
+        self.errors = collections.deque(maxlen=max(p, m * sq))
+
+    def _add_lag_features(self, x, y_trues, errors):
+
+        if x is None:
+            x = {}
+
+        # AR
+        for t in range(self.p):
+            try:
+                x[f"y-{t+1}"] = y_trues[t]
+            except IndexError:
+                break
+
+        # Seasonal AR
+        for t in range(self.m - 1, (self.m - 1) * self.sp, self.m):
+            try:
+                x[f"sy-{t+1}"] = y_trues[t]
+            except IndexError:
+                break
+
+        # MA
+        for t in range(self.q):
+            try:
+                x[f"e-{t+1}"] = errors[t]
+            except IndexError:
+                break
+
+        # Seasonal MA
+        for t in range(self.m - 1, (self.m - 1) * self.sq, self.m):
+            try:
+                x[f"se-{t+1}"] = errors[t]
+            except IndexError:
+                break
+
+        return x
+
+    def learn_one(self, y, x=None):
+        y = self.differencer.diff(y=y, y_previous=self.y_trues)
+        x = self._add_lag_features(x=x, y_trues=self.y_trues, errors=self.errors)
+        y_pred = self.regressor.predict_one(x)
+        self.regressor.learn_one(x, y)
+        self.y_trues.appendleft(y)
+        self.errors.appendleft(y - y_pred)
+        return self
+
+    def forecast(self, horizon, xs=None):
+
+        if xs is None:
+            xs = [{}] * horizon
+
+        if len(xs) != horizon:
+            raise ValueError(
+                "the length of xs should be equal to the specified horizon"
+            )
+
+        y_trues = collections.deque(self.y_trues)
+        errors = collections.deque(self.errors)
+        forecasts = [None] * horizon
+
+        for t, x in enumerate(xs):
+            x = self._add_lag_features(x=x, y_trues=y_trues, errors=errors)
+            y_pred = self.regressor.predict_one(x)
+            forecasts[t] = self.differencer.undiff(y=y_pred, y_previous=y_trues)
+
+            y_trues.appendleft(y_pred)
+            errors.appendleft(0)
+
+        return forecasts
```

### Comparing `river-0.8.0/river/tree/__init__.py` & `river-0.9.0/river/tree/__init__.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,74 +1,74 @@
-"""
-
-This module implements incremental Decision Tree (iDT) algorithms for handling classification
-and regression tasks.
-
-Each family of iDT will be presented in a dedicated section.
-
-At any moment, iDT might face situations where an input feature previously used to make
-a split decision is missing in an incoming sample. In this case, the most traversed path is
-selected to pass down the instance. Moreover, in the case of nominal features, if a new category
-arises and the feature is used in a decision node, a new branch is created to accommodate the new
-value.
-
-**1. Hoeffding Trees**
-
-This family of iDT algorithms use the Hoeffding Bound to determine whether or not the
-incrementally computed best split candidates would be equivalent to the ones obtained in a
-batch-processing fashion.
-
-All the available Hoeffding Tree (HT) implementation share some common functionalities:
-
-* Set the maximum tree depth allowed (`max_depth`).
-
-* Handle *Active* and *Inactive* nodes: Active learning nodes update their own
-internal state to improve predictions and monitor input features to perform split
-attempts. Inactive learning nodes do not update their internal state and only keep the
-predictors; they are used to save memory in the tree (`max_size`).
-
-*  Enable/disable memory management.
-
-* Define strategies to sort leaves according to how likely they are going to be split.
-This enables deactivating non-promising leaves to save memory.
-
-* Disabling ‘poor’ attributes to save memory and speed up tree construction.
-A poor attribute is an input feature whose split merit is much smaller than the current
-best candidate. Once a feature is disabled, the tree stops saving statistics necessary
-to split such a feature.
-
-* Define properties to access leaf prediction strategies, split criteria, and other
-relevant characteristics.
-
-**2. Stochastic Gradient Trees**
-
-Stochastic Gradient Trees (SGT) directly optimize a loss function, rather than relying on split
-heuristics to guide the tree growth. F-tests are performed do decide whether a leaf should be
-expanded or its prediction value should be updated.
-
-SGTs can deal with binary classification and single-target regression. They also support
-dynamic and static feature quantizers to deal with numerical inputs.
-
-"""
-
-from . import splitter
-from .extremely_fast_decision_tree import ExtremelyFastDecisionTreeClassifier
-from .hoeffding_adaptive_tree_classifier import HoeffdingAdaptiveTreeClassifier
-from .hoeffding_adaptive_tree_regressor import HoeffdingAdaptiveTreeRegressor
-from .hoeffding_tree_classifier import HoeffdingTreeClassifier
-from .hoeffding_tree_regressor import HoeffdingTreeRegressor
-from .isoup_tree_regressor import iSOUPTreeRegressor
-from .label_combination_hoeffding_tree import LabelCombinationHoeffdingTreeClassifier
-from .stochastic_gradient_tree import SGTClassifier, SGTRegressor
-
-__all__ = [
-    "splitter",
-    "HoeffdingTreeClassifier",
-    "ExtremelyFastDecisionTreeClassifier",
-    "HoeffdingAdaptiveTreeClassifier",
-    "HoeffdingTreeRegressor",
-    "HoeffdingAdaptiveTreeRegressor",
-    "iSOUPTreeRegressor",
-    "LabelCombinationHoeffdingTreeClassifier",
-    "SGTClassifier",
-    "SGTRegressor",
-]
+"""
+
+This module implements incremental Decision Tree (iDT) algorithms for handling classification
+and regression tasks.
+
+Each family of iDT will be presented in a dedicated section.
+
+At any moment, iDT might face situations where an input feature previously used to make
+a split decision is missing in an incoming sample. In this case, the most traversed path is
+selected to pass down the instance. Moreover, in the case of nominal features, if a new category
+arises and the feature is used in a decision node, a new branch is created to accommodate the new
+value.
+
+**1. Hoeffding Trees**
+
+This family of iDT algorithms use the Hoeffding Bound to determine whether or not the
+incrementally computed best split candidates would be equivalent to the ones obtained in a
+batch-processing fashion.
+
+All the available Hoeffding Tree (HT) implementation share some common functionalities:
+
+* Set the maximum tree depth allowed (`max_depth`).
+
+* Handle *Active* and *Inactive* nodes: Active learning nodes update their own
+internal state to improve predictions and monitor input features to perform split
+attempts. Inactive learning nodes do not update their internal state and only keep the
+predictors; they are used to save memory in the tree (`max_size`).
+
+*  Enable/disable memory management.
+
+* Define strategies to sort leaves according to how likely they are going to be split.
+This enables deactivating non-promising leaves to save memory.
+
+* Disabling ‘poor’ attributes to save memory and speed up tree construction.
+A poor attribute is an input feature whose split merit is much smaller than the current
+best candidate. Once a feature is disabled, the tree stops saving statistics necessary
+to split such a feature.
+
+* Define properties to access leaf prediction strategies, split criteria, and other
+relevant characteristics.
+
+**2. Stochastic Gradient Trees**
+
+Stochastic Gradient Trees (SGT) directly optimize a loss function, rather than relying on split
+heuristics to guide the tree growth. F-tests are performed do decide whether a leaf should be
+expanded or its prediction value should be updated.
+
+SGTs can deal with binary classification and single-target regression. They also support
+dynamic and static feature quantizers to deal with numerical inputs.
+
+"""
+
+from . import splitter
+from .extremely_fast_decision_tree import ExtremelyFastDecisionTreeClassifier
+from .hoeffding_adaptive_tree_classifier import HoeffdingAdaptiveTreeClassifier
+from .hoeffding_adaptive_tree_regressor import HoeffdingAdaptiveTreeRegressor
+from .hoeffding_tree_classifier import HoeffdingTreeClassifier
+from .hoeffding_tree_regressor import HoeffdingTreeRegressor
+from .isoup_tree_regressor import iSOUPTreeRegressor
+from .label_combination_hoeffding_tree import LabelCombinationHoeffdingTreeClassifier
+from .stochastic_gradient_tree import SGTClassifier, SGTRegressor
+
+__all__ = [
+    "splitter",
+    "HoeffdingTreeClassifier",
+    "ExtremelyFastDecisionTreeClassifier",
+    "HoeffdingAdaptiveTreeClassifier",
+    "HoeffdingTreeRegressor",
+    "HoeffdingAdaptiveTreeRegressor",
+    "iSOUPTreeRegressor",
+    "LabelCombinationHoeffdingTreeClassifier",
+    "SGTClassifier",
+    "SGTRegressor",
+]
```

### Comparing `river-0.8.0/river/tree/base.py` & `river-0.9.0/river/tree/base.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,200 +1,195 @@
-"""
-
-This module defines generic branch and leaf implementations. These should be used in River by each
-tree-based model. Using these classes makes the code more DRY. The only exception for not doing so
-would be for performance, whereby a tree-based model uses a bespoke implementation.
-
-This module defines a bunch of methods to ease the manipulation and diagnostic of trees. Its
-intention is to provide utilities for walking over a tree and visualizing it.
-
-"""
-import abc
-from collections import defaultdict
-from queue import Queue
-from typing import Iterable, Tuple, Union
-from xml.etree import ElementTree as ET
-
-import pandas as pd
-
-from river.base import Base
-
-
-class Branch(Base, abc.ABC):
-    """A generic tree branch."""
-
-    def __init__(self, *children):
-        self.children = children
-
-    @abc.abstractmethod
-    def next(self, x) -> Union["Branch", "Leaf"]:
-        """Move to the next node down the tree."""
-
-    @abc.abstractmethod
-    def most_common_path(self) -> Tuple[int, Union["Leaf", "Branch"]]:
-        """Return a tuple with the branch index and the child node related to the most
-        traversed path.
-
-        Used in case the split feature is missing from an instance.
-        """
-        pass
-
-    @property
-    @abc.abstractmethod
-    def repr_split(self):
-        """String representation of the split."""
-
-    def walk(self, x, until_leaf=True) -> Iterable[Union["Branch", "Leaf"]]:
-        """Iterate over the nodes of the path induced by x."""
-        yield self
-        try:
-            yield from self.next(x).walk(x, until_leaf)
-        except KeyError:
-            if until_leaf:
-                _, node = self.most_common_path()
-                yield node
-                yield from node.walk(x, until_leaf)
-
-    def traverse(self, x, until_leaf=True) -> "Leaf":
-        """Return the leaf corresponding to the given input."""
-        for node in self.walk(x, until_leaf):
-            pass
-        return node  # noqa
-
-    @property
-    def n_nodes(self):
-        """Number of descendants, including thyself."""
-        return 1 + sum(child.n_nodes for child in self.children)
-
-    @property
-    def n_branches(self):
-        """Number of branches, including thyself."""
-        return 1 + sum(child.n_branches for child in self.children)
-
-    @property
-    def n_leaves(self):
-        """Number of leaves."""
-        return sum(child.n_leaves for child in self.children)
-
-    @property
-    def height(self):
-        """Distance to the deepest descendant."""
-        return 1 + max(child.height for child in self.children)
-
-    def iter_dfs(self):
-        """Iterate over nodes in depth-first order."""
-        yield self
-        for child in self.children:
-            yield from child.iter_dfs()
-
-    def iter_bfs(self):
-        """Iterate over nodes in breadth-first order."""
-
-        queue = Queue()
-
-        queue.put(self)
-
-        while not queue.empty():
-            node = queue.get()
-            yield node
-            if isinstance(node, Branch):
-                for child in node.children:
-                    queue.put(child)
-
-    def iter_leaves(self):
-        """Iterate over leaves from the left-most one to the right-most one."""
-        for child in self.children:
-            yield from child.iter_leaves()
-
-    def iter_branches(self):
-        """Iterate over branches in depth-first order."""
-        yield self
-        for child in self.children:
-            yield from child.iter_branches()
-
-    def iter_edges(self):
-        """Iterate over edges in depth-first order."""
-        for child in self.children:
-            yield self, child
-            yield from child.iter_edges()
-
-    def to_dataframe(self) -> pd.DataFrame:
-        """Build a DataFrame containing one record for each node."""
-
-        node_ids = defaultdict(lambda: len(node_ids))
-        nodes = []
-
-        queue = Queue()
-        queue.put((self, None, 0))
-
-        while not queue.empty():
-            node, parent, depth = queue.get()
-            nodes.append(
-                {
-                    "node": node_ids[id(node)],
-                    "parent": node_ids[id(parent)] if parent else pd.NA,
-                    "is_leaf": isinstance(node, Leaf),
-                    "depth": depth,
-                    **{k: v for k, v in node.__dict__.items() if k != "children"},
-                }
-            )
-            try:
-                for child in node.children:
-                    queue.put((child, node, depth + 1))
-            except AttributeError:
-                pass
-
-        return pd.DataFrame.from_records(nodes).set_index("node")
-
-    def _repr_html_(self):
-
-        from river.tree import viz
-
-        html = ET.Element("html")
-        body = ET.Element("body")
-        html.append(body)
-        body.append(viz.tree_to_html(self))
-
-        return f"<html>{ET.tostring(body).decode()}<style>{viz.CSS}</style></html>"
-
-
-class Leaf(Base):
-    """A generic tree node."""
-
-    def __init__(self, **kwargs):
-        self.__dict__.update(kwargs)
-
-    def walk(self, x, until_leaf=True):  # noqa
-        yield self
-
-    @property
-    @abc.abstractmethod
-    def __repr__(self):
-        """String representation for visualization purposes."""
-
-    @property
-    def n_nodes(self):
-        return 1
-
-    @property
-    def n_branches(self):
-        return 0
-
-    @property
-    def n_leaves(self):
-        return 1
-
-    @property
-    def height(self):
-        return 1
-
-    def iter_dfs(self):
-        yield self
-
-    def iter_leaves(self):
-        yield self
-
-    def iter_branches(self):  # noqa
-        yield from ()
-
-    def iter_edges(self):  # noqa
-        yield from ()
+"""
+
+This module defines generic branch and leaf implementations. These should be used in River by each
+tree-based model. Using these classes makes the code more DRY. The only exception for not doing so
+would be for performance, whereby a tree-based model uses a bespoke implementation.
+
+This module defines a bunch of methods to ease the manipulation and diagnostic of trees. Its
+intention is to provide utilities for walking over a tree and visualizing it.
+
+"""
+import abc
+from collections import defaultdict
+from queue import Queue
+from typing import Iterable, Tuple, Union
+from xml.etree import ElementTree as ET
+
+import pandas as pd
+
+from river.base import Base
+
+
+class Branch(Base, abc.ABC):
+    """A generic tree branch."""
+
+    def __init__(self, *children):
+        self.children = children
+
+    @abc.abstractmethod
+    def next(self, x) -> Union["Branch", "Leaf"]:
+        """Move to the next node down the tree."""
+
+    @abc.abstractmethod
+    def most_common_path(self) -> Tuple[int, Union["Leaf", "Branch"]]:
+        """Return a tuple with the branch index and the child node related to the most
+        traversed path.
+
+        Used in case the split feature is missing from an instance.
+        """
+        pass
+
+    @property
+    @abc.abstractmethod
+    def repr_split(self):
+        """String representation of the split."""
+
+    def walk(self, x, until_leaf=True) -> Iterable[Union["Branch", "Leaf"]]:
+        """Iterate over the nodes of the path induced by x."""
+        yield self
+        try:
+            yield from self.next(x).walk(x, until_leaf)
+        except KeyError:
+            if until_leaf:
+                _, node = self.most_common_path()
+                yield node
+                yield from node.walk(x, until_leaf)
+
+    def traverse(self, x, until_leaf=True) -> "Leaf":
+        """Return the leaf corresponding to the given input."""
+        for node in self.walk(x, until_leaf):
+            pass
+        return node  # noqa
+
+    @property
+    def n_nodes(self):
+        """Number of descendants, including thyself."""
+        return 1 + sum(child.n_nodes for child in self.children)
+
+    @property
+    def n_branches(self):
+        """Number of branches, including thyself."""
+        return 1 + sum(child.n_branches for child in self.children)
+
+    @property
+    def n_leaves(self):
+        """Number of leaves."""
+        return sum(child.n_leaves for child in self.children)
+
+    @property
+    def height(self):
+        """Distance to the deepest descendant."""
+        return 1 + max(child.height for child in self.children)
+
+    def iter_dfs(self):
+        """Iterate over nodes in depth-first order."""
+        yield self
+        for child in self.children:
+            yield from child.iter_dfs()
+
+    def iter_bfs(self):
+        """Iterate over nodes in breadth-first order."""
+
+        queue = Queue()
+
+        queue.put(self)
+
+        while not queue.empty():
+            node = queue.get()
+            yield node
+            if isinstance(node, Branch):
+                for child in node.children:
+                    queue.put(child)
+
+    def iter_leaves(self):
+        """Iterate over leaves from the left-most one to the right-most one."""
+        for child in self.children:
+            yield from child.iter_leaves()
+
+    def iter_branches(self):
+        """Iterate over branches in depth-first order."""
+        yield self
+        for child in self.children:
+            yield from child.iter_branches()
+
+    def iter_edges(self):
+        """Iterate over edges in depth-first order."""
+        for child in self.children:
+            yield self, child
+            yield from child.iter_edges()
+
+    def to_dataframe(self) -> pd.DataFrame:
+        """Build a DataFrame containing one record for each node."""
+
+        node_ids = defaultdict(lambda: len(node_ids))
+        nodes = []
+
+        queue = Queue()
+        queue.put((self, None, 0))
+
+        while not queue.empty():
+            node, parent, depth = queue.get()
+            nodes.append(
+                {
+                    "node": node_ids[id(node)],
+                    "parent": node_ids[id(parent)] if parent else pd.NA,
+                    "is_leaf": isinstance(node, Leaf),
+                    "depth": depth,
+                    **{k: v for k, v in node.__dict__.items() if k != "children"},
+                }
+            )
+            try:
+                for child in node.children:
+                    queue.put((child, node, depth + 1))
+            except AttributeError:
+                pass
+
+        return pd.DataFrame.from_records(nodes).set_index("node")
+
+    def _repr_html_(self):
+        from river.tree import viz
+
+        div = viz.tree_to_html(self)
+        return f"<div>{ET.tostring(div, encoding='unicode')}<style scoped>{viz.CSS}</style></div>"
+
+
+class Leaf(Base):
+    """A generic tree node."""
+
+    def __init__(self, **kwargs):
+        self.__dict__.update(kwargs)
+
+    def walk(self, x, until_leaf=True):  # noqa
+        yield self
+
+    @property
+    @abc.abstractmethod
+    def __repr__(self):
+        """String representation for visualization purposes."""
+
+    @property
+    def n_nodes(self):
+        return 1
+
+    @property
+    def n_branches(self):
+        return 0
+
+    @property
+    def n_leaves(self):
+        return 1
+
+    @property
+    def height(self):
+        return 1
+
+    def iter_dfs(self):
+        yield self
+
+    def iter_leaves(self):
+        yield self
+
+    def iter_branches(self):  # noqa
+        yield from ()
+
+    def iter_edges(self):  # noqa
+        yield from ()
```

### Comparing `river-0.8.0/river/tree/extremely_fast_decision_tree.py` & `river-0.9.0/river/tree/extremely_fast_decision_tree.py`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,582 +1,582 @@
-import typing
-
-from .hoeffding_tree_classifier import HoeffdingTreeClassifier
-from .nodes.branch import DTBranch
-from .nodes.efdtc_nodes import (
-    BaseEFDTBranch,
-    EFDTLeafMajorityClass,
-    EFDTLeafNaiveBayes,
-    EFDTLeafNaiveBayesAdaptive,
-    EFDTNominalBinaryBranch,
-    EFDTNominalMultiwayBranch,
-    EFDTNumericBinaryBranch,
-    EFDTNumericMultiwayBranch,
-)
-from .nodes.leaf import HTLeaf
-from .splitter import Splitter
-from .utils import BranchFactory
-
-
-class ExtremelyFastDecisionTreeClassifier(HoeffdingTreeClassifier):
-    """Extremely Fast Decision Tree classifier.
-
-    Also referred to as Hoeffding AnyTime Tree (HATT) classifier.
-
-    Parameters
-    ----------
-    grace_period
-        Number of instances a leaf should observe between split attempts.
-    max_depth
-        The maximum depth a tree can reach. If `None`, the tree will grow indefinitely.
-    min_samples_reevaluate
-        Number of instances a node should observe before reevaluating the best split.
-    split_criterion
-        Split criterion to use.</br>
-        - 'gini' - Gini</br>
-        - 'info_gain' - Information Gain</br>
-        - 'hellinger' - Helinger Distance</br>
-    split_confidence
-        Allowed error in split decision, a value closer to 0 takes longer to decide.
-    tie_threshold
-        Threshold below which a split will be forced to break ties.
-    leaf_prediction
-        Prediction mechanism used at leafs.</br>
-        - 'mc' - Majority Class</br>
-        - 'nb' - Naive Bayes</br>
-        - 'nba' - Naive Bayes Adaptive</br>
-    nb_threshold
-        Number of instances a leaf should observe before allowing Naive Bayes.
-    nominal_attributes
-        List of Nominal attributes identifiers. If empty, then assume that all numeric attributes
-        should be treated as continuous.
-    splitter
-        The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric
-        features and perform splits. Splitters are available in the `tree.splitter` module.
-        Different splitters are available for classification and regression tasks. Classification
-        and regression splitters can be distinguished by their property `is_target_class`.
-        This is an advanced option. Special care must be taken when choosing different splitters.
-        By default, `tree.splitter.GaussianSplitter` is used if `splitter` is `None`.
-    binary_split
-        If True, only allow binary splits.
-    max_size
-        The max size of the tree, in Megabytes (MB).
-    memory_estimate_period
-        Interval (number of processed instances) between memory consumption checks.
-    stop_mem_management
-        If True, stop growing as soon as memory limit is hit.
-    remove_poor_attrs
-        If True, disable poor attributes to reduce memory usage.
-    merit_preprune
-        If True, enable merit-based tree pre-pruning.
-
-    Notes
-    -----
-    The Extremely Fast Decision Tree (EFDT) [^1] constructs a tree incrementally. The EFDT seeks to
-    select and deploy a split as soon as it is confident the split is useful, and then revisits
-    that decision, replacing the split if it subsequently becomes evident that a better split is
-    available. The EFDT learns rapidly from a stationary distribution and eventually it learns the
-    asymptotic batch tree if the distribution from which the data are drawn is stationary.
-
-    References
-    ----------
-    [^1]:  C. Manapragada, G. Webb, and M. Salehi. Extremely Fast Decision Tree.
-    In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data
-    Mining (KDD '18). ACM, New York, NY, USA, 1953-1962.
-    DOI: https://doi.org/10.1145/3219819.3220005
-
-    Examples
-    --------
-    >>> from river import synth
-    >>> from river import evaluate
-    >>> from river import metrics
-    >>> from river import tree
-
-    >>> gen = synth.Agrawal(classification_function=0, seed=42)
-    >>> # Take 1000 instances from the infinite data generator
-    >>> dataset = iter(gen.take(1000))
-
-    >>> model = tree.ExtremelyFastDecisionTreeClassifier(
-    ...     grace_period=100,
-    ...     split_confidence=1e-5,
-    ...     nominal_attributes=['elevel', 'car', 'zipcode'],
-    ...     min_samples_reevaluate=100
-    ... )
-
-    >>> metric = metrics.Accuracy()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    Accuracy: 87.89%
-    """
-
-    def __init__(
-        self,
-        grace_period: int = 200,
-        max_depth: int = None,
-        min_samples_reevaluate: int = 20,
-        split_criterion: str = "info_gain",
-        split_confidence: float = 1e-7,
-        tie_threshold: float = 0.05,
-        leaf_prediction: str = "nba",
-        nb_threshold: int = 0,
-        nominal_attributes: list = None,
-        splitter: Splitter = None,
-        binary_split: bool = False,
-        max_size: int = 100,
-        memory_estimate_period: int = 1000000,
-        stop_mem_management: bool = False,
-        remove_poor_attrs: bool = False,
-        merit_preprune: bool = True,
-    ):
-
-        super().__init__(
-            grace_period=grace_period,
-            max_depth=max_depth,
-            split_criterion=split_criterion,
-            split_confidence=split_confidence,
-            tie_threshold=tie_threshold,
-            leaf_prediction=leaf_prediction,
-            nb_threshold=nb_threshold,
-            nominal_attributes=nominal_attributes,
-            splitter=splitter,
-            binary_split=binary_split,
-            max_size=max_size,
-            memory_estimate_period=memory_estimate_period,
-            stop_mem_management=stop_mem_management,
-            remove_poor_attrs=remove_poor_attrs,
-            merit_preprune=merit_preprune,
-        )
-
-        self.min_samples_reevaluate = min_samples_reevaluate
-
-    def _new_leaf(self, initial_stats=None, parent=None):
-        if initial_stats is None:
-            initial_stats = {}
-        if parent is None:
-            depth = 0
-        else:
-            depth = parent.depth + 1
-
-        if self._leaf_prediction == self._MAJORITY_CLASS:
-            return EFDTLeafMajorityClass(initial_stats, depth, self.splitter)
-        elif self._leaf_prediction == self._NAIVE_BAYES:
-            return EFDTLeafNaiveBayes(initial_stats, depth, self.splitter)
-        else:  # NAIVE BAYES ADAPTIVE (default)
-            return EFDTLeafNaiveBayesAdaptive(initial_stats, depth, self.splitter)
-
-    def _branch_selector(
-        self, numerical_feature=True, multiway_split=False
-    ) -> typing.Type[DTBranch]:
-        """Create a new split node."""
-        if numerical_feature:
-            if not multiway_split:
-                return EFDTNumericBinaryBranch
-            else:
-                return EFDTNumericMultiwayBranch
-        else:
-            if not multiway_split:
-                return EFDTNominalBinaryBranch
-            else:
-                return EFDTNominalMultiwayBranch
-
-    def learn_one(self, x, y, *, sample_weight=1.0):
-        """Incrementally train the model
-
-        Parameters
-        ----------
-        x
-            Instance attributes.
-        y
-            The label of the instance.
-        sample_weight
-            The weight of the sample.
-
-        Notes
-        -----
-        Training tasks:
-
-        * If the tree is empty, create a leaf node as the root.
-        * If the tree is already initialized, find the path from root to the corresponding leaf for
-        the instance and sort the instance.
-        * Reevaluate the best split for each internal node.
-        * Attempt to split the leaf.
-
-        Returns
-        -------
-        self
-        """
-        # Updates the set of observed classes
-        self.classes.add(y)
-
-        self._train_weight_seen_by_model += sample_weight
-
-        if self._root is None:
-            self._root = self._new_leaf()
-            self._n_active_leaves = 1
-
-        # Sort instance X into a leaf
-        self._sort_to_leaf(x, y, sample_weight)
-        # Process all nodes, starting from root to the leaf where the instance x belongs.
-        self._process_nodes(x, y, sample_weight, self._root, None, None)
-
-        return self
-
-    def _sort_to_leaf(self, x, y, sample_weight):
-        """For a given instance, find the corresponding leaf and update it.
-
-        Private function where leaf learn from instance.
-
-        1. Find the node where instance should be.
-        2. If no node have been found, create new learning node.
-        3.1 Update the node with the provided instance.
-
-        Parameters
-        ----------
-        x
-            Instance attributes.
-        y
-            The instance label.
-        sample_weight
-            The weight of the sample.
-
-        """
-        node = self._root
-        if isinstance(self._root, DTBranch):
-            node = self._root.traverse(x, until_leaf=False)
-            # Something went wrong in the way: a missing split feature or emerging category
-            # Let's deal with these situations
-            if isinstance(node, DTBranch):
-                while True:
-                    if node.max_branches() == -1 and node.feature in x:
-                        # Emerging feature in nominal feature: create a new branch
-                        leaf = self._new_leaf(parent=node)
-                        node.add_child(x[node.feature], leaf)
-                        self._n_active_leaves += 1
-                        node = leaf
-                    else:
-                        # Missing split feature: select the most traversed path to continue the
-                        # walk
-                        _, node = node.most_common_path()
-                        if isinstance(node, DTBranch):
-                            node = node.traverse(x, until_leaf=False)
-                    if isinstance(node, HTLeaf):
-                        break
-        node.learn_one(x, y, sample_weight=sample_weight, tree=self)
-
-        if self._train_weight_seen_by_model % self.memory_estimate_period == 0:
-            self._estimate_model_size()
-
-    def _process_nodes(self, x, y, sample_weight, node, parent, branch_index):
-        """Process nodes from the root to the leaf where the instance belongs.
-
-        1. If the node is internal:
-            1.1 If the number of samples seen since the last reevaluation are greater than
-            `min_samples_reevaluate`, reevaluate the best split for the internal node.
-        2. If the node is leaf, attempt to split leaf node.
-
-        Parameters
-        ----------
-        x
-            Instance attributes.
-        y
-            The label of the instance.
-        sample_weight
-            The weight of the sample.
-        node
-            The node to process.
-        parent
-            The node's parent.
-        branch_index
-            Parent node's branch index.
-        """
-        if isinstance(node, BaseEFDTBranch):
-            # Update split nodes as the tree is traversed
-            node.learn_one(x, y, sample_weight=sample_weight, tree=self)
-
-            old_weight = node.last_split_reevaluation_at
-            new_weight = node.total_weight
-            stop_flag = False
-
-            if (new_weight - old_weight) >= self.min_samples_reevaluate:
-                # Reevaluate the best split
-                stop_flag = self._reevaluate_best_split(
-                    node,
-                    parent,
-                    branch_index,
-                    # The attribute observer template that will be replicated to new leaves
-                    splitter=self.splitter,
-                    # Existing attribute observers that will be leveraged
-                    splitters=node.splitters,
-                )
-
-            if not stop_flag:
-                # Move in depth
-                try:
-                    child_index = node.branch_no(x)
-                    child = node.children[child_index]
-                except KeyError:
-                    child_index, child = node.most_common_path()
-                self._process_nodes(x, y, sample_weight, child, node, child_index)
-        elif self._growth_allowed and node.is_active():
-            if node.depth >= self.max_depth:  # Max depth reached
-                node.deactivate()
-                self._n_inactive_leaves += 1
-                self._n_active_leaves -= 1
-            else:
-                weight_seen = node.total_weight
-                weight_diff = weight_seen - node.last_split_attempt_at
-                if weight_diff >= self.grace_period:
-                    # Attempt to split
-                    self._attempt_to_split(
-                        node,
-                        parent,
-                        branch_index,
-                        splitter=self.splitter,
-                        splitters=node.splitters,
-                    )
-                    node.last_split_attempt_at = weight_seen
-
-    def _reevaluate_best_split(self, node, parent, branch_index, **kwargs):
-        """Reevaluate the best split for a node.
-
-        If the samples seen so far are not from the same class then:
-        1. Find split candidates and select the best one.
-        2. Compute the Hoeffding bound.
-        3. If the null split candidate is higher than the top split candidate:
-            3.1 Kill subtree and replace it with a leaf.
-            3.2 Update the tree.
-            3.3 Update tree's metrics
-        4. If the difference between the top split candidate and the current split is larger than
-        the Hoeffding bound:
-           4.1 Create a new split node.
-           4.2 Update the tree.
-           4.3 Update tree's metrics
-        5. If the top split candidate is the current split but with different split test:
-           5.1 Update the split test of the current split.
-
-        Parameters
-        ----------
-        node
-            The node to reevaluate.
-        parent
-            The node's parent.
-        branch_index
-            Parent node's branch index.
-        kwargs
-            Other parameters passed to the branch node.
-
-        Returns
-        -------
-            Flag to stop moving in depth.
-        """
-        stop_flag = False
-        if not node.observed_class_distribution_is_pure():
-            split_criterion = self._new_split_criterion()
-            best_split_suggestions = node.best_split_suggestions(split_criterion, self)
-            if len(best_split_suggestions) > 0:
-                # Sort the attribute accordingly to their split merit for each attribute
-                # (except the null one)
-                best_split_suggestions.sort()
-
-                # x_best is the attribute with the highest merit
-                x_best = best_split_suggestions[-1]
-                id_best = x_best.feature
-
-                # Best split candidate is the null split
-                if x_best.feature is None:
-                    return True
-
-                # x_current is the current attribute used in this SplitNode
-                id_current = node.feature
-                x_current = node.find_attribute(id_current, best_split_suggestions)
-
-                # Get x_null
-                x_null = BranchFactory(merit=0)
-
-                # Compute Hoeffding bound
-                hoeffding_bound = self._hoeffding_bound(
-                    split_criterion.range_of_merit(node.stats),
-                    self.split_confidence,
-                    node.total_weight,
-                )
-
-                if x_null.merit - x_best.merit > hoeffding_bound:
-                    # Kill subtree & replace the branch by a leaf
-                    best_split = self._kill_subtree(node)
-
-                    # update EFDT
-                    if parent is None:
-                        # Root case : replace the root node by a new split node
-                        self._root = best_split
-                    else:
-                        parent.children[branch_index] = best_split
-
-                    n_active = n_inactive = 0
-                    for leaf in node.iter_leaves():
-                        if leaf.is_active():
-                            n_active += 1
-                        else:
-                            n_inactive += 1
-
-                    self._n_active_leaves += 1
-                    self._n_active_leaves -= n_active
-                    self._n_inactive_leaves -= n_inactive
-                    stop_flag = True
-
-                    # Manage memory
-                    self._enforce_size_limit()
-
-                elif (
-                    x_best.merit - x_current.merit > hoeffding_bound
-                    or hoeffding_bound < self.tie_threshold
-                ) and (id_current != id_best):
-                    # Create a new branch
-                    branch = self._branch_selector(
-                        x_best.numerical_feature, x_best.multiway_split
-                    )
-                    leaves = tuple(
-                        self._new_leaf(initial_stats, parent=node)
-                        for initial_stats in x_best.children_stats
-                    )
-
-                    new_split = x_best.assemble(
-                        branch, node.stats, node.depth, *leaves, **kwargs
-                    )
-                    # Update weights in new_split
-                    new_split.last_split_reevaluation_at = node.total_weight
-
-                    n_active = n_inactive = 0
-                    for leaf in node.iter_leaves():
-                        if leaf.is_active():
-                            n_active += 1
-                        else:
-                            n_inactive += 1
-
-                    self._n_active_leaves -= n_active
-                    self._n_inactive_leaves -= n_inactive
-                    self._n_active_leaves += len(leaves)
-
-                    if parent is None:
-                        # Root case : replace the root node by a new split node
-                        self._root = new_split
-                    else:
-                        parent.children[branch_index] = new_split
-
-                    stop_flag = True
-
-                    # Manage memory
-                    self._enforce_size_limit()
-
-                elif (
-                    x_best.merit - x_current.merit > hoeffding_bound
-                    or hoeffding_bound < self.tie_threshold
-                ) and (id_current == id_best):
-                    branch = self._branch_selector(
-                        x_best.numerical_feature, x_best.multiway_split
-                    )
-                    # Change the branch but keep the existing children nodes
-                    new_split = x_best.assemble(
-                        branch, node.stats, node.depth, *tuple(node.children), **kwargs
-                    )
-                    # Update weights in new_split
-                    new_split.last_split_reevaluation_at = node.total_weight
-
-                    if parent is None:
-                        # Root case : replace the root node by a new split node
-                        self._root = new_split
-                    else:
-                        parent.children[branch_index] = new_split
-
-        return stop_flag
-
-    def _attempt_to_split(self, node, parent, branch_index, **kwargs):
-        """Attempt to split a node.
-
-        If the samples seen so far are not from the same class then:
-
-        1. Find split candidates and select the best one.
-        2. Compute the Hoeffding bound.
-        3. If the difference between the best split candidate and the don't split candidate is
-        larger than the Hoeffding bound:
-            3.1 Replace the leaf node by a split node.
-            3.2 Add a new leaf node on each branch of the new split node.
-            3.3 Update tree's metrics
-
-        Parameters
-        ----------
-        node
-            The node to reevaluate.
-        parent
-            The node's parent.
-        branch_index
-            Parent node's branch index.
-        kwargs
-            Other parameters passed to the new branch node.
-
-        """
-        if not node.observed_class_distribution_is_pure():  # noqa
-            split_criterion = self._new_split_criterion()
-
-            best_split_suggestions = node.best_split_suggestions(split_criterion, self)
-
-            if len(best_split_suggestions) > 0:
-                # x_best is the attribute with the highest merit
-                best_split_suggestions.sort()
-                x_best = best_split_suggestions[-1]
-
-                # Get x_null
-                x_null = BranchFactory(merit=0)
-
-                hoeffding_bound = self._hoeffding_bound(
-                    split_criterion.range_of_merit(node.stats),
-                    self.split_confidence,
-                    node.total_weight,
-                )
-
-                if (
-                    x_best.merit - x_null.merit > hoeffding_bound
-                    or hoeffding_bound < self.tie_threshold
-                ):
-                    # Create a new branch
-                    branch = self._branch_selector(
-                        x_best.numerical_feature, x_best.multiway_split
-                    )
-                    leaves = tuple(
-                        self._new_leaf(initial_stats, parent=node)
-                        for initial_stats in x_best.children_stats
-                    )
-                    new_split = x_best.assemble(
-                        branch, node.stats, node.depth, *leaves, **kwargs
-                    )
-
-                    new_split.last_split_reevaluation_at = node.total_weight
-
-                    self._n_active_leaves -= 1
-                    self._n_active_leaves += len(leaves)
-
-                    if parent is None:
-                        # root case: replace the root node by a new split node
-                        self._root = new_split
-                    else:
-                        parent.children[branch_index] = new_split
-
-                    # Manage memory
-                    self._enforce_size_limit()
-
-    def _kill_subtree(self, node: BaseEFDTBranch):
-        """Kill subtree that starts from node.
-
-        Parameters
-        ----------
-        node
-            The node to reevaluate.
-        Returns
-        -------
-            The new leaf.
-        """
-
-        leaf = self._new_leaf()
-        leaf.depth = node.depth  # noqa
-        leaf.stats = node.stats
-        leaf.splitters = node.splitters
-
-        return leaf
+import typing
+
+from .hoeffding_tree_classifier import HoeffdingTreeClassifier
+from .nodes.branch import DTBranch
+from .nodes.efdtc_nodes import (
+    BaseEFDTBranch,
+    EFDTLeafMajorityClass,
+    EFDTLeafNaiveBayes,
+    EFDTLeafNaiveBayesAdaptive,
+    EFDTNominalBinaryBranch,
+    EFDTNominalMultiwayBranch,
+    EFDTNumericBinaryBranch,
+    EFDTNumericMultiwayBranch,
+)
+from .nodes.leaf import HTLeaf
+from .splitter import Splitter
+from .utils import BranchFactory
+
+
+class ExtremelyFastDecisionTreeClassifier(HoeffdingTreeClassifier):
+    """Extremely Fast Decision Tree classifier.
+
+    Also referred to as Hoeffding AnyTime Tree (HATT) classifier.
+
+    Parameters
+    ----------
+    grace_period
+        Number of instances a leaf should observe between split attempts.
+    max_depth
+        The maximum depth a tree can reach. If `None`, the tree will grow indefinitely.
+    min_samples_reevaluate
+        Number of instances a node should observe before reevaluating the best split.
+    split_criterion
+        Split criterion to use.</br>
+        - 'gini' - Gini</br>
+        - 'info_gain' - Information Gain</br>
+        - 'hellinger' - Helinger Distance</br>
+    split_confidence
+        Allowed error in split decision, a value closer to 0 takes longer to decide.
+    tie_threshold
+        Threshold below which a split will be forced to break ties.
+    leaf_prediction
+        Prediction mechanism used at leafs.</br>
+        - 'mc' - Majority Class</br>
+        - 'nb' - Naive Bayes</br>
+        - 'nba' - Naive Bayes Adaptive</br>
+    nb_threshold
+        Number of instances a leaf should observe before allowing Naive Bayes.
+    nominal_attributes
+        List of Nominal attributes identifiers. If empty, then assume that all numeric attributes
+        should be treated as continuous.
+    splitter
+        The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric
+        features and perform splits. Splitters are available in the `tree.splitter` module.
+        Different splitters are available for classification and regression tasks. Classification
+        and regression splitters can be distinguished by their property `is_target_class`.
+        This is an advanced option. Special care must be taken when choosing different splitters.
+        By default, `tree.splitter.GaussianSplitter` is used if `splitter` is `None`.
+    binary_split
+        If True, only allow binary splits.
+    max_size
+        The max size of the tree, in Megabytes (MB).
+    memory_estimate_period
+        Interval (number of processed instances) between memory consumption checks.
+    stop_mem_management
+        If True, stop growing as soon as memory limit is hit.
+    remove_poor_attrs
+        If True, disable poor attributes to reduce memory usage.
+    merit_preprune
+        If True, enable merit-based tree pre-pruning.
+
+    Notes
+    -----
+    The Extremely Fast Decision Tree (EFDT) [^1] constructs a tree incrementally. The EFDT seeks to
+    select and deploy a split as soon as it is confident the split is useful, and then revisits
+    that decision, replacing the split if it subsequently becomes evident that a better split is
+    available. The EFDT learns rapidly from a stationary distribution and eventually it learns the
+    asymptotic batch tree if the distribution from which the data are drawn is stationary.
+
+    References
+    ----------
+    [^1]:  C. Manapragada, G. Webb, and M. Salehi. Extremely Fast Decision Tree.
+    In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data
+    Mining (KDD '18). ACM, New York, NY, USA, 1953-1962.
+    DOI: https://doi.org/10.1145/3219819.3220005
+
+    Examples
+    --------
+    >>> from river import synth
+    >>> from river import evaluate
+    >>> from river import metrics
+    >>> from river import tree
+
+    >>> gen = synth.Agrawal(classification_function=0, seed=42)
+    >>> # Take 1000 instances from the infinite data generator
+    >>> dataset = iter(gen.take(1000))
+
+    >>> model = tree.ExtremelyFastDecisionTreeClassifier(
+    ...     grace_period=100,
+    ...     split_confidence=1e-5,
+    ...     nominal_attributes=['elevel', 'car', 'zipcode'],
+    ...     min_samples_reevaluate=100
+    ... )
+
+    >>> metric = metrics.Accuracy()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    Accuracy: 87.89%
+    """
+
+    def __init__(
+        self,
+        grace_period: int = 200,
+        max_depth: int = None,
+        min_samples_reevaluate: int = 20,
+        split_criterion: str = "info_gain",
+        split_confidence: float = 1e-7,
+        tie_threshold: float = 0.05,
+        leaf_prediction: str = "nba",
+        nb_threshold: int = 0,
+        nominal_attributes: list = None,
+        splitter: Splitter = None,
+        binary_split: bool = False,
+        max_size: int = 100,
+        memory_estimate_period: int = 1000000,
+        stop_mem_management: bool = False,
+        remove_poor_attrs: bool = False,
+        merit_preprune: bool = True,
+    ):
+
+        super().__init__(
+            grace_period=grace_period,
+            max_depth=max_depth,
+            split_criterion=split_criterion,
+            split_confidence=split_confidence,
+            tie_threshold=tie_threshold,
+            leaf_prediction=leaf_prediction,
+            nb_threshold=nb_threshold,
+            nominal_attributes=nominal_attributes,
+            splitter=splitter,
+            binary_split=binary_split,
+            max_size=max_size,
+            memory_estimate_period=memory_estimate_period,
+            stop_mem_management=stop_mem_management,
+            remove_poor_attrs=remove_poor_attrs,
+            merit_preprune=merit_preprune,
+        )
+
+        self.min_samples_reevaluate = min_samples_reevaluate
+
+    def _new_leaf(self, initial_stats=None, parent=None):
+        if initial_stats is None:
+            initial_stats = {}
+        if parent is None:
+            depth = 0
+        else:
+            depth = parent.depth + 1
+
+        if self._leaf_prediction == self._MAJORITY_CLASS:
+            return EFDTLeafMajorityClass(initial_stats, depth, self.splitter)
+        elif self._leaf_prediction == self._NAIVE_BAYES:
+            return EFDTLeafNaiveBayes(initial_stats, depth, self.splitter)
+        else:  # NAIVE BAYES ADAPTIVE (default)
+            return EFDTLeafNaiveBayesAdaptive(initial_stats, depth, self.splitter)
+
+    def _branch_selector(
+        self, numerical_feature=True, multiway_split=False
+    ) -> typing.Type[DTBranch]:
+        """Create a new split node."""
+        if numerical_feature:
+            if not multiway_split:
+                return EFDTNumericBinaryBranch
+            else:
+                return EFDTNumericMultiwayBranch
+        else:
+            if not multiway_split:
+                return EFDTNominalBinaryBranch
+            else:
+                return EFDTNominalMultiwayBranch
+
+    def learn_one(self, x, y, *, sample_weight=1.0):
+        """Incrementally train the model
+
+        Parameters
+        ----------
+        x
+            Instance attributes.
+        y
+            The label of the instance.
+        sample_weight
+            The weight of the sample.
+
+        Notes
+        -----
+        Training tasks:
+
+        * If the tree is empty, create a leaf node as the root.
+        * If the tree is already initialized, find the path from root to the corresponding leaf for
+        the instance and sort the instance.
+        * Reevaluate the best split for each internal node.
+        * Attempt to split the leaf.
+
+        Returns
+        -------
+        self
+        """
+        # Updates the set of observed classes
+        self.classes.add(y)
+
+        self._train_weight_seen_by_model += sample_weight
+
+        if self._root is None:
+            self._root = self._new_leaf()
+            self._n_active_leaves = 1
+
+        # Sort instance X into a leaf
+        self._sort_to_leaf(x, y, sample_weight)
+        # Process all nodes, starting from root to the leaf where the instance x belongs.
+        self._process_nodes(x, y, sample_weight, self._root, None, None)
+
+        return self
+
+    def _sort_to_leaf(self, x, y, sample_weight):
+        """For a given instance, find the corresponding leaf and update it.
+
+        Private function where leaf learn from instance.
+
+        1. Find the node where instance should be.
+        2. If no node have been found, create new learning node.
+        3.1 Update the node with the provided instance.
+
+        Parameters
+        ----------
+        x
+            Instance attributes.
+        y
+            The instance label.
+        sample_weight
+            The weight of the sample.
+
+        """
+        node = self._root
+        if isinstance(self._root, DTBranch):
+            node = self._root.traverse(x, until_leaf=False)
+            # Something went wrong in the way: a missing split feature or emerging category
+            # Let's deal with these situations
+            if isinstance(node, DTBranch):
+                while True:
+                    if node.max_branches() == -1 and node.feature in x:
+                        # Emerging feature in nominal feature: create a new branch
+                        leaf = self._new_leaf(parent=node)
+                        node.add_child(x[node.feature], leaf)
+                        self._n_active_leaves += 1
+                        node = leaf
+                    else:
+                        # Missing split feature: select the most traversed path to continue the
+                        # walk
+                        _, node = node.most_common_path()
+                        if isinstance(node, DTBranch):
+                            node = node.traverse(x, until_leaf=False)
+                    if isinstance(node, HTLeaf):
+                        break
+        node.learn_one(x, y, sample_weight=sample_weight, tree=self)
+
+        if self._train_weight_seen_by_model % self.memory_estimate_period == 0:
+            self._estimate_model_size()
+
+    def _process_nodes(self, x, y, sample_weight, node, parent, branch_index):
+        """Process nodes from the root to the leaf where the instance belongs.
+
+        1. If the node is internal:
+            1.1 If the number of samples seen since the last reevaluation are greater than
+            `min_samples_reevaluate`, reevaluate the best split for the internal node.
+        2. If the node is leaf, attempt to split leaf node.
+
+        Parameters
+        ----------
+        x
+            Instance attributes.
+        y
+            The label of the instance.
+        sample_weight
+            The weight of the sample.
+        node
+            The node to process.
+        parent
+            The node's parent.
+        branch_index
+            Parent node's branch index.
+        """
+        if isinstance(node, BaseEFDTBranch):
+            # Update split nodes as the tree is traversed
+            node.learn_one(x, y, sample_weight=sample_weight, tree=self)
+
+            old_weight = node.last_split_reevaluation_at
+            new_weight = node.total_weight
+            stop_flag = False
+
+            if (new_weight - old_weight) >= self.min_samples_reevaluate:
+                # Reevaluate the best split
+                stop_flag = self._reevaluate_best_split(
+                    node,
+                    parent,
+                    branch_index,
+                    # The attribute observer template that will be replicated to new leaves
+                    splitter=self.splitter,
+                    # Existing attribute observers that will be leveraged
+                    splitters=node.splitters,
+                )
+
+            if not stop_flag:
+                # Move in depth
+                try:
+                    child_index = node.branch_no(x)
+                    child = node.children[child_index]
+                except KeyError:
+                    child_index, child = node.most_common_path()
+                self._process_nodes(x, y, sample_weight, child, node, child_index)
+        elif self._growth_allowed and node.is_active():
+            if node.depth >= self.max_depth:  # Max depth reached
+                node.deactivate()
+                self._n_inactive_leaves += 1
+                self._n_active_leaves -= 1
+            else:
+                weight_seen = node.total_weight
+                weight_diff = weight_seen - node.last_split_attempt_at
+                if weight_diff >= self.grace_period:
+                    # Attempt to split
+                    self._attempt_to_split(
+                        node,
+                        parent,
+                        branch_index,
+                        splitter=self.splitter,
+                        splitters=node.splitters,
+                    )
+                    node.last_split_attempt_at = weight_seen
+
+    def _reevaluate_best_split(self, node, parent, branch_index, **kwargs):
+        """Reevaluate the best split for a node.
+
+        If the samples seen so far are not from the same class then:
+        1. Find split candidates and select the best one.
+        2. Compute the Hoeffding bound.
+        3. If the null split candidate is higher than the top split candidate:
+            3.1 Kill subtree and replace it with a leaf.
+            3.2 Update the tree.
+            3.3 Update tree's metrics
+        4. If the difference between the top split candidate and the current split is larger than
+        the Hoeffding bound:
+           4.1 Create a new split node.
+           4.2 Update the tree.
+           4.3 Update tree's metrics
+        5. If the top split candidate is the current split but with different split test:
+           5.1 Update the split test of the current split.
+
+        Parameters
+        ----------
+        node
+            The node to reevaluate.
+        parent
+            The node's parent.
+        branch_index
+            Parent node's branch index.
+        kwargs
+            Other parameters passed to the branch node.
+
+        Returns
+        -------
+            Flag to stop moving in depth.
+        """
+        stop_flag = False
+        if not node.observed_class_distribution_is_pure():
+            split_criterion = self._new_split_criterion()
+            best_split_suggestions = node.best_split_suggestions(split_criterion, self)
+            if len(best_split_suggestions) > 0:
+                # Sort the attribute accordingly to their split merit for each attribute
+                # (except the null one)
+                best_split_suggestions.sort()
+
+                # x_best is the attribute with the highest merit
+                x_best = best_split_suggestions[-1]
+                id_best = x_best.feature
+
+                # Best split candidate is the null split
+                if x_best.feature is None:
+                    return True
+
+                # x_current is the current attribute used in this SplitNode
+                id_current = node.feature
+                x_current = node.find_attribute(id_current, best_split_suggestions)
+
+                # Get x_null
+                x_null = BranchFactory(merit=0)
+
+                # Compute Hoeffding bound
+                hoeffding_bound = self._hoeffding_bound(
+                    split_criterion.range_of_merit(node.stats),
+                    self.split_confidence,
+                    node.total_weight,
+                )
+
+                if x_null.merit - x_best.merit > hoeffding_bound:
+                    # Kill subtree & replace the branch by a leaf
+                    best_split = self._kill_subtree(node)
+
+                    # update EFDT
+                    if parent is None:
+                        # Root case : replace the root node by a new split node
+                        self._root = best_split
+                    else:
+                        parent.children[branch_index] = best_split
+
+                    n_active = n_inactive = 0
+                    for leaf in node.iter_leaves():
+                        if leaf.is_active():
+                            n_active += 1
+                        else:
+                            n_inactive += 1
+
+                    self._n_active_leaves += 1
+                    self._n_active_leaves -= n_active
+                    self._n_inactive_leaves -= n_inactive
+                    stop_flag = True
+
+                    # Manage memory
+                    self._enforce_size_limit()
+
+                elif (
+                    x_best.merit - x_current.merit > hoeffding_bound
+                    or hoeffding_bound < self.tie_threshold
+                ) and (id_current != id_best):
+                    # Create a new branch
+                    branch = self._branch_selector(
+                        x_best.numerical_feature, x_best.multiway_split
+                    )
+                    leaves = tuple(
+                        self._new_leaf(initial_stats, parent=node)
+                        for initial_stats in x_best.children_stats
+                    )
+
+                    new_split = x_best.assemble(
+                        branch, node.stats, node.depth, *leaves, **kwargs
+                    )
+                    # Update weights in new_split
+                    new_split.last_split_reevaluation_at = node.total_weight
+
+                    n_active = n_inactive = 0
+                    for leaf in node.iter_leaves():
+                        if leaf.is_active():
+                            n_active += 1
+                        else:
+                            n_inactive += 1
+
+                    self._n_active_leaves -= n_active
+                    self._n_inactive_leaves -= n_inactive
+                    self._n_active_leaves += len(leaves)
+
+                    if parent is None:
+                        # Root case : replace the root node by a new split node
+                        self._root = new_split
+                    else:
+                        parent.children[branch_index] = new_split
+
+                    stop_flag = True
+
+                    # Manage memory
+                    self._enforce_size_limit()
+
+                elif (
+                    x_best.merit - x_current.merit > hoeffding_bound
+                    or hoeffding_bound < self.tie_threshold
+                ) and (id_current == id_best):
+                    branch = self._branch_selector(
+                        x_best.numerical_feature, x_best.multiway_split
+                    )
+                    # Change the branch but keep the existing children nodes
+                    new_split = x_best.assemble(
+                        branch, node.stats, node.depth, *tuple(node.children), **kwargs
+                    )
+                    # Update weights in new_split
+                    new_split.last_split_reevaluation_at = node.total_weight
+
+                    if parent is None:
+                        # Root case : replace the root node by a new split node
+                        self._root = new_split
+                    else:
+                        parent.children[branch_index] = new_split
+
+        return stop_flag
+
+    def _attempt_to_split(self, node, parent, branch_index, **kwargs):
+        """Attempt to split a node.
+
+        If the samples seen so far are not from the same class then:
+
+        1. Find split candidates and select the best one.
+        2. Compute the Hoeffding bound.
+        3. If the difference between the best split candidate and the don't split candidate is
+        larger than the Hoeffding bound:
+            3.1 Replace the leaf node by a split node.
+            3.2 Add a new leaf node on each branch of the new split node.
+            3.3 Update tree's metrics
+
+        Parameters
+        ----------
+        node
+            The node to reevaluate.
+        parent
+            The node's parent.
+        branch_index
+            Parent node's branch index.
+        kwargs
+            Other parameters passed to the new branch node.
+
+        """
+        if not node.observed_class_distribution_is_pure():  # noqa
+            split_criterion = self._new_split_criterion()
+
+            best_split_suggestions = node.best_split_suggestions(split_criterion, self)
+
+            if len(best_split_suggestions) > 0:
+                # x_best is the attribute with the highest merit
+                best_split_suggestions.sort()
+                x_best = best_split_suggestions[-1]
+
+                # Get x_null
+                x_null = BranchFactory(merit=0)
+
+                hoeffding_bound = self._hoeffding_bound(
+                    split_criterion.range_of_merit(node.stats),
+                    self.split_confidence,
+                    node.total_weight,
+                )
+
+                if (
+                    x_best.merit - x_null.merit > hoeffding_bound
+                    or hoeffding_bound < self.tie_threshold
+                ):
+                    # Create a new branch
+                    branch = self._branch_selector(
+                        x_best.numerical_feature, x_best.multiway_split
+                    )
+                    leaves = tuple(
+                        self._new_leaf(initial_stats, parent=node)
+                        for initial_stats in x_best.children_stats
+                    )
+                    new_split = x_best.assemble(
+                        branch, node.stats, node.depth, *leaves, **kwargs
+                    )
+
+                    new_split.last_split_reevaluation_at = node.total_weight
+
+                    self._n_active_leaves -= 1
+                    self._n_active_leaves += len(leaves)
+
+                    if parent is None:
+                        # root case: replace the root node by a new split node
+                        self._root = new_split
+                    else:
+                        parent.children[branch_index] = new_split
+
+                    # Manage memory
+                    self._enforce_size_limit()
+
+    def _kill_subtree(self, node: BaseEFDTBranch):
+        """Kill subtree that starts from node.
+
+        Parameters
+        ----------
+        node
+            The node to reevaluate.
+        Returns
+        -------
+            The new leaf.
+        """
+
+        leaf = self._new_leaf()
+        leaf.depth = node.depth  # noqa
+        leaf.stats = node.stats
+        leaf.splitters = node.splitters
+
+        return leaf
```

### Comparing `river-0.8.0/river/tree/hoeffding_adaptive_tree_classifier.py` & `river-0.9.0/river/tree/hoeffding_adaptive_tree_classifier.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,271 +1,271 @@
-import typing
-
-from river.utils.skmultiflow_utils import add_dict_values, normalize_values_in_dict
-
-from .hoeffding_tree_classifier import HoeffdingTreeClassifier
-from .nodes.branch import DTBranch
-from .nodes.hatc_nodes import (
-    AdaBranchClassifier,
-    AdaLeafClassifier,
-    AdaNomBinaryBranchClass,
-    AdaNomMultiwayBranchClass,
-    AdaNumBinaryBranchClass,
-    AdaNumMultiwayBranchClass,
-)
-from .splitter import Splitter
-
-
-class HoeffdingAdaptiveTreeClassifier(HoeffdingTreeClassifier):
-    """Hoeffding Adaptive Tree classifier.
-
-    Parameters
-    ----------
-    grace_period
-        Number of instances a leaf should observe between split attempts.
-    max_depth
-        The maximum depth a tree can reach. If `None`, the tree will grow indefinitely.
-    split_criterion
-        Split criterion to use.</br>
-        - 'gini' - Gini</br>
-        - 'info_gain' - Information Gain</br>
-        - 'hellinger' - Helinger Distance</br>
-    split_confidence
-        Allowed error in split decision, a value closer to 0 takes longer to decide.
-    tie_threshold
-        Threshold below which a split will be forced to break ties.
-    leaf_prediction
-        Prediction mechanism used at leafs.</br>
-        - 'mc' - Majority Class</br>
-        - 'nb' - Naive Bayes</br>
-        - 'nba' - Naive Bayes Adaptive</br>
-    nb_threshold
-        Number of instances a leaf should observe before allowing Naive Bayes.
-    nominal_attributes
-        List of Nominal attributes. If empty, then assume that all numeric attributes should
-        be treated as continuous.
-    splitter
-        The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric
-        features and perform splits. Splitters are available in the `tree.splitter` module.
-        Different splitters are available for classification and regression tasks. Classification
-        and regression splitters can be distinguished by their property `is_target_class`.
-        This is an advanced option. Special care must be taken when choosing different splitters.
-        By default, `tree.splitter.GaussianSplitter` is used if `splitter` is `None`.
-    bootstrap_sampling
-        If True, perform bootstrap sampling in the leaf nodes.
-    drift_window_threshold
-        Minimum number of examples an alternate tree must observe before being considered as a
-        potential replacement to the current one.
-    adwin_confidence
-        The delta parameter used in the nodes' ADWIN drift detectors.
-    binary_split
-        If True, only allow binary splits.
-    max_size
-        The max size of the tree, in Megabytes (MB).
-    memory_estimate_period
-        Interval (number of processed instances) between memory consumption checks.
-    stop_mem_management
-        If True, stop growing as soon as memory limit is hit.
-    remove_poor_attrs
-        If True, disable poor attributes to reduce memory usage.
-    merit_preprune
-        If True, enable merit-based tree pre-pruning.
-    seed
-       If int, `seed` is the seed used by the random number generator;</br>
-       If RandomState instance, `seed` is the random number generator;</br>
-       If None, the random number generator is the RandomState instance used
-       by `np.random`. Only used when `bootstrap_sampling=True` to direct the
-       bootstrap sampling.</br>
-
-
-    Notes
-    -----
-    The Hoeffding Adaptive Tree [^1] uses ADWIN [^2] to monitor performance of branches on the tree
-    and to replace them with new branches when their accuracy decreases if the new branches are
-    more accurate.
-
-    The bootstrap sampling strategy is an improvement over the original Hoeffding Adaptive Tree
-    algorithm. It is enabled by default since, in general, it results in better performance.
-
-    References
-    ----------
-    [^1]: Bifet, Albert, and Ricard Gavaldà. "Adaptive learning from evolving data streams."
-       In International Symposium on Intelligent Data Analysis, pp. 249-260. Springer, Berlin,
-       Heidelberg, 2009.
-    [^2]: Bifet, Albert, and Ricard Gavaldà. "Learning from time-changing data with adaptive
-       windowing." In Proceedings of the 2007 SIAM international conference on data mining,
-       pp. 443-448. Society for Industrial and Applied Mathematics, 2007.
-
-    Examples
-    --------
-    >>> from river import synth
-    >>> from river import evaluate
-    >>> from river import metrics
-    >>> from river import tree
-
-    >>> gen = synth.ConceptDriftStream(stream=synth.SEA(seed=42, variant=0),
-    ...                                drift_stream=synth.SEA(seed=42, variant=1),
-    ...                                seed=1, position=500, width=50)
-    >>> # Take 1000 instances from the infinite data generator
-    >>> dataset = iter(gen.take(1000))
-
-    >>> model = tree.HoeffdingAdaptiveTreeClassifier(
-    ...     grace_period=100,
-    ...     split_confidence=1e-5,
-    ...     leaf_prediction='nb',
-    ...     nb_threshold=10,
-    ...     seed=0
-    ... )
-
-    >>> metric = metrics.Accuracy()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    Accuracy: 91.09%
-
-    """
-
-    # =============================================
-    # == Hoeffding Adaptive Tree implementation ===
-    # =============================================
-
-    def __init__(
-        self,
-        grace_period: int = 200,
-        max_depth: int = None,
-        split_criterion: str = "info_gain",
-        split_confidence: float = 1e-7,
-        tie_threshold: float = 0.05,
-        leaf_prediction: str = "nba",
-        nb_threshold: int = 0,
-        nominal_attributes: list = None,
-        splitter: Splitter = None,
-        bootstrap_sampling: bool = True,
-        drift_window_threshold: int = 300,
-        adwin_confidence: float = 0.002,
-        binary_split: bool = False,
-        max_size: int = 100,
-        memory_estimate_period: int = 1000000,
-        stop_mem_management: bool = False,
-        remove_poor_attrs: bool = False,
-        merit_preprune: bool = True,
-        seed=None,
-    ):
-
-        super().__init__(
-            grace_period=grace_period,
-            max_depth=max_depth,
-            split_criterion=split_criterion,
-            split_confidence=split_confidence,
-            tie_threshold=tie_threshold,
-            leaf_prediction=leaf_prediction,
-            nb_threshold=nb_threshold,
-            nominal_attributes=nominal_attributes,
-            splitter=splitter,
-            binary_split=binary_split,
-            max_size=max_size,
-            memory_estimate_period=memory_estimate_period,
-            stop_mem_management=stop_mem_management,
-            remove_poor_attrs=remove_poor_attrs,
-            merit_preprune=merit_preprune,
-        )
-
-        self._n_alternate_trees = 0
-        self._n_pruned_alternate_trees = 0
-        self._n_switch_alternate_trees = 0
-
-        self.bootstrap_sampling = bootstrap_sampling
-        self.drift_window_threshold = drift_window_threshold
-        self.adwin_confidence = adwin_confidence
-        self.seed = seed
-
-    @property
-    def n_alternate_trees(self):
-        return self._n_alternate_trees
-
-    @property
-    def n_pruned_alternate_trees(self):
-        return self._n_pruned_alternate_trees
-
-    @property
-    def n_switch_alternate_trees(self):
-        return self._n_switch_alternate_trees
-
-    @property
-    def summary(self):
-        summ = super().summary
-        summ.update(
-            {
-                "n_alternate_trees": self.n_alternate_trees,
-                "n_pruned_alternate_trees": self.n_pruned_alternate_trees,
-                "n_switch_alternate_trees": self.n_switch_alternate_trees,
-            }
-        )
-        return summ
-
-    def learn_one(self, x, y, *, sample_weight=1.0):
-        # Updates the set of observed classes
-        self.classes.add(y)
-
-        self._train_weight_seen_by_model += sample_weight
-
-        if self._root is None:
-            self._root = self._new_leaf()
-            self._n_active_leaves = 1
-
-        self._root.learn_one(x, y, sample_weight=sample_weight, tree=self)
-
-        if self._train_weight_seen_by_model % self.memory_estimate_period == 0:
-            self._estimate_model_size()
-
-        return self
-
-    # Override HoeffdingTreeClassifier
-    def predict_proba_one(self, x):
-        proba = {c: 0.0 for c in self.classes}
-        if self._root is not None:
-            found_nodes = [self._root]
-            if isinstance(self._root, DTBranch):
-                found_nodes = self._root.traverse(x, until_leaf=True)
-            for leaf in found_nodes:
-                dist = leaf.prediction(x, tree=self)
-                # Option Tree prediction (of sorts): combine the response of all leaves reached
-                # by the instance
-                proba = add_dict_values(proba, dist, inplace=True)
-            proba = normalize_values_in_dict(proba)
-
-        return proba
-
-    def _new_leaf(self, initial_stats=None, parent=None):
-        if initial_stats is None:
-            initial_stats = {}
-
-        if parent is not None:
-            depth = parent.depth + 1
-        else:
-            depth = 0
-
-        return AdaLeafClassifier(
-            stats=initial_stats,
-            depth=depth,
-            splitter=self.splitter,
-            adwin_delta=self.adwin_confidence,
-            seed=self.seed,
-        )
-
-    def _branch_selector(
-        self, numerical_feature=True, multiway_split=False
-    ) -> typing.Type[AdaBranchClassifier]:
-        """Create a new split node."""
-        if numerical_feature:
-            if not multiway_split:
-                return AdaNumBinaryBranchClass
-            else:
-                return AdaNumMultiwayBranchClass
-        else:
-            if not multiway_split:
-                return AdaNomBinaryBranchClass
-            else:
-                return AdaNomMultiwayBranchClass
-
-    @classmethod
-    def _unit_test_params(cls):
-        return {"seed": 1}
+import typing
+
+from river.utils.skmultiflow_utils import add_dict_values, normalize_values_in_dict
+
+from .hoeffding_tree_classifier import HoeffdingTreeClassifier
+from .nodes.branch import DTBranch
+from .nodes.hatc_nodes import (
+    AdaBranchClassifier,
+    AdaLeafClassifier,
+    AdaNomBinaryBranchClass,
+    AdaNomMultiwayBranchClass,
+    AdaNumBinaryBranchClass,
+    AdaNumMultiwayBranchClass,
+)
+from .splitter import Splitter
+
+
+class HoeffdingAdaptiveTreeClassifier(HoeffdingTreeClassifier):
+    """Hoeffding Adaptive Tree classifier.
+
+    Parameters
+    ----------
+    grace_period
+        Number of instances a leaf should observe between split attempts.
+    max_depth
+        The maximum depth a tree can reach. If `None`, the tree will grow indefinitely.
+    split_criterion
+        Split criterion to use.</br>
+        - 'gini' - Gini</br>
+        - 'info_gain' - Information Gain</br>
+        - 'hellinger' - Helinger Distance</br>
+    split_confidence
+        Allowed error in split decision, a value closer to 0 takes longer to decide.
+    tie_threshold
+        Threshold below which a split will be forced to break ties.
+    leaf_prediction
+        Prediction mechanism used at leafs.</br>
+        - 'mc' - Majority Class</br>
+        - 'nb' - Naive Bayes</br>
+        - 'nba' - Naive Bayes Adaptive</br>
+    nb_threshold
+        Number of instances a leaf should observe before allowing Naive Bayes.
+    nominal_attributes
+        List of Nominal attributes. If empty, then assume that all numeric attributes should
+        be treated as continuous.
+    splitter
+        The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric
+        features and perform splits. Splitters are available in the `tree.splitter` module.
+        Different splitters are available for classification and regression tasks. Classification
+        and regression splitters can be distinguished by their property `is_target_class`.
+        This is an advanced option. Special care must be taken when choosing different splitters.
+        By default, `tree.splitter.GaussianSplitter` is used if `splitter` is `None`.
+    bootstrap_sampling
+        If True, perform bootstrap sampling in the leaf nodes.
+    drift_window_threshold
+        Minimum number of examples an alternate tree must observe before being considered as a
+        potential replacement to the current one.
+    adwin_confidence
+        The delta parameter used in the nodes' ADWIN drift detectors.
+    binary_split
+        If True, only allow binary splits.
+    max_size
+        The max size of the tree, in Megabytes (MB).
+    memory_estimate_period
+        Interval (number of processed instances) between memory consumption checks.
+    stop_mem_management
+        If True, stop growing as soon as memory limit is hit.
+    remove_poor_attrs
+        If True, disable poor attributes to reduce memory usage.
+    merit_preprune
+        If True, enable merit-based tree pre-pruning.
+    seed
+       If int, `seed` is the seed used by the random number generator;</br>
+       If RandomState instance, `seed` is the random number generator;</br>
+       If None, the random number generator is the RandomState instance used
+       by `np.random`. Only used when `bootstrap_sampling=True` to direct the
+       bootstrap sampling.</br>
+
+
+    Notes
+    -----
+    The Hoeffding Adaptive Tree [^1] uses ADWIN [^2] to monitor performance of branches on the tree
+    and to replace them with new branches when their accuracy decreases if the new branches are
+    more accurate.
+
+    The bootstrap sampling strategy is an improvement over the original Hoeffding Adaptive Tree
+    algorithm. It is enabled by default since, in general, it results in better performance.
+
+    References
+    ----------
+    [^1]: Bifet, Albert, and Ricard Gavaldà. "Adaptive learning from evolving data streams."
+       In International Symposium on Intelligent Data Analysis, pp. 249-260. Springer, Berlin,
+       Heidelberg, 2009.
+    [^2]: Bifet, Albert, and Ricard Gavaldà. "Learning from time-changing data with adaptive
+       windowing." In Proceedings of the 2007 SIAM international conference on data mining,
+       pp. 443-448. Society for Industrial and Applied Mathematics, 2007.
+
+    Examples
+    --------
+    >>> from river import synth
+    >>> from river import evaluate
+    >>> from river import metrics
+    >>> from river import tree
+
+    >>> gen = synth.ConceptDriftStream(stream=synth.SEA(seed=42, variant=0),
+    ...                                drift_stream=synth.SEA(seed=42, variant=1),
+    ...                                seed=1, position=500, width=50)
+    >>> # Take 1000 instances from the infinite data generator
+    >>> dataset = iter(gen.take(1000))
+
+    >>> model = tree.HoeffdingAdaptiveTreeClassifier(
+    ...     grace_period=100,
+    ...     split_confidence=1e-5,
+    ...     leaf_prediction='nb',
+    ...     nb_threshold=10,
+    ...     seed=0
+    ... )
+
+    >>> metric = metrics.Accuracy()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    Accuracy: 91.09%
+
+    """
+
+    # =============================================
+    # == Hoeffding Adaptive Tree implementation ===
+    # =============================================
+
+    def __init__(
+        self,
+        grace_period: int = 200,
+        max_depth: int = None,
+        split_criterion: str = "info_gain",
+        split_confidence: float = 1e-7,
+        tie_threshold: float = 0.05,
+        leaf_prediction: str = "nba",
+        nb_threshold: int = 0,
+        nominal_attributes: list = None,
+        splitter: Splitter = None,
+        bootstrap_sampling: bool = True,
+        drift_window_threshold: int = 300,
+        adwin_confidence: float = 0.002,
+        binary_split: bool = False,
+        max_size: int = 100,
+        memory_estimate_period: int = 1000000,
+        stop_mem_management: bool = False,
+        remove_poor_attrs: bool = False,
+        merit_preprune: bool = True,
+        seed=None,
+    ):
+
+        super().__init__(
+            grace_period=grace_period,
+            max_depth=max_depth,
+            split_criterion=split_criterion,
+            split_confidence=split_confidence,
+            tie_threshold=tie_threshold,
+            leaf_prediction=leaf_prediction,
+            nb_threshold=nb_threshold,
+            nominal_attributes=nominal_attributes,
+            splitter=splitter,
+            binary_split=binary_split,
+            max_size=max_size,
+            memory_estimate_period=memory_estimate_period,
+            stop_mem_management=stop_mem_management,
+            remove_poor_attrs=remove_poor_attrs,
+            merit_preprune=merit_preprune,
+        )
+
+        self._n_alternate_trees = 0
+        self._n_pruned_alternate_trees = 0
+        self._n_switch_alternate_trees = 0
+
+        self.bootstrap_sampling = bootstrap_sampling
+        self.drift_window_threshold = drift_window_threshold
+        self.adwin_confidence = adwin_confidence
+        self.seed = seed
+
+    @property
+    def n_alternate_trees(self):
+        return self._n_alternate_trees
+
+    @property
+    def n_pruned_alternate_trees(self):
+        return self._n_pruned_alternate_trees
+
+    @property
+    def n_switch_alternate_trees(self):
+        return self._n_switch_alternate_trees
+
+    @property
+    def summary(self):
+        summ = super().summary
+        summ.update(
+            {
+                "n_alternate_trees": self.n_alternate_trees,
+                "n_pruned_alternate_trees": self.n_pruned_alternate_trees,
+                "n_switch_alternate_trees": self.n_switch_alternate_trees,
+            }
+        )
+        return summ
+
+    def learn_one(self, x, y, *, sample_weight=1.0):
+        # Updates the set of observed classes
+        self.classes.add(y)
+
+        self._train_weight_seen_by_model += sample_weight
+
+        if self._root is None:
+            self._root = self._new_leaf()
+            self._n_active_leaves = 1
+
+        self._root.learn_one(x, y, sample_weight=sample_weight, tree=self)
+
+        if self._train_weight_seen_by_model % self.memory_estimate_period == 0:
+            self._estimate_model_size()
+
+        return self
+
+    # Override HoeffdingTreeClassifier
+    def predict_proba_one(self, x):
+        proba = {c: 0.0 for c in self.classes}
+        if self._root is not None:
+            found_nodes = [self._root]
+            if isinstance(self._root, DTBranch):
+                found_nodes = self._root.traverse(x, until_leaf=True)
+            for leaf in found_nodes:
+                dist = leaf.prediction(x, tree=self)
+                # Option Tree prediction (of sorts): combine the response of all leaves reached
+                # by the instance
+                proba = add_dict_values(proba, dist, inplace=True)
+            proba = normalize_values_in_dict(proba)
+
+        return proba
+
+    def _new_leaf(self, initial_stats=None, parent=None):
+        if initial_stats is None:
+            initial_stats = {}
+
+        if parent is not None:
+            depth = parent.depth + 1
+        else:
+            depth = 0
+
+        return AdaLeafClassifier(
+            stats=initial_stats,
+            depth=depth,
+            splitter=self.splitter,
+            adwin_delta=self.adwin_confidence,
+            seed=self.seed,
+        )
+
+    def _branch_selector(
+        self, numerical_feature=True, multiway_split=False
+    ) -> typing.Type[AdaBranchClassifier]:
+        """Create a new split node."""
+        if numerical_feature:
+            if not multiway_split:
+                return AdaNumBinaryBranchClass
+            else:
+                return AdaNumMultiwayBranchClass
+        else:
+            if not multiway_split:
+                return AdaNomBinaryBranchClass
+            else:
+                return AdaNomMultiwayBranchClass
+
+    @classmethod
+    def _unit_test_params(cls):
+        yield {"seed": 1}
```

### Comparing `river-0.8.0/river/tree/hoeffding_adaptive_tree_regressor.py` & `river-0.9.0/river/tree/hoeffding_adaptive_tree_regressor.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,317 +1,317 @@
-import typing
-from copy import deepcopy
-
-from river import base
-
-from .hoeffding_tree_regressor import HoeffdingTreeRegressor
-from .nodes.branch import DTBranch
-from .nodes.hatr_nodes import (
-    AdaBranchRegressor,
-    AdaLeafRegAdaptive,
-    AdaLeafRegMean,
-    AdaLeafRegModel,
-    AdaLeafRegressor,
-    AdaNomBinaryBranchReg,
-    AdaNomMultiwayBranchReg,
-    AdaNumBinaryBranchReg,
-    AdaNumMultiwayBranchReg,
-)
-from .splitter import Splitter
-
-
-class HoeffdingAdaptiveTreeRegressor(HoeffdingTreeRegressor):
-    """Hoeffding Adaptive Tree regressor (HATR).
-
-    This class implements a regression version of the Hoeffding Adaptive Tree Classifier. Hence,
-    it also uses an ADWIN concept-drift detector instance at each decision node to monitor
-    possible changes in the data distribution. If a drift is detected in a node, an alternate
-    tree begins to be induced in the background. When enough information is gathered, HATR
-    swaps the node where the change was detected by its alternate tree.
-
-    Parameters
-    ----------
-    grace_period
-        Number of instances a leaf should observe between split attempts.
-    max_depth
-        The maximum depth a tree can reach. If `None`, the tree will grow indefinitely.
-    split_confidence
-        Allowed error in split decision, a value closer to 0 takes longer to decide.
-    tie_threshold
-        Threshold below which a split will be forced to break ties.
-    leaf_prediction
-        Prediction mechanism used at leafs.</br>
-        - 'mean' - Target mean</br>
-        - 'model' - Uses the model defined in `leaf_model`</br>
-        - 'adaptive' - Chooses between 'mean' and 'model' dynamically</br>
-    leaf_model
-        The regression model used to provide responses if `leaf_prediction='model'`. If not
-        provided an instance of `river.linear_model.LinearRegression` with the default
-        hyperparameters is used.
-    model_selector_decay
-        The exponential decaying factor applied to the learning models' squared errors, that
-        are monitored if `leaf_prediction='adaptive'`. Must be between `0` and `1`. The closer
-        to `1`, the more importance is going to be given to past observations. On the other hand,
-        if its value approaches `0`, the recent observed errors are going to have more influence
-        on the final decision.
-    nominal_attributes
-        List of Nominal attributes. If empty, then assume that all numeric attributes should
-        be treated as continuous.
-    splitter
-        The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric
-        features and perform splits. Splitters are available in the `tree.splitter` module.
-        Different splitters are available for classification and regression tasks. Classification
-        and regression splitters can be distinguished by their property `is_target_class`.
-        This is an advanced option. Special care must be taken when choosing different splitters.
-        By default, `tree.splitter.EBSTSplitter` is used if `splitter` is `None`.
-    min_samples_split
-        The minimum number of samples every branch resulting from a split candidate must have
-        to be considered valid.
-    bootstrap_sampling
-        If True, perform bootstrap sampling in the leaf nodes.
-    drift_window_threshold
-        Minimum number of examples an alternate tree must observe before being considered as a
-        potential replacement to the current one.
-    adwin_confidence
-        The delta parameter used in the nodes' ADWIN drift detectors.
-    binary_split
-        If True, only allow binary splits.
-    max_size
-        The max size of the tree, in Megabytes (MB).
-    memory_estimate_period
-        Interval (number of processed instances) between memory consumption checks.
-    stop_mem_management
-        If True, stop growing as soon as memory limit is hit.
-    remove_poor_attrs
-        If True, disable poor attributes to reduce memory usage.
-    merit_preprune
-        If True, enable merit-based tree pre-pruning.
-    seed
-       If int, `seed` is the seed used by the random number generator;</br>
-       If RandomState instance, `seed` is the random number generator;</br>
-       If None, the random number generator is the RandomState instance used
-       by `np.random`. Only used when `bootstrap_sampling=True` to direct the
-       bootstrap sampling.</br>
-
-    Notes
-    -----
-    The Hoeffding Adaptive Tree [^1] uses ADWIN [^2] to monitor performance of branches on the tree
-    and to replace them with new branches when their accuracy decreases if the new branches are
-    more accurate.
-
-    The bootstrap sampling strategy is an improvement over the original Hoeffding Adaptive Tree
-    algorithm. It is enabled by default since, in general, it results in better performance.
-
-    To cope with ADWIN's requirements of bounded input data, HATR uses a novel error normalization
-    strategy based on the empiral rule of Gaussian distributions. We assume the deviations
-    of the predictions from the expected values follow a normal distribution. Hence, we subject
-    these errors to a min-max normalization assuming that most of the data lies in the
-    $\\left[-3\\sigma, 3\\sigma\\right]$ range. These normalized errors are passed to the ADWIN
-    instances. This is the same strategy used by Adaptive Random Forest Regressor.
-
-    References
-    ----------
-    [^1]: Bifet, Albert, and Ricard Gavaldà. "Adaptive learning from evolving data streams."
-    In International Symposium on Intelligent Data Analysis, pp. 249-260. Springer, Berlin,
-    Heidelberg, 2009.
-    [^2]: Bifet, Albert, and Ricard Gavaldà. "Learning from time-changing data with adaptive
-    windowing." In Proceedings of the 2007 SIAM international conference on data mining,
-    pp. 443-448. Society for Industrial and Applied Mathematics, 2007.
-
-    Examples
-    --------
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import metrics
-    >>> from river import tree
-    >>> from river import preprocessing
-
-    >>> dataset = datasets.TrumpApproval()
-
-    >>> model = (
-    ...     preprocessing.StandardScaler() |
-    ...     tree.HoeffdingAdaptiveTreeRegressor(
-    ...         grace_period=50,
-    ...         leaf_prediction='adaptive',
-    ...         model_selector_decay=0.3,
-    ...         seed=0
-    ...     )
-    ... )
-
-    >>> metric = metrics.MAE()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    MAE: 0.78838
-    """
-
-    def __init__(
-        self,
-        grace_period: int = 200,
-        max_depth: int = None,
-        split_confidence: float = 1e-7,
-        tie_threshold: float = 0.05,
-        leaf_prediction: str = "model",
-        leaf_model: base.Regressor = None,
-        model_selector_decay: float = 0.95,
-        nominal_attributes: list = None,
-        splitter: Splitter = None,
-        min_samples_split: int = 5,
-        bootstrap_sampling: bool = True,
-        drift_window_threshold: int = 300,
-        adwin_confidence: float = 0.002,
-        binary_split: bool = False,
-        max_size: int = 100,
-        memory_estimate_period: int = 1000000,
-        stop_mem_management: bool = False,
-        remove_poor_attrs: bool = False,
-        merit_preprune: bool = True,
-        seed=None,
-    ):
-
-        super().__init__(
-            grace_period=grace_period,
-            max_depth=max_depth,
-            split_confidence=split_confidence,
-            tie_threshold=tie_threshold,
-            leaf_prediction=leaf_prediction,
-            leaf_model=leaf_model,
-            model_selector_decay=model_selector_decay,
-            nominal_attributes=nominal_attributes,
-            splitter=splitter,
-            min_samples_split=min_samples_split,
-            binary_split=binary_split,
-            max_size=max_size,
-            memory_estimate_period=memory_estimate_period,
-            stop_mem_management=stop_mem_management,
-            remove_poor_attrs=remove_poor_attrs,
-            merit_preprune=merit_preprune,
-        )
-
-        self._n_alternate_trees = 0
-        self._n_pruned_alternate_trees = 0
-        self._n_switch_alternate_trees = 0
-
-        self.bootstrap_sampling = bootstrap_sampling
-        self.drift_window_threshold = drift_window_threshold
-        self.adwin_confidence = adwin_confidence
-        self.seed = seed
-
-    @property
-    def n_alternate_trees(self):
-        return self._n_alternate_trees
-
-    @property
-    def n_pruned_alternate_trees(self):
-        return self._n_pruned_alternate_trees
-
-    @property
-    def n_switch_alternate_trees(self):
-        return self._n_switch_alternate_trees
-
-    @property
-    def summary(self):
-        summ = super().summary
-        summ.update(
-            {
-                "n_alternate_trees": self.n_alternate_trees,
-                "n_pruned_alternate_trees": self.n_pruned_alternate_trees,
-                "n_switch_alternate_trees": self.n_switch_alternate_trees,
-            }
-        )
-        return summ
-
-    def learn_one(self, x, y, *, sample_weight=1.0):
-        self._train_weight_seen_by_model += sample_weight
-
-        if self._root is None:
-            self._root = self._new_leaf()
-            self._n_active_leaves = 1
-        self._root.learn_one(x, y, sample_weight=sample_weight, tree=self)
-
-        if self._train_weight_seen_by_model % self.memory_estimate_period == 0:
-            self._estimate_model_size()
-
-        return self
-
-    def predict_one(self, x):
-        pred = 0.0
-        if self._root is not None:
-            found_nodes = [self._root]
-            if isinstance(self._root, DTBranch):
-                found_nodes = self._root.traverse(x, until_leaf=True)
-            for leaf in found_nodes:
-                pred += leaf.prediction(x, tree=self)
-            # Mean prediction among the reached leaves
-            pred /= len(found_nodes)
-
-        return pred
-
-    def _new_leaf(self, initial_stats=None, parent=None, is_active=True):
-        """Create a new learning node.
-
-        The type of learning node depends on the tree configuration.
-        """
-        if parent is not None:
-            depth = parent.depth + 1
-        else:
-            depth = 0
-
-        leaf_model = None
-        if self.leaf_prediction in {self._MODEL, self._ADAPTIVE}:
-            if parent is None:
-                leaf_model = deepcopy(self.leaf_model)
-            else:
-                try:
-                    leaf_model = deepcopy(parent._leaf_model)  # noqa
-                except AttributeError:
-                    leaf_model = deepcopy(self.leaf_model)
-
-        if self.leaf_prediction == self._TARGET_MEAN:
-            return AdaLeafRegMean(
-                initial_stats,
-                depth,
-                self.splitter,
-                adwin_delta=self.adwin_confidence,
-                seed=self.seed,
-            )
-        elif self.leaf_prediction == self._MODEL:
-            return AdaLeafRegModel(
-                initial_stats,
-                depth,
-                self.splitter,
-                adwin_delta=self.adwin_confidence,
-                seed=self.seed,
-                leaf_model=leaf_model,
-            )
-        else:  # adaptive learning node
-            new_adaptive = AdaLeafRegAdaptive(
-                initial_stats,
-                depth,
-                self.splitter,
-                adwin_delta=self.adwin_confidence,
-                seed=self.seed,
-                leaf_model=leaf_model,
-            )
-            if parent is not None and isinstance(parent, AdaLeafRegressor):
-                new_adaptive._fmse_mean = parent._fmse_mean  # noqa
-                new_adaptive._fmse_model = parent._fmse_model  # noqa
-
-            return new_adaptive
-
-    def _branch_selector(
-        self, numerical_feature=True, multiway_split=False
-    ) -> typing.Type[AdaBranchRegressor]:
-        """Create a new split node."""
-        if numerical_feature:
-            if not multiway_split:
-                return AdaNumBinaryBranchReg
-            else:
-                return AdaNumMultiwayBranchReg
-        else:
-            if not multiway_split:
-                return AdaNomBinaryBranchReg
-            else:
-                return AdaNomMultiwayBranchReg
-
-    @classmethod
-    def _unit_test_params(cls):
-        return {"seed": 1}
+import typing
+from copy import deepcopy
+
+from river import base
+
+from .hoeffding_tree_regressor import HoeffdingTreeRegressor
+from .nodes.branch import DTBranch
+from .nodes.hatr_nodes import (
+    AdaBranchRegressor,
+    AdaLeafRegAdaptive,
+    AdaLeafRegMean,
+    AdaLeafRegModel,
+    AdaLeafRegressor,
+    AdaNomBinaryBranchReg,
+    AdaNomMultiwayBranchReg,
+    AdaNumBinaryBranchReg,
+    AdaNumMultiwayBranchReg,
+)
+from .splitter import Splitter
+
+
+class HoeffdingAdaptiveTreeRegressor(HoeffdingTreeRegressor):
+    """Hoeffding Adaptive Tree regressor (HATR).
+
+    This class implements a regression version of the Hoeffding Adaptive Tree Classifier. Hence,
+    it also uses an ADWIN concept-drift detector instance at each decision node to monitor
+    possible changes in the data distribution. If a drift is detected in a node, an alternate
+    tree begins to be induced in the background. When enough information is gathered, HATR
+    swaps the node where the change was detected by its alternate tree.
+
+    Parameters
+    ----------
+    grace_period
+        Number of instances a leaf should observe between split attempts.
+    max_depth
+        The maximum depth a tree can reach. If `None`, the tree will grow indefinitely.
+    split_confidence
+        Allowed error in split decision, a value closer to 0 takes longer to decide.
+    tie_threshold
+        Threshold below which a split will be forced to break ties.
+    leaf_prediction
+        Prediction mechanism used at leafs.</br>
+        - 'mean' - Target mean</br>
+        - 'model' - Uses the model defined in `leaf_model`</br>
+        - 'adaptive' - Chooses between 'mean' and 'model' dynamically</br>
+    leaf_model
+        The regression model used to provide responses if `leaf_prediction='model'`. If not
+        provided an instance of `river.linear_model.LinearRegression` with the default
+        hyperparameters is used.
+    model_selector_decay
+        The exponential decaying factor applied to the learning models' squared errors, that
+        are monitored if `leaf_prediction='adaptive'`. Must be between `0` and `1`. The closer
+        to `1`, the more importance is going to be given to past observations. On the other hand,
+        if its value approaches `0`, the recent observed errors are going to have more influence
+        on the final decision.
+    nominal_attributes
+        List of Nominal attributes. If empty, then assume that all numeric attributes should
+        be treated as continuous.
+    splitter
+        The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric
+        features and perform splits. Splitters are available in the `tree.splitter` module.
+        Different splitters are available for classification and regression tasks. Classification
+        and regression splitters can be distinguished by their property `is_target_class`.
+        This is an advanced option. Special care must be taken when choosing different splitters.
+        By default, `tree.splitter.EBSTSplitter` is used if `splitter` is `None`.
+    min_samples_split
+        The minimum number of samples every branch resulting from a split candidate must have
+        to be considered valid.
+    bootstrap_sampling
+        If True, perform bootstrap sampling in the leaf nodes.
+    drift_window_threshold
+        Minimum number of examples an alternate tree must observe before being considered as a
+        potential replacement to the current one.
+    adwin_confidence
+        The delta parameter used in the nodes' ADWIN drift detectors.
+    binary_split
+        If True, only allow binary splits.
+    max_size
+        The max size of the tree, in Megabytes (MB).
+    memory_estimate_period
+        Interval (number of processed instances) between memory consumption checks.
+    stop_mem_management
+        If True, stop growing as soon as memory limit is hit.
+    remove_poor_attrs
+        If True, disable poor attributes to reduce memory usage.
+    merit_preprune
+        If True, enable merit-based tree pre-pruning.
+    seed
+       If int, `seed` is the seed used by the random number generator;</br>
+       If RandomState instance, `seed` is the random number generator;</br>
+       If None, the random number generator is the RandomState instance used
+       by `np.random`. Only used when `bootstrap_sampling=True` to direct the
+       bootstrap sampling.</br>
+
+    Notes
+    -----
+    The Hoeffding Adaptive Tree [^1] uses ADWIN [^2] to monitor performance of branches on the tree
+    and to replace them with new branches when their accuracy decreases if the new branches are
+    more accurate.
+
+    The bootstrap sampling strategy is an improvement over the original Hoeffding Adaptive Tree
+    algorithm. It is enabled by default since, in general, it results in better performance.
+
+    To cope with ADWIN's requirements of bounded input data, HATR uses a novel error normalization
+    strategy based on the empiral rule of Gaussian distributions. We assume the deviations
+    of the predictions from the expected values follow a normal distribution. Hence, we subject
+    these errors to a min-max normalization assuming that most of the data lies in the
+    $\\left[-3\\sigma, 3\\sigma\\right]$ range. These normalized errors are passed to the ADWIN
+    instances. This is the same strategy used by Adaptive Random Forest Regressor.
+
+    References
+    ----------
+    [^1]: Bifet, Albert, and Ricard Gavaldà. "Adaptive learning from evolving data streams."
+    In International Symposium on Intelligent Data Analysis, pp. 249-260. Springer, Berlin,
+    Heidelberg, 2009.
+    [^2]: Bifet, Albert, and Ricard Gavaldà. "Learning from time-changing data with adaptive
+    windowing." In Proceedings of the 2007 SIAM international conference on data mining,
+    pp. 443-448. Society for Industrial and Applied Mathematics, 2007.
+
+    Examples
+    --------
+    >>> from river import datasets
+    >>> from river import evaluate
+    >>> from river import metrics
+    >>> from river import tree
+    >>> from river import preprocessing
+
+    >>> dataset = datasets.TrumpApproval()
+
+    >>> model = (
+    ...     preprocessing.StandardScaler() |
+    ...     tree.HoeffdingAdaptiveTreeRegressor(
+    ...         grace_period=50,
+    ...         leaf_prediction='adaptive',
+    ...         model_selector_decay=0.3,
+    ...         seed=0
+    ...     )
+    ... )
+
+    >>> metric = metrics.MAE()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    MAE: 0.719929
+    """
+
+    def __init__(
+        self,
+        grace_period: int = 200,
+        max_depth: int = None,
+        split_confidence: float = 1e-7,
+        tie_threshold: float = 0.05,
+        leaf_prediction: str = "model",
+        leaf_model: base.Regressor = None,
+        model_selector_decay: float = 0.95,
+        nominal_attributes: list = None,
+        splitter: Splitter = None,
+        min_samples_split: int = 5,
+        bootstrap_sampling: bool = True,
+        drift_window_threshold: int = 300,
+        adwin_confidence: float = 0.002,
+        binary_split: bool = False,
+        max_size: int = 100,
+        memory_estimate_period: int = 1000000,
+        stop_mem_management: bool = False,
+        remove_poor_attrs: bool = False,
+        merit_preprune: bool = True,
+        seed=None,
+    ):
+
+        super().__init__(
+            grace_period=grace_period,
+            max_depth=max_depth,
+            split_confidence=split_confidence,
+            tie_threshold=tie_threshold,
+            leaf_prediction=leaf_prediction,
+            leaf_model=leaf_model,
+            model_selector_decay=model_selector_decay,
+            nominal_attributes=nominal_attributes,
+            splitter=splitter,
+            min_samples_split=min_samples_split,
+            binary_split=binary_split,
+            max_size=max_size,
+            memory_estimate_period=memory_estimate_period,
+            stop_mem_management=stop_mem_management,
+            remove_poor_attrs=remove_poor_attrs,
+            merit_preprune=merit_preprune,
+        )
+
+        self._n_alternate_trees = 0
+        self._n_pruned_alternate_trees = 0
+        self._n_switch_alternate_trees = 0
+
+        self.bootstrap_sampling = bootstrap_sampling
+        self.drift_window_threshold = drift_window_threshold
+        self.adwin_confidence = adwin_confidence
+        self.seed = seed
+
+    @property
+    def n_alternate_trees(self):
+        return self._n_alternate_trees
+
+    @property
+    def n_pruned_alternate_trees(self):
+        return self._n_pruned_alternate_trees
+
+    @property
+    def n_switch_alternate_trees(self):
+        return self._n_switch_alternate_trees
+
+    @property
+    def summary(self):
+        summ = super().summary
+        summ.update(
+            {
+                "n_alternate_trees": self.n_alternate_trees,
+                "n_pruned_alternate_trees": self.n_pruned_alternate_trees,
+                "n_switch_alternate_trees": self.n_switch_alternate_trees,
+            }
+        )
+        return summ
+
+    def learn_one(self, x, y, *, sample_weight=1.0):
+        self._train_weight_seen_by_model += sample_weight
+
+        if self._root is None:
+            self._root = self._new_leaf()
+            self._n_active_leaves = 1
+        self._root.learn_one(x, y, sample_weight=sample_weight, tree=self)
+
+        if self._train_weight_seen_by_model % self.memory_estimate_period == 0:
+            self._estimate_model_size()
+
+        return self
+
+    def predict_one(self, x):
+        pred = 0.0
+        if self._root is not None:
+            found_nodes = [self._root]
+            if isinstance(self._root, DTBranch):
+                found_nodes = self._root.traverse(x, until_leaf=True)
+            for leaf in found_nodes:
+                pred += leaf.prediction(x, tree=self)
+            # Mean prediction among the reached leaves
+            pred /= len(found_nodes)
+
+        return pred
+
+    def _new_leaf(self, initial_stats=None, parent=None, is_active=True):
+        """Create a new learning node.
+
+        The type of learning node depends on the tree configuration.
+        """
+        if parent is not None:
+            depth = parent.depth + 1
+        else:
+            depth = 0
+
+        leaf_model = None
+        if self.leaf_prediction in {self._MODEL, self._ADAPTIVE}:
+            if parent is None:
+                leaf_model = deepcopy(self.leaf_model)
+            else:
+                try:
+                    leaf_model = deepcopy(parent._leaf_model)  # noqa
+                except AttributeError:
+                    leaf_model = deepcopy(self.leaf_model)
+
+        if self.leaf_prediction == self._TARGET_MEAN:
+            return AdaLeafRegMean(
+                initial_stats,
+                depth,
+                self.splitter,
+                adwin_delta=self.adwin_confidence,
+                seed=self.seed,
+            )
+        elif self.leaf_prediction == self._MODEL:
+            return AdaLeafRegModel(
+                initial_stats,
+                depth,
+                self.splitter,
+                adwin_delta=self.adwin_confidence,
+                seed=self.seed,
+                leaf_model=leaf_model,
+            )
+        else:  # adaptive learning node
+            new_adaptive = AdaLeafRegAdaptive(
+                initial_stats,
+                depth,
+                self.splitter,
+                adwin_delta=self.adwin_confidence,
+                seed=self.seed,
+                leaf_model=leaf_model,
+            )
+            if parent is not None and isinstance(parent, AdaLeafRegressor):
+                new_adaptive._fmse_mean = parent._fmse_mean  # noqa
+                new_adaptive._fmse_model = parent._fmse_model  # noqa
+
+            return new_adaptive
+
+    def _branch_selector(
+        self, numerical_feature=True, multiway_split=False
+    ) -> typing.Type[AdaBranchRegressor]:
+        """Create a new split node."""
+        if numerical_feature:
+            if not multiway_split:
+                return AdaNumBinaryBranchReg
+            else:
+                return AdaNumMultiwayBranchReg
+        else:
+            if not multiway_split:
+                return AdaNomBinaryBranchReg
+            else:
+                return AdaNomMultiwayBranchReg
+
+    @classmethod
+    def _unit_test_params(cls):
+        yield {"seed": 1}
```

### Comparing `river-0.8.0/river/tree/hoeffding_tree.py` & `river-0.9.0/river/tree/hoeffding_tree.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,545 +1,545 @@
-import collections
-import functools
-import io
-import math
-import typing
-from abc import ABC, abstractmethod
-
-from river import base
-from river.utils.skmultiflow_utils import (
-    calculate_object_size,
-    normalize_values_in_dict,
-)
-
-from .nodes.branch import (
-    DTBranch,
-    NominalBinaryBranch,
-    NominalMultiwayBranch,
-    NumericBinaryBranch,
-    NumericMultiwayBranch,
-)
-from .nodes.leaf import HTLeaf
-
-try:
-    import graphviz
-
-    GRAPHVIZ_INSTALLED = True
-except ImportError:
-    GRAPHVIZ_INSTALLED = False
-
-
-class HoeffdingTree(ABC):
-    """Base class for Hoeffding Decision Trees.
-
-    This is an **abstract class**, so it cannot be used directly. It defines base operations
-    and properties that all the Hoeffding decision trees must inherit or implement according to
-    their own design.
-
-    Parameters
-    ----------
-    max_depth
-        The maximum depth a tree can reach. If `None`, the tree will grow indefinitely.
-    binary_split
-        If True, only allow binary splits.
-    max_size
-        The max size of the tree, in Megabytes (MB).
-    memory_estimate_period
-        Interval (number of processed instances) between memory consumption checks.
-    stop_mem_management
-        If True, stop growing as soon as memory limit is hit.
-    remove_poor_attrs
-        If True, disable poor attributes to reduce memory usage.
-    merit_preprune
-        If True, enable merit-based tree pre-pruning.
-
-    """
-
-    def __init__(
-        self,
-        max_depth: int = None,
-        binary_split: bool = False,
-        max_size: int = 100,
-        memory_estimate_period: int = 1000000,
-        stop_mem_management: bool = False,
-        remove_poor_attrs: bool = False,
-        merit_preprune: bool = True,
-    ):
-        # Properties common to all the Hoeffding trees
-        self._split_criterion: str = ""
-        self._leaf_prediction: str = ""
-
-        self.max_depth: float = max_depth if max_depth is not None else math.inf
-        self.binary_split: bool = binary_split
-        self._max_size: float = max_size
-        self._max_byte_size: float = self._max_size * (2 ** 20)  # convert to byte
-        self.memory_estimate_period: int = memory_estimate_period
-        self.stop_mem_management: bool = stop_mem_management
-        self.remove_poor_attrs: bool = remove_poor_attrs
-        self.merit_preprune: bool = merit_preprune
-
-        self._root: typing.Union[DTBranch, HTLeaf, None] = None
-        self._n_active_leaves: int = 0
-        self._n_inactive_leaves: int = 0
-        self._inactive_leaf_size_estimate: float = 0.0
-        self._active_leaf_size_estimate: float = 0.0
-        self._size_estimate_overhead_fraction: float = 1.0
-        self._growth_allowed = True
-        self._train_weight_seen_by_model: float = 0.0
-
-    @staticmethod
-    def _hoeffding_bound(range_val, confidence, n):
-        r"""Compute the Hoeffding bound, used to decide how many samples are necessary at each
-        node.
-
-        Notes
-        -----
-        The Hoeffding bound is defined as:
-
-        $\\epsilon = \\sqrt{\\frac{R^2\\ln(1/\\delta))}{2n}}$
-
-        where:
-
-        $\\epsilon$: Hoeffding bound.
-        $R$: Range of a random variable. For a probability the range is 1, and for an
-        information gain the range is log *c*, where *c* is the number of classes.
-        $\\delta$: Confidence. 1 minus the desired probability of choosing the correct
-        attribute at any given node.
-        $n$: Number of samples.
-
-        Parameters
-        ----------
-        range_val
-            Range value.
-        confidence
-            Confidence of choosing the correct attribute.
-        n
-            Number of processed samples.
-        """
-        return math.sqrt(
-            (range_val * range_val * math.log(1.0 / confidence)) / (2.0 * n)
-        )
-
-    @property
-    def max_size(self):
-        """Max allowed size tree can reach (in MB)."""
-        return self._max_size
-
-    @max_size.setter
-    def max_size(self, size):
-        self._max_size = size
-        self._max_byte_size = self._max_size * (2 ** 20)
-
-    @property
-    def height(self) -> int:
-        if self._root:
-            return self._root.height
-
-    @property
-    def n_nodes(self):
-        if self._root:
-            return self._root.n_nodes
-
-    @property
-    def n_branches(self):
-        if self._root:
-            return self._root.n_branches
-
-    @property
-    def n_leaves(self):
-        if self._root:
-            return self._root.n_leaves
-
-    @property
-    def n_active_leaves(self):
-        return self._n_active_leaves
-
-    @property
-    def n_inactive_leaves(self):
-        return self._n_inactive_leaves
-
-    @property
-    def summary(self):
-        """Collect metrics corresponding to the current status of the tree
-        in a string buffer.
-        """
-        summary = {
-            "n_nodes": self.n_nodes,
-            "n_branches": self.n_branches,
-            "n_leaves": self.n_leaves,
-            "n_active_leaves": self.n_active_leaves,
-            "n_inactive_leaves": self.n_inactive_leaves,
-            "height": self.height,
-            "total_observed_weight": self._train_weight_seen_by_model,
-        }
-        return summary
-
-    def to_dataframe(self):
-        """Return a representation of the current tree structure organized in a
-        `pandas.DataFrame` object.
-
-        In case the tree is empty or it only contains a single node (a leaf), `None` is returned.
-
-        Returns
-        -------
-        df
-            A `pandas.DataFrame` depicting the tree structure.
-        """
-        if self._root is not None and isinstance(self._root, DTBranch):
-            return self._root.to_dataframe()
-
-    def _branch_selector(
-        self, numerical_feature=True, multiway_split=False
-    ) -> typing.Type[DTBranch]:
-        """Create a new split node."""
-        if numerical_feature:
-            if not multiway_split:
-                return NumericBinaryBranch
-            else:
-                return NumericMultiwayBranch
-        else:
-            if not multiway_split:
-                return NominalBinaryBranch
-            else:
-                return NominalMultiwayBranch
-
-    @abstractmethod
-    def _new_leaf(
-        self, initial_stats: dict = None, parent: typing.Union[HTLeaf, DTBranch] = None
-    ) -> HTLeaf:
-        """Create a new learning node.
-
-        The characteristics of the learning node depends on the tree algorithm.
-
-        Parameters
-        ----------
-        initial_stats
-            Target statistics set from the parent node.
-        parent
-            Parent node to inherit from.
-
-        Returns
-        -------
-        A new learning node.
-        """
-
-    @property
-    def split_criterion(self) -> str:
-        """Return a string with the name of the split criterion being used by the tree. """
-        return self._split_criterion
-
-    @split_criterion.setter
-    @abstractmethod
-    def split_criterion(self, split_criterion):
-        """Define the split criterion to be used by the tree. """
-
-    @property
-    def leaf_prediction(self) -> str:
-        """Return the prediction strategy used by the tree at its leaves. """
-        return self._leaf_prediction
-
-    @leaf_prediction.setter
-    @abstractmethod
-    def leaf_prediction(self, leaf_prediction):
-        """Define the prediction strategy used by the tree in its leaves."""
-
-    def _enforce_size_limit(self):
-        """Track the size of the tree and disable/enable nodes if required.
-
-        This memory-management routine shared by all the Hoeffding Trees is based on [^1].
-
-        References
-        ----------
-        [^1]: Kirkby, R.B., 2007. Improving hoeffding trees (Doctoral dissertation,
-        The University of Waikato).
-        """
-        tree_size = self._size_estimate_overhead_fraction * (
-            self._active_leaf_size_estimate
-            + self._n_inactive_leaves * self._inactive_leaf_size_estimate
-        )
-        if self._n_inactive_leaves > 0 or tree_size > self._max_byte_size:
-            if self.stop_mem_management:
-                self._growth_allowed = False
-                return
-        leaves = self._find_leaves()
-        leaves.sort(key=lambda leaf: leaf.calculate_promise())
-        max_active = 0
-        while max_active < len(leaves):
-            max_active += 1
-            if (
-                (
-                    max_active * self._active_leaf_size_estimate
-                    + (len(leaves) - max_active) * self._inactive_leaf_size_estimate
-                )
-                * self._size_estimate_overhead_fraction
-            ) > self._max_byte_size:
-                max_active -= 1
-                break
-        cutoff = len(leaves) - max_active
-        for i in range(cutoff):
-            if leaves[i].is_active():
-                leaves[i].deactivate()
-                self._n_inactive_leaves += 1
-                self._n_active_leaves -= 1
-        for i in range(cutoff, len(leaves)):
-            if not leaves[i].is_active() and leaves[i].depth < self.max_depth:
-                leaves[i].activate()
-                self._n_active_leaves += 1
-                self._n_inactive_leaves -= 1
-
-    def _estimate_model_size(self):
-        """Calculate the size of the model and trigger tracker function
-        if the actual model size exceeds the max size in the configuration.
-
-        This memory-management routine shared by all the Hoeffding Trees is based on [^1].
-
-        References
-        ----------
-        [^1]: Kirkby, R.B., 2007. Improving hoeffding trees (Doctoral dissertation,
-        The University of Waikato).
-        """
-        leaves = self._find_leaves()
-        total_active_size = 0
-        total_inactive_size = 0
-        for leaf in leaves:
-            if leaf.is_active():
-                total_active_size += calculate_object_size(leaf)
-            else:
-                total_inactive_size += calculate_object_size(leaf)
-        if total_active_size > 0:
-            self._active_leaf_size_estimate = total_active_size / self._n_active_leaves
-        if total_inactive_size > 0:
-            self._inactive_leaf_size_estimate = (
-                total_inactive_size / self._n_inactive_leaves
-            )
-        actual_model_size = calculate_object_size(self)
-        estimated_model_size = (
-            self._n_active_leaves * self._active_leaf_size_estimate
-            + self._n_inactive_leaves * self._inactive_leaf_size_estimate
-        )
-        self._size_estimate_overhead_fraction = actual_model_size / estimated_model_size
-        if actual_model_size > self._max_byte_size:
-            self._enforce_size_limit()
-
-    def _deactivate_all_leaves(self):
-        """Deactivate all leaves. """
-        leaves = self._find_leaves()
-        for leaf in leaves:
-            leaf.deactivate()
-            self._n_inactive_leaves += 1
-            self._n_active_leaves -= 1
-
-    def _find_leaves(self) -> typing.List[HTLeaf]:
-        """Find learning nodes in the tree.
-
-        Returns
-        -------
-        List of learning nodes in the tree.
-        """
-        return [leaf for leaf in self._root.iter_leaves()]
-
-    # Adapted from creme's original implementation
-    def debug_one(self, x: dict) -> typing.Union[str, None]:
-        """Print an explanation of how `x` is predicted.
-
-        Parameters
-        ----------
-        x
-            A dictionary of features.
-
-        Returns
-        -------
-            A representation of the path followed by the tree to predict `x`; `None` if
-            the tree is empty.
-
-        Notes
-        -----
-        Currently, Label Combination Hoeffding Tree Classifier (for multi-label
-        classification) is not supported.
-        """
-        if self._root is None:
-            return
-
-        # We'll redirect all the print statement to a buffer, we'll return the content of the
-        # buffer at the end
-        buffer = io.StringIO()
-        _print = functools.partial(print, file=buffer)
-
-        for node in self._root.walk(x, until_leaf=True):
-            if isinstance(node, HTLeaf):
-                _print(repr(node))
-            else:
-                try:
-                    child_index = node.branch_no(x)  # noqa
-                except KeyError:
-                    child_index, _ = node.most_common_path()
-
-                _print(node.repr_branch(child_index))  # noqa
-
-        return buffer.getvalue()
-
-    def draw(self, max_depth: int = None):
-        """Draw the tree using the `graphviz` library.
-
-        Since the tree is drawn without passing incoming samples, classification trees
-        will show the majority class in their leaves, whereas regression trees will
-        use the target mean.
-
-        Parameters
-        ----------
-        max_depth
-            Only the root will be drawn when set to `0`. Every node will be drawn when
-            set to `None`.
-
-        Notes
-        -----
-        Currently, Label Combination Hoeffding Tree Classifier (for multi-label
-        classification) is not supported.
-
-        Examples
-        --------
-        >>> from river import datasets
-        >>> from river import tree
-        >>> model = tree.HoeffdingTreeClassifier(
-        ...    grace_period=5,
-        ...    split_confidence=1e-5,
-        ...    split_criterion='gini',
-        ...    max_depth=10,
-        ...    tie_threshold=0.05,
-        ... )
-        >>> for x, y in datasets.Phishing():
-        ...    model = model.learn_one(x, y)
-        >>> dot = model.draw()
-
-        .. image:: ../../docs/img/dtree_draw.svg
-            :align: center
-        """
-        counter = 0
-
-        def iterate(node=None):
-            if node is None:
-                yield None, None, self._root, 0, None
-                yield from iterate(self._root)
-
-            nonlocal counter
-            parent_no = counter
-
-            if isinstance(node, DTBranch):
-                for branch_index, child in enumerate(node.children):
-                    counter += 1
-                    yield parent_no, node, child, counter, branch_index
-                    if isinstance(child, DTBranch):
-                        yield from iterate(child)
-
-        if max_depth is None:
-            max_depth = math.inf
-
-        dot = graphviz.Digraph(
-            graph_attr={"splines": "ortho", "forcelabels": "true", "overlap": "false"},
-            node_attr={
-                "shape": "box",
-                "penwidth": "1.2",
-                "fontname": "trebuchet",
-                "fontsize": "11",
-                "margin": "0.1,0.0",
-            },
-            edge_attr={"penwidth": "0.6", "center": "true", "fontsize": "7  "},
-        )
-
-        if isinstance(self, base.Classifier):
-            n_colors = len(self.classes)  # noqa
-        else:
-            n_colors = 1
-
-        # Pick a color palette which maps classes to colors
-        new_color = functools.partial(next, iter(_color_brew(n_colors)))
-        palette = collections.defaultdict(new_color)
-
-        for parent_no, parent, child, child_no, branch_index in iterate():
-            if child.depth > max_depth:
-                continue
-
-            if isinstance(child, DTBranch):
-                text = f"{child.feature}"  # noqa
-            else:
-                text = f"{repr(child)}\nsamples: {int(child.total_weight)}"
-
-            # Pick a color, the hue depends on the class and the transparency on the distribution
-            if isinstance(self, base.Classifier):
-                class_proba = normalize_values_in_dict(child.stats, inplace=False)
-                mode = max(class_proba, key=class_proba.get)
-                p_mode = class_proba[mode]
-                try:
-                    alpha = (p_mode - 1 / n_colors) / (1 - 1 / n_colors)
-                    fillcolor = str(transparency_hex(color=palette[mode], alpha=alpha))
-                except ZeroDivisionError:
-                    fillcolor = "#FFFFFF"
-            else:
-                fillcolor = "#FFFFFF"
-
-            dot.node(f"{child_no}", text, fillcolor=fillcolor, style="filled")
-
-            if parent_no is not None:
-                dot.edge(
-                    f"{parent_no}",
-                    f"{child_no}",
-                    xlabel=parent.repr_branch(branch_index, shorten=True),
-                )
-
-        return dot
-
-
-# Utility adapted from the original creme's implementation
-def _color_brew(n: int) -> typing.List[typing.Tuple[int, int, int]]:
-    """Generate n colors with equally spaced hues.
-
-    Parameters
-    ----------
-    n
-        The number of required colors.
-
-    Returns
-    -------
-        List of n tuples of form (R, G, B) being the components of each color.
-    References
-    ----------
-    https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py
-    """
-    colors = []
-
-    # Initialize saturation & value; calculate chroma & value shift
-    s, v = 0.75, 0.9
-    c = s * v
-    m = v - c
-
-    for h in [i for i in range(25, 385, int(360 / n))]:
-
-        # Calculate some intermediate values
-        h_bar = h / 60.0
-        x = c * (1 - abs((h_bar % 2) - 1))
-
-        # Initialize RGB with same hue & chroma as our color
-        rgb = [
-            (c, x, 0),
-            (x, c, 0),
-            (0, c, x),
-            (0, x, c),
-            (x, 0, c),
-            (c, 0, x),
-            (c, x, 0),
-        ]
-        r, g, b = rgb[int(h_bar)]
-
-        # Shift the initial RGB values to match value and store
-        colors.append(
-            ((int(255 * (r + m))), (int(255 * (g + m))), (int(255 * (b + m))))
-        )
-
-    return colors
-
-
-# Utility adapted from the original creme's implementation
-def transparency_hex(color: typing.Tuple[int, int, int], alpha: float) -> str:
-    """Apply alpha coefficient on hexadecimal color."""
-    return "#%02x%02x%02x" % tuple(
-        [int(round(alpha * c + (1 - alpha) * 255, 0)) for c in color]
-    )
+import collections
+import functools
+import io
+import math
+import typing
+from abc import ABC, abstractmethod
+
+from river import base
+from river.utils.skmultiflow_utils import (
+    calculate_object_size,
+    normalize_values_in_dict,
+)
+
+from .nodes.branch import (
+    DTBranch,
+    NominalBinaryBranch,
+    NominalMultiwayBranch,
+    NumericBinaryBranch,
+    NumericMultiwayBranch,
+)
+from .nodes.leaf import HTLeaf
+
+try:
+    import graphviz
+
+    GRAPHVIZ_INSTALLED = True
+except ImportError:
+    GRAPHVIZ_INSTALLED = False
+
+
+class HoeffdingTree(ABC):
+    """Base class for Hoeffding Decision Trees.
+
+    This is an **abstract class**, so it cannot be used directly. It defines base operations
+    and properties that all the Hoeffding decision trees must inherit or implement according to
+    their own design.
+
+    Parameters
+    ----------
+    max_depth
+        The maximum depth a tree can reach. If `None`, the tree will grow indefinitely.
+    binary_split
+        If True, only allow binary splits.
+    max_size
+        The max size of the tree, in Megabytes (MB).
+    memory_estimate_period
+        Interval (number of processed instances) between memory consumption checks.
+    stop_mem_management
+        If True, stop growing as soon as memory limit is hit.
+    remove_poor_attrs
+        If True, disable poor attributes to reduce memory usage.
+    merit_preprune
+        If True, enable merit-based tree pre-pruning.
+
+    """
+
+    def __init__(
+        self,
+        max_depth: int = None,
+        binary_split: bool = False,
+        max_size: int = 100,
+        memory_estimate_period: int = 1000000,
+        stop_mem_management: bool = False,
+        remove_poor_attrs: bool = False,
+        merit_preprune: bool = True,
+    ):
+        # Properties common to all the Hoeffding trees
+        self._split_criterion: str = ""
+        self._leaf_prediction: str = ""
+
+        self.max_depth: float = max_depth if max_depth is not None else math.inf
+        self.binary_split: bool = binary_split
+        self._max_size: float = max_size
+        self._max_byte_size: float = self._max_size * (2 ** 20)  # convert to byte
+        self.memory_estimate_period: int = memory_estimate_period
+        self.stop_mem_management: bool = stop_mem_management
+        self.remove_poor_attrs: bool = remove_poor_attrs
+        self.merit_preprune: bool = merit_preprune
+
+        self._root: typing.Union[DTBranch, HTLeaf, None] = None
+        self._n_active_leaves: int = 0
+        self._n_inactive_leaves: int = 0
+        self._inactive_leaf_size_estimate: float = 0.0
+        self._active_leaf_size_estimate: float = 0.0
+        self._size_estimate_overhead_fraction: float = 1.0
+        self._growth_allowed = True
+        self._train_weight_seen_by_model: float = 0.0
+
+    @staticmethod
+    def _hoeffding_bound(range_val, confidence, n):
+        r"""Compute the Hoeffding bound, used to decide how many samples are necessary at each
+        node.
+
+        Notes
+        -----
+        The Hoeffding bound is defined as:
+
+        $\\epsilon = \\sqrt{\\frac{R^2\\ln(1/\\delta))}{2n}}$
+
+        where:
+
+        $\\epsilon$: Hoeffding bound.
+        $R$: Range of a random variable. For a probability the range is 1, and for an
+        information gain the range is log *c*, where *c* is the number of classes.
+        $\\delta$: Confidence. 1 minus the desired probability of choosing the correct
+        attribute at any given node.
+        $n$: Number of samples.
+
+        Parameters
+        ----------
+        range_val
+            Range value.
+        confidence
+            Confidence of choosing the correct attribute.
+        n
+            Number of processed samples.
+        """
+        return math.sqrt(
+            (range_val * range_val * math.log(1.0 / confidence)) / (2.0 * n)
+        )
+
+    @property
+    def max_size(self):
+        """Max allowed size tree can reach (in MB)."""
+        return self._max_size
+
+    @max_size.setter
+    def max_size(self, size):
+        self._max_size = size
+        self._max_byte_size = self._max_size * (2 ** 20)
+
+    @property
+    def height(self) -> int:
+        if self._root:
+            return self._root.height
+
+    @property
+    def n_nodes(self):
+        if self._root:
+            return self._root.n_nodes
+
+    @property
+    def n_branches(self):
+        if self._root:
+            return self._root.n_branches
+
+    @property
+    def n_leaves(self):
+        if self._root:
+            return self._root.n_leaves
+
+    @property
+    def n_active_leaves(self):
+        return self._n_active_leaves
+
+    @property
+    def n_inactive_leaves(self):
+        return self._n_inactive_leaves
+
+    @property
+    def summary(self):
+        """Collect metrics corresponding to the current status of the tree
+        in a string buffer.
+        """
+        summary = {
+            "n_nodes": self.n_nodes,
+            "n_branches": self.n_branches,
+            "n_leaves": self.n_leaves,
+            "n_active_leaves": self.n_active_leaves,
+            "n_inactive_leaves": self.n_inactive_leaves,
+            "height": self.height,
+            "total_observed_weight": self._train_weight_seen_by_model,
+        }
+        return summary
+
+    def to_dataframe(self):
+        """Return a representation of the current tree structure organized in a
+        `pandas.DataFrame` object.
+
+        In case the tree is empty or it only contains a single node (a leaf), `None` is returned.
+
+        Returns
+        -------
+        df
+            A `pandas.DataFrame` depicting the tree structure.
+        """
+        if self._root is not None and isinstance(self._root, DTBranch):
+            return self._root.to_dataframe()
+
+    def _branch_selector(
+        self, numerical_feature=True, multiway_split=False
+    ) -> typing.Type[DTBranch]:
+        """Create a new split node."""
+        if numerical_feature:
+            if not multiway_split:
+                return NumericBinaryBranch
+            else:
+                return NumericMultiwayBranch
+        else:
+            if not multiway_split:
+                return NominalBinaryBranch
+            else:
+                return NominalMultiwayBranch
+
+    @abstractmethod
+    def _new_leaf(
+        self, initial_stats: dict = None, parent: typing.Union[HTLeaf, DTBranch] = None
+    ) -> HTLeaf:
+        """Create a new learning node.
+
+        The characteristics of the learning node depends on the tree algorithm.
+
+        Parameters
+        ----------
+        initial_stats
+            Target statistics set from the parent node.
+        parent
+            Parent node to inherit from.
+
+        Returns
+        -------
+        A new learning node.
+        """
+
+    @property
+    def split_criterion(self) -> str:
+        """Return a string with the name of the split criterion being used by the tree. """
+        return self._split_criterion
+
+    @split_criterion.setter
+    @abstractmethod
+    def split_criterion(self, split_criterion):
+        """Define the split criterion to be used by the tree. """
+
+    @property
+    def leaf_prediction(self) -> str:
+        """Return the prediction strategy used by the tree at its leaves. """
+        return self._leaf_prediction
+
+    @leaf_prediction.setter
+    @abstractmethod
+    def leaf_prediction(self, leaf_prediction):
+        """Define the prediction strategy used by the tree in its leaves."""
+
+    def _enforce_size_limit(self):
+        """Track the size of the tree and disable/enable nodes if required.
+
+        This memory-management routine shared by all the Hoeffding Trees is based on [^1].
+
+        References
+        ----------
+        [^1]: Kirkby, R.B., 2007. Improving hoeffding trees (Doctoral dissertation,
+        The University of Waikato).
+        """
+        tree_size = self._size_estimate_overhead_fraction * (
+            self._active_leaf_size_estimate
+            + self._n_inactive_leaves * self._inactive_leaf_size_estimate
+        )
+        if self._n_inactive_leaves > 0 or tree_size > self._max_byte_size:
+            if self.stop_mem_management:
+                self._growth_allowed = False
+                return
+        leaves = self._find_leaves()
+        leaves.sort(key=lambda leaf: leaf.calculate_promise())
+        max_active = 0
+        while max_active < len(leaves):
+            max_active += 1
+            if (
+                (
+                    max_active * self._active_leaf_size_estimate
+                    + (len(leaves) - max_active) * self._inactive_leaf_size_estimate
+                )
+                * self._size_estimate_overhead_fraction
+            ) > self._max_byte_size:
+                max_active -= 1
+                break
+        cutoff = len(leaves) - max_active
+        for i in range(cutoff):
+            if leaves[i].is_active():
+                leaves[i].deactivate()
+                self._n_inactive_leaves += 1
+                self._n_active_leaves -= 1
+        for i in range(cutoff, len(leaves)):
+            if not leaves[i].is_active() and leaves[i].depth < self.max_depth:
+                leaves[i].activate()
+                self._n_active_leaves += 1
+                self._n_inactive_leaves -= 1
+
+    def _estimate_model_size(self):
+        """Calculate the size of the model and trigger tracker function
+        if the actual model size exceeds the max size in the configuration.
+
+        This memory-management routine shared by all the Hoeffding Trees is based on [^1].
+
+        References
+        ----------
+        [^1]: Kirkby, R.B., 2007. Improving hoeffding trees (Doctoral dissertation,
+        The University of Waikato).
+        """
+        leaves = self._find_leaves()
+        total_active_size = 0
+        total_inactive_size = 0
+        for leaf in leaves:
+            if leaf.is_active():
+                total_active_size += calculate_object_size(leaf)
+            else:
+                total_inactive_size += calculate_object_size(leaf)
+        if total_active_size > 0:
+            self._active_leaf_size_estimate = total_active_size / self._n_active_leaves
+        if total_inactive_size > 0:
+            self._inactive_leaf_size_estimate = (
+                total_inactive_size / self._n_inactive_leaves
+            )
+        actual_model_size = calculate_object_size(self)
+        estimated_model_size = (
+            self._n_active_leaves * self._active_leaf_size_estimate
+            + self._n_inactive_leaves * self._inactive_leaf_size_estimate
+        )
+        self._size_estimate_overhead_fraction = actual_model_size / estimated_model_size
+        if actual_model_size > self._max_byte_size:
+            self._enforce_size_limit()
+
+    def _deactivate_all_leaves(self):
+        """Deactivate all leaves. """
+        leaves = self._find_leaves()
+        for leaf in leaves:
+            leaf.deactivate()
+            self._n_inactive_leaves += 1
+            self._n_active_leaves -= 1
+
+    def _find_leaves(self) -> typing.List[HTLeaf]:
+        """Find learning nodes in the tree.
+
+        Returns
+        -------
+        List of learning nodes in the tree.
+        """
+        return [leaf for leaf in self._root.iter_leaves()]
+
+    # Adapted from creme's original implementation
+    def debug_one(self, x: dict) -> typing.Union[str, None]:
+        """Print an explanation of how `x` is predicted.
+
+        Parameters
+        ----------
+        x
+            A dictionary of features.
+
+        Returns
+        -------
+            A representation of the path followed by the tree to predict `x`; `None` if
+            the tree is empty.
+
+        Notes
+        -----
+        Currently, Label Combination Hoeffding Tree Classifier (for multi-label
+        classification) is not supported.
+        """
+        if self._root is None:
+            return
+
+        # We'll redirect all the print statement to a buffer, we'll return the content of the
+        # buffer at the end
+        buffer = io.StringIO()
+        _print = functools.partial(print, file=buffer)
+
+        for node in self._root.walk(x, until_leaf=True):
+            if isinstance(node, HTLeaf):
+                _print(repr(node))
+            else:
+                try:
+                    child_index = node.branch_no(x)  # noqa
+                except KeyError:
+                    child_index, _ = node.most_common_path()
+
+                _print(node.repr_branch(child_index))  # noqa
+
+        return buffer.getvalue()
+
+    def draw(self, max_depth: int = None):
+        """Draw the tree using the `graphviz` library.
+
+        Since the tree is drawn without passing incoming samples, classification trees
+        will show the majority class in their leaves, whereas regression trees will
+        use the target mean.
+
+        Parameters
+        ----------
+        max_depth
+            Only the root will be drawn when set to `0`. Every node will be drawn when
+            set to `None`.
+
+        Notes
+        -----
+        Currently, Label Combination Hoeffding Tree Classifier (for multi-label
+        classification) is not supported.
+
+        Examples
+        --------
+        >>> from river import datasets
+        >>> from river import tree
+        >>> model = tree.HoeffdingTreeClassifier(
+        ...    grace_period=5,
+        ...    split_confidence=1e-5,
+        ...    split_criterion='gini',
+        ...    max_depth=10,
+        ...    tie_threshold=0.05,
+        ... )
+        >>> for x, y in datasets.Phishing():
+        ...    model = model.learn_one(x, y)
+        >>> dot = model.draw()
+
+        .. image:: ../../docs/img/dtree_draw.svg
+            :align: center
+        """
+        counter = 0
+
+        def iterate(node=None):
+            if node is None:
+                yield None, None, self._root, 0, None
+                yield from iterate(self._root)
+
+            nonlocal counter
+            parent_no = counter
+
+            if isinstance(node, DTBranch):
+                for branch_index, child in enumerate(node.children):
+                    counter += 1
+                    yield parent_no, node, child, counter, branch_index
+                    if isinstance(child, DTBranch):
+                        yield from iterate(child)
+
+        if max_depth is None:
+            max_depth = math.inf
+
+        dot = graphviz.Digraph(
+            graph_attr={"splines": "ortho", "forcelabels": "true", "overlap": "false"},
+            node_attr={
+                "shape": "box",
+                "penwidth": "1.2",
+                "fontname": "trebuchet",
+                "fontsize": "11",
+                "margin": "0.1,0.0",
+            },
+            edge_attr={"penwidth": "0.6", "center": "true", "fontsize": "7  "},
+        )
+
+        if isinstance(self, base.Classifier):
+            n_colors = len(self.classes)  # noqa
+        else:
+            n_colors = 1
+
+        # Pick a color palette which maps classes to colors
+        new_color = functools.partial(next, iter(_color_brew(n_colors)))
+        palette = collections.defaultdict(new_color)
+
+        for parent_no, parent, child, child_no, branch_index in iterate():
+            if child.depth > max_depth:
+                continue
+
+            if isinstance(child, DTBranch):
+                text = f"{child.feature}"  # noqa
+            else:
+                text = f"{repr(child)}\nsamples: {int(child.total_weight)}"
+
+            # Pick a color, the hue depends on the class and the transparency on the distribution
+            if isinstance(self, base.Classifier):
+                class_proba = normalize_values_in_dict(child.stats, inplace=False)
+                mode = max(class_proba, key=class_proba.get)
+                p_mode = class_proba[mode]
+                try:
+                    alpha = (p_mode - 1 / n_colors) / (1 - 1 / n_colors)
+                    fillcolor = str(transparency_hex(color=palette[mode], alpha=alpha))
+                except ZeroDivisionError:
+                    fillcolor = "#FFFFFF"
+            else:
+                fillcolor = "#FFFFFF"
+
+            dot.node(f"{child_no}", text, fillcolor=fillcolor, style="filled")
+
+            if parent_no is not None:
+                dot.edge(
+                    f"{parent_no}",
+                    f"{child_no}",
+                    xlabel=parent.repr_branch(branch_index, shorten=True),
+                )
+
+        return dot
+
+
+# Utility adapted from the original creme's implementation
+def _color_brew(n: int) -> typing.List[typing.Tuple[int, int, int]]:
+    """Generate n colors with equally spaced hues.
+
+    Parameters
+    ----------
+    n
+        The number of required colors.
+
+    Returns
+    -------
+        List of n tuples of form (R, G, B) being the components of each color.
+    References
+    ----------
+    https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_export.py
+    """
+    colors = []
+
+    # Initialize saturation & value; calculate chroma & value shift
+    s, v = 0.75, 0.9
+    c = s * v
+    m = v - c
+
+    for h in [i for i in range(25, 385, int(360 / n))]:
+
+        # Calculate some intermediate values
+        h_bar = h / 60.0
+        x = c * (1 - abs((h_bar % 2) - 1))
+
+        # Initialize RGB with same hue & chroma as our color
+        rgb = [
+            (c, x, 0),
+            (x, c, 0),
+            (0, c, x),
+            (0, x, c),
+            (x, 0, c),
+            (c, 0, x),
+            (c, x, 0),
+        ]
+        r, g, b = rgb[int(h_bar)]
+
+        # Shift the initial RGB values to match value and store
+        colors.append(
+            ((int(255 * (r + m))), (int(255 * (g + m))), (int(255 * (b + m))))
+        )
+
+    return colors
+
+
+# Utility adapted from the original creme's implementation
+def transparency_hex(color: typing.Tuple[int, int, int], alpha: float) -> str:
+    """Apply alpha coefficient on hexadecimal color."""
+    return "#%02x%02x%02x" % tuple(
+        [int(round(alpha * c + (1 - alpha) * 255, 0)) for c in color]
+    )
```

### Comparing `river-0.8.0/river/tree/hoeffding_tree_classifier.py` & `river-0.9.0/river/tree/hoeffding_tree_classifier.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,419 +1,419 @@
-from river import base
-
-from .hoeffding_tree import HoeffdingTree
-from .nodes.branch import DTBranch
-from .nodes.htc_nodes import LeafMajorityClass, LeafNaiveBayes, LeafNaiveBayesAdaptive
-from .nodes.leaf import HTLeaf
-from .split_criterion import (
-    GiniSplitCriterion,
-    HellingerDistanceCriterion,
-    InfoGainSplitCriterion,
-)
-from .splitter import GaussianSplitter, Splitter
-
-
-class HoeffdingTreeClassifier(HoeffdingTree, base.Classifier):
-    """Hoeffding Tree or Very Fast Decision Tree classifier.
-
-    Parameters
-    ----------
-    grace_period
-        Number of instances a leaf should observe between split attempts.
-    max_depth
-        The maximum depth a tree can reach. If `None`, the tree will grow indefinitely.
-    split_criterion
-        Split criterion to use.</br>
-        - 'gini' - Gini</br>
-        - 'info_gain' - Information Gain</br>
-        - 'hellinger' - Helinger Distance</br>
-    split_confidence
-        Allowed error in split decision, a value closer to 0 takes longer to decide.
-    tie_threshold
-        Threshold below which a split will be forced to break ties.
-    leaf_prediction
-        Prediction mechanism used at leafs.</br>
-        - 'mc' - Majority Class</br>
-        - 'nb' - Naive Bayes</br>
-        - 'nba' - Naive Bayes Adaptive</br>
-    nb_threshold
-        Number of instances a leaf should observe before allowing Naive Bayes.
-    nominal_attributes
-        List of Nominal attributes identifiers. If empty, then assume that all numeric
-        attributes should be treated as continuous.
-    splitter
-        The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric
-        features and perform splits. Splitters are available in the `tree.splitter` module.
-        Different splitters are available for classification and regression tasks. Classification
-        and regression splitters can be distinguished by their property `is_target_class`.
-        This is an advanced option. Special care must be taken when choosing different splitters.
-        By default, `tree.splitter.GaussianSplitter` is used if `splitter` is `None`.
-    binary_split
-        If True, only allow binary splits.
-    max_size
-        The max size of the tree, in Megabytes (MB).
-    memory_estimate_period
-        Interval (number of processed instances) between memory consumption checks.
-    stop_mem_management
-        If True, stop growing as soon as memory limit is hit.
-    remove_poor_attrs
-        If True, disable poor attributes to reduce memory usage.
-    merit_preprune
-        If True, enable merit-based tree pre-pruning.
-
-    Notes
-    -----
-    A Hoeffding Tree [^1] is an incremental, anytime decision tree induction algorithm that is
-    capable of learning from massive data streams, assuming that the distribution generating
-    examples does not change over time. Hoeffding trees exploit the fact that a small sample can
-    often be enough to choose an optimal splitting attribute. This idea is supported mathematically
-    by the Hoeffding bound, which quantifies the number of observations (in our case, examples)
-    needed to estimate some statistics within a prescribed precision (in our case, the goodness of
-    an attribute).
-
-    A theoretically appealing feature of Hoeffding Trees not shared by other incremental decision
-    tree learners is that it has sound guarantees of performance. Using the Hoeffding bound one
-    can show that its output is asymptotically nearly identical to that of a non-incremental
-    learner using infinitely many examples. Implementation based on MOA [^2].
-
-    References
-    ----------
-
-    [^1]: G. Hulten, L. Spencer, and P. Domingos. Mining time-changing data streams.
-       In KDD’01, pages 97–106, San Francisco, CA, 2001. ACM Press.
-
-    [^2]: Albert Bifet, Geoff Holmes, Richard Kirkby, Bernhard Pfahringer.
-       MOA: Massive Online Analysis; Journal of Machine Learning Research 11: 1601-1604, 2010.
-
-    Examples
-    --------
-    >>> from river import synth
-    >>> from river import evaluate
-    >>> from river import metrics
-    >>> from river import tree
-
-    >>> gen = synth.Agrawal(classification_function=0, seed=42)
-    >>> # Take 1000 instances from the infinite data generator
-    >>> dataset = iter(gen.take(1000))
-
-    >>> model = tree.HoeffdingTreeClassifier(
-    ...     grace_period=100,
-    ...     split_confidence=1e-5,
-    ...     nominal_attributes=['elevel', 'car', 'zipcode']
-    ... )
-
-    >>> metric = metrics.Accuracy()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    Accuracy: 83.78%
-    """
-
-    _GINI_SPLIT = "gini"
-    _INFO_GAIN_SPLIT = "info_gain"
-    _HELLINGER_SPLIT = "hellinger"
-    _VALID_SPLIT_CRITERIA = [_GINI_SPLIT, _INFO_GAIN_SPLIT, _HELLINGER_SPLIT]
-
-    _MAJORITY_CLASS = "mc"
-    _NAIVE_BAYES = "nb"
-    _NAIVE_BAYES_ADAPTIVE = "nba"
-    _VALID_LEAF_PREDICTION = [_MAJORITY_CLASS, _NAIVE_BAYES, _NAIVE_BAYES_ADAPTIVE]
-
-    def __init__(
-        self,
-        grace_period: int = 200,
-        max_depth: int = None,
-        split_criterion: str = "info_gain",
-        split_confidence: float = 1e-7,
-        tie_threshold: float = 0.05,
-        leaf_prediction: str = "nba",
-        nb_threshold: int = 0,
-        nominal_attributes: list = None,
-        splitter: Splitter = None,
-        binary_split: bool = False,
-        max_size: int = 100,
-        memory_estimate_period: int = 1000000,
-        stop_mem_management: bool = False,
-        remove_poor_attrs: bool = False,
-        merit_preprune: bool = True,
-    ):
-
-        super().__init__(
-            max_depth=max_depth,
-            binary_split=binary_split,
-            max_size=max_size,
-            memory_estimate_period=memory_estimate_period,
-            stop_mem_management=stop_mem_management,
-            remove_poor_attrs=remove_poor_attrs,
-            merit_preprune=merit_preprune,
-        )
-        self.grace_period = grace_period
-        self.split_criterion = split_criterion
-        self.split_confidence = split_confidence
-        self.tie_threshold = tie_threshold
-        self.leaf_prediction = leaf_prediction
-        self.nb_threshold = nb_threshold
-        self.nominal_attributes = nominal_attributes
-
-        if splitter is None:
-            self.splitter = GaussianSplitter()
-        else:
-            if not splitter.is_target_class:
-                raise ValueError(
-                    "The chosen splitter cannot be used in classification tasks."
-                )
-            self.splitter = splitter
-
-        # To keep track of the observed classes
-        self.classes: set = set()
-
-    @HoeffdingTree.split_criterion.setter
-    def split_criterion(self, split_criterion):
-        if split_criterion not in self._VALID_SPLIT_CRITERIA:
-            print(
-                "Invalid split_criterion option {}', will use default '{}'".format(
-                    split_criterion, self._INFO_GAIN_SPLIT
-                )
-            )
-            self._split_criterion = self._INFO_GAIN_SPLIT
-        else:
-            self._split_criterion = split_criterion
-
-    @HoeffdingTree.leaf_prediction.setter
-    def leaf_prediction(self, leaf_prediction):
-        if leaf_prediction not in self._VALID_LEAF_PREDICTION:
-            print(
-                "Invalid leaf_prediction option {}', will use default '{}'".format(
-                    leaf_prediction, self._NAIVE_BAYES_ADAPTIVE
-                )
-            )
-            self._leaf_prediction = self._NAIVE_BAYES_ADAPTIVE
-        else:
-            self._leaf_prediction = leaf_prediction
-
-    def _new_leaf(self, initial_stats=None, parent=None):
-        if initial_stats is None:
-            initial_stats = {}
-        if parent is None:
-            depth = 0
-        else:
-            depth = parent.depth + 1
-
-        if self._leaf_prediction == self._MAJORITY_CLASS:
-            return LeafMajorityClass(initial_stats, depth, self.splitter)
-        elif self._leaf_prediction == self._NAIVE_BAYES:
-            return LeafNaiveBayes(initial_stats, depth, self.splitter)
-        else:  # Naives Bayes Adaptive (default)
-            return LeafNaiveBayesAdaptive(initial_stats, depth, self.splitter)
-
-    def _new_split_criterion(self):
-        if self._split_criterion == self._GINI_SPLIT:
-            split_criterion = GiniSplitCriterion()
-        elif self._split_criterion == self._INFO_GAIN_SPLIT:
-            split_criterion = InfoGainSplitCriterion()
-        elif self._split_criterion == self._HELLINGER_SPLIT:
-            split_criterion = HellingerDistanceCriterion()
-        else:
-            split_criterion = InfoGainSplitCriterion()
-
-        return split_criterion
-
-    def _attempt_to_split(
-        self, leaf: HTLeaf, parent: DTBranch, parent_branch: int, **kwargs
-    ):
-        """Attempt to split a leaf.
-
-        If the samples seen so far are not from the same class then:
-
-        1. Find split candidates and select the top 2.
-        2. Compute the Hoeffding bound.
-        3. If the difference between the top 2 split candidates is larger than the Hoeffding bound:
-           3.1 Replace the leaf node by a split node (branch node).
-           3.2 Add a new leaf node on each branch of the new split node.
-           3.3 Update tree's metrics
-
-        Optional: Disable poor attributes. Depends on the tree's configuration.
-
-        Parameters
-        ----------
-        leaf
-            The leaf to evaluate.
-        parent
-            The leaf's parent.
-        parent_branch
-            Parent leaf's branch index.
-        kwargs
-            Other parameters passed to the new branch.
-        """
-        if not leaf.observed_class_distribution_is_pure():  # noqa
-            split_criterion = self._new_split_criterion()
-
-            best_split_suggestions = leaf.best_split_suggestions(split_criterion, self)
-            best_split_suggestions.sort()
-            should_split = False
-            if len(best_split_suggestions) < 2:
-                should_split = len(best_split_suggestions) > 0
-            else:
-                hoeffding_bound = self._hoeffding_bound(
-                    split_criterion.range_of_merit(leaf.stats),
-                    self.split_confidence,
-                    leaf.total_weight,
-                )
-                best_suggestion = best_split_suggestions[-1]
-                second_best_suggestion = best_split_suggestions[-2]
-                if (
-                    best_suggestion.merit - second_best_suggestion.merit
-                    > hoeffding_bound
-                    or hoeffding_bound < self.tie_threshold
-                ):
-                    should_split = True
-                if self.remove_poor_attrs:
-                    poor_atts = set()
-                    # Add any poor attribute to set
-                    for suggestion in best_split_suggestions:
-                        if (
-                            suggestion.feature
-                            and best_suggestion.merit - suggestion.merit
-                            > hoeffding_bound
-                        ):
-                            poor_atts.add(suggestion.feature)
-                    for poor_att in poor_atts:
-                        leaf.disable_attribute(poor_att)
-            if should_split:
-                split_decision = best_split_suggestions[-1]
-                if split_decision.feature is None:
-                    # Pre-pruning - null wins
-                    leaf.deactivate()
-                    self._n_inactive_leaves += 1
-                    self._n_active_leaves -= 1
-                else:
-                    branch = self._branch_selector(
-                        split_decision.numerical_feature, split_decision.multiway_split
-                    )
-                    leaves = tuple(
-                        self._new_leaf(initial_stats, parent=leaf)
-                        for initial_stats in split_decision.children_stats
-                    )
-
-                    new_split = split_decision.assemble(
-                        branch, leaf.stats, leaf.depth, *leaves, **kwargs
-                    )
-
-                    self._n_active_leaves -= 1
-                    self._n_active_leaves += len(leaves)
-                    if parent is None:
-                        self._root = new_split
-                    else:
-                        parent.children[parent_branch] = new_split
-
-                # Manage memory
-                self._enforce_size_limit()
-
-    def learn_one(self, x, y, *, sample_weight=1.0):
-        """Train the model on instance x and corresponding target y.
-
-        Parameters
-        ----------
-        x
-            Instance attributes.
-        y
-            Class label for sample x.
-        sample_weight
-            Sample weight.
-
-        Returns
-        -------
-        self
-
-        Notes
-        -----
-        Training tasks:
-
-        * If the tree is empty, create a leaf node as the root.
-        * If the tree is already initialized, find the corresponding leaf for
-          the instance and update the leaf node statistics.
-        * If growth is allowed and the number of instances that the leaf has
-          observed between split attempts exceed the grace period then attempt
-          to split.
-        """
-
-        # Updates the set of observed classes
-        self.classes.add(y)
-
-        self._train_weight_seen_by_model += sample_weight
-
-        if self._root is None:
-            self._root = self._new_leaf()
-            self._n_active_leaves = 1
-
-        p_node = None
-        node = None
-        if isinstance(self._root, DTBranch):
-            path = iter(self._root.walk(x, until_leaf=False))
-            while True:
-                aux = next(path, None)
-                if aux is None:
-                    break
-                p_node = node
-                node = aux
-        else:
-            node = self._root
-
-        if isinstance(node, HTLeaf):
-            node.learn_one(x, y, sample_weight=sample_weight, tree=self)
-            if self._growth_allowed and node.is_active():
-                if node.depth >= self.max_depth:  # Max depth reached
-                    node.deactivate()
-                    self._n_active_leaves -= 1
-                    self._n_inactive_leaves += 1
-                else:
-                    weight_seen = node.total_weight
-                    weight_diff = weight_seen - node.last_split_attempt_at
-                    if weight_diff >= self.grace_period:
-                        p_branch = (
-                            p_node.branch_no(x)
-                            if isinstance(p_node, DTBranch)
-                            else None
-                        )
-                        self._attempt_to_split(node, p_node, p_branch)
-                        node.last_split_attempt_at = weight_seen
-        else:
-            while True:
-                # Split node encountered a previously unseen categorical value (in a multi-way
-                #  test), so there is no branch to sort the instance to
-                if node.max_branches() == -1 and node.feature in x:
-                    # Create a new branch to the new categorical value
-                    leaf = self._new_leaf(parent=node)
-                    node.add_child(x[node.feature], leaf)
-                    self._n_active_leaves += 1
-                    node = leaf
-                # The split feature is missing in the instance. Hence, we pass the new example
-                # to the most traversed path in the current subtree
-                else:
-                    _, node = node.most_common_path()
-                    # And we keep trying to reach a leaf
-                    if isinstance(node, DTBranch):
-                        node = node.traverse(x, until_leaf=False)
-                # Once a leaf is reached, the traversal can stop
-                if isinstance(node, HTLeaf):
-                    break
-            # Learn from the sample
-            node.learn_one(x, y, sample_weight=sample_weight, tree=self)
-
-        if self._train_weight_seen_by_model % self.memory_estimate_period == 0:
-            self._estimate_model_size()
-
-        return self
-
-    def predict_proba_one(self, x):
-        proba = {c: 0.0 for c in self.classes}
-        if self._root is not None:
-            if isinstance(self._root, DTBranch):
-                leaf = self._root.traverse(x, until_leaf=True)
-            else:
-                leaf = self._root
-
-            proba.update(leaf.prediction(x, tree=self))
-        return proba
-
-    @property
-    def _multiclass(self):
-        return True
+from river import base
+
+from .hoeffding_tree import HoeffdingTree
+from .nodes.branch import DTBranch
+from .nodes.htc_nodes import LeafMajorityClass, LeafNaiveBayes, LeafNaiveBayesAdaptive
+from .nodes.leaf import HTLeaf
+from .split_criterion import (
+    GiniSplitCriterion,
+    HellingerDistanceCriterion,
+    InfoGainSplitCriterion,
+)
+from .splitter import GaussianSplitter, Splitter
+
+
+class HoeffdingTreeClassifier(HoeffdingTree, base.Classifier):
+    """Hoeffding Tree or Very Fast Decision Tree classifier.
+
+    Parameters
+    ----------
+    grace_period
+        Number of instances a leaf should observe between split attempts.
+    max_depth
+        The maximum depth a tree can reach. If `None`, the tree will grow indefinitely.
+    split_criterion
+        Split criterion to use.</br>
+        - 'gini' - Gini</br>
+        - 'info_gain' - Information Gain</br>
+        - 'hellinger' - Helinger Distance</br>
+    split_confidence
+        Allowed error in split decision, a value closer to 0 takes longer to decide.
+    tie_threshold
+        Threshold below which a split will be forced to break ties.
+    leaf_prediction
+        Prediction mechanism used at leafs.</br>
+        - 'mc' - Majority Class</br>
+        - 'nb' - Naive Bayes</br>
+        - 'nba' - Naive Bayes Adaptive</br>
+    nb_threshold
+        Number of instances a leaf should observe before allowing Naive Bayes.
+    nominal_attributes
+        List of Nominal attributes identifiers. If empty, then assume that all numeric
+        attributes should be treated as continuous.
+    splitter
+        The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric
+        features and perform splits. Splitters are available in the `tree.splitter` module.
+        Different splitters are available for classification and regression tasks. Classification
+        and regression splitters can be distinguished by their property `is_target_class`.
+        This is an advanced option. Special care must be taken when choosing different splitters.
+        By default, `tree.splitter.GaussianSplitter` is used if `splitter` is `None`.
+    binary_split
+        If True, only allow binary splits.
+    max_size
+        The max size of the tree, in Megabytes (MB).
+    memory_estimate_period
+        Interval (number of processed instances) between memory consumption checks.
+    stop_mem_management
+        If True, stop growing as soon as memory limit is hit.
+    remove_poor_attrs
+        If True, disable poor attributes to reduce memory usage.
+    merit_preprune
+        If True, enable merit-based tree pre-pruning.
+
+    Notes
+    -----
+    A Hoeffding Tree [^1] is an incremental, anytime decision tree induction algorithm that is
+    capable of learning from massive data streams, assuming that the distribution generating
+    examples does not change over time. Hoeffding trees exploit the fact that a small sample can
+    often be enough to choose an optimal splitting attribute. This idea is supported mathematically
+    by the Hoeffding bound, which quantifies the number of observations (in our case, examples)
+    needed to estimate some statistics within a prescribed precision (in our case, the goodness of
+    an attribute).
+
+    A theoretically appealing feature of Hoeffding Trees not shared by other incremental decision
+    tree learners is that it has sound guarantees of performance. Using the Hoeffding bound one
+    can show that its output is asymptotically nearly identical to that of a non-incremental
+    learner using infinitely many examples. Implementation based on MOA [^2].
+
+    References
+    ----------
+
+    [^1]: G. Hulten, L. Spencer, and P. Domingos. Mining time-changing data streams.
+       In KDD’01, pages 97–106, San Francisco, CA, 2001. ACM Press.
+
+    [^2]: Albert Bifet, Geoff Holmes, Richard Kirkby, Bernhard Pfahringer.
+       MOA: Massive Online Analysis; Journal of Machine Learning Research 11: 1601-1604, 2010.
+
+    Examples
+    --------
+    >>> from river import synth
+    >>> from river import evaluate
+    >>> from river import metrics
+    >>> from river import tree
+
+    >>> gen = synth.Agrawal(classification_function=0, seed=42)
+    >>> # Take 1000 instances from the infinite data generator
+    >>> dataset = iter(gen.take(1000))
+
+    >>> model = tree.HoeffdingTreeClassifier(
+    ...     grace_period=100,
+    ...     split_confidence=1e-5,
+    ...     nominal_attributes=['elevel', 'car', 'zipcode']
+    ... )
+
+    >>> metric = metrics.Accuracy()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    Accuracy: 83.78%
+    """
+
+    _GINI_SPLIT = "gini"
+    _INFO_GAIN_SPLIT = "info_gain"
+    _HELLINGER_SPLIT = "hellinger"
+    _VALID_SPLIT_CRITERIA = [_GINI_SPLIT, _INFO_GAIN_SPLIT, _HELLINGER_SPLIT]
+
+    _MAJORITY_CLASS = "mc"
+    _NAIVE_BAYES = "nb"
+    _NAIVE_BAYES_ADAPTIVE = "nba"
+    _VALID_LEAF_PREDICTION = [_MAJORITY_CLASS, _NAIVE_BAYES, _NAIVE_BAYES_ADAPTIVE]
+
+    def __init__(
+        self,
+        grace_period: int = 200,
+        max_depth: int = None,
+        split_criterion: str = "info_gain",
+        split_confidence: float = 1e-7,
+        tie_threshold: float = 0.05,
+        leaf_prediction: str = "nba",
+        nb_threshold: int = 0,
+        nominal_attributes: list = None,
+        splitter: Splitter = None,
+        binary_split: bool = False,
+        max_size: int = 100,
+        memory_estimate_period: int = 1000000,
+        stop_mem_management: bool = False,
+        remove_poor_attrs: bool = False,
+        merit_preprune: bool = True,
+    ):
+
+        super().__init__(
+            max_depth=max_depth,
+            binary_split=binary_split,
+            max_size=max_size,
+            memory_estimate_period=memory_estimate_period,
+            stop_mem_management=stop_mem_management,
+            remove_poor_attrs=remove_poor_attrs,
+            merit_preprune=merit_preprune,
+        )
+        self.grace_period = grace_period
+        self.split_criterion = split_criterion
+        self.split_confidence = split_confidence
+        self.tie_threshold = tie_threshold
+        self.leaf_prediction = leaf_prediction
+        self.nb_threshold = nb_threshold
+        self.nominal_attributes = nominal_attributes
+
+        if splitter is None:
+            self.splitter = GaussianSplitter()
+        else:
+            if not splitter.is_target_class:
+                raise ValueError(
+                    "The chosen splitter cannot be used in classification tasks."
+                )
+            self.splitter = splitter
+
+        # To keep track of the observed classes
+        self.classes: set = set()
+
+    @HoeffdingTree.split_criterion.setter
+    def split_criterion(self, split_criterion):
+        if split_criterion not in self._VALID_SPLIT_CRITERIA:
+            print(
+                "Invalid split_criterion option {}', will use default '{}'".format(
+                    split_criterion, self._INFO_GAIN_SPLIT
+                )
+            )
+            self._split_criterion = self._INFO_GAIN_SPLIT
+        else:
+            self._split_criterion = split_criterion
+
+    @HoeffdingTree.leaf_prediction.setter
+    def leaf_prediction(self, leaf_prediction):
+        if leaf_prediction not in self._VALID_LEAF_PREDICTION:
+            print(
+                "Invalid leaf_prediction option {}', will use default '{}'".format(
+                    leaf_prediction, self._NAIVE_BAYES_ADAPTIVE
+                )
+            )
+            self._leaf_prediction = self._NAIVE_BAYES_ADAPTIVE
+        else:
+            self._leaf_prediction = leaf_prediction
+
+    def _new_leaf(self, initial_stats=None, parent=None):
+        if initial_stats is None:
+            initial_stats = {}
+        if parent is None:
+            depth = 0
+        else:
+            depth = parent.depth + 1
+
+        if self._leaf_prediction == self._MAJORITY_CLASS:
+            return LeafMajorityClass(initial_stats, depth, self.splitter)
+        elif self._leaf_prediction == self._NAIVE_BAYES:
+            return LeafNaiveBayes(initial_stats, depth, self.splitter)
+        else:  # Naives Bayes Adaptive (default)
+            return LeafNaiveBayesAdaptive(initial_stats, depth, self.splitter)
+
+    def _new_split_criterion(self):
+        if self._split_criterion == self._GINI_SPLIT:
+            split_criterion = GiniSplitCriterion()
+        elif self._split_criterion == self._INFO_GAIN_SPLIT:
+            split_criterion = InfoGainSplitCriterion()
+        elif self._split_criterion == self._HELLINGER_SPLIT:
+            split_criterion = HellingerDistanceCriterion()
+        else:
+            split_criterion = InfoGainSplitCriterion()
+
+        return split_criterion
+
+    def _attempt_to_split(
+        self, leaf: HTLeaf, parent: DTBranch, parent_branch: int, **kwargs
+    ):
+        """Attempt to split a leaf.
+
+        If the samples seen so far are not from the same class then:
+
+        1. Find split candidates and select the top 2.
+        2. Compute the Hoeffding bound.
+        3. If the difference between the top 2 split candidates is larger than the Hoeffding bound:
+           3.1 Replace the leaf node by a split node (branch node).
+           3.2 Add a new leaf node on each branch of the new split node.
+           3.3 Update tree's metrics
+
+        Optional: Disable poor attributes. Depends on the tree's configuration.
+
+        Parameters
+        ----------
+        leaf
+            The leaf to evaluate.
+        parent
+            The leaf's parent.
+        parent_branch
+            Parent leaf's branch index.
+        kwargs
+            Other parameters passed to the new branch.
+        """
+        if not leaf.observed_class_distribution_is_pure():  # noqa
+            split_criterion = self._new_split_criterion()
+
+            best_split_suggestions = leaf.best_split_suggestions(split_criterion, self)
+            best_split_suggestions.sort()
+            should_split = False
+            if len(best_split_suggestions) < 2:
+                should_split = len(best_split_suggestions) > 0
+            else:
+                hoeffding_bound = self._hoeffding_bound(
+                    split_criterion.range_of_merit(leaf.stats),
+                    self.split_confidence,
+                    leaf.total_weight,
+                )
+                best_suggestion = best_split_suggestions[-1]
+                second_best_suggestion = best_split_suggestions[-2]
+                if (
+                    best_suggestion.merit - second_best_suggestion.merit
+                    > hoeffding_bound
+                    or hoeffding_bound < self.tie_threshold
+                ):
+                    should_split = True
+                if self.remove_poor_attrs:
+                    poor_atts = set()
+                    # Add any poor attribute to set
+                    for suggestion in best_split_suggestions:
+                        if (
+                            suggestion.feature
+                            and best_suggestion.merit - suggestion.merit
+                            > hoeffding_bound
+                        ):
+                            poor_atts.add(suggestion.feature)
+                    for poor_att in poor_atts:
+                        leaf.disable_attribute(poor_att)
+            if should_split:
+                split_decision = best_split_suggestions[-1]
+                if split_decision.feature is None:
+                    # Pre-pruning - null wins
+                    leaf.deactivate()
+                    self._n_inactive_leaves += 1
+                    self._n_active_leaves -= 1
+                else:
+                    branch = self._branch_selector(
+                        split_decision.numerical_feature, split_decision.multiway_split
+                    )
+                    leaves = tuple(
+                        self._new_leaf(initial_stats, parent=leaf)
+                        for initial_stats in split_decision.children_stats
+                    )
+
+                    new_split = split_decision.assemble(
+                        branch, leaf.stats, leaf.depth, *leaves, **kwargs
+                    )
+
+                    self._n_active_leaves -= 1
+                    self._n_active_leaves += len(leaves)
+                    if parent is None:
+                        self._root = new_split
+                    else:
+                        parent.children[parent_branch] = new_split
+
+                # Manage memory
+                self._enforce_size_limit()
+
+    def learn_one(self, x, y, *, sample_weight=1.0):
+        """Train the model on instance x and corresponding target y.
+
+        Parameters
+        ----------
+        x
+            Instance attributes.
+        y
+            Class label for sample x.
+        sample_weight
+            Sample weight.
+
+        Returns
+        -------
+        self
+
+        Notes
+        -----
+        Training tasks:
+
+        * If the tree is empty, create a leaf node as the root.
+        * If the tree is already initialized, find the corresponding leaf for
+          the instance and update the leaf node statistics.
+        * If growth is allowed and the number of instances that the leaf has
+          observed between split attempts exceed the grace period then attempt
+          to split.
+        """
+
+        # Updates the set of observed classes
+        self.classes.add(y)
+
+        self._train_weight_seen_by_model += sample_weight
+
+        if self._root is None:
+            self._root = self._new_leaf()
+            self._n_active_leaves = 1
+
+        p_node = None
+        node = None
+        if isinstance(self._root, DTBranch):
+            path = iter(self._root.walk(x, until_leaf=False))
+            while True:
+                aux = next(path, None)
+                if aux is None:
+                    break
+                p_node = node
+                node = aux
+        else:
+            node = self._root
+
+        if isinstance(node, HTLeaf):
+            node.learn_one(x, y, sample_weight=sample_weight, tree=self)
+            if self._growth_allowed and node.is_active():
+                if node.depth >= self.max_depth:  # Max depth reached
+                    node.deactivate()
+                    self._n_active_leaves -= 1
+                    self._n_inactive_leaves += 1
+                else:
+                    weight_seen = node.total_weight
+                    weight_diff = weight_seen - node.last_split_attempt_at
+                    if weight_diff >= self.grace_period:
+                        p_branch = (
+                            p_node.branch_no(x)
+                            if isinstance(p_node, DTBranch)
+                            else None
+                        )
+                        self._attempt_to_split(node, p_node, p_branch)
+                        node.last_split_attempt_at = weight_seen
+        else:
+            while True:
+                # Split node encountered a previously unseen categorical value (in a multi-way
+                #  test), so there is no branch to sort the instance to
+                if node.max_branches() == -1 and node.feature in x:
+                    # Create a new branch to the new categorical value
+                    leaf = self._new_leaf(parent=node)
+                    node.add_child(x[node.feature], leaf)
+                    self._n_active_leaves += 1
+                    node = leaf
+                # The split feature is missing in the instance. Hence, we pass the new example
+                # to the most traversed path in the current subtree
+                else:
+                    _, node = node.most_common_path()
+                    # And we keep trying to reach a leaf
+                    if isinstance(node, DTBranch):
+                        node = node.traverse(x, until_leaf=False)
+                # Once a leaf is reached, the traversal can stop
+                if isinstance(node, HTLeaf):
+                    break
+            # Learn from the sample
+            node.learn_one(x, y, sample_weight=sample_weight, tree=self)
+
+        if self._train_weight_seen_by_model % self.memory_estimate_period == 0:
+            self._estimate_model_size()
+
+        return self
+
+    def predict_proba_one(self, x):
+        proba = {c: 0.0 for c in self.classes}
+        if self._root is not None:
+            if isinstance(self._root, DTBranch):
+                leaf = self._root.traverse(x, until_leaf=True)
+            else:
+                leaf = self._root
+
+            proba.update(leaf.prediction(x, tree=self))
+        return proba
+
+    @property
+    def _multiclass(self):
+        return True
```

### Comparing `river-0.8.0/river/tree/hoeffding_tree_regressor.py` & `river-0.9.0/river/tree/hoeffding_tree_regressor.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,425 +1,425 @@
-from copy import deepcopy
-
-from river import base, linear_model
-
-from .hoeffding_tree import HoeffdingTree
-from .nodes.branch import DTBranch
-from .nodes.htr_nodes import LeafAdaptive, LeafMean, LeafModel
-from .nodes.leaf import HTLeaf
-from .split_criterion import VarianceReductionSplitCriterion
-from .splitter import EBSTSplitter, Splitter
-
-
-class HoeffdingTreeRegressor(HoeffdingTree, base.Regressor):
-    """Hoeffding Tree regressor.
-
-    Parameters
-    ----------
-    grace_period
-        Number of instances a leaf should observe between split attempts.
-    max_depth
-        The maximum depth a tree can reach. If `None`, the tree will grow indefinitely.
-    split_confidence
-        Allowed error in split decision, a value closer to 0 takes longer to decide.
-    tie_threshold
-        Threshold below which a split will be forced to break ties.
-    leaf_prediction
-        Prediction mechanism used at leafs.</br>
-        - 'mean' - Target mean</br>
-        - 'model' - Uses the model defined in `leaf_model`</br>
-        - 'adaptive' - Chooses between 'mean' and 'model' dynamically</br>
-    leaf_model
-        The regression model used to provide responses if `leaf_prediction='model'`. If not
-        provided an instance of `river.linear_model.LinearRegression` with the default
-        hyperparameters is used.
-    model_selector_decay
-        The exponential decaying factor applied to the learning models' squared errors, that
-        are monitored if `leaf_prediction='adaptive'`. Must be between `0` and `1`. The closer
-        to `1`, the more importance is going to be given to past observations. On the other hand,
-        if its value approaches `0`, the recent observed errors are going to have more influence
-        on the final decision.
-    nominal_attributes
-        List of Nominal attributes identifiers. If empty, then assume that all numeric attributes
-        should be treated as continuous.
-    splitter
-        The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric
-        features and perform splits. Splitters are available in the `tree.splitter` module.
-        Different splitters are available for classification and regression tasks. Classification
-        and regression splitters can be distinguished by their property `is_target_class`.
-        This is an advanced option. Special care must be taken when choosing different splitters.
-        By default, `tree.splitter.EBSTSplitter` is used if `splitter` is `None`.
-    min_samples_split
-        The minimum number of samples every branch resulting from a split candidate must have
-        to be considered valid.
-    binary_split
-        If True, only allow binary splits.
-    max_size
-        The max size of the tree, in Megabytes (MB).
-    memory_estimate_period
-        Interval (number of processed instances) between memory consumption checks.
-    stop_mem_management
-        If True, stop growing as soon as memory limit is hit.
-    remove_poor_attrs
-        If True, disable poor attributes to reduce memory usage.
-    merit_preprune
-        If True, enable merit-based tree pre-pruning.
-
-    Notes
-    -----
-    The Hoeffding Tree Regressor (HTR) is an adaptation of the incremental tree algorithm of the
-    same name for classification. Similarly to its classification counterpart, HTR uses the
-    Hoeffding bound to control its split decisions. Differently from the classification algorithm,
-    HTR relies on calculating the reduction of variance in the target space to decide among the
-    split candidates. The smallest the variance at its leaf nodes, the more homogeneous the
-    partitions are. At its leaf nodes, HTR fits either linear models or uses the target
-    average as the predictor.
-
-    Examples
-    --------
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import metrics
-    >>> from river import tree
-    >>> from river import preprocessing
-
-    >>> dataset = datasets.TrumpApproval()
-
-    >>> model = (
-    ...     preprocessing.StandardScaler() |
-    ...     tree.HoeffdingTreeRegressor(
-    ...         grace_period=100,
-    ...         leaf_prediction='adaptive',
-    ...         model_selector_decay=0.9
-    ...     )
-    ... )
-
-    >>> metric = metrics.MAE()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    MAE: 0.852902
-    """
-
-    _TARGET_MEAN = "mean"
-    _MODEL = "model"
-    _ADAPTIVE = "adaptive"
-
-    _VALID_LEAF_PREDICTION = [_TARGET_MEAN, _MODEL, _ADAPTIVE]
-
-    def __init__(
-        self,
-        grace_period: int = 200,
-        max_depth: int = None,
-        split_confidence: float = 1e-7,
-        tie_threshold: float = 0.05,
-        leaf_prediction: str = "model",
-        leaf_model: base.Regressor = None,
-        model_selector_decay: float = 0.95,
-        nominal_attributes: list = None,
-        splitter: Splitter = None,
-        min_samples_split: int = 5,
-        binary_split: bool = False,
-        max_size: int = 500,
-        memory_estimate_period: int = 1000000,
-        stop_mem_management: bool = False,
-        remove_poor_attrs: bool = False,
-        merit_preprune: bool = True,
-    ):
-        super().__init__(
-            max_depth=max_depth,
-            binary_split=binary_split,
-            max_size=max_size,
-            memory_estimate_period=memory_estimate_period,
-            stop_mem_management=stop_mem_management,
-            remove_poor_attrs=remove_poor_attrs,
-            merit_preprune=merit_preprune,
-        )
-
-        self._split_criterion: str = "vr"
-        self.grace_period = grace_period
-        self.split_confidence = split_confidence
-        self.tie_threshold = tie_threshold
-        self.leaf_prediction = leaf_prediction
-        self.leaf_model = leaf_model if leaf_model else linear_model.LinearRegression()
-        self.model_selector_decay = model_selector_decay
-        self.nominal_attributes = nominal_attributes
-        self.min_samples_split = min_samples_split
-
-        if splitter is None:
-            self.splitter = EBSTSplitter()
-        else:
-            if splitter.is_target_class:
-                raise ValueError(
-                    "The chosen splitter cannot be used in regression tasks."
-                )
-            self.splitter = splitter
-
-    @HoeffdingTree.leaf_prediction.setter
-    def leaf_prediction(self, leaf_prediction):
-        if leaf_prediction not in self._VALID_LEAF_PREDICTION:
-            print(
-                'Invalid leaf_prediction option "{}", will use default "{}"'.format(
-                    leaf_prediction, self._MODEL
-                )
-            )
-            self._leaf_prediction = self._MODEL
-        else:
-            self._leaf_prediction = leaf_prediction
-
-    @HoeffdingTree.split_criterion.setter
-    def split_criterion(self, split_criterion):
-        if split_criterion != "vr":  # variance reduction
-            print(
-                "Invalid split_criterion option {}', will use default '{}'".format(
-                    split_criterion, "vr"
-                )
-            )
-            self._split_criterion = "vr"
-        else:
-            self._split_criterion = split_criterion
-
-    def _new_split_criterion(self):
-        return VarianceReductionSplitCriterion(min_samples_split=self.min_samples_split)
-
-    def _new_leaf(self, initial_stats=None, parent=None):
-        """Create a new learning node.
-
-        The type of learning node depends on the tree configuration.
-        """
-        if parent is not None:
-            depth = parent.depth + 1
-        else:
-            depth = 0
-
-        leaf_model = None
-        if self.leaf_prediction in {self._MODEL, self._ADAPTIVE}:
-            if parent is None:
-                leaf_model = deepcopy(self.leaf_model)
-            else:
-                try:
-                    leaf_model = deepcopy(parent._leaf_model)  # noqa
-                except AttributeError:
-                    leaf_model = deepcopy(self.leaf_model)
-
-        if self.leaf_prediction == self._TARGET_MEAN:
-            return LeafMean(initial_stats, depth, self.splitter)
-        elif self.leaf_prediction == self._MODEL:
-            return LeafModel(initial_stats, depth, self.splitter, leaf_model)
-        else:  # adaptive learning node
-            new_adaptive = LeafAdaptive(initial_stats, depth, self.splitter, leaf_model)
-            if parent is not None and isinstance(parent, LeafAdaptive):
-                new_adaptive._fmse_mean = parent._fmse_mean  # noqa
-                new_adaptive._fmse_model = parent._fmse_model  # noqa
-
-            return new_adaptive
-
-    def learn_one(self, x, y, *, sample_weight=1.0):
-        """Train the tree model on sample x and corresponding target y.
-
-        Parameters
-        ----------
-        x
-            Instance attributes.
-        y
-            Target value for sample x.
-        sample_weight
-            The weight of the sample.
-
-        Returns
-        -------
-        self
-        """
-
-        self._train_weight_seen_by_model += sample_weight
-
-        if self._root is None:
-            self._root = self._new_leaf()
-            self._n_active_leaves = 1
-
-        p_node = None
-        node = None
-        if isinstance(self._root, DTBranch):
-            path = iter(self._root.walk(x, until_leaf=False))
-            while True:
-                aux = next(path, None)
-                if aux is None:
-                    break
-                p_node = node
-                node = aux
-        else:
-            node = self._root
-
-        if isinstance(node, HTLeaf):
-            node.learn_one(x, y, sample_weight=sample_weight, tree=self)
-            if self._growth_allowed and node.is_active():
-                if node.depth >= self.max_depth:  # Max depth reached
-                    node.deactivate()
-                    self._n_active_leaves -= 1
-                    self._n_inactive_leaves += 1
-                else:
-                    weight_seen = node.total_weight
-                    weight_diff = weight_seen - node.last_split_attempt_at
-                    if weight_diff >= self.grace_period:
-                        p_branch = (
-                            p_node.branch_no(x)
-                            if isinstance(p_node, DTBranch)
-                            else None
-                        )
-                        self._attempt_to_split(node, p_node, p_branch)
-                        node.last_split_attempt_at = weight_seen
-        else:
-            while True:
-                # Split node encountered a previously unseen categorical value (in a multi-way
-                #  test), so there is no branch to sort the instance to
-                if node.max_branches() == -1 and node.feature in x:
-                    # Create a new branch to the new categorical value
-                    leaf = self._new_leaf(parent=node)
-                    node.add_child(x[node.feature], leaf)
-                    self._n_active_leaves += 1
-                    node = leaf
-                # The split feature is missing in the instance. Hence, we pass the new example
-                # to the most traversed path in the current subtree
-                else:
-                    _, node = node.most_common_path()
-                    # And we keep trying to reach a leaf
-                    if isinstance(node, DTBranch):
-                        node = node.traverse(x, until_leaf=False)
-                # Once a leaf is reached, the traversal can stop
-                if isinstance(node, HTLeaf):
-                    break
-            # Learn from the sample
-            node.learn_one(x, y, sample_weight=sample_weight, tree=self)
-
-        if self._train_weight_seen_by_model % self.memory_estimate_period == 0:
-            self._estimate_model_size()
-
-        return self
-
-    def predict_one(self, x):
-        """Predict the target value using one of the leaf prediction strategies.
-
-        Parameters
-        ----------
-        x
-            Instance for which we want to predict the target.
-
-        Returns
-        -------
-        Predicted target value.
-
-        """
-        pred = 0.0
-        if self._root is not None:
-            if isinstance(self._root, DTBranch):
-                leaf = self._root.traverse(x, until_leaf=True)
-            else:
-                leaf = self._root
-
-            pred = leaf.prediction(x, tree=self)
-        return pred
-
-    def _attempt_to_split(
-        self, leaf: HTLeaf, parent: DTBranch, parent_branch: int, **kwargs
-    ):
-        """Attempt to split a node.
-
-        If the target's variance is high at the leaf node, then:
-
-        1. Find split candidates and select the top 2.
-        2. Compute the Hoeffding bound.
-        3. If the ratio between the merit of the second best split candidate and the merit of the
-        best one is smaller than 1 minus the Hoeffding bound (or a tie breaking decision
-        takes place), then:
-           3.1 Replace the leaf node by a split node.
-           3.2 Add a new leaf node on each branch of the new split node.
-           3.3 Update tree's metrics
-
-        Optional: Disable poor attribute. Depends on the tree's configuration.
-
-        Parameters
-        ----------
-        leaf
-            The node to evaluate.
-        parent
-            The node's parent in the tree.
-        parent_branch
-            Parent node's branch index.
-        kwargs
-            Other parameters passed to the new branch.
-
-        """
-        split_criterion = self._new_split_criterion()
-        best_split_suggestions = leaf.best_split_suggestions(split_criterion, self)
-        best_split_suggestions.sort()
-        should_split = False
-        if len(best_split_suggestions) < 2:
-            should_split = len(best_split_suggestions) > 0
-        else:
-            hoeffding_bound = self._hoeffding_bound(
-                split_criterion.range_of_merit(leaf.stats),
-                self.split_confidence,
-                leaf.total_weight,
-            )
-            best_suggestion = best_split_suggestions[-1]
-            second_best_suggestion = best_split_suggestions[-2]
-            if best_suggestion.merit > 0.0 and (
-                second_best_suggestion.merit / best_suggestion.merit
-                < 1 - hoeffding_bound
-                or hoeffding_bound < self.tie_threshold
-            ):
-                should_split = True
-            if self.remove_poor_attrs:
-                poor_attrs = set()
-                best_ratio = second_best_suggestion.merit / best_suggestion.merit
-
-                # Add any poor attribute to set
-                for suggestion in best_split_suggestions:
-                    if (
-                        suggestion.feature
-                        and suggestion.merit / best_suggestion.merit
-                        < best_ratio - 2 * hoeffding_bound
-                    ):
-                        poor_attrs.add(suggestion.feature)
-                for poor_att in poor_attrs:
-                    leaf.disable_attribute(poor_att)
-        if should_split:
-            split_decision = best_split_suggestions[-1]
-            if split_decision.feature is None:
-                # Pre-pruning - null wins
-                leaf.deactivate()
-                self._n_inactive_leaves += 1
-                self._n_active_leaves -= 1
-            else:
-                branch = self._branch_selector(
-                    split_decision.numerical_feature, split_decision.multiway_split
-                )
-                leaves = tuple(
-                    self._new_leaf(initial_stats, parent=leaf)
-                    for initial_stats in split_decision.children_stats
-                )
-
-                new_split = split_decision.assemble(
-                    branch, leaf.stats, leaf.depth, *leaves, **kwargs
-                )
-
-                self._n_active_leaves -= 1
-                self._n_active_leaves += len(leaves)
-                if parent is None:
-                    self._root = new_split
-                else:
-                    parent.children[parent_branch] = new_split
-
-            # Manage memory
-            self._enforce_size_limit()
-        elif (
-            len(best_split_suggestions) >= 2
-            and best_split_suggestions[-1].merit > 0
-            and best_split_suggestions[-2].merit > 0
-        ):
-            last_check_ratio = (
-                best_split_suggestions[-2].merit / best_split_suggestions[-1].merit
-            )
-            last_check_vr = best_split_suggestions[-1].merit
-
-            leaf.manage_memory(
-                split_criterion, last_check_ratio, last_check_vr, hoeffding_bound
-            )
+from copy import deepcopy
+
+from river import base, linear_model
+
+from .hoeffding_tree import HoeffdingTree
+from .nodes.branch import DTBranch
+from .nodes.htr_nodes import LeafAdaptive, LeafMean, LeafModel
+from .nodes.leaf import HTLeaf
+from .split_criterion import VarianceReductionSplitCriterion
+from .splitter import EBSTSplitter, Splitter
+
+
+class HoeffdingTreeRegressor(HoeffdingTree, base.Regressor):
+    """Hoeffding Tree regressor.
+
+    Parameters
+    ----------
+    grace_period
+        Number of instances a leaf should observe between split attempts.
+    max_depth
+        The maximum depth a tree can reach. If `None`, the tree will grow indefinitely.
+    split_confidence
+        Allowed error in split decision, a value closer to 0 takes longer to decide.
+    tie_threshold
+        Threshold below which a split will be forced to break ties.
+    leaf_prediction
+        Prediction mechanism used at leafs.</br>
+        - 'mean' - Target mean</br>
+        - 'model' - Uses the model defined in `leaf_model`</br>
+        - 'adaptive' - Chooses between 'mean' and 'model' dynamically</br>
+    leaf_model
+        The regression model used to provide responses if `leaf_prediction='model'`. If not
+        provided an instance of `river.linear_model.LinearRegression` with the default
+        hyperparameters is used.
+    model_selector_decay
+        The exponential decaying factor applied to the learning models' squared errors, that
+        are monitored if `leaf_prediction='adaptive'`. Must be between `0` and `1`. The closer
+        to `1`, the more importance is going to be given to past observations. On the other hand,
+        if its value approaches `0`, the recent observed errors are going to have more influence
+        on the final decision.
+    nominal_attributes
+        List of Nominal attributes identifiers. If empty, then assume that all numeric attributes
+        should be treated as continuous.
+    splitter
+        The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric
+        features and perform splits. Splitters are available in the `tree.splitter` module.
+        Different splitters are available for classification and regression tasks. Classification
+        and regression splitters can be distinguished by their property `is_target_class`.
+        This is an advanced option. Special care must be taken when choosing different splitters.
+        By default, `tree.splitter.EBSTSplitter` is used if `splitter` is `None`.
+    min_samples_split
+        The minimum number of samples every branch resulting from a split candidate must have
+        to be considered valid.
+    binary_split
+        If True, only allow binary splits.
+    max_size
+        The max size of the tree, in Megabytes (MB).
+    memory_estimate_period
+        Interval (number of processed instances) between memory consumption checks.
+    stop_mem_management
+        If True, stop growing as soon as memory limit is hit.
+    remove_poor_attrs
+        If True, disable poor attributes to reduce memory usage.
+    merit_preprune
+        If True, enable merit-based tree pre-pruning.
+
+    Notes
+    -----
+    The Hoeffding Tree Regressor (HTR) is an adaptation of the incremental tree algorithm of the
+    same name for classification. Similarly to its classification counterpart, HTR uses the
+    Hoeffding bound to control its split decisions. Differently from the classification algorithm,
+    HTR relies on calculating the reduction of variance in the target space to decide among the
+    split candidates. The smallest the variance at its leaf nodes, the more homogeneous the
+    partitions are. At its leaf nodes, HTR fits either linear models or uses the target
+    average as the predictor.
+
+    Examples
+    --------
+    >>> from river import datasets
+    >>> from river import evaluate
+    >>> from river import metrics
+    >>> from river import tree
+    >>> from river import preprocessing
+
+    >>> dataset = datasets.TrumpApproval()
+
+    >>> model = (
+    ...     preprocessing.StandardScaler() |
+    ...     tree.HoeffdingTreeRegressor(
+    ...         grace_period=100,
+    ...         leaf_prediction='adaptive',
+    ...         model_selector_decay=0.9
+    ...     )
+    ... )
+
+    >>> metric = metrics.MAE()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    MAE: 0.782258
+    """
+
+    _TARGET_MEAN = "mean"
+    _MODEL = "model"
+    _ADAPTIVE = "adaptive"
+
+    _VALID_LEAF_PREDICTION = [_TARGET_MEAN, _MODEL, _ADAPTIVE]
+
+    def __init__(
+        self,
+        grace_period: int = 200,
+        max_depth: int = None,
+        split_confidence: float = 1e-7,
+        tie_threshold: float = 0.05,
+        leaf_prediction: str = "model",
+        leaf_model: base.Regressor = None,
+        model_selector_decay: float = 0.95,
+        nominal_attributes: list = None,
+        splitter: Splitter = None,
+        min_samples_split: int = 5,
+        binary_split: bool = False,
+        max_size: int = 500,
+        memory_estimate_period: int = 1000000,
+        stop_mem_management: bool = False,
+        remove_poor_attrs: bool = False,
+        merit_preprune: bool = True,
+    ):
+        super().__init__(
+            max_depth=max_depth,
+            binary_split=binary_split,
+            max_size=max_size,
+            memory_estimate_period=memory_estimate_period,
+            stop_mem_management=stop_mem_management,
+            remove_poor_attrs=remove_poor_attrs,
+            merit_preprune=merit_preprune,
+        )
+
+        self._split_criterion: str = "vr"
+        self.grace_period = grace_period
+        self.split_confidence = split_confidence
+        self.tie_threshold = tie_threshold
+        self.leaf_prediction = leaf_prediction
+        self.leaf_model = leaf_model if leaf_model else linear_model.LinearRegression()
+        self.model_selector_decay = model_selector_decay
+        self.nominal_attributes = nominal_attributes
+        self.min_samples_split = min_samples_split
+
+        if splitter is None:
+            self.splitter = EBSTSplitter()
+        else:
+            if splitter.is_target_class:
+                raise ValueError(
+                    "The chosen splitter cannot be used in regression tasks."
+                )
+            self.splitter = splitter
+
+    @HoeffdingTree.leaf_prediction.setter
+    def leaf_prediction(self, leaf_prediction):
+        if leaf_prediction not in self._VALID_LEAF_PREDICTION:
+            print(
+                'Invalid leaf_prediction option "{}", will use default "{}"'.format(
+                    leaf_prediction, self._MODEL
+                )
+            )
+            self._leaf_prediction = self._MODEL
+        else:
+            self._leaf_prediction = leaf_prediction
+
+    @HoeffdingTree.split_criterion.setter
+    def split_criterion(self, split_criterion):
+        if split_criterion != "vr":  # variance reduction
+            print(
+                "Invalid split_criterion option {}', will use default '{}'".format(
+                    split_criterion, "vr"
+                )
+            )
+            self._split_criterion = "vr"
+        else:
+            self._split_criterion = split_criterion
+
+    def _new_split_criterion(self):
+        return VarianceReductionSplitCriterion(min_samples_split=self.min_samples_split)
+
+    def _new_leaf(self, initial_stats=None, parent=None):
+        """Create a new learning node.
+
+        The type of learning node depends on the tree configuration.
+        """
+        if parent is not None:
+            depth = parent.depth + 1
+        else:
+            depth = 0
+
+        leaf_model = None
+        if self.leaf_prediction in {self._MODEL, self._ADAPTIVE}:
+            if parent is None:
+                leaf_model = deepcopy(self.leaf_model)
+            else:
+                try:
+                    leaf_model = deepcopy(parent._leaf_model)  # noqa
+                except AttributeError:
+                    leaf_model = deepcopy(self.leaf_model)
+
+        if self.leaf_prediction == self._TARGET_MEAN:
+            return LeafMean(initial_stats, depth, self.splitter)
+        elif self.leaf_prediction == self._MODEL:
+            return LeafModel(initial_stats, depth, self.splitter, leaf_model)
+        else:  # adaptive learning node
+            new_adaptive = LeafAdaptive(initial_stats, depth, self.splitter, leaf_model)
+            if parent is not None and isinstance(parent, LeafAdaptive):
+                new_adaptive._fmse_mean = parent._fmse_mean  # noqa
+                new_adaptive._fmse_model = parent._fmse_model  # noqa
+
+            return new_adaptive
+
+    def learn_one(self, x, y, *, sample_weight=1.0):
+        """Train the tree model on sample x and corresponding target y.
+
+        Parameters
+        ----------
+        x
+            Instance attributes.
+        y
+            Target value for sample x.
+        sample_weight
+            The weight of the sample.
+
+        Returns
+        -------
+        self
+        """
+
+        self._train_weight_seen_by_model += sample_weight
+
+        if self._root is None:
+            self._root = self._new_leaf()
+            self._n_active_leaves = 1
+
+        p_node = None
+        node = None
+        if isinstance(self._root, DTBranch):
+            path = iter(self._root.walk(x, until_leaf=False))
+            while True:
+                aux = next(path, None)
+                if aux is None:
+                    break
+                p_node = node
+                node = aux
+        else:
+            node = self._root
+
+        if isinstance(node, HTLeaf):
+            node.learn_one(x, y, sample_weight=sample_weight, tree=self)
+            if self._growth_allowed and node.is_active():
+                if node.depth >= self.max_depth:  # Max depth reached
+                    node.deactivate()
+                    self._n_active_leaves -= 1
+                    self._n_inactive_leaves += 1
+                else:
+                    weight_seen = node.total_weight
+                    weight_diff = weight_seen - node.last_split_attempt_at
+                    if weight_diff >= self.grace_period:
+                        p_branch = (
+                            p_node.branch_no(x)
+                            if isinstance(p_node, DTBranch)
+                            else None
+                        )
+                        self._attempt_to_split(node, p_node, p_branch)
+                        node.last_split_attempt_at = weight_seen
+        else:
+            while True:
+                # Split node encountered a previously unseen categorical value (in a multi-way
+                #  test), so there is no branch to sort the instance to
+                if node.max_branches() == -1 and node.feature in x:
+                    # Create a new branch to the new categorical value
+                    leaf = self._new_leaf(parent=node)
+                    node.add_child(x[node.feature], leaf)
+                    self._n_active_leaves += 1
+                    node = leaf
+                # The split feature is missing in the instance. Hence, we pass the new example
+                # to the most traversed path in the current subtree
+                else:
+                    _, node = node.most_common_path()
+                    # And we keep trying to reach a leaf
+                    if isinstance(node, DTBranch):
+                        node = node.traverse(x, until_leaf=False)
+                # Once a leaf is reached, the traversal can stop
+                if isinstance(node, HTLeaf):
+                    break
+            # Learn from the sample
+            node.learn_one(x, y, sample_weight=sample_weight, tree=self)
+
+        if self._train_weight_seen_by_model % self.memory_estimate_period == 0:
+            self._estimate_model_size()
+
+        return self
+
+    def predict_one(self, x):
+        """Predict the target value using one of the leaf prediction strategies.
+
+        Parameters
+        ----------
+        x
+            Instance for which we want to predict the target.
+
+        Returns
+        -------
+        Predicted target value.
+
+        """
+        pred = 0.0
+        if self._root is not None:
+            if isinstance(self._root, DTBranch):
+                leaf = self._root.traverse(x, until_leaf=True)
+            else:
+                leaf = self._root
+
+            pred = leaf.prediction(x, tree=self)
+        return pred
+
+    def _attempt_to_split(
+        self, leaf: HTLeaf, parent: DTBranch, parent_branch: int, **kwargs
+    ):
+        """Attempt to split a node.
+
+        If the target's variance is high at the leaf node, then:
+
+        1. Find split candidates and select the top 2.
+        2. Compute the Hoeffding bound.
+        3. If the ratio between the merit of the second best split candidate and the merit of the
+        best one is smaller than 1 minus the Hoeffding bound (or a tie breaking decision
+        takes place), then:
+           3.1 Replace the leaf node by a split node.
+           3.2 Add a new leaf node on each branch of the new split node.
+           3.3 Update tree's metrics
+
+        Optional: Disable poor attribute. Depends on the tree's configuration.
+
+        Parameters
+        ----------
+        leaf
+            The node to evaluate.
+        parent
+            The node's parent in the tree.
+        parent_branch
+            Parent node's branch index.
+        kwargs
+            Other parameters passed to the new branch.
+
+        """
+        split_criterion = self._new_split_criterion()
+        best_split_suggestions = leaf.best_split_suggestions(split_criterion, self)
+        best_split_suggestions.sort()
+        should_split = False
+        if len(best_split_suggestions) < 2:
+            should_split = len(best_split_suggestions) > 0
+        else:
+            hoeffding_bound = self._hoeffding_bound(
+                split_criterion.range_of_merit(leaf.stats),
+                self.split_confidence,
+                leaf.total_weight,
+            )
+            best_suggestion = best_split_suggestions[-1]
+            second_best_suggestion = best_split_suggestions[-2]
+            if best_suggestion.merit > 0.0 and (
+                second_best_suggestion.merit / best_suggestion.merit
+                < 1 - hoeffding_bound
+                or hoeffding_bound < self.tie_threshold
+            ):
+                should_split = True
+            if self.remove_poor_attrs:
+                poor_attrs = set()
+                best_ratio = second_best_suggestion.merit / best_suggestion.merit
+
+                # Add any poor attribute to set
+                for suggestion in best_split_suggestions:
+                    if (
+                        suggestion.feature
+                        and suggestion.merit / best_suggestion.merit
+                        < best_ratio - 2 * hoeffding_bound
+                    ):
+                        poor_attrs.add(suggestion.feature)
+                for poor_att in poor_attrs:
+                    leaf.disable_attribute(poor_att)
+        if should_split:
+            split_decision = best_split_suggestions[-1]
+            if split_decision.feature is None:
+                # Pre-pruning - null wins
+                leaf.deactivate()
+                self._n_inactive_leaves += 1
+                self._n_active_leaves -= 1
+            else:
+                branch = self._branch_selector(
+                    split_decision.numerical_feature, split_decision.multiway_split
+                )
+                leaves = tuple(
+                    self._new_leaf(initial_stats, parent=leaf)
+                    for initial_stats in split_decision.children_stats
+                )
+
+                new_split = split_decision.assemble(
+                    branch, leaf.stats, leaf.depth, *leaves, **kwargs
+                )
+
+                self._n_active_leaves -= 1
+                self._n_active_leaves += len(leaves)
+                if parent is None:
+                    self._root = new_split
+                else:
+                    parent.children[parent_branch] = new_split
+
+            # Manage memory
+            self._enforce_size_limit()
+        elif (
+            len(best_split_suggestions) >= 2
+            and best_split_suggestions[-1].merit > 0
+            and best_split_suggestions[-2].merit > 0
+        ):
+            last_check_ratio = (
+                best_split_suggestions[-2].merit / best_split_suggestions[-1].merit
+            )
+            last_check_vr = best_split_suggestions[-1].merit
+
+            leaf.manage_memory(
+                split_criterion, last_check_ratio, last_check_vr, hoeffding_bound
+            )
```

### Comparing `river-0.8.0/river/tree/isoup_tree_regressor.py` & `river-0.9.0/river/tree/isoup_tree_regressor.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,270 +1,271 @@
-import typing
-from copy import deepcopy
-
-from river import base, tree
-
-from .nodes.branch import DTBranch
-from .nodes.isouptr_nodes import (
-    LeafAdaptiveMultiTarget,
-    LeafMeanMultiTarget,
-    LeafModelMultiTarget,
-)
-from .split_criterion import IntraClusterVarianceReductionSplitCriterion
-from .splitter import Splitter
-
-
-class iSOUPTreeRegressor(tree.HoeffdingTreeRegressor, base.MultiOutputMixin):
-    """Incremental Structured Output Prediction Tree (iSOUP-Tree) for multi-target regression.
-
-    This is an implementation of the iSOUP-Tree proposed by A. Osojnik, P. Panov, and
-    S. Džeroski [^1].
-
-    Parameters
-    ----------
-    grace_period
-        Number of instances a leaf should observe between split attempts.
-    max_depth
-        The maximum depth a tree can reach. If `None`, the tree will grow indefinitely.
-    split_confidence
-        Allowed error in split decision, a value closer to 0 takes longer to
-        decide.
-    tie_threshold
-        Threshold below which a split will be forced to break ties.
-    leaf_prediction
-        Prediction mechanism used at leafs.</br>
-        - 'mean' - Target mean</br>
-        - 'model' - Uses the model defined in `leaf_model`</br>
-        - 'adaptive' - Chooses between 'mean' and 'model' dynamically</br>
-    leaf_model
-        The regression model(s) used to provide responses if `leaf_prediction='model'`. It can
-        be either a regressor (in which case it is going to be replicated to all the targets)
-        or a dictionary whose keys are target identifiers, and the values are instances of
-        `river.base.Regressor.` If not provided, instances of `river.linear_model.LinearRegression`
-        with the default hyperparameters are used for all the targets. If a dictionary is passed
-        and not all target models are specified, copies from the first model match in the
-        dictionary will be used to the remaining targets.
-    model_selector_decay
-        The exponential decaying factor applied to the learning models' squared errors, that
-        are monitored if `leaf_prediction='adaptive'`. Must be between `0` and `1`. The closer
-        to `1`, the more importance is going to be given to past observations. On the other hand,
-        if its value approaches `0`, the recent observed errors are going to have more influence
-        on the final decision.
-    nominal_attributes
-        List of Nominal attributes identifiers. If empty, then assume that all numeric attributes
-        should be treated as continuous.
-    splitter
-        The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric
-        features and perform splits. Splitters are available in the `tree.splitter` module.
-        Different splitters are available for classification and regression tasks. Classification
-        and regression splitters can be distinguished by their property `is_target_class`.
-        This is an advanced option. Special care must be taken when choosing different splitters.
-        By default, `tree.splitter.EBSTSplitter` is used if `splitter` is `None`.
-    min_samples_split
-        The minimum number of samples every branch resulting from a split candidate must have
-        to be considered valid.
-    binary_split
-        If True, only allow binary splits.
-    max_size
-        The max size of the tree, in Megabytes (MB).
-    memory_estimate_period
-        Interval (number of processed instances) between memory consumption checks.
-    stop_mem_management
-        If True, stop growing as soon as memory limit is hit.
-    remove_poor_attrs
-        If True, disable poor attributes to reduce memory usage.
-    merit_preprune
-        If True, enable merit-based tree pre-pruning.
-
-    References
-    ----------
-    [^1]: Aljaž Osojnik, Panče Panov, and Sašo Džeroski. "Tree-based methods for online
-        multi-target regression." Journal of Intelligent Information Systems 50.2 (2018): 315-339.
-
-    Examples
-    --------
-    >>> import numbers
-    >>> from river import compose
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import linear_model
-    >>> from river import metrics
-    >>> from river import preprocessing
-    >>> from river import tree
-
-    >>> dataset = datasets.SolarFlare()
-
-    >>> num = compose.SelectType(numbers.Number) | preprocessing.MinMaxScaler()
-    >>> cat = compose.SelectType(str) | preprocessing.OneHotEncoder(sparse=False)
-
-    >>> model = tree.iSOUPTreeRegressor(
-    ...     grace_period=100,
-    ...     leaf_prediction='model',
-    ...     leaf_model={
-    ...         'c-class-flares': linear_model.LinearRegression(l2=0.02),
-    ...         'm-class-flares': linear_model.PARegressor(),
-    ...         'x-class-flares': linear_model.LinearRegression(l2=0.1)
-    ...     }
-    ... )
-
-    >>> pipeline = (num + cat) | model
-    >>> metric = metrics.RegressionMultiOutput(metrics.MAE())
-
-    >>> evaluate.progressive_val_score(dataset, pipeline, metric)
-    MAE: 0.425929
-    """
-
-    def __init__(
-        self,
-        grace_period: int = 200,
-        max_depth: int = None,
-        split_confidence: float = 1e-7,
-        tie_threshold: float = 0.05,
-        leaf_prediction: str = "model",
-        leaf_model: typing.Union[base.Regressor, typing.Dict] = None,
-        model_selector_decay: float = 0.95,
-        nominal_attributes: list = None,
-        splitter: Splitter = None,
-        min_samples_split: int = 5,
-        binary_split: bool = False,
-        max_size: int = 500,
-        memory_estimate_period: int = 1000000,
-        stop_mem_management: bool = False,
-        remove_poor_attrs: bool = False,
-        merit_preprune: bool = True,
-    ):
-        super().__init__(
-            grace_period=grace_period,
-            max_depth=max_depth,
-            split_confidence=split_confidence,
-            tie_threshold=tie_threshold,
-            leaf_prediction=leaf_prediction,
-            leaf_model=leaf_model,
-            model_selector_decay=model_selector_decay,
-            nominal_attributes=nominal_attributes,
-            splitter=splitter,
-            min_samples_split=min_samples_split,
-            binary_split=binary_split,
-            max_size=max_size,
-            memory_estimate_period=memory_estimate_period,
-            stop_mem_management=stop_mem_management,
-            remove_poor_attrs=remove_poor_attrs,
-            merit_preprune=merit_preprune,
-        )
-
-        self.split_criterion: str = "icvr"  # intra cluster variance reduction
-        self.targets: set = set()
-
-    @tree.HoeffdingTreeRegressor.split_criterion.setter
-    def split_criterion(self, split_criterion):
-        if split_criterion == "vr":
-            # Corner case due to parent class initialization
-            split_criterion = "icvr"
-        if split_criterion != "icvr":  # intra cluster variance reduction
-            print(
-                'Invalid split_criterion option "{}", will use default "{}"'.format(
-                    split_criterion, "icvr"
-                )
-            )
-            self._split_criterion = "icvr"
-        else:
-            self._split_criterion = split_criterion
-
-    def _new_split_criterion(self):
-        return IntraClusterVarianceReductionSplitCriterion(
-            min_samples_split=self.min_samples_split
-        )
-
-    def _new_leaf(self, initial_stats=None, parent=None):
-        """Create a new learning node. The type of learning node depends on
-        the tree configuration.
-        """
-        if parent is not None:
-            depth = parent.depth + 1
-        else:
-            depth = 0
-
-        leaf_models = None
-        if self.leaf_prediction in {self._MODEL, self._ADAPTIVE}:
-            if parent is None:
-                leaf_models = {}
-            else:
-                try:
-                    leaf_models = deepcopy(parent._leaf_models)  # noqa
-                except AttributeError:
-                    # Due to an emerging category in a nominal feature, a split node was reached
-                    leaf_models = {}
-
-        if self.leaf_prediction == self._TARGET_MEAN:
-            return LeafMeanMultiTarget(initial_stats, depth, self.splitter)
-        elif self.leaf_prediction == self._MODEL:
-            return LeafModelMultiTarget(
-                initial_stats, depth, self.splitter, leaf_models
-            )
-        else:  # adaptive learning node
-            new_adaptive = LeafAdaptiveMultiTarget(
-                initial_stats, depth, self.splitter, leaf_models
-            )
-            if parent is not None and isinstance(parent, LeafAdaptiveMultiTarget):
-                new_adaptive._fmse_mean = parent._fmse_mean.copy()  # noqa
-                new_adaptive._fmse_model = parent._fmse_model.copy()  # noqa
-
-            return new_adaptive
-
-    def learn_one(
-        self,
-        x: dict,
-        y: typing.Dict[typing.Hashable, base.typing.RegTarget],
-        *,
-        sample_weight: float = 1.0
-    ) -> "iSOUPTreeRegressor":
-        """Incrementally train the model with one sample.
-
-        Training tasks:
-
-        * If the tree is empty, create a leaf node as the root.
-        * If the tree is already initialized, find the corresponding leaf for
-          the instance and update the leaf node statistics.
-        * If growth is allowed and the number of instances that the leaf has
-          observed between split attempts exceed the grace period then attempt
-          to split.
-
-        Parameters
-        ----------
-        x
-            Instance attributes.
-        y
-            Target values.
-        sample_weight
-            The weight of the passed sample.
-        """
-        # Update target set
-        self.targets.update(y.keys())
-
-        super().learn_one(x, y, sample_weight=sample_weight)  # noqa
-
-        return self
-
-    def predict_one(
-        self, x: dict
-    ) -> typing.Dict[typing.Hashable, base.typing.RegTarget]:
-        """Predict the target values for a given instance.
-
-        Parameters
-        ----------
-        x
-            Sample for which we want to predict the labels.
-
-        Returns
-        -------
-        dict
-            Predicted target values.
-        """
-        pred = {}
-        if self._root is not None:
-            if isinstance(self._root, DTBranch):
-                leaf = self._root.traverse(x, until_leaf=True)
-            else:
-                leaf = self._root
-
-            pred = leaf.prediction(x, tree=self)
-        return pred
+import typing
+from copy import deepcopy
+
+from river import base, tree
+
+from .nodes.branch import DTBranch
+from .nodes.isouptr_nodes import (
+    LeafAdaptiveMultiTarget,
+    LeafMeanMultiTarget,
+    LeafModelMultiTarget,
+)
+from .split_criterion import IntraClusterVarianceReductionSplitCriterion
+from .splitter import Splitter
+
+
+class iSOUPTreeRegressor(tree.HoeffdingTreeRegressor, base.MultiOutputMixin):
+    """Incremental Structured Output Prediction Tree (iSOUP-Tree) for multi-target regression.
+
+    This is an implementation of the iSOUP-Tree proposed by A. Osojnik, P. Panov, and
+    S. Džeroski [^1].
+
+    Parameters
+    ----------
+    grace_period
+        Number of instances a leaf should observe between split attempts.
+    max_depth
+        The maximum depth a tree can reach. If `None`, the tree will grow indefinitely.
+    split_confidence
+        Allowed error in split decision, a value closer to 0 takes longer to
+        decide.
+    tie_threshold
+        Threshold below which a split will be forced to break ties.
+    leaf_prediction
+        Prediction mechanism used at leafs.</br>
+        - 'mean' - Target mean</br>
+        - 'model' - Uses the model defined in `leaf_model`</br>
+        - 'adaptive' - Chooses between 'mean' and 'model' dynamically</br>
+    leaf_model
+        The regression model(s) used to provide responses if `leaf_prediction='model'`. It can
+        be either a regressor (in which case it is going to be replicated to all the targets)
+        or a dictionary whose keys are target identifiers, and the values are instances of
+        `river.base.Regressor.` If not provided, instances of `river.linear_model.LinearRegression`
+        with the default hyperparameters are used for all the targets. If a dictionary is passed
+        and not all target models are specified, copies from the first model match in the
+        dictionary will be used to the remaining targets.
+    model_selector_decay
+        The exponential decaying factor applied to the learning models' squared errors, that
+        are monitored if `leaf_prediction='adaptive'`. Must be between `0` and `1`. The closer
+        to `1`, the more importance is going to be given to past observations. On the other hand,
+        if its value approaches `0`, the recent observed errors are going to have more influence
+        on the final decision.
+    nominal_attributes
+        List of Nominal attributes identifiers. If empty, then assume that all numeric attributes
+        should be treated as continuous.
+    splitter
+        The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric
+        features and perform splits. Splitters are available in the `tree.splitter` module.
+        Different splitters are available for classification and regression tasks. Classification
+        and regression splitters can be distinguished by their property `is_target_class`.
+        This is an advanced option. Special care must be taken when choosing different splitters.
+        By default, `tree.splitter.EBSTSplitter` is used if `splitter` is `None`.
+    min_samples_split
+        The minimum number of samples every branch resulting from a split candidate must have
+        to be considered valid.
+    binary_split
+        If True, only allow binary splits.
+    max_size
+        The max size of the tree, in Megabytes (MB).
+    memory_estimate_period
+        Interval (number of processed instances) between memory consumption checks.
+    stop_mem_management
+        If True, stop growing as soon as memory limit is hit.
+    remove_poor_attrs
+        If True, disable poor attributes to reduce memory usage.
+    merit_preprune
+        If True, enable merit-based tree pre-pruning.
+
+    References
+    ----------
+    [^1]: Aljaž Osojnik, Panče Panov, and Sašo Džeroski. "Tree-based methods for online
+        multi-target regression." Journal of Intelligent Information Systems 50.2 (2018): 315-339.
+
+    Examples
+    --------
+    >>> import numbers
+    >>> from river import compose
+    >>> from river import datasets
+    >>> from river import evaluate
+    >>> from river import linear_model
+    >>> from river import metrics
+    >>> from river import preprocessing
+    >>> from river import tree
+
+    >>> dataset = datasets.SolarFlare()
+
+    >>> num = compose.SelectType(numbers.Number) | preprocessing.MinMaxScaler()
+    >>> cat = compose.SelectType(str) | preprocessing.OneHotEncoder(sparse=False)
+
+    >>> model = tree.iSOUPTreeRegressor(
+    ...     grace_period=100,
+    ...     leaf_prediction='model',
+    ...     leaf_model={
+    ...         'c-class-flares': linear_model.LinearRegression(l2=0.02),
+    ...         'm-class-flares': linear_model.PARegressor(),
+    ...         'x-class-flares': linear_model.LinearRegression(l2=0.1)
+    ...     }
+    ... )
+
+    >>> pipeline = (num + cat) | model
+    >>> metric = metrics.RegressionMultiOutput(metrics.MAE())
+
+    >>> evaluate.progressive_val_score(dataset, pipeline, metric)
+    MAE: 0.426177
+
+    """
+
+    def __init__(
+        self,
+        grace_period: int = 200,
+        max_depth: int = None,
+        split_confidence: float = 1e-7,
+        tie_threshold: float = 0.05,
+        leaf_prediction: str = "model",
+        leaf_model: typing.Union[base.Regressor, typing.Dict] = None,
+        model_selector_decay: float = 0.95,
+        nominal_attributes: list = None,
+        splitter: Splitter = None,
+        min_samples_split: int = 5,
+        binary_split: bool = False,
+        max_size: int = 500,
+        memory_estimate_period: int = 1000000,
+        stop_mem_management: bool = False,
+        remove_poor_attrs: bool = False,
+        merit_preprune: bool = True,
+    ):
+        super().__init__(
+            grace_period=grace_period,
+            max_depth=max_depth,
+            split_confidence=split_confidence,
+            tie_threshold=tie_threshold,
+            leaf_prediction=leaf_prediction,
+            leaf_model=leaf_model,
+            model_selector_decay=model_selector_decay,
+            nominal_attributes=nominal_attributes,
+            splitter=splitter,
+            min_samples_split=min_samples_split,
+            binary_split=binary_split,
+            max_size=max_size,
+            memory_estimate_period=memory_estimate_period,
+            stop_mem_management=stop_mem_management,
+            remove_poor_attrs=remove_poor_attrs,
+            merit_preprune=merit_preprune,
+        )
+
+        self.split_criterion: str = "icvr"  # intra cluster variance reduction
+        self.targets: set = set()
+
+    @tree.HoeffdingTreeRegressor.split_criterion.setter
+    def split_criterion(self, split_criterion):
+        if split_criterion == "vr":
+            # Corner case due to parent class initialization
+            split_criterion = "icvr"
+        if split_criterion != "icvr":  # intra cluster variance reduction
+            print(
+                'Invalid split_criterion option "{}", will use default "{}"'.format(
+                    split_criterion, "icvr"
+                )
+            )
+            self._split_criterion = "icvr"
+        else:
+            self._split_criterion = split_criterion
+
+    def _new_split_criterion(self):
+        return IntraClusterVarianceReductionSplitCriterion(
+            min_samples_split=self.min_samples_split
+        )
+
+    def _new_leaf(self, initial_stats=None, parent=None):
+        """Create a new learning node. The type of learning node depends on
+        the tree configuration.
+        """
+        if parent is not None:
+            depth = parent.depth + 1
+        else:
+            depth = 0
+
+        leaf_models = None
+        if self.leaf_prediction in {self._MODEL, self._ADAPTIVE}:
+            if parent is None:
+                leaf_models = {}
+            else:
+                try:
+                    leaf_models = deepcopy(parent._leaf_models)  # noqa
+                except AttributeError:
+                    # Due to an emerging category in a nominal feature, a split node was reached
+                    leaf_models = {}
+
+        if self.leaf_prediction == self._TARGET_MEAN:
+            return LeafMeanMultiTarget(initial_stats, depth, self.splitter)
+        elif self.leaf_prediction == self._MODEL:
+            return LeafModelMultiTarget(
+                initial_stats, depth, self.splitter, leaf_models
+            )
+        else:  # adaptive learning node
+            new_adaptive = LeafAdaptiveMultiTarget(
+                initial_stats, depth, self.splitter, leaf_models
+            )
+            if parent is not None and isinstance(parent, LeafAdaptiveMultiTarget):
+                new_adaptive._fmse_mean = parent._fmse_mean.copy()  # noqa
+                new_adaptive._fmse_model = parent._fmse_model.copy()  # noqa
+
+            return new_adaptive
+
+    def learn_one(
+        self,
+        x: dict,
+        y: typing.Dict[typing.Hashable, base.typing.RegTarget],
+        *,
+        sample_weight: float = 1.0
+    ) -> "iSOUPTreeRegressor":
+        """Incrementally train the model with one sample.
+
+        Training tasks:
+
+        * If the tree is empty, create a leaf node as the root.
+        * If the tree is already initialized, find the corresponding leaf for
+          the instance and update the leaf node statistics.
+        * If growth is allowed and the number of instances that the leaf has
+          observed between split attempts exceed the grace period then attempt
+          to split.
+
+        Parameters
+        ----------
+        x
+            Instance attributes.
+        y
+            Target values.
+        sample_weight
+            The weight of the passed sample.
+        """
+        # Update target set
+        self.targets.update(y.keys())
+
+        super().learn_one(x, y, sample_weight=sample_weight)  # noqa
+
+        return self
+
+    def predict_one(
+        self, x: dict
+    ) -> typing.Dict[typing.Hashable, base.typing.RegTarget]:
+        """Predict the target values for a given instance.
+
+        Parameters
+        ----------
+        x
+            Sample for which we want to predict the labels.
+
+        Returns
+        -------
+        dict
+            Predicted target values.
+        """
+        pred = {}
+        if self._root is not None:
+            if isinstance(self._root, DTBranch):
+                leaf = self._root.traverse(x, until_leaf=True)
+            else:
+                leaf = self._root
+
+            pred = leaf.prediction(x, tree=self)
+        return pred
```

### Comparing `river-0.8.0/river/tree/label_combination_hoeffding_tree.py` & `river-0.9.0/river/tree/label_combination_hoeffding_tree.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,203 +1,203 @@
-import typing
-import warnings
-
-from river import base, tree
-from river.utils.skmultiflow_utils import normalize_values_in_dict
-
-from .splitter import Splitter
-
-
-class LabelCombinationHoeffdingTreeClassifier(
-    tree.HoeffdingTreeClassifier, base.MultiOutputMixin
-):
-    """Label Combination Hoeffding Tree for multi-label classification.
-
-    Label combination transforms the problem from multi-label to multi-class.
-    For each unique combination of labels it assigns a class and proceeds
-    with training the hoeffding tree normally.
-
-    The transformation is done by changing the label set which could be seen
-    as a binary number to an int which will represent the class, and after
-    the prediction the int is converted back to a binary number which is the
-    predicted label-set.
-
-    Parameters
-    ----------
-    grace_period
-        Number of instances a leaf should observe between split attempts.
-    max_depth
-        The maximum depth a tree can reach. If `None`, the tree will grow indefinitely.
-    split_criterion
-        Split criterion to use.</br>
-        - 'gini' - Gini</br>
-        - 'info_gain' - Information Gain</br>
-        - 'hellinger' - Helinger Distance</br>
-    split_confidence
-        Allowed error in split decision, a value closer to 0 takes longer to decide.
-    tie_threshold
-        Threshold below which a split will be forced to break ties.
-    leaf_prediction
-        Prediction mechanism used at leafs.</br>
-        - 'mc' - Majority Class</br>
-        - 'nb' - Naive Bayes</br>
-        - 'nba' - Naive Bayes Adaptive</br>
-    nb_threshold
-        Number of instances a leaf should observe before allowing Naive Bayes.
-    nominal_attributes
-        List of Nominal attributes identifiers. If empty, then assume that all numeric attributes
-        should be treated as continuous.
-    splitter
-        The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric
-        features and perform splits. Splitters are available in the `tree.splitter` module.
-        Different splitters are available for classification and regression tasks. Classification
-        and regression splitters can be distinguished by their property `is_target_class`.
-        This is an advanced option. Special care must be taken when choosing different splitters.
-        By default, `tree.splitter.GaussianSplitter` is used if `splitter` is `None`.
-    binary_split
-        If True, only allow binary splits.
-    max_size
-        The max size of the tree, in Megabytes (MB).
-    memory_estimate_period
-        Interval (number of processed instances) between memory consumption checks.
-    stop_mem_management
-        If True, stop growing as soon as memory limit is hit.
-    remove_poor_attrs
-        If True, disable poor attributes to reduce memory usage.
-    merit_preprune
-        If True, enable merit-based tree pre-pruning.
-
-    Examples
-    --------
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import metrics
-    >>> from river import tree
-
-    >>> dataset = iter(datasets.Music().take(200))
-    >>> model = tree.LabelCombinationHoeffdingTreeClassifier(
-    ...     split_confidence=1e-5,
-    ...     grace_period=50
-    ... )
-
-    >>> metric = metrics.Hamming()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    Hamming: 0.154104
-    """
-
-    def __init__(
-        self,
-        grace_period: int = 200,
-        max_depth: int = None,
-        split_criterion: str = "info_gain",
-        split_confidence: float = 1e-7,
-        tie_threshold: float = 0.05,
-        leaf_prediction: str = "nba",
-        nb_threshold: int = 0,
-        nominal_attributes: list = None,
-        splitter: Splitter = None,
-        binary_split: bool = False,
-        max_size: int = 100,
-        memory_estimate_period: int = 1000000,
-        stop_mem_management: bool = False,
-        remove_poor_attrs: bool = False,
-        merit_preprune: bool = True,
-    ):
-
-        super().__init__(
-            grace_period=grace_period,
-            max_depth=max_depth,
-            split_criterion=split_criterion,
-            split_confidence=split_confidence,
-            tie_threshold=tie_threshold,
-            leaf_prediction=leaf_prediction,
-            nb_threshold=nb_threshold,
-            nominal_attributes=nominal_attributes,
-            splitter=splitter,
-            binary_split=binary_split,
-            max_size=max_size,
-            memory_estimate_period=memory_estimate_period,
-            stop_mem_management=stop_mem_management,
-            remove_poor_attrs=remove_poor_attrs,
-            merit_preprune=merit_preprune,
-        )
-
-        self._next_label_code: int = 0
-        self._label_map: typing.Dict[typing.Tuple, int] = {}
-        self._r_label_map: typing.Dict[int, typing.Tuple] = {}
-        self._labels: typing.Set[typing.Hashable] = set()
-
-    def learn_one(self, x, y, *, sample_weight=1.0):
-        """Update the Multi-label Hoeffding Tree Classifier.
-
-        Parameters
-        ----------
-        x
-            Instance attributes.
-        y
-            Labels of the instance.
-        sample_weight
-            The weight of the sample.
-
-        Returns
-        -------
-        self
-        """
-        self._labels.update(y.keys())  # noqa
-
-        aux_label = tuple(sorted(y.items()))  # noqa
-        if aux_label not in self._label_map:
-            self._label_map[aux_label] = self._next_label_code
-            self._r_label_map[self._next_label_code] = aux_label
-            self._next_label_code += 1
-        y_encoded = self._label_map[aux_label]
-
-        super().learn_one(x, y_encoded, sample_weight=sample_weight)
-
-        return self
-
-    def predict_proba_one(self, x):
-        if self._root is None:
-            return None
-
-        enc_probas = super().predict_proba_one(x)
-        enc_class = max(enc_probas, key=enc_probas.get)
-
-        result = {}
-        for lbl in self._labels:
-            result[lbl] = {False: 0.0, True: 0.0}
-
-        for label_id, label_val in self._r_label_map[enc_class]:
-            result[label_id][label_val] = enc_probas[enc_class]
-            result[label_id] = normalize_values_in_dict(result[label_id])
-
-        return result
-
-    def predict_one(self, x):
-        """Predict the labels of an instance.
-
-        Parameters
-        ----------
-        x
-            The instance for which we want to predict labels.
-
-        Returns
-        -------
-        Predicted labels.
-        """
-        if self._root is None:
-            return None
-
-        probas = self.predict_proba_one(x)
-
-        preds = {}
-        for label_id, label_probas in probas.items():
-            preds[label_id] = max(label_probas, key=label_probas.get)
-
-        return preds
-
-    def debug_one(self, x: dict):
-        warnings.warn(f"'debug_one' is not supported by {self.__class__.__name__}")
-
-    def draw(self, max_depth: int = None):
-        warnings.warn(f"'draw' is not supported by {self.__class__.__name__}")
+import typing
+import warnings
+
+from river import base, tree
+from river.utils.skmultiflow_utils import normalize_values_in_dict
+
+from .splitter import Splitter
+
+
+class LabelCombinationHoeffdingTreeClassifier(
+    tree.HoeffdingTreeClassifier, base.MultiOutputMixin
+):
+    """Label Combination Hoeffding Tree for multi-label classification.
+
+    Label combination transforms the problem from multi-label to multi-class.
+    For each unique combination of labels it assigns a class and proceeds
+    with training the hoeffding tree normally.
+
+    The transformation is done by changing the label set which could be seen
+    as a binary number to an int which will represent the class, and after
+    the prediction the int is converted back to a binary number which is the
+    predicted label-set.
+
+    Parameters
+    ----------
+    grace_period
+        Number of instances a leaf should observe between split attempts.
+    max_depth
+        The maximum depth a tree can reach. If `None`, the tree will grow indefinitely.
+    split_criterion
+        Split criterion to use.</br>
+        - 'gini' - Gini</br>
+        - 'info_gain' - Information Gain</br>
+        - 'hellinger' - Helinger Distance</br>
+    split_confidence
+        Allowed error in split decision, a value closer to 0 takes longer to decide.
+    tie_threshold
+        Threshold below which a split will be forced to break ties.
+    leaf_prediction
+        Prediction mechanism used at leafs.</br>
+        - 'mc' - Majority Class</br>
+        - 'nb' - Naive Bayes</br>
+        - 'nba' - Naive Bayes Adaptive</br>
+    nb_threshold
+        Number of instances a leaf should observe before allowing Naive Bayes.
+    nominal_attributes
+        List of Nominal attributes identifiers. If empty, then assume that all numeric attributes
+        should be treated as continuous.
+    splitter
+        The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric
+        features and perform splits. Splitters are available in the `tree.splitter` module.
+        Different splitters are available for classification and regression tasks. Classification
+        and regression splitters can be distinguished by their property `is_target_class`.
+        This is an advanced option. Special care must be taken when choosing different splitters.
+        By default, `tree.splitter.GaussianSplitter` is used if `splitter` is `None`.
+    binary_split
+        If True, only allow binary splits.
+    max_size
+        The max size of the tree, in Megabytes (MB).
+    memory_estimate_period
+        Interval (number of processed instances) between memory consumption checks.
+    stop_mem_management
+        If True, stop growing as soon as memory limit is hit.
+    remove_poor_attrs
+        If True, disable poor attributes to reduce memory usage.
+    merit_preprune
+        If True, enable merit-based tree pre-pruning.
+
+    Examples
+    --------
+    >>> from river import datasets
+    >>> from river import evaluate
+    >>> from river import metrics
+    >>> from river import tree
+
+    >>> dataset = iter(datasets.Music().take(200))
+    >>> model = tree.LabelCombinationHoeffdingTreeClassifier(
+    ...     split_confidence=1e-5,
+    ...     grace_period=50
+    ... )
+
+    >>> metric = metrics.Hamming()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    Hamming: 0.154104
+    """
+
+    def __init__(
+        self,
+        grace_period: int = 200,
+        max_depth: int = None,
+        split_criterion: str = "info_gain",
+        split_confidence: float = 1e-7,
+        tie_threshold: float = 0.05,
+        leaf_prediction: str = "nba",
+        nb_threshold: int = 0,
+        nominal_attributes: list = None,
+        splitter: Splitter = None,
+        binary_split: bool = False,
+        max_size: int = 100,
+        memory_estimate_period: int = 1000000,
+        stop_mem_management: bool = False,
+        remove_poor_attrs: bool = False,
+        merit_preprune: bool = True,
+    ):
+
+        super().__init__(
+            grace_period=grace_period,
+            max_depth=max_depth,
+            split_criterion=split_criterion,
+            split_confidence=split_confidence,
+            tie_threshold=tie_threshold,
+            leaf_prediction=leaf_prediction,
+            nb_threshold=nb_threshold,
+            nominal_attributes=nominal_attributes,
+            splitter=splitter,
+            binary_split=binary_split,
+            max_size=max_size,
+            memory_estimate_period=memory_estimate_period,
+            stop_mem_management=stop_mem_management,
+            remove_poor_attrs=remove_poor_attrs,
+            merit_preprune=merit_preprune,
+        )
+
+        self._next_label_code: int = 0
+        self._label_map: typing.Dict[typing.Tuple, int] = {}
+        self._r_label_map: typing.Dict[int, typing.Tuple] = {}
+        self._labels: typing.Set[typing.Hashable] = set()
+
+    def learn_one(self, x, y, *, sample_weight=1.0):
+        """Update the Multi-label Hoeffding Tree Classifier.
+
+        Parameters
+        ----------
+        x
+            Instance attributes.
+        y
+            Labels of the instance.
+        sample_weight
+            The weight of the sample.
+
+        Returns
+        -------
+        self
+        """
+        self._labels.update(y.keys())  # noqa
+
+        aux_label = tuple(sorted(y.items()))  # noqa
+        if aux_label not in self._label_map:
+            self._label_map[aux_label] = self._next_label_code
+            self._r_label_map[self._next_label_code] = aux_label
+            self._next_label_code += 1
+        y_encoded = self._label_map[aux_label]
+
+        super().learn_one(x, y_encoded, sample_weight=sample_weight)
+
+        return self
+
+    def predict_proba_one(self, x):
+        if self._root is None:
+            return None
+
+        enc_probas = super().predict_proba_one(x)
+        enc_class = max(enc_probas, key=enc_probas.get)
+
+        result = {}
+        for lbl in self._labels:
+            result[lbl] = {False: 0.0, True: 0.0}
+
+        for label_id, label_val in self._r_label_map[enc_class]:
+            result[label_id][label_val] = enc_probas[enc_class]
+            result[label_id] = normalize_values_in_dict(result[label_id])
+
+        return result
+
+    def predict_one(self, x):
+        """Predict the labels of an instance.
+
+        Parameters
+        ----------
+        x
+            The instance for which we want to predict labels.
+
+        Returns
+        -------
+        Predicted labels.
+        """
+        if self._root is None:
+            return None
+
+        probas = self.predict_proba_one(x)
+
+        preds = {}
+        for label_id, label_probas in probas.items():
+            preds[label_id] = max(label_probas, key=label_probas.get)
+
+        return preds
+
+    def debug_one(self, x: dict):
+        warnings.warn(f"'debug_one' is not supported by {self.__class__.__name__}")
+
+    def draw(self, max_depth: int = None):
+        warnings.warn(f"'draw' is not supported by {self.__class__.__name__}")
```

### Comparing `river-0.8.0/river/tree/losses.py` & `river-0.9.0/river/tree/losses.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,52 +1,52 @@
-import abc
-import math
-
-from .utils import GradHess
-
-
-class Loss(abc.ABC):
-    """Base class to implement optimization objectives used in Stochastic Gradient Trees. """
-
-    def compute_derivatives(self, y_true: float, y_pred: float) -> GradHess:
-        """ Return the gradient and hessian data concerning one instance and its prediction.
-
-        Parameters
-        ----------
-        y_true
-            Target value.
-        y_pred
-            Predicted target value.
-        """
-        pass
-
-    def transfer(self, y: float) -> float:
-        """ Optionally apply some transformation to the value predicted by the tree before
-        returning it.
-
-        For instance, in classification, the softmax operation might be applied.
-
-        Parameters
-        ----------
-        y
-            Value to be transformed.
-        """
-        return y
-
-
-class BinaryCrossEntropyLoss(Loss):
-    """ Loss function used in binary classification tasks. """
-
-    def compute_derivatives(self, y_true, y_pred):
-        y_trs = self.transfer(y_pred)
-
-        return GradHess(y_trs - y_true, y_trs * (1.0 - y_trs))
-
-    def transfer(self, y):
-        return 1.0 / (1.0 + math.exp(-y))
-
-
-class SquaredErrorLoss(Loss):
-    """ Loss function used in regression tasks. """
-
-    def compute_derivatives(self, y_true, y_pred):
-        return GradHess(y_pred - y_true, 1.0)
+import abc
+import math
+
+from .utils import GradHess
+
+
+class Loss(abc.ABC):
+    """Base class to implement optimization objectives used in Stochastic Gradient Trees. """
+
+    def compute_derivatives(self, y_true: float, y_pred: float) -> GradHess:
+        """ Return the gradient and hessian data concerning one instance and its prediction.
+
+        Parameters
+        ----------
+        y_true
+            Target value.
+        y_pred
+            Predicted target value.
+        """
+        pass
+
+    def transfer(self, y: float) -> float:
+        """ Optionally apply some transformation to the value predicted by the tree before
+        returning it.
+
+        For instance, in classification, the softmax operation might be applied.
+
+        Parameters
+        ----------
+        y
+            Value to be transformed.
+        """
+        return y
+
+
+class BinaryCrossEntropyLoss(Loss):
+    """ Loss function used in binary classification tasks. """
+
+    def compute_derivatives(self, y_true, y_pred):
+        y_trs = self.transfer(y_pred)
+
+        return GradHess(y_trs - y_true, y_trs * (1.0 - y_trs))
+
+    def transfer(self, y):
+        return 1.0 / (1.0 + math.exp(-y))
+
+
+class SquaredErrorLoss(Loss):
+    """ Loss function used in regression tasks. """
+
+    def compute_derivatives(self, y_true, y_pred):
+        return GradHess(y_pred - y_true, 1.0)
```

### Comparing `river-0.8.0/river/tree/nodes/arf_htc_nodes.py` & `river-0.9.0/river/tree/nodes/arf_htc_nodes.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,136 +1,136 @@
-import typing
-
-from river.utils.skmultiflow_utils import check_random_state
-
-from .htc_nodes import LeafMajorityClass, LeafNaiveBayes, LeafNaiveBayesAdaptive
-from .leaf import HTLeaf
-
-
-class BaseRandomLeaf(HTLeaf):
-    """The Random Learning Node changes the way in which the attribute observers
-    are updated (by using subsets of features).
-
-    Parameters
-    ----------
-    stats
-        Initial class observations.
-    depth
-        The depth of the node.
-    splitter
-        The numeric attribute observer algorithm used to monitor target statistics
-        and perform split attempts.
-    max_features
-        Number of attributes per subset for each node split.
-    seed
-        If int, seed is the seed used by the random number generator;
-        If RandomState instance, seed is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
-    kwargs
-        Other parameters passed to the learning node.
-    """
-
-    def __init__(self, stats, depth, splitter, max_features, seed, **kwargs):
-        super().__init__(stats, depth, splitter, **kwargs)
-        self.max_features = max_features
-        self.seed = seed
-        self._rng = check_random_state(self.seed)
-        self.feature_indices = []
-
-    def _iter_features(self, x) -> typing.Iterable:
-        # Only a random subset of the features is monitored
-        if len(self.feature_indices) == 0:
-            self.feature_indices = self._sample_features(x, self.max_features)
-
-        for att_id in self.feature_indices:
-            # First check if the feature is available
-            if att_id in x:
-                yield att_id, x[att_id]
-
-    def _sample_features(self, x, max_features):
-        selected = self._rng.choice(len(x), size=max_features, replace=False)
-        features = list(x.keys())
-        return [features[s] for s in selected]
-
-
-class RandomLeafMajorityClass(BaseRandomLeaf, LeafMajorityClass):
-    """ARF learning node that always predicts the majority class.
-
-    Parameters
-    ----------
-    stats
-        Initial class observations.
-    depth
-        The depth of the node.
-    splitter
-        The numeric attribute observer algorithm used to monitor target statistics
-        and perform split attempts.
-    max_features
-        Number of attributes per subset for each node split.
-    seed
-        If int, seed is the seed used by the random number generator;
-        If RandomState instance, seed is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
-    kwargs
-        Other parameters passed to the learning node.
-
-    """
-
-    def __init__(self, stats, depth, splitter, max_features, seed, **kwargs):
-        super().__init__(stats, depth, splitter, max_features, seed, **kwargs)
-
-
-class RandomLeafNaiveBayes(BaseRandomLeaf, LeafNaiveBayes):
-    """ARF Naive Bayes learning node class.
-
-    Parameters
-    ----------
-    stats
-        Initial class observations.
-    depth
-        The depth of the node.
-    splitter
-        The numeric attribute observer algorithm used to monitor target statistics
-        and perform split attempts.
-        Number of attributes per subset for each node split.
-    max_features
-        Number of attributes per subset for each node split.
-    seed
-        If int, seed is the seed used by the random number generator;
-        If RandomState instance, seed is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
-    kwargs
-        Other parameters passed to the learning node.
-    """
-
-    def __init__(self, stats, depth, splitter, max_features, seed, **kwargs):
-        super().__init__(stats, depth, splitter, max_features, seed, **kwargs)
-
-
-class RandomLeafNaiveBayesAdaptive(BaseRandomLeaf, LeafNaiveBayesAdaptive):
-    """ARF Naive Bayes Adaptive learning node class.
-
-    Parameters
-    ----------
-    stats
-        Initial class observations.
-    depth
-        The depth of the node.
-    splitter
-        The numeric attribute observer algorithm used to monitor target statistics
-        and perform split attempts.
-    max_features
-        Number of attributes per subset for each node split.
-    seed
-        If int, seed is the seed used by the random number generator;
-        If RandomState instance, seed is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
-    kwargs
-        Other parameters passed to the learning node.
-    """
-
-    def __init__(self, stats, depth, splitter, max_features, seed, **kwargs):
-        super().__init__(stats, depth, splitter, max_features, seed, **kwargs)
+import typing
+
+from river.utils.skmultiflow_utils import check_random_state
+
+from .htc_nodes import LeafMajorityClass, LeafNaiveBayes, LeafNaiveBayesAdaptive
+from .leaf import HTLeaf
+
+
+class BaseRandomLeaf(HTLeaf):
+    """The Random Learning Node changes the way in which the attribute observers
+    are updated (by using subsets of features).
+
+    Parameters
+    ----------
+    stats
+        Initial class observations.
+    depth
+        The depth of the node.
+    splitter
+        The numeric attribute observer algorithm used to monitor target statistics
+        and perform split attempts.
+    max_features
+        Number of attributes per subset for each node split.
+    seed
+        If int, seed is the seed used by the random number generator;
+        If RandomState instance, seed is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
+    kwargs
+        Other parameters passed to the learning node.
+    """
+
+    def __init__(self, stats, depth, splitter, max_features, seed, **kwargs):
+        super().__init__(stats, depth, splitter, **kwargs)
+        self.max_features = max_features
+        self.seed = seed
+        self._rng = check_random_state(self.seed)
+        self.feature_indices = []
+
+    def _iter_features(self, x) -> typing.Iterable:
+        # Only a random subset of the features is monitored
+        if len(self.feature_indices) == 0:
+            self.feature_indices = self._sample_features(x, self.max_features)
+
+        for att_id in self.feature_indices:
+            # First check if the feature is available
+            if att_id in x:
+                yield att_id, x[att_id]
+
+    def _sample_features(self, x, max_features):
+        selected = self._rng.choice(len(x), size=max_features, replace=False)
+        features = list(x.keys())
+        return [features[s] for s in selected]
+
+
+class RandomLeafMajorityClass(BaseRandomLeaf, LeafMajorityClass):
+    """ARF learning node that always predicts the majority class.
+
+    Parameters
+    ----------
+    stats
+        Initial class observations.
+    depth
+        The depth of the node.
+    splitter
+        The numeric attribute observer algorithm used to monitor target statistics
+        and perform split attempts.
+    max_features
+        Number of attributes per subset for each node split.
+    seed
+        If int, seed is the seed used by the random number generator;
+        If RandomState instance, seed is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
+    kwargs
+        Other parameters passed to the learning node.
+
+    """
+
+    def __init__(self, stats, depth, splitter, max_features, seed, **kwargs):
+        super().__init__(stats, depth, splitter, max_features, seed, **kwargs)
+
+
+class RandomLeafNaiveBayes(BaseRandomLeaf, LeafNaiveBayes):
+    """ARF Naive Bayes learning node class.
+
+    Parameters
+    ----------
+    stats
+        Initial class observations.
+    depth
+        The depth of the node.
+    splitter
+        The numeric attribute observer algorithm used to monitor target statistics
+        and perform split attempts.
+        Number of attributes per subset for each node split.
+    max_features
+        Number of attributes per subset for each node split.
+    seed
+        If int, seed is the seed used by the random number generator;
+        If RandomState instance, seed is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
+    kwargs
+        Other parameters passed to the learning node.
+    """
+
+    def __init__(self, stats, depth, splitter, max_features, seed, **kwargs):
+        super().__init__(stats, depth, splitter, max_features, seed, **kwargs)
+
+
+class RandomLeafNaiveBayesAdaptive(BaseRandomLeaf, LeafNaiveBayesAdaptive):
+    """ARF Naive Bayes Adaptive learning node class.
+
+    Parameters
+    ----------
+    stats
+        Initial class observations.
+    depth
+        The depth of the node.
+    splitter
+        The numeric attribute observer algorithm used to monitor target statistics
+        and perform split attempts.
+    max_features
+        Number of attributes per subset for each node split.
+    seed
+        If int, seed is the seed used by the random number generator;
+        If RandomState instance, seed is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
+    kwargs
+        Other parameters passed to the learning node.
+    """
+
+    def __init__(self, stats, depth, splitter, max_features, seed, **kwargs):
+        super().__init__(stats, depth, splitter, max_features, seed, **kwargs)
```

### Comparing `river-0.8.0/river/tree/nodes/arf_htr_nodes.py` & `river-0.9.0/river/tree/nodes/arf_htr_nodes.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,103 +1,103 @@
-from .arf_htc_nodes import BaseRandomLeaf
-from .htr_nodes import LeafAdaptive, LeafMean, LeafModel
-
-
-class RandomLeafMean(BaseRandomLeaf, LeafMean):
-    """ARF learning Node for regression tasks that always use the average target
-    value as response.
-
-    Parameters
-    ----------
-    stats
-        In regression tasks the node keeps an instance of `river.stats.Var` to estimate
-        the target's statistics.
-    depth
-        The depth of the node.
-    splitter
-        The numeric attribute observer algorithm used to monitor target statistics
-        and perform split attempts.
-    max_features
-        Number of attributes per subset for each node split.
-    seed
-        If int, seed is the seed used by the random number generator;
-        If RandomState instance, seed is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
-    kwargs
-        Other parameters passed to the learning node.
-    """
-
-    def __init__(self, stats, depth, splitter, max_features, seed, **kwargs):
-        super().__init__(stats, depth, splitter, max_features, seed, **kwargs)
-
-
-class RandomLeafModel(BaseRandomLeaf, LeafModel):
-    """ARF learning Node for regression tasks that always use a learning model to provide
-    responses.
-
-    Parameters
-    ----------
-    stats
-        In regression tasks the node keeps an instance of `river.stats.Var` to estimate
-        the target's statistics.
-    depth
-        The depth of the node.
-    splitter
-        The numeric attribute observer algorithm used to monitor target statistics
-        and perform split attempts.
-    max_features
-        Number of attributes per subset for each node split.
-    seed
-        If int, seed is the seed used by the random number generator;
-        If RandomState instance, seed is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
-    leaf_model
-        A `base.Regressor` instance used to learn from instances and provide
-        responses.
-    kwargs
-        Other parameters passed to the learning node.
-    """
-
-    def __init__(
-        self, stats, depth, splitter, max_features, seed, leaf_model, **kwargs
-    ):
-        super().__init__(
-            stats, depth, splitter, max_features, seed, leaf_model=leaf_model, **kwargs
-        )
-
-
-class RandomLeafAdaptive(BaseRandomLeaf, LeafAdaptive):
-    """ARF learning node for regression tasks that dynamically selects between the
-    target mean and the output of a learning model to provide responses.
-
-    Parameters
-    ----------
-    stats
-        In regression tasks the node keeps an instance of `river.stats.Var` to estimate
-        the target's statistics.
-    depth
-        The depth of the node.
-    splitter
-        The numeric attribute observer algorithm used to monitor target statistics
-        and perform split attempts.
-    max_features
-        Number of attributes per subset for each node split.
-    seed
-        If int, seed is the seed used by the random number generator;
-        If RandomState instance, seed is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
-    leaf_model
-        A `base.Regressor` instance used to learn from instances and provide
-        responses.
-    kwargs
-        Other parameters passed to the learning node.
-    """
-
-    def __init__(
-        self, stats, depth, splitter, max_features, seed, leaf_model, **kwargs
-    ):
-        super().__init__(
-            stats, depth, splitter, max_features, seed, leaf_model=leaf_model, **kwargs
-        )
+from .arf_htc_nodes import BaseRandomLeaf
+from .htr_nodes import LeafAdaptive, LeafMean, LeafModel
+
+
+class RandomLeafMean(BaseRandomLeaf, LeafMean):
+    """ARF learning Node for regression tasks that always use the average target
+    value as response.
+
+    Parameters
+    ----------
+    stats
+        In regression tasks the node keeps an instance of `river.stats.Var` to estimate
+        the target's statistics.
+    depth
+        The depth of the node.
+    splitter
+        The numeric attribute observer algorithm used to monitor target statistics
+        and perform split attempts.
+    max_features
+        Number of attributes per subset for each node split.
+    seed
+        If int, seed is the seed used by the random number generator;
+        If RandomState instance, seed is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
+    kwargs
+        Other parameters passed to the learning node.
+    """
+
+    def __init__(self, stats, depth, splitter, max_features, seed, **kwargs):
+        super().__init__(stats, depth, splitter, max_features, seed, **kwargs)
+
+
+class RandomLeafModel(BaseRandomLeaf, LeafModel):
+    """ARF learning Node for regression tasks that always use a learning model to provide
+    responses.
+
+    Parameters
+    ----------
+    stats
+        In regression tasks the node keeps an instance of `river.stats.Var` to estimate
+        the target's statistics.
+    depth
+        The depth of the node.
+    splitter
+        The numeric attribute observer algorithm used to monitor target statistics
+        and perform split attempts.
+    max_features
+        Number of attributes per subset for each node split.
+    seed
+        If int, seed is the seed used by the random number generator;
+        If RandomState instance, seed is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
+    leaf_model
+        A `base.Regressor` instance used to learn from instances and provide
+        responses.
+    kwargs
+        Other parameters passed to the learning node.
+    """
+
+    def __init__(
+        self, stats, depth, splitter, max_features, seed, leaf_model, **kwargs
+    ):
+        super().__init__(
+            stats, depth, splitter, max_features, seed, leaf_model=leaf_model, **kwargs
+        )
+
+
+class RandomLeafAdaptive(BaseRandomLeaf, LeafAdaptive):
+    """ARF learning node for regression tasks that dynamically selects between the
+    target mean and the output of a learning model to provide responses.
+
+    Parameters
+    ----------
+    stats
+        In regression tasks the node keeps an instance of `river.stats.Var` to estimate
+        the target's statistics.
+    depth
+        The depth of the node.
+    splitter
+        The numeric attribute observer algorithm used to monitor target statistics
+        and perform split attempts.
+    max_features
+        Number of attributes per subset for each node split.
+    seed
+        If int, seed is the seed used by the random number generator;
+        If RandomState instance, seed is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`.
+    leaf_model
+        A `base.Regressor` instance used to learn from instances and provide
+        responses.
+    kwargs
+        Other parameters passed to the learning node.
+    """
+
+    def __init__(
+        self, stats, depth, splitter, max_features, seed, leaf_model, **kwargs
+    ):
+        super().__init__(
+            stats, depth, splitter, max_features, seed, leaf_model=leaf_model, **kwargs
+        )
```

### Comparing `river-0.8.0/river/tree/nodes/branch.py` & `river-0.9.0/river/tree/nodes/branch.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,210 +1,210 @@
-import abc
-import math
-
-from ..base import Branch
-
-
-class DTBranch(Branch):
-    def __init__(self, stats, *children, **attributes):
-        super().__init__(*children)
-        # The number of branches can increase in runtime
-        self.children = list(self.children)
-
-        self.stats = stats
-        self.__dict__.update(attributes)
-
-    @property
-    def total_weight(self):
-        return sum(child.total_weight for child in filter(None, self.children))
-
-    @abc.abstractmethod
-    def branch_no(self, x):
-        pass
-
-    def next(self, x):
-        return self.children[self.branch_no(x)]
-
-    @abc.abstractmethod
-    def max_branches(self):
-        pass
-
-    @abc.abstractmethod
-    def repr_branch(self, index: int, shorten=False):
-        """Return a string representation of the test performed in the branch at `index`.
-
-        Parameters
-        ----------
-        index
-            The branch index.
-        shorten
-            If True, return a shortened version of the performed test.
-        """
-        pass
-
-
-class NumericBinaryBranch(DTBranch):
-    def __init__(self, stats, feature, threshold, depth, left, right, **attributes):
-        super().__init__(stats, left, right, **attributes)
-        self.feature = feature
-        self.threshold = threshold
-        self.depth = depth
-
-    def branch_no(self, x):
-        if x[self.feature] <= self.threshold:
-            return 0
-        return 1
-
-    def max_branches(self):
-        return 2
-
-    def most_common_path(self):
-        left, right = self.children
-
-        if left.total_weight < right.total_weight:
-            return 1, right
-        return 0, left
-
-    def repr_branch(self, index: int, shorten=False):
-        if shorten:
-            if index == 0:
-                return f"≤ {round(self.threshold, 4)}"
-            return f"> {round(self.threshold, 4)}"
-        else:
-            if index == 0:
-                return f"{self.feature} ≤ {self.threshold}"
-            return f"{self.feature} > {self.threshold}"
-
-    @property
-    def repr_split(self):
-        return f"{self.feature} ≤ {self.threshold}"
-
-
-class NominalBinaryBranch(DTBranch):
-    def __init__(self, stats, feature, value, depth, left, right, **attributes):
-        super().__init__(stats, left, right, **attributes)
-        self.feature = feature
-        self.value = value
-        self.depth = depth
-
-    def branch_no(self, x):
-        if x[self.feature] == self.value:
-            return 0
-        return 1
-
-    def max_branches(self):
-        return 2
-
-    def most_common_path(self):
-        left, right = self.children
-
-        if left.total_weight < right.total_weight:
-            return 1, right
-        return 0, left
-
-    def repr_branch(self, index: int, shorten=False):
-        if shorten:
-            if index == 0:
-                return str(self.value)
-            else:
-                return f"not {self.value}"
-        else:
-            if index == 0:
-                return f"{self.feature} = {self.value}"
-            return f"{self.feature} ≠ {self.value}"
-
-    @property
-    def repr_split(self):
-        return f"{self.feature} {{=, ≠}} {self.value}"
-
-
-class NumericMultiwayBranch(DTBranch):
-    def __init__(
-        self, stats, feature, radius_and_slots, depth, *children, **attributes
-    ):
-        super().__init__(stats, *children, **attributes)
-
-        self.feature = feature
-        self.radius, slot_ids = radius_and_slots
-        self.depth = depth
-
-        # Controls the branch mapping
-        self._mapping = {s: i for i, s in enumerate(slot_ids)}
-        self._r_mapping = {i: s for s, i in self._mapping.items()}
-
-    def branch_no(self, x):
-        slot = math.floor(x[self.feature] / self.radius)
-
-        return self._mapping[slot]
-
-    def max_branches(self):
-        return -1
-
-    def most_common_path(self):
-        # Get the most traversed path
-        pos = max(
-            range(len(self.children)), key=lambda i: self.children[i].total_weight
-        )
-
-        return pos, self.children[pos]
-
-    def add_child(self, feature_val, child):
-        slot = math.floor(feature_val / self.radius)
-
-        self._mapping[slot] = len(self.children)
-        self._r_mapping[len(self.children)] = slot
-        self.children.append(child)
-
-    def repr_branch(self, index: int, shorten=False):
-        lower = self._r_mapping[index] * self.radius
-        upper = lower + self.radius
-
-        if shorten:
-            return f"[{round(lower, 4)}, {round(upper, 4)})"
-
-        return f"{lower} ≤ {self.feature} < {upper}"
-
-    @property
-    def repr_split(self):
-        return f"{self.feature} ÷ {self.radius}"
-
-
-class NominalMultiwayBranch(DTBranch):
-    def __init__(self, stats, feature, feature_values, depth, *children, **attributes):
-        super().__init__(stats, *children, **attributes)
-        self.feature = feature
-        self.depth = depth
-
-        # Controls the branch mapping
-        self._mapping = {feat_v: i for i, feat_v in enumerate(feature_values)}
-        self._r_mapping = {i: feat_v for feat_v, i in self._mapping.items()}
-
-    def branch_no(self, x):
-        return self._mapping[x[self.feature]]
-
-    def max_branches(self):
-        return -1
-
-    def most_common_path(self):
-        # Get the most traversed path
-        pos = max(
-            range(len(self.children)), key=lambda i: self.children[i].total_weight
-        )
-
-        return pos, self.children[pos]
-
-    def add_child(self, feature_val, child):
-        self._mapping[feature_val] = len(self.children)
-        self._r_mapping[len(self.children)] = feature_val
-        self.children.append(child)
-
-    def repr_branch(self, index: int, shorten=False):
-        feat_val = self._r_mapping[index]
-
-        if shorten:
-            return str(feat_val)
-
-        return f"{self.feature} = {feat_val}"
-
-    @property
-    def repr_split(self):
-        return f"{self.feature} in {set(self._mapping.keys())}"
+import abc
+import math
+
+from ..base import Branch
+
+
+class DTBranch(Branch):
+    def __init__(self, stats, *children, **attributes):
+        super().__init__(*children)
+        # The number of branches can increase in runtime
+        self.children = list(self.children)
+
+        self.stats = stats
+        self.__dict__.update(attributes)
+
+    @property
+    def total_weight(self):
+        return sum(child.total_weight for child in filter(None, self.children))
+
+    @abc.abstractmethod
+    def branch_no(self, x):
+        pass
+
+    def next(self, x):
+        return self.children[self.branch_no(x)]
+
+    @abc.abstractmethod
+    def max_branches(self):
+        pass
+
+    @abc.abstractmethod
+    def repr_branch(self, index: int, shorten=False):
+        """Return a string representation of the test performed in the branch at `index`.
+
+        Parameters
+        ----------
+        index
+            The branch index.
+        shorten
+            If True, return a shortened version of the performed test.
+        """
+        pass
+
+
+class NumericBinaryBranch(DTBranch):
+    def __init__(self, stats, feature, threshold, depth, left, right, **attributes):
+        super().__init__(stats, left, right, **attributes)
+        self.feature = feature
+        self.threshold = threshold
+        self.depth = depth
+
+    def branch_no(self, x):
+        if x[self.feature] <= self.threshold:
+            return 0
+        return 1
+
+    def max_branches(self):
+        return 2
+
+    def most_common_path(self):
+        left, right = self.children
+
+        if left.total_weight < right.total_weight:
+            return 1, right
+        return 0, left
+
+    def repr_branch(self, index: int, shorten=False):
+        if shorten:
+            if index == 0:
+                return f"≤ {round(self.threshold, 4)}"
+            return f"> {round(self.threshold, 4)}"
+        else:
+            if index == 0:
+                return f"{self.feature} ≤ {self.threshold}"
+            return f"{self.feature} > {self.threshold}"
+
+    @property
+    def repr_split(self):
+        return f"{self.feature} ≤ {self.threshold}"
+
+
+class NominalBinaryBranch(DTBranch):
+    def __init__(self, stats, feature, value, depth, left, right, **attributes):
+        super().__init__(stats, left, right, **attributes)
+        self.feature = feature
+        self.value = value
+        self.depth = depth
+
+    def branch_no(self, x):
+        if x[self.feature] == self.value:
+            return 0
+        return 1
+
+    def max_branches(self):
+        return 2
+
+    def most_common_path(self):
+        left, right = self.children
+
+        if left.total_weight < right.total_weight:
+            return 1, right
+        return 0, left
+
+    def repr_branch(self, index: int, shorten=False):
+        if shorten:
+            if index == 0:
+                return str(self.value)
+            else:
+                return f"not {self.value}"
+        else:
+            if index == 0:
+                return f"{self.feature} = {self.value}"
+            return f"{self.feature} ≠ {self.value}"
+
+    @property
+    def repr_split(self):
+        return f"{self.feature} {{=, ≠}} {self.value}"
+
+
+class NumericMultiwayBranch(DTBranch):
+    def __init__(
+        self, stats, feature, radius_and_slots, depth, *children, **attributes
+    ):
+        super().__init__(stats, *children, **attributes)
+
+        self.feature = feature
+        self.radius, slot_ids = radius_and_slots
+        self.depth = depth
+
+        # Controls the branch mapping
+        self._mapping = {s: i for i, s in enumerate(slot_ids)}
+        self._r_mapping = {i: s for s, i in self._mapping.items()}
+
+    def branch_no(self, x):
+        slot = math.floor(x[self.feature] / self.radius)
+
+        return self._mapping[slot]
+
+    def max_branches(self):
+        return -1
+
+    def most_common_path(self):
+        # Get the most traversed path
+        pos = max(
+            range(len(self.children)), key=lambda i: self.children[i].total_weight
+        )
+
+        return pos, self.children[pos]
+
+    def add_child(self, feature_val, child):
+        slot = math.floor(feature_val / self.radius)
+
+        self._mapping[slot] = len(self.children)
+        self._r_mapping[len(self.children)] = slot
+        self.children.append(child)
+
+    def repr_branch(self, index: int, shorten=False):
+        lower = self._r_mapping[index] * self.radius
+        upper = lower + self.radius
+
+        if shorten:
+            return f"[{round(lower, 4)}, {round(upper, 4)})"
+
+        return f"{lower} ≤ {self.feature} < {upper}"
+
+    @property
+    def repr_split(self):
+        return f"{self.feature} ÷ {self.radius}"
+
+
+class NominalMultiwayBranch(DTBranch):
+    def __init__(self, stats, feature, feature_values, depth, *children, **attributes):
+        super().__init__(stats, *children, **attributes)
+        self.feature = feature
+        self.depth = depth
+
+        # Controls the branch mapping
+        self._mapping = {feat_v: i for i, feat_v in enumerate(feature_values)}
+        self._r_mapping = {i: feat_v for feat_v, i in self._mapping.items()}
+
+    def branch_no(self, x):
+        return self._mapping[x[self.feature]]
+
+    def max_branches(self):
+        return -1
+
+    def most_common_path(self):
+        # Get the most traversed path
+        pos = max(
+            range(len(self.children)), key=lambda i: self.children[i].total_weight
+        )
+
+        return pos, self.children[pos]
+
+    def add_child(self, feature_val, child):
+        self._mapping[feature_val] = len(self.children)
+        self._r_mapping[len(self.children)] = feature_val
+        self.children.append(child)
+
+    def repr_branch(self, index: int, shorten=False):
+        feat_val = self._r_mapping[index]
+
+        if shorten:
+            return str(feat_val)
+
+        return f"{self.feature} = {feat_val}"
+
+    @property
+    def repr_split(self):
+        return f"{self.feature} in {set(self._mapping.keys())}"
```

### Comparing `river-0.8.0/river/tree/nodes/efdtc_nodes.py` & `river-0.9.0/river/tree/nodes/efdtc_nodes.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,313 +1,313 @@
-import copy
-import numbers
-
-from river.utils.skmultiflow_utils import normalize_values_in_dict
-
-from ..splitter.nominal_splitter_classif import NominalSplitterClassif
-from .branch import (
-    DTBranch,
-    NominalBinaryBranch,
-    NominalMultiwayBranch,
-    NumericBinaryBranch,
-    NumericMultiwayBranch,
-)
-from .htc_nodes import LeafMajorityClass, LeafNaiveBayes, LeafNaiveBayesAdaptive
-from .leaf import HTLeaf
-
-
-class BaseEFDTLeaf(HTLeaf):
-    """Helper class that define basic operations of EFDT's nodes.
-
-    It inherits from `LearningNode` and provides extra functionalities, while changing
-    the splitting behavior of its parent class. This is an abstract class, since it does
-    not implement all the inherited abstract methods from its parent class. BaseEDFTNode
-    is designed to work with other learning/split nodes.
-
-    Parameters
-    ----------
-    stats
-        Class observations.
-    depth
-        The depth of the node in the tree.
-    splitter
-        The numeric attribute observer algorithm used to monitor target statistics
-        and perform split attempts.
-    kwargs
-        Other parameters passed to the node.
-    """
-
-    def __init__(self, stats, depth, splitter, **kwargs):
-        super().__init__(stats=stats, depth=depth, splitter=splitter, **kwargs)
-
-    def best_split_suggestions(self, criterion, tree):
-        """Find possible split candidates without taking into account the
-        null split.
-
-        Parameters
-        ----------
-        criterion
-            The splitting criterion to be used.
-        tree
-            The EFDT which the node belongs to.
-
-        Returns
-        -------
-            The list of split candidates.
-        """
-        best_suggestions = []
-        pre_split_dist = self.stats
-
-        for idx, splitter in self.splitters.items():
-            best_suggestion = splitter.best_evaluated_split_suggestion(
-                criterion, pre_split_dist, idx, tree.binary_split
-            )
-            best_suggestions.append(best_suggestion)
-
-        return best_suggestions
-
-
-class BaseEFDTBranch(DTBranch):
-    """Node that splits the data in a EFDT.
-
-    This node is an exception among the tree's nodes. EFDTSplitNode is both a split node
-    and a learning node. EFDT updates all of the nodes in the path from the root to a leaf
-    when a new instance arrives. Besides that, it also revisit split decisions from time
-    to time. For that reason, this decision node also needs to be able to learn from new
-    instances.
-
-    Parameters
-    ----------
-    stats
-        Class observations
-    children
-        The children nodes.
-    attributes
-        Other parameters passed to the learning nodes.
-    """
-
-    def __init__(self, stats, *children, splitter, splitters, **attributes):
-        super().__init__(stats, *children, **attributes)
-
-        self.splitter = splitter
-        self.splitters = splitters
-
-        self._disabled_attrs = set()
-        self._last_split_reevaluation_at = 0
-
-    @property
-    def total_weight(self) -> float:
-        return sum(self.stats.values()) if self.stats else 0
-
-    @staticmethod
-    def new_nominal_splitter():
-        return NominalSplitterClassif()
-
-    def update_stats(self, y, sample_weight):
-        try:
-            self.stats[y] += sample_weight
-        except KeyError:
-            self.stats[y] = sample_weight
-
-    def update_splitters(self, x, y, sample_weight, nominal_attributes):
-        for att_id, att_val in x.items():
-            if att_id in self._disabled_attrs:
-                continue
-
-            try:
-                splitter = self.splitters[att_id]
-            except KeyError:
-                if (
-                    nominal_attributes is not None and att_id in nominal_attributes
-                ) or not isinstance(att_val, numbers.Number):
-                    splitter = self.new_nominal_splitter()
-                else:
-                    splitter = copy.deepcopy(self.splitter)
-
-                self.splitters[att_id] = splitter
-            splitter.update(att_val, y, sample_weight)
-
-    def learn_one(self, x, y, *, sample_weight=1.0, tree=None):
-        """Update branch with the provided sample.
-
-        Parameters
-        ----------
-        x
-            Sample attributes for updating the node.
-        y
-            Target value.
-        sample_weight
-            Sample weight.
-        tree
-            Tree to update.
-        """
-        self.update_stats(y, sample_weight)
-        self.update_splitters(x, y, sample_weight, tree.nominal_attributes)
-
-    def prediction(self, x, *, tree=None):
-        return normalize_values_in_dict(self.stats, inplace=False)
-
-    @staticmethod
-    def find_attribute(id_att, split_suggestions):
-        """Find the attribute given the id.
-
-        Parameters
-        ----------
-        id_att
-            Id of attribute to find.
-        split_suggestions
-            Possible split candidates.
-        Returns
-        -------
-            Found attribute.
-        """
-        x_current = None
-        for suggestion in split_suggestions:
-            if suggestion.feature == id_att:
-                x_current = suggestion
-                break
-
-        return x_current
-
-    @property
-    def last_split_reevaluation_at(self) -> float:
-        """Get the weight seen at the last split reevaluation.
-
-        Returns
-        -------
-            Total weight seen at last split reevaluation.
-        """
-        return self._last_split_reevaluation_at
-
-    @last_split_reevaluation_at.setter
-    def last_split_reevaluation_at(self, value: float):
-        """Update weight seen at the last split in the reevaluation. """
-        self._last_split_reevaluation_at = value
-
-    def observed_class_distribution_is_pure(self):
-        """Check if observed class distribution is pure, i.e. if all samples
-        belong to the same class.
-
-        Returns
-        -------
-            True if the observed number of classes is smaller than 2, False otherwise.
-        """
-        count = 0
-        for weight in self.stats.values():
-            if weight != 0:
-                count += 1
-                if count == 2:  # No need to count beyond this point
-                    break
-        return count < 2
-
-    def best_split_suggestions(self, criterion, tree):
-        """Find possible split candidates without taking into account the
-        null split.
-
-        Parameters
-        ----------
-        criterion
-            The splitting criterion to be used.
-        tree
-            The EFDT which the node belongs to.
-
-        Returns
-        -------
-            The list of split candidates.
-        """
-        best_suggestions = []
-        pre_split_dist = self.stats
-
-        for idx, splitter in self.splitters.items():
-            best_suggestion = splitter.best_evaluated_split_suggestion(
-                criterion, pre_split_dist, idx, tree.binary_split
-            )
-            if best_suggestion is not None:
-                best_suggestions.append(best_suggestion)
-
-        return best_suggestions
-
-
-class EFDTLeafMajorityClass(BaseEFDTLeaf, LeafMajorityClass):
-    """Active Learning node for the Hoeffding Anytime Tree.
-
-    Parameters
-    ----------
-    stats
-        Initial class observations.
-    depth
-        The depth of the node.
-    splitter
-        The numeric attribute observer algorithm used to monitor target statistics
-        and perform split attempts.
-    kwargs
-        Other parameters passed to the learning nodes.
-    """
-
-    def __init__(self, stats, depth, splitter, **kwargs):
-        super().__init__(stats, depth, splitter, **kwargs)
-
-
-class EFDTLeafNaiveBayes(BaseEFDTLeaf, LeafNaiveBayes):
-    """Learning node  for the Hoeffding Anytime Tree that uses Naive Bayes
-    models.
-
-    Parameters
-    ----------
-    stats
-        Initial class observations
-    depth
-        The depth of the node.
-    splitter
-        The numeric attribute observer algorithm used to monitor target statistics
-        and perform split attempts.
-    kwargs
-        Other parameters passed to the learning nodes.
-    """
-
-    def __init__(self, stats, depth, splitter, **kwargs):
-        super().__init__(stats, depth, splitter, **kwargs)
-
-
-class EFDTLeafNaiveBayesAdaptive(BaseEFDTLeaf, LeafNaiveBayesAdaptive):
-    """Learning node for the Hoeffding Anytime Tree that uses Adaptive Naive
-    Bayes models.
-
-    Parameters
-    ----------
-    stats
-        Initial class observations.
-    depth
-        The depth of the node.
-    splitter
-        The numeric attribute observer algorithm used to monitor target statistics
-        and perform split attempts.
-    kwargs
-        Other parameters passed to the learning nodes.
-    """
-
-    def __init__(self, stats, depth, splitter, **kwargs):
-        super().__init__(stats, depth, splitter, **kwargs)
-
-
-class EFDTNominalBinaryBranch(BaseEFDTBranch, NominalBinaryBranch):
-    def __init__(self, stats, feature, value, depth, left, right, **attributes):
-        super().__init__(stats, feature, value, depth, left, right, **attributes)
-
-
-class EFDTNominalMultiwayBranch(BaseEFDTBranch, NominalMultiwayBranch):
-    def __init__(self, stats, feature, feature_values, depth, *children, **attributes):
-        super().__init__(stats, feature, feature_values, depth, *children, **attributes)
-
-
-class EFDTNumericBinaryBranch(BaseEFDTBranch, NumericBinaryBranch):
-    def __init__(self, stats, feature, threshold, depth, left, right, **attributes):
-        super().__init__(stats, feature, threshold, depth, left, right, **attributes)
-
-
-class EFDTNumericMultiwayBranch(BaseEFDTBranch, NumericMultiwayBranch):
-    def __init__(
-        self, stats, feature, radius_and_slots, depth, *children, **attributes
-    ):
-        super().__init__(
-            stats, feature, radius_and_slots, depth, *children, **attributes
-        )
+import copy
+import numbers
+
+from river.utils.skmultiflow_utils import normalize_values_in_dict
+
+from ..splitter.nominal_splitter_classif import NominalSplitterClassif
+from .branch import (
+    DTBranch,
+    NominalBinaryBranch,
+    NominalMultiwayBranch,
+    NumericBinaryBranch,
+    NumericMultiwayBranch,
+)
+from .htc_nodes import LeafMajorityClass, LeafNaiveBayes, LeafNaiveBayesAdaptive
+from .leaf import HTLeaf
+
+
+class BaseEFDTLeaf(HTLeaf):
+    """Helper class that define basic operations of EFDT's nodes.
+
+    It inherits from `LearningNode` and provides extra functionalities, while changing
+    the splitting behavior of its parent class. This is an abstract class, since it does
+    not implement all the inherited abstract methods from its parent class. BaseEDFTNode
+    is designed to work with other learning/split nodes.
+
+    Parameters
+    ----------
+    stats
+        Class observations.
+    depth
+        The depth of the node in the tree.
+    splitter
+        The numeric attribute observer algorithm used to monitor target statistics
+        and perform split attempts.
+    kwargs
+        Other parameters passed to the node.
+    """
+
+    def __init__(self, stats, depth, splitter, **kwargs):
+        super().__init__(stats=stats, depth=depth, splitter=splitter, **kwargs)
+
+    def best_split_suggestions(self, criterion, tree):
+        """Find possible split candidates without taking into account the
+        null split.
+
+        Parameters
+        ----------
+        criterion
+            The splitting criterion to be used.
+        tree
+            The EFDT which the node belongs to.
+
+        Returns
+        -------
+            The list of split candidates.
+        """
+        best_suggestions = []
+        pre_split_dist = self.stats
+
+        for idx, splitter in self.splitters.items():
+            best_suggestion = splitter.best_evaluated_split_suggestion(
+                criterion, pre_split_dist, idx, tree.binary_split
+            )
+            best_suggestions.append(best_suggestion)
+
+        return best_suggestions
+
+
+class BaseEFDTBranch(DTBranch):
+    """Node that splits the data in a EFDT.
+
+    This node is an exception among the tree's nodes. EFDTSplitNode is both a split node
+    and a learning node. EFDT updates all of the nodes in the path from the root to a leaf
+    when a new instance arrives. Besides that, it also revisit split decisions from time
+    to time. For that reason, this decision node also needs to be able to learn from new
+    instances.
+
+    Parameters
+    ----------
+    stats
+        Class observations
+    children
+        The children nodes.
+    attributes
+        Other parameters passed to the learning nodes.
+    """
+
+    def __init__(self, stats, *children, splitter, splitters, **attributes):
+        super().__init__(stats, *children, **attributes)
+
+        self.splitter = splitter
+        self.splitters = splitters
+
+        self._disabled_attrs = set()
+        self._last_split_reevaluation_at = 0
+
+    @property
+    def total_weight(self) -> float:
+        return sum(self.stats.values()) if self.stats else 0
+
+    @staticmethod
+    def new_nominal_splitter():
+        return NominalSplitterClassif()
+
+    def update_stats(self, y, sample_weight):
+        try:
+            self.stats[y] += sample_weight
+        except KeyError:
+            self.stats[y] = sample_weight
+
+    def update_splitters(self, x, y, sample_weight, nominal_attributes):
+        for att_id, att_val in x.items():
+            if att_id in self._disabled_attrs:
+                continue
+
+            try:
+                splitter = self.splitters[att_id]
+            except KeyError:
+                if (
+                    nominal_attributes is not None and att_id in nominal_attributes
+                ) or not isinstance(att_val, numbers.Number):
+                    splitter = self.new_nominal_splitter()
+                else:
+                    splitter = copy.deepcopy(self.splitter)
+
+                self.splitters[att_id] = splitter
+            splitter.update(att_val, y, sample_weight)
+
+    def learn_one(self, x, y, *, sample_weight=1.0, tree=None):
+        """Update branch with the provided sample.
+
+        Parameters
+        ----------
+        x
+            Sample attributes for updating the node.
+        y
+            Target value.
+        sample_weight
+            Sample weight.
+        tree
+            Tree to update.
+        """
+        self.update_stats(y, sample_weight)
+        self.update_splitters(x, y, sample_weight, tree.nominal_attributes)
+
+    def prediction(self, x, *, tree=None):
+        return normalize_values_in_dict(self.stats, inplace=False)
+
+    @staticmethod
+    def find_attribute(id_att, split_suggestions):
+        """Find the attribute given the id.
+
+        Parameters
+        ----------
+        id_att
+            Id of attribute to find.
+        split_suggestions
+            Possible split candidates.
+        Returns
+        -------
+            Found attribute.
+        """
+        x_current = None
+        for suggestion in split_suggestions:
+            if suggestion.feature == id_att:
+                x_current = suggestion
+                break
+
+        return x_current
+
+    @property
+    def last_split_reevaluation_at(self) -> float:
+        """Get the weight seen at the last split reevaluation.
+
+        Returns
+        -------
+            Total weight seen at last split reevaluation.
+        """
+        return self._last_split_reevaluation_at
+
+    @last_split_reevaluation_at.setter
+    def last_split_reevaluation_at(self, value: float):
+        """Update weight seen at the last split in the reevaluation. """
+        self._last_split_reevaluation_at = value
+
+    def observed_class_distribution_is_pure(self):
+        """Check if observed class distribution is pure, i.e. if all samples
+        belong to the same class.
+
+        Returns
+        -------
+            True if the observed number of classes is smaller than 2, False otherwise.
+        """
+        count = 0
+        for weight in self.stats.values():
+            if weight != 0:
+                count += 1
+                if count == 2:  # No need to count beyond this point
+                    break
+        return count < 2
+
+    def best_split_suggestions(self, criterion, tree):
+        """Find possible split candidates without taking into account the
+        null split.
+
+        Parameters
+        ----------
+        criterion
+            The splitting criterion to be used.
+        tree
+            The EFDT which the node belongs to.
+
+        Returns
+        -------
+            The list of split candidates.
+        """
+        best_suggestions = []
+        pre_split_dist = self.stats
+
+        for idx, splitter in self.splitters.items():
+            best_suggestion = splitter.best_evaluated_split_suggestion(
+                criterion, pre_split_dist, idx, tree.binary_split
+            )
+            if best_suggestion is not None:
+                best_suggestions.append(best_suggestion)
+
+        return best_suggestions
+
+
+class EFDTLeafMajorityClass(BaseEFDTLeaf, LeafMajorityClass):
+    """Active Learning node for the Hoeffding Anytime Tree.
+
+    Parameters
+    ----------
+    stats
+        Initial class observations.
+    depth
+        The depth of the node.
+    splitter
+        The numeric attribute observer algorithm used to monitor target statistics
+        and perform split attempts.
+    kwargs
+        Other parameters passed to the learning nodes.
+    """
+
+    def __init__(self, stats, depth, splitter, **kwargs):
+        super().__init__(stats, depth, splitter, **kwargs)
+
+
+class EFDTLeafNaiveBayes(BaseEFDTLeaf, LeafNaiveBayes):
+    """Learning node  for the Hoeffding Anytime Tree that uses Naive Bayes
+    models.
+
+    Parameters
+    ----------
+    stats
+        Initial class observations
+    depth
+        The depth of the node.
+    splitter
+        The numeric attribute observer algorithm used to monitor target statistics
+        and perform split attempts.
+    kwargs
+        Other parameters passed to the learning nodes.
+    """
+
+    def __init__(self, stats, depth, splitter, **kwargs):
+        super().__init__(stats, depth, splitter, **kwargs)
+
+
+class EFDTLeafNaiveBayesAdaptive(BaseEFDTLeaf, LeafNaiveBayesAdaptive):
+    """Learning node for the Hoeffding Anytime Tree that uses Adaptive Naive
+    Bayes models.
+
+    Parameters
+    ----------
+    stats
+        Initial class observations.
+    depth
+        The depth of the node.
+    splitter
+        The numeric attribute observer algorithm used to monitor target statistics
+        and perform split attempts.
+    kwargs
+        Other parameters passed to the learning nodes.
+    """
+
+    def __init__(self, stats, depth, splitter, **kwargs):
+        super().__init__(stats, depth, splitter, **kwargs)
+
+
+class EFDTNominalBinaryBranch(BaseEFDTBranch, NominalBinaryBranch):
+    def __init__(self, stats, feature, value, depth, left, right, **attributes):
+        super().__init__(stats, feature, value, depth, left, right, **attributes)
+
+
+class EFDTNominalMultiwayBranch(BaseEFDTBranch, NominalMultiwayBranch):
+    def __init__(self, stats, feature, feature_values, depth, *children, **attributes):
+        super().__init__(stats, feature, feature_values, depth, *children, **attributes)
+
+
+class EFDTNumericBinaryBranch(BaseEFDTBranch, NumericBinaryBranch):
+    def __init__(self, stats, feature, threshold, depth, left, right, **attributes):
+        super().__init__(stats, feature, threshold, depth, left, right, **attributes)
+
+
+class EFDTNumericMultiwayBranch(BaseEFDTBranch, NumericMultiwayBranch):
+    def __init__(
+        self, stats, feature, radius_and_slots, depth, *children, **attributes
+    ):
+        super().__init__(
+            stats, feature, radius_and_slots, depth, *children, **attributes
+        )
```

### Comparing `river-0.8.0/river/tree/nodes/hatc_nodes.py` & `river-0.9.0/river/tree/nodes/hatr_nodes.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,406 +1,391 @@
-import abc
-import math
-import typing
-
-from river.drift import ADWIN
-from river.utils.skmultiflow_utils import check_random_state, normalize_values_in_dict
-
-from ..utils import do_naive_bayes_prediction
-from .branch import (
-    DTBranch,
-    NominalBinaryBranch,
-    NominalMultiwayBranch,
-    NumericBinaryBranch,
-    NumericMultiwayBranch,
-)
-from .htc_nodes import LeafNaiveBayesAdaptive
-from .leaf import HTLeaf
-
-
-class AdaNode(abc.ABC):
-    """Abstract Class to create a new Node for the Hoeffding Adaptive Tree
-    Classifier/Regressor"""
-
-    @property
-    @abc.abstractmethod
-    def error_estimation(self):
-        pass
-
-    @property
-    @abc.abstractmethod
-    def error_width(self):
-        pass
-
-    @abc.abstractmethod
-    def error_is_null(self):
-        pass
-
-    @abc.abstractmethod
-    def kill_tree_children(self, hat):
-        pass
-
-
-class AdaLeafClassifier(LeafNaiveBayesAdaptive, AdaNode):
-    """Learning node for Hoeffding Adaptive Tree.
-
-    Parameters
-    ----------
-    stats
-        Initial class observations.
-    depth
-        The depth of the learning node in the tree.
-    splitter
-        The numeric attribute observer algorithm used to monitor target statistics
-        and perform split attempts.
-    adwin_delta
-        The delta parameter of ADWIN.
-    seed
-        Seed to control the generation of random numbers and support reproducibility.
-    kwargs
-        Other parameters passed to the learning node.
-    """
-
-    def __init__(self, stats, depth, splitter, adwin_delta, seed, **kwargs):
-        super().__init__(stats, depth, splitter, **kwargs)
-        self.adwin_delta = adwin_delta
-        self._adwin = ADWIN(delta=self.adwin_delta)
-        self._error_change = False
-        self._rng = check_random_state(seed)
-
-    @property
-    def error_estimation(self):
-        return self._adwin.estimation
-
-    @property
-    def error_width(self):
-        return self._adwin.width
-
-    def error_is_null(self):
-        return self._adwin is None
-
-    def kill_tree_children(self, hat):
-        pass
-
-    def learn_one(
-        self, x, y, *, sample_weight=1.0, tree=None, parent=None, parent_branch=None
-    ):
-        if tree.bootstrap_sampling:
-            # Perform bootstrap-sampling
-            k = self._rng.poisson(1.0)
-            if k > 0:
-                sample_weight = sample_weight * k
-
-        aux = self.prediction(x, tree=tree)
-        class_prediction = max(aux, key=aux.get) if aux else None
-
-        is_correct = y == class_prediction
-
-        if self._adwin is None:
-            self._adwin = ADWIN(delta=self.adwin_delta)
-
-        old_error = self.error_estimation
-
-        # Update ADWIN
-        self._error_change, _ = self._adwin.update(int(not is_correct))
-
-        # Error is decreasing
-        if self._error_change and old_error > self.error_estimation:
-            self._error_change = False
-
-        # Update statistics
-        super().learn_one(x, y, sample_weight=sample_weight, tree=tree)
-
-        weight_seen = self.total_weight
-
-        if weight_seen - self.last_split_attempt_at >= tree.grace_period:
-            if self.depth >= tree.max_depth:
-                # Depth-based pre-pruning
-                self.deactivate()
-                tree._n_inactive_leaves += 1
-                tree._n_active_leaves -= 1
-            elif self.is_active():
-                tree._attempt_to_split(
-                    self,
-                    parent,
-                    parent_branch,
-                    adwin_delta=tree.adwin_confidence,
-                    seed=tree.seed,
-                )
-                self.last_split_attempt_at = weight_seen
-
-    # Override LearningNodeNBA
-    def prediction(self, x, *, tree=None):
-        if not self.stats:
-            return
-
-        prediction_option = tree.leaf_prediction
-        if not self.is_active() or prediction_option == tree._MAJORITY_CLASS:
-            dist = normalize_values_in_dict(self.stats, inplace=False)
-        elif prediction_option == tree._NAIVE_BAYES:
-            if self.total_weight >= tree.nb_threshold:
-                dist = do_naive_bayes_prediction(x, self.stats, self.splitters)
-            else:  # Use majority class
-                dist = normalize_values_in_dict(self.stats, inplace=False)
-        else:  # Naive Bayes Adaptive
-            dist = super().prediction(x, tree=tree)
-
-        dist_sum = sum(dist.values())
-        normalization_factor = dist_sum * self.error_estimation * self.error_estimation
-
-        # Weight node's responses accordingly to the estimated error monitored by ADWIN
-        # Useful if both the predictions of the alternate tree and the ones from the main tree
-        # are combined -> give preference to the most accurate one
-        dist = normalize_values_in_dict(dist, normalization_factor, inplace=False)
-
-        return dist
-
-
-class AdaBranchClassifier(DTBranch, AdaNode):
-    """Node that splits the data in a Hoeffding Adaptive Tree.
-
-    Parameters
-    ----------
-    stats
-        Class observations
-    adwin_delta
-        The delta parameter of ADWIN.
-    seed
-        Internal random state used to sample from poisson distributions.
-    children
-        Sequence of children nodes of this branch.
-    attributes
-        Other parameters passed to the split node.
-    """
-
-    def __init__(self, stats, *children, adwin_delta, seed, **attributes):
-        super().__init__(stats, *children, **attributes)
-        self.adwin_delta = adwin_delta
-        self._adwin = ADWIN(delta=self.adwin_delta)
-        self._alternate_tree = None
-        self._error_change = False
-
-        self._rng = check_random_state(seed)
-
-    def traverse(self, x, until_leaf=True) -> typing.List[HTLeaf]:
-        """Return the leaves corresponding to the given input.
-
-        Alternate subtree leaves are also included.
-
-        Parameters
-        ----------
-        x
-            The input instance.
-        until_leaf
-            Whether or not branch nodes can be returned in case of missing features or emerging
-            feature categories.
-        """
-        found_nodes = []
-        for node in self.walk(x, until_leaf=until_leaf):
-            if (
-                isinstance(node, AdaBranchClassifier)
-                and node._alternate_tree is not None
-            ):
-                if isinstance(node._alternate_tree, AdaBranchClassifier):
-                    found_nodes.append(
-                        node._alternate_tree.traverse(x, until_leaf=until_leaf)
-                    )
-                else:
-                    found_nodes.append(node._alternate_tree)
-
-        found_nodes.append(node)
-        return found_nodes
-
-    def iter_leaves(self):
-        """Iterate over leaves from the left-most one to the right-most one.
-
-        Overrides the base implementation by also including alternate subtrees.
-        """
-        for child in self.children:
-            yield from child.iter_leaves()
-
-            if (
-                isinstance(child, AdaBranchClassifier)
-                and child._alternate_tree is not None
-            ):
-                yield from child._alternate_tree.iter_leaves()
-
-    @property
-    def error_estimation(self):
-        return self._adwin.estimation
-
-    @property
-    def error_width(self):
-        w = 0.0
-        if not self.error_is_null():
-            w = self._adwin.width
-
-        return w
-
-    def error_is_null(self):
-        return self._adwin is None
-
-    def learn_one(
-        self, x, y, *, sample_weight=1.0, tree=None, parent=None, parent_branch=None
-    ):
-        leaf = super().traverse(x, until_leaf=True)
-        aux = leaf.prediction(x, tree=tree)
-        class_prediction = max(aux, key=aux.get) if aux else None
-        is_correct = y == class_prediction
-
-        # Update stats as traverse the tree to improve predictions (in case split nodes are used
-        # to provide responses)
-        try:
-            self.stats[y] += sample_weight
-        except KeyError:
-            self.stats[y] = sample_weight
-
-        if self._adwin is None:
-            self._adwin = ADWIN(self.adwin_delta)
-
-        old_error = self.error_estimation
-
-        # Update ADWIN
-        self._error_change, _ = self._adwin.update(int(not is_correct))
-
-        # Classification error is decreasing: skip drift adaptation
-        if self._error_change and old_error > self.error_estimation:
-            self._error_change = False
-
-        # Condition to build a new alternate tree
-        if self._error_change:
-            self._alternate_tree = tree._new_leaf(parent=self)
-            self._alternate_tree.depth -= 1  # To ensure we do not skip a tree level
-            tree._n_alternate_trees += 1
-        # Condition to replace alternate tree
-        elif (
-            self._alternate_tree is not None
-            and not self._alternate_tree.error_is_null()
-        ):
-            if (
-                self.error_width > tree.drift_window_threshold
-                and self._alternate_tree.error_width > tree.drift_window_threshold
-            ):
-                old_error_rate = self.error_estimation
-                alt_error_rate = self._alternate_tree.error_estimation
-                f_delta = 0.05
-                f_n = 1.0 / self._alternate_tree.error_width + 1.0 / self.error_width
-
-                bound = math.sqrt(
-                    2.0
-                    * old_error_rate
-                    * (1.0 - old_error_rate)
-                    * math.log(2.0 / f_delta)
-                    * f_n
-                )
-                if bound < (old_error_rate - alt_error_rate):
-                    tree._n_active_leaves -= self.n_leaves
-                    tree._n_active_leaves += self._alternate_tree.n_leaves
-                    self.kill_tree_children(tree)
-
-                    if parent is not None:
-                        parent.children[parent_branch] = self._alternate_tree
-                        self._alternate_tree = None
-                    else:
-                        # Switch tree root
-                        tree._root = tree._root._alternate_tree
-                    tree._n_switch_alternate_trees += 1
-                elif bound < alt_error_rate - old_error_rate:
-                    if isinstance(self._alternate_tree, DTBranch):
-                        self._alternate_tree.kill_tree_children(tree)  # noqa
-                    self._alternate_tree = None
-                    tree._n_pruned_alternate_trees += 1
-
-        # Learn one sample in alternate tree and child nodes
-        if self._alternate_tree is not None:
-            self._alternate_tree.learn_one(
-                x,
-                y,
-                sample_weight=sample_weight,
-                tree=tree,
-                parent=parent,
-                parent_branch=parent_branch,
-            )
-
-        try:
-            child = self.next(x)
-        except KeyError:
-            child = None
-
-        if child is not None:
-            child.learn_one(
-                x,
-                y,
-                sample_weight=sample_weight,
-                tree=tree,
-                parent=self,
-                parent_branch=self.branch_no(x),
-            )
-        else:
-            # Instance contains a categorical value previously unseen by the split node
-            if self.max_branches() == -1 and self.feature in x:  # noqa
-                # Creates a new learning node to encompass the new observed feature value
-                leaf = tree._new_leaf(parent=self)
-                self.add_child(x[self.feature], leaf)  # noqa
-                tree._n_active_leaves += 1
-                leaf.learn_one(
-                    x,
-                    y,
-                    sample_weight=sample_weight,
-                    tree=tree,
-                    parent=self,
-                    parent_branch=self.branch_no(x),
-                )
-            # The split feature is missing in the instance. Hence, we pass the new example
-            # to the most traversed path in the current subtree
-            else:
-                child_id, child = self.most_common_path()
-                child.learn_one(
-                    x,
-                    y,
-                    sample_weight=sample_weight,
-                    tree=tree,
-                    parent=self,
-                    parent_branch=child_id,
-                )
-
-    # Override AdaNode
-    def kill_tree_children(self, tree):
-        for child in self.children:
-            # Delete alternate tree if it exists
-            if isinstance(child, DTBranch):
-                if child._alternate_tree is not None:
-                    child._alternate_tree.kill_tree_children(tree)
-                    tree._n_pruned_alternate_trees += 1
-                    child._alternate_tree = None
-
-                # Recursive delete of SplitNodes
-                child.kill_tree_children(tree)  # noqa
-            else:
-                if child.is_active():  # noqa
-                    tree._n_active_leaves -= 1
-                else:
-                    tree._n_inactive_leaves -= 1
-
-
-class AdaNomBinaryBranchClass(AdaBranchClassifier, NominalBinaryBranch):
-    def __init__(self, stats, feature, value, depth, left, right, **attributes):
-        super().__init__(stats, feature, value, depth, left, right, **attributes)
-
-
-class AdaNumBinaryBranchClass(AdaBranchClassifier, NumericBinaryBranch):
-    def __init__(self, stats, feature, threshold, depth, left, right, **attributes):
-        super().__init__(stats, feature, threshold, depth, left, right, **attributes)
-
-
-class AdaNomMultiwayBranchClass(AdaBranchClassifier, NominalMultiwayBranch):
-    def __init__(self, stats, feature, feature_values, depth, *children, **attributes):
-        super().__init__(stats, feature, feature_values, depth, *children, **attributes)
-
-
-class AdaNumMultiwayBranchClass(AdaBranchClassifier, NumericMultiwayBranch):
-    def __init__(
-        self, stats, feature, radius_and_slots, depth, *children, **attributes
-    ):
-        super().__init__(
-            stats, feature, radius_and_slots, depth, *children, **attributes
-        )
+import math
+import typing
+
+from river.drift.adwin import ADWIN
+from river.stats import Var
+from river.utils.skmultiflow_utils import check_random_state
+
+from .branch import (
+    DTBranch,
+    NominalBinaryBranch,
+    NominalMultiwayBranch,
+    NumericBinaryBranch,
+    NumericMultiwayBranch,
+)
+from .hatc_nodes import AdaNode
+from .htr_nodes import LeafAdaptive, LeafMean, LeafModel
+from .leaf import HTLeaf
+
+
+class AdaLeafRegressor(HTLeaf, AdaNode):
+    """Learning Node of the Hoeffding Adaptive Tree regressor.
+
+    Parameters
+    ----------
+    stats
+        Initial class observations.
+    depth
+        The depth of the learning node in the tree.
+    splitter
+        The numeric attribute observer algorithm used to monitor target statistics
+        and perform split attempts.
+    adwin_delta
+        The delta parameter of ADWIN.
+    seed
+        Seed to control the generation of random numbers and support reproducibility.
+    kwargs
+        Other parameters passed to the learning node.
+    """
+
+    def __init__(self, stats, depth, splitter, adwin_delta, seed, **kwargs):
+        super().__init__(stats, depth, splitter, **kwargs)
+
+        self.adwin_delta = adwin_delta
+        self._adwin = ADWIN(delta=self.adwin_delta)
+        self._error_change = False
+        self._rng = check_random_state(seed)
+
+        # Normalization of info monitored by drift detectors (using Welford's algorithm)
+        self._error_normalizer = Var(ddof=1)
+
+    @property
+    def error_estimation(self):
+        return self._adwin.estimation
+
+    @property
+    def error_width(self):
+        return self._adwin.width
+
+    def error_is_null(self):
+        return self._adwin is None
+
+    def kill_tree_children(self, hatr):
+        pass
+
+    def learn_one(
+        self, x, y, *, sample_weight=1.0, tree=None, parent=None, parent_branch=None
+    ):
+        y_pred = self.prediction(x, tree=tree)
+        normalized_error = normalize_error(y, y_pred, self)
+
+        if tree.bootstrap_sampling:
+            # Perform bootstrap-sampling
+            k = self._rng.poisson(1.0)
+            if k > 0:
+                sample_weight *= k
+
+        if self._adwin is None:
+            self._adwin = ADWIN(delta=self.adwin_delta)
+
+        old_error = self.error_estimation
+
+        # Update ADWIN
+        self._error_change, _ = self._adwin.update(normalized_error)
+
+        # Error is decreasing
+        if self._error_change and old_error > self.error_estimation:
+            self._error_change = False
+
+        # Update learning model
+        super().learn_one(x, y, sample_weight=sample_weight, tree=tree)
+
+        weight_seen = self.total_weight
+
+        if weight_seen - self.last_split_attempt_at >= tree.grace_period:
+            if self.depth >= tree.max_depth:
+                # Depth-based pre-pruning
+                self.deactivate()
+                tree._n_inactive_leaves += 1
+                tree._n_active_leaves -= 1
+            elif self.is_active():
+                tree._attempt_to_split(
+                    self,
+                    parent,
+                    parent_branch,
+                    adwin_delta=tree.adwin_confidence,
+                    seed=tree.seed,
+                )
+                self.last_split_attempt_at = weight_seen
+
+
+class AdaBranchRegressor(DTBranch, AdaNode):
+    """Node that splits the data in a Hoeffding Adaptive Tree Regression.
+
+    Parameters
+    ----------
+    stats
+        Target stats.
+    depth
+        The depth of the node.
+    adwin_delta
+        The delta parameter of ADWIN.
+    seed
+        Internal random state used to sample from poisson distributions.
+    attributes
+        Other parameters passed to the split node.
+    """
+
+    def __init__(self, stats, *children, adwin_delta, seed, **attributes):
+        stats = stats if stats else Var()
+        super().__init__(stats, *children, **attributes)
+        self.adwin_delta = adwin_delta
+        self._adwin = ADWIN(delta=self.adwin_delta)
+        self._alternate_tree = None
+        self._error_change = False
+
+        self._rng = check_random_state(seed)
+
+        # Normalization of info monitored by drift detectors (using Welford's algorithm)
+        self._error_normalizer = Var(ddof=1)
+
+    def traverse(self, x, until_leaf=True) -> typing.List[HTLeaf]:
+        """Return the leaves corresponding to the given input.
+
+        Alternate subtree leaves are also included.
+
+        Parameters
+        ----------
+        x
+            The input instance.
+        until_leaf
+            Whether or not branch nodes can be returned in case of missing features or emerging
+            feature categories.
+        """
+        found_nodes = []
+        for node in self.walk(x, until_leaf=until_leaf):
+            if (
+                isinstance(node, AdaBranchRegressor)
+                and node._alternate_tree is not None
+            ):
+                if isinstance(node._alternate_tree, AdaBranchRegressor):
+                    found_nodes.append(
+                        node._alternate_tree.traverse(x, until_leaf=until_leaf)
+                    )
+                else:
+                    found_nodes.append(node._alternate_tree)
+
+        found_nodes.append(node)
+        return found_nodes
+
+    def iter_leaves(self):
+        """Iterate over leaves from the left-most one to the right-most one.
+
+        Overrides the base implementation by also including alternate subtrees.
+        """
+        for child in self.children:
+            yield from child.iter_leaves()
+
+            if (
+                isinstance(child, AdaBranchRegressor)
+                and child._alternate_tree is not None
+            ):
+                yield from child._alternate_tree.iter_leaves()
+
+    @property
+    def error_estimation(self):
+        return self._adwin.estimation
+
+    @property
+    def error_width(self):
+        w = 0.0
+        if not self.error_is_null():
+            w = self._adwin.width
+
+        return w
+
+    def error_is_null(self):
+        return self._adwin is None
+
+    def learn_one(
+        self, x, y, *, sample_weight=1.0, tree=None, parent=None, parent_branch=None
+    ):
+        leaf = super().traverse(x, until_leaf=True)
+        y_pred = leaf.prediction(x, tree=tree)
+        normalized_error = normalize_error(y, y_pred, self)
+
+        # Update stats as traverse the tree to improve predictions (in case split nodes are used
+        # to provide responses)
+        self.stats.update(y, sample_weight)
+
+        if self._adwin is None:
+            self._adwin = ADWIN(self.adwin_delta)
+
+        old_error = self.error_estimation
+
+        # Update ADWIN
+        self._error_change, _ = self._adwin.update(normalized_error)
+
+        if self._error_change and old_error > self.error_estimation:
+            self._error_change = False
+
+        # Condition to build a new alternate tree
+        if self._error_change:
+            self._alternate_tree = tree._new_leaf(parent=self)
+            self._alternate_tree.depth -= 1  # To ensure we do not skip a tree level
+            tree._n_alternate_trees += 1
+
+        # Condition to replace alternate tree
+        elif (
+            self._alternate_tree is not None
+            and not self._alternate_tree.error_is_null()
+        ):
+            if (
+                self.error_width > tree.drift_window_threshold
+                and self._alternate_tree.error_width > tree.drift_window_threshold
+            ):
+                old_error_rate = self.error_estimation
+                alt_error_rate = self._alternate_tree.error_estimation
+                f_delta = 0.05
+                f_n = 1.0 / self._alternate_tree.error_width + 1.0 / self.error_width
+
+                try:
+                    bound = math.sqrt(
+                        2.0
+                        * old_error_rate
+                        * (1.0 - old_error_rate)
+                        * math.log(2.0 / f_delta)
+                        * f_n
+                    )
+                except ValueError:  # error rate exceeds 1, so we clip it
+                    bound = 0.0
+                if bound < (old_error_rate - alt_error_rate):
+                    tree._n_active_leaves -= self.n_leaves
+                    tree._n_active_leaves += self._alternate_tree.n_leaves
+                    self.kill_tree_children(tree)
+
+                    if parent is not None:
+                        parent.children[parent_branch] = self._alternate_tree
+                        self._alternate_tree = None
+                    else:
+                        # Switch tree root
+                        tree._root = tree._root._alternate_tree
+                    tree._n_switch_alternate_trees += 1
+                elif bound < alt_error_rate - old_error_rate:
+                    if isinstance(self._alternate_tree, DTBranch):
+                        self._alternate_tree.kill_tree_children(tree)  # noqa
+                    self._alternate_tree = None
+                    tree._n_pruned_alternate_trees += 1
+
+        # Learn one sample in alternate tree and child nodes
+        if self._alternate_tree is not None:
+            self._alternate_tree.learn_one(
+                x,
+                y,
+                sample_weight=sample_weight,
+                tree=tree,
+                parent=parent,
+                parent_branch=parent_branch,
+            )
+        try:
+            child = self.next(x)
+        except KeyError:
+            child = None
+
+        if child is not None:
+            child.learn_one(
+                x,
+                y,
+                sample_weight=sample_weight,
+                tree=tree,
+                parent=self,
+                parent_branch=self.branch_no(x),
+            )
+        else:
+            # Instance contains a categorical value previously unseen by the split node
+            if self.max_branches() == -1 and self.feature in x:  # noqa
+                # Creates a new learning node to encompass the new observed feature value
+                leaf = tree._new_leaf(parent=self)
+                self.add_child(x[self.feature], leaf)  # noqa
+                tree._n_active_leaves += 1
+                leaf.learn_one(
+                    x,
+                    y,
+                    sample_weight=sample_weight,
+                    tree=tree,
+                    parent=self,
+                    parent_branch=self.branch_no(x),
+                )
+            # The split feature is missing in the instance. Hence, we pass the new example
+            # to the most traversed path in the current subtree
+            else:
+                child_id, child = self.most_common_path()
+                child.learn_one(
+                    x,
+                    y,
+                    sample_weight=sample_weight,
+                    tree=tree,
+                    parent=self,
+                    parent_branch=child_id,
+                )
+
+    # Override AdaNode
+    def kill_tree_children(self, tree):
+        for child in self.children:
+            # Delete alternate tree if it exists
+            if isinstance(child, DTBranch):
+                if child._alternate_tree is not None:
+                    child._alternate_tree.kill_tree_children(tree)
+                    tree._n_pruned_alternate_trees += 1
+                    child._alternate_tree = None
+
+                # Recursive delete of SplitNodes
+                child.kill_tree_children(tree)  # noqa
+            else:
+                if child.is_active():  # noqa
+                    tree._n_active_leaves -= 1
+                else:
+                    tree._n_inactive_leaves -= 1
+
+
+class AdaNomBinaryBranchReg(AdaBranchRegressor, NominalBinaryBranch):
+    def __init__(self, stats, feature, value, depth, left, right, **attributes):
+        super().__init__(stats, feature, value, depth, left, right, **attributes)
+
+
+class AdaNumBinaryBranchReg(AdaBranchRegressor, NumericBinaryBranch):
+    def __init__(self, stats, feature, threshold, depth, left, right, **attributes):
+        super().__init__(stats, feature, threshold, depth, left, right, **attributes)
+
+
+class AdaNomMultiwayBranchReg(AdaBranchRegressor, NominalMultiwayBranch):
+    def __init__(self, stats, feature, feature_values, depth, *children, **attributes):
+        super().__init__(stats, feature, feature_values, depth, *children, **attributes)
+
+
+class AdaNumMultiwayBranchReg(AdaBranchRegressor, NumericMultiwayBranch):
+    def __init__(
+        self, stats, feature, radius_and_slots, depth, *children, **attributes
+    ):
+        super().__init__(
+            stats, feature, radius_and_slots, depth, *children, **attributes
+        )
+
+
+class AdaLeafRegMean(AdaLeafRegressor, LeafMean):
+    def __init__(self, stats, depth, splitter, adwin_delta, seed, **kwargs):
+        super().__init__(stats, depth, splitter, adwin_delta, seed, **kwargs)
+
+
+class AdaLeafRegModel(AdaLeafRegressor, LeafModel):
+    def __init__(self, stats, depth, splitter, adwin_delta, seed, **kwargs):
+        super().__init__(stats, depth, splitter, adwin_delta, seed, **kwargs)
+
+
+class AdaLeafRegAdaptive(AdaLeafRegressor, LeafAdaptive):
+    def __init__(self, stats, depth, splitter, adwin_delta, seed, **kwargs):
+        super().__init__(stats, depth, splitter, adwin_delta, seed, **kwargs)
+
+
+def normalize_error(y_true, y_pred, node):
+    drift_input = y_true - y_pred
+    node._error_normalizer.update(drift_input)
+
+    if node._error_normalizer.mean.n == 1:
+        return 0.5  # The expected error is the normalized mean error
+
+    sd = math.sqrt(node._error_normalizer.get())
+
+    # We assume the error follows a normal distribution -> (empirical rule) 99.73% of the values
+    # lie  between [mean - 3*sd, mean + 3*sd]. We assume this range for the normalized data.
+    # Hence, we can apply the  min-max norm to cope with  ADWIN's requirements
+    return (drift_input + 3 * sd) / (6 * sd) if sd > 0 else 0.5
```

### Comparing `river-0.8.0/river/tree/nodes/htr_nodes.py` & `river-0.9.0/river/tree/nodes/htr_nodes.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,191 +1,191 @@
-import inspect
-
-from river.stats import Var
-
-from ..splitter import EBSTSplitter
-from ..splitter.nominal_splitter_reg import NominalSplitterReg
-from .leaf import HTLeaf
-
-
-class LeafMean(HTLeaf):
-    """Learning Node for regression tasks that always use the average target
-        value as response.
-
-    Parameters
-    ----------
-    stats
-        In regression tasks the node keeps an instance of `river.stats.Var` to estimate
-        the target's statistics.
-    depth
-        The depth of the node.
-    splitter
-        The numeric attribute observer algorithm used to monitor target statistics
-        and perform split attempts.
-    kwargs
-        Other parameters passed to the learning node.
-    """
-
-    def __init__(self, stats, depth, splitter, **kwargs):
-        if stats is None:
-            # Enforce the usage of Var to keep track of target statistics
-            stats = Var()
-        super().__init__(stats, depth, splitter, **kwargs)
-
-    @staticmethod
-    def new_nominal_splitter():
-        return NominalSplitterReg()
-
-    def manage_memory(self, criterion, last_check_ratio, last_check_vr, last_check_e):
-        """Trigger Attribute Observers' memory management routines.
-
-        Currently, only `EBSTSplitter` and `TEBSTSplitter` have support to this feature.
-
-        Parameters
-        ----------
-        criterion
-            Split criterion
-        last_check_ratio
-            The ratio between the second best candidate's merit and the merit of the best
-            split candidate.
-        last_check_vr
-            The best candidate's split merit.
-        last_check_e
-            Hoeffding bound value calculated in the last split attempt.
-        """
-        for splitter in self.splitters.values():
-            if isinstance(splitter, EBSTSplitter):
-                splitter.remove_bad_splits(
-                    criterion=criterion,
-                    last_check_ratio=last_check_ratio,
-                    last_check_vr=last_check_vr,
-                    last_check_e=last_check_e,
-                    pre_split_dist=self.stats,
-                )
-
-    def update_stats(self, y, sample_weight):
-        self.stats.update(y, sample_weight)
-
-    def prediction(self, x, *, tree=None):
-        return self.stats.mean.get()
-
-    @property
-    def total_weight(self):
-        """Calculate the total weight seen by the node.
-
-        Returns
-        -------
-        float
-            Total weight seen.
-
-        """
-        return self.stats.mean.n
-
-    def calculate_promise(self) -> int:
-        """Estimate how likely a leaf node is going to be split.
-
-        Uses the node's depth as a heuristic to estimate how likely the leaf is going to become
-        a decision node. The deeper the node is in the tree, the more unlikely it is going to be
-        split. To cope with the general tree memory management framework, takes the negative of
-        the node's depth as return value. In this way, when sorting the tree leaves by their
-        "promise value", the deepest nodes are going to be placed at the first positions as
-        candidates to be deactivated.
-
-
-        Returns
-        -------
-        int
-            The smaller the value, the more unlikely the node is going to be split.
-
-        """
-        return -self.depth
-
-    def __repr__(self):
-        return f"{repr(self.stats.mean)} | {repr(self.stats)}" if self.stats else ""
-
-
-class LeafModel(LeafMean):
-    """Learning Node for regression tasks that always use a learning model to provide
-        responses.
-
-    Parameters
-    ----------
-    stats
-        In regression tasks the node keeps an instance of `river.stats.Var` to estimate
-        the target's statistics.
-    depth
-        The depth of the node.
-    splitter
-        The numeric attribute observer algorithm used to monitor target statistics
-        and perform split attempts.
-    leaf_model
-        A `river.base.Regressor` instance used to learn from instances and provide
-        responses.
-    kwargs
-        Other parameters passed to the learning node.
-    """
-
-    def __init__(self, stats, depth, splitter, leaf_model, **kwargs):
-        super().__init__(stats, depth, splitter, **kwargs)
-
-        self._leaf_model = leaf_model
-        sign = inspect.signature(leaf_model.learn_one).parameters
-        self._model_supports_weights = "sample_weight" in sign or "w" in sign
-
-    def learn_one(self, x, y, *, sample_weight=1.0, tree=None):
-        super().learn_one(x, y, sample_weight=sample_weight, tree=tree)
-
-        if self._model_supports_weights:
-            self._leaf_model.learn_one(x, y, sample_weight)
-        else:
-            for _ in range(int(sample_weight)):
-                self._leaf_model.learn_one(x, y)
-
-    def prediction(self, x, *, tree=None):
-        return self._leaf_model.predict_one(x)
-
-
-class LeafAdaptive(LeafModel):
-    """Learning Node for regression tasks that dynamically selects between predictors and
-        might behave as a regression tree node or a model tree node, depending on which predictor
-        is the best one.
-
-    Parameters
-    ----------
-    stats
-        In regression tasks the node keeps an instance of `river.stats.Var` to estimate
-        the target's statistics.
-    depth
-        The depth of the node.
-    splitter
-        The numeric attribute observer algorithm used to monitor target statistics
-        and perform split attempts.
-    leaf_model
-        A `river.base.Regressor` instance used to learn from instances and provide
-        responses.
-    kwargs
-        Other parameters passed to the learning node.
-    """
-
-    def __init__(self, stats, depth, splitter, leaf_model, **kwargs):
-        super().__init__(stats, depth, splitter, leaf_model, **kwargs)
-        self._fmse_mean = 0.0
-        self._fmse_model = 0.0
-
-    def learn_one(self, x, y, *, sample_weight=1.0, tree=None):
-        pred_mean = self.stats.mean.get()
-        pred_model = self._leaf_model.predict_one(x)
-
-        self._fmse_mean = (
-            tree.model_selector_decay * self._fmse_mean + (y - pred_mean) ** 2
-        )
-        self._fmse_model = (
-            tree.model_selector_decay * self._fmse_model + (y - pred_model) ** 2
-        )
-
-        super().learn_one(x, y, sample_weight=sample_weight, tree=tree)
-
-    def prediction(self, x, *, tree=None):
-        if self._fmse_mean < self._fmse_model:  # Act as a regression tree
-            return self.stats.mean.get()
-        else:  # Act as a model tree
-            return super().prediction(x)
+import inspect
+
+from river.stats import Var
+
+from ..splitter import EBSTSplitter
+from ..splitter.nominal_splitter_reg import NominalSplitterReg
+from .leaf import HTLeaf
+
+
+class LeafMean(HTLeaf):
+    """Learning Node for regression tasks that always use the average target
+        value as response.
+
+    Parameters
+    ----------
+    stats
+        In regression tasks the node keeps an instance of `river.stats.Var` to estimate
+        the target's statistics.
+    depth
+        The depth of the node.
+    splitter
+        The numeric attribute observer algorithm used to monitor target statistics
+        and perform split attempts.
+    kwargs
+        Other parameters passed to the learning node.
+    """
+
+    def __init__(self, stats, depth, splitter, **kwargs):
+        if stats is None:
+            # Enforce the usage of Var to keep track of target statistics
+            stats = Var()
+        super().__init__(stats, depth, splitter, **kwargs)
+
+    @staticmethod
+    def new_nominal_splitter():
+        return NominalSplitterReg()
+
+    def manage_memory(self, criterion, last_check_ratio, last_check_vr, last_check_e):
+        """Trigger Attribute Observers' memory management routines.
+
+        Currently, only `EBSTSplitter` and `TEBSTSplitter` have support to this feature.
+
+        Parameters
+        ----------
+        criterion
+            Split criterion
+        last_check_ratio
+            The ratio between the second best candidate's merit and the merit of the best
+            split candidate.
+        last_check_vr
+            The best candidate's split merit.
+        last_check_e
+            Hoeffding bound value calculated in the last split attempt.
+        """
+        for splitter in self.splitters.values():
+            if isinstance(splitter, EBSTSplitter):
+                splitter.remove_bad_splits(
+                    criterion=criterion,
+                    last_check_ratio=last_check_ratio,
+                    last_check_vr=last_check_vr,
+                    last_check_e=last_check_e,
+                    pre_split_dist=self.stats,
+                )
+
+    def update_stats(self, y, sample_weight):
+        self.stats.update(y, sample_weight)
+
+    def prediction(self, x, *, tree=None):
+        return self.stats.mean.get()
+
+    @property
+    def total_weight(self):
+        """Calculate the total weight seen by the node.
+
+        Returns
+        -------
+        float
+            Total weight seen.
+
+        """
+        return self.stats.mean.n
+
+    def calculate_promise(self) -> int:
+        """Estimate how likely a leaf node is going to be split.
+
+        Uses the node's depth as a heuristic to estimate how likely the leaf is going to become
+        a decision node. The deeper the node is in the tree, the more unlikely it is going to be
+        split. To cope with the general tree memory management framework, takes the negative of
+        the node's depth as return value. In this way, when sorting the tree leaves by their
+        "promise value", the deepest nodes are going to be placed at the first positions as
+        candidates to be deactivated.
+
+
+        Returns
+        -------
+        int
+            The smaller the value, the more unlikely the node is going to be split.
+
+        """
+        return -self.depth
+
+    def __repr__(self):
+        return f"{repr(self.stats.mean)} | {repr(self.stats)}" if self.stats else ""
+
+
+class LeafModel(LeafMean):
+    """Learning Node for regression tasks that always use a learning model to provide
+        responses.
+
+    Parameters
+    ----------
+    stats
+        In regression tasks the node keeps an instance of `river.stats.Var` to estimate
+        the target's statistics.
+    depth
+        The depth of the node.
+    splitter
+        The numeric attribute observer algorithm used to monitor target statistics
+        and perform split attempts.
+    leaf_model
+        A `river.base.Regressor` instance used to learn from instances and provide
+        responses.
+    kwargs
+        Other parameters passed to the learning node.
+    """
+
+    def __init__(self, stats, depth, splitter, leaf_model, **kwargs):
+        super().__init__(stats, depth, splitter, **kwargs)
+
+        self._leaf_model = leaf_model
+        sign = inspect.signature(leaf_model.learn_one).parameters
+        self._model_supports_weights = "sample_weight" in sign or "w" in sign
+
+    def learn_one(self, x, y, *, sample_weight=1.0, tree=None):
+        super().learn_one(x, y, sample_weight=sample_weight, tree=tree)
+
+        if self._model_supports_weights:
+            self._leaf_model.learn_one(x, y, sample_weight)
+        else:
+            for _ in range(int(sample_weight)):
+                self._leaf_model.learn_one(x, y)
+
+    def prediction(self, x, *, tree=None):
+        return self._leaf_model.predict_one(x)
+
+
+class LeafAdaptive(LeafModel):
+    """Learning Node for regression tasks that dynamically selects between predictors and
+        might behave as a regression tree node or a model tree node, depending on which predictor
+        is the best one.
+
+    Parameters
+    ----------
+    stats
+        In regression tasks the node keeps an instance of `river.stats.Var` to estimate
+        the target's statistics.
+    depth
+        The depth of the node.
+    splitter
+        The numeric attribute observer algorithm used to monitor target statistics
+        and perform split attempts.
+    leaf_model
+        A `river.base.Regressor` instance used to learn from instances and provide
+        responses.
+    kwargs
+        Other parameters passed to the learning node.
+    """
+
+    def __init__(self, stats, depth, splitter, leaf_model, **kwargs):
+        super().__init__(stats, depth, splitter, leaf_model, **kwargs)
+        self._fmse_mean = 0.0
+        self._fmse_model = 0.0
+
+    def learn_one(self, x, y, *, sample_weight=1.0, tree=None):
+        pred_mean = self.stats.mean.get()
+        pred_model = self._leaf_model.predict_one(x)
+
+        self._fmse_mean = (
+            tree.model_selector_decay * self._fmse_mean + (y - pred_mean) ** 2
+        )
+        self._fmse_model = (
+            tree.model_selector_decay * self._fmse_model + (y - pred_model) ** 2
+        )
+
+        super().learn_one(x, y, sample_weight=sample_weight, tree=tree)
+
+    def prediction(self, x, *, tree=None):
+        if self._fmse_mean < self._fmse_model:  # Act as a regression tree
+            return self.stats.mean.get()
+        else:  # Act as a model tree
+            return super().prediction(x)
```

### Comparing `river-0.8.0/river/tree/nodes/isouptr_nodes.py` & `river-0.9.0/river/tree/nodes/isouptr_nodes.py`

 * *Ordering differences only*

 * *Files 25% similar despite different names*

```diff
@@ -1,182 +1,182 @@
-import collections
-import functools
-import inspect
-from copy import deepcopy
-
-from river.stats import Var
-from river.utils import VectorDict
-
-from .htr_nodes import LeafMean
-
-
-class LeafMeanMultiTarget(LeafMean):
-    """Learning Node for Multi-target Regression tasks that always uses the mean value
-    of the targets as responses.
-
-    Parameters
-    ----------
-    stats
-        In regression tasks the node keeps a `utils.VectorDict` with instances of
-        `stats.Var` to estimate the targets' statistics.
-    depth
-        The depth of the node.
-    splitter
-        The numeric attribute observer algorithm used to monitor target statistics
-        and perform split attempts.
-    kwargs
-        Other parameters passed to the learning node.
-    """
-
-    def __init__(self, stats, depth, splitter, **kwargs):
-        stats = stats if stats else VectorDict(default_factory=functools.partial(Var))
-        super().__init__(stats, depth, splitter, **kwargs)
-
-    def update_stats(self, y, sample_weight):
-        for t in y:
-            self.stats[t].update(y[t], sample_weight)
-
-    def prediction(self, x, *, tree=None):
-        return {
-            t: self.stats[t].mean.get() if t in self.stats else 0.0
-            for t in tree.targets
-        }
-
-    @property
-    def total_weight(self):
-        return list(self.stats.values())[0].mean.n if self.stats else 0
-
-    def __repr__(self):
-        if self.stats:
-            buffer = "Targets' statistics:"
-            for t, var in self.stats.items():
-                buffer += f"\n\t{t}: {repr(var.mean)} | {repr(var)}"
-            return buffer
-        return ""
-
-
-class LeafModelMultiTarget(LeafMeanMultiTarget):
-    """Learning Node for Multi-target Regression tasks that always uses learning models
-    for each target.
-
-    Parameters
-    ----------
-    stats
-        In regression tasks the node keeps a `utils.VectorDict` with instances of
-        `stats.Var` to estimate the targets' statistics.
-    depth
-        The depth of the node.
-    splitter
-        The numeric attribute observer algorithm used to monitor target statistics
-        and perform split attempts.
-    leaf_models
-        A dictionary composed of target identifiers and their respective predictive models.
-    kwargs
-        Other parameters passed to the learning node.
-    """
-
-    def __init__(self, stats, depth, splitter, leaf_models, **kwargs):
-        super().__init__(stats, depth, splitter, **kwargs)
-        self._leaf_models = leaf_models
-        self._model_supports_weights = {}
-        if self._leaf_models:
-            for t in self._leaf_models:
-                sign = inspect.signature(self._leaf_models[t].learn_one).parameters
-                self._model_supports_weights[t] = "sample_weight" in sign or "w" in sign
-
-    def learn_one(self, x, y, *, sample_weight=1.0, tree=None):
-        super().learn_one(x, y, sample_weight=sample_weight, tree=tree)
-
-        for target_id, y_ in y.items():
-            try:
-                model = self._leaf_models[target_id]
-            except KeyError:
-                if isinstance(tree.leaf_model, dict):
-                    if target_id in tree.leaf_model:
-                        self._leaf_models[target_id] = deepcopy(
-                            tree.leaf_model[target_id]
-                        )
-                    else:
-                        # Pick the first available model in case not all the targets' models
-                        # are defined
-                        self._leaf_models[target_id] = deepcopy(
-                            next(iter(self._leaf_models.values()))
-                        )
-                    model = self._leaf_models[target_id]
-                else:
-                    self._leaf_models[target_id] = deepcopy(tree.leaf_model)
-                    model = self._leaf_models[target_id]
-                sign = inspect.signature(model.learn_one).parameters
-                self._model_supports_weights[target_id] = (
-                    "sample_weight" in sign or "w" in sign
-                )
-
-            # Now the proper training
-            if self._model_supports_weights[target_id]:
-                model.learn_one(x, y_, sample_weight)
-            else:
-                for _ in range(int(sample_weight)):
-                    model.learn_one(x, y_)
-
-    def prediction(self, x, *, tree=None):
-        return {
-            t: self._leaf_models[t].predict_one(x) if t in self._leaf_models else 0.0
-            for t in tree.targets
-        }
-
-
-class LeafAdaptiveMultiTarget(LeafModelMultiTarget):
-    """Learning Node for multi-target regression tasks that dynamically selects between
-    predictors and might behave as a regression tree node or a model tree node, depending
-    on which predictor is the best one.
-
-    Parameters
-    ----------
-    stats
-        In regression tasks the node keeps a `utils.VectorDict` with instances of
-        `stats.Var` to estimate the targets' statistics.
-    depth
-        The depth of the node.
-    splitter
-        The numeric attribute observer algorithm used to monitor target statistics
-        and perform split attempts.
-    leaf_models
-        A dictionary composed of target identifiers and their respective predictive models.
-    kwargs
-        Other parameters passed to the learning node.
-    """
-
-    def __init__(self, stats, depth, splitter, leaf_models, **kwargs):
-        super().__init__(stats, depth, splitter, leaf_models, **kwargs)
-        self._fmse_mean = collections.defaultdict(float)
-        self._fmse_model = collections.defaultdict(float)
-
-    def learn_one(self, x, y, *, sample_weight=1.0, tree=None):
-        pred_mean = {
-            t: self.stats[t].mean.get() if t in self.stats else 0.0
-            for t in tree.targets
-        }
-        pred_model = super().prediction(x, tree=tree)
-
-        for t in tree.targets:  # Update the faded errors
-            self._fmse_mean[t] = (
-                tree.model_selector_decay * self._fmse_mean[t]
-                + (y[t] - pred_mean[t]) ** 2
-            )
-            self._fmse_model[t] = (
-                tree.model_selector_decay * self._fmse_model[t]
-                + (y[t] - pred_model[t]) ** 2
-            )
-
-        super().learn_one(x, y, sample_weight=sample_weight, tree=tree)
-
-    def prediction(self, x, *, tree=None):
-        pred = {}
-        for t in tree.targets:
-            if self._fmse_mean[t] < self._fmse_model[t]:  # Act as a regression tree
-                pred[t] = self.stats[t].mean.get() if t in self.stats else 0.0
-            else:  # Act as a model tree
-                try:
-                    pred[t] = self._leaf_models[t].predict_one(x)
-                except KeyError:
-                    pred[t] = 0.0
-        return pred
+import collections
+import functools
+import inspect
+from copy import deepcopy
+
+from river.stats import Var
+from river.utils import VectorDict
+
+from .htr_nodes import LeafMean
+
+
+class LeafMeanMultiTarget(LeafMean):
+    """Learning Node for Multi-target Regression tasks that always uses the mean value
+    of the targets as responses.
+
+    Parameters
+    ----------
+    stats
+        In regression tasks the node keeps a `utils.VectorDict` with instances of
+        `stats.Var` to estimate the targets' statistics.
+    depth
+        The depth of the node.
+    splitter
+        The numeric attribute observer algorithm used to monitor target statistics
+        and perform split attempts.
+    kwargs
+        Other parameters passed to the learning node.
+    """
+
+    def __init__(self, stats, depth, splitter, **kwargs):
+        stats = stats if stats else VectorDict(default_factory=functools.partial(Var))
+        super().__init__(stats, depth, splitter, **kwargs)
+
+    def update_stats(self, y, sample_weight):
+        for t in y:
+            self.stats[t].update(y[t], sample_weight)
+
+    def prediction(self, x, *, tree=None):
+        return {
+            t: self.stats[t].mean.get() if t in self.stats else 0.0
+            for t in tree.targets
+        }
+
+    @property
+    def total_weight(self):
+        return list(self.stats.values())[0].mean.n if self.stats else 0
+
+    def __repr__(self):
+        if self.stats:
+            buffer = "Targets' statistics:"
+            for t, var in self.stats.items():
+                buffer += f"\n\t{t}: {repr(var.mean)} | {repr(var)}"
+            return buffer
+        return ""
+
+
+class LeafModelMultiTarget(LeafMeanMultiTarget):
+    """Learning Node for Multi-target Regression tasks that always uses learning models
+    for each target.
+
+    Parameters
+    ----------
+    stats
+        In regression tasks the node keeps a `utils.VectorDict` with instances of
+        `stats.Var` to estimate the targets' statistics.
+    depth
+        The depth of the node.
+    splitter
+        The numeric attribute observer algorithm used to monitor target statistics
+        and perform split attempts.
+    leaf_models
+        A dictionary composed of target identifiers and their respective predictive models.
+    kwargs
+        Other parameters passed to the learning node.
+    """
+
+    def __init__(self, stats, depth, splitter, leaf_models, **kwargs):
+        super().__init__(stats, depth, splitter, **kwargs)
+        self._leaf_models = leaf_models
+        self._model_supports_weights = {}
+        if self._leaf_models:
+            for t in self._leaf_models:
+                sign = inspect.signature(self._leaf_models[t].learn_one).parameters
+                self._model_supports_weights[t] = "sample_weight" in sign or "w" in sign
+
+    def learn_one(self, x, y, *, sample_weight=1.0, tree=None):
+        super().learn_one(x, y, sample_weight=sample_weight, tree=tree)
+
+        for target_id, y_ in y.items():
+            try:
+                model = self._leaf_models[target_id]
+            except KeyError:
+                if isinstance(tree.leaf_model, dict):
+                    if target_id in tree.leaf_model:
+                        self._leaf_models[target_id] = deepcopy(
+                            tree.leaf_model[target_id]
+                        )
+                    else:
+                        # Pick the first available model in case not all the targets' models
+                        # are defined
+                        self._leaf_models[target_id] = deepcopy(
+                            next(iter(self._leaf_models.values()))
+                        )
+                    model = self._leaf_models[target_id]
+                else:
+                    self._leaf_models[target_id] = deepcopy(tree.leaf_model)
+                    model = self._leaf_models[target_id]
+                sign = inspect.signature(model.learn_one).parameters
+                self._model_supports_weights[target_id] = (
+                    "sample_weight" in sign or "w" in sign
+                )
+
+            # Now the proper training
+            if self._model_supports_weights[target_id]:
+                model.learn_one(x, y_, sample_weight)
+            else:
+                for _ in range(int(sample_weight)):
+                    model.learn_one(x, y_)
+
+    def prediction(self, x, *, tree=None):
+        return {
+            t: self._leaf_models[t].predict_one(x) if t in self._leaf_models else 0.0
+            for t in tree.targets
+        }
+
+
+class LeafAdaptiveMultiTarget(LeafModelMultiTarget):
+    """Learning Node for multi-target regression tasks that dynamically selects between
+    predictors and might behave as a regression tree node or a model tree node, depending
+    on which predictor is the best one.
+
+    Parameters
+    ----------
+    stats
+        In regression tasks the node keeps a `utils.VectorDict` with instances of
+        `stats.Var` to estimate the targets' statistics.
+    depth
+        The depth of the node.
+    splitter
+        The numeric attribute observer algorithm used to monitor target statistics
+        and perform split attempts.
+    leaf_models
+        A dictionary composed of target identifiers and their respective predictive models.
+    kwargs
+        Other parameters passed to the learning node.
+    """
+
+    def __init__(self, stats, depth, splitter, leaf_models, **kwargs):
+        super().__init__(stats, depth, splitter, leaf_models, **kwargs)
+        self._fmse_mean = collections.defaultdict(float)
+        self._fmse_model = collections.defaultdict(float)
+
+    def learn_one(self, x, y, *, sample_weight=1.0, tree=None):
+        pred_mean = {
+            t: self.stats[t].mean.get() if t in self.stats else 0.0
+            for t in tree.targets
+        }
+        pred_model = super().prediction(x, tree=tree)
+
+        for t in tree.targets:  # Update the faded errors
+            self._fmse_mean[t] = (
+                tree.model_selector_decay * self._fmse_mean[t]
+                + (y[t] - pred_mean[t]) ** 2
+            )
+            self._fmse_model[t] = (
+                tree.model_selector_decay * self._fmse_model[t]
+                + (y[t] - pred_model[t]) ** 2
+            )
+
+        super().learn_one(x, y, sample_weight=sample_weight, tree=tree)
+
+    def prediction(self, x, *, tree=None):
+        pred = {}
+        for t in tree.targets:
+            if self._fmse_mean[t] < self._fmse_model[t]:  # Act as a regression tree
+                pred[t] = self.stats[t].mean.get() if t in self.stats else 0.0
+            else:  # Act as a model tree
+                try:
+                    pred[t] = self._leaf_models[t].predict_one(x)
+                except KeyError:
+                    pred[t] = 0.0
+        return pred
```

### Comparing `river-0.8.0/river/tree/nodes/leaf.py` & `river-0.9.0/river/tree/nodes/leaf.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,190 +1,190 @@
-import abc
-import copy
-import numbers
-import typing
-
-from ..base import Leaf
-from ..utils import BranchFactory
-
-
-class HTLeaf(Leaf, abc.ABC):
-    """Base leaf class to be used in Hoeffding Trees.
-
-    Parameters
-    ----------
-    stats
-        Target statistics (they differ in classification and regression tasks).
-    depth
-        The depth of the node
-    splitter
-        The numeric attribute observer algorithm used to monitor target statistics
-        and perform split attempts.
-    attributes
-        Other parameters passed to the learning node.
-    """
-
-    def __init__(self, stats, depth, splitter, **kwargs):
-        super().__init__(**kwargs)
-        self.stats = stats
-        self.depth = depth
-
-        self.splitter = splitter
-
-        self.splitters = {}
-        self._disabled_attrs = set()
-        self._last_split_attempt_at = self.total_weight
-
-    @property
-    @abc.abstractmethod
-    def total_weight(self) -> float:
-        pass
-
-    def is_active(self):
-        return self.splitters is not None
-
-    def activate(self):
-        if not self.is_active():
-            self.splitters = {}
-
-    def deactivate(self):
-        self.splitters = None
-
-    @property
-    def last_split_attempt_at(self) -> float:
-        """The weight seen at last split evaluation.
-
-        Returns
-        -------
-        Weight seen at last split evaluation.
-        """
-        return self._last_split_attempt_at
-
-    @last_split_attempt_at.setter
-    def last_split_attempt_at(self, weight):
-        """Set the weight seen at last split evaluation.
-
-        Parameters
-        ----------
-        weight
-            Weight seen at last split evaluation.
-        """
-        self._last_split_attempt_at = weight
-
-    @staticmethod
-    @abc.abstractmethod
-    def new_nominal_splitter():
-        pass
-
-    @abc.abstractmethod
-    def update_stats(self, y, sample_weight):
-        pass
-
-    def _iter_features(self, x) -> typing.Iterable:
-        """Determine how the input instance is looped through when updating the splitters.
-
-        Parameters
-        ----------
-        x
-            The input instance.
-        """
-        for att_id, att_val in x.items():
-            yield att_id, att_val
-
-    def update_splitters(self, x, y, sample_weight, nominal_attributes):
-        for att_id, att_val in self._iter_features(x):
-            if att_id in self._disabled_attrs:
-                continue
-
-            try:
-                splitter = self.splitters[att_id]
-            except KeyError:
-                if (
-                    nominal_attributes is not None and att_id in nominal_attributes
-                ) or not isinstance(att_val, numbers.Number):
-                    splitter = self.new_nominal_splitter()
-                else:
-                    splitter = copy.deepcopy(self.splitter)
-
-                self.splitters[att_id] = splitter
-            splitter.update(att_val, y, sample_weight)
-
-    def best_split_suggestions(self, criterion, tree) -> typing.List[BranchFactory]:
-        """Find possible split candidates.
-
-        Parameters
-        ----------
-        criterion
-            The splitting criterion to be used.
-        tree
-            Decision tree.
-
-        Returns
-        -------
-        Split candidates.
-        """
-        best_suggestions = []
-        pre_split_dist = self.stats
-        if tree.merit_preprune:
-            # Add null split as an option
-            null_split = BranchFactory()
-            best_suggestions.append(null_split)
-        for att_id, splitter in self.splitters.items():
-            best_suggestion = splitter.best_evaluated_split_suggestion(
-                criterion, pre_split_dist, att_id, tree.binary_split
-            )
-            best_suggestions.append(best_suggestion)
-
-        return best_suggestions
-
-    def disable_attribute(self, att_id):
-        """Disable an attribute observer.
-
-        Parameters
-        ----------
-        att_id
-            Attribute index.
-
-        """
-        if att_id in self.splitters:
-            del self.splitters[att_id]
-            self._disabled_attrs.add(att_id)
-
-    def learn_one(self, x, y, *, sample_weight=1.0, tree=None):
-        """Update the node with the provided sample.
-
-        Parameters
-        ----------
-        x
-            Sample attributes for updating the node.
-        y
-            Target value.
-        sample_weight
-            Sample weight.
-        tree
-            Tree to update.
-
-        Notes
-        -----
-        This base implementation defines the basic functioning of a learning node.
-        All classes overriding this method should include a call to `super().learn_one`
-        to guarantee the learning process happens consistently.
-        """
-        self.update_stats(y, sample_weight)
-        if self.is_active():
-            self.update_splitters(x, y, sample_weight, tree.nominal_attributes)
-
-    @abc.abstractmethod
-    def prediction(self, x, *, tree=None) -> dict:
-        pass
-
-    @abc.abstractmethod
-    def calculate_promise(self) -> int:
-        """Calculate node's promise.
-
-        Returns
-        -------
-        int
-            A small value indicates that the node has seen more samples of a
-            given class than the other classes.
-
-        """
+import abc
+import copy
+import numbers
+import typing
+
+from ..base import Leaf
+from ..utils import BranchFactory
+
+
+class HTLeaf(Leaf, abc.ABC):
+    """Base leaf class to be used in Hoeffding Trees.
+
+    Parameters
+    ----------
+    stats
+        Target statistics (they differ in classification and regression tasks).
+    depth
+        The depth of the node
+    splitter
+        The numeric attribute observer algorithm used to monitor target statistics
+        and perform split attempts.
+    attributes
+        Other parameters passed to the learning node.
+    """
+
+    def __init__(self, stats, depth, splitter, **kwargs):
+        super().__init__(**kwargs)
+        self.stats = stats
+        self.depth = depth
+
+        self.splitter = splitter
+
+        self.splitters = {}
+        self._disabled_attrs = set()
+        self._last_split_attempt_at = self.total_weight
+
+    @property
+    @abc.abstractmethod
+    def total_weight(self) -> float:
+        pass
+
+    def is_active(self):
+        return self.splitters is not None
+
+    def activate(self):
+        if not self.is_active():
+            self.splitters = {}
+
+    def deactivate(self):
+        self.splitters = None
+
+    @property
+    def last_split_attempt_at(self) -> float:
+        """The weight seen at last split evaluation.
+
+        Returns
+        -------
+        Weight seen at last split evaluation.
+        """
+        return self._last_split_attempt_at
+
+    @last_split_attempt_at.setter
+    def last_split_attempt_at(self, weight):
+        """Set the weight seen at last split evaluation.
+
+        Parameters
+        ----------
+        weight
+            Weight seen at last split evaluation.
+        """
+        self._last_split_attempt_at = weight
+
+    @staticmethod
+    @abc.abstractmethod
+    def new_nominal_splitter():
+        pass
+
+    @abc.abstractmethod
+    def update_stats(self, y, sample_weight):
+        pass
+
+    def _iter_features(self, x) -> typing.Iterable:
+        """Determine how the input instance is looped through when updating the splitters.
+
+        Parameters
+        ----------
+        x
+            The input instance.
+        """
+        for att_id, att_val in x.items():
+            yield att_id, att_val
+
+    def update_splitters(self, x, y, sample_weight, nominal_attributes):
+        for att_id, att_val in self._iter_features(x):
+            if att_id in self._disabled_attrs:
+                continue
+
+            try:
+                splitter = self.splitters[att_id]
+            except KeyError:
+                if (
+                    nominal_attributes is not None and att_id in nominal_attributes
+                ) or not isinstance(att_val, numbers.Number):
+                    splitter = self.new_nominal_splitter()
+                else:
+                    splitter = copy.deepcopy(self.splitter)
+
+                self.splitters[att_id] = splitter
+            splitter.update(att_val, y, sample_weight)
+
+    def best_split_suggestions(self, criterion, tree) -> typing.List[BranchFactory]:
+        """Find possible split candidates.
+
+        Parameters
+        ----------
+        criterion
+            The splitting criterion to be used.
+        tree
+            Decision tree.
+
+        Returns
+        -------
+        Split candidates.
+        """
+        best_suggestions = []
+        pre_split_dist = self.stats
+        if tree.merit_preprune:
+            # Add null split as an option
+            null_split = BranchFactory()
+            best_suggestions.append(null_split)
+        for att_id, splitter in self.splitters.items():
+            best_suggestion = splitter.best_evaluated_split_suggestion(
+                criterion, pre_split_dist, att_id, tree.binary_split
+            )
+            best_suggestions.append(best_suggestion)
+
+        return best_suggestions
+
+    def disable_attribute(self, att_id):
+        """Disable an attribute observer.
+
+        Parameters
+        ----------
+        att_id
+            Attribute index.
+
+        """
+        if att_id in self.splitters:
+            del self.splitters[att_id]
+            self._disabled_attrs.add(att_id)
+
+    def learn_one(self, x, y, *, sample_weight=1.0, tree=None):
+        """Update the node with the provided sample.
+
+        Parameters
+        ----------
+        x
+            Sample attributes for updating the node.
+        y
+            Target value.
+        sample_weight
+            Sample weight.
+        tree
+            Tree to update.
+
+        Notes
+        -----
+        This base implementation defines the basic functioning of a learning node.
+        All classes overriding this method should include a call to `super().learn_one`
+        to guarantee the learning process happens consistently.
+        """
+        self.update_stats(y, sample_weight)
+        if self.is_active():
+            self.update_splitters(x, y, sample_weight, tree.nominal_attributes)
+
+    @abc.abstractmethod
+    def prediction(self, x, *, tree=None) -> dict:
+        pass
+
+    @abc.abstractmethod
+    def calculate_promise(self) -> int:
+        """Calculate node's promise.
+
+        Returns
+        -------
+        int
+            A small value indicates that the node has seen more samples of a
+            given class than the other classes.
+
+        """
```

### Comparing `river-0.8.0/river/tree/nodes/sgt_nodes.py` & `river-0.9.0/river/tree/nodes/sgt_nodes.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,246 +1,246 @@
-import collections
-import math
-import numbers
-import sys
-import typing
-from typing import Dict, Hashable, Optional, Union
-
-from river import stats
-from river.base.typing import FeatureName
-
-from ..base import Leaf
-from ..splitter.sgt_quantizer import DynamicQuantizer, StaticQuantizer
-from ..utils import BranchFactory, GradHess, GradHessMerit, GradHessStats
-from .branch import NominalMultiwayBranch, NumericBinaryBranch
-
-
-class SGTLeaf(Leaf):
-    """Leaf Node of the Stochastic Gradient Trees (SGT).
-
-    There is only one type of leaf in SGTs. It handles only gradient and hessian
-    information about the target. The tree handles target transformation and encoding.
-
-    Parameters
-    ----------
-    prediction
-        The initial prediction of the leaf.
-    depth
-        The depth of the leaf in the tree.
-    split_params
-        Parameters passed to the feature quantizers.
-    """
-
-    def __init__(self, prediction=0.0, depth=0, split_params=None):
-        super().__init__()
-        self._prediction = prediction
-        self.depth = depth
-
-        # Quantizer params are dynamically updated
-        self.split_params = (
-            split_params if split_params is not None else collections.defaultdict(dict)
-        )
-        self.last_split_attempt_at = 0
-
-        self._split_stats: Optional[
-            Dict[
-                FeatureName,
-                Union[Dict[Hashable, GradHessStats], DynamicQuantizer, StaticQuantizer],
-            ]
-        ] = {}
-        self._update_stats = GradHessStats()
-
-    def reset(self):
-        self._split_stats = {}
-        self._update_stats = GradHessStats()
-
-    @staticmethod
-    def is_categorical(idx, x_val, nominal_attributes):
-        return not isinstance(x_val, numbers.Number) or idx in nominal_attributes
-
-    def update(self, x: dict, gh: GradHess, sgt, w: float = 1.0):
-        for idx, x_val in x.items():
-            if self.is_categorical(idx, x_val, sgt.nominal_attributes):
-                # Update the set of nominal features
-                sgt.nominal_attributes.add(idx)
-                try:
-                    self._split_stats[idx][x_val].update(gh, w)
-                except KeyError:
-                    if idx not in self._split_stats:
-                        # Categorical features are treated with a simple dict structure
-                        self._split_stats[idx] = {}
-                    self._split_stats[idx][x_val] = GradHessStats()
-                    self._split_stats[idx][x_val].update(gh, w)
-            else:
-                try:
-                    self._split_stats[idx].update(x_val, gh, w)
-                except KeyError:
-                    # Create a new quantizer
-                    self._split_stats[idx] = sgt.feature_quantizer._set_params(
-                        self.split_params[idx]
-                    )
-                    self._split_stats[idx].update(x_val, gh, w)
-
-        self._update_stats.update(gh, w=w)
-
-    def prediction(self) -> float:
-        return self._prediction
-
-    def _eval_categorical_splits(
-        self, feature_idx, candidate, sgt
-    ) -> typing.Tuple[BranchFactory, bool]:
-        skip_candidate = True
-
-        # Nominal attribute has been already used in a previous split
-        if feature_idx in sgt._split_features:
-            return candidate, skip_candidate
-
-        skip_candidate = False
-        candidate.numerical_feature = False
-        candidate.merit.delta_pred = {}
-        all_dlms = stats.Var()
-
-        cat_collection = self._split_stats[feature_idx]
-        candidate.split_info = list(cat_collection.keys())
-        for category in cat_collection:
-            dp = self.delta_prediction(cat_collection[category].mean, sgt.lambda_value)
-
-            dlms = cat_collection[category].delta_loss_mean_var(dp)
-            candidate.merit.delta_pred[category] = dp
-
-            all_dlms += dlms
-
-        candidate.merit.loss_mean = (
-            all_dlms.mean.get() + len(cat_collection) * sgt.gamma / self.total_weight
-        )
-        candidate.merit.loss_var = all_dlms.get()
-
-        return candidate, skip_candidate
-
-    def _eval_numerical_splits(
-        self, feature_idx, candidate, sgt
-    ) -> typing.Tuple[BranchFactory, bool]:
-        skip_candidate = True
-        quantizer = self._split_stats[feature_idx]
-
-        # Get updated quantizer params
-        self.split_params[feature_idx].update(quantizer._get_params())
-
-        n_bins = len(quantizer)
-        if n_bins == 1:  # Insufficient number of bins to perform splits
-            return candidate, skip_candidate
-
-        skip_candidate = False
-        candidate.merit.loss_mean = math.inf
-        candidate.merit.delta_pred = {}
-
-        # Auxiliary gradient and hessian statistics
-        left_ghs = GradHessStats()
-        left_dlms = stats.Var()
-        for thresh, ghs in quantizer:
-            left_ghs += ghs
-            left_delta_pred = self.delta_prediction(left_ghs.mean, sgt.lambda_value)
-            left_dlms += left_ghs.delta_loss_mean_var(left_delta_pred)
-
-            right_ghs = self._update_stats - left_ghs
-            right_delta_pred = self.delta_prediction(right_ghs.mean, sgt.lambda_value)
-            right_dlms = right_ghs.delta_loss_mean_var(right_delta_pred)
-
-            all_dlms = left_dlms + right_dlms
-
-            loss_mean = all_dlms.mean.get()
-            loss_var = all_dlms.get()
-
-            if loss_mean < candidate.merit.loss_mean:
-                candidate.merit.loss_mean = (
-                    loss_mean + 2.0 * sgt.gamma / self.total_weight
-                )
-                candidate.merit.loss_var = loss_var
-                candidate.merit.delta_pred[0] = left_delta_pred
-                candidate.merit.delta_pred[1] = right_delta_pred
-
-                candidate.split_info = thresh
-
-        return candidate, skip_candidate
-
-    def find_best_split(self, sgt) -> BranchFactory:
-        best_split = BranchFactory()
-        best_split.merit = GradHessMerit()
-
-        # Null split: update the prediction using the new gradient information
-        best_split.merit.delta_pred = self.delta_prediction(
-            self._update_stats.mean, sgt.lambda_value
-        )
-        dlms = self._update_stats.delta_loss_mean_var(best_split.merit.delta_pred)
-        best_split.merit.loss_mean = dlms.mean.get()
-        best_split.merit.loss_var = dlms.get()
-
-        for feature_idx in self._split_stats:
-            candidate = BranchFactory()
-            candidate.merit = GradHessMerit()
-            candidate.feature = feature_idx
-
-            if feature_idx in sgt.nominal_attributes:
-                candidate, skip_candidate = self._eval_categorical_splits(
-                    feature_idx, candidate, sgt
-                )
-            else:  # Numerical features
-                candidate, skip_candidate = self._eval_numerical_splits(
-                    feature_idx, candidate, sgt
-                )
-
-            if skip_candidate:
-                continue
-
-            if candidate.merit.loss_mean < best_split.merit.loss_mean:
-                best_split = candidate
-
-        return best_split
-
-    def apply_split(self, split, p_node, p_branch, sgt):
-        # Null split: update tree prediction and reset learning node
-        if split.feature is None:
-            self._prediction += split.merit.delta_pred
-            sgt._n_node_updates += 1
-            self.reset()
-            return
-
-        sgt._n_splits += 1
-        sgt._split_features.add(split.feature)
-
-        branch = (
-            NumericBinaryBranch if split.numerical_feature else NominalMultiwayBranch
-        )
-        child_depth = self.depth + 1
-        leaves = tuple(
-            SGTLeaf(
-                self._prediction + delta_pred,
-                depth=child_depth,
-                split_params=self.split_params.copy(),
-            )
-            for delta_pred in split.merit.delta_pred.values()
-        )
-
-        new_split = split.assemble(
-            branch, self.split_params.copy(), self.depth, *leaves
-        )
-
-        if p_branch is None:
-            sgt._root = new_split
-        else:
-            p_node.children[p_branch] = new_split
-
-    @property
-    def total_weight(self) -> float:
-        return self._update_stats.total_weight
-
-    @property
-    def update_stats(self):
-        return self._update_stats
-
-    @staticmethod
-    def delta_prediction(gh: GradHess, lambda_value: float):
-        # Add small constant value to avoid division by zero
-        return -gh.gradient / (gh.hessian + sys.float_info.min + lambda_value)
-
-    def __repr__(self):
-        return str(self.prediction())
+import collections
+import math
+import numbers
+import sys
+import typing
+from typing import Dict, Hashable, Optional, Union
+
+from river import stats
+from river.base.typing import FeatureName
+
+from ..base import Leaf
+from ..splitter.sgt_quantizer import DynamicQuantizer, StaticQuantizer
+from ..utils import BranchFactory, GradHess, GradHessMerit, GradHessStats
+from .branch import NominalMultiwayBranch, NumericBinaryBranch
+
+
+class SGTLeaf(Leaf):
+    """Leaf Node of the Stochastic Gradient Trees (SGT).
+
+    There is only one type of leaf in SGTs. It handles only gradient and hessian
+    information about the target. The tree handles target transformation and encoding.
+
+    Parameters
+    ----------
+    prediction
+        The initial prediction of the leaf.
+    depth
+        The depth of the leaf in the tree.
+    split_params
+        Parameters passed to the feature quantizers.
+    """
+
+    def __init__(self, prediction=0.0, depth=0, split_params=None):
+        super().__init__()
+        self._prediction = prediction
+        self.depth = depth
+
+        # Quantizer params are dynamically updated
+        self.split_params = (
+            split_params if split_params is not None else collections.defaultdict(dict)
+        )
+        self.last_split_attempt_at = 0
+
+        self._split_stats: Optional[
+            Dict[
+                FeatureName,
+                Union[Dict[Hashable, GradHessStats], DynamicQuantizer, StaticQuantizer],
+            ]
+        ] = {}
+        self._update_stats = GradHessStats()
+
+    def reset(self):
+        self._split_stats = {}
+        self._update_stats = GradHessStats()
+
+    @staticmethod
+    def is_categorical(idx, x_val, nominal_attributes):
+        return not isinstance(x_val, numbers.Number) or idx in nominal_attributes
+
+    def update(self, x: dict, gh: GradHess, sgt, w: float = 1.0):
+        for idx, x_val in x.items():
+            if self.is_categorical(idx, x_val, sgt.nominal_attributes):
+                # Update the set of nominal features
+                sgt.nominal_attributes.add(idx)
+                try:
+                    self._split_stats[idx][x_val].update(gh, w)
+                except KeyError:
+                    if idx not in self._split_stats:
+                        # Categorical features are treated with a simple dict structure
+                        self._split_stats[idx] = {}
+                    self._split_stats[idx][x_val] = GradHessStats()
+                    self._split_stats[idx][x_val].update(gh, w)
+            else:
+                try:
+                    self._split_stats[idx].update(x_val, gh, w)
+                except KeyError:
+                    # Create a new quantizer
+                    self._split_stats[idx] = sgt.feature_quantizer._set_params(
+                        self.split_params[idx]
+                    )
+                    self._split_stats[idx].update(x_val, gh, w)
+
+        self._update_stats.update(gh, w=w)
+
+    def prediction(self) -> float:
+        return self._prediction
+
+    def _eval_categorical_splits(
+        self, feature_idx, candidate, sgt
+    ) -> typing.Tuple[BranchFactory, bool]:
+        skip_candidate = True
+
+        # Nominal attribute has been already used in a previous split
+        if feature_idx in sgt._split_features:
+            return candidate, skip_candidate
+
+        skip_candidate = False
+        candidate.numerical_feature = False
+        candidate.merit.delta_pred = {}
+        all_dlms = stats.Var()
+
+        cat_collection = self._split_stats[feature_idx]
+        candidate.split_info = list(cat_collection.keys())
+        for category in cat_collection:
+            dp = self.delta_prediction(cat_collection[category].mean, sgt.lambda_value)
+
+            dlms = cat_collection[category].delta_loss_mean_var(dp)
+            candidate.merit.delta_pred[category] = dp
+
+            all_dlms += dlms
+
+        candidate.merit.loss_mean = (
+            all_dlms.mean.get() + len(cat_collection) * sgt.gamma / self.total_weight
+        )
+        candidate.merit.loss_var = all_dlms.get()
+
+        return candidate, skip_candidate
+
+    def _eval_numerical_splits(
+        self, feature_idx, candidate, sgt
+    ) -> typing.Tuple[BranchFactory, bool]:
+        skip_candidate = True
+        quantizer = self._split_stats[feature_idx]
+
+        # Get updated quantizer params
+        self.split_params[feature_idx].update(quantizer._get_params())
+
+        n_bins = len(quantizer)
+        if n_bins == 1:  # Insufficient number of bins to perform splits
+            return candidate, skip_candidate
+
+        skip_candidate = False
+        candidate.merit.loss_mean = math.inf
+        candidate.merit.delta_pred = {}
+
+        # Auxiliary gradient and hessian statistics
+        left_ghs = GradHessStats()
+        left_dlms = stats.Var()
+        for thresh, ghs in quantizer:
+            left_ghs += ghs
+            left_delta_pred = self.delta_prediction(left_ghs.mean, sgt.lambda_value)
+            left_dlms += left_ghs.delta_loss_mean_var(left_delta_pred)
+
+            right_ghs = self._update_stats - left_ghs
+            right_delta_pred = self.delta_prediction(right_ghs.mean, sgt.lambda_value)
+            right_dlms = right_ghs.delta_loss_mean_var(right_delta_pred)
+
+            all_dlms = left_dlms + right_dlms
+
+            loss_mean = all_dlms.mean.get()
+            loss_var = all_dlms.get()
+
+            if loss_mean < candidate.merit.loss_mean:
+                candidate.merit.loss_mean = (
+                    loss_mean + 2.0 * sgt.gamma / self.total_weight
+                )
+                candidate.merit.loss_var = loss_var
+                candidate.merit.delta_pred[0] = left_delta_pred
+                candidate.merit.delta_pred[1] = right_delta_pred
+
+                candidate.split_info = thresh
+
+        return candidate, skip_candidate
+
+    def find_best_split(self, sgt) -> BranchFactory:
+        best_split = BranchFactory()
+        best_split.merit = GradHessMerit()
+
+        # Null split: update the prediction using the new gradient information
+        best_split.merit.delta_pred = self.delta_prediction(
+            self._update_stats.mean, sgt.lambda_value
+        )
+        dlms = self._update_stats.delta_loss_mean_var(best_split.merit.delta_pred)
+        best_split.merit.loss_mean = dlms.mean.get()
+        best_split.merit.loss_var = dlms.get()
+
+        for feature_idx in self._split_stats:
+            candidate = BranchFactory()
+            candidate.merit = GradHessMerit()
+            candidate.feature = feature_idx
+
+            if feature_idx in sgt.nominal_attributes:
+                candidate, skip_candidate = self._eval_categorical_splits(
+                    feature_idx, candidate, sgt
+                )
+            else:  # Numerical features
+                candidate, skip_candidate = self._eval_numerical_splits(
+                    feature_idx, candidate, sgt
+                )
+
+            if skip_candidate:
+                continue
+
+            if candidate.merit.loss_mean < best_split.merit.loss_mean:
+                best_split = candidate
+
+        return best_split
+
+    def apply_split(self, split, p_node, p_branch, sgt):
+        # Null split: update tree prediction and reset learning node
+        if split.feature is None:
+            self._prediction += split.merit.delta_pred
+            sgt._n_node_updates += 1
+            self.reset()
+            return
+
+        sgt._n_splits += 1
+        sgt._split_features.add(split.feature)
+
+        branch = (
+            NumericBinaryBranch if split.numerical_feature else NominalMultiwayBranch
+        )
+        child_depth = self.depth + 1
+        leaves = tuple(
+            SGTLeaf(
+                self._prediction + delta_pred,
+                depth=child_depth,
+                split_params=self.split_params.copy(),
+            )
+            for delta_pred in split.merit.delta_pred.values()
+        )
+
+        new_split = split.assemble(
+            branch, self.split_params.copy(), self.depth, *leaves
+        )
+
+        if p_branch is None:
+            sgt._root = new_split
+        else:
+            p_node.children[p_branch] = new_split
+
+    @property
+    def total_weight(self) -> float:
+        return self._update_stats.total_weight
+
+    @property
+    def update_stats(self):
+        return self._update_stats
+
+    @staticmethod
+    def delta_prediction(gh: GradHess, lambda_value: float):
+        # Add small constant value to avoid division by zero
+        return -gh.gradient / (gh.hessian + sys.float_info.min + lambda_value)
+
+    def __repr__(self):
+        return str(self.prediction())
```

### Comparing `river-0.8.0/river/tree/split_criterion/base.py` & `river-0.9.0/river/tree/split_criterion/base.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,46 +1,46 @@
-import abc
-
-
-class SplitCriterion(abc.ABC):
-    """SplitCriterion
-
-    Abstract class for computing splitting criteria with respect to distributions of class values.
-    The split criterion is used as a parameter on decision trees and decision stumps.
-
-    This class should not me instantiated, as none of its methods are implemented.
-
-    """
-
-    def __init__(self):
-        super().__init__()
-
-    @abc.abstractmethod
-    def merit_of_split(self, pre_split_dist, post_split_dist):
-        """Compute the merit of splitting for a given distribution before the split and after it.
-
-        Parameters
-        ----------
-        pre_split_dist
-            The target statistics before the split.
-        post_split_dist
-            the target statistics after the split.
-
-        Returns
-        -------
-        Value of the merit of splitting
-        """
-
-    @staticmethod
-    @abc.abstractmethod
-    def range_of_merit(pre_split_dist):
-        """Compute the range of splitting merit.
-
-        Parameters
-        ----------
-        pre_split_dist
-            The target statistics before the split.
-
-        Returns
-        -------
-        Value of the range of splitting merit
-        """
+import abc
+
+
+class SplitCriterion(abc.ABC):
+    """SplitCriterion
+
+    Abstract class for computing splitting criteria with respect to distributions of class values.
+    The split criterion is used as a parameter on decision trees and decision stumps.
+
+    This class should not me instantiated, as none of its methods are implemented.
+
+    """
+
+    def __init__(self):
+        super().__init__()
+
+    @abc.abstractmethod
+    def merit_of_split(self, pre_split_dist, post_split_dist):
+        """Compute the merit of splitting for a given distribution before the split and after it.
+
+        Parameters
+        ----------
+        pre_split_dist
+            The target statistics before the split.
+        post_split_dist
+            the target statistics after the split.
+
+        Returns
+        -------
+        Value of the merit of splitting
+        """
+
+    @staticmethod
+    @abc.abstractmethod
+    def range_of_merit(pre_split_dist):
+        """Compute the range of splitting merit.
+
+        Parameters
+        ----------
+        pre_split_dist
+            The target statistics before the split.
+
+        Returns
+        -------
+        Value of the range of splitting merit
+        """
```

### Comparing `river-0.8.0/river/tree/split_criterion/info_gain_split_criterion.py` & `river-0.9.0/river/tree/split_criterion/info_gain_split_criterion.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,86 +1,86 @@
-import math
-
-from .base import SplitCriterion
-
-
-class InfoGainSplitCriterion(SplitCriterion):
-    """Information Gain split criterion.
-
-    A measure of how often a randomly chosen element from the set would be
-    incorrectly labeled if it was randomly labeled according to the
-    distribution of labels in the subset.
-
-    References
-    ----------
-    [Wikipedia entry](https://en.wikipedia.org/wiki/Decision_tree_learning#Information_gain)
-
-    """
-
-    def __init__(self, min_branch_frac_option=0.01):
-        super().__init__()
-        # Minimum fraction of weight required down at least two branches.
-        self.min_branch_frac_option = min_branch_frac_option
-
-    def merit_of_split(self, pre_split_dist, post_split_dist):
-        if (
-            self.num_subsets_greater_than_frac(
-                post_split_dist, self.min_branch_frac_option
-            )
-            < 2
-        ):
-            return -math.inf
-        return self.compute_entropy(pre_split_dist) - self.compute_entropy(
-            post_split_dist
-        )
-
-    @staticmethod
-    def range_of_merit(pre_split_dist):
-        num_classes = len(pre_split_dist)
-        num_classes = num_classes if num_classes > 2 else 2
-        return math.log2(num_classes)
-
-    def compute_entropy(self, dist):
-        if isinstance(dist, dict):
-            return self._compute_entropy_dict(dist)
-        elif isinstance(dist, list):
-            return self._compute_entropy_list(dist)
-
-    @staticmethod
-    def _compute_entropy_dict(dist):
-        entropy = 0.0
-        dis_sums = 0.0
-        for _, d in dist.items():
-            if d > 0.0:  # TODO: How small can d be before log2 overflows?
-                entropy -= d * math.log2(d)
-                dis_sums += d
-        return (
-            (entropy + dis_sums * math.log2(dis_sums)) / dis_sums
-            if dis_sums > 0.0
-            else 0.0
-        )
-
-    def _compute_entropy_list(self, dists):
-        total_weight = 0.0
-        dist_weights = [0.0] * len(dists)
-        for i in range(len(dists)):
-            dist_weights[i] = sum(dists[i].values())
-            total_weight += dist_weights[i]
-        entropy = 0.0
-        for i in range(len(dists)):
-            entropy += dist_weights[i] * self.compute_entropy(dists[i])
-        return entropy / total_weight
-
-    @staticmethod
-    def num_subsets_greater_than_frac(distributions, min_frac):
-        total_weight = 0.0
-        dist_sums = [0.0] * len(distributions)
-        for i in range(len(dist_sums)):
-            dist_sums[i] = sum(distributions[i].values())
-            total_weight += dist_sums[i]
-        num_greater = 0
-
-        if total_weight > 0:
-            for d in dist_sums:
-                if (d / total_weight) > min_frac:
-                    num_greater += 1
-        return num_greater
+import math
+
+from .base import SplitCriterion
+
+
+class InfoGainSplitCriterion(SplitCriterion):
+    """Information Gain split criterion.
+
+    A measure of how often a randomly chosen element from the set would be
+    incorrectly labeled if it was randomly labeled according to the
+    distribution of labels in the subset.
+
+    References
+    ----------
+    [Wikipedia entry](https://en.wikipedia.org/wiki/Decision_tree_learning#Information_gain)
+
+    """
+
+    def __init__(self, min_branch_frac_option=0.01):
+        super().__init__()
+        # Minimum fraction of weight required down at least two branches.
+        self.min_branch_frac_option = min_branch_frac_option
+
+    def merit_of_split(self, pre_split_dist, post_split_dist):
+        if (
+            self.num_subsets_greater_than_frac(
+                post_split_dist, self.min_branch_frac_option
+            )
+            < 2
+        ):
+            return -math.inf
+        return self.compute_entropy(pre_split_dist) - self.compute_entropy(
+            post_split_dist
+        )
+
+    @staticmethod
+    def range_of_merit(pre_split_dist):
+        num_classes = len(pre_split_dist)
+        num_classes = num_classes if num_classes > 2 else 2
+        return math.log2(num_classes)
+
+    def compute_entropy(self, dist):
+        if isinstance(dist, dict):
+            return self._compute_entropy_dict(dist)
+        elif isinstance(dist, list):
+            return self._compute_entropy_list(dist)
+
+    @staticmethod
+    def _compute_entropy_dict(dist):
+        entropy = 0.0
+        dis_sums = 0.0
+        for _, d in dist.items():
+            if d > 0.0:  # TODO: How small can d be before log2 overflows?
+                entropy -= d * math.log2(d)
+                dis_sums += d
+        return (
+            (entropy + dis_sums * math.log2(dis_sums)) / dis_sums
+            if dis_sums > 0.0
+            else 0.0
+        )
+
+    def _compute_entropy_list(self, dists):
+        total_weight = 0.0
+        dist_weights = [0.0] * len(dists)
+        for i in range(len(dists)):
+            dist_weights[i] = sum(dists[i].values())
+            total_weight += dist_weights[i]
+        entropy = 0.0
+        for i in range(len(dists)):
+            entropy += dist_weights[i] * self.compute_entropy(dists[i])
+        return entropy / total_weight
+
+    @staticmethod
+    def num_subsets_greater_than_frac(distributions, min_frac):
+        total_weight = 0.0
+        dist_sums = [0.0] * len(distributions)
+        for i in range(len(dist_sums)):
+            dist_sums[i] = sum(distributions[i].values())
+            total_weight += dist_sums[i]
+        num_greater = 0
+
+        if total_weight > 0:
+            for d in dist_sums:
+                if (d / total_weight) > min_frac:
+                    num_greater += 1
+        return num_greater
```

### Comparing `river-0.8.0/river/tree/split_criterion/intra_cluster_variance_reduction_split_criterion.py` & `river-0.9.0/river/tree/split_criterion/intra_cluster_variance_reduction_split_criterion.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,32 +1,32 @@
-from .variance_reduction_split_criterion import VarianceReductionSplitCriterion
-
-
-# This class extends VarianceReductionSplitCriterion since it just computes
-# the variance differently than its ancestor (considering multiple targets)
-class IntraClusterVarianceReductionSplitCriterion(VarianceReductionSplitCriterion):
-    def __init__(self, min_samples_split: int = 5):
-        super().__init__(min_samples_split)
-
-    def merit_of_split(self, pre_split_dist, post_split_dist):
-        icvr = 0.0
-        n = list(pre_split_dist.values())[0].mean.n
-
-        count = 0
-
-        for dist in post_split_dist:
-            n_i = list(dist.values())[0].mean.n
-            if n_i >= self.min_samples_split:
-                count += 1
-
-        if count == len(post_split_dist):
-            icvr = self.compute_var(pre_split_dist)
-            for dist in post_split_dist:
-                n_i = list(dist.values())[0].mean.n
-                icvr -= n_i / n * self.compute_var(dist)
-        return icvr
-
-    @staticmethod
-    def compute_var(dist):
-        icvr = [vr.get() for vr in dist.values()]
-        n = len(icvr)
-        return sum(icvr) / n if n > 0 else 0.0
+from .variance_reduction_split_criterion import VarianceReductionSplitCriterion
+
+
+# This class extends VarianceReductionSplitCriterion since it just computes
+# the variance differently than its ancestor (considering multiple targets)
+class IntraClusterVarianceReductionSplitCriterion(VarianceReductionSplitCriterion):
+    def __init__(self, min_samples_split: int = 5):
+        super().__init__(min_samples_split)
+
+    def merit_of_split(self, pre_split_dist, post_split_dist):
+        icvr = 0.0
+        n = list(pre_split_dist.values())[0].mean.n
+
+        count = 0
+
+        for dist in post_split_dist:
+            n_i = list(dist.values())[0].mean.n
+            if n_i >= self.min_samples_split:
+                count += 1
+
+        if count == len(post_split_dist):
+            icvr = self.compute_var(pre_split_dist)
+            for dist in post_split_dist:
+                n_i = list(dist.values())[0].mean.n
+                icvr -= n_i / n * self.compute_var(dist)
+        return icvr
+
+    @staticmethod
+    def compute_var(dist):
+        icvr = [vr.get() for vr in dist.values()]
+        n = len(icvr)
+        return sum(icvr) / n if n > 0 else 0.0
```

### Comparing `river-0.8.0/river/tree/split_criterion/variance_ratio_split_criterion.py` & `river-0.9.0/river/tree/split_criterion/variance_reduction_split_criterion.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,54 +1,46 @@
-from .base import SplitCriterion
-
-
-class VarianceRatioSplitCriterion(SplitCriterion):
-    """Variance Ratio split criterion.
-
-    Parameters
-    ----------
-    min_samples_split
-        The minimum number of samples per post split dist element.
-
-    """
-
-    def __init__(self, min_samples_split: int = 5):
-        super().__init__()
-        self.min_samples_split = min_samples_split
-
-    def merit_of_split(self, pre_split_dist, post_split_dist):
-        vr = 0
-        n = pre_split_dist.mean.n
-
-        count = 0
-        for i in range(len(post_split_dist)):
-            n_i = post_split_dist[i].mean.n
-            if n_i >= self.min_samples_split:
-                count += 1
-        if count == len(post_split_dist):
-            vr = 1
-            var = self.compute_var(pre_split_dist)
-            for i in range(len(post_split_dist)):
-                n_i = post_split_dist[i].mean.n
-                vr -= (n_i / n) * (self.compute_var(post_split_dist[i]) / var)
-        return vr
-
-    @staticmethod
-    def compute_var(dist):
-        return dist.get()
-
-    @staticmethod
-    def range_of_merit(pre_split_dist):
-        return 1.0
-
-    @staticmethod
-    def select_best_branch(children_stats):
-        n0 = children_stats[0].mean.n
-        n1 = children_stats[1].mean.n
-
-        n = n0 + n1
-
-        vr0 = (n0 / n) * VarianceRatioSplitCriterion.compute_var(children_stats[0])
-        vr1 = (n1 / n) * VarianceRatioSplitCriterion.compute_var(children_stats[1])
-
-        # Return the branch that most minimizes the variance
-        return 0 if vr0 <= vr1 else 1
+from .base import SplitCriterion
+
+
+class VarianceReductionSplitCriterion(SplitCriterion):
+    """Variance Reduction split criterion.
+
+    Often employed in cases where the target variable is continuous (regression tree),
+    meaning that use of many other metrics would first require discretization before being applied.
+
+    References
+    ----------
+    [Wikipedia entry](https://en.wikipedia.org/wiki/Decision_tree_learning#Variance_reduction)
+
+    """
+
+    def __init__(self, min_samples_split: int = 5):
+        super().__init__()
+        self.min_samples_split = min_samples_split
+
+    def merit_of_split(self, pre_split_dist, post_split_dist):
+        vr = 0.0
+        n = pre_split_dist.mean.n
+
+        count = 0
+        for i in range(len(post_split_dist)):
+            n_i = post_split_dist[i].mean.n
+            if n_i >= self.min_samples_split:
+                count += 1
+        if count == len(post_split_dist):
+
+            vr = self.compute_var(pre_split_dist)
+            for i in range(len(post_split_dist)):
+                n_i = post_split_dist[i].mean.n
+                vr -= n_i / n * self.compute_var(post_split_dist[i])
+        return vr
+
+    @staticmethod
+    def compute_var(dist):
+        return dist.get()
+
+    @staticmethod
+    def range_of_merit(pre_split_dist):
+        # The VR values are unbounded, but as we compare the ratio between the attributes' VRs
+        # the actual range is between 0 (the second best candidate has a merit of zero) and 1
+        # (both compared split candidates have the same merit).
+        return 1.0
```

### Comparing `river-0.8.0/river/tree/splitter/__init__.py` & `river-0.9.0/river/tree/splitter/__init__.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,37 +1,37 @@
-"""
-This module implements the Attribute Observers (AO) (or tree splitters) that are used by the
-Hoeffding Trees (HT). It also implements the feature quantizers (FQ) used by Stochastic Gradient
-Trees (SGT). AOs are a core aspect of the HTs construction, and might represent one of the major
-bottlenecks when building the trees. The same holds for SGTs and FQs. The correct choice and setup
-of a splitter might result in significant differences in the running time and memory usage of the
-incremental decision trees.
-
-AOs for classification and regression trees can be differentiated by using the property
-`is_target_class` (`True` for splitters designed to classification tasks). An error will be raised
-if one tries to use a classification splitter in a regression tree and vice-versa.
-Lastly, AOs cannot be used in SGT and FQs cannot be used in Hoeffding Trees. So, care must be taken
-when choosing the correct feature splitter.
-
-"""
-
-from .base import Quantizer, Splitter
-from .ebst_splitter import EBSTSplitter
-from .exhaustive_splitter import ExhaustiveSplitter
-from .gaussian_splitter import GaussianSplitter
-from .histogram_splitter import HistogramSplitter
-from .qo_splitter import QOSplitter
-from .sgt_quantizer import DynamicQuantizer, StaticQuantizer
-from .tebst_splitter import TEBSTSplitter
-
-__all__ = [
-    "DynamicQuantizer",
-    "EBSTSplitter",
-    "ExhaustiveSplitter",
-    "GaussianSplitter",
-    "HistogramSplitter",
-    "QOSplitter",
-    "Quantizer",
-    "Splitter",
-    "StaticQuantizer",
-    "TEBSTSplitter",
-]
+"""
+This module implements the Attribute Observers (AO) (or tree splitters) that are used by the
+Hoeffding Trees (HT). It also implements the feature quantizers (FQ) used by Stochastic Gradient
+Trees (SGT). AOs are a core aspect of the HTs construction, and might represent one of the major
+bottlenecks when building the trees. The same holds for SGTs and FQs. The correct choice and setup
+of a splitter might result in significant differences in the running time and memory usage of the
+incremental decision trees.
+
+AOs for classification and regression trees can be differentiated by using the property
+`is_target_class` (`True` for splitters designed to classification tasks). An error will be raised
+if one tries to use a classification splitter in a regression tree and vice-versa.
+Lastly, AOs cannot be used in SGT and FQs cannot be used in Hoeffding Trees. So, care must be taken
+when choosing the correct feature splitter.
+
+"""
+
+from .base import Quantizer, Splitter
+from .ebst_splitter import EBSTSplitter
+from .exhaustive_splitter import ExhaustiveSplitter
+from .gaussian_splitter import GaussianSplitter
+from .histogram_splitter import HistogramSplitter
+from .qo_splitter import QOSplitter
+from .sgt_quantizer import DynamicQuantizer, StaticQuantizer
+from .tebst_splitter import TEBSTSplitter
+
+__all__ = [
+    "DynamicQuantizer",
+    "EBSTSplitter",
+    "ExhaustiveSplitter",
+    "GaussianSplitter",
+    "HistogramSplitter",
+    "QOSplitter",
+    "Quantizer",
+    "Splitter",
+    "StaticQuantizer",
+    "TEBSTSplitter",
+]
```

### Comparing `river-0.8.0/river/tree/splitter/base.py` & `river-0.9.0/river/tree/splitter/base.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,117 +1,117 @@
-import abc
-import typing
-
-from river import base
-
-from ..split_criterion.base import SplitCriterion
-from ..utils import BranchFactory, GradHess, GradHessStats
-
-
-class Splitter(base.Estimator, abc.ABC):
-    """Base class for the tree splitters.
-
-    Each Attribute Observer (AO) or Splitter monitors one input feature and finds the best
-    split point for this attribute. AOs can also perform other tasks related to the monitored
-    feature, such as estimating its probability density function (classification case).
-
-    This class should not be instantiated, as none of its methods are implemented.
-    """
-
-    def __init__(self):
-        super().__init__()
-
-    @abc.abstractmethod
-    def update(self, att_val, target_val: base.typing.Target, sample_weight: float):
-        """Update statistics of this observer given an attribute value, its target value
-        and the weight of the instance observed.
-
-        Parameters
-        ----------
-        att_val
-            The value of the monitored attribute.
-        target_val
-            The target value.
-        sample_weight
-            The weight of the instance.
-        """
-
-    @abc.abstractmethod
-    def cond_proba(self, att_val, target_val: base.typing.ClfTarget) -> float:
-        """Get the probability for an attribute value given a class.
-
-        Parameters
-        ----------
-        att_val
-            The value of the attribute.
-        target_val
-            The target (class label) value.
-
-        Returns
-        -------
-            Probability for an attribute value given a class.
-        """
-
-    @abc.abstractmethod
-    def best_evaluated_split_suggestion(
-        self,
-        criterion: SplitCriterion,
-        pre_split_dist: typing.Union[typing.List, typing.Dict],
-        att_idx: base.typing.FeatureName,
-        binary_only: bool,
-    ) -> BranchFactory:
-        """Get the best split suggestion given a criterion and the target's statistics.
-
-        Parameters
-        ----------
-        criterion
-            The split criterion to use.
-        pre_split_dist
-            The target statistics before the split.
-        att_idx
-            The attribute index.
-        binary_only
-            True if only binary splits are allowed.
-
-        Returns
-        -------
-            Suggestion of the best attribute split.
-        """
-
-    @property
-    def is_numeric(self) -> bool:
-        """Determine whether or not the splitter works with numerical features."""
-        return True
-
-    @property
-    def is_target_class(self) -> bool:
-        """Check on which kind of learning task the splitter is designed to work.
-
-        If `True`, the splitter works with classification trees, otherwise it is designed for
-        regression trees.
-        """
-        return True
-
-
-class Quantizer(base.Estimator, abc.ABC):
-    """Base class for the feature quantizers used in Stochastic Gradient Trees[^1].
-
-    References
-    ----------
-    [^1]: Gouk, H., Pfahringer, B., & Frank, E. (2019, October). Stochastic Gradient Trees.
-    In Asian Conference on Machine Learning (pp. 1094-1109).
-    """
-
-    def __init__(self):
-        super().__init__()
-
-    @abc.abstractmethod
-    def __len__(self):
-        pass
-
-    @abc.abstractmethod
-    def update(self, x_val, gh: GradHess, w: float):
-        pass
-
-    @abc.abstractmethod
-    def __iter__(self) -> typing.Tuple[float, typing.Iterator[GradHessStats]]:
-        pass
+import abc
+import typing
+
+from river import base
+
+from ..split_criterion.base import SplitCriterion
+from ..utils import BranchFactory, GradHess, GradHessStats
+
+
+class Splitter(base.Estimator, abc.ABC):
+    """Base class for the tree splitters.
+
+    Each Attribute Observer (AO) or Splitter monitors one input feature and finds the best
+    split point for this attribute. AOs can also perform other tasks related to the monitored
+    feature, such as estimating its probability density function (classification case).
+
+    This class should not be instantiated, as none of its methods are implemented.
+    """
+
+    def __init__(self):
+        super().__init__()
+
+    @abc.abstractmethod
+    def update(self, att_val, target_val: base.typing.Target, sample_weight: float):
+        """Update statistics of this observer given an attribute value, its target value
+        and the weight of the instance observed.
+
+        Parameters
+        ----------
+        att_val
+            The value of the monitored attribute.
+        target_val
+            The target value.
+        sample_weight
+            The weight of the instance.
+        """
+
+    @abc.abstractmethod
+    def cond_proba(self, att_val, target_val: base.typing.ClfTarget) -> float:
+        """Get the probability for an attribute value given a class.
+
+        Parameters
+        ----------
+        att_val
+            The value of the attribute.
+        target_val
+            The target (class label) value.
+
+        Returns
+        -------
+            Probability for an attribute value given a class.
+        """
+
+    @abc.abstractmethod
+    def best_evaluated_split_suggestion(
+        self,
+        criterion: SplitCriterion,
+        pre_split_dist: typing.Union[typing.List, typing.Dict],
+        att_idx: base.typing.FeatureName,
+        binary_only: bool,
+    ) -> BranchFactory:
+        """Get the best split suggestion given a criterion and the target's statistics.
+
+        Parameters
+        ----------
+        criterion
+            The split criterion to use.
+        pre_split_dist
+            The target statistics before the split.
+        att_idx
+            The attribute index.
+        binary_only
+            True if only binary splits are allowed.
+
+        Returns
+        -------
+            Suggestion of the best attribute split.
+        """
+
+    @property
+    def is_numeric(self) -> bool:
+        """Determine whether or not the splitter works with numerical features."""
+        return True
+
+    @property
+    def is_target_class(self) -> bool:
+        """Check on which kind of learning task the splitter is designed to work.
+
+        If `True`, the splitter works with classification trees, otherwise it is designed for
+        regression trees.
+        """
+        return True
+
+
+class Quantizer(base.Estimator, abc.ABC):
+    """Base class for the feature quantizers used in Stochastic Gradient Trees[^1].
+
+    References
+    ----------
+    [^1]: Gouk, H., Pfahringer, B., & Frank, E. (2019, October). Stochastic Gradient Trees.
+    In Asian Conference on Machine Learning (pp. 1094-1109).
+    """
+
+    def __init__(self):
+        super().__init__()
+
+    @abc.abstractmethod
+    def __len__(self):
+        pass
+
+    @abc.abstractmethod
+    def update(self, x_val, gh: GradHess, w: float):
+        pass
+
+    @abc.abstractmethod
+    def __iter__(self) -> typing.Tuple[float, typing.Iterator[GradHessStats]]:
+        pass
```

### Comparing `river-0.8.0/river/tree/splitter/ebst_splitter.py` & `river-0.9.0/river/tree/splitter/ebst_splitter.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,295 +1,295 @@
-import functools
-import typing
-
-from river.stats import Var
-from river.utils import VectorDict
-
-from ..utils import BranchFactory
-from .base import Splitter
-
-
-class EBSTSplitter(Splitter):
-    """iSOUP-Tree's Extended Binary Search Tree (E-BST).
-
-    This class implements the Extended Binary Search Tree[^1] (E-BST)
-    structure, using the variant employed by Osojnik et al.[^2] in the
-    iSOUP-Tree algorithm. This structure is employed to observe the target
-    space distribution.
-
-    Proposed along with Fast Incremental Model Tree with Drift Detection[^1] (FIMT-DD), E-BST was
-    the first attribute observer (AO) proposed for incremental Hoeffding Tree regressors. This
-    AO works by storing all observations between splits in an extended binary search tree
-    structure. E-BST stores the input feature realizations and statistics of the target(s) that
-    enable calculating the split heuristic at any time. To alleviate time and memory costs, E-BST
-    implements a memory management routine, where the worst split candidates are pruned from the
-    binary tree.
-
-    In this variant, only the left branch statistics are stored and the complete split-enabling
-    statistics are calculated with an in-order traversal of the binary search tree.
-
-    References
-    ----------
-    [^1]: Ikonomovska, E., Gama, J., & Džeroski, S. (2011). Learning model trees from evolving
-        data streams. Data mining and knowledge discovery, 23(1), 128-168.
-    [^2]: [Osojnik, Aljaž. 2017. Structured output prediction on Data Streams
-    (Doctoral Dissertation)](http://kt.ijs.si/theses/phd_aljaz_osojnik.pdf)
-    """
-
-    def __init__(self):
-        super().__init__()
-        self._root = None
-
-    @property
-    def is_target_class(self) -> bool:
-        return False
-
-    def update(self, att_val, target_val, sample_weight):
-        if att_val is None:
-            return
-        else:
-            if self._root is None:
-                self._root = EBSTNode(att_val, target_val, sample_weight)
-            else:
-                self._root.insert_value(att_val, target_val, sample_weight)
-
-    def cond_proba(self, att_val, target_val):
-        """Not implemented in regression splitters."""
-        raise NotImplementedError
-
-    def best_evaluated_split_suggestion(
-        self, criterion, pre_split_dist, att_idx, binary_only=True
-    ):
-        candidate = BranchFactory()
-
-        if self._root is None:
-            return candidate
-
-        self._criterion = criterion
-        self._pre_split_dist = pre_split_dist
-        self._att_idx = att_idx
-
-        # Handles both single-target and multi-target tasks
-        if isinstance(pre_split_dist, VectorDict):
-            self._aux_estimator = VectorDict(default_factory=functools.partial(Var))
-        else:
-            self._aux_estimator = Var()
-
-        best_split = self._find_best_split(self._root, candidate)
-
-        # Delete auxiliary variables
-        del self._criterion
-        del self._pre_split_dist
-        del self._att_idx
-        del self._aux_estimator
-
-        return best_split
-
-    def _find_best_split(self, node, candidate):
-        if node._left is not None:
-            candidate = self._find_best_split(node._left, candidate)
-        # Left post split distribution
-        left_dist = node.estimator + self._aux_estimator
-
-        # The right split distribution is calculated as the difference between the total
-        # distribution (pre split distribution) and the left distribution
-        right_dist = self._pre_split_dist - left_dist
-
-        post_split_dists = [left_dist, right_dist]
-
-        merit = self._criterion.merit_of_split(self._pre_split_dist, post_split_dists)
-        if merit > candidate.merit:
-            candidate = BranchFactory(
-                merit, self._att_idx, node.att_val, post_split_dists
-            )
-
-        if node._right is not None:
-            self._aux_estimator += node.estimator
-
-            right_candidate = self._find_best_split(node._right, candidate)
-
-            if right_candidate.merit > candidate.merit:
-                candidate = right_candidate
-
-            self._aux_estimator -= node.estimator
-
-        return candidate
-
-    def remove_bad_splits(
-        self,
-        criterion,
-        last_check_ratio: float,
-        last_check_vr: float,
-        last_check_e: float,
-        pre_split_dist: typing.Union[typing.List, typing.Dict],
-    ):
-        """Remove bad splits.
-
-        Based on FIMT-DD's [^1] procedure to remove bad split candidates from the E-BST. This
-        mechanism is triggered every time a split attempt fails. The rationale is to remove
-        points whose split merit is much worse than the best candidate overall (for which the
-        growth decision already failed).
-
-        Let $m_1$ be the merit of the best split point and $m_2$ be the merit of the
-        second best split candidate. The ratio $r = m_2/m_1$ along with the Hoeffding bound
-        ($\\epsilon$) are used to decide upon creating a split. A split occurs when
-        $r < 1 - \\epsilon$. A split candidate, with merit $m_i$, is considered badr
-        if $m_i / m_1 < r - 2\\epsilon$. The rationale is the following: if the merit ratio
-        for this point is smaller than the lower bound of $r$, then the true merit of that
-        split relative to the best one is small. Hence, this candidate can be safely removed.
-
-        To avoid excessive and costly manipulations of the E-BST to update the stored statistics,
-        only the nodes whose children are all bad split points are pruned, as defined in [^1].
-
-        Parameters
-        ----------
-        criterion
-            The split criterion used by the regression tree.
-        last_check_ratio
-            The ratio between the merit of the second best split candidate and the merit of the
-            best split candidate observed in the last failed split attempt.
-        last_check_vr
-            The merit (variance reduction) of the best split candidate observed in the last
-            failed split attempt.
-        last_check_e
-            The Hoeffding bound value calculated in the last failed split attempt.
-        pre_split_dist
-            The complete statistics of the target observed in the leaf node.
-
-        References
-        ----------
-        [^1]: Ikonomovska, E., Gama, J., & Džeroski, S. (2011). Learning model trees from evolving
-        data streams. Data mining and knowledge discovery, 23(1), 128-168.
-        """
-
-        if self._root is None:
-            return
-
-        # Auxiliary variables
-        self._criterion = criterion
-        self._pre_split_dist = pre_split_dist
-        self._last_check_ratio = last_check_ratio
-        self._last_check_vr = last_check_vr
-        self._last_check_e = last_check_e
-
-        # Handles both single-target and multi-target tasks
-        if isinstance(pre_split_dist, VectorDict):
-            self._aux_estimator = VectorDict(default_factory=functools.partial(Var))
-        else:
-            self._aux_estimator = Var()
-
-        self._remove_bad_split_nodes(self._root)
-
-        # Delete auxiliary variables
-        del self._criterion
-        del self._pre_split_dist
-        del self._last_check_ratio
-        del self._last_check_vr
-        del self._last_check_e
-        del self._aux_estimator
-
-    def _remove_bad_split_nodes(self, current_node, parent=None, is_left_child=True):
-        is_bad = False
-
-        if current_node._left is not None:
-            is_bad = self._remove_bad_split_nodes(
-                current_node._left, current_node, True
-            )
-        else:  # Every leaf node is potentially a bad candidate
-            is_bad = True
-
-        if is_bad:
-            if current_node._right is not None:
-                self._aux_estimator += current_node.estimator
-
-                is_bad = self._remove_bad_split_nodes(
-                    current_node._right, current_node, False
-                )
-
-                self._aux_estimator -= current_node.estimator
-            else:  # Every leaf node is potentially a bad candidate
-                is_bad = True
-
-        if is_bad:
-            # Left post split distribution
-            left_dist = current_node.estimator + self._aux_estimator
-
-            # The right split distribution is calculated as the difference between the total
-            # distribution (pre split distribution) and the left distribution
-            right_dist = self._pre_split_dist - left_dist
-
-            post_split_dists = [left_dist, right_dist]
-            merit = self._criterion.merit_of_split(
-                self._pre_split_dist, post_split_dists
-            )
-            if (merit / self._last_check_vr) < (
-                self._last_check_ratio - 2 * self._last_check_e
-            ):
-                # Remove children nodes
-                current_node._left = None
-                current_node._right = None
-
-                # Root node
-                if parent is None:
-                    self._root = None
-                else:  # Root node
-                    # Remove bad candidate
-                    if is_left_child:
-                        parent._left = None
-                    else:
-                        parent._right = None
-                return True
-
-        return False
-
-
-class EBSTNode:
-    def __init__(self, att_val, target_val, sample_weight):
-        self.att_val = att_val
-
-        if isinstance(target_val, dict):
-            self.estimator = VectorDict(default_factory=functools.partial(Var))
-            self._update_estimator = self._update_estimator_multivariate
-        else:
-            self.estimator = Var()
-            self._update_estimator = self._update_estimator_univariate
-
-        self._update_estimator(self, target_val, sample_weight)
-
-        self._left = None
-        self._right = None
-
-    @staticmethod
-    def _update_estimator_univariate(node, target, sample_weight):
-        node.estimator.update(target, sample_weight)
-
-    @staticmethod
-    def _update_estimator_multivariate(node, target, sample_weight):
-        for t in target:
-            node.estimator[t].update(target[t], sample_weight)
-
-    # Incremental implementation of the insert method. Avoiding unnecessary
-    # stack tracing must decrease memory costs
-    def insert_value(self, att_val, target_val, sample_weight):
-        current = self
-        antecedent = None
-        is_right = False
-
-        while current is not None:
-            antecedent = current
-            if att_val == current.att_val:
-                self._update_estimator(current, target_val, sample_weight)
-                return
-            elif att_val < current.att_val:
-                self._update_estimator(current, target_val, sample_weight)
-
-                current = current._left
-                is_right = False
-            else:
-                current = current._right
-                is_right = True
-
-        # Value was not yet added to the tree
-        if is_right:
-            antecedent._right = EBSTNode(att_val, target_val, sample_weight)
-        else:
-            antecedent._left = EBSTNode(att_val, target_val, sample_weight)
+import functools
+import typing
+
+from river.stats import Var
+from river.utils import VectorDict
+
+from ..utils import BranchFactory
+from .base import Splitter
+
+
+class EBSTSplitter(Splitter):
+    """iSOUP-Tree's Extended Binary Search Tree (E-BST).
+
+    This class implements the Extended Binary Search Tree[^1] (E-BST)
+    structure, using the variant employed by Osojnik et al.[^2] in the
+    iSOUP-Tree algorithm. This structure is employed to observe the target
+    space distribution.
+
+    Proposed along with Fast Incremental Model Tree with Drift Detection[^1] (FIMT-DD), E-BST was
+    the first attribute observer (AO) proposed for incremental Hoeffding Tree regressors. This
+    AO works by storing all observations between splits in an extended binary search tree
+    structure. E-BST stores the input feature realizations and statistics of the target(s) that
+    enable calculating the split heuristic at any time. To alleviate time and memory costs, E-BST
+    implements a memory management routine, where the worst split candidates are pruned from the
+    binary tree.
+
+    In this variant, only the left branch statistics are stored and the complete split-enabling
+    statistics are calculated with an in-order traversal of the binary search tree.
+
+    References
+    ----------
+    [^1]: Ikonomovska, E., Gama, J., & Džeroski, S. (2011). Learning model trees from evolving
+        data streams. Data mining and knowledge discovery, 23(1), 128-168.
+    [^2]: [Osojnik, Aljaž. 2017. Structured output prediction on Data Streams
+    (Doctoral Dissertation)](http://kt.ijs.si/theses/phd_aljaz_osojnik.pdf)
+    """
+
+    def __init__(self):
+        super().__init__()
+        self._root = None
+
+    @property
+    def is_target_class(self) -> bool:
+        return False
+
+    def update(self, att_val, target_val, sample_weight):
+        if att_val is None:
+            return
+        else:
+            if self._root is None:
+                self._root = EBSTNode(att_val, target_val, sample_weight)
+            else:
+                self._root.insert_value(att_val, target_val, sample_weight)
+
+    def cond_proba(self, att_val, target_val):
+        """Not implemented in regression splitters."""
+        raise NotImplementedError
+
+    def best_evaluated_split_suggestion(
+        self, criterion, pre_split_dist, att_idx, binary_only=True
+    ):
+        candidate = BranchFactory()
+
+        if self._root is None:
+            return candidate
+
+        self._criterion = criterion
+        self._pre_split_dist = pre_split_dist
+        self._att_idx = att_idx
+
+        # Handles both single-target and multi-target tasks
+        if isinstance(pre_split_dist, VectorDict):
+            self._aux_estimator = VectorDict(default_factory=functools.partial(Var))
+        else:
+            self._aux_estimator = Var()
+
+        best_split = self._find_best_split(self._root, candidate)
+
+        # Delete auxiliary variables
+        del self._criterion
+        del self._pre_split_dist
+        del self._att_idx
+        del self._aux_estimator
+
+        return best_split
+
+    def _find_best_split(self, node, candidate):
+        if node._left is not None:
+            candidate = self._find_best_split(node._left, candidate)
+        # Left post split distribution
+        left_dist = node.estimator + self._aux_estimator
+
+        # The right split distribution is calculated as the difference between the total
+        # distribution (pre split distribution) and the left distribution
+        right_dist = self._pre_split_dist - left_dist
+
+        post_split_dists = [left_dist, right_dist]
+
+        merit = self._criterion.merit_of_split(self._pre_split_dist, post_split_dists)
+        if merit > candidate.merit:
+            candidate = BranchFactory(
+                merit, self._att_idx, node.att_val, post_split_dists
+            )
+
+        if node._right is not None:
+            self._aux_estimator += node.estimator
+
+            right_candidate = self._find_best_split(node._right, candidate)
+
+            if right_candidate.merit > candidate.merit:
+                candidate = right_candidate
+
+            self._aux_estimator -= node.estimator
+
+        return candidate
+
+    def remove_bad_splits(
+        self,
+        criterion,
+        last_check_ratio: float,
+        last_check_vr: float,
+        last_check_e: float,
+        pre_split_dist: typing.Union[typing.List, typing.Dict],
+    ):
+        """Remove bad splits.
+
+        Based on FIMT-DD's [^1] procedure to remove bad split candidates from the E-BST. This
+        mechanism is triggered every time a split attempt fails. The rationale is to remove
+        points whose split merit is much worse than the best candidate overall (for which the
+        growth decision already failed).
+
+        Let $m_1$ be the merit of the best split point and $m_2$ be the merit of the
+        second best split candidate. The ratio $r = m_2/m_1$ along with the Hoeffding bound
+        ($\\epsilon$) are used to decide upon creating a split. A split occurs when
+        $r < 1 - \\epsilon$. A split candidate, with merit $m_i$, is considered badr
+        if $m_i / m_1 < r - 2\\epsilon$. The rationale is the following: if the merit ratio
+        for this point is smaller than the lower bound of $r$, then the true merit of that
+        split relative to the best one is small. Hence, this candidate can be safely removed.
+
+        To avoid excessive and costly manipulations of the E-BST to update the stored statistics,
+        only the nodes whose children are all bad split points are pruned, as defined in [^1].
+
+        Parameters
+        ----------
+        criterion
+            The split criterion used by the regression tree.
+        last_check_ratio
+            The ratio between the merit of the second best split candidate and the merit of the
+            best split candidate observed in the last failed split attempt.
+        last_check_vr
+            The merit (variance reduction) of the best split candidate observed in the last
+            failed split attempt.
+        last_check_e
+            The Hoeffding bound value calculated in the last failed split attempt.
+        pre_split_dist
+            The complete statistics of the target observed in the leaf node.
+
+        References
+        ----------
+        [^1]: Ikonomovska, E., Gama, J., & Džeroski, S. (2011). Learning model trees from evolving
+        data streams. Data mining and knowledge discovery, 23(1), 128-168.
+        """
+
+        if self._root is None:
+            return
+
+        # Auxiliary variables
+        self._criterion = criterion
+        self._pre_split_dist = pre_split_dist
+        self._last_check_ratio = last_check_ratio
+        self._last_check_vr = last_check_vr
+        self._last_check_e = last_check_e
+
+        # Handles both single-target and multi-target tasks
+        if isinstance(pre_split_dist, VectorDict):
+            self._aux_estimator = VectorDict(default_factory=functools.partial(Var))
+        else:
+            self._aux_estimator = Var()
+
+        self._remove_bad_split_nodes(self._root)
+
+        # Delete auxiliary variables
+        del self._criterion
+        del self._pre_split_dist
+        del self._last_check_ratio
+        del self._last_check_vr
+        del self._last_check_e
+        del self._aux_estimator
+
+    def _remove_bad_split_nodes(self, current_node, parent=None, is_left_child=True):
+        is_bad = False
+
+        if current_node._left is not None:
+            is_bad = self._remove_bad_split_nodes(
+                current_node._left, current_node, True
+            )
+        else:  # Every leaf node is potentially a bad candidate
+            is_bad = True
+
+        if is_bad:
+            if current_node._right is not None:
+                self._aux_estimator += current_node.estimator
+
+                is_bad = self._remove_bad_split_nodes(
+                    current_node._right, current_node, False
+                )
+
+                self._aux_estimator -= current_node.estimator
+            else:  # Every leaf node is potentially a bad candidate
+                is_bad = True
+
+        if is_bad:
+            # Left post split distribution
+            left_dist = current_node.estimator + self._aux_estimator
+
+            # The right split distribution is calculated as the difference between the total
+            # distribution (pre split distribution) and the left distribution
+            right_dist = self._pre_split_dist - left_dist
+
+            post_split_dists = [left_dist, right_dist]
+            merit = self._criterion.merit_of_split(
+                self._pre_split_dist, post_split_dists
+            )
+            if (merit / self._last_check_vr) < (
+                self._last_check_ratio - 2 * self._last_check_e
+            ):
+                # Remove children nodes
+                current_node._left = None
+                current_node._right = None
+
+                # Root node
+                if parent is None:
+                    self._root = None
+                else:  # Root node
+                    # Remove bad candidate
+                    if is_left_child:
+                        parent._left = None
+                    else:
+                        parent._right = None
+                return True
+
+        return False
+
+
+class EBSTNode:
+    def __init__(self, att_val, target_val, sample_weight):
+        self.att_val = att_val
+
+        if isinstance(target_val, dict):
+            self.estimator = VectorDict(default_factory=functools.partial(Var))
+            self._update_estimator = self._update_estimator_multivariate
+        else:
+            self.estimator = Var()
+            self._update_estimator = self._update_estimator_univariate
+
+        self._update_estimator(self, target_val, sample_weight)
+
+        self._left = None
+        self._right = None
+
+    @staticmethod
+    def _update_estimator_univariate(node, target, sample_weight):
+        node.estimator.update(target, sample_weight)
+
+    @staticmethod
+    def _update_estimator_multivariate(node, target, sample_weight):
+        for t in target:
+            node.estimator[t].update(target[t], sample_weight)
+
+    # Incremental implementation of the insert method. Avoiding unnecessary
+    # stack tracing must decrease memory costs
+    def insert_value(self, att_val, target_val, sample_weight):
+        current = self
+        antecedent = None
+        is_right = False
+
+        while current is not None:
+            antecedent = current
+            if att_val == current.att_val:
+                self._update_estimator(current, target_val, sample_weight)
+                return
+            elif att_val < current.att_val:
+                self._update_estimator(current, target_val, sample_weight)
+
+                current = current._left
+                is_right = False
+            else:
+                current = current._right
+                is_right = True
+
+        # Value was not yet added to the tree
+        if is_right:
+            antecedent._right = EBSTNode(att_val, target_val, sample_weight)
+        else:
+            antecedent._left = EBSTNode(att_val, target_val, sample_weight)
```

### Comparing `river-0.8.0/river/tree/splitter/exhaustive_splitter.py` & `river-0.9.0/river/tree/splitter/exhaustive_splitter.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,190 +1,190 @@
-from collections import Counter, defaultdict
-
-from ..utils import BranchFactory
-from .base import Splitter
-
-
-class ExhaustiveSplitter(Splitter):
-    """Numeric attribute observer for classification tasks that is based on
-    a Binary Search Tree.
-
-    This algorithm[^1] is also referred to as exhaustive attribute observer,
-    since it ends up storing all the observations between split attempts[^2].
-
-    This splitter cannot perform probability density estimations, so it does not work well
-    when coupled with tree leaves using naive bayes models.
-
-    References
-    ----------
-    [^1]: Domingos, P. and Hulten, G., 2000, August. Mining high-speed data streams.
-    In Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery
-    and data mining (pp. 71-80).
-    [^2]: Pfahringer, B., Holmes, G. and Kirkby, R., 2008, May. Handling numeric attributes in
-    hoeffding trees. In Pacific-Asia Conference on Knowledge Discovery and Data Mining
-    (pp. 296-307). Springer, Berlin, Heidelberg.
-    """
-
-    def __init__(self):
-        super().__init__()
-        self._root = None
-
-    def update(self, att_val, target_val, sample_weight):
-        if att_val is None:
-            return
-        else:
-            if self._root is None:
-                self._root = ExhaustiveNode(att_val, target_val, sample_weight)
-            else:
-                self._root.insert_value(att_val, target_val, sample_weight)
-
-    def cond_proba(self, att_val, target_val):
-        """The underlying data structure used to monitor the input does not allow probability
-        density estimations. Hence, it always returns zero for any given input."""
-        return 0.0
-
-    def best_evaluated_split_suggestion(
-        self, criterion, pre_split_dist, att_idx, binary_only
-    ):
-        current_best_option = BranchFactory()
-
-        return self._search_for_best_split_option(
-            current_node=self._root,
-            current_best_option=current_best_option,
-            actual_parent_left=None,
-            parent_left=None,
-            parent_right=None,
-            left_child=False,
-            criterion=criterion,
-            pre_split_dist=pre_split_dist,
-            att_idx=att_idx,
-        )
-
-    def _search_for_best_split_option(
-        self,
-        current_node,
-        current_best_option,
-        actual_parent_left,
-        parent_left,
-        parent_right,
-        left_child,
-        criterion,
-        pre_split_dist,
-        att_idx,
-    ):
-        if current_node is None:
-            return current_best_option
-
-        left_dist = {}
-        right_dist = {}
-
-        if parent_left is None:
-            left_dist.update(
-                dict(Counter(left_dist) + Counter(current_node.class_count_left))
-            )
-            right_dist.update(
-                dict(Counter(right_dist) + Counter(current_node.class_count_right))
-            )
-        else:
-            left_dist.update(dict(Counter(left_dist) + Counter(parent_left)))
-            right_dist.update(dict(Counter(right_dist) + Counter(parent_right)))
-
-            if left_child:
-                # get the exact statistics of the parent value
-                exact_parent_dist = {}
-                exact_parent_dist.update(
-                    dict(Counter(exact_parent_dist) + Counter(actual_parent_left))
-                )
-                exact_parent_dist.update(
-                    dict(
-                        Counter(exact_parent_dist)
-                        - Counter(current_node.class_count_left)
-                    )
-                )
-                exact_parent_dist.update(
-                    dict(
-                        Counter(exact_parent_dist)
-                        - Counter(current_node.class_count_right)
-                    )
-                )
-
-                # move the subtrees
-                left_dist.update(
-                    dict(Counter(left_dist) - Counter(current_node.class_count_right))
-                )
-                right_dist.update(
-                    dict(Counter(right_dist) + Counter(current_node.class_count_right))
-                )
-
-                # move the exact value from the parent
-                right_dist.update(
-                    dict(Counter(right_dist) + Counter(exact_parent_dist))
-                )
-                left_dist.update(dict(Counter(left_dist) - Counter(exact_parent_dist)))
-            else:
-                left_dist.update(
-                    dict(Counter(left_dist) + Counter(current_node.class_count_left))
-                )
-                right_dist.update(
-                    dict(Counter(right_dist) - Counter(current_node.class_count_left))
-                )
-
-        post_split_dists = [left_dist, right_dist]
-        merit = criterion.merit_of_split(pre_split_dist, post_split_dists)
-
-        if merit > current_best_option.merit:
-            current_best_option = BranchFactory(
-                merit, att_idx, current_node.cut_point, post_split_dists
-            )
-
-        current_best_option = self._search_for_best_split_option(
-            current_node=current_node._left,  # noqa
-            current_best_option=current_best_option,
-            actual_parent_left=current_node.class_count_left,
-            parent_left=post_split_dists[0],
-            parent_right=post_split_dists[1],
-            left_child=True,
-            criterion=criterion,
-            pre_split_dist=pre_split_dist,
-            att_idx=att_idx,
-        )
-
-        current_best_option = self._search_for_best_split_option(
-            current_node=current_node._right,  # noqa
-            current_best_option=current_best_option,
-            actual_parent_left=current_node.class_count_left,
-            parent_left=post_split_dists[0],
-            parent_right=post_split_dists[1],
-            left_child=False,
-            criterion=criterion,
-            pre_split_dist=pre_split_dist,
-            att_idx=att_idx,
-        )
-
-        return current_best_option
-
-
-class ExhaustiveNode:
-    def __init__(self, att_val, target_val, sample_weight):
-        self.class_count_left = defaultdict(float)
-        self.class_count_right = defaultdict(float)
-        self._left = None
-        self._right = None
-
-        self.cut_point = att_val
-        self.class_count_left[target_val] += sample_weight
-
-    def insert_value(self, val, label, sample_weight):
-        if val == self.cut_point:
-            self.class_count_left[label] += sample_weight
-        elif val < self.cut_point:
-            self.class_count_left[label] += sample_weight
-            if self._left is None:
-                self._left = ExhaustiveNode(val, label, sample_weight)
-            else:
-                self._left.insert_value(val, label, sample_weight)
-        else:
-            self.class_count_right[label] += sample_weight
-            if self._right is None:
-                self._right = ExhaustiveNode(val, label, sample_weight)
-            else:
-                self._right.insert_value(val, label, sample_weight)
+from collections import Counter, defaultdict
+
+from ..utils import BranchFactory
+from .base import Splitter
+
+
+class ExhaustiveSplitter(Splitter):
+    """Numeric attribute observer for classification tasks that is based on
+    a Binary Search Tree.
+
+    This algorithm[^1] is also referred to as exhaustive attribute observer,
+    since it ends up storing all the observations between split attempts[^2].
+
+    This splitter cannot perform probability density estimations, so it does not work well
+    when coupled with tree leaves using naive bayes models.
+
+    References
+    ----------
+    [^1]: Domingos, P. and Hulten, G., 2000, August. Mining high-speed data streams.
+    In Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery
+    and data mining (pp. 71-80).
+    [^2]: Pfahringer, B., Holmes, G. and Kirkby, R., 2008, May. Handling numeric attributes in
+    hoeffding trees. In Pacific-Asia Conference on Knowledge Discovery and Data Mining
+    (pp. 296-307). Springer, Berlin, Heidelberg.
+    """
+
+    def __init__(self):
+        super().__init__()
+        self._root = None
+
+    def update(self, att_val, target_val, sample_weight):
+        if att_val is None:
+            return
+        else:
+            if self._root is None:
+                self._root = ExhaustiveNode(att_val, target_val, sample_weight)
+            else:
+                self._root.insert_value(att_val, target_val, sample_weight)
+
+    def cond_proba(self, att_val, target_val):
+        """The underlying data structure used to monitor the input does not allow probability
+        density estimations. Hence, it always returns zero for any given input."""
+        return 0.0
+
+    def best_evaluated_split_suggestion(
+        self, criterion, pre_split_dist, att_idx, binary_only
+    ):
+        current_best_option = BranchFactory()
+
+        return self._search_for_best_split_option(
+            current_node=self._root,
+            current_best_option=current_best_option,
+            actual_parent_left=None,
+            parent_left=None,
+            parent_right=None,
+            left_child=False,
+            criterion=criterion,
+            pre_split_dist=pre_split_dist,
+            att_idx=att_idx,
+        )
+
+    def _search_for_best_split_option(
+        self,
+        current_node,
+        current_best_option,
+        actual_parent_left,
+        parent_left,
+        parent_right,
+        left_child,
+        criterion,
+        pre_split_dist,
+        att_idx,
+    ):
+        if current_node is None:
+            return current_best_option
+
+        left_dist = {}
+        right_dist = {}
+
+        if parent_left is None:
+            left_dist.update(
+                dict(Counter(left_dist) + Counter(current_node.class_count_left))
+            )
+            right_dist.update(
+                dict(Counter(right_dist) + Counter(current_node.class_count_right))
+            )
+        else:
+            left_dist.update(dict(Counter(left_dist) + Counter(parent_left)))
+            right_dist.update(dict(Counter(right_dist) + Counter(parent_right)))
+
+            if left_child:
+                # get the exact statistics of the parent value
+                exact_parent_dist = {}
+                exact_parent_dist.update(
+                    dict(Counter(exact_parent_dist) + Counter(actual_parent_left))
+                )
+                exact_parent_dist.update(
+                    dict(
+                        Counter(exact_parent_dist)
+                        - Counter(current_node.class_count_left)
+                    )
+                )
+                exact_parent_dist.update(
+                    dict(
+                        Counter(exact_parent_dist)
+                        - Counter(current_node.class_count_right)
+                    )
+                )
+
+                # move the subtrees
+                left_dist.update(
+                    dict(Counter(left_dist) - Counter(current_node.class_count_right))
+                )
+                right_dist.update(
+                    dict(Counter(right_dist) + Counter(current_node.class_count_right))
+                )
+
+                # move the exact value from the parent
+                right_dist.update(
+                    dict(Counter(right_dist) + Counter(exact_parent_dist))
+                )
+                left_dist.update(dict(Counter(left_dist) - Counter(exact_parent_dist)))
+            else:
+                left_dist.update(
+                    dict(Counter(left_dist) + Counter(current_node.class_count_left))
+                )
+                right_dist.update(
+                    dict(Counter(right_dist) - Counter(current_node.class_count_left))
+                )
+
+        post_split_dists = [left_dist, right_dist]
+        merit = criterion.merit_of_split(pre_split_dist, post_split_dists)
+
+        if merit > current_best_option.merit:
+            current_best_option = BranchFactory(
+                merit, att_idx, current_node.cut_point, post_split_dists
+            )
+
+        current_best_option = self._search_for_best_split_option(
+            current_node=current_node._left,  # noqa
+            current_best_option=current_best_option,
+            actual_parent_left=current_node.class_count_left,
+            parent_left=post_split_dists[0],
+            parent_right=post_split_dists[1],
+            left_child=True,
+            criterion=criterion,
+            pre_split_dist=pre_split_dist,
+            att_idx=att_idx,
+        )
+
+        current_best_option = self._search_for_best_split_option(
+            current_node=current_node._right,  # noqa
+            current_best_option=current_best_option,
+            actual_parent_left=current_node.class_count_left,
+            parent_left=post_split_dists[0],
+            parent_right=post_split_dists[1],
+            left_child=False,
+            criterion=criterion,
+            pre_split_dist=pre_split_dist,
+            att_idx=att_idx,
+        )
+
+        return current_best_option
+
+
+class ExhaustiveNode:
+    def __init__(self, att_val, target_val, sample_weight):
+        self.class_count_left = defaultdict(float)
+        self.class_count_right = defaultdict(float)
+        self._left = None
+        self._right = None
+
+        self.cut_point = att_val
+        self.class_count_left[target_val] += sample_weight
+
+    def insert_value(self, val, label, sample_weight):
+        if val == self.cut_point:
+            self.class_count_left[label] += sample_weight
+        elif val < self.cut_point:
+            self.class_count_left[label] += sample_weight
+            if self._left is None:
+                self._left = ExhaustiveNode(val, label, sample_weight)
+            else:
+                self._left.insert_value(val, label, sample_weight)
+        else:
+            self.class_count_right[label] += sample_weight
+            if self._right is None:
+                self._right = ExhaustiveNode(val, label, sample_weight)
+            else:
+                self._right.insert_value(val, label, sample_weight)
```

### Comparing `river-0.8.0/river/tree/splitter/gaussian_splitter.py` & `river-0.9.0/river/tree/splitter/gaussian_splitter.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,101 +1,101 @@
-import math
-import typing
-
-from river.base.typing import ClfTarget
-from river.proba import Gaussian
-
-from ..utils import BranchFactory
-from .base import Splitter
-
-
-class GaussianSplitter(Splitter):
-    """Numeric attribute observer for classification tasks that is based on
-    Gaussian estimators.
-
-    The distribution of each class is approximated using a Gaussian distribution. Hence,
-    the probability density function can be easily calculated.
-
-
-    Parameters
-    ----------
-    n_splits
-        The number of partitions to consider when querying for split candidates.
-    """
-
-    def __init__(self, n_splits: int = 10):
-        super().__init__()
-        self._min_per_class: typing.Dict[ClfTarget, float] = {}
-        self._max_per_class: typing.Dict[ClfTarget, float] = {}
-        self._att_dist_per_class: typing.Dict[ClfTarget, Gaussian] = {}
-        self.n_splits = n_splits
-
-    def update(self, att_val, target_val, sample_weight):
-        if att_val is None:
-            return
-        else:
-            try:
-                val_dist = self._att_dist_per_class[target_val]
-                if att_val < self._min_per_class[target_val]:
-                    self._min_per_class[target_val] = att_val
-                if att_val > self._max_per_class[target_val]:
-                    self._max_per_class[target_val] = att_val
-            except KeyError:
-                val_dist = Gaussian()
-                self._att_dist_per_class[target_val] = val_dist
-                self._min_per_class[target_val] = att_val
-                self._max_per_class[target_val] = att_val
-
-            val_dist.update(att_val, sample_weight)
-
-    def cond_proba(self, att_val, target_val):
-        if target_val in self._att_dist_per_class:
-            obs = self._att_dist_per_class[target_val]
-            return obs.pdf(att_val)
-        else:
-            return 0.0
-
-    def best_evaluated_split_suggestion(
-        self, criterion, pre_split_dist, att_idx, binary_only
-    ):
-        best_suggestion = BranchFactory()
-        suggested_split_values = self._split_point_suggestions()
-        for split_value in suggested_split_values:
-            post_split_dist = self._class_dists_from_binary_split(split_value)
-            merit = criterion.merit_of_split(pre_split_dist, post_split_dist)
-            if merit > best_suggestion.merit:
-                best_suggestion = BranchFactory(
-                    merit, att_idx, split_value, post_split_dist
-                )
-
-        return best_suggestion
-
-    def _split_point_suggestions(self):
-        suggested_split_values = []
-        min_value = math.inf
-        max_value = -math.inf
-        for k, estimator in self._att_dist_per_class.items():
-            if self._min_per_class[k] < min_value:
-                min_value = self._min_per_class[k]
-            if self._max_per_class[k] > max_value:
-                max_value = self._max_per_class[k]
-        if min_value < math.inf:
-            bin_size = max_value - min_value
-            bin_size /= self.n_splits + 1.0
-            for i in range(self.n_splits):
-                split_value = min_value + (bin_size * (i + 1))
-                if min_value < split_value < max_value:
-                    suggested_split_values.append(split_value)
-        return suggested_split_values
-
-    def _class_dists_from_binary_split(self, split_value):
-        lhs_dist = {}
-        rhs_dist = {}
-        for k, estimator in self._att_dist_per_class.items():
-            if split_value < self._min_per_class[k]:
-                rhs_dist[k] = estimator.n_samples
-            elif split_value >= self._max_per_class[k]:
-                lhs_dist[k] = estimator.n_samples
-            else:
-                lhs_dist[k] = estimator.cdf(split_value) * estimator.n_samples
-                rhs_dist[k] = estimator.n_samples - lhs_dist[k]
-        return [lhs_dist, rhs_dist]
+import math
+import typing
+
+from river.base.typing import ClfTarget
+from river.proba import Gaussian
+
+from ..utils import BranchFactory
+from .base import Splitter
+
+
+class GaussianSplitter(Splitter):
+    """Numeric attribute observer for classification tasks that is based on
+    Gaussian estimators.
+
+    The distribution of each class is approximated using a Gaussian distribution. Hence,
+    the probability density function can be easily calculated.
+
+
+    Parameters
+    ----------
+    n_splits
+        The number of partitions to consider when querying for split candidates.
+    """
+
+    def __init__(self, n_splits: int = 10):
+        super().__init__()
+        self._min_per_class: typing.Dict[ClfTarget, float] = {}
+        self._max_per_class: typing.Dict[ClfTarget, float] = {}
+        self._att_dist_per_class: typing.Dict[ClfTarget, Gaussian] = {}
+        self.n_splits = n_splits
+
+    def update(self, att_val, target_val, sample_weight):
+        if att_val is None:
+            return
+        else:
+            try:
+                val_dist = self._att_dist_per_class[target_val]
+                if att_val < self._min_per_class[target_val]:
+                    self._min_per_class[target_val] = att_val
+                if att_val > self._max_per_class[target_val]:
+                    self._max_per_class[target_val] = att_val
+            except KeyError:
+                val_dist = Gaussian()
+                self._att_dist_per_class[target_val] = val_dist
+                self._min_per_class[target_val] = att_val
+                self._max_per_class[target_val] = att_val
+
+            val_dist.update(att_val, sample_weight)
+
+    def cond_proba(self, att_val, target_val):
+        if target_val in self._att_dist_per_class:
+            obs = self._att_dist_per_class[target_val]
+            return obs.pdf(att_val)
+        else:
+            return 0.0
+
+    def best_evaluated_split_suggestion(
+        self, criterion, pre_split_dist, att_idx, binary_only
+    ):
+        best_suggestion = BranchFactory()
+        suggested_split_values = self._split_point_suggestions()
+        for split_value in suggested_split_values:
+            post_split_dist = self._class_dists_from_binary_split(split_value)
+            merit = criterion.merit_of_split(pre_split_dist, post_split_dist)
+            if merit > best_suggestion.merit:
+                best_suggestion = BranchFactory(
+                    merit, att_idx, split_value, post_split_dist
+                )
+
+        return best_suggestion
+
+    def _split_point_suggestions(self):
+        suggested_split_values = []
+        min_value = math.inf
+        max_value = -math.inf
+        for k, estimator in self._att_dist_per_class.items():
+            if self._min_per_class[k] < min_value:
+                min_value = self._min_per_class[k]
+            if self._max_per_class[k] > max_value:
+                max_value = self._max_per_class[k]
+        if min_value < math.inf:
+            bin_size = max_value - min_value
+            bin_size /= self.n_splits + 1.0
+            for i in range(self.n_splits):
+                split_value = min_value + (bin_size * (i + 1))
+                if min_value < split_value < max_value:
+                    suggested_split_values.append(split_value)
+        return suggested_split_values
+
+    def _class_dists_from_binary_split(self, split_value):
+        lhs_dist = {}
+        rhs_dist = {}
+        for k, estimator in self._att_dist_per_class.items():
+            if split_value < self._min_per_class[k]:
+                rhs_dist[k] = estimator.n_samples
+            elif split_value >= self._max_per_class[k]:
+                lhs_dist[k] = estimator.n_samples
+            else:
+                lhs_dist[k] = estimator.cdf(split_value) * estimator.n_samples
+                rhs_dist[k] = estimator.n_samples - lhs_dist[k]
+        return [lhs_dist, rhs_dist]
```

### Comparing `river-0.8.0/river/tree/splitter/histogram_splitter.py` & `river-0.9.0/river/tree/splitter/histogram_splitter.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,114 +1,114 @@
-import bisect
-import collections
-import functools
-
-from river import utils
-from river.utils.histogram import Bin  # noqa
-
-from ..utils import BranchFactory
-from .base import Splitter
-
-
-class HistogramSplitter(Splitter):
-    """Numeric attribute observer for classification tasks that discretizes features
-    using histograms.
-
-
-    Parameters
-    ----------
-    n_bins
-        The maximum number of bins in the histogram.
-    n_splits
-        The number of split points to evaluate when querying for the best split
-        candidate.
-    """
-
-    def __init__(self, n_bins: int = 256, n_splits: int = 32):
-        super().__init__()
-        self.n_bins = n_bins
-        self.n_splits = n_splits
-        self.hists = collections.defaultdict(
-            functools.partial(utils.Histogram, max_bins=self.n_bins)
-        )
-
-    def update(self, att_val, target_val, sample_weight):
-        for _ in range(int(sample_weight)):
-            self.hists[target_val].update(att_val)
-
-    def cond_proba(self, att_val, target_val):
-        if target_val not in self.hists:
-            return 0.0
-
-        total_weight = self.hists[target_val].n
-        if not total_weight > 0:
-            return 0.0
-
-        i = bisect.bisect(self.hists[target_val], Bin(att_val, att_val, 1))
-
-        if i < len(self.hists[target_val]):
-            b = self.hists[target_val][i]
-        else:  # att_val exceeds the range: take the last bin
-            b = self.hists[target_val][-1]
-
-        # Approximates the PDF of x by using the frequency in its corresponding
-        # histogram bin
-        if b.left == b.right:
-            return b.count / total_weight
-        else:
-            return (b.count * (att_val - b.left) / (b.right - b.left)) / total_weight
-
-    def best_evaluated_split_suggestion(
-        self, criterion, pre_split_dist, att_idx, binary_only
-    ):
-        best_suggestion = BranchFactory()
-
-        low = min(h[0].right for h in self.hists.values())
-        high = min(h[-1].right for h in self.hists.values())
-
-        # If only one single value has been observed, then no split can be proposed
-        if low >= high:
-            return best_suggestion
-
-        n_thresholds = min(self.n_splits, max(map(len, self.hists.values())) - 1)
-
-        thresholds = list(decimal_range(start=low, stop=high, num=n_thresholds))
-        cdfs = {y: hist.iter_cdf(thresholds) for y, hist in self.hists.items()}
-
-        total_weight = sum(pre_split_dist.values())
-        for at in thresholds:
-
-            l_dist = {}
-            r_dist = {}
-
-            for y in pre_split_dist:
-                if y in cdfs:
-                    p_xy = next(cdfs[y])  # P(x <= t | y)
-                    p_y = pre_split_dist[y] / total_weight  # P(y)
-                    l_dist[y] = total_weight * p_y * p_xy  # P(y | x <= t)
-                    r_dist[y] = total_weight * p_y * (1 - p_xy)  # P(y | x > t)
-
-            post_split_dist = [l_dist, r_dist]
-            merit = criterion.merit_of_split(pre_split_dist, post_split_dist)
-
-            if merit > best_suggestion.merit:
-                best_suggestion = BranchFactory(merit, att_idx, at, post_split_dist)
-
-        return best_suggestion
-
-
-def decimal_range(start, stop, num):
-    """
-    Example
-    -------
-    >>> for x in decimal_range(0, 1, 4):
-    ...     print(x)
-    0.2
-    0.4
-    0.6
-    0.8
-    """
-    step = (stop - start) / (num + 1)
-
-    for _ in range(num):
-        start += step
-        yield start
+import bisect
+import collections
+import functools
+
+from river import utils
+from river.utils.histogram import Bin  # noqa
+
+from ..utils import BranchFactory
+from .base import Splitter
+
+
+class HistogramSplitter(Splitter):
+    """Numeric attribute observer for classification tasks that discretizes features
+    using histograms.
+
+
+    Parameters
+    ----------
+    n_bins
+        The maximum number of bins in the histogram.
+    n_splits
+        The number of split points to evaluate when querying for the best split
+        candidate.
+    """
+
+    def __init__(self, n_bins: int = 256, n_splits: int = 32):
+        super().__init__()
+        self.n_bins = n_bins
+        self.n_splits = n_splits
+        self.hists = collections.defaultdict(
+            functools.partial(utils.Histogram, max_bins=self.n_bins)
+        )
+
+    def update(self, att_val, target_val, sample_weight):
+        for _ in range(int(sample_weight)):
+            self.hists[target_val].update(att_val)
+
+    def cond_proba(self, att_val, target_val):
+        if target_val not in self.hists:
+            return 0.0
+
+        total_weight = self.hists[target_val].n
+        if not total_weight > 0:
+            return 0.0
+
+        i = bisect.bisect(self.hists[target_val], Bin(att_val, att_val, 1))
+
+        if i < len(self.hists[target_val]):
+            b = self.hists[target_val][i]
+        else:  # att_val exceeds the range: take the last bin
+            b = self.hists[target_val][-1]
+
+        # Approximates the PDF of x by using the frequency in its corresponding
+        # histogram bin
+        if b.left == b.right:
+            return b.count / total_weight
+        else:
+            return (b.count * (att_val - b.left) / (b.right - b.left)) / total_weight
+
+    def best_evaluated_split_suggestion(
+        self, criterion, pre_split_dist, att_idx, binary_only
+    ):
+        best_suggestion = BranchFactory()
+
+        low = min(h[0].right for h in self.hists.values())
+        high = min(h[-1].right for h in self.hists.values())
+
+        # If only one single value has been observed, then no split can be proposed
+        if low >= high:
+            return best_suggestion
+
+        n_thresholds = min(self.n_splits, max(map(len, self.hists.values())) - 1)
+
+        thresholds = list(decimal_range(start=low, stop=high, num=n_thresholds))
+        cdfs = {y: hist.iter_cdf(thresholds) for y, hist in self.hists.items()}
+
+        total_weight = sum(pre_split_dist.values())
+        for at in thresholds:
+
+            l_dist = {}
+            r_dist = {}
+
+            for y in pre_split_dist:
+                if y in cdfs:
+                    p_xy = next(cdfs[y])  # P(x <= t | y)
+                    p_y = pre_split_dist[y] / total_weight  # P(y)
+                    l_dist[y] = total_weight * p_y * p_xy  # P(y | x <= t)
+                    r_dist[y] = total_weight * p_y * (1 - p_xy)  # P(y | x > t)
+
+            post_split_dist = [l_dist, r_dist]
+            merit = criterion.merit_of_split(pre_split_dist, post_split_dist)
+
+            if merit > best_suggestion.merit:
+                best_suggestion = BranchFactory(merit, att_idx, at, post_split_dist)
+
+        return best_suggestion
+
+
+def decimal_range(start, stop, num):
+    """
+    Example
+    -------
+    >>> for x in decimal_range(0, 1, 4):
+    ...     print(x)
+    0.2
+    0.4
+    0.6
+    0.8
+    """
+    step = (stop - start) / (num + 1)
+
+    for _ in range(num):
+        start += step
+        yield start
```

### Comparing `river-0.8.0/river/tree/splitter/nominal_splitter_classif.py` & `river-0.9.0/river/tree/splitter/nominal_splitter_classif.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,105 +1,105 @@
-import collections
-import functools
-
-from ..utils import BranchFactory
-from .base import Splitter
-
-
-class NominalSplitterClassif(Splitter):
-    """Splitter utilized to monitor nominal features in classification tasks.
-
-    As the monitored feature is nominal, it already has well-defined partitions. Hence,
-    this splitter uses dictionary structures to keep class counts for each incoming category.
-    """
-
-    def __init__(self):
-        super().__init__()
-        self._total_weight_observed = 0.0
-        self._missing_weight_observed = 0.0
-        self._att_dist_per_class = collections.defaultdict(
-            functools.partial(collections.defaultdict, float)
-        )
-        self._att_values = set()
-
-    @property
-    def is_numeric(self):
-        return False
-
-    def update(self, att_val, target_val, sample_weight):
-        if att_val is None:
-            self._missing_weight_observed += sample_weight
-        else:
-            self._att_values.add(att_val)
-            self._att_dist_per_class[target_val][att_val] += sample_weight
-
-        self._total_weight_observed += sample_weight
-
-    def cond_proba(self, att_val, target_val):
-        class_dist = self._att_dist_per_class[target_val]
-        value = class_dist[att_val]
-        try:
-            return value / sum(class_dist.values())
-        except ZeroDivisionError:
-            return 0.0
-
-    def best_evaluated_split_suggestion(
-        self, criterion, pre_split_dist, att_idx, binary_only
-    ):
-        best_suggestion = BranchFactory()
-
-        if not binary_only:
-            post_split_dist = self._class_dist_from_multiway_split()
-            merit = criterion.merit_of_split(pre_split_dist, post_split_dist)
-
-            best_suggestion = BranchFactory(
-                merit,
-                att_idx,
-                sorted(self._att_values),
-                post_split_dist,
-                numerical_feature=False,
-                multiway_split=True,
-            )
-
-        for att_val in self._att_values:
-            post_split_dist = self._class_dist_from_binary_split(att_val)
-            merit = criterion.merit_of_split(pre_split_dist, post_split_dist)
-            if best_suggestion is None or merit > best_suggestion.merit:
-                best_suggestion = BranchFactory(
-                    merit,
-                    att_idx,
-                    att_val,
-                    post_split_dist,
-                    numerical_feature=False,
-                    multiway_split=False,
-                )
-
-        return best_suggestion
-
-    def _class_dist_from_multiway_split(self):
-        resulting_dist = {}
-        for class_val, att_dist in self._att_dist_per_class.items():
-            for att_val, weight in att_dist.items():
-                if att_val not in resulting_dist:
-                    resulting_dist[att_val] = {}
-                if class_val not in resulting_dist[att_val]:
-                    resulting_dist[att_val][class_val] = 0.0
-                resulting_dist[att_val][class_val] += weight
-
-        sorted_keys = sorted(resulting_dist.keys())
-        distributions = [dict(sorted(resulting_dist[k].items())) for k in sorted_keys]
-        return distributions
-
-    def _class_dist_from_binary_split(self, val_idx):
-        equal_dist = {}
-        not_equal_dist = {}
-        for class_val, att_dist in self._att_dist_per_class.items():
-            for att_val, weight in att_dist.items():
-                if att_val == val_idx:
-                    if class_val not in equal_dist:
-                        equal_dist[class_val] = 0.0
-                    equal_dist[class_val] += weight
-                else:
-                    if class_val not in not_equal_dist:
-                        not_equal_dist[class_val] = 0.0
-                    not_equal_dist[class_val] += weight
-        return [equal_dist, not_equal_dist]
+import collections
+import functools
+
+from ..utils import BranchFactory
+from .base import Splitter
+
+
+class NominalSplitterClassif(Splitter):
+    """Splitter utilized to monitor nominal features in classification tasks.
+
+    As the monitored feature is nominal, it already has well-defined partitions. Hence,
+    this splitter uses dictionary structures to keep class counts for each incoming category.
+    """
+
+    def __init__(self):
+        super().__init__()
+        self._total_weight_observed = 0.0
+        self._missing_weight_observed = 0.0
+        self._att_dist_per_class = collections.defaultdict(
+            functools.partial(collections.defaultdict, float)
+        )
+        self._att_values = set()
+
+    @property
+    def is_numeric(self):
+        return False
+
+    def update(self, att_val, target_val, sample_weight):
+        if att_val is None:
+            self._missing_weight_observed += sample_weight
+        else:
+            self._att_values.add(att_val)
+            self._att_dist_per_class[target_val][att_val] += sample_weight
+
+        self._total_weight_observed += sample_weight
+
+    def cond_proba(self, att_val, target_val):
+        class_dist = self._att_dist_per_class[target_val]
+        value = class_dist[att_val]
+        try:
+            return value / sum(class_dist.values())
+        except ZeroDivisionError:
+            return 0.0
+
+    def best_evaluated_split_suggestion(
+        self, criterion, pre_split_dist, att_idx, binary_only
+    ):
+        best_suggestion = BranchFactory()
+
+        if not binary_only:
+            post_split_dist = self._class_dist_from_multiway_split()
+            merit = criterion.merit_of_split(pre_split_dist, post_split_dist)
+
+            best_suggestion = BranchFactory(
+                merit,
+                att_idx,
+                sorted(self._att_values),
+                post_split_dist,
+                numerical_feature=False,
+                multiway_split=True,
+            )
+
+        for att_val in self._att_values:
+            post_split_dist = self._class_dist_from_binary_split(att_val)
+            merit = criterion.merit_of_split(pre_split_dist, post_split_dist)
+            if best_suggestion is None or merit > best_suggestion.merit:
+                best_suggestion = BranchFactory(
+                    merit,
+                    att_idx,
+                    att_val,
+                    post_split_dist,
+                    numerical_feature=False,
+                    multiway_split=False,
+                )
+
+        return best_suggestion
+
+    def _class_dist_from_multiway_split(self):
+        resulting_dist = {}
+        for class_val, att_dist in self._att_dist_per_class.items():
+            for att_val, weight in att_dist.items():
+                if att_val not in resulting_dist:
+                    resulting_dist[att_val] = {}
+                if class_val not in resulting_dist[att_val]:
+                    resulting_dist[att_val][class_val] = 0.0
+                resulting_dist[att_val][class_val] += weight
+
+        sorted_keys = sorted(resulting_dist.keys())
+        distributions = [dict(sorted(resulting_dist[k].items())) for k in sorted_keys]
+        return distributions
+
+    def _class_dist_from_binary_split(self, val_idx):
+        equal_dist = {}
+        not_equal_dist = {}
+        for class_val, att_dist in self._att_dist_per_class.items():
+            for att_val, weight in att_dist.items():
+                if att_val == val_idx:
+                    if class_val not in equal_dist:
+                        equal_dist[class_val] = 0.0
+                    equal_dist[class_val] += weight
+                else:
+                    if class_val not in not_equal_dist:
+                        not_equal_dist[class_val] = 0.0
+                    not_equal_dist[class_val] += weight
+        return [equal_dist, not_equal_dist]
```

### Comparing `river-0.8.0/river/tree/splitter/nominal_splitter_reg.py` & `river-0.9.0/river/tree/splitter/nominal_splitter_reg.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,93 +1,93 @@
-import functools
-
-from river.stats import Var
-from river.utils import VectorDict
-
-from ..utils import BranchFactory
-from .base import Splitter
-
-
-class NominalSplitterReg(Splitter):
-    """Splitter utilized to monitor nominal features in regression tasks.
-
-    As the monitored feature is nominal, it already has well-defined partitions. Hence,
-    this splitter simply keeps the split-enabling target statistics for each incoming
-    category."""
-
-    def __init__(self):
-        super().__init__()
-        self._statistics = {}
-        # The way in which the target statistics are update are selected on the fly.
-        # Different strategies are used for univariate and multivariate regression.
-        self._update_estimator = self._update_estimator_univariate
-
-    @property
-    def is_numeric(self):
-        return False
-
-    @staticmethod
-    def _update_estimator_univariate(estimator, target, sample_weight):
-        estimator.update(target, sample_weight)
-
-    @staticmethod
-    def _update_estimator_multivariate(estimator, target, sample_weight):
-        for t in target:
-            estimator[t].update(target[t], sample_weight)
-
-    def update(self, att_val, target_val, sample_weight):
-        if att_val is None or sample_weight is None:
-            return
-        else:
-            try:
-                estimator = self._statistics[att_val]
-            except KeyError:
-                if isinstance(target_val, dict):  # Multi-target case
-                    self._statistics[att_val] = VectorDict(
-                        default_factory=functools.partial(Var)
-                    )
-                    self._update_estimator = self._update_estimator_multivariate
-                else:
-                    self._statistics[att_val] = Var()
-                estimator = self._statistics[att_val]
-            self._update_estimator(estimator, target_val, sample_weight)
-
-    def cond_proba(self, att_val, target_val):
-        """Not implemented in regression splitters."""
-        raise NotImplementedError
-
-    def best_evaluated_split_suggestion(
-        self, criterion, pre_split_dist, att_idx, binary_only
-    ):
-        current_best = BranchFactory()
-        ordered_feature_values = sorted(list(self._statistics.keys()))
-        if not binary_only and len(self._statistics) > 2:
-            post_split_dist = [self._statistics[k] for k in ordered_feature_values]
-
-            merit = criterion.merit_of_split(pre_split_dist, post_split_dist)
-            current_best = BranchFactory(
-                merit,
-                att_idx,
-                ordered_feature_values,
-                post_split_dist,
-                numerical_feature=False,
-                multiway_split=True,
-            )
-
-        for att_val in ordered_feature_values:
-            actual_dist = self._statistics[att_val]
-            remaining_dist = pre_split_dist - actual_dist
-            post_split_dist = [actual_dist, remaining_dist]
-
-            merit = criterion.merit_of_split(pre_split_dist, post_split_dist)
-
-            if merit > current_best.merit:
-                current_best = BranchFactory(
-                    merit,
-                    att_idx,
-                    att_val,
-                    post_split_dist,
-                    numerical_feature=False,
-                    multiway_split=False,
-                )
-
-        return current_best
+import functools
+
+from river.stats import Var
+from river.utils import VectorDict
+
+from ..utils import BranchFactory
+from .base import Splitter
+
+
+class NominalSplitterReg(Splitter):
+    """Splitter utilized to monitor nominal features in regression tasks.
+
+    As the monitored feature is nominal, it already has well-defined partitions. Hence,
+    this splitter simply keeps the split-enabling target statistics for each incoming
+    category."""
+
+    def __init__(self):
+        super().__init__()
+        self._statistics = {}
+        # The way in which the target statistics are update are selected on the fly.
+        # Different strategies are used for univariate and multivariate regression.
+        self._update_estimator = self._update_estimator_univariate
+
+    @property
+    def is_numeric(self):
+        return False
+
+    @staticmethod
+    def _update_estimator_univariate(estimator, target, sample_weight):
+        estimator.update(target, sample_weight)
+
+    @staticmethod
+    def _update_estimator_multivariate(estimator, target, sample_weight):
+        for t in target:
+            estimator[t].update(target[t], sample_weight)
+
+    def update(self, att_val, target_val, sample_weight):
+        if att_val is None or sample_weight is None:
+            return
+        else:
+            try:
+                estimator = self._statistics[att_val]
+            except KeyError:
+                if isinstance(target_val, dict):  # Multi-target case
+                    self._statistics[att_val] = VectorDict(
+                        default_factory=functools.partial(Var)
+                    )
+                    self._update_estimator = self._update_estimator_multivariate
+                else:
+                    self._statistics[att_val] = Var()
+                estimator = self._statistics[att_val]
+            self._update_estimator(estimator, target_val, sample_weight)
+
+    def cond_proba(self, att_val, target_val):
+        """Not implemented in regression splitters."""
+        raise NotImplementedError
+
+    def best_evaluated_split_suggestion(
+        self, criterion, pre_split_dist, att_idx, binary_only
+    ):
+        current_best = BranchFactory()
+        ordered_feature_values = sorted(list(self._statistics.keys()))
+        if not binary_only and len(self._statistics) > 2:
+            post_split_dist = [self._statistics[k] for k in ordered_feature_values]
+
+            merit = criterion.merit_of_split(pre_split_dist, post_split_dist)
+            current_best = BranchFactory(
+                merit,
+                att_idx,
+                ordered_feature_values,
+                post_split_dist,
+                numerical_feature=False,
+                multiway_split=True,
+            )
+
+        for att_val in ordered_feature_values:
+            actual_dist = self._statistics[att_val]
+            remaining_dist = pre_split_dist - actual_dist
+            post_split_dist = [actual_dist, remaining_dist]
+
+            merit = criterion.merit_of_split(pre_split_dist, post_split_dist)
+
+            if merit > current_best.merit:
+                current_best = BranchFactory(
+                    merit,
+                    att_idx,
+                    att_val,
+                    post_split_dist,
+                    numerical_feature=False,
+                    multiway_split=False,
+                )
+
+        return current_best
```

### Comparing `river-0.8.0/river/tree/splitter/qo_splitter.py` & `river-0.9.0/river/tree/splitter/qo_splitter.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,228 +1,228 @@
-import functools
-import math
-import typing
-
-from river import stats, utils
-
-from ..utils import BranchFactory
-from .base import Splitter
-
-
-class QOSplitter(Splitter):
-    """Quantization observer (QO).
-
-    This splitter utilizes a hash-based quantization algorithm to keep track of the target
-    statistics and evaluate split candidates. QO, relies on the radius parameter to define
-    discretization intervals for each incoming feature. Split candidates are defined as the
-    midpoints between two consecutive hash slots. Both binary splits and multi-way splits can be
-    created by this attribute observer. This class implements the algorithm described
-    in [^1].
-
-    The smaller the quantization radius, the more hash slots will be created to accommodate the
-    discretized data. Hence, both the running time and memory consumption increase, but the
-    resulting splits ought to be closer to the ones obtained by a batch exhaustive approach.
-    On the other hand, if the radius is too large, fewer slots will be created, less memory and
-    running time will be required, but at the cost of coarse split suggestions.
-
-    QO assumes that all features have the same range. It is always advised to scale the features
-    to apply this splitter. That can be done using the `preprocessing` module. A good "rule of
-    thumb" is to scale data using `preprocessing.StandardScaler` and define the radius as a
-    proportion of the features' standard deviation. For instance, the default radius value
-    would correspond to one quarter of the normalized features' standard deviation (since the
-    scaled data has zero mean and unit variance). If the features come from normal
-    distributions, by following the empirical rule, roughly `32` hash slots will be created.
-
-    Parameters
-    ----------
-    radius
-        The quantization radius. QO discretizes the incoming feature in intervals of equal length
-        that are defined by this parameter.
-    allow_multiway_splits
-        Whether or not allow that multiway splits are evaluated. Numeric multi-way splits use
-        the same quantization strategy of QO to create multiple tree branches. The same
-        quantization radius is used, and each stored slot represents the split enabling
-        statistics of one branch.
-
-
-    References
-    ----------
-    [^1]: Mastelini, S.M. and de Leon Ferreira, A.C.P., 2021. Using dynamical quantization to
-    perform split attempts in online tree regressors. Pattern Recognition Letters.
-
-    """
-
-    def __init__(self, radius: float = 0.25, allow_multiway_splits=False):
-        super().__init__()
-
-        if radius <= 0:
-            raise ValueError("'radius' must be greater than zero.")
-        self.radius = radius
-        self._quantizer = FeatureQuantizer(radius=self.radius)
-
-        self.allow_multiway_splits = allow_multiway_splits
-
-    def update(self, att_val, target_val, sample_weight):
-        if att_val is None:
-            return
-        else:
-            self._quantizer.update(att_val, target_val, sample_weight)
-
-    def cond_proba(self, att_val, target_val):
-        raise NotImplementedError
-
-    def best_evaluated_split_suggestion(
-        self, criterion, pre_split_dist, att_idx, binary_only=True
-    ):
-        candidate = BranchFactory()
-
-        # In case the hash carries just one element return the null split
-        if len(self._quantizer) == 1:
-            return candidate
-
-        # Numeric multiway test
-        if self.allow_multiway_splits and not binary_only:
-            slot_ids, post_split_dists = self._quantizer.all()
-            merit = criterion.merit_of_split(pre_split_dist, post_split_dists)
-
-            candidate = BranchFactory(
-                merit,
-                att_idx,
-                (self.radius, slot_ids),
-                post_split_dists,
-                numerical_feature=True,
-                multiway_split=True,
-            )
-
-        # The previously evaluated x value
-        prev_x = None
-
-        for (x, left_dist) in self._quantizer:
-            # First hash element
-            if prev_x is None:
-                prev_x = x
-                continue
-
-            right_dist = pre_split_dist - left_dist
-            post_split_dists = [left_dist, right_dist]
-            merit = criterion.merit_of_split(pre_split_dist, post_split_dists)
-
-            if merit > candidate.merit:
-                split_point = (prev_x + x) / 2.0
-                candidate = BranchFactory(
-                    merit,
-                    att_idx,
-                    split_point,
-                    post_split_dists,
-                    numerical_feature=True,
-                    multiway_split=False,
-                )
-
-            prev_x = x
-        return candidate
-
-    @property
-    def is_target_class(self) -> bool:
-        return False
-
-
-class Slot:
-    """ The element stored in the quantization hash.
-
-    Each slot keeps the mean values of the numerical feature, as well as the variance
-    and mean of the target.
-
-    """
-
-    def __init__(
-        self, x: float, y=typing.Union[float, utils.VectorDict], weight: float = 1.0
-    ):
-        self.x_stats = stats.Mean()
-        self.x_stats.update(x, weight)
-
-        self.y_stats: typing.Union[stats.Var, utils.VectorDict]
-
-        self._update_estimator: typing.Callable[
-            [typing.Union[float, utils.VectorDict], float], None
-        ]
-        self.is_single_target = True
-
-        self._init_estimator(y)
-        self._update_estimator(y, weight)
-
-    def _init_estimator(self, y):
-        if isinstance(y, dict):
-            self.is_single_target = False
-            self.y_stats = utils.VectorDict(
-                default_factory=functools.partial(stats.Var)
-            )
-            self._update_estimator = self._update_estimator_multivariate
-        else:
-            self.y_stats = stats.Var()
-            self._update_estimator = self._update_estimator_univariate
-
-    def _update_estimator_univariate(self, target, sample_weight):
-        self.y_stats.update(target, sample_weight)
-
-    def _update_estimator_multivariate(self, target, sample_weight):
-        for t in target:
-            self.y_stats[t].update(target[t], sample_weight)
-
-    def __iadd__(self, o):
-        self.x_stats += o.x_stats
-        self.y_stats += o.y_stats
-
-        return self
-
-    def update(self, x, y, sample_weight):
-        self.x_stats.update(x, sample_weight)
-        self._update_estimator(y, sample_weight)
-
-
-class FeatureQuantizer:
-    """ The actual dynamic feature quantization mechanism.
-
-    The input numerical feature is partitioned using equal-sized intervals that are defined
-    by the `radius` parameter. For each interval, estimators of target's mean and variance are
-    kept, as well as an estimator of the feature's mean value. At any time, one can iterate
-    over the ordered stored values in the hash. The ordering is defined by the quantized values
-    of the feature.
-
-    """
-
-    def __init__(self, radius: float):
-        self.radius = radius
-        self.hash = {}
-
-    def __getitem__(self, k):
-        return self.hash[k]
-
-    def __len__(self):
-        return len(self.hash)
-
-    def update(self, x: float, y: typing.Union[float, utils.VectorDict], weight: float):
-        index = math.floor(x / self.radius)
-        try:
-            self.hash[index].update(x, y, weight)
-        except KeyError:
-            self.hash[index] = Slot(x, y, weight)
-
-    def __iter__(self):
-        aux_stats = (
-            stats.Var()
-            if next(iter(self.hash.values())).is_single_target
-            else utils.VectorDict(default_factory=functools.partial(stats.Var))
-        )
-
-        for i in sorted(self.hash.keys()):
-            x = self.hash[i].x_stats.get()
-            aux_stats += self.hash[i].y_stats
-            yield x, aux_stats
-
-    def all(self):
-        branch_ids = []
-        split_stats = []
-        for slot_id in sorted(self.hash.keys()):
-            branch_ids.append(slot_id)
-            split_stats.append(self.hash[slot_id].y_stats)
-
-        return branch_ids, split_stats
+import functools
+import math
+import typing
+
+from river import stats, utils
+
+from ..utils import BranchFactory
+from .base import Splitter
+
+
+class QOSplitter(Splitter):
+    """Quantization observer (QO).
+
+    This splitter utilizes a hash-based quantization algorithm to keep track of the target
+    statistics and evaluate split candidates. QO, relies on the radius parameter to define
+    discretization intervals for each incoming feature. Split candidates are defined as the
+    midpoints between two consecutive hash slots. Both binary splits and multi-way splits can be
+    created by this attribute observer. This class implements the algorithm described
+    in [^1].
+
+    The smaller the quantization radius, the more hash slots will be created to accommodate the
+    discretized data. Hence, both the running time and memory consumption increase, but the
+    resulting splits ought to be closer to the ones obtained by a batch exhaustive approach.
+    On the other hand, if the radius is too large, fewer slots will be created, less memory and
+    running time will be required, but at the cost of coarse split suggestions.
+
+    QO assumes that all features have the same range. It is always advised to scale the features
+    to apply this splitter. That can be done using the `preprocessing` module. A good "rule of
+    thumb" is to scale data using `preprocessing.StandardScaler` and define the radius as a
+    proportion of the features' standard deviation. For instance, the default radius value
+    would correspond to one quarter of the normalized features' standard deviation (since the
+    scaled data has zero mean and unit variance). If the features come from normal
+    distributions, by following the empirical rule, roughly `32` hash slots will be created.
+
+    Parameters
+    ----------
+    radius
+        The quantization radius. QO discretizes the incoming feature in intervals of equal length
+        that are defined by this parameter.
+    allow_multiway_splits
+        Whether or not allow that multiway splits are evaluated. Numeric multi-way splits use
+        the same quantization strategy of QO to create multiple tree branches. The same
+        quantization radius is used, and each stored slot represents the split enabling
+        statistics of one branch.
+
+
+    References
+    ----------
+    [^1]: Mastelini, S.M. and de Leon Ferreira, A.C.P., 2021. Using dynamical quantization to
+    perform split attempts in online tree regressors. Pattern Recognition Letters.
+
+    """
+
+    def __init__(self, radius: float = 0.25, allow_multiway_splits=False):
+        super().__init__()
+
+        if radius <= 0:
+            raise ValueError("'radius' must be greater than zero.")
+        self.radius = radius
+        self._quantizer = FeatureQuantizer(radius=self.radius)
+
+        self.allow_multiway_splits = allow_multiway_splits
+
+    def update(self, att_val, target_val, sample_weight):
+        if att_val is None:
+            return
+        else:
+            self._quantizer.update(att_val, target_val, sample_weight)
+
+    def cond_proba(self, att_val, target_val):
+        raise NotImplementedError
+
+    def best_evaluated_split_suggestion(
+        self, criterion, pre_split_dist, att_idx, binary_only=True
+    ):
+        candidate = BranchFactory()
+
+        # In case the hash carries just one element return the null split
+        if len(self._quantizer) == 1:
+            return candidate
+
+        # Numeric multiway test
+        if self.allow_multiway_splits and not binary_only:
+            slot_ids, post_split_dists = self._quantizer.all()
+            merit = criterion.merit_of_split(pre_split_dist, post_split_dists)
+
+            candidate = BranchFactory(
+                merit,
+                att_idx,
+                (self.radius, slot_ids),
+                post_split_dists,
+                numerical_feature=True,
+                multiway_split=True,
+            )
+
+        # The previously evaluated x value
+        prev_x = None
+
+        for (x, left_dist) in self._quantizer:
+            # First hash element
+            if prev_x is None:
+                prev_x = x
+                continue
+
+            right_dist = pre_split_dist - left_dist
+            post_split_dists = [left_dist, right_dist]
+            merit = criterion.merit_of_split(pre_split_dist, post_split_dists)
+
+            if merit > candidate.merit:
+                split_point = (prev_x + x) / 2.0
+                candidate = BranchFactory(
+                    merit,
+                    att_idx,
+                    split_point,
+                    post_split_dists,
+                    numerical_feature=True,
+                    multiway_split=False,
+                )
+
+            prev_x = x
+        return candidate
+
+    @property
+    def is_target_class(self) -> bool:
+        return False
+
+
+class Slot:
+    """ The element stored in the quantization hash.
+
+    Each slot keeps the mean values of the numerical feature, as well as the variance
+    and mean of the target.
+
+    """
+
+    def __init__(
+        self, x: float, y=typing.Union[float, utils.VectorDict], weight: float = 1.0
+    ):
+        self.x_stats = stats.Mean()
+        self.x_stats.update(x, weight)
+
+        self.y_stats: typing.Union[stats.Var, utils.VectorDict]
+
+        self._update_estimator: typing.Callable[
+            [typing.Union[float, utils.VectorDict], float], None
+        ]
+        self.is_single_target = True
+
+        self._init_estimator(y)
+        self._update_estimator(y, weight)
+
+    def _init_estimator(self, y):
+        if isinstance(y, dict):
+            self.is_single_target = False
+            self.y_stats = utils.VectorDict(
+                default_factory=functools.partial(stats.Var)
+            )
+            self._update_estimator = self._update_estimator_multivariate
+        else:
+            self.y_stats = stats.Var()
+            self._update_estimator = self._update_estimator_univariate
+
+    def _update_estimator_univariate(self, target, sample_weight):
+        self.y_stats.update(target, sample_weight)
+
+    def _update_estimator_multivariate(self, target, sample_weight):
+        for t in target:
+            self.y_stats[t].update(target[t], sample_weight)
+
+    def __iadd__(self, o):
+        self.x_stats += o.x_stats
+        self.y_stats += o.y_stats
+
+        return self
+
+    def update(self, x, y, sample_weight):
+        self.x_stats.update(x, sample_weight)
+        self._update_estimator(y, sample_weight)
+
+
+class FeatureQuantizer:
+    """ The actual dynamic feature quantization mechanism.
+
+    The input numerical feature is partitioned using equal-sized intervals that are defined
+    by the `radius` parameter. For each interval, estimators of target's mean and variance are
+    kept, as well as an estimator of the feature's mean value. At any time, one can iterate
+    over the ordered stored values in the hash. The ordering is defined by the quantized values
+    of the feature.
+
+    """
+
+    def __init__(self, radius: float):
+        self.radius = radius
+        self.hash = {}
+
+    def __getitem__(self, k):
+        return self.hash[k]
+
+    def __len__(self):
+        return len(self.hash)
+
+    def update(self, x: float, y: typing.Union[float, utils.VectorDict], weight: float):
+        index = math.floor(x / self.radius)
+        try:
+            self.hash[index].update(x, y, weight)
+        except KeyError:
+            self.hash[index] = Slot(x, y, weight)
+
+    def __iter__(self):
+        aux_stats = (
+            stats.Var()
+            if next(iter(self.hash.values())).is_single_target
+            else utils.VectorDict(default_factory=functools.partial(stats.Var))
+        )
+
+        for i in sorted(self.hash.keys()):
+            x = self.hash[i].x_stats.get()
+            aux_stats += self.hash[i].y_stats
+            yield x, aux_stats
+
+    def all(self):
+        branch_ids = []
+        split_stats = []
+        for slot_id in sorted(self.hash.keys()):
+            branch_ids.append(slot_id)
+            split_stats.append(self.hash[slot_id].y_stats)
+
+        return branch_ids, split_stats
```

### Comparing `river-0.8.0/river/tree/splitter/sgt_quantizer.py` & `river-0.9.0/river/tree/splitter/sgt_quantizer.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,181 +1,181 @@
-import math
-import typing
-
-from river import stats
-
-from ..utils import GradHess, GradHessStats
-from .base import Quantizer
-
-
-class DynamicQuantizer(Quantizer):
-    """ Adapted version of the Quantizer Observer (QO)[^1] that is applied to Stochastic Gradient
-    Trees (SGT).
-
-    This feature quantizer starts by partitioning the inputs using the passed `radius` value.
-    As more splits are created in the SGTs, new feature quantizers will use `std * std_prop` as
-    the quantization radius. In the expression, `std` represents the standard deviation of the
-    input data, which is calculated incrementally.
-
-    Parameters
-    ----------
-    radius
-        The initial quantization radius.
-    std_prop
-        The proportion of the standard deviation that is going to be used to define the radius
-        value for new quantizer instances following the initial one.
-
-    References
-    ----------
-    [^1]: Mastelini, S.M. and de Leon Ferreira, A.C.P., 2021. Using dynamical quantization
-    to perform split attempts in online tree regressors. Pattern Recognition Letters.
-    """
-
-    def __init__(self, radius: float = 0.5, std_prop: float = 0.25):
-        super().__init__()
-        self.radius = radius
-        self.std_prop = std_prop
-
-        self.feat_var = stats.Var()
-        self.hash = {}
-
-    def update(self, x_val, gh: GradHess, w: float):
-        self.feat_var.update(x_val, w)
-
-        index = math.floor(x_val / self.radius)
-        if index in self.hash:
-            self.hash[index].update(gh, w)
-        else:
-            ghs = GradHessStats()
-            ghs.update(gh, w)
-            self.hash[index] = ghs
-
-    def __len__(self):
-        return len(self.hash)
-
-    def __getitem__(self, k):
-        return self.hash[k]
-
-    def __iter__(self):
-        for k in sorted(self.hash):
-            yield self.radius * (k + 1), self.hash[k]
-
-    def _get_params(self):
-        params = super()._get_params()
-        new_radius = self.std_prop * math.sqrt(self.feat_var.get())
-
-        if new_radius > 0:
-            params["radius"] = new_radius
-
-        return params
-
-
-class StaticQuantizer(Quantizer):
-    """Quantization strategy originally used in Stochastic Gradient Trees (SGT)[^1].
-
-    Firstly, a buffer of size `warm_start` is stored. The data stored in the buffer is then used
-    to quantize the input feature into `n_bins` intervals. These intervals will be replicated
-    to every new quantizer. Feature values lying outside of the limits defined by the initial
-    buffer will be mapped to the head or tail of the list of intervals.
-
-    Parameters
-    ----------
-    n_bins
-        The number of bins (intervals) to divide the input feature.
-    warm_start
-        The number of observations used to initialize the quantization intervals.
-    buckets
-        This parameter is only used internally by the quantizer, so it must not be set.
-        Once the intervals are defined, new instances of this quantizer will receive the
-        quantization information via this parameter.
-
-    References
-    ----------
-    [^1]: Gouk, H., Pfahringer, B., & Frank, E. (2019, October). Stochastic Gradient Trees.
-    In Asian Conference on Machine Learning (pp. 1094-1109).
-
-    """
-
-    def __init__(
-        self, n_bins: int = 64, warm_start: int = 100, *, buckets: typing.List = None
-    ):
-        super().__init__()
-
-        self.n_bins = n_bins
-        self.warm_start = warm_start
-        self.buckets = buckets
-
-        if self.buckets is None:
-            self._buffer = []
-            self._min = None
-            self._radius = None
-        else:
-            self._buffer = None
-            # Define the quantization radius and the minimum value to data perform shift
-            self._radius = self.buckets[1][0][1] - self.buckets[1][0][0]
-            self._min = self.buckets[0][0][1] - self._radius
-
-    def update(self, x_val, gh: GradHess, w: float):
-        if self.buckets is None:
-            self._buffer.append((x_val, gh, w))
-
-            if len(self._buffer) < self.warm_start:
-                return
-
-            self._min = min(self._buffer, key=lambda t: t[0])[0]
-            _max = max(self._buffer, key=lambda t: t[0])[0]
-            self._radius = (_max - self._min) / self.n_bins
-
-            splits = (
-                [-math.inf]
-                + [self._min + i * self._radius for i in range(1, self.n_bins)]
-                + [math.inf]
-            )
-
-            self.buckets = [
-                ((splits[i], splits[i + 1]), GradHessStats())
-                for i in range(self.n_bins)
-            ]
-
-            # Replay buffer
-            for x_val, gh, w in self._buffer:
-                pos = math.floor((x_val - self._min) / self._radius)
-                if pos >= self.n_bins:
-                    pos = self.n_bins - 1
-                self.buckets[pos][1].update(gh, w)
-
-            # And empty it
-            self._buffer = None
-
-        # Projection scheme
-        pos = math.floor((x_val - self._min) / self._radius)
-        if pos < 0:
-            pos = 0
-        if pos >= self.n_bins:
-            pos = self.n_bins - 1
-
-        # Update the corresponding bucket
-        self.buckets[pos][1].update(gh, w)
-
-    def __len__(self):
-        if self.buckets:
-            return len(self.buckets)
-        return 0
-
-    def __iter__(self):
-        if self.buckets is None:
-            return
-
-        for x_range, ghs in self.buckets:
-            if ghs.total_weight == 0:
-                continue
-            yield x_range[1], ghs
-
-    def _get_params(self):
-        params = super()._get_params()
-
-        if self.buckets is not None:
-            # Create buckets with empty stats: only the tuples with data range are kept
-            buckets = [(b[0], GradHessStats()) for b in self.buckets]
-
-            params["buckets"] = buckets
-        return params
+import math
+import typing
+
+from river import stats
+
+from ..utils import GradHess, GradHessStats
+from .base import Quantizer
+
+
+class DynamicQuantizer(Quantizer):
+    """ Adapted version of the Quantizer Observer (QO)[^1] that is applied to Stochastic Gradient
+    Trees (SGT).
+
+    This feature quantizer starts by partitioning the inputs using the passed `radius` value.
+    As more splits are created in the SGTs, new feature quantizers will use `std * std_prop` as
+    the quantization radius. In the expression, `std` represents the standard deviation of the
+    input data, which is calculated incrementally.
+
+    Parameters
+    ----------
+    radius
+        The initial quantization radius.
+    std_prop
+        The proportion of the standard deviation that is going to be used to define the radius
+        value for new quantizer instances following the initial one.
+
+    References
+    ----------
+    [^1]: Mastelini, S.M. and de Leon Ferreira, A.C.P., 2021. Using dynamical quantization
+    to perform split attempts in online tree regressors. Pattern Recognition Letters.
+    """
+
+    def __init__(self, radius: float = 0.5, std_prop: float = 0.25):
+        super().__init__()
+        self.radius = radius
+        self.std_prop = std_prop
+
+        self.feat_var = stats.Var()
+        self.hash = {}
+
+    def update(self, x_val, gh: GradHess, w: float):
+        self.feat_var.update(x_val, w)
+
+        index = math.floor(x_val / self.radius)
+        if index in self.hash:
+            self.hash[index].update(gh, w)
+        else:
+            ghs = GradHessStats()
+            ghs.update(gh, w)
+            self.hash[index] = ghs
+
+    def __len__(self):
+        return len(self.hash)
+
+    def __getitem__(self, k):
+        return self.hash[k]
+
+    def __iter__(self):
+        for k in sorted(self.hash):
+            yield self.radius * (k + 1), self.hash[k]
+
+    def _get_params(self):
+        params = super()._get_params()
+        new_radius = self.std_prop * math.sqrt(self.feat_var.get())
+
+        if new_radius > 0:
+            params["radius"] = new_radius
+
+        return params
+
+
+class StaticQuantizer(Quantizer):
+    """Quantization strategy originally used in Stochastic Gradient Trees (SGT)[^1].
+
+    Firstly, a buffer of size `warm_start` is stored. The data stored in the buffer is then used
+    to quantize the input feature into `n_bins` intervals. These intervals will be replicated
+    to every new quantizer. Feature values lying outside of the limits defined by the initial
+    buffer will be mapped to the head or tail of the list of intervals.
+
+    Parameters
+    ----------
+    n_bins
+        The number of bins (intervals) to divide the input feature.
+    warm_start
+        The number of observations used to initialize the quantization intervals.
+    buckets
+        This parameter is only used internally by the quantizer, so it must not be set.
+        Once the intervals are defined, new instances of this quantizer will receive the
+        quantization information via this parameter.
+
+    References
+    ----------
+    [^1]: Gouk, H., Pfahringer, B., & Frank, E. (2019, October). Stochastic Gradient Trees.
+    In Asian Conference on Machine Learning (pp. 1094-1109).
+
+    """
+
+    def __init__(
+        self, n_bins: int = 64, warm_start: int = 100, *, buckets: typing.List = None
+    ):
+        super().__init__()
+
+        self.n_bins = n_bins
+        self.warm_start = warm_start
+        self.buckets = buckets
+
+        if self.buckets is None:
+            self._buffer = []
+            self._min = None
+            self._radius = None
+        else:
+            self._buffer = None
+            # Define the quantization radius and the minimum value to data perform shift
+            self._radius = self.buckets[1][0][1] - self.buckets[1][0][0]
+            self._min = self.buckets[0][0][1] - self._radius
+
+    def update(self, x_val, gh: GradHess, w: float):
+        if self.buckets is None:
+            self._buffer.append((x_val, gh, w))
+
+            if len(self._buffer) < self.warm_start:
+                return
+
+            self._min = min(self._buffer, key=lambda t: t[0])[0]
+            _max = max(self._buffer, key=lambda t: t[0])[0]
+            self._radius = (_max - self._min) / self.n_bins
+
+            splits = (
+                [-math.inf]
+                + [self._min + i * self._radius for i in range(1, self.n_bins)]
+                + [math.inf]
+            )
+
+            self.buckets = [
+                ((splits[i], splits[i + 1]), GradHessStats())
+                for i in range(self.n_bins)
+            ]
+
+            # Replay buffer
+            for x_val, gh, w in self._buffer:
+                pos = math.floor((x_val - self._min) / self._radius)
+                if pos >= self.n_bins:
+                    pos = self.n_bins - 1
+                self.buckets[pos][1].update(gh, w)
+
+            # And empty it
+            self._buffer = None
+
+        # Projection scheme
+        pos = math.floor((x_val - self._min) / self._radius)
+        if pos < 0:
+            pos = 0
+        if pos >= self.n_bins:
+            pos = self.n_bins - 1
+
+        # Update the corresponding bucket
+        self.buckets[pos][1].update(gh, w)
+
+    def __len__(self):
+        if self.buckets:
+            return len(self.buckets)
+        return 0
+
+    def __iter__(self):
+        if self.buckets is None:
+            return
+
+        for x_range, ghs in self.buckets:
+            if ghs.total_weight == 0:
+                continue
+            yield x_range[1], ghs
+
+    def _get_params(self):
+        params = super()._get_params()
+
+        if self.buckets is not None:
+            # Create buckets with empty stats: only the tuples with data range are kept
+            buckets = [(b[0], GradHessStats()) for b in self.buckets]
+
+            params["buckets"] = buckets
+        return params
```

### Comparing `river-0.8.0/river/tree/splitter/tebst_splitter.py` & `river-0.9.0/river/tree/splitter/tebst_splitter.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,32 +1,32 @@
-from .ebst_splitter import EBSTSplitter
-
-
-class TEBSTSplitter(EBSTSplitter):
-    """Truncated E-BST.
-
-    Variation of E-BST that rounds the incoming feature values before passing them to the binary
-    search tree (BST). By doing so, the attribute observer might reduce its processing time and
-    memory usage since small variations in the input values will end up being mapped to the same
-    BST node.
-
-    Parameters
-    ----------
-    digits
-        The number of decimal places used to round the input feature values.
-
-    """
-
-    def __init__(self, digits: int = 3):
-        super().__init__()
-        self.digits = digits
-
-    def update(self, att_val, target_val, sample_weight):
-        try:
-            att_val = round(att_val, self.digits)
-            super().update(att_val, target_val, sample_weight)
-        except TypeError:  # feature value is None
-            pass
-
-    def cond_proba(self, att_val, target_val):
-        """Not implemented in regression splitters."""
-        raise NotImplementedError
+from .ebst_splitter import EBSTSplitter
+
+
+class TEBSTSplitter(EBSTSplitter):
+    """Truncated E-BST.
+
+    Variation of E-BST that rounds the incoming feature values before passing them to the binary
+    search tree (BST). By doing so, the attribute observer might reduce its processing time and
+    memory usage since small variations in the input values will end up being mapped to the same
+    BST node.
+
+    Parameters
+    ----------
+    digits
+        The number of decimal places used to round the input feature values.
+
+    """
+
+    def __init__(self, digits: int = 3):
+        super().__init__()
+        self.digits = digits
+
+    def update(self, att_val, target_val, sample_weight):
+        try:
+            att_val = round(att_val, self.digits)
+            super().update(att_val, target_val, sample_weight)
+        except TypeError:  # feature value is None
+            pass
+
+    def cond_proba(self, att_val, target_val):
+        """Not implemented in regression splitters."""
+        raise NotImplementedError
```

### Comparing `river-0.8.0/river/tree/stochastic_gradient_tree.py` & `river-0.9.0/river/tree/stochastic_gradient_tree.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,417 +1,419 @@
-import abc
-import math
-import typing
-
-from scipy.stats import f as f_dist
-
-from river import base, tree
-
-from .losses import BinaryCrossEntropyLoss, SquaredErrorLoss
-from .nodes.branch import DTBranch, NominalMultiwayBranch, NumericBinaryBranch
-from .nodes.sgt_nodes import SGTLeaf
-from .utils import BranchFactory, GradHessMerit
-
-
-class StochasticGradientTree(base.Estimator, abc.ABC):
-    """ Base Stochastic Gradient Tree (SGT) class.
-
-    This class defines the main characteristics that are shared by the different SGT
-    implementations.
-
-    """
-
-    def __init__(
-        self,
-        loss_func,
-        delta,
-        grace_period,
-        init_pred,
-        max_depth,
-        lambda_value,
-        gamma,
-        nominal_attributes,
-        feature_quantizer,
-    ):
-        # What really defines how a SGT works is its loss function
-        self.loss_func = loss_func
-        self.delta = delta
-        self.grace_period = grace_period
-        self.init_pred = init_pred
-        self.max_depth = max_depth if max_depth else math.inf
-
-        if lambda_value < 0.0:
-            raise ValueError('Invalid value: "lambda_value" must be positive.')
-
-        if gamma < 0.0:
-            raise ValueError('Invalid value: "gamma" must be positive.')
-
-        self.lambda_value = lambda_value
-        self.gamma = gamma
-        self.nominal_attributes = (
-            set(nominal_attributes) if nominal_attributes else set()
-        )
-        self.feature_quantizer = (
-            feature_quantizer
-            if feature_quantizer is not None
-            else tree.splitter.StaticQuantizer()
-        )
-
-        self._root: SGTLeaf = SGTLeaf(prediction=self.init_pred)
-
-        # set used to check whether categorical feature has been already split
-        self._split_features = set()
-        self._n_splits = 0
-        self._n_node_updates = 0
-        self._n_observations = 0
-
-    def _target_transform(self, y):
-        """Apply transformation to the raw target input.
-
-        Different strategies are used for classification and regression. By default, use
-        an identity function.
-
-        Parameters
-        ----------
-        y
-            The target value, over which the transformation will be applied.
-        """
-        return y
-
-    def learn_one(self, x, y, *, w=1.0):
-        self._n_observations += w
-
-        """ Update Stochastic Gradient Tree with a single instance. """
-        y_true_trs = self._target_transform(y)
-
-        p_node = None
-        node = None
-        if not isinstance(self._root, SGTLeaf):
-            path = iter(self._root.walk(x, until_leaf=False))
-            while True:
-                aux = next(path, None)
-                if aux is None:
-                    break
-                p_node = node
-                node = aux
-        else:
-            node = self._root
-
-        # A leaf could not be reached in a single attempt let's deal with that
-        if isinstance(node, (NumericBinaryBranch, NominalMultiwayBranch)):
-            while True:
-                # Split node encountered a previously unseen categorical value (in a multi-way
-                #  test), so there is no branch to sort the instance to
-                if node.max_branches() == -1 and node.feature in x:
-                    # Create a new branch to the new categorical value
-                    leaf = SGTLeaf(depth=node.depth + 1, split_params=node.stats.copy())
-                    #
-                    node.add_child(x[node.feature], leaf)
-                    node = leaf
-                # The split feature is missing in the instance. Hence, we pass the new example
-                # to the most traversed path in the current subtree
-                else:
-                    _, node = node.most_common_path()
-                    # And we keep trying to reach a leaf
-                    if isinstance(node, DTBranch):
-                        node = node.traverse(x, until_leaf=False)
-                # Once a leaf is reached, the traversal can stop
-                if isinstance(node, SGTLeaf):
-                    break
-
-            y_pred_raw = self.loss_func.transfer(node.prediction())
-            grad_hess = self.loss_func.compute_derivatives(y_true_trs, y_pred_raw)
-            node.update(x, grad_hess, self, w)
-        else:  # Node is a leaf
-            y_pred_raw = self.loss_func.transfer(node.prediction())
-            grad_hess = self.loss_func.compute_derivatives(y_true_trs, y_pred_raw)
-            node.update(x, grad_hess, self, w)
-
-            if node.total_weight - node.last_split_attempt_at < self.grace_period:
-                return self
-
-            # Update split attempt data
-            node.last_split_attempt_at = node.total_weight
-
-            # If the maximum depth is reached, attempt to apply a "null split", i.e., update the
-            # prediction value
-            if node.depth >= self.max_depth:
-                # Null split: update the prediction using the new gradient information
-                best_split = BranchFactory()
-                best_split.merit = GradHessMerit()
-                best_split.merit.delta_pred = node.delta_prediction(
-                    node.update_stats.mean, self.lambda_value
-                )
-                dlms = node.update_stats.delta_loss_mean_var(
-                    best_split.merit.delta_pred
-                )
-                best_split.merit.loss_mean = dlms.mean.get()
-                best_split.merit.loss_var = dlms.get()
-            else:  # Proceed with the standard split attempt procedure
-                best_split = node.find_best_split(self)
-
-            p = self._compute_p_value(best_split.merit, node.total_weight)
-
-            if p < self.delta and best_split.merit.loss_mean < 0:
-                p_branch = p_node.branch_no(x) if isinstance(p_node, DTBranch) else None
-                node.apply_split(best_split, p_node, p_branch, self)
-
-        return self
-
-    @staticmethod
-    def _compute_p_value(merit, n_observations):
-        # Null hypothesis: expected loss is zero
-        # Alternative hypothesis: expected loss is not zero
-
-        f_value = (
-            n_observations * (merit.loss_mean * merit.loss_mean) / merit.loss_var
-            if merit.loss_var > 0.0
-            else None
-        )
-
-        if f_value is None:
-            return 1.0
-
-        return 1 - f_dist.cdf(f_value, 1, n_observations - 1)
-
-    @property
-    def n_splits(self):
-        return self._n_splits
-
-    @property
-    def n_node_updates(self):
-        return self._n_node_updates
-
-    @property
-    def n_observations(self):
-        return self._n_observations
-
-    @property
-    def height(self) -> int:
-        if self._root:
-            return self._root.height
-
-    @property
-    def n_nodes(self):
-        if self._root:
-            return self._root.n_nodes
-
-    @property
-    def n_branches(self):
-        if self._root:
-            return self._root.n_branches
-
-    @property
-    def n_leaves(self):
-        if self._root:
-            return self._root.n_leaves
-
-
-class SGTClassifier(StochasticGradientTree, base.Classifier):
-    """Stochastic Gradient Tree[^1] for binary classification.
-
-    Binary decision tree classifier that minimizes the binary cross-entropy to guide its growth.
-
-    Stochastic Gradient Trees (SGT) directly minimize a loss function to guide tree growth and
-    update their predictions. Thus, they differ from other incrementally tree learners that do
-    not directly optimize the loss, but data impurity-related heuristics.
-
-    Parameters
-    ----------
-    delta
-        Define the significance level of the F-tests performed to decide upon creating splits
-        or updating predictions.
-    grace_period
-        Interval between split attempts or prediction updates.
-    init_pred
-        Initial value predicted by the tree.
-    max_depth
-        The maximum depth the tree might reach. If set to `None`, the trees will grow
-        indefinitely.
-    lambda_value
-        Positive float value used to impose a penalty over the tree's predictions and force
-        them to become smaller. The greater the lambda value, the more constrained are the
-        predictions.
-    gamma
-        Positive float value used to impose a penalty over the tree's splits and force them to
-        be avoided when possible. The greater the gamma value, the smaller the chance of a
-        split occurring.
-    nominal_attributes
-        List with identifiers of the nominal attributes. If None, all features containing
-        numbers are assumed to be numeric.
-    feature_quantizer
-        The algorithm used to quantize numeric features. Either a static quantizer (as in the
-        original implementation) or a dynamic quantizer can be used. The correct choice and setup
-        of the feature quantizer is a crucial step to determine the performance of SGTs.
-        Feature quantizers are akin to the attribute observers used in Hoeffding Trees. By
-        default, an instance of `tree.splitter.StaticQuantizer` (with default parameters) is
-        used if this parameter is not set.
-
-    Examples
-    --------
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import metrics
-    >>> from river import tree
-
-    >>> dataset = datasets.Phishing()
-    >>> model = tree.SGTClassifier(
-    ...     feature_quantizer=tree.splitter.StaticQuantizer(
-    ...         n_bins=32, warm_start=10
-    ...     )
-    ... )
-    >>> metric = metrics.Accuracy()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    Accuracy: 82.24%
-
-    References
-    ---------
-    [^1]: Gouk, H., Pfahringer, B., & Frank, E. (2019, October). Stochastic Gradient Trees.
-    In Asian Conference on Machine Learning (pp. 1094-1109).
-
-    """
-
-    def __init__(
-        self,
-        delta: float = 1e-7,
-        grace_period: int = 200,
-        init_pred: float = 0.0,
-        max_depth: typing.Optional[int] = None,
-        lambda_value: float = 0.1,
-        gamma: float = 1.0,
-        nominal_attributes: typing.Optional[typing.List] = None,
-        feature_quantizer: tree.splitter.Quantizer = None,
-    ):
-
-        super().__init__(
-            loss_func=BinaryCrossEntropyLoss(),
-            delta=delta,
-            grace_period=grace_period,
-            init_pred=init_pred,
-            max_depth=max_depth,
-            lambda_value=lambda_value,
-            gamma=gamma,
-            nominal_attributes=nominal_attributes,
-            feature_quantizer=feature_quantizer,
-        )
-
-    def _target_transform(self, y):
-        return float(y)
-
-    def predict_proba_one(self, x: dict) -> typing.Dict[base.typing.ClfTarget, float]:
-        if isinstance(self._root, DTBranch):
-            leaf = self._root.traverse(x, until_leaf=True)
-        else:
-            leaf = self._root
-
-        t_proba = self.loss_func.transfer(leaf.prediction())
-
-        return {True: t_proba, False: 1 - t_proba}
-
-
-class SGTRegressor(StochasticGradientTree, base.Regressor):
-    """Stochastic Gradient Tree for regression.
-
-    Incremental decision tree regressor that minimizes the mean square error to guide its growth.
-
-    Stochastic Gradient Trees (SGT) directly minimize a loss function to guide tree growth and
-    update their predictions. Thus, they differ from other incrementally tree learners that do
-    not directly optimize the loss, but a data impurity-related heuristic.
-
-    Parameters
-    ----------
-    delta
-        Define the significance level of the F-tests performed to decide upon creating splits
-        or updating predictions.
-    grace_period
-        Interval between split attempts or prediction updates.
-    init_pred
-        Initial value predicted by the tree.
-    max_depth
-        The maximum depth the tree might reach. If set to `None`, the trees will grow
-        indefinitely.
-    lambda_value
-        Positive float value used to impose a penalty over the tree's predictions and force
-        them to become smaller. The greater the lambda value, the more constrained are the
-        predictions.
-    gamma
-        Positive float value used to impose a penalty over the tree's splits and force them to
-        be avoided when possible. The greater the gamma value, the smaller the chance of a
-        split occurring.
-    nominal_attributes
-        List with identifiers of the nominal attributes. If None, all features containing
-        numbers are assumed to be numeric.
-    feature_quantizer
-        The algorithm used to quantize numeric features. Either a static quantizer (as in the
-        original implementation) or a dynamic quantizer can be used. The correct choice and setup
-        of the feature quantizer is a crucial step to determine the performance of SGTs.
-        Feature quantizers are akin to the attribute observers used in Hoeffding Trees. By
-        default, an instance of `tree.splitter.StaticQuantizer` (with default parameters) is
-        used if this parameter is not set.
-
-    Examples
-    --------
-    >>> from river import datasets
-    >>> from river import evaluate
-    >>> from river import metrics
-    >>> from river import tree
-
-    >>> dataset = datasets.TrumpApproval()
-    >>> model = tree.SGTRegressor(
-    ...     grace_period=20,
-    ...     feature_quantizer=tree.splitter.DynamicQuantizer(std_prop=0.1)
-    ... )
-    >>> metric = metrics.MAE()
-
-    >>> evaluate.progressive_val_score(dataset, model, metric)
-    MAE: 1.819874
-
-    Notes
-    -----
-    This implementation enhances the original proposal [^1] by using an incremental strategy to
-    discretize numerical features dynamically, rather than relying on a calibration set and
-    parameterized number of bins. The strategy used is an adaptation of the Quantization Observer
-    (QO) [^2]. Different bin size setting policies are available for selection.
-    They directly related to number of split candidates the tree is going to explore, and thus,
-    how accurate its split decisions are going to be. Besides, the number of stored bins per
-    feature is directly related to the tree's memory usage and runtime.
-
-    References
-    ---------
-    [^1]: Gouk, H., Pfahringer, B., & Frank, E. (2019, October). Stochastic Gradient Trees.
-    In Asian Conference on Machine Learning (pp. 1094-1109).
-    [^2]: Mastelini, S.M. and de Leon Ferreira, A.C.P., 2021. Using dynamical quantization
-    to perform split attempts in online tree regressors. Pattern Recognition Letters.
-
-    """
-
-    def __init__(
-        self,
-        delta: float = 1e-7,
-        grace_period: int = 200,
-        init_pred: float = 0.0,
-        max_depth: typing.Optional[int] = None,
-        lambda_value: float = 0.1,
-        gamma: float = 1.0,
-        nominal_attributes: typing.Optional[typing.List] = None,
-        feature_quantizer: tree.splitter.Quantizer = None,
-    ):
-
-        super().__init__(
-            loss_func=SquaredErrorLoss(),
-            delta=delta,
-            grace_period=grace_period,
-            init_pred=init_pred,
-            max_depth=max_depth,
-            lambda_value=lambda_value,
-            gamma=gamma,
-            nominal_attributes=nominal_attributes,
-            feature_quantizer=feature_quantizer,
-        )
-
-    def predict_one(self, x: dict) -> base.typing.RegTarget:
-        if isinstance(self._root, DTBranch):
-            leaf = self._root.traverse(x, until_leaf=True)
-        else:
-            leaf = self._root
-        return self.loss_func.transfer(leaf.prediction())
+import abc
+import math
+import typing
+
+from scipy.stats import f as f_dist
+
+from river import base, tree
+
+from .losses import BinaryCrossEntropyLoss, SquaredErrorLoss
+from .nodes.branch import DTBranch, NominalMultiwayBranch, NumericBinaryBranch
+from .nodes.sgt_nodes import SGTLeaf
+from .utils import BranchFactory, GradHessMerit
+
+
+class StochasticGradientTree(base.Estimator, abc.ABC):
+    """ Base Stochastic Gradient Tree (SGT) class.
+
+    This class defines the main characteristics that are shared by the different SGT
+    implementations.
+
+    """
+
+    def __init__(
+        self,
+        loss_func,
+        delta,
+        grace_period,
+        init_pred,
+        max_depth,
+        lambda_value,
+        gamma,
+        nominal_attributes,
+        feature_quantizer,
+    ):
+        # What really defines how a SGT works is its loss function
+        self.loss_func = loss_func
+        self.delta = delta
+        self.grace_period = grace_period
+        self.init_pred = init_pred
+        self.max_depth = max_depth if max_depth else math.inf
+
+        if lambda_value < 0.0:
+            raise ValueError('Invalid value: "lambda_value" must be positive.')
+
+        if gamma < 0.0:
+            raise ValueError('Invalid value: "gamma" must be positive.')
+
+        self.lambda_value = lambda_value
+        self.gamma = gamma
+        self.nominal_attributes = (
+            set(nominal_attributes) if nominal_attributes else set()
+        )
+        self.feature_quantizer = (
+            feature_quantizer
+            if feature_quantizer is not None
+            else tree.splitter.StaticQuantizer()
+        )
+
+        self._root: SGTLeaf = SGTLeaf(prediction=self.init_pred)
+
+        # set used to check whether categorical feature has been already split
+        self._split_features = set()
+        self._n_splits = 0
+        self._n_node_updates = 0
+        self._n_observations = 0
+
+    def _target_transform(self, y):
+        """Apply transformation to the raw target input.
+
+        Different strategies are used for classification and regression. By default, use
+        an identity function.
+
+        Parameters
+        ----------
+        y
+            The target value, over which the transformation will be applied.
+        """
+        return y
+
+    def learn_one(self, x, y, *, w=1.0):
+        self._n_observations += w
+
+        """ Update Stochastic Gradient Tree with a single instance. """
+        y_true_trs = self._target_transform(y)
+
+        p_node = None
+        node = None
+        if not isinstance(self._root, SGTLeaf):
+            path = iter(self._root.walk(x, until_leaf=False))
+            while True:
+                aux = next(path, None)
+                if aux is None:
+                    break
+                p_node = node
+                node = aux
+        else:
+            node = self._root
+
+        # A leaf could not be reached in a single attempt let's deal with that
+        if isinstance(node, (NumericBinaryBranch, NominalMultiwayBranch)):
+            while True:
+                # Split node encountered a previously unseen categorical value (in a multi-way
+                #  test), so there is no branch to sort the instance to
+                if node.max_branches() == -1 and node.feature in x:
+                    # Create a new branch to the new categorical value
+                    leaf = SGTLeaf(depth=node.depth + 1, split_params=node.stats.copy())
+                    #
+                    node.add_child(x[node.feature], leaf)
+                    node = leaf
+                # The split feature is missing in the instance. Hence, we pass the new example
+                # to the most traversed path in the current subtree
+                else:
+                    _, node = node.most_common_path()
+                    # And we keep trying to reach a leaf
+                    if isinstance(node, DTBranch):
+                        node = node.traverse(x, until_leaf=False)
+                # Once a leaf is reached, the traversal can stop
+                if isinstance(node, SGTLeaf):
+                    break
+
+            y_pred_raw = self.loss_func.transfer(node.prediction())
+            grad_hess = self.loss_func.compute_derivatives(y_true_trs, y_pred_raw)
+            node.update(x, grad_hess, self, w)
+        else:  # Node is a leaf
+            y_pred_raw = self.loss_func.transfer(node.prediction())
+            grad_hess = self.loss_func.compute_derivatives(y_true_trs, y_pred_raw)
+            node.update(x, grad_hess, self, w)
+
+            if node.total_weight - node.last_split_attempt_at < self.grace_period:
+                return self
+
+            # Update split attempt data
+            node.last_split_attempt_at = node.total_weight
+
+            # If the maximum depth is reached, attempt to apply a "null split", i.e., update the
+            # prediction value
+            if node.depth >= self.max_depth:
+                # Null split: update the prediction using the new gradient information
+                best_split = BranchFactory()
+                best_split.merit = GradHessMerit()
+                best_split.merit.delta_pred = node.delta_prediction(
+                    node.update_stats.mean, self.lambda_value
+                )
+                dlms = node.update_stats.delta_loss_mean_var(
+                    best_split.merit.delta_pred
+                )
+                best_split.merit.loss_mean = dlms.mean.get()
+                best_split.merit.loss_var = dlms.get()
+            else:  # Proceed with the standard split attempt procedure
+                best_split = node.find_best_split(self)
+
+            p = self._compute_p_value(best_split.merit, node.total_weight)
+
+            if p < self.delta and best_split.merit.loss_mean < 0:
+                p_branch = p_node.branch_no(x) if isinstance(p_node, DTBranch) else None
+                node.apply_split(best_split, p_node, p_branch, self)
+
+        return self
+
+    @staticmethod
+    def _compute_p_value(merit, n_observations):
+        # Null hypothesis: expected loss is zero
+        # Alternative hypothesis: expected loss is not zero
+
+        f_value = (
+            n_observations * (merit.loss_mean * merit.loss_mean) / merit.loss_var
+            if merit.loss_var > 0.0
+            else None
+        )
+
+        if f_value is None:
+            return 1.0
+
+        return 1 - f_dist.cdf(f_value, 1, n_observations - 1)
+
+    @property
+    def n_splits(self):
+        return self._n_splits
+
+    @property
+    def n_node_updates(self):
+        return self._n_node_updates
+
+    @property
+    def n_observations(self):
+        return self._n_observations
+
+    @property
+    def height(self) -> int:
+        if self._root:
+            return self._root.height
+
+    @property
+    def n_nodes(self):
+        if self._root:
+            return self._root.n_nodes
+
+    @property
+    def n_branches(self):
+        if self._root:
+            return self._root.n_branches
+
+    @property
+    def n_leaves(self):
+        if self._root:
+            return self._root.n_leaves
+
+
+class SGTClassifier(StochasticGradientTree, base.Classifier):
+    """Stochastic Gradient Tree[^1] for binary classification.
+
+    Binary decision tree classifier that minimizes the binary cross-entropy to guide its growth.
+
+    Stochastic Gradient Trees (SGT) directly minimize a loss function to guide tree growth and
+    update their predictions. Thus, they differ from other incrementally tree learners that do
+    not directly optimize the loss, but data impurity-related heuristics.
+
+    Parameters
+    ----------
+    delta
+        Define the significance level of the F-tests performed to decide upon creating splits
+        or updating predictions.
+    grace_period
+        Interval between split attempts or prediction updates.
+    init_pred
+        Initial value predicted by the tree.
+    max_depth
+        The maximum depth the tree might reach. If set to `None`, the trees will grow
+        indefinitely.
+    lambda_value
+        Positive float value used to impose a penalty over the tree's predictions and force
+        them to become smaller. The greater the lambda value, the more constrained are the
+        predictions.
+    gamma
+        Positive float value used to impose a penalty over the tree's splits and force them to
+        be avoided when possible. The greater the gamma value, the smaller the chance of a
+        split occurring.
+    nominal_attributes
+        List with identifiers of the nominal attributes. If None, all features containing
+        numbers are assumed to be numeric.
+    feature_quantizer
+        The algorithm used to quantize numeric features. Either a static quantizer (as in the
+        original implementation) or a dynamic quantizer can be used. The correct choice and setup
+        of the feature quantizer is a crucial step to determine the performance of SGTs.
+        Feature quantizers are akin to the attribute observers used in Hoeffding Trees. By
+        default, an instance of `tree.splitter.StaticQuantizer` (with default parameters) is
+        used if this parameter is not set.
+
+    Examples
+    --------
+    >>> from river import datasets
+    >>> from river import evaluate
+    >>> from river import metrics
+    >>> from river import tree
+
+    >>> dataset = datasets.Phishing()
+    >>> model = tree.SGTClassifier(
+    ...     feature_quantizer=tree.splitter.StaticQuantizer(
+    ...         n_bins=32, warm_start=10
+    ...     )
+    ... )
+    >>> metric = metrics.Accuracy()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    Accuracy: 82.24%
+
+    References
+    ---------
+    [^1]: Gouk, H., Pfahringer, B., & Frank, E. (2019, October). Stochastic Gradient Trees.
+    In Asian Conference on Machine Learning (pp. 1094-1109).
+
+    """
+
+    def __init__(
+        self,
+        delta: float = 1e-7,
+        grace_period: int = 200,
+        init_pred: float = 0.0,
+        max_depth: typing.Optional[int] = None,
+        lambda_value: float = 0.1,
+        gamma: float = 1.0,
+        nominal_attributes: typing.Optional[typing.List] = None,
+        feature_quantizer: tree.splitter.Quantizer = None,
+    ):
+
+        super().__init__(
+            loss_func=BinaryCrossEntropyLoss(),
+            delta=delta,
+            grace_period=grace_period,
+            init_pred=init_pred,
+            max_depth=max_depth,
+            lambda_value=lambda_value,
+            gamma=gamma,
+            nominal_attributes=nominal_attributes,
+            feature_quantizer=feature_quantizer,
+        )
+
+    def _target_transform(self, y):
+        return float(y)
+
+    def predict_proba_one(self, x: dict) -> typing.Dict[base.typing.ClfTarget, float]:
+        if isinstance(self._root, DTBranch):
+            leaf = self._root.traverse(x, until_leaf=True)
+        else:
+            leaf = self._root
+
+        t_proba = self.loss_func.transfer(leaf.prediction())
+
+        return {True: t_proba, False: 1 - t_proba}
+
+
+class SGTRegressor(StochasticGradientTree, base.Regressor):
+    """Stochastic Gradient Tree for regression.
+
+    Incremental decision tree regressor that minimizes the mean square error to guide its growth.
+
+    Stochastic Gradient Trees (SGT) directly minimize a loss function to guide tree growth and
+    update their predictions. Thus, they differ from other incrementally tree learners that do
+    not directly optimize the loss, but a data impurity-related heuristic.
+
+    Parameters
+    ----------
+    delta
+        Define the significance level of the F-tests performed to decide upon creating splits
+        or updating predictions.
+    grace_period
+        Interval between split attempts or prediction updates.
+    init_pred
+        Initial value predicted by the tree.
+    max_depth
+        The maximum depth the tree might reach. If set to `None`, the trees will grow
+        indefinitely.
+    lambda_value
+        Positive float value used to impose a penalty over the tree's predictions and force
+        them to become smaller. The greater the lambda value, the more constrained are the
+        predictions.
+    gamma
+        Positive float value used to impose a penalty over the tree's splits and force them to
+        be avoided when possible. The greater the gamma value, the smaller the chance of a
+        split occurring.
+    nominal_attributes
+        List with identifiers of the nominal attributes. If None, all features containing
+        numbers are assumed to be numeric.
+    feature_quantizer
+        The algorithm used to quantize numeric features. Either a static quantizer (as in the
+        original implementation) or a dynamic quantizer can be used. The correct choice and setup
+        of the feature quantizer is a crucial step to determine the performance of SGTs.
+        Feature quantizers are akin to the attribute observers used in Hoeffding Trees. By
+        default, an instance of `tree.splitter.StaticQuantizer` (with default parameters) is
+        used if this parameter is not set.
+
+    Examples
+    --------
+    >>> from river import datasets
+    >>> from river import evaluate
+    >>> from river import metrics
+    >>> from river import tree
+
+    >>> dataset = datasets.TrumpApproval()
+    >>> model = tree.SGTRegressor(
+    ...     delta=0.01,
+    ...     lambda_value=0.01,
+    ...     grace_period=20,
+    ...     feature_quantizer=tree.splitter.DynamicQuantizer(std_prop=0.1)
+    ... )
+    >>> metric = metrics.MAE()
+
+    >>> evaluate.progressive_val_score(dataset, model, metric)
+    MAE: 1.721818
+
+    Notes
+    -----
+    This implementation enhances the original proposal [^1] by using an incremental strategy to
+    discretize numerical features dynamically, rather than relying on a calibration set and
+    parameterized number of bins. The strategy used is an adaptation of the Quantization Observer
+    (QO) [^2]. Different bin size setting policies are available for selection.
+    They directly related to number of split candidates the tree is going to explore, and thus,
+    how accurate its split decisions are going to be. Besides, the number of stored bins per
+    feature is directly related to the tree's memory usage and runtime.
+
+    References
+    ---------
+    [^1]: Gouk, H., Pfahringer, B., & Frank, E. (2019, October). Stochastic Gradient Trees.
+    In Asian Conference on Machine Learning (pp. 1094-1109).
+    [^2]: Mastelini, S.M. and de Leon Ferreira, A.C.P., 2021. Using dynamical quantization
+    to perform split attempts in online tree regressors. Pattern Recognition Letters.
+
+    """
+
+    def __init__(
+        self,
+        delta: float = 1e-7,
+        grace_period: int = 200,
+        init_pred: float = 0.0,
+        max_depth: typing.Optional[int] = None,
+        lambda_value: float = 0.1,
+        gamma: float = 1.0,
+        nominal_attributes: typing.Optional[typing.List] = None,
+        feature_quantizer: tree.splitter.Quantizer = None,
+    ):
+
+        super().__init__(
+            loss_func=SquaredErrorLoss(),
+            delta=delta,
+            grace_period=grace_period,
+            init_pred=init_pred,
+            max_depth=max_depth,
+            lambda_value=lambda_value,
+            gamma=gamma,
+            nominal_attributes=nominal_attributes,
+            feature_quantizer=feature_quantizer,
+        )
+
+    def predict_one(self, x: dict) -> base.typing.RegTarget:
+        if isinstance(self._root, DTBranch):
+            leaf = self._root.traverse(x, until_leaf=True)
+        else:
+            leaf = self._root
+        return self.loss_func.transfer(leaf.prediction())
```

### Comparing `river-0.8.0/river/tree/test_base.py` & `river-0.9.0/river/tree/test_base.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,111 +1,111 @@
-from river.tree.base import Branch, Leaf
-
-
-class BinaryBranch(Branch):
-    def __init__(self, left, right, threshold=None, **kwargs):
-        super().__init__(left, right)
-        self.threshold = threshold
-        self.__dict__.update(kwargs)
-
-    def next(self, x):
-        if x < self.threshold:
-            return self.children[0]
-        return self.children[1]
-
-    def most_common_path(self):
-        raise NotImplementedError
-
-    @property
-    def repr_split(self):
-        return f"x < {self.threshold}"
-
-
-def test_size():
-
-    tree = BinaryBranch(
-        BinaryBranch(
-            BinaryBranch(BinaryBranch(Leaf(), Leaf()), Leaf()),
-            BinaryBranch(Leaf(), Leaf()),
-        ),
-        BinaryBranch(Leaf(), Leaf()),
-    )
-
-    assert tree.n_nodes == tree.n_branches + tree.n_leaves == 6 + 7
-    assert (
-        tree.children[0].n_nodes
-        == tree.children[0].n_branches + tree.children[0].n_leaves
-        == 4 + 5
-    )
-    assert (
-        tree.children[1].n_nodes
-        == tree.children[1].n_branches + tree.children[1].n_leaves
-        == 1 + 2
-    )
-    assert (
-        tree.children[1].children[0].n_nodes
-        == tree.children[1].children[0].n_branches
-        + tree.children[1].children[0].n_leaves
-        == 0 + 1
-    )
-
-
-def test_height():
-
-    tree = BinaryBranch(
-        BinaryBranch(
-            BinaryBranch(BinaryBranch(Leaf(), Leaf()), Leaf(),),
-            BinaryBranch(Leaf(), Leaf()),
-        ),
-        BinaryBranch(Leaf(), Leaf()),
-    )
-
-    assert tree.height == 5
-    assert tree.children[0].height == 4
-    assert tree.children[1].height == 2
-    assert tree.children[1].children[0].height == 1
-
-
-def test_iter_dfs():
-
-    tree = BinaryBranch(BinaryBranch(Leaf(no=3), Leaf(no=4), no=2), Leaf(no=5), no=1)
-
-    for i, node in enumerate(tree.iter_dfs(), start=1):
-        assert i == node.no
-
-
-def test_iter_bfs():
-
-    tree = BinaryBranch(BinaryBranch(Leaf(no=4), Leaf(no=5), no=2), Leaf(no=3), no=1)
-
-    for i, node in enumerate(tree.iter_bfs(), start=1):
-        assert i == node.no
-
-
-def test_iter_leaves():
-
-    tree = BinaryBranch(BinaryBranch(Leaf(no=1), Leaf(no=2)), Leaf(no=3))
-
-    for i, leaf in enumerate(tree.iter_leaves(), start=1):
-        assert i == leaf.no
-
-
-def test_iter_branches():
-
-    tree = BinaryBranch(
-        BinaryBranch(BinaryBranch(Leaf(), Leaf(), no=3), Leaf(), no=2),
-        BinaryBranch(Leaf(), Leaf(), no=4),
-        no=1,
-    )
-
-    for i, branch in enumerate(tree.iter_branches(), start=1):
-        assert i == branch.no
-
-
-def test_iter_edges():
-
-    tree = BinaryBranch(BinaryBranch(Leaf(no=3), Leaf(no=4), no=2), Leaf(no=5), no=1)
-
-    order = [(1, 2), (2, 3), (2, 4), (1, 5)]
-
-    for i, (parent, child) in enumerate(tree.iter_edges()):
-        assert order[i] == (parent.no, child.no)
+from river.tree.base import Branch, Leaf
+
+
+class BinaryBranch(Branch):
+    def __init__(self, left, right, threshold=None, **kwargs):
+        super().__init__(left, right)
+        self.threshold = threshold
+        self.__dict__.update(kwargs)
+
+    def next(self, x):
+        if x < self.threshold:
+            return self.children[0]
+        return self.children[1]
+
+    def most_common_path(self):
+        raise NotImplementedError
+
+    @property
+    def repr_split(self):
+        return f"x < {self.threshold}"
+
+
+def test_size():
+
+    tree = BinaryBranch(
+        BinaryBranch(
+            BinaryBranch(BinaryBranch(Leaf(), Leaf()), Leaf()),
+            BinaryBranch(Leaf(), Leaf()),
+        ),
+        BinaryBranch(Leaf(), Leaf()),
+    )
+
+    assert tree.n_nodes == tree.n_branches + tree.n_leaves == 6 + 7
+    assert (
+        tree.children[0].n_nodes
+        == tree.children[0].n_branches + tree.children[0].n_leaves
+        == 4 + 5
+    )
+    assert (
+        tree.children[1].n_nodes
+        == tree.children[1].n_branches + tree.children[1].n_leaves
+        == 1 + 2
+    )
+    assert (
+        tree.children[1].children[0].n_nodes
+        == tree.children[1].children[0].n_branches
+        + tree.children[1].children[0].n_leaves
+        == 0 + 1
+    )
+
+
+def test_height():
+
+    tree = BinaryBranch(
+        BinaryBranch(
+            BinaryBranch(BinaryBranch(Leaf(), Leaf()), Leaf(),),
+            BinaryBranch(Leaf(), Leaf()),
+        ),
+        BinaryBranch(Leaf(), Leaf()),
+    )
+
+    assert tree.height == 5
+    assert tree.children[0].height == 4
+    assert tree.children[1].height == 2
+    assert tree.children[1].children[0].height == 1
+
+
+def test_iter_dfs():
+
+    tree = BinaryBranch(BinaryBranch(Leaf(no=3), Leaf(no=4), no=2), Leaf(no=5), no=1)
+
+    for i, node in enumerate(tree.iter_dfs(), start=1):
+        assert i == node.no
+
+
+def test_iter_bfs():
+
+    tree = BinaryBranch(BinaryBranch(Leaf(no=4), Leaf(no=5), no=2), Leaf(no=3), no=1)
+
+    for i, node in enumerate(tree.iter_bfs(), start=1):
+        assert i == node.no
+
+
+def test_iter_leaves():
+
+    tree = BinaryBranch(BinaryBranch(Leaf(no=1), Leaf(no=2)), Leaf(no=3))
+
+    for i, leaf in enumerate(tree.iter_leaves(), start=1):
+        assert i == leaf.no
+
+
+def test_iter_branches():
+
+    tree = BinaryBranch(
+        BinaryBranch(BinaryBranch(Leaf(), Leaf(), no=3), Leaf(), no=2),
+        BinaryBranch(Leaf(), Leaf(), no=4),
+        no=1,
+    )
+
+    for i, branch in enumerate(tree.iter_branches(), start=1):
+        assert i == branch.no
+
+
+def test_iter_edges():
+
+    tree = BinaryBranch(BinaryBranch(Leaf(no=3), Leaf(no=4), no=2), Leaf(no=5), no=1)
+
+    order = [(1, 2), (2, 3), (2, 4), (1, 5)]
+
+    for i, (parent, child) in enumerate(tree.iter_edges()):
+        assert order[i] == (parent.no, child.no)
```

### Comparing `river-0.8.0/river/tree/test_splitter.py` & `river-0.9.0/river/tree/test_splitter.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,67 +1,67 @@
-import pytest
-
-from river import datasets, synth, tree
-
-
-def get_regression_data():
-    return iter(synth.Friedman(seed=42).take(200))
-
-
-@pytest.mark.parametrize(
-    "dataset, splitter",
-    [
-        (datasets.Phishing(), tree.splitter.ExhaustiveSplitter()),
-        (datasets.Phishing(), tree.splitter.HistogramSplitter()),
-        (datasets.Phishing(), tree.splitter.GaussianSplitter()),
-    ],
-)
-def test_class_splitter(dataset, splitter):
-    model = tree.HoeffdingTreeClassifier(
-        splitter=splitter, grace_period=10, leaf_prediction="mc", split_confidence=0.1
-    )
-
-    for x, y in dataset:
-        model.learn_one(x, y)
-
-    assert model.height > 0
-
-
-@pytest.mark.parametrize(
-    "dataset, splitter",
-    [
-        (get_regression_data(), tree.splitter.EBSTSplitter()),
-        (get_regression_data(), tree.splitter.TEBSTSplitter()),
-        (get_regression_data(), tree.splitter.QOSplitter()),
-        (get_regression_data(), tree.splitter.QOSplitter(allow_multiway_splits=True)),
-    ],
-)
-def test_reg_splitter(dataset, splitter):
-    model = tree.HoeffdingTreeRegressor(
-        splitter=splitter, grace_period=20, split_confidence=0.1, leaf_prediction="mean"
-    )
-
-    for x, y in dataset:
-        model.learn_one(x, y)
-
-    assert model.height > 0
-
-
-def test_nominal_reg_splitter():
-    dataset = synth.Mv(seed=42).take(200)
-    model = tree.HoeffdingTreeRegressor(grace_period=10, leaf_prediction="mean")
-
-    for x, y in dataset:
-        model.learn_one(x, y)
-
-    assert model.height > 0
-
-    # Evaluates nominal binary splits
-    dataset = synth.Mv(seed=42).take(200)
-    model = tree.HoeffdingTreeRegressor(
-        grace_period=10, leaf_prediction="mean", binary_split=True
-    )
-
-    for x, y in dataset:
-        model.learn_one(x, y)
-
-    assert model.height > 0
+import pytest
+
+from river import datasets, synth, tree
+
+
+def get_regression_data():
+    return iter(synth.Friedman(seed=42).take(200))
+
+
+@pytest.mark.parametrize(
+    "dataset, splitter",
+    [
+        (datasets.Phishing(), tree.splitter.ExhaustiveSplitter()),
+        (datasets.Phishing(), tree.splitter.HistogramSplitter()),
+        (datasets.Phishing(), tree.splitter.GaussianSplitter()),
+    ],
+)
+def test_class_splitter(dataset, splitter):
+    model = tree.HoeffdingTreeClassifier(
+        splitter=splitter, grace_period=10, leaf_prediction="mc", split_confidence=0.1
+    )
+
+    for x, y in dataset:
+        model.learn_one(x, y)
+
+    assert model.height > 0
+
+
+@pytest.mark.parametrize(
+    "dataset, splitter",
+    [
+        (get_regression_data(), tree.splitter.EBSTSplitter()),
+        (get_regression_data(), tree.splitter.TEBSTSplitter()),
+        (get_regression_data(), tree.splitter.QOSplitter()),
+        (get_regression_data(), tree.splitter.QOSplitter(allow_multiway_splits=True)),
+    ],
+)
+def test_reg_splitter(dataset, splitter):
+    model = tree.HoeffdingTreeRegressor(
+        splitter=splitter, grace_period=20, split_confidence=0.1, leaf_prediction="mean"
+    )
+
+    for x, y in dataset:
+        model.learn_one(x, y)
+
+    assert model.height > 0
+
+
+def test_nominal_reg_splitter():
+    dataset = synth.Mv(seed=42).take(200)
+    model = tree.HoeffdingTreeRegressor(grace_period=10, leaf_prediction="mean")
+
+    for x, y in dataset:
+        model.learn_one(x, y)
+
+    assert model.height > 0
+
+    # Evaluates nominal binary splits
+    dataset = synth.Mv(seed=42).take(200)
+    model = tree.HoeffdingTreeRegressor(
+        grace_period=10, leaf_prediction="mean", binary_split=True
+    )
+
+    for x, y in dataset:
+        model.learn_one(x, y)
+
+    assert model.height > 0
```

### Comparing `river-0.8.0/river/tree/test_trees.py` & `river-0.9.0/river/tree/test_trees.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,181 +1,181 @@
-import random
-
-import pytest
-
-from river import datasets, synth, tree
-
-
-def get_classification_data():
-    return synth.LED(seed=42).take(500)
-
-
-def get_regression_data():
-    return synth.Friedman(seed=42).take(500)
-
-
-@pytest.mark.parametrize(
-    "dataset, model",
-    [
-        (
-            get_classification_data(),
-            tree.HoeffdingTreeClassifier(
-                leaf_prediction="mc",
-                max_size=0.025,
-                grace_period=50,
-                memory_estimate_period=50,
-                splitter=tree.splitter.ExhaustiveSplitter(),
-            ),
-        ),
-        (
-            get_classification_data(),
-            tree.HoeffdingAdaptiveTreeClassifier(
-                leaf_prediction="mc",
-                max_size=0.025,
-                grace_period=50,
-                memory_estimate_period=50,
-                splitter=tree.splitter.ExhaustiveSplitter(),
-            ),
-        ),
-        (
-            get_classification_data(),
-            tree.ExtremelyFastDecisionTreeClassifier(
-                leaf_prediction="mc",
-                max_size=0.025,
-                grace_period=50,
-                memory_estimate_period=50,
-                splitter=tree.splitter.ExhaustiveSplitter(),
-            ),
-        ),
-    ],
-)
-def test_memory_usage_class(dataset, model):
-    for x, y in dataset:
-        model.learn_one(x, y)
-
-    assert model._raw_memory_usage / (2 ** 20) < 0.025
-
-
-@pytest.mark.parametrize(
-    "dataset, model",
-    [
-        (
-            get_regression_data(),
-            tree.HoeffdingTreeRegressor(
-                leaf_prediction="mean", max_size=0.5, memory_estimate_period=100
-            ),
-        ),
-        (
-            get_regression_data(),
-            tree.HoeffdingAdaptiveTreeRegressor(
-                leaf_prediction="mean", max_size=0.5, memory_estimate_period=100
-            ),
-        ),
-    ],
-)
-def test_memory_usage_reg(dataset, model):
-    for x, y in dataset:
-        model.learn_one(x, y)
-
-    assert model._raw_memory_usage / (2 ** 20) < 0.5
-
-
-def test_memory_usage_multilabel():
-    dataset = datasets.Music().take(500)
-
-    model = tree.LabelCombinationHoeffdingTreeClassifier(
-        leaf_prediction="mc",
-        splitter=tree.splitter.ExhaustiveSplitter(),
-        max_size=1,
-        memory_estimate_period=100,
-    )
-    for x, y in dataset:
-        model.learn_one(x, y)
-
-    assert model._raw_memory_usage / (2 ** 20) < 1
-
-
-def test_memory_usage_multitarget():
-    dataset = get_regression_data()
-
-    model = tree.iSOUPTreeRegressor(
-        leaf_prediction="mean", max_size=0.5, memory_estimate_period=100,
-    )
-
-    for x, y in dataset:
-        # Handcrafted targets
-        y_ = {0: y, 1: 2 * y, 2: 3 * y}
-        model.learn_one(x, y_)
-
-    assert model._raw_memory_usage / (2 ** 20) < 0.5
-
-
-def test_efdt_split_reevaluation():
-    dataset = synth.SEA(seed=7, variant=2).take(500)
-
-    model = tree.ExtremelyFastDecisionTreeClassifier(
-        leaf_prediction="nb",
-        grace_period=50,
-        min_samples_reevaluate=10,
-        split_criterion="hellinger",
-        split_confidence=0.1,
-    )
-
-    max_depth = -1
-    for x, y in dataset:
-        model.learn_one(x, y)
-
-        if model.height > max_depth:
-            max_depth = model.height
-
-    assert model.height != max_depth
-
-
-def test_drift_adaptation_hatc():
-    rng = random.Random(42)
-    dataset = iter(synth.Sine(seed=8, classification_function=0, has_noise=True))
-
-    model = tree.HoeffdingAdaptiveTreeClassifier(
-        leaf_prediction="mc",
-        grace_period=10,
-        adwin_confidence=0.1,
-        split_confidence=0.1,
-        drift_window_threshold=2,
-        seed=42,
-        max_depth=3,
-    )
-
-    for i in range(1000):
-        if i % 200 == 0 and i > 0:
-            dataset = iter(
-                synth.Sine(
-                    seed=8, classification_function=rng.randint(0, 3), has_noise=False
-                )
-            )
-
-        x, y = next(dataset)
-        model.learn_one(x, y)
-
-    assert model._n_switch_alternate_trees > 0
-
-
-def test_drift_adaptation_hatr():
-    dataset = synth.Friedman(seed=7).take(500)
-
-    model = tree.HoeffdingAdaptiveTreeRegressor(
-        leaf_prediction="model",
-        grace_period=50,
-        split_confidence=0.1,
-        adwin_confidence=0.1,
-        drift_window_threshold=10,
-        seed=7,
-        max_depth=3,
-    )
-
-    for i, (x, y) in enumerate(dataset):
-        y_ = y
-        if i > 250:
-            # Emulate an abrupt drift
-            y_ = 3 * y
-        model.learn_one(x, y_)
-
-    assert model._n_alternate_trees > 0
+import random
+
+import pytest
+
+from river import datasets, synth, tree
+
+
+def get_classification_data():
+    return synth.LED(seed=42).take(500)
+
+
+def get_regression_data():
+    return synth.Friedman(seed=42).take(500)
+
+
+@pytest.mark.parametrize(
+    "dataset, model",
+    [
+        (
+            get_classification_data(),
+            tree.HoeffdingTreeClassifier(
+                leaf_prediction="mc",
+                max_size=0.025,
+                grace_period=50,
+                memory_estimate_period=50,
+                splitter=tree.splitter.ExhaustiveSplitter(),
+            ),
+        ),
+        (
+            get_classification_data(),
+            tree.HoeffdingAdaptiveTreeClassifier(
+                leaf_prediction="mc",
+                max_size=0.025,
+                grace_period=50,
+                memory_estimate_period=50,
+                splitter=tree.splitter.ExhaustiveSplitter(),
+            ),
+        ),
+        (
+            get_classification_data(),
+            tree.ExtremelyFastDecisionTreeClassifier(
+                leaf_prediction="mc",
+                max_size=0.025,
+                grace_period=50,
+                memory_estimate_period=50,
+                splitter=tree.splitter.ExhaustiveSplitter(),
+            ),
+        ),
+    ],
+)
+def test_memory_usage_class(dataset, model):
+    for x, y in dataset:
+        model.learn_one(x, y)
+
+    assert model._raw_memory_usage / (2 ** 20) < 0.025
+
+
+@pytest.mark.parametrize(
+    "dataset, model",
+    [
+        (
+            get_regression_data(),
+            tree.HoeffdingTreeRegressor(
+                leaf_prediction="mean", max_size=0.5, memory_estimate_period=100
+            ),
+        ),
+        (
+            get_regression_data(),
+            tree.HoeffdingAdaptiveTreeRegressor(
+                leaf_prediction="mean", max_size=0.5, memory_estimate_period=100
+            ),
+        ),
+    ],
+)
+def test_memory_usage_reg(dataset, model):
+    for x, y in dataset:
+        model.learn_one(x, y)
+
+    assert model._raw_memory_usage / (2 ** 20) < 0.5
+
+
+def test_memory_usage_multilabel():
+    dataset = datasets.Music().take(500)
+
+    model = tree.LabelCombinationHoeffdingTreeClassifier(
+        leaf_prediction="mc",
+        splitter=tree.splitter.ExhaustiveSplitter(),
+        max_size=1,
+        memory_estimate_period=100,
+    )
+    for x, y in dataset:
+        model.learn_one(x, y)
+
+    assert model._raw_memory_usage / (2 ** 20) < 1
+
+
+def test_memory_usage_multitarget():
+    dataset = get_regression_data()
+
+    model = tree.iSOUPTreeRegressor(
+        leaf_prediction="mean", max_size=0.5, memory_estimate_period=100,
+    )
+
+    for x, y in dataset:
+        # Handcrafted targets
+        y_ = {0: y, 1: 2 * y, 2: 3 * y}
+        model.learn_one(x, y_)
+
+    assert model._raw_memory_usage / (2 ** 20) < 0.5
+
+
+def test_efdt_split_reevaluation():
+    dataset = synth.SEA(seed=7, variant=2).take(500)
+
+    model = tree.ExtremelyFastDecisionTreeClassifier(
+        leaf_prediction="nb",
+        grace_period=50,
+        min_samples_reevaluate=10,
+        split_criterion="hellinger",
+        split_confidence=0.1,
+    )
+
+    max_depth = -1
+    for x, y in dataset:
+        model.learn_one(x, y)
+
+        if model.height > max_depth:
+            max_depth = model.height
+
+    assert model.height != max_depth
+
+
+def test_drift_adaptation_hatc():
+    rng = random.Random(42)
+    dataset = iter(synth.Sine(seed=8, classification_function=0, has_noise=True))
+
+    model = tree.HoeffdingAdaptiveTreeClassifier(
+        leaf_prediction="mc",
+        grace_period=10,
+        adwin_confidence=0.1,
+        split_confidence=0.1,
+        drift_window_threshold=2,
+        seed=42,
+        max_depth=3,
+    )
+
+    for i in range(1000):
+        if i % 200 == 0 and i > 0:
+            dataset = iter(
+                synth.Sine(
+                    seed=8, classification_function=rng.randint(0, 3), has_noise=False
+                )
+            )
+
+        x, y = next(dataset)
+        model.learn_one(x, y)
+
+    assert model._n_switch_alternate_trees > 0
+
+
+def test_drift_adaptation_hatr():
+    dataset = synth.Friedman(seed=7).take(500)
+
+    model = tree.HoeffdingAdaptiveTreeRegressor(
+        leaf_prediction="model",
+        grace_period=50,
+        split_confidence=0.1,
+        adwin_confidence=0.1,
+        drift_window_threshold=10,
+        seed=7,
+        max_depth=3,
+    )
+
+    for i, (x, y) in enumerate(dataset):
+        y_ = y
+        if i > 250:
+            # Emulate an abrupt drift
+            y_ = 3 * y
+        model.learn_one(x, y_)
+
+    assert model._n_alternate_trees > 0
```

### Comparing `river-0.8.0/river/tree/utils.py` & `river-0.9.0/river/tree/utils.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,246 +1,241 @@
-import copy
-import dataclasses
-import functools
-import math
-import typing
-
-from river.base.typing import FeatureName
-from river.stats import Cov, Var
-
-
-def do_naive_bayes_prediction(x, observed_class_distribution: dict, splitters: dict):
-    """Perform Naive Bayes prediction
-
-    Parameters
-    ----------
-    x
-        The feature values.
-
-    observed_class_distribution
-        Observed class distribution.
-
-    splitters
-        Attribute (features) observers.
-
-    Returns
-    -------
-    The probabilities related to each class.
-
-    Notes
-    -----
-    This method is not intended to be used as a stand-alone method.
-    """
-    total_weight = sum(observed_class_distribution.values())
-    if not observed_class_distribution or total_weight == 0:
-        # No observed class distributions, all classes equal
-        return None
-
-    votes = {}
-    for class_index, class_weight in observed_class_distribution.items():
-        # Prior
-        if class_weight > 0:
-            votes[class_index] = math.log(class_weight / total_weight)
-        else:
-            votes[class_index] = 0.0
-            continue
-
-        if splitters:
-            for att_idx in splitters:
-                if att_idx not in x:
-                    continue
-                obs = splitters[att_idx]
-                # Prior plus the log likelihood
-                tmp = obs.cond_proba(x[att_idx], class_index)
-                votes[class_index] += math.log(tmp) if tmp > 0 else 0.0
-
-    # Max log-likelihood
-    max_ll = max(votes.values())
-    # Apply the log-sum-exp trick (https://stats.stackexchange.com/a/253319)
-    lse = max_ll + math.log(
-        sum(math.exp(log_proba - max_ll) for log_proba in votes.values())
-    )
-
-    for class_index in votes:
-        votes[class_index] = math.exp(votes[class_index] - lse)
-
-    return votes
-
-
-@functools.total_ordering
-@dataclasses.dataclass
-class BranchFactory:
-    """Helper class used to assemble branches designed by the splitters.
-
-    If constructed using the default values, a null-split suggestion is assumed.
-    """
-
-    merit: float = -math.inf
-    feature: typing.Optional[FeatureName] = None
-    split_info: typing.Optional[
-        typing.Union[
-            typing.Hashable,
-            typing.List[typing.Hashable],
-            typing.Tuple[typing.Hashable, typing.List[typing.Hashable]],
-        ]
-    ] = None
-    children_stats: typing.Optional[typing.List] = None
-    numerical_feature: bool = True
-    multiway_split: bool = False
-
-    def assemble(
-        self,
-        branch,  # typing.Type[DTBranch],
-        stats: typing.Union[typing.Dict, Var],
-        depth: int,
-        *children,
-        **kwargs
-    ):
-        return branch(stats, self.feature, self.split_info, depth, *children, **kwargs)
-
-    def __lt__(self, other):
-        return self.merit < other.merit
-
-    def __eq__(self, other):
-        return self.merit == other.merit
-
-
-class GradHess:
-    """ The most basic inner structure of the Stochastic Gradient Trees that carries information
-    about the gradient and hessian of a given observation.
-    """
-
-    __slots__ = ["gradient", "hessian"]
-
-    def __init__(self, gradient: float = 0.0, hessian: float = 0.0, *, grad_hess=None):
-        if grad_hess:
-            self.gradient = grad_hess.gradient
-            self.hessian = grad_hess.hessian
-        else:
-            self.gradient = gradient
-            self.hessian = hessian
-
-    def __iadd__(self, other):
-        self.gradient += other.gradient
-        self.hessian += other.hessian
-
-        return self
-
-    def __isub__(self, other):
-        self.gradient -= other.gradient
-        self.hessian -= other.hessian
-
-        return self
-
-    def __add__(self, other):
-        new = copy.deepcopy(self)
-        new += other
-        return new
-
-    def __sub__(self, other):
-        new = copy.deepcopy(self)
-        new -= other
-        return new
-
-
-@functools.total_ordering
-@dataclasses.dataclass
-class GradHessMerit:
-    """Class used to keep the split merit of each split candidate, accordingly to its
-    gradient and hessian information.
-
-    In Stochastic Gradient Trees, the split merit is given by a combination of the loss mean and
-    variance. Additionally, the loss in each resulting tree branch is also accounted.
-    """
-
-    loss_mean: float = 0.0
-    loss_var: float = 0.0
-    delta_pred: typing.Union[float, typing.Dict] = None
-
-    def __lt__(self, other):
-        return self.loss_mean < other.loss_mean
-
-    def __eq__(self, other):
-        return self.loss_mean == other.loss_mean
-
-
-class GradHessStats:
-    """ Class used to monitor and update the gradient/hessian information in Stochastic Gradient
-    Trees.
-
-    Represents the aggregated gradient/hessian data in a node (global node statistics), category,
-    or numerical feature's discretized bin.
-    """
-
-    def __init__(self):
-        self.g_var = Var()
-        self.h_var = Var()
-        self.gh_cov = Cov()
-
-    def __iadd__(self, other):
-        self.g_var += other.g_var
-        self.h_var += other.h_var
-        self.gh_cov += other.gh_cov
-
-        return self
-
-    def __isub__(self, other):
-        self.g_var -= other.g_var
-        self.h_var -= other.h_var
-        self.gh_cov -= other.gh_cov
-
-        return self
-
-    def __add__(self, other):
-        new = copy.deepcopy(self)
-        new += other
-
-        return new
-
-    def __sub__(self, other):
-        new = copy.deepcopy(self)
-        new -= other
-
-        return new
-
-    def update(self, gh: GradHess, w: float = 1.0):
-        self.g_var.update(gh.gradient, w)
-        self.h_var.update(gh.hessian, w)
-        self.gh_cov.update(gh.gradient, gh.hessian, w)
-
-    @property
-    def mean(self) -> GradHess:
-        return GradHess(self.g_var.mean.get(), self.h_var.mean.get())
-
-    @property
-    def variance(self) -> GradHess:
-        return GradHess(self.g_var.get(), self.h_var.get())
-
-    @property
-    def covariance(self) -> float:
-        return self.gh_cov.get()
-
-    @property
-    def total_weight(self) -> float:
-        return self.g_var.mean.n
-
-    # This method ignores correlations between delta_pred and the gradients/hessians! Considering
-    # delta_pred is derived from the gradient and hessian sample, this assumption is definitely
-    # violated. However, as empirically demonstrated in the original SGT, this fact does not seem
-    # to significantly impact on the obtained results.
-    def delta_loss_mean_var(self, delta_pred: float) -> Var:
-        m = self.mean
-        dlms = Var()
-        dlms.mean.n = self.total_weight
-        dlms.mean.mean = (
-            delta_pred * m.gradient + 0.5 * m.hessian * delta_pred * delta_pred
-        )
-
-        variance = self.variance
-        covariance = self.covariance
-
-        grad_term_var = delta_pred * delta_pred * variance.gradient
-        hess_term_var = 0.25 * variance.hessian * (delta_pred ** 4.0)
-        dlms.sigma = max(
-            0.0, grad_term_var + hess_term_var + (delta_pred ** 3) * covariance
-        )
-        return dlms
+import copy
+import dataclasses
+import functools
+import math
+import typing
+
+from river.base.typing import FeatureName
+from river.stats import Cov, Var
+
+
+def do_naive_bayes_prediction(x, observed_class_distribution: dict, splitters: dict):
+    """Perform Naive Bayes prediction
+
+    Parameters
+    ----------
+    x
+        The feature values.
+
+    observed_class_distribution
+        Observed class distribution.
+
+    splitters
+        Attribute (features) observers.
+
+    Returns
+    -------
+    The probabilities related to each class.
+
+    Notes
+    -----
+    This method is not intended to be used as a stand-alone method.
+    """
+    total_weight = sum(observed_class_distribution.values())
+    if not observed_class_distribution or total_weight == 0:
+        # No observed class distributions, all classes equal
+        return None
+
+    votes = {}
+    for class_index, class_weight in observed_class_distribution.items():
+        # Prior
+        if class_weight > 0:
+            votes[class_index] = math.log(class_weight / total_weight)
+        else:
+            votes[class_index] = 0.0
+            continue
+
+        if splitters:
+            for att_idx in splitters:
+                if att_idx not in x:
+                    continue
+                obs = splitters[att_idx]
+                # Prior plus the log likelihood
+                tmp = obs.cond_proba(x[att_idx], class_index)
+                votes[class_index] += math.log(tmp) if tmp > 0 else 0.0
+
+    # Max log-likelihood
+    max_ll = max(votes.values())
+    # Apply the log-sum-exp trick (https://stats.stackexchange.com/a/253319)
+    lse = max_ll + math.log(
+        sum(math.exp(log_proba - max_ll) for log_proba in votes.values())
+    )
+
+    for class_index in votes:
+        votes[class_index] = math.exp(votes[class_index] - lse)
+
+    return votes
+
+
+@functools.total_ordering
+@dataclasses.dataclass
+class BranchFactory:
+    """Helper class used to assemble branches designed by the splitters.
+
+    If constructed using the default values, a null-split suggestion is assumed.
+    """
+
+    merit: float = -math.inf
+    feature: typing.Optional[FeatureName] = None
+    split_info: typing.Optional[
+        typing.Union[
+            typing.Hashable,
+            typing.List[typing.Hashable],
+            typing.Tuple[typing.Hashable, typing.List[typing.Hashable]],
+        ]
+    ] = None
+    children_stats: typing.Optional[typing.List] = None
+    numerical_feature: bool = True
+    multiway_split: bool = False
+
+    def assemble(
+        self,
+        branch,  # typing.Type[DTBranch],
+        stats: typing.Union[typing.Dict, Var],
+        depth: int,
+        *children,
+        **kwargs
+    ):
+        return branch(stats, self.feature, self.split_info, depth, *children, **kwargs)
+
+    def __lt__(self, other):
+        return self.merit < other.merit
+
+    def __eq__(self, other):
+        return self.merit == other.merit
+
+
+class GradHess:
+    """ The most basic inner structure of the Stochastic Gradient Trees that carries information
+    about the gradient and hessian of a given observation.
+    """
+
+    __slots__ = ["gradient", "hessian"]
+
+    def __init__(self, gradient: float = 0.0, hessian: float = 0.0, *, grad_hess=None):
+        if grad_hess:
+            self.gradient = grad_hess.gradient
+            self.hessian = grad_hess.hessian
+        else:
+            self.gradient = gradient
+            self.hessian = hessian
+
+    def __iadd__(self, other):
+        self.gradient += other.gradient
+        self.hessian += other.hessian
+
+        return self
+
+    def __isub__(self, other):
+        self.gradient -= other.gradient
+        self.hessian -= other.hessian
+
+        return self
+
+    def __add__(self, other):
+        new = copy.deepcopy(self)
+        new += other
+        return new
+
+    def __sub__(self, other):
+        new = copy.deepcopy(self)
+        new -= other
+        return new
+
+
+@functools.total_ordering
+@dataclasses.dataclass
+class GradHessMerit:
+    """Class used to keep the split merit of each split candidate, accordingly to its
+    gradient and hessian information.
+
+    In Stochastic Gradient Trees, the split merit is given by a combination of the loss mean and
+    variance. Additionally, the loss in each resulting tree branch is also accounted.
+    """
+
+    loss_mean: float = 0.0
+    loss_var: float = 0.0
+    delta_pred: typing.Union[float, typing.Dict] = None
+
+    def __lt__(self, other):
+        return self.loss_mean < other.loss_mean
+
+    def __eq__(self, other):
+        return self.loss_mean == other.loss_mean
+
+
+class GradHessStats:
+    """ Class used to monitor and update the gradient/hessian information in Stochastic Gradient
+    Trees.
+
+    Represents the aggregated gradient/hessian data in a node (global node statistics), category,
+    or numerical feature's discretized bin.
+    """
+
+    def __init__(self):
+        self.g_var = Var()
+        self.h_var = Var()
+        self.gh_cov = Cov()
+
+    def __iadd__(self, other):
+        self.g_var += other.g_var
+        self.h_var += other.h_var
+        self.gh_cov += other.gh_cov
+
+        return self
+
+    def __isub__(self, other):
+        self.g_var -= other.g_var
+        self.h_var -= other.h_var
+        self.gh_cov -= other.gh_cov
+
+        return self
+
+    def __add__(self, other):
+        new = copy.deepcopy(self)
+        new += other
+
+        return new
+
+    def __sub__(self, other):
+        new = copy.deepcopy(self)
+        new -= other
+
+        return new
+
+    def update(self, gh: GradHess, w: float = 1.0):
+        self.g_var.update(gh.gradient, w)
+        self.h_var.update(gh.hessian, w)
+        self.gh_cov.update(gh.gradient, gh.hessian, w)
+
+    @property
+    def mean(self) -> GradHess:
+        return GradHess(self.g_var.mean.get(), self.h_var.mean.get())
+
+    @property
+    def variance(self) -> GradHess:
+        return GradHess(self.g_var.get(), self.h_var.get())
+
+    @property
+    def covariance(self) -> float:
+        return self.gh_cov.get()
+
+    @property
+    def total_weight(self) -> float:
+        return self.g_var.mean.n
+
+    # This method ignores correlations between delta_pred and the gradients/hessians! Considering
+    # delta_pred is derived from the gradient and hessian sample, this assumption is definitely
+    # violated. However, as empirically demonstrated in the original SGT, this fact does not seem
+    # to significantly impact on the obtained results.
+    def delta_loss_mean_var(self, delta_pred: float) -> Var:
+        m = self.mean
+        n = self.total_weight
+        mean = delta_pred * m.gradient + 0.5 * m.hessian * delta_pred * delta_pred
+
+        variance = self.variance
+        covariance = self.covariance
+
+        grad_term_var = delta_pred * delta_pred * variance.gradient
+        hess_term_var = 0.25 * variance.hessian * (delta_pred ** 4.0)
+        sigma = max(0.0, grad_term_var + hess_term_var + (delta_pred ** 3) * covariance)
+        return Var._from_state(n, mean, sigma)  # noqa
```

### Comparing `river-0.8.0/river/utils/data_conversion.py` & `river-0.9.0/river/utils/data_conversion.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,54 +1,54 @@
-import numpy as np
-
-
-def dict2numpy(data) -> np.ndarray:
-    """Convert a dictionary containing data to a numpy array.
-
-    There is not restriction to the type of keys in `data`, but values must
-    be strictly numeric. To make sure random permutations of the features
-    do not impact on the learning algorithms, keys are first converted to
-    strings and then sorted prior to the conversion.
-
-    Parameters
-    ----------
-    data
-        A dictionary whose keys represent input attributes and the values
-        represent their observed contents.
-
-    Returns
-    -------
-    An array representation of the values in `data`.
-
-    Examples
-    --------
-    >>> from river.utils import dict2numpy
-    >>> dict2numpy({'a': 1, 'b': 2, 3: 3})
-    array([3, 1, 2])
-
-    """
-    data_ = {str(k): v for k, v in data.items()}
-    return np.asarray(list(x for _, x in sorted(data_.items())))
-
-
-def numpy2dict(data: np.ndarray) -> dict:
-    """Convert a numpy array to a dictionary.
-
-    Parameters
-    ----------
-    data
-        An one-dimensional numpy.array.
-
-    Returns
-    -------
-    A dictionary where keys are integers $k \\in \\left{0, 1, ..., |\\text{data}| - 1\\right}$,
-    and the values are each one of the $k$ entries in `data`.
-
-    Examples
-    --------
-    >>> import numpy as np
-    >>> from river.utils import numpy2dict
-    >>> numpy2dict(np.array([1.0, 2.0, 3.0]))
-    {0: 1.0, 1: 2.0, 2: 3.0}
-
-    """
-    return {k: v for k, v in enumerate(data)}
+import numpy as np
+
+
+def dict2numpy(data) -> np.ndarray:
+    """Convert a dictionary containing data to a numpy array.
+
+    There is not restriction to the type of keys in `data`, but values must
+    be strictly numeric. To make sure random permutations of the features
+    do not impact on the learning algorithms, keys are first converted to
+    strings and then sorted prior to the conversion.
+
+    Parameters
+    ----------
+    data
+        A dictionary whose keys represent input attributes and the values
+        represent their observed contents.
+
+    Returns
+    -------
+    An array representation of the values in `data`.
+
+    Examples
+    --------
+    >>> from river.utils import dict2numpy
+    >>> dict2numpy({'a': 1, 'b': 2, 3: 3})
+    array([3, 1, 2])
+
+    """
+    data_ = {str(k): v for k, v in data.items()}
+    return np.asarray(list(x for _, x in sorted(data_.items())))
+
+
+def numpy2dict(data: np.ndarray) -> dict:
+    """Convert a numpy array to a dictionary.
+
+    Parameters
+    ----------
+    data
+        An one-dimensional numpy.array.
+
+    Returns
+    -------
+    A dictionary where keys are integers $k \\in \\left{0, 1, ..., |\\text{data}| - 1\\right}$,
+    and the values are each one of the $k$ entries in `data`.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from river.utils import numpy2dict
+    >>> numpy2dict(np.array([1.0, 2.0, 3.0]))
+    {0: 1.0, 1: 2.0, 2: 3.0}
+
+    """
+    return {k: v for k, v in enumerate(data)}
```

### Comparing `river-0.8.0/river/utils/estimator_checks.py` & `river-0.9.0/river/utils/estimator_checks.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,370 +1,393 @@
-"""Utilities for unit testing and sanity checking estimators."""
-import copy
-import functools
-import inspect
-import math
-import pickle
-import random
-
-import numpy as np
-
-__all__ = ["check_estimator"]
-
-
-def yield_datasets(model):
-
-    from sklearn import datasets as sk_datasets
-
-    from river import base, compose, datasets, preprocessing, stream, utils
-
-    # Multi-output regression
-    if utils.inspect.ismoregressor(model):
-
-        # 1
-        yield stream.iter_sklearn_dataset(sk_datasets.load_linnerud())
-
-        # 2
-        class SolarFlare:
-            """One-hot encoded version of `datasets.SolarFlare"""
-
-            def __iter__(self):
-                oh = (
-                    compose.SelectType(str) | preprocessing.OneHotEncoder()
-                ) + compose.SelectType(int)
-                for x, y in datasets.SolarFlare().take(200):
-                    yield oh.transform_one(x), y
-
-        yield SolarFlare()
-
-    # Regression
-    elif utils.inspect.isregressor(model):
-        yield datasets.TrumpApproval().take(200)
-
-    # Multi-output classification
-    if utils.inspect.ismoclassifier(model):
-        yield datasets.Music().take(200)
-
-    # Classification
-    elif utils.inspect.isclassifier(model):
-
-        yield datasets.Phishing().take(200)
-        yield ((x, np.bool_(y)) for x, y in datasets.Phishing().take(200))
-
-        # Multi-class classification
-        if model._multiclass and base.tags.POSITIVE_INPUT not in model._tags:
-            yield datasets.ImageSegments().take(200)
-
-
-def check_learn_one(model, dataset):
-    """learn_one should return the calling model and be pure."""
-
-    klass = model.__class__
-
-    for x, y in dataset:
-
-        xx, yy = copy.deepcopy(x), copy.deepcopy(y)
-
-        model = model.learn_one(x, y)
-
-        # Check the model returns itself
-        assert isinstance(model, klass)
-
-        # Check learn_one is pure (i.e. x and y haven't changed)
-        assert x == xx
-        assert y == yy
-
-
-def check_predict_proba_one(classifier, dataset):
-    """predict_proba_one should return a valid probability distribution and be pure."""
-
-    if not hasattr(classifier, "predict_proba_one"):
-        return
-
-    for x, y in dataset:
-
-        xx, yy = copy.deepcopy(x), copy.deepcopy(y)
-
-        classifier = classifier.learn_one(x, y)
-        y_pred = classifier.predict_proba_one(x)
-
-        # Check the probabilities are coherent
-        assert isinstance(y_pred, dict)
-        for proba in y_pred.values():
-            assert 0.0 <= proba <= 1.0
-        assert math.isclose(sum(y_pred.values()), 1.0)
-
-        # Check predict_proba_one is pure (i.e. x and y haven't changed)
-        assert x == xx
-        assert y == yy
-
-
-def check_predict_proba_one_binary(classifier, dataset):
-    """predict_proba_one should return a dict with True and False keys."""
-
-    for x, y in dataset:
-        y_pred = classifier.predict_proba_one(x)
-        classifier = classifier.learn_one(x, y)
-        assert set(y_pred.keys()) == {False, True}
-
-
-def assert_predictions_are_close(y1, y2):
-
-    if isinstance(y1, dict):
-        for k in y1:
-            assert_predictions_are_close(y1[k], y2[k])
-    elif isinstance(y1, float):
-        assert math.isclose(y1, y2, rel_tol=1e-06)
-    else:
-        assert y1 == y2
-
-
-def check_shuffle_features_no_impact(model, dataset):
-    """Changing the order of the features between calls should have no effect on a model."""
-
-    from river import utils
-
-    shuffled = copy.deepcopy(model)
-
-    for x, y in dataset:
-
-        # Shuffle the features
-        features = list(x.keys())
-        random.shuffle(features)
-        x_shuffled = {i: x[i] for i in features}
-
-        assert x == x_shuffled  # order doesn't matter for dicts
-
-        if utils.inspect.isclassifier(model):
-            try:
-                y_pred = model.predict_proba_one(x)
-                y_pred_shuffled = shuffled.predict_proba_one(x_shuffled)
-            except NotImplementedError:
-                y_pred = model.predict_one(x)
-                y_pred_shuffled = shuffled.predict_one(x_shuffled)
-        else:
-            y_pred = model.predict_one(x)
-            y_pred_shuffled = shuffled.predict_one(x_shuffled)
-
-        assert_predictions_are_close(y_pred, y_pred_shuffled)
-
-        model.learn_one(x, y)
-        shuffled.learn_one(x_shuffled, y)
-
-
-def check_emerging_features(model, dataset):
-    """The model should work fine when new features appear."""
-
-    for x, y in dataset:
-        features = list(x.keys())
-        random.shuffle(features)
-        model.predict_one(x)
-        model.learn_one(
-            {i: x[i] for i in features[:-3]}, y
-        )  # drop 3 features at random
-
-
-def check_disappearing_features(model, dataset):
-    """The model should work fine when features disappear."""
-
-    for x, y in dataset:
-        features = list(x.keys())
-        random.shuffle(features)
-        model.predict_one({i: x[i] for i in features[:-3]})  # drop 3 features at random
-        model.learn_one(x, y)
-
-
-def check_debug_one(model, dataset):
-    for x, y in dataset:
-        model.debug_one(x)
-        model.learn_one(x, y)
-        model.debug_one(x)
-        break
-
-
-def check_pickling(model, dataset):
-    assert isinstance(pickle.loads(pickle.dumps(model)), model.__class__)
-    for x, y in dataset:
-        model.predict_one(x)
-        model.learn_one(x, y)
-    assert isinstance(pickle.loads(pickle.dumps(model)), model.__class__)
-
-
-def check_has_tag(model, tag):
-    assert tag in model._tags
-
-
-def check_repr(model):
-    assert isinstance(repr(model), str)
-
-
-def check_str(model):
-    assert isinstance(str(model), str)
-
-
-def check_tags(model):
-    """Checks that the `_tags` property works."""
-    assert isinstance(model._tags, set)
-
-
-def check_set_params_idempotent(model):
-    assert len(model.__dict__) == len(model._set_params().__dict__)
-
-
-def check_init_has_default_params_for_tests(model):
-    params = model._unit_test_params()
-    assert isinstance(model.__class__(**params), model.__class__)
-
-
-def check_init_default_params_are_not_mutable(model):
-    """Mutable parameters in signatures are discouraged, as explained in
-    https://docs.python-guide.org/writing/gotchas/#mutable-default-arguments
-
-    We enforce immutable parameters by only allowing a certain list of basic types.
-
-    """
-
-    allowed = (type(None), float, int, tuple, str, bool, type)
-
-    for param in inspect.signature(model.__class__).parameters.values():
-        assert param.default is inspect._empty or isinstance(param.default, allowed)
-
-
-def check_doc(model):
-    assert model.__doc__
-
-
-def check_clone(model):
-    clone = model.clone()
-    assert id(clone) != id(model)
-    assert dir(clone) == dir(model)
-
-
-def seed_params(params, seed):
-    """Looks for "seed" keys and sets the value."""
-
-    def is_class_param(param):
-        return (
-            isinstance(param, tuple)
-            and inspect.isclass(param[0])
-            and isinstance(param[1], dict)
-        )
-
-    if is_class_param(params):
-        return params[0], seed_params(params[1], seed)
-
-    if not isinstance(params, dict):
-        return params
-
-    return {
-        name: seed if name == "seed" else seed_params(param, seed)
-        for name, param in params.items()
-    }
-
-
-def check_seeding_is_idempotent(model, dataset):
-
-    params = model._get_params()
-    seeded_params = seed_params(params, seed=42)
-
-    A = model._set_params(seeded_params)
-    B = model._set_params(seeded_params)
-
-    for x, y in dataset:
-        assert A.predict_one(x) == B.predict_one(x)
-        A.learn_one(x, y)
-        B.learn_one(x, y)
-
-
-def check_multiclass_is_bool(model):
-    assert isinstance(model._multiclass, bool)
-
-
-def wrapped_partial(func, *args, **kwargs):
-    """
-
-    Taken from http://louistiao.me/posts/adding-__name__-and-__doc__-attributes-to-functoolspartial-objects/
-
-    """
-    partial = functools.partial(func, *args, **kwargs)
-    functools.update_wrapper(partial, func)
-    return partial
-
-
-def allow_exception(func, exception):
-    def f(*args, **kwargs):
-        try:
-            func(*args, **kwargs)
-        except exception:
-            pass
-
-    f.__name__ = func.__name__
-    return f
-
-
-def yield_checks(model):
-    """Generates unit tests for a given model.
-
-    Parameters:
-        model (base.Estimator)
-
-    """
-
-    from river import utils
-
-    # General checks
-    yield check_repr
-    yield check_str
-    yield check_tags
-    yield check_set_params_idempotent
-    yield check_init_has_default_params_for_tests
-    yield check_init_default_params_are_not_mutable
-    yield check_doc
-    yield check_clone
-
-    if utils.inspect.isclassifier(model):
-        yield check_multiclass_is_bool
-
-    # Checks that make use of datasets
-    checks = [
-        check_learn_one,
-        check_pickling,
-        check_shuffle_features_no_impact,
-        check_emerging_features,
-        check_disappearing_features,
-    ]
-
-    if hasattr(model, "debug_one"):
-        checks.append(check_debug_one)
-
-    if model._is_stochastic:
-        checks.append(check_seeding_is_idempotent)
-
-    # Classifier checks
-    if utils.inspect.isclassifier(model) and not utils.inspect.ismoclassifier(model):
-        checks.append(allow_exception(check_predict_proba_one, NotImplementedError))
-        # Specific checks for binary classifiers
-        if not model._multiclass:
-            checks.append(
-                allow_exception(check_predict_proba_one_binary, NotImplementedError)
-            )
-
-    for check in checks:
-        for dataset in yield_datasets(model):
-            yield wrapped_partial(check, dataset=dataset)
-
-
-def check_estimator(model):
-    """Check if a model adheres to `river`'s conventions.
-
-    This will run a series of unit tests. The nature of the unit tests depends on the type of
-    model.
-
-    Parameters
-    ----------
-    model
-
-    """
-    for check in yield_checks(model):
-        if check.__name__ in model._unit_test_skips():
-            continue
-        check(copy.deepcopy(model))
+"""Utilities for unit testing and sanity checking estimators."""
+import copy
+import functools
+import inspect
+import itertools
+import math
+import pickle
+import random
+
+import numpy as np
+
+__all__ = ["check_estimator"]
+
+
+def yield_datasets(model):
+
+    from sklearn import datasets as sk_datasets
+
+    from river import base, compose, datasets, preprocessing, stream, utils
+
+    # Multi-output regression
+    if utils.inspect.ismoregressor(model):
+
+        # 1
+        yield stream.iter_sklearn_dataset(sk_datasets.load_linnerud())
+
+        # 2
+        class SolarFlare:
+            """One-hot encoded version of `datasets.SolarFlare"""
+
+            def __iter__(self):
+                oh = (
+                    compose.SelectType(str) | preprocessing.OneHotEncoder()
+                ) + compose.SelectType(int)
+                for x, y in datasets.SolarFlare().take(200):
+                    yield oh.transform_one(x), y
+
+        yield SolarFlare()
+
+    # Regression
+    elif utils.inspect.isregressor(model):
+        yield datasets.TrumpApproval().take(200)
+
+    # Multi-output classification
+    if utils.inspect.ismoclassifier(model):
+        yield datasets.Music().take(200)
+
+    # Classification
+    elif utils.inspect.isclassifier(model):
+
+        yield datasets.Phishing().take(200)
+        yield ((x, np.bool_(y)) for x, y in datasets.Phishing().take(200))
+
+        # Multi-class classification
+        if model._multiclass and base.tags.POSITIVE_INPUT not in model._tags:
+            yield datasets.ImageSegments().take(200)
+
+
+def check_learn_one(model, dataset):
+    """learn_one should return the calling model and be pure."""
+
+    klass = model.__class__
+
+    for x, y in dataset:
+
+        xx, yy = copy.deepcopy(x), copy.deepcopy(y)
+
+        model = model.learn_one(x, y)
+
+        # Check the model returns itself
+        assert isinstance(model, klass)
+
+        # Check learn_one is pure (i.e. x and y haven't changed)
+        assert x == xx
+        assert y == yy
+
+
+def check_predict_proba_one(classifier, dataset):
+    """predict_proba_one should return a valid probability distribution and be pure."""
+
+    if not hasattr(classifier, "predict_proba_one"):
+        return
+
+    for x, y in dataset:
+
+        xx, yy = copy.deepcopy(x), copy.deepcopy(y)
+
+        classifier = classifier.learn_one(x, y)
+        y_pred = classifier.predict_proba_one(x)
+
+        # Check the probabilities are coherent
+        assert isinstance(y_pred, dict)
+        for proba in y_pred.values():
+            assert 0.0 <= proba <= 1.0
+        assert math.isclose(sum(y_pred.values()), 1.0)
+
+        # Check predict_proba_one is pure (i.e. x and y haven't changed)
+        assert x == xx
+        assert y == yy
+
+
+def check_predict_proba_one_binary(classifier, dataset):
+    """predict_proba_one should return a dict with True and False keys."""
+
+    for x, y in dataset:
+        y_pred = classifier.predict_proba_one(x)
+        classifier = classifier.learn_one(x, y)
+        assert set(y_pred.keys()) == {False, True}
+
+
+def assert_predictions_are_close(y1, y2):
+
+    if isinstance(y1, dict):
+        for k in y1:
+            assert_predictions_are_close(y1[k], y2[k])
+    elif isinstance(y1, float):
+        assert math.isclose(y1, y2, rel_tol=1e-06)
+    else:
+        assert y1 == y2
+
+
+def check_shuffle_features_no_impact(model, dataset):
+    """Changing the order of the features between calls should have no effect on a model."""
+
+    from river import utils
+
+    params = seed_params(model._get_params(), seed=42)
+    model = model._set_params(params)
+    shuffled = copy.deepcopy(model)
+
+    for x, y in dataset:
+
+        # Shuffle the features
+        features = list(x.keys())
+        random.shuffle(features)
+        x_shuffled = {i: x[i] for i in features}
+
+        assert x == x_shuffled  # order doesn't matter for dicts
+
+        if utils.inspect.isclassifier(model):
+            try:
+                y_pred = model.predict_proba_one(x)
+                y_pred_shuffled = shuffled.predict_proba_one(x_shuffled)
+            except NotImplementedError:
+                y_pred = model.predict_one(x)
+                y_pred_shuffled = shuffled.predict_one(x_shuffled)
+        else:
+            y_pred = model.predict_one(x)
+            y_pred_shuffled = shuffled.predict_one(x_shuffled)
+
+        assert_predictions_are_close(y_pred, y_pred_shuffled)
+
+        model.learn_one(x, y)
+        shuffled.learn_one(x_shuffled, y)
+
+
+def check_emerging_features(model, dataset):
+    """The model should work fine when new features appear."""
+
+    for x, y in dataset:
+        features = list(x.keys())
+        random.shuffle(features)
+        model.predict_one(x)
+        model.learn_one(
+            {i: x[i] for i in features[:-3]}, y
+        )  # drop 3 features at random
+
+
+def check_disappearing_features(model, dataset):
+    """The model should work fine when features disappear."""
+
+    for x, y in dataset:
+        features = list(x.keys())
+        random.shuffle(features)
+        model.predict_one({i: x[i] for i in features[:-3]})  # drop 3 features at random
+        model.learn_one(x, y)
+
+
+def check_debug_one(model, dataset):
+    for x, y in dataset:
+        model.debug_one(x)
+        model.learn_one(x, y)
+        model.debug_one(x)
+        break
+
+
+def check_pickling(model, dataset):
+    assert isinstance(pickle.loads(pickle.dumps(model)), model.__class__)
+    for x, y in dataset:
+        model.predict_one(x)
+        model.learn_one(x, y)
+    assert isinstance(pickle.loads(pickle.dumps(model)), model.__class__)
+
+
+def check_has_tag(model, tag):
+    assert tag in model._tags
+
+
+def check_repr(model):
+    assert isinstance(repr(model), str)
+
+
+def check_str(model):
+    assert isinstance(str(model), str)
+
+
+def check_tags(model):
+    """Checks that the `_tags` property works."""
+    assert isinstance(model._tags, set)
+
+
+def check_set_params_idempotent(model):
+    assert len(model.__dict__) == len(model._set_params().__dict__)
+
+
+def check_init_has_default_params_for_tests(model):
+    for params in model._unit_test_params():
+        assert isinstance(model.__class__(**params), model.__class__)
+
+
+def check_init_default_params_are_not_mutable(model):
+    """Mutable parameters in signatures are discouraged, as explained in
+    https://docs.python-guide.org/writing/gotchas/#mutable-default-arguments
+
+    We enforce immutable parameters by only allowing a certain list of basic types.
+
+    """
+
+    allowed = (type(None), float, int, tuple, str, bool, type)
+
+    for param in inspect.signature(model.__class__).parameters.values():
+        assert param.default is inspect._empty or isinstance(param.default, allowed)
+
+
+def check_doc(model):
+    assert model.__doc__
+
+
+def check_clone(model):
+    clone = model.clone()
+    assert id(clone) != id(model)
+    assert dir(clone) == dir(model)
+
+
+def check_model_selection_order_does_not_matter(model, dataset):
+    best_params = []
+    permutations = list(itertools.permutations(model.models))
+    datasets = itertools.tee(dataset, len(permutations))
+
+    for permutation, dataset in zip(permutations, datasets):
+        models = [model.clone() for model in permutation]
+        clone = model._set_params(new_params={"models": models})
+        for x, y in dataset:
+            clone.predict_one(x)
+            clone.learn_one(x, y)
+        best_params.append(clone.best_model._get_params())
+
+    # Check that the best params are always the same
+    assert all(params == best_params[0] for params in best_params)
+
+
+def seed_params(params, seed):
+    """Looks for "seed" keys and sets the value."""
+
+    def is_class_param(param):
+        return (
+            isinstance(param, tuple)
+            and inspect.isclass(param[0])
+            and isinstance(param[1], dict)
+        )
+
+    if is_class_param(params):
+        return params[0], seed_params(params[1], seed)
+
+    if not isinstance(params, dict):
+        return params
+
+    return {
+        name: seed if name == "seed" else seed_params(param, seed)
+        for name, param in params.items()
+    }
+
+
+def check_seeding_is_idempotent(model, dataset):
+
+    params = model._get_params()
+    seeded_params = seed_params(params, seed=42)
+
+    A = model._set_params(seeded_params)
+    B = model._set_params(seeded_params)
+
+    for x, y in dataset:
+        assert A.predict_one(x) == B.predict_one(x)
+        A.learn_one(x, y)
+        B.learn_one(x, y)
+
+
+def check_multiclass_is_bool(model):
+    assert isinstance(model._multiclass, bool)
+
+
+def wrapped_partial(func, *args, **kwargs):
+    """
+
+    Taken from http://louistiao.me/posts/adding-__name__-and-__doc__-attributes-to-functoolspartial-objects/
+
+    """
+    partial = functools.partial(func, *args, **kwargs)
+    functools.update_wrapper(partial, func)
+    return partial
+
+
+def allow_exception(func, exception):
+    def f(*args, **kwargs):
+        try:
+            func(*args, **kwargs)
+        except exception:
+            pass
+
+    f.__name__ = func.__name__
+    return f
+
+
+def yield_checks(model):
+    """Generates unit tests for a given model.
+
+    Parameters:
+        model (base.Estimator)
+
+    """
+
+    from river import model_selection, utils
+
+    # General checks
+    yield check_repr
+    yield check_str
+    yield check_tags
+    yield check_set_params_idempotent
+    yield check_init_has_default_params_for_tests
+    yield check_init_default_params_are_not_mutable
+    yield check_doc
+    yield check_clone
+
+    if utils.inspect.isclassifier(model):
+        yield check_multiclass_is_bool
+
+    # Checks that make use of datasets
+    checks = [
+        check_learn_one,
+        check_pickling,
+        check_shuffle_features_no_impact,
+        check_emerging_features,
+        check_disappearing_features,
+    ]
+
+    if hasattr(model, "debug_one"):
+        checks.append(check_debug_one)
+
+    if model._is_stochastic:
+        checks.append(check_seeding_is_idempotent)
+
+    # Classifier checks
+    if utils.inspect.isclassifier(model) and not utils.inspect.ismoclassifier(model):
+        checks.append(allow_exception(check_predict_proba_one, NotImplementedError))
+        # Specific checks for binary classifiers
+        if not model._multiclass:
+            checks.append(
+                allow_exception(check_predict_proba_one_binary, NotImplementedError)
+            )
+
+    if isinstance(utils.inspect.extract_relevant(model), model_selection.ModelSelector):
+        checks.append(check_model_selection_order_does_not_matter)
+
+    for check in checks:
+        for dataset in yield_datasets(model):
+            yield wrapped_partial(check, dataset=dataset)
+
+
+def check_estimator(model):
+    """Check if a model adheres to `river`'s conventions.
+
+    This will run a series of unit tests. The nature of the unit tests depends on the type of
+    model.
+
+    Parameters
+    ----------
+    model
+
+    """
+    for check in yield_checks(model):
+        if check.__name__ in model._unit_test_skips():
+            continue
+        check(copy.deepcopy(model))
```

### Comparing `river-0.8.0/river/utils/inspect.py` & `river-0.9.0/river/utils/inspect.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,67 +1,67 @@
-"""Utilities for inspecting a model's type.
-
-Sometimes we need to check if a model can perform regression, classification, etc. However, for
-some models the model's type is only known at runtime. For instance, we can't do
-`isinstance(pipeline, base.Regressor)` or `isinstance(wrapper, base.Regressor)`. This submodule
-thus provides utilities for determining an arbitrary model's type.
-
-"""
-from river import base, compose
-
-# TODO: maybe all of this could be done by monkeypatching isintance for pipelines?
-
-
-__all__ = [
-    "extract_relevant",
-    "isclassifier",
-    "isregressor",
-    "ismoclassifier",
-    "ismoregressor",
-    "isdriftdetector",
-]
-
-
-def extract_relevant(model: base.Estimator):
-    """Extracts the relevant part of a model.
-
-    Parameters
-    ----------
-    model
-
-    """
-
-    if isinstance(model, compose.Pipeline):
-        return extract_relevant(list(model.steps.values())[-1])  # look at last step
-    return model
-
-
-def isclassifier(model):
-    return isinstance(extract_relevant(model), base.Classifier)
-
-
-def isclusterer(model):
-    return isinstance(extract_relevant(model), base.Clusterer)
-
-
-def ismoclassifier(model):
-    return isclassifier(model) and isinstance(
-        extract_relevant(model), base.MultiOutputMixin
-    )
-
-
-def isregressor(model):
-    return isinstance(extract_relevant(model), base.Regressor)
-
-
-def istransformer(model):
-    return isinstance(extract_relevant(model), base.Transformer)
-
-
-def ismoregressor(model):
-    return isregressor(model) and isinstance(
-        extract_relevant(model), base.MultiOutputMixin
-    )
-
-
-def isdriftdetector(model):
-    return isinstance(extract_relevant(model), base.DriftDetector)
+"""Utilities for inspecting a model's type.
+
+Sometimes we need to check if a model can perform regression, classification, etc. However, for
+some models the model's type is only known at runtime. For instance, we can't do
+`isinstance(pipeline, base.Regressor)` or `isinstance(wrapper, base.Regressor)`. This submodule
+thus provides utilities for determining an arbitrary model's type.
+
+"""
+from river import base, compose
+
+# TODO: maybe all of this could be done by monkeypatching isintance for pipelines?
+
+
+__all__ = [
+    "extract_relevant",
+    "isclassifier",
+    "isregressor",
+    "ismoclassifier",
+    "ismoregressor",
+    "isdriftdetector",
+]
+
+
+def extract_relevant(model: base.Estimator):
+    """Extracts the relevant part of a model.
+
+    Parameters
+    ----------
+    model
+
+    """
+
+    if isinstance(model, compose.Pipeline):
+        return extract_relevant(model._last_step)
+    return model
+
+
+def isclassifier(model):
+    return isinstance(extract_relevant(model), base.Classifier)
+
+
+def isclusterer(model):
+    return isinstance(extract_relevant(model), base.Clusterer)
+
+
+def ismoclassifier(model):
+    return isclassifier(model) and isinstance(
+        extract_relevant(model), base.MultiOutputMixin
+    )
+
+
+def isregressor(model):
+    return isinstance(extract_relevant(model), base.Regressor)
+
+
+def istransformer(model):
+    return isinstance(extract_relevant(model), base.Transformer)
+
+
+def ismoregressor(model):
+    return isregressor(model) and isinstance(
+        extract_relevant(model), base.MultiOutputMixin
+    )
+
+
+def isdriftdetector(model):
+    return isinstance(extract_relevant(model), base.DriftDetector)
```

### Comparing `river-0.8.0/river/utils/param_grid.py` & `river-0.9.0/river/utils/param_grid.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,171 +1,171 @@
-import itertools
-import typing
-
-import numpy as np
-
-from river import base
-
-__all__ = ["expand_param_grid"]
-
-
-def expand_param_grid(model: base.Estimator, grid: dict) -> typing.List[base.Estimator]:
-    """Expands a grid of parameters.
-
-    This method can be used to generate a list of model parametrizations from a dictionary where
-    each parameter is associated with a list of possible parameters. In other words, it expands a
-    grid of parameters.
-
-    Typically, this method can be used to create copies of a given model with different parameter
-    choices. The models can then be used as part of a model selection process, such as a
-    `expert.SuccessiveHalvingClassifier` or a `expert.EWARegressor`.
-
-    The syntax for the parameter grid is quite flexible. It allows nesting parameters and can
-    therefore be used to generate parameters for a pipeline.
-
-    Parameters
-    ----------
-    model
-    grid
-        The grid of parameters to expand. The provided dictionary can be nested. The only
-        requirement is that the values at the leaves need to be lists.
-
-    Examples
-    --------
-
-    As an initial example, we can expand a grid of parameters for a single model.
-
-    >>> from river import linear_model
-    >>> from river import optim
-    >>> from river import utils
-
-    >>> model = linear_model.LinearRegression()
-
-    >>> grid = {'optimizer': [optim.SGD(.1), optim.SGD(.01), optim.SGD(.001)]}
-    >>> models = utils.expand_param_grid(model, grid)
-    >>> len(models)
-    3
-
-    >>> models[0]
-    LinearRegression (
-      optimizer=SGD (
-        lr=Constant (
-          learning_rate=0.1
-        )
-      )
-      loss=Squared ()
-      l2=0.
-      intercept_init=0.
-      intercept_lr=Constant (
-        learning_rate=0.01
-      )
-      clip_gradient=1e+12
-      initializer=Zeros ()
-    )
-
-    You can expand parameters for multiple choices like so:
-
-    >>> grid = {
-    ...     'optimizer': [
-    ...         (optim.SGD, {'lr': [.1, .01, .001]}),
-    ...         (optim.Adam, {'lr': [.1, .01, .01]})
-    ...     ]
-    ... }
-    >>> models = utils.expand_param_grid(model, grid)
-    >>> len(models)
-    6
-
-    You may specify a grid of parameters for a pipeline via nesting:
-
-    >>> from river import feature_extraction
-
-    >>> model = (
-    ...     feature_extraction.BagOfWords() |
-    ...     linear_model.LinearRegression()
-    ... )
-
-    >>> grid = {
-    ...     'BagOfWords': {
-    ...         'strip_accents': [False, True]
-    ...     },
-    ...     'LinearRegression': {
-    ...         'optimizer': [
-    ...             (optim.SGD, {'lr': [.1, .01]}),
-    ...             (optim.Adam, {'lr': [.1, .01]})
-    ...         ]
-    ...     }
-    ... }
-
-    >>> models = utils.expand_param_grid(model, grid)
-    >>> len(models)
-    8
-
-    """
-
-    return [model._set_params(params) for params in _expand_param_grid(grid)]
-
-
-def _expand_param_grid(grid: dict) -> typing.Iterator[dict]:
-    def expand_tuple(t):
-
-        klass, params = t
-
-        if not isinstance(klass, type):
-            raise ValueError(f"Expected first element to be a class, got {klass}")
-
-        if not isinstance(params, dict):
-            raise ValueError(f"Expected second element to be a dict, got {params}")
-
-        return (klass(**combo) for combo in _expand_param_grid(params))
-
-    def expand(k, v):
-
-        if isinstance(v, tuple):
-            return ((k, el) for el in expand_tuple(v))
-
-        # Example:
-        # k = 'lr'
-        # v = [0.001, 0.01, 0.1]
-        if isinstance(v, (list, set, np.ndarray)):
-
-            combos = []
-
-            for el in v:
-                if isinstance(el, tuple):
-                    for combo in expand_tuple(el):
-                        combos.append((k, combo))
-                else:
-                    combos.append((k, el))
-
-            return combos
-
-        if isinstance(v, dict):
-
-            # Example:
-            # k = 'LinearRegression'
-            # v = {
-            #     'intercept_lr': [0.001],
-            #     'l2': [1],
-            #     'optimizer': {
-            #         optim.Adam: {
-            #             'beta_1': [0.1, 0.01, 0.001],
-            #             'lr': [0.1, 0.01, 0.001]
-            #         },
-            #      }
-            # }
-            return ((k, el) for el in _expand_param_grid(v))
-
-        raise ValueError(f"unsupported type: {type(v)}")
-
-    for key in grid:
-        if not isinstance(key, str):
-            raise ValueError(f"Expected a key of type str; got {key}")
-
-    # Example:
-    # grid = {
-    #     'beta_1': [0.1, 0.01, 0.001],
-    #     'lr': [0.1, 0.01, 0.001]
-    # }
-    return (
-        dict(el) if isinstance(el[0], tuple) else el[0]
-        for el in itertools.product(*(expand(k, v) for k, v in grid.items()))
-    )
+import itertools
+import typing
+
+import numpy as np
+
+from river import base
+
+__all__ = ["expand_param_grid"]
+
+
+def expand_param_grid(model: base.Estimator, grid: dict) -> typing.List[base.Estimator]:
+    """Expands a grid of parameters.
+
+    This method can be used to generate a list of model parametrizations from a dictionary where
+    each parameter is associated with a list of possible parameters. In other words, it expands a
+    grid of parameters.
+
+    Typically, this method can be used to create copies of a given model with different parameter
+    choices. The models can then be used as part of a model selection process, such as a
+    `selection.SuccessiveHalvingClassifier` or a `selection.EWARegressor`.
+
+    The syntax for the parameter grid is quite flexible. It allows nesting parameters and can
+    therefore be used to generate parameters for a pipeline.
+
+    Parameters
+    ----------
+    model
+    grid
+        The grid of parameters to expand. The provided dictionary can be nested. The only
+        requirement is that the values at the leaves need to be lists.
+
+    Examples
+    --------
+
+    As an initial example, we can expand a grid of parameters for a single model.
+
+    >>> from river import linear_model
+    >>> from river import optim
+    >>> from river import utils
+
+    >>> model = linear_model.LinearRegression()
+
+    >>> grid = {'optimizer': [optim.SGD(.1), optim.SGD(.01), optim.SGD(.001)]}
+    >>> models = utils.expand_param_grid(model, grid)
+    >>> len(models)
+    3
+
+    >>> models[0]
+    LinearRegression (
+      optimizer=SGD (
+        lr=Constant (
+          learning_rate=0.1
+        )
+      )
+      loss=Squared ()
+      l2=0.
+      intercept_init=0.
+      intercept_lr=Constant (
+        learning_rate=0.01
+      )
+      clip_gradient=1e+12
+      initializer=Zeros ()
+    )
+
+    You can expand parameters for multiple choices like so:
+
+    >>> grid = {
+    ...     'optimizer': [
+    ...         (optim.SGD, {'lr': [.1, .01, .001]}),
+    ...         (optim.Adam, {'lr': [.1, .01, .01]})
+    ...     ]
+    ... }
+    >>> models = utils.expand_param_grid(model, grid)
+    >>> len(models)
+    6
+
+    You may specify a grid of parameters for a pipeline via nesting:
+
+    >>> from river import feature_extraction
+
+    >>> model = (
+    ...     feature_extraction.BagOfWords() |
+    ...     linear_model.LinearRegression()
+    ... )
+
+    >>> grid = {
+    ...     'BagOfWords': {
+    ...         'strip_accents': [False, True]
+    ...     },
+    ...     'LinearRegression': {
+    ...         'optimizer': [
+    ...             (optim.SGD, {'lr': [.1, .01]}),
+    ...             (optim.Adam, {'lr': [.1, .01]})
+    ...         ]
+    ...     }
+    ... }
+
+    >>> models = utils.expand_param_grid(model, grid)
+    >>> len(models)
+    8
+
+    """
+
+    return [model._set_params(params) for params in _expand_param_grid(grid)]
+
+
+def _expand_param_grid(grid: dict) -> typing.Iterator[dict]:
+    def expand_tuple(t):
+
+        klass, params = t
+
+        if not isinstance(klass, type):
+            raise ValueError(f"Expected first element to be a class, got {klass}")
+
+        if not isinstance(params, dict):
+            raise ValueError(f"Expected second element to be a dict, got {params}")
+
+        return (klass(**combo) for combo in _expand_param_grid(params))
+
+    def expand(k, v):
+
+        if isinstance(v, tuple):
+            return ((k, el) for el in expand_tuple(v))
+
+        # Example:
+        # k = 'lr'
+        # v = [0.001, 0.01, 0.1]
+        if isinstance(v, (list, set, np.ndarray)):
+
+            combos = []
+
+            for el in v:
+                if isinstance(el, tuple):
+                    for combo in expand_tuple(el):
+                        combos.append((k, combo))
+                else:
+                    combos.append((k, el))
+
+            return combos
+
+        if isinstance(v, dict):
+
+            # Example:
+            # k = 'LinearRegression'
+            # v = {
+            #     'intercept_lr': [0.001],
+            #     'l2': [1],
+            #     'optimizer': {
+            #         optim.Adam: {
+            #             'beta_1': [0.1, 0.01, 0.001],
+            #             'lr': [0.1, 0.01, 0.001]
+            #         },
+            #      }
+            # }
+            return ((k, el) for el in _expand_param_grid(v))
+
+        raise ValueError(f"unsupported type: {type(v)}")
+
+    for key in grid:
+        if not isinstance(key, str):
+            raise ValueError(f"Expected a key of type str; got {key}")
+
+    # Example:
+    # grid = {
+    #     'beta_1': [0.1, 0.01, 0.001],
+    #     'lr': [0.1, 0.01, 0.001]
+    # }
+    return (
+        dict(el) if isinstance(el[0], tuple) else el[0]
+        for el in itertools.product(*(expand(k, v) for k, v in grid.items()))
+    )
```

### Comparing `river-0.8.0/river/utils/pretty.py` & `river-0.9.0/river/utils/pretty.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,79 +1,79 @@
-"""Helper functions for making things readable by humans."""
-import math
-import typing
-
-__all__ = ["humanize_bytes", "print_table"]
-
-
-def print_table(
-    headers: typing.List[str],
-    columns: typing.List[typing.List[str]],
-    order: typing.List[int] = None,
-):
-    """Pretty-prints a table.
-
-    Parameters
-    ----------
-    headers
-        The column names.
-    columns
-        The column values.
-    order
-        Order in which to print the column the values. Defaults to the order in which the values
-        are given.
-
-    """
-
-    # Check inputs
-    if len(headers) != len(columns):
-        raise ValueError("there must be as many headers as columns")
-
-    if len(set(map(len, columns))) > 1:
-        raise ValueError("all the columns must be of the same length")
-
-    # Determine the width of each column based on the maximum length of it's elements
-    col_widths = [
-        max(*map(len, col), len(header)) for header, col in zip(headers, columns)
-    ]
-
-    # Make a template to print out rows one by one
-    row_format = " ".join(["{:" + str(width + 2) + "s}" for width in col_widths])
-
-    # Determine the order in which to print the column values
-    if order is None:
-        order = list(range(len(columns[0])))
-
-    # Build the table
-    table = (
-        row_format.format(*headers)
-        + "\n"
-        + "\n".join(
-            (
-                row_format.format(
-                    *[col[i].rjust(width) for col, width in zip(columns, col_widths)]
-                )
-                for i in order
-            )
-        )
-    )
-
-    return table
-
-
-def humanize_bytes(n_bytes: int):
-    """Returns a human-friendly byte size.
-
-    Parameters
-    ----------
-    n_bytes
-
-    """
-    suffixes = ["B", "KB", "MB", "GB", "TB", "PB"]
-    human = float(n_bytes)
-    rank = 0
-    if n_bytes != 0:
-        rank = int((math.log10(n_bytes)) / 3)
-        rank = min(rank, len(suffixes) - 1)
-        human = n_bytes / (1024.0 ** rank)
-    f = ("%.2f" % human).rstrip("0").rstrip(".")
-    return "%s %s" % (f, suffixes[rank])
+"""Helper functions for making things readable by humans."""
+import math
+import typing
+
+__all__ = ["humanize_bytes", "print_table"]
+
+
+def print_table(
+    headers: typing.List[str],
+    columns: typing.List[typing.List[str]],
+    order: typing.List[int] = None,
+):
+    """Pretty-prints a table.
+
+    Parameters
+    ----------
+    headers
+        The column names.
+    columns
+        The column values.
+    order
+        Order in which to print the column the values. Defaults to the order in which the values
+        are given.
+
+    """
+
+    # Check inputs
+    if len(headers) != len(columns):
+        raise ValueError("there must be as many headers as columns")
+
+    if len(set(map(len, columns))) > 1:
+        raise ValueError("all the columns must be of the same length")
+
+    # Determine the width of each column based on the maximum length of it's elements
+    col_widths = [
+        max(*map(len, col), len(header)) for header, col in zip(headers, columns)
+    ]
+
+    # Make a template to print out rows one by one
+    row_format = " ".join(["{:" + str(width + 2) + "s}" for width in col_widths])
+
+    # Determine the order in which to print the column values
+    if order is None:
+        order = list(range(len(columns[0])))
+
+    # Build the table
+    table = (
+        row_format.format(*headers)
+        + "\n"
+        + "\n".join(
+            (
+                row_format.format(
+                    *[col[i].rjust(width) for col, width in zip(columns, col_widths)]
+                )
+                for i in order
+            )
+        )
+    )
+
+    return table
+
+
+def humanize_bytes(n_bytes: int):
+    """Returns a human-friendly byte size.
+
+    Parameters
+    ----------
+    n_bytes
+
+    """
+    suffixes = ["B", "KB", "MB", "GB", "TB", "PB"]
+    human = float(n_bytes)
+    rank = 0
+    if n_bytes != 0:
+        rank = int((math.log10(n_bytes)) / 3)
+        rank = min(rank, len(suffixes) - 1)
+        human = n_bytes / (1024.0 ** rank)
+    f = ("%.2f" % human).rstrip("0").rstrip(".")
+    return "%s %s" % (f, suffixes[rank])
```

### Comparing `river-0.8.0/river/utils/sdft.py` & `river-0.9.0/river/utils/sdft.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,69 +1,69 @@
-import numpy as np
-
-from . import window
-
-
-class SDFT(window.Window):
-    """Sliding Discrete Fourier Transform (SDFT).
-
-    Initially, the coefficients are all equal to 0, up until enough values have been seen. A call
-    to `numpy.fft.fft` is triggered once `window_size` values have been seen. Subsequent values
-    will update the coefficients online. This is much faster than recomputing an FFT from scratch
-    for every new value.
-
-    Parameters
-    ----------
-    window_size
-        The size of the window.
-
-    Attributes
-    ----------
-    window : utils.Window
-        The window of values.
-
-    Examples
-    --------
-
-    >>> from river import utils
-
-    >>> X = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
-
-    >>> window_size = 5
-    >>> sdft = utils.SDFT(window_size)
-
-    >>> for i, x in enumerate(X):
-    ...     sdft = sdft.update(x)
-    ...
-    ...     if i + 1 >= window_size:
-    ...         assert np.allclose(sdft, np.fft.fft(X[i+1 - window_size:i+1]))
-
-    References
-    ----------
-    [^1]: `Jacobsen, E. and Lyons, R., 2003. The sliding DFT. IEEE Signal Processing Magazine, 20(2), pp.74-80. <https://www.comm.utoronto.ca/~dimitris/ece431/slidingdft.pdf>`_
-    [^2]: `Understanding and Implementing the Sliding DFT <https://www.dsprelated.com/showarticle/776.php>`_
-
-    """
-
-    def __init__(self, window_size):
-        super().__init__(size=window_size)
-        self.window = window.Window(size=window_size)
-
-    def update(self, x):
-
-        # Simply append the new value if the window isn't full yet
-        if len(self.window) < self.window.size - 1:
-            self.window.append(x)
-
-        # Compute an initial FFT the first time the window is full
-        elif len(self.window) == self.window.size - 1:
-            self.window.append(x)
-            self.extend(np.fft.fft(self.window))
-
-        # Update the coefficients for subsequent values
-        else:
-            diff = x - self.window[0]
-            for i in range(self.size):
-                self[i] = (self[i] + diff) * np.exp(2j * np.pi * i / self.size)
-            self.window.append(x)
-
-        return self
+import numpy as np
+
+from . import window
+
+
+class SDFT(window.Window):
+    """Sliding Discrete Fourier Transform (SDFT).
+
+    Initially, the coefficients are all equal to 0, up until enough values have been seen. A call
+    to `numpy.fft.fft` is triggered once `window_size` values have been seen. Subsequent values
+    will update the coefficients online. This is much faster than recomputing an FFT from scratch
+    for every new value.
+
+    Parameters
+    ----------
+    window_size
+        The size of the window.
+
+    Attributes
+    ----------
+    window : utils.Window
+        The window of values.
+
+    Examples
+    --------
+
+    >>> from river import utils
+
+    >>> X = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
+
+    >>> window_size = 5
+    >>> sdft = utils.SDFT(window_size)
+
+    >>> for i, x in enumerate(X):
+    ...     sdft = sdft.update(x)
+    ...
+    ...     if i + 1 >= window_size:
+    ...         assert np.allclose(sdft, np.fft.fft(X[i+1 - window_size:i+1]))
+
+    References
+    ----------
+    [^1]: `Jacobsen, E. and Lyons, R., 2003. The sliding DFT. IEEE Signal Processing Magazine, 20(2), pp.74-80. <https://www.comm.utoronto.ca/~dimitris/ece431/slidingdft.pdf>`_
+    [^2]: `Understanding and Implementing the Sliding DFT <https://www.dsprelated.com/showarticle/776.php>`_
+
+    """
+
+    def __init__(self, window_size):
+        super().__init__(size=window_size)
+        self.window = window.Window(size=window_size)
+
+    def update(self, x):
+
+        # Simply append the new value if the window isn't full yet
+        if len(self.window) < self.window.size - 1:
+            self.window.append(x)
+
+        # Compute an initial FFT the first time the window is full
+        elif len(self.window) == self.window.size - 1:
+            self.window.append(x)
+            self.extend(np.fft.fft(self.window))
+
+        # Update the coefficients for subsequent values
+        else:
+            diff = x - self.window[0]
+            for i in range(self.size):
+                self[i] = (self[i] + diff) * np.exp(2j * np.pi * i / self.size)
+            self.window.append(x)
+
+        return self
```

### Comparing `river-0.8.0/river/utils/skmultiflow_utils.py` & `river-0.9.0/river/utils/skmultiflow_utils.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,347 +1,347 @@
-import copy
-import math
-import numbers
-import sys
-from collections import deque
-
-import numpy as np
-
-
-def get_dimensions(X) -> tuple:
-    """Return the dimensions from a numpy.array, numpy.ndarray or list.
-
-    Parameters
-    ----------
-    X
-        numpy.array, numpy.ndarray, list, list of lists.
-
-    Returns
-    -------
-    A tuple representing the X structure's dimensions.
-
-    """
-    r, c = 1, 1
-    if isinstance(X, type(np.array([0]))):
-        if X.ndim > 1:
-            r, c = X.shape
-        else:
-            r, c = 1, X.size
-
-    elif isinstance(X, type([])):
-        if isinstance(X[0], type([])):
-            r, c = len(X), len(X[0])
-        else:
-            c = len(X)
-
-    return r, c
-
-
-def normalize_values_in_dict(dictionary, factor=None, inplace=True, raise_error=False):
-    """Normalize the values in a dictionary using the given factor.
-
-    For each element in the dictionary, applies `value/factor`.
-
-    Parameters
-    ----------
-    dictionary
-        Dictionary to normalize.
-    factor
-        Normalization factor value. If not set, use the sum of values.
-    inplace
-        If True, perform operation in-place
-    raise_error
-        In case the normalization factor is either `0` or `None`:</br>
-        - `True`: raise an error.
-        - `False`: return gracefully (if `inplace=False`, a copy of) `dictionary`.
-
-    Raises
-    ------
-    ValueError
-        In case the normalization factor is either `0` or `None` and `raise_error=True`.
-
-    """
-    if factor is None:
-        factor = sum(dictionary.values())
-
-    if not inplace:
-        dictionary = copy.deepcopy(dictionary)
-
-    if factor == 0 or math.isnan(factor):
-        # Can not normalize
-        if raise_error:
-            raise ValueError(f"Can not normalize, normalization factor is {factor}")
-        # return gracefully
-        return dictionary
-
-    scale_values_in_dict(dictionary, 1 / factor, inplace=True)
-
-    return dictionary
-
-
-def scale_values_in_dict(dictionary, multiplier, inplace=True):
-    """Scale the values in a dictionary.
-
-        For each element in the dictionary, applies `value * multiplier`.
-
-        Parameters
-        ----------
-        dictionary
-            Dictionary to scale.
-        multiplier
-            Scaling value.
-        inplace
-            If True, perform operation in-place
-
-        """
-
-    if not inplace:
-        dictionary = copy.deepcopy(dictionary)
-
-    for key, value in dictionary.items():
-        dictionary[key] = value * multiplier
-
-    return dictionary
-
-
-def get_max_value_key(dictionary):
-    """Get the key of the maximum value in a dictionary.
-
-    Parameters
-    ----------
-    dictionary
-        Dictionary to evaluate.
-
-    Returns
-    -------
-    int
-        Key of the maximum value.
-
-    """
-    if dictionary and isinstance(dictionary, dict):
-        return max(dictionary, key=dictionary.get)
-    else:
-        return 0
-
-
-def calculate_object_size(obj, unit="byte") -> int:
-    """Iteratively calculates the `obj` size in bytes.
-
-    Visits all the elements related to obj accounting for their respective
-    sizes.
-
-    Parameters
-    ----------
-    object
-        Object to evaluate.
-    string
-        The unit in which the accounted value is going to be returned.
-        Values: 'byte', 'kB', 'MB' (Default: 'byte').
-
-    Returns
-    -------
-    The size of the object and its related properties and objects, in 'unit'.
-
-    """
-    seen = set()
-    to_visit = deque()
-    byte_size = 0
-
-    to_visit.append(obj)
-
-    while True:
-        try:
-            obj = to_visit.popleft()
-        except IndexError:
-            break
-
-        # If element was already covered, skip it
-        if id(obj) in seen:
-            continue
-
-        # Update size accounting
-        byte_size += sys.getsizeof(obj)
-
-        # Mark element as seen
-        seen.add(id(obj))
-
-        # Add keys and values for size account
-        if isinstance(obj, dict):
-            for v in obj.values():
-                to_visit.append(v)
-
-            for k in obj.keys():
-                to_visit.append(k)
-        elif hasattr(obj, "__dict__"):
-            to_visit.append(obj.__dict__)
-        elif hasattr(obj, "__iter__") and not isinstance(obj, (str, bytes, bytearray)):
-            for i in obj:
-                to_visit.append(i)
-
-    if unit == "kB":
-        final_size = byte_size / 1024
-    elif unit == "MB":
-        final_size = byte_size / (2 ** 20)
-    else:
-        final_size = byte_size
-
-    return final_size
-
-
-def is_scalar_nan(x) -> bool:
-    """Tests if x is NaN
-
-    This function is meant to overcome the issue that np.isnan does not allow
-    non-numerical types as input, and that np.nan is not np.float('nan').
-
-    Parameters
-    ----------
-    x
-        any type
-
-    Examples
-    --------
-    >>> is_scalar_nan(np.nan)
-    True
-    >>> is_scalar_nan(float("nan"))
-    True
-    >>> is_scalar_nan(None)
-    False
-    >>> is_scalar_nan("")
-    False
-    >>> is_scalar_nan([np.nan])
-    False
-    """
-    # convert from numpy.bool_ to python bool to ensure that testing
-    # is_scalar_nan(x) is True does not fail.
-    return bool(isinstance(x, numbers.Real) and np.isnan(x))
-
-
-def add_dict_values(dict_a: dict, dict_b: dict, inplace=False) -> dict:
-    """Adds two dictionaries, summing the values of elements with the same key.
-
-    This function iterates over the keys of dict_b and adds their corresponding
-    values to the elements in dict_a. If dict_b has a (key, value) pair that
-    does not belong to dict_a, this pair is added to the latter dictionary.
-
-    Parameters
-    ----------
-    dict_a
-        dictionary to update.
-    dict_b
-        dictionary whose values will be added to `dict_a`.
-    inplace
-        If `True`, the addition is performed in-place and results are stored in `dict_a`.
-        If `False`, `dict_a` is not changed and the results are returned in a new dictionary.
-
-    Returns
-    -------
-    A dictionary containing the result of the operation. Either a pointer to
-    `dict_a` or a new dictionary depending on parameter `inplace`.
-
-    """
-    if inplace:
-        result = dict_a
-    else:
-        result = copy.deepcopy(dict_a)
-
-    for k, v in dict_b.items():
-        try:
-            result[k] += v
-        except KeyError:
-            result[k] = v
-    return result
-
-
-def add_delay_to_timestamps(timestamps, delay):
-    """Add a given delay to a list of timestamps.
-
-    This function iterates over the timestamps, adding a time delay to them.
-
-    Parameters
-    ----------
-    timestamps
-        np.ndarray(dtype=datetime64).
-    delay
-        np.timedelta64.
-
-    Returns
-    -------
-    A list of timestamps with a delay added to all timestamp.
-
-    """
-
-    delay_timestamps = []
-    for t in timestamps:
-        delay_timestamps.append(t + delay)
-    return np.array(delay_timestamps, dtype="datetime64")
-
-
-def check_random_state(seed):
-    """Turn seed into a np.random.RandomState instance.
-
-    Parameters
-    ----------
-    seed : None | int | instance of RandomState
-        If seed is None, return the RandomState singleton used by np.random.
-        If seed is an int, return a new RandomState instance seeded with seed.
-        If seed is already a RandomState instance, return it.
-        Otherwise raise ValueError.
-
-    Notes
-    -----
-    Code from sklearn.
-    This method is exclusive for cases where np.random is used.
-
-    """
-    if seed is None or seed is np.random:
-        return np.random.mtrand._rand  # noqa
-    if isinstance(seed, (numbers.Integral, np.integer)):
-        return np.random.RandomState(seed)
-    if isinstance(seed, np.random.RandomState):
-        return seed
-    raise ValueError(
-        f"{seed} cannot be used to seed a numpy.random.RandomState instance"
-    )
-
-
-def round_sig_fig(x, significant_digits=2) -> float:
-    """Round considering of significant figures of x, given the select
-    `significant_digits` prototype.
-
-    If`significant_digits` match the number of significant figures in `x`, its value
-    will be used for rounding; otherwise, decimal places will removed
-    accordingly to the significant figures in `x`.
-
-    Parameters
-    ----------
-    x
-        A floating point scalar.
-    significant_digits
-        The number of intended rounding figures.
-
-    Returns
-    -------
-        The rounded value of `x`.
-
-    Examples
-    --------
-    >>> round_sig_fig(1.2345)
-    1.2
-    >>> round_sig_fig(1.2345, significant_digits=3)
-    1.23
-    >>> round_sig_fig(0.0)
-    0.0
-    >>> round_sig_fig(0)
-    0
-    >>> round_sig_fig(1999, significant_digits=1)
-    2000
-    >>> round_sig_fig(1999, significant_digits=4)
-    1999
-    >>> round_sig_fig(0.025, significant_digits=3)
-    0.03
-    >>> round_sig_fig(0.025, significant_digits=10)
-    0.025
-    >>> round_sig_fig(0.0250, significant_digits=10)
-    0.025
-    """
-    return round(x, significant_digits - int(math.floor(math.log10(abs(x) + 1))) - 1)
+import copy
+import math
+import numbers
+import sys
+from collections import deque
+
+import numpy as np
+
+
+def get_dimensions(X) -> tuple:
+    """Return the dimensions from a numpy.array, numpy.ndarray or list.
+
+    Parameters
+    ----------
+    X
+        numpy.array, numpy.ndarray, list, list of lists.
+
+    Returns
+    -------
+    A tuple representing the X structure's dimensions.
+
+    """
+    r, c = 1, 1
+    if isinstance(X, type(np.array([0]))):
+        if X.ndim > 1:
+            r, c = X.shape
+        else:
+            r, c = 1, X.size
+
+    elif isinstance(X, type([])):
+        if isinstance(X[0], type([])):
+            r, c = len(X), len(X[0])
+        else:
+            c = len(X)
+
+    return r, c
+
+
+def normalize_values_in_dict(dictionary, factor=None, inplace=True, raise_error=False):
+    """Normalize the values in a dictionary using the given factor.
+
+    For each element in the dictionary, applies `value/factor`.
+
+    Parameters
+    ----------
+    dictionary
+        Dictionary to normalize.
+    factor
+        Normalization factor value. If not set, use the sum of values.
+    inplace
+        If True, perform operation in-place
+    raise_error
+        In case the normalization factor is either `0` or `None`:</br>
+        - `True`: raise an error.
+        - `False`: return gracefully (if `inplace=False`, a copy of) `dictionary`.
+
+    Raises
+    ------
+    ValueError
+        In case the normalization factor is either `0` or `None` and `raise_error=True`.
+
+    """
+    if factor is None:
+        factor = sum(dictionary.values())
+
+    if not inplace:
+        dictionary = copy.deepcopy(dictionary)
+
+    if factor == 0 or math.isnan(factor):
+        # Can not normalize
+        if raise_error:
+            raise ValueError(f"Can not normalize, normalization factor is {factor}")
+        # return gracefully
+        return dictionary
+
+    scale_values_in_dict(dictionary, 1 / factor, inplace=True)
+
+    return dictionary
+
+
+def scale_values_in_dict(dictionary, multiplier, inplace=True):
+    """Scale the values in a dictionary.
+
+        For each element in the dictionary, applies `value * multiplier`.
+
+        Parameters
+        ----------
+        dictionary
+            Dictionary to scale.
+        multiplier
+            Scaling value.
+        inplace
+            If True, perform operation in-place
+
+        """
+
+    if not inplace:
+        dictionary = copy.deepcopy(dictionary)
+
+    for key, value in dictionary.items():
+        dictionary[key] = value * multiplier
+
+    return dictionary
+
+
+def get_max_value_key(dictionary):
+    """Get the key of the maximum value in a dictionary.
+
+    Parameters
+    ----------
+    dictionary
+        Dictionary to evaluate.
+
+    Returns
+    -------
+    int
+        Key of the maximum value.
+
+    """
+    if dictionary and isinstance(dictionary, dict):
+        return max(dictionary, key=dictionary.get)
+    else:
+        return 0
+
+
+def calculate_object_size(obj, unit="byte") -> int:
+    """Iteratively calculates the `obj` size in bytes.
+
+    Visits all the elements related to obj accounting for their respective
+    sizes.
+
+    Parameters
+    ----------
+    object
+        Object to evaluate.
+    string
+        The unit in which the accounted value is going to be returned.
+        Values: 'byte', 'kB', 'MB' (Default: 'byte').
+
+    Returns
+    -------
+    The size of the object and its related properties and objects, in 'unit'.
+
+    """
+    seen = set()
+    to_visit = deque()
+    byte_size = 0
+
+    to_visit.append(obj)
+
+    while True:
+        try:
+            obj = to_visit.popleft()
+        except IndexError:
+            break
+
+        # If element was already covered, skip it
+        if id(obj) in seen:
+            continue
+
+        # Update size accounting
+        byte_size += sys.getsizeof(obj)
+
+        # Mark element as seen
+        seen.add(id(obj))
+
+        # Add keys and values for size account
+        if isinstance(obj, dict):
+            for v in obj.values():
+                to_visit.append(v)
+
+            for k in obj.keys():
+                to_visit.append(k)
+        elif hasattr(obj, "__dict__"):
+            to_visit.append(obj.__dict__)
+        elif hasattr(obj, "__iter__") and not isinstance(obj, (str, bytes, bytearray)):
+            for i in obj:
+                to_visit.append(i)
+
+    if unit == "kB":
+        final_size = byte_size / 1024
+    elif unit == "MB":
+        final_size = byte_size / (2 ** 20)
+    else:
+        final_size = byte_size
+
+    return final_size
+
+
+def is_scalar_nan(x) -> bool:
+    """Tests if x is NaN
+
+    This function is meant to overcome the issue that np.isnan does not allow
+    non-numerical types as input, and that np.nan is not np.float('nan').
+
+    Parameters
+    ----------
+    x
+        any type
+
+    Examples
+    --------
+    >>> is_scalar_nan(np.nan)
+    True
+    >>> is_scalar_nan(float("nan"))
+    True
+    >>> is_scalar_nan(None)
+    False
+    >>> is_scalar_nan("")
+    False
+    >>> is_scalar_nan([np.nan])
+    False
+    """
+    # convert from numpy.bool_ to python bool to ensure that testing
+    # is_scalar_nan(x) is True does not fail.
+    return bool(isinstance(x, numbers.Real) and np.isnan(x))
+
+
+def add_dict_values(dict_a: dict, dict_b: dict, inplace=False) -> dict:
+    """Adds two dictionaries, summing the values of elements with the same key.
+
+    This function iterates over the keys of dict_b and adds their corresponding
+    values to the elements in dict_a. If dict_b has a (key, value) pair that
+    does not belong to dict_a, this pair is added to the latter dictionary.
+
+    Parameters
+    ----------
+    dict_a
+        dictionary to update.
+    dict_b
+        dictionary whose values will be added to `dict_a`.
+    inplace
+        If `True`, the addition is performed in-place and results are stored in `dict_a`.
+        If `False`, `dict_a` is not changed and the results are returned in a new dictionary.
+
+    Returns
+    -------
+    A dictionary containing the result of the operation. Either a pointer to
+    `dict_a` or a new dictionary depending on parameter `inplace`.
+
+    """
+    if inplace:
+        result = dict_a
+    else:
+        result = copy.deepcopy(dict_a)
+
+    for k, v in dict_b.items():
+        try:
+            result[k] += v
+        except KeyError:
+            result[k] = v
+    return result
+
+
+def add_delay_to_timestamps(timestamps, delay):
+    """Add a given delay to a list of timestamps.
+
+    This function iterates over the timestamps, adding a time delay to them.
+
+    Parameters
+    ----------
+    timestamps
+        np.ndarray(dtype=datetime64).
+    delay
+        np.timedelta64.
+
+    Returns
+    -------
+    A list of timestamps with a delay added to all timestamp.
+
+    """
+
+    delay_timestamps = []
+    for t in timestamps:
+        delay_timestamps.append(t + delay)
+    return np.array(delay_timestamps, dtype="datetime64")
+
+
+def check_random_state(seed):
+    """Turn seed into a np.random.RandomState instance.
+
+    Parameters
+    ----------
+    seed : None | int | instance of RandomState
+        If seed is None, return the RandomState singleton used by np.random.
+        If seed is an int, return a new RandomState instance seeded with seed.
+        If seed is already a RandomState instance, return it.
+        Otherwise raise ValueError.
+
+    Notes
+    -----
+    Code from sklearn.
+    This method is exclusive for cases where np.random is used.
+
+    """
+    if seed is None or seed is np.random:
+        return np.random.mtrand._rand  # noqa
+    if isinstance(seed, (numbers.Integral, np.integer)):
+        return np.random.RandomState(seed)
+    if isinstance(seed, np.random.RandomState):
+        return seed
+    raise ValueError(
+        f"{seed} cannot be used to seed a numpy.random.RandomState instance"
+    )
+
+
+def round_sig_fig(x, significant_digits=2) -> float:
+    """Round considering of significant figures of x, given the select
+    `significant_digits` prototype.
+
+    If`significant_digits` match the number of significant figures in `x`, its value
+    will be used for rounding; otherwise, decimal places will removed
+    accordingly to the significant figures in `x`.
+
+    Parameters
+    ----------
+    x
+        A floating point scalar.
+    significant_digits
+        The number of intended rounding figures.
+
+    Returns
+    -------
+        The rounded value of `x`.
+
+    Examples
+    --------
+    >>> round_sig_fig(1.2345)
+    1.2
+    >>> round_sig_fig(1.2345, significant_digits=3)
+    1.23
+    >>> round_sig_fig(0.0)
+    0.0
+    >>> round_sig_fig(0)
+    0
+    >>> round_sig_fig(1999, significant_digits=1)
+    2000
+    >>> round_sig_fig(1999, significant_digits=4)
+    1999
+    >>> round_sig_fig(0.025, significant_digits=3)
+    0.03
+    >>> round_sig_fig(0.025, significant_digits=10)
+    0.025
+    >>> round_sig_fig(0.0250, significant_digits=10)
+    0.025
+    """
+    return round(x, significant_digits - int(math.floor(math.log10(abs(x) + 1))) - 1)
```

### Comparing `river-0.8.0/river/utils/test_param_grid.py` & `river-0.9.0/river/utils/test_param_grid.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,69 +1,69 @@
-import pytest
-
-from river import compose, linear_model, optim, preprocessing, tree, utils
-
-
-@pytest.mark.parametrize(
-    "model, param_grid, count",
-    [
-        (
-            linear_model.LinearRegression(),
-            {
-                "optimizer": [
-                    (optim.SGD, {"lr": [1, 2]}),
-                    (
-                        optim.Adam,
-                        {
-                            "beta_1": [0.1, 0.01, 0.001],
-                            "lr": [0.1, 0.01, 0.001, 0.0001],
-                        },
-                    ),
-                ]
-            },
-            2 + 3 * 4,
-        ),
-        (
-            preprocessing.StandardScaler() | linear_model.LinearRegression(),
-            {
-                "LinearRegression": {
-                    "optimizer": [
-                        (optim.SGD, {"lr": [1, 2]}),
-                        (
-                            optim.Adam,
-                            {
-                                "beta_1": [0.1, 0.01, 0.001],
-                                "lr": [0.1, 0.01, 0.001, 0.0001],
-                            },
-                        ),
-                    ]
-                }
-            },
-            2 + 3 * 4,
-        ),
-        (
-            compose.Pipeline(("Scaler", None), linear_model.LinearRegression()),
-            {
-                "Scaler": [
-                    preprocessing.MinMaxScaler(),
-                    preprocessing.MaxAbsScaler(),
-                    preprocessing.StandardScaler(),
-                ],
-                "LinearRegression": {"optimizer": {"lr": [1e-1, 1e-2, 1e-3]}},
-            },
-            3 * 3,
-        ),
-    ],
-)
-def test_expand_param_grid_count(model, param_grid, count):
-    assert len(utils.expand_param_grid(model, param_grid)) == count
-
-
-def test_decision_tree_max_depth():
-
-    model = tree.HoeffdingTreeClassifier()
-
-    max_depths = [1, 2, 3, 4, 5, 6]
-    models = utils.expand_param_grid(model, {"max_depth": max_depths})
-
-    for model, max_depth in zip(models, max_depths):
-        assert model.max_depth == max_depth
+import pytest
+
+from river import compose, linear_model, optim, preprocessing, tree, utils
+
+
+@pytest.mark.parametrize(
+    "model, param_grid, count",
+    [
+        (
+            linear_model.LinearRegression(),
+            {
+                "optimizer": [
+                    (optim.SGD, {"lr": [1, 2]}),
+                    (
+                        optim.Adam,
+                        {
+                            "beta_1": [0.1, 0.01, 0.001],
+                            "lr": [0.1, 0.01, 0.001, 0.0001],
+                        },
+                    ),
+                ]
+            },
+            2 + 3 * 4,
+        ),
+        (
+            preprocessing.StandardScaler() | linear_model.LinearRegression(),
+            {
+                "LinearRegression": {
+                    "optimizer": [
+                        (optim.SGD, {"lr": [1, 2]}),
+                        (
+                            optim.Adam,
+                            {
+                                "beta_1": [0.1, 0.01, 0.001],
+                                "lr": [0.1, 0.01, 0.001, 0.0001],
+                            },
+                        ),
+                    ]
+                }
+            },
+            2 + 3 * 4,
+        ),
+        (
+            compose.Pipeline(("Scaler", None), linear_model.LinearRegression()),
+            {
+                "Scaler": [
+                    preprocessing.MinMaxScaler(),
+                    preprocessing.MaxAbsScaler(),
+                    preprocessing.StandardScaler(),
+                ],
+                "LinearRegression": {"optimizer": {"lr": [1e-1, 1e-2, 1e-3]}},
+            },
+            3 * 3,
+        ),
+    ],
+)
+def test_expand_param_grid_count(model, param_grid, count):
+    assert len(utils.expand_param_grid(model, param_grid)) == count
+
+
+def test_decision_tree_max_depth():
+
+    model = tree.HoeffdingTreeClassifier()
+
+    max_depths = [1, 2, 3, 4, 5, 6]
+    models = utils.expand_param_grid(model, {"max_depth": max_depths})
+
+    for model, max_depth in zip(models, max_depths):
+        assert model.max_depth == max_depth
```

### Comparing `river-0.8.0/river/utils/vectordict.c` & `river-0.9.0/river/utils/vectordict.c`

 * *Files 0% similar despite different names*

```diff
@@ -6,22 +6,19 @@
         "define_macros": [
             [
                 "NPY_NO_DEPRECATED_API",
                 "NPY_1_7_API_VERSION"
             ]
         ],
         "include_dirs": [
-            "/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/core/include"
-        ],
-        "libraries": [
-            "m"
+            "c:\\hostedtoolcache\\windows\\python\\3.8.10\\x64\\lib\\site-packages\\numpy\\core\\include"
         ],
         "name": "river.utils.vectordict",
         "sources": [
-            "river/utils/vectordict.pyx"
+            "river\\utils\\vectordict.pyx"
         ]
     },
     "module_name": "river.utils.vectordict"
 }
 END: Cython Metadata */
 
 #ifndef PY_SSIZE_T_CLEAN
@@ -843,15 +840,15 @@
 static int __pyx_lineno;
 static int __pyx_clineno = 0;
 static const char * __pyx_cfilenm= __FILE__;
 static const char *__pyx_filename;
 
 
 static const char *__pyx_f[] = {
-  "river/utils/vectordict.pyx",
+  "river\\utils\\vectordict.pyx",
   "stringsource",
 };
 
 /*--- Type declarations ---*/
 struct __pyx_obj_5river_5utils_10vectordict_VectorDict;
 struct __pyx_obj_5river_5utils_10vectordict___pyx_scope_struct____pyx_f_5river_5utils_10vectordict_get_union_keys;
 struct __pyx_obj_5river_5utils_10vectordict___pyx_scope_struct_1_genexpr;
@@ -2071,15 +2068,15 @@
 static const char __pyx_k_VectorDict_to_numpy[] = "VectorDict.to_numpy";
 static const char __pyx_k_VectorDict_with_mask[] = "VectorDict.with_mask";
 static const char __pyx_k_VectorDict_setdefault[] = "VectorDict.setdefault";
 static const char __pyx_k_river_utils_vectordict[] = "river.utils.vectordict";
 static const char __pyx_k_pyx_unpickle_VectorDict[] = "__pyx_unpickle_VectorDict";
 static const char __pyx_k_Unsupported_type_for_data[] = "Unsupported type for data: ";
 static const char __pyx_k_VectorDict___reduce_cython[] = "VectorDict.__reduce_cython__";
-static const char __pyx_k_river_utils_vectordict_pyx[] = "river/utils/vectordict.pyx";
+static const char __pyx_k_river_utils_vectordict_pyx[] = "river\\utils\\vectordict.pyx";
 static const char __pyx_k_VectorDict___setstate_cython[] = "VectorDict.__setstate_cython__";
 static const char __pyx_k_get_union_keys_locals_genexpr[] = "get_union_keys.<locals>.genexpr";
 static const char __pyx_k_Cannot_mask_a_masked_VectorDict[] = "Cannot mask a masked VectorDict without copy";
 static const char __pyx_k_VectorDict__keys_locals_genexpr[] = "VectorDict._keys.<locals>.genexpr";
 static const char __pyx_k_Incompatible_checksums_s_vs_0x16[] = "Incompatible checksums (%s vs 0x1613777 = (_data, _default_factory, _lazy_mask, _mask, _use_factory, _use_mask))";
 static const char __pyx_k_get_intersection_keys_locals_gen[] = "get_intersection_keys.<locals>.genexpr";
 static PyObject *__pyx_kp_u_Cannot_mask_a_masked_VectorDict;
```

### Comparing `river-0.8.0/river/utils/window.py` & `river-0.9.0/river/utils/window.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,117 +1,117 @@
-import bisect
-import collections
-import typing
-
-
-class Window:
-    """Running window data structure.
-
-    This is just a convenience layer on top of a `collections.deque`. The only reason this exists
-    is that deepcopying a class which inherits from `collections.deque` seems to bug out when the
-    class has a parameter with no default value.
-
-    Parameters
-    ----------
-    size
-        Size of the rolling window.
-
-    Examples
-    --------
-
-    >>> from river import utils
-
-    >>> window = utils.Window(size=2)
-
-    >>> for x in [1, 2, 3, 4, 5, 6]:
-    ...     print(window.append(x))
-    [1]
-    [1, 2]
-    [2, 3]
-    [3, 4]
-    [4, 5]
-    [5, 6]
-
-    """
-
-    def __init__(self, size: int):
-        self.values: typing.Deque[typing.Any] = collections.deque(maxlen=size)
-
-    @property
-    def size(self):
-        return self.values.maxlen
-
-    def __repr__(self):
-        return str(list(self.values))
-
-    def __len__(self):
-        return len(self.values)
-
-    def __getitem__(self, idx):
-        return self.values[idx]
-
-    def __setitem__(self, idx, val):
-        self.values[idx] = val
-        return self
-
-    def extend(self, values):
-        self.values.extend(values)
-        return self
-
-    def append(self, x):
-        self.values.append(x)
-        return self
-
-    def popleft(self):
-        return self.values.popleft()
-
-
-class SortedWindow(collections.UserList):
-    """Sorted running window data structure.
-
-    Parameters
-    ----------
-    size
-        Size of the window to compute the rolling quantile.
-
-    Examples
-    --------
-
-    >>> from river import utils
-
-    >>> window = utils.SortedWindow(size=3)
-
-    >>> for i in reversed(range(9)):
-    ...     print(window.append(i))
-    [8]
-    [7, 8]
-    [6, 7, 8]
-    [5, 6, 7]
-    [4, 5, 6]
-    [3, 4, 5]
-    [2, 3, 4]
-    [1, 2, 3]
-    [0, 1, 2]
-
-    References
-    ----------
-    [^1]: [Left sorted inserts in Python](https://stackoverflow.com/questions/8024571/insert-an-item-into-sorted-list-in-python)
-
-    """
-
-    def __init__(self, size: int):
-        super().__init__()
-        self.unsorted_window = Window(size)
-
-    @property
-    def size(self):
-        return self.unsorted_window.size
-
-    def append(self, x):
-
-        if len(self) >= self.size:
-            self.remove(self.unsorted_window[0])
-
-        bisect.insort_left(self, x)
-        self.unsorted_window.append(x)
-
-        return self
+import bisect
+import collections
+import typing
+
+
+class Window:
+    """Running window data structure.
+
+    This is just a convenience layer on top of a `collections.deque`. The only reason this exists
+    is that deepcopying a class which inherits from `collections.deque` seems to bug out when the
+    class has a parameter with no default value.
+
+    Parameters
+    ----------
+    size
+        Size of the rolling window.
+
+    Examples
+    --------
+
+    >>> from river import utils
+
+    >>> window = utils.Window(size=2)
+
+    >>> for x in [1, 2, 3, 4, 5, 6]:
+    ...     print(window.append(x))
+    [1]
+    [1, 2]
+    [2, 3]
+    [3, 4]
+    [4, 5]
+    [5, 6]
+
+    """
+
+    def __init__(self, size: int):
+        self.values: typing.Deque[typing.Any] = collections.deque(maxlen=size)
+
+    @property
+    def size(self):
+        return self.values.maxlen
+
+    def __repr__(self):
+        return str(list(self.values))
+
+    def __len__(self):
+        return len(self.values)
+
+    def __getitem__(self, idx):
+        return self.values[idx]
+
+    def __setitem__(self, idx, val):
+        self.values[idx] = val
+        return self
+
+    def extend(self, values):
+        self.values.extend(values)
+        return self
+
+    def append(self, x):
+        self.values.append(x)
+        return self
+
+    def popleft(self):
+        return self.values.popleft()
+
+
+class SortedWindow(collections.UserList):
+    """Sorted running window data structure.
+
+    Parameters
+    ----------
+    size
+        Size of the window to compute the rolling quantile.
+
+    Examples
+    --------
+
+    >>> from river import utils
+
+    >>> window = utils.SortedWindow(size=3)
+
+    >>> for i in reversed(range(9)):
+    ...     print(window.append(i))
+    [8]
+    [7, 8]
+    [6, 7, 8]
+    [5, 6, 7]
+    [4, 5, 6]
+    [3, 4, 5]
+    [2, 3, 4]
+    [1, 2, 3]
+    [0, 1, 2]
+
+    References
+    ----------
+    [^1]: [Left sorted inserts in Python](https://stackoverflow.com/questions/8024571/insert-an-item-into-sorted-list-in-python)
+
+    """
+
+    def __init__(self, size: int):
+        super().__init__()
+        self.unsorted_window = Window(size)
+
+    @property
+    def size(self):
+        return self.unsorted_window.size
+
+    def append(self, x):
+
+        if len(self) >= self.size:
+            self.remove(self.unsorted_window[0])
+
+        bisect.insort_left(self, x)
+        self.unsorted_window.append(x)
+
+        return self
```

### Comparing `river-0.8.0/river.egg-info/PKG-INFO` & `river-0.9.0/river.egg-info/PKG-INFO`

 * *Files 12% similar despite different names*

```diff
@@ -1,244 +1,245 @@
-Metadata-Version: 2.1
-Name: river
-Version: 0.8.0
-Summary: Online machine learning in Python
-Home-page: https://github.com/online-ml/river
-Author: Max Halford
-Author-email: maxhalford25@gmail.com
-License: BSD-3
-Description: 
-        </br>
-        
-        <p align="center">
-          <img height="80px" src="docs/img/logo.svg" alt="river_logo">
-        </p>
-        
-        </br>
-        
-        <p align="center">
-          <!-- Tests -->
-          <a href="https://github.com/online-ml/river/actions?query=workflow%3Atests+branch%3Amaster">
-            <img src="https://github.com/online-ml/river/workflows/tests/badge.svg?branch=master" alt="tests">
-          </a>
-          <!-- Code coverage -->
-          <a href="https://codecov.io/gh/online-ml/river">
-            <img src="https://codecov.io/gh/online-ml/river/branch/master/graph/badge.svg?token=luK6eFoMa9"/>
-          </a>
-          <!-- Documentation -->
-          <a href="https://riverml.xyz">
-            <img src="https://img.shields.io/website?label=docs&style=flat-square&url=https%3A%2F%2Friverml.xyz%2F" alt="documentation">
-          </a>
-          <!-- Roadmap -->
-          <a href="https://www.notion.so/d1e86fcdf21e4deda16eedab2b3361fb?v=503f44740b8b44a99a961aa96e9e46e1">
-            <img src="https://img.shields.io/website?label=roadmap&style=flat-square&url=https://www.notion.so/d1e86fcdf21e4deda16eedab2b3361fb?v=503f44740b8b44a99a961aa96e9e46e1" alt="roadmap">
-          </a>
-          <!-- PyPI -->
-          <a href="https://pypi.org/project/river">
-            <img src="https://img.shields.io/pypi/v/river.svg?label=release&color=blue&style=flat-square" alt="pypi">
-          </a>
-          <!-- PePy -->
-          <a href="https://pepy.tech/project/river">
-            <img src="https://static.pepy.tech/badge/river?style=flat-square" alt="pepy">
-          </a>
-          <!-- License -->
-          <a href="https://opensource.org/licenses/BSD-3-Clause">
-            <img src="https://img.shields.io/badge/License-BSD%203--Clause-blue.svg?style=flat-square" alt="bsd_3_license">
-          </a>
-        </p>
-        
-        </br>
-        
-        <p align="center">
-          River is a Python library for <a href="https://www.wikiwand.com/en/Online_machine_learning">online machine learning</a>. It is the result of a merger between <a href="https://github.com/MaxHalford/creme">creme</a> and <a href="https://github.com/scikit-multiflow/scikit-multiflow">scikit-multiflow</a>. River's ambition is to be the go-to library for doing machine learning on streaming data.
-        </p>
-        
-        ## ⚡️ Quickstart
-        
-        As a quick example, we'll train a logistic regression to classify the [website phishing dataset](http://archive.ics.uci.edu/ml/datasets/Website+Phishing). Here's a look at the first observation in the dataset.
-        
-        ```python
-        >>> from pprint import pprint
-        >>> from river import datasets
-        
-        >>> dataset = datasets.Phishing()
-        
-        >>> for x, y in dataset:
-        ...     pprint(x)
-        ...     print(y)
-        ...     break
-        {'age_of_domain': 1,
-         'anchor_from_other_domain': 0.0,
-         'empty_server_form_handler': 0.0,
-         'https': 0.0,
-         'ip_in_url': 1,
-         'is_popular': 0.5,
-         'long_url': 1.0,
-         'popup_window': 0.0,
-         'request_from_other_domain': 0.0}
-        True
-        
-        ```
-        
-        Now let's run the model on the dataset in a streaming fashion. We sequentially interleave predictions and model updates. Meanwhile, we update a performance metric to see how well the model is doing.
-        
-        ```python
-        >>> from river import compose
-        >>> from river import linear_model
-        >>> from river import metrics
-        >>> from river import preprocessing
-        
-        >>> model = compose.Pipeline(
-        ...     preprocessing.StandardScaler(),
-        ...     linear_model.LogisticRegression()
-        ... )
-        
-        >>> metric = metrics.Accuracy()
-        
-        >>> for x, y in dataset:
-        ...     y_pred = model.predict_one(x)      # make a prediction
-        ...     metric = metric.update(y, y_pred)  # update the metric
-        ...     model = model.learn_one(x, y)      # make the model learn
-        
-        >>> metric
-        Accuracy: 89.20%
-        
-        ```
-        
-        ## 🛠 Installation
-        
-        River is intended to work with **Python 3.6 or above**. Installation can be done with `pip`:
-        
-        ```sh
-        pip install river
-        ```
-        
-        There are [wheels available](https://pypi.org/project/river/#files) for Linux, MacOS, and Windows, which means that you most probably won't have to build River from source.
-        
-        You can install the latest development version from GitHub as so:
-        
-        ```sh
-        pip install git+https://github.com/online-ml/river --upgrade
-        ```
-        
-        Or, through SSH:
-        
-        ```sh
-        pip install git+ssh://git@github.com/online-ml/river.git --upgrade
-        ```
-        
-        ## 🧠 Philosophy
-        
-        Machine learning is often done in a batch setting, whereby a model is fitted to a dataset in one go. This results in a static model which has to be retrained in order to learn from new data. In many cases, this isn't elegant nor efficient, and usually incurs [a fair amount of technical debt](https://research.google/pubs/pub43146/). Indeed, if you're using a batch model, then you need to think about maintaining a training set, monitoring real-time performance, model retraining, etc.
-        
-        With River, we encourage a different approach, which is to continuously learn a stream of data. This means that the model process one observation at a time, and can therefore be updated on the fly. This allows to learn from massive datasets that don't fit in main memory. Online machine learning also integrates nicely in cases where new data is constantly arriving. It shines in many use cases, such as time series forecasting, spam filtering, recommender systems, CTR prediction, and IoT applications. If you're bored with retraining models and want to instead build dynamic models, then online machine learning (and therefore River!) might be what you're looking for.
-        
-        Here are some benefits of using River (and online machine learning in general):
-        
-        - **Incremental**: models can update themselves in real-time.
-        - **Adaptive**: models can adapt to [concept drift](https://www.wikiwand.com/en/Concept_drift).
-        - **Production-ready**: working with data streams makes it simple to replicate production scenarios during model development.
-        - **Efficient**: models don't have to be retrained and require little compute power, which [lowers their carbon footprint](https://arxiv.org/abs/1907.10597)
-        - **Fast**: when the goal is to learn and predict with a single instance at a time, then River is an order of magnitude faster than PyTorch, Tensorflow, and scikit-learn.
-        
-        ## 🔥 Features
-        
-        - Linear models with a wide array of optimizers
-        - Nearest neighbors, decision trees, naïve Bayes
-        - [Progressive model validation](https://hunch.net/~jl/projects/prediction_bounds/progressive_validation/coltfinal.pdf)
-        - Model pipelines as a first-class citizen
-        - Anomaly detection
-        - Recommender systems
-        - Time series forecasting
-        - Imbalanced learning
-        - Clustering
-        - Feature extraction and selection
-        - Online statistics and metrics
-        - Built-in datasets
-        - And [much more](https://riverml.xyz/latest/api/overview/)
-        
-        ## 🔗 Useful links
-        
-        - [Documentation](https://riverml.xyz)
-        - [Benchmarks](https://github.com/online-ml/river/tree/master/benchmarks)
-        - [Issue tracker](https://github.com/online-ml/river/issues)
-        - [Package releases](https://pypi.org/project/river/#history)
-        
-        ## 👁️ Media
-        
-        - PyData Amsterdam 2019 presentation ([slides](https://maxhalford.github.io/slides/creme-pydata), [video](https://www.youtube.com/watch?v=P3M6dt7bY9U&list=PLGVZCDnMOq0q7_6SdrC2wRtdkojGBTAht&index=11))
-        - [Toulouse Data Science Meetup presentation](https://maxhalford.github.io/slides/creme-tds)
-        - [Machine learning for streaming data with creme](https://towardsdatascience.com/machine-learning-for-streaming-data-with-creme-dacf5fb469df)
-        - [Hong Kong Data Science Meetup presentation](https://maxhalford.github.io/slides/hkml2020.pdf)
-        
-        ## 👍 Contributing
-        
-        Feel free to contribute in any way you like, we're always open to new ideas and approaches.
-        
-        There are three ways for users to get involved:
-        
-        - [Issue tracker](https://github.com/online-ml/river/issues): this place is meant to report bugs, request for minor features, or small improvements. Issues should be short-lived and solved as fast as possible.
-        - [Discussions](https://github.com/online-ml/river/discussions): you can ask for new features, submit your questions and get help, propose new ideas, or even show the community what you are achieving with River! If you have a new technique or want to port a new functionality to River, this is the place to discuss.
-        - [Roadmap](https://www.notion.so/d1e86fcdf21e4deda16eedab2b3361fb?v=503f44740b8b44a99a961aa96e9e46e1): you can check what we are doing, what are the next planned milestones for River, and look for cool ideas that still need someone to make them become a reality!
-        
-        Please check out the [contribution guidelines](https://github.com/online-ml/river/blob/master/CONTRIBUTING.md) if you want to bring modifications to the code base. You can view the list of people who have contributed [here](https://github.com/online-ml/river/graphs/contributors).
-        
-        ## ❤️ They've used us
-        
-        These are companies that we know have been using River, be it in production or for prototyping.
-        
-        <p align="center">
-          <img width="70%" src="https://docs.google.com/drawings/d/e/2PACX-1vQbCUQkTU74dBf411r4nDl4udmqOEbLqzRtokUC-N7JDJUA7BGTfnMGmiMNqbcSuOaWAmazp1rFGwDC/pub?w=1194&h=567" alt="companies">
-        </p>
-        
-        Feel welcome to get in touch if you want us to add your company logo!
-        
-        ## 🤝 Affiliations
-        
-        **Sponsors**
-        
-        <p align="center">
-          <img width="55%" src="https://docs.google.com/drawings/d/e/2PACX-1vSagEhWAjDsb0c24En_fhWAf9DJZbyh5YjU7lK0sNowD2m9uv9TuFm-U77k6ObqTyN2mP05Avf6TCJc/pub?w=2073&h=1127" alt="sponsors">
-        </p>
-        
-        **Collaborating institutions and groups**
-        
-        <p align="center">
-          <img width="55%" src="https://docs.google.com/drawings/d/e/2PACX-1vQB0C8YgnkCt_3C3cp-Csaw8NLZUwishdbJFB3iSbBPUD0AxEVS9AlF-Rs5PJq8UVRzRtFwZIOucuXj/pub?w=1442&h=489" alt="collaborations">
-        </p>
-        
-        ## 💬 Citation
-        
-        If `river` has been useful for your research and you would like to cite it in an scientific publication, please refer to this [paper](https://arxiv.org/abs/2012.04740):
-        
-        ```bibtex
-        @misc{2020river,
-              title={River: machine learning for streaming data in Python},
-              author={Jacob Montiel and Max Halford and Saulo Martiello Mastelini
-                      and Geoffrey Bolmier and Raphael Sourty and Robin Vaysse
-                      and Adil Zouitine and Heitor Murilo Gomes and Jesse Read
-                      and Talel Abdessalem and Albert Bifet},
-              year={2020},
-              eprint={2012.04740},
-              archivePrefix={arXiv},
-              primaryClass={cs.LG}
-        }
-        ```
-        
-        ## 📝 License
-        
-        River is free and open-source software licensed under the [3-clause BSD license](https://github.com/online-ml/river/blob/master/LICENSE).
-        
-Platform: UNKNOWN
-Classifier: License :: OSI Approved :: BSD License
-Classifier: Programming Language :: Python
-Classifier: Programming Language :: Python :: 3
-Classifier: Programming Language :: Python :: 3.6
-Classifier: Programming Language :: Python :: 3.7
-Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: Implementation :: CPython
-Classifier: Programming Language :: Python :: Implementation :: PyPy
-Requires-Python: >=3.6.0
-Description-Content-Type: text/markdown
-Provides-Extra: dev
-Provides-Extra: compat
-Provides-Extra: docs
+Metadata-Version: 2.1
+Name: river
+Version: 0.9.0
+Summary: Online machine learning in Python
+Home-page: https://github.com/online-ml/river
+Author: Max Halford
+Author-email: maxhalford25@gmail.com
+License: BSD-3
+Description: 
+        </br>
+        
+        <p align="center">
+          <img height="80px" src="docs/img/logo.svg" alt="river_logo">
+        </p>
+        
+        </br>
+        
+        <p align="center">
+          <!-- Tests -->
+          <a href="https://github.com/online-ml/river/actions/workflows/unit-tests.yml">
+            <img src="https://github.com/online-ml/river/actions/workflows/unit-tests.yml/badge.svg" alt="tests">
+          </a>
+          <!-- Code coverage -->
+          <a href="https://codecov.io/gh/online-ml/river">
+            <img src="https://codecov.io/gh/online-ml/river/branch/main/graph/badge.svg?token=luK6eFoMa9"/>
+          </a>
+          <!-- Documentation -->
+          <a href="https://riverml.xyz">
+            <img src="https://img.shields.io/website?label=docs&style=flat-square&url=https%3A%2F%2Friverml.xyz%2F" alt="documentation">
+          </a>
+          <!-- Roadmap -->
+          <a href="https://www.notion.so/d1e86fcdf21e4deda16eedab2b3361fb?v=503f44740b8b44a99a961aa96e9e46e1">
+            <img src="https://img.shields.io/website?label=roadmap&style=flat-square&url=https://www.notion.so/d1e86fcdf21e4deda16eedab2b3361fb?v=503f44740b8b44a99a961aa96e9e46e1" alt="roadmap">
+          </a>
+          <!-- PyPI -->
+          <a href="https://pypi.org/project/river">
+            <img src="https://img.shields.io/pypi/v/river.svg?label=release&color=blue&style=flat-square" alt="pypi">
+          </a>
+          <!-- PePy -->
+          <a href="https://pepy.tech/project/river">
+            <img src="https://static.pepy.tech/badge/river?style=flat-square" alt="pepy">
+          </a>
+          <!-- License -->
+          <a href="https://opensource.org/licenses/BSD-3-Clause">
+            <img src="https://img.shields.io/badge/License-BSD%203--Clause-blue.svg?style=flat-square" alt="bsd_3_license">
+          </a>
+        </p>
+        
+        </br>
+        
+        <p align="center">
+          River is a Python library for <a href="https://www.wikiwand.com/en/Online_machine_learning">online machine learning</a>. It is the result of a merger between <a href="https://github.com/MaxHalford/creme">creme</a> and <a href="https://github.com/scikit-multiflow/scikit-multiflow">scikit-multiflow</a>. River's ambition is to be the go-to library for doing machine learning on streaming data.
+        </p>
+        
+        ## ⚡️ Quickstart
+        
+        As a quick example, we'll train a logistic regression to classify the [website phishing dataset](http://archive.ics.uci.edu/ml/datasets/Website+Phishing). Here's a look at the first observation in the dataset.
+        
+        ```python
+        >>> from pprint import pprint
+        >>> from river import datasets
+        
+        >>> dataset = datasets.Phishing()
+        
+        >>> for x, y in dataset:
+        ...     pprint(x)
+        ...     print(y)
+        ...     break
+        {'age_of_domain': 1,
+         'anchor_from_other_domain': 0.0,
+         'empty_server_form_handler': 0.0,
+         'https': 0.0,
+         'ip_in_url': 1,
+         'is_popular': 0.5,
+         'long_url': 1.0,
+         'popup_window': 0.0,
+         'request_from_other_domain': 0.0}
+        True
+        
+        ```
+        
+        Now let's run the model on the dataset in a streaming fashion. We sequentially interleave predictions and model updates. Meanwhile, we update a performance metric to see how well the model is doing.
+        
+        ```python
+        >>> from river import compose
+        >>> from river import linear_model
+        >>> from river import metrics
+        >>> from river import preprocessing
+        
+        >>> model = compose.Pipeline(
+        ...     preprocessing.StandardScaler(),
+        ...     linear_model.LogisticRegression()
+        ... )
+        
+        >>> metric = metrics.Accuracy()
+        
+        >>> for x, y in dataset:
+        ...     y_pred = model.predict_one(x)      # make a prediction
+        ...     metric = metric.update(y, y_pred)  # update the metric
+        ...     model = model.learn_one(x, y)      # make the model learn
+        
+        >>> metric
+        Accuracy: 89.20%
+        
+        ```
+        
+        ## 🛠 Installation
+        
+        River is intended to work with **Python 3.6 or above**. Installation can be done with `pip`:
+        
+        ```sh
+        pip install river
+        ```
+        
+        There are [wheels available](https://pypi.org/project/river/#files) for Linux, MacOS, and Windows, which means that you most probably won't have to build River from source.
+        
+        You can install the latest development version from GitHub as so:
+        
+        ```sh
+        pip install git+https://github.com/online-ml/river --upgrade
+        ```
+        
+        Or, through SSH:
+        
+        ```sh
+        pip install git+ssh://git@github.com/online-ml/river.git --upgrade
+        ```
+        
+        ## 🧠 Philosophy
+        
+        Machine learning is often done in a batch setting, whereby a model is fitted to a dataset in one go. This results in a static model which has to be retrained in order to learn from new data. In many cases, this isn't elegant nor efficient, and usually incurs [a fair amount of technical debt](https://research.google/pubs/pub43146/). Indeed, if you're using a batch model, then you need to think about maintaining a training set, monitoring real-time performance, model retraining, etc.
+        
+        With River, we encourage a different approach, which is to continuously learn a stream of data. This means that the model process one observation at a time, and can therefore be updated on the fly. This allows to learn from massive datasets that don't fit in main memory. Online machine learning also integrates nicely in cases where new data is constantly arriving. It shines in many use cases, such as time series forecasting, spam filtering, recommender systems, CTR prediction, and IoT applications. If you're bored with retraining models and want to instead build dynamic models, then online machine learning (and therefore River!) might be what you're looking for.
+        
+        Here are some benefits of using River (and online machine learning in general):
+        
+        - **Incremental**: models can update themselves in real-time.
+        - **Adaptive**: models can adapt to [concept drift](https://www.wikiwand.com/en/Concept_drift).
+        - **Production-ready**: working with data streams makes it simple to replicate production scenarios during model development.
+        - **Efficient**: models don't have to be retrained and require little compute power, which [lowers their carbon footprint](https://arxiv.org/abs/1907.10597)
+        - **Fast**: when the goal is to learn and predict with a single instance at a time, then River is an order of magnitude faster than PyTorch, Tensorflow, and scikit-learn.
+        
+        ## 🔥 Features
+        
+        - Linear models with a wide array of optimizers
+        - Nearest neighbors, decision trees, naïve Bayes
+        - [Progressive model validation](https://hunch.net/~jl/projects/prediction_bounds/progressive_validation/coltfinal.pdf)
+        - Model pipelines as a first-class citizen
+        - Anomaly detection
+        - Recommender systems
+        - Time series forecasting
+        - Imbalanced learning
+        - Clustering
+        - Feature extraction and selection
+        - Online statistics and metrics
+        - Built-in datasets
+        - And [much more](https://riverml.xyz/latest/api/overview/)
+        
+        ## 🔗 Useful links
+        
+        - [Documentation](https://riverml.xyz)
+        - [Benchmarks](https://github.com/online-ml/river/tree/main/benchmarks)
+        - [Issue tracker](https://github.com/online-ml/river/issues)
+        - [Package releases](https://pypi.org/project/river/#history)
+        
+        ## 👁️ Media
+        
+        - PyData Amsterdam 2019 presentation ([slides](https://maxhalford.github.io/slides/creme-pydata), [video](https://www.youtube.com/watch?v=P3M6dt7bY9U&list=PLGVZCDnMOq0q7_6SdrC2wRtdkojGBTAht&index=11))
+        - [Toulouse Data Science Meetup presentation](https://maxhalford.github.io/slides/creme-tds)
+        - [Machine learning for streaming data with creme](https://towardsdatascience.com/machine-learning-for-streaming-data-with-creme-dacf5fb469df)
+        - [Hong Kong Data Science Meetup presentation](https://maxhalford.github.io/slides/hkml2020.pdf)
+        
+        ## 👍 Contributing
+        
+        Feel free to contribute in any way you like, we're always open to new ideas and approaches.
+        
+        There are three ways for users to get involved:
+        
+        - [Issue tracker](https://github.com/online-ml/river/issues): this place is meant to report bugs, request for minor features, or small improvements. Issues should be short-lived and solved as fast as possible.
+        - [Discussions](https://github.com/online-ml/river/discussions): you can ask for new features, submit your questions and get help, propose new ideas, or even show the community what you are achieving with River! If you have a new technique or want to port a new functionality to River, this is the place to discuss.
+        - [Roadmap](https://www.notion.so/d1e86fcdf21e4deda16eedab2b3361fb?v=503f44740b8b44a99a961aa96e9e46e1): you can check what we are doing, what are the next planned milestones for River, and look for cool ideas that still need someone to make them become a reality!
+        
+        Please check out the [contribution guidelines](https://github.com/online-ml/river/blob/main/CONTRIBUTING.md) if you want to bring modifications to the code base. You can view the list of people who have contributed [here](https://github.com/online-ml/river/graphs/contributors).
+        
+        ## ❤️ They've used us
+        
+        These are companies that we know have been using River, be it in production or for prototyping.
+        
+        <p align="center">
+          <img width="70%" src="https://docs.google.com/drawings/d/e/2PACX-1vQbCUQkTU74dBf411r4nDl4udmqOEbLqzRtokUC-N7JDJUA7BGTfnMGmiMNqbcSuOaWAmazp1rFGwDC/pub?w=1194&h=567" alt="companies">
+        </p>
+        
+        Feel welcome to get in touch if you want us to add your company logo!
+        
+        ## 🤝 Affiliations
+        
+        **Sponsors**
+        
+        <p align="center">
+          <img width="55%" src="https://docs.google.com/drawings/d/e/2PACX-1vSagEhWAjDsb0c24En_fhWAf9DJZbyh5YjU7lK0sNowD2m9uv9TuFm-U77k6ObqTyN2mP05Avf6TCJc/pub?w=2073&h=1127" alt="sponsors">
+        </p>
+        
+        **Collaborating institutions and groups**
+        
+        <p align="center">
+          <img width="55%" src="https://docs.google.com/drawings/d/e/2PACX-1vQB0C8YgnkCt_3C3cp-Csaw8NLZUwishdbJFB3iSbBPUD0AxEVS9AlF-Rs5PJq8UVRzRtFwZIOucuXj/pub?w=1442&h=489" alt="collaborations">
+        </p>
+        
+        ## 💬 Citation
+        
+        If `river` has been useful for your research and you would like to cite it in an scientific publication, please refer to this [paper](https://arxiv.org/abs/2012.04740):
+        
+        ```bibtex
+        @misc{2020river,
+              title={River: machine learning for streaming data in Python},
+              author={Jacob Montiel and Max Halford and Saulo Martiello Mastelini
+                      and Geoffrey Bolmier and Raphael Sourty and Robin Vaysse
+                      and Adil Zouitine and Heitor Murilo Gomes and Jesse Read
+                      and Talel Abdessalem and Albert Bifet},
+              year={2020},
+              eprint={2012.04740},
+              archivePrefix={arXiv},
+              primaryClass={cs.LG}
+        }
+        ```
+        
+        ## 📝 License
+        
+        River is free and open-source software licensed under the [3-clause BSD license](https://github.com/online-ml/river/blob/main/LICENSE).
+        
+Platform: UNKNOWN
+Classifier: License :: OSI Approved :: BSD License
+Classifier: Programming Language :: Python
+Classifier: Programming Language :: Python :: 3
+Classifier: Programming Language :: Python :: 3.6
+Classifier: Programming Language :: Python :: 3.7
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: Implementation :: CPython
+Classifier: Programming Language :: Python :: Implementation :: PyPy
+Requires-Python: >=3.7.0
+Description-Content-Type: text/markdown
+Provides-Extra: dev
+Provides-Extra: compat
+Provides-Extra: docs
+Provides-Extra: extra
```

#### html2text {}

```diff
@@ -1,12 +1,12 @@
-Metadata-Version: 2.1 Name: river Version: 0.8.0 Summary: Online machine
+Metadata-Version: 2.1 Name: river Version: 0.9.0 Summary: Online machine
 learning in Python Home-page: https://github.com/online-ml/river Author: Max
 Halford Author-email: maxhalford25@gmail.com License: BSD-3 Description:
                                  [river_logo]
-      _[_t_e_s_t_s_]_[_h_t_t_p_s_:_/_/_c_o_d_e_c_o_v_._i_o_/_g_h_/_o_n_l_i_n_e_-_m_l_/_r_i_v_e_r_/_b_r_a_n_c_h_/_m_a_s_t_e_r_/_g_r_a_p_h_/
+       _[_t_e_s_t_s_]_[_h_t_t_p_s_:_/_/_c_o_d_e_c_o_v_._i_o_/_g_h_/_o_n_l_i_n_e_-_m_l_/_r_i_v_e_r_/_b_r_a_n_c_h_/_m_a_i_n_/_g_r_a_p_h_/
 _b_a_d_g_e_._s_v_g_?_t_o_k_e_n_=_l_u_K_6_e_F_o_M_a_9_]_[_d_o_c_u_m_e_n_t_a_t_i_o_n_]_[_r_o_a_d_m_a_p_]_[_p_y_p_i_]_[_p_e_p_y_]_[_b_s_d___3___l_i_c_e_n_s_e_]
  River is a Python library for _o_n_l_i_n_e_ _m_a_c_h_i_n_e_ _l_e_a_r_n_i_n_g. It is the result of a
 merger between _c_r_e_m_e and _s_c_i_k_i_t_-_m_u_l_t_i_f_l_o_w. River's ambition is to be the go-to
              library for doing machine learning on streaming data.
 ## â¡ï¸ Quickstart As a quick example, we'll train a logistic regression to
 classify the [website phishing dataset](http://archive.ics.uci.edu/ml/datasets/
 Website+Phishing). Here's a look at the first observation in the dataset.
@@ -60,17 +60,17 @@
 trees, naÃ¯ve Bayes - [Progressive model validation](https://hunch.net/~jl/
 projects/prediction_bounds/progressive_validation/coltfinal.pdf) - Model
 pipelines as a first-class citizen - Anomaly detection - Recommender systems -
 Time series forecasting - Imbalanced learning - Clustering - Feature extraction
 and selection - Online statistics and metrics - Built-in datasets - And [much
 more](https://riverml.xyz/latest/api/overview/) ## ð Useful links -
 [Documentation](https://riverml.xyz) - [Benchmarks](https://github.com/online-
-ml/river/tree/master/benchmarks) - [Issue tracker](https://github.com/online-
-ml/river/issues) - [Package releases](https://pypi.org/project/river/#history)
-## ðï¸ Media - PyData Amsterdam 2019 presentation ([slides](https://
+ml/river/tree/main/benchmarks) - [Issue tracker](https://github.com/online-ml/
+river/issues) - [Package releases](https://pypi.org/project/river/#history) ##
+ðï¸ Media - PyData Amsterdam 2019 presentation ([slides](https://
 maxhalford.github.io/slides/creme-pydata), [video](https://www.youtube.com/
 watch?v=P3M6dt7bY9U&list=PLGVZCDnMOq0q7_6SdrC2wRtdkojGBTAht&index=11)) -
 [Toulouse Data Science Meetup presentation](https://maxhalford.github.io/
 slides/creme-tds) - [Machine learning for streaming data with creme](https://
 towardsdatascience.com/machine-learning-for-streaming-data-with-creme-
 dacf5fb469df) - [Hong Kong Data Science Meetup presentation](https://
 maxhalford.github.io/slides/hkml2020.pdf) ## ð Contributing Feel free to
@@ -83,16 +83,16 @@
 propose new ideas, or even show the community what you are achieving with
 River! If you have a new technique or want to port a new functionality to
 River, this is the place to discuss. - [Roadmap](https://www.notion.so/
 d1e86fcdf21e4deda16eedab2b3361fb?v=503f44740b8b44a99a961aa96e9e46e1): you can
 check what we are doing, what are the next planned milestones for River, and
 look for cool ideas that still need someone to make them become a reality!
 Please check out the [contribution guidelines](https://github.com/online-ml/
-river/blob/master/CONTRIBUTING.md) if you want to bring modifications to the
-code base. You can view the list of people who have contributed [here](https://
+river/blob/main/CONTRIBUTING.md) if you want to bring modifications to the code
+base. You can view the list of people who have contributed [here](https://
 github.com/online-ml/river/graphs/contributors). ## â¤ï¸ They've used us
 These are companies that we know have been using River, be it in production or
 for prototyping.
                                   [companies]
 Feel welcome to get in touch if you want us to add your company logo! ## ð¤
 Affiliations **Sponsors**
                                   [sponsors]
@@ -103,16 +103,16 @@
 (https://arxiv.org/abs/2012.04740): ```bibtex @misc{2020river, title={River:
 machine learning for streaming data in Python}, author={Jacob Montiel and Max
 Halford and Saulo Martiello Mastelini and Geoffrey Bolmier and Raphael Sourty
 and Robin Vaysse and Adil Zouitine and Heitor Murilo Gomes and Jesse Read and
 Talel Abdessalem and Albert Bifet}, year={2020}, eprint={2012.04740},
 archivePrefix={arXiv}, primaryClass={cs.LG} } ``` ## ð License River is free
 and open-source software licensed under the [3-clause BSD license](https://
-github.com/online-ml/river/blob/master/LICENSE). Platform: UNKNOWN Classifier:
+github.com/online-ml/river/blob/main/LICENSE). Platform: UNKNOWN Classifier:
 License :: OSI Approved :: BSD License Classifier: Programming Language ::
 Python Classifier: Programming Language :: Python :: 3 Classifier: Programming
 Language :: Python :: 3.6 Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8 Classifier: Programming
 Language :: Python :: Implementation :: CPython Classifier: Programming
-Language :: Python :: Implementation :: PyPy Requires-Python: >=3.6.0
+Language :: Python :: Implementation :: PyPy Requires-Python: >=3.7.0
 Description-Content-Type: text/markdown Provides-Extra: dev Provides-Extra:
-compat Provides-Extra: docs
+compat Provides-Extra: docs Provides-Extra: extra
```

### Comparing `river-0.8.0/river.egg-info/requires.txt` & `river-0.9.0/river.egg-info/requires.txt`

 * *Files 22% similar despite different names*

```diff
@@ -22,32 +22,35 @@
 asv
 graphviz>=0.10.1
 matplotlib>=3.0.2
 mypy>=0.761
 pre-commit>=2.9.2
 pytest>=4.5.0
 pytest-cov>=2.6.1
-scikit-learn>=0.22.1
+scikit-learn>=1.0.1
 sqlalchemy>=1.4
 
 [docs]
 numpy>=1.18.1
 scipy>=1.4.1
 pandas>=1.0.1
 asv
 graphviz>=0.10.1
 matplotlib>=3.0.2
 mypy>=0.761
 pre-commit>=2.9.2
 pytest>=4.5.0
 pytest-cov>=2.6.1
-scikit-learn>=0.22.1
+scikit-learn>=1.0.1
 sqlalchemy>=1.4
 flask
 ipykernel
 jupyter-client
 mike==0.5.3
 mkdocs
 mkdocs-awesome-pages-plugin
 mkdocs-material
 nbconvert
 spacy
+
+[extra]
+river_extra==0.9.0
```

### Comparing `river-0.8.0/setup.py` & `river-0.9.0/setup.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,143 +1,144 @@
-import io
-import os
-import platform
-import subprocess
-import sys
-
-import setuptools
-
-try:
-    from numpy import get_include
-except ImportError:
-    subprocess.check_call([sys.executable, "-m", "pip", "install", "numpy"])
-    from numpy import get_include
-
-try:
-    from Cython.Build import cythonize
-except ImportError:
-    subprocess.check_call([sys.executable, "-m", "pip", "install", "Cython"])
-    from Cython.Build import cythonize
-
-
-# Package meta-data.
-NAME = "river"
-DESCRIPTION = "Online machine learning in Python"
-LONG_DESCRIPTION_CONTENT_TYPE = "text/markdown"
-URL = "https://github.com/online-ml/river"
-EMAIL = "maxhalford25@gmail.com"
-AUTHOR = "Max Halford"
-REQUIRES_PYTHON = ">=3.6.0"
-
-# Package requirements.
-base_packages = ["numpy>=1.18.1", "scipy>=1.4.1", "pandas>=1.0.1"]
-
-compat_packages = base_packages + [
-    "scikit-learn",
-    "scikit-surprise",
-    "sqlalchemy>=1.4",
-    "torch",
-    "vaex",
-]
-
-dev_packages = base_packages + [
-    "asv",
-    "graphviz>=0.10.1",
-    "matplotlib>=3.0.2",
-    "mypy>=0.761",
-    "pre-commit>=2.9.2",
-    "pytest>=4.5.0",
-    "pytest-cov>=2.6.1",
-    "scikit-learn>=0.22.1",
-    "sqlalchemy>=1.4",
-]
-
-docs_packages = dev_packages + [
-    "flask",
-    "ipykernel",
-    "jupyter-client",
-    "mike==0.5.3",
-    "mkdocs",
-    "mkdocs-awesome-pages-plugin",
-    "mkdocs-material",
-    "nbconvert",
-    "spacy",
-]
-
-here = os.path.abspath(os.path.dirname(__file__))
-
-# Import the README and use it as the long-description.
-with io.open(os.path.join(here, "README.md"), encoding="utf-8") as f:
-    long_description = "\n" + f.read()
-
-# Load the package's __version__.py module as a dictionary.
-about = {}
-with open(os.path.join(here, NAME, "__version__.py")) as f:
-    exec(f.read(), about)
-
-# Where the magic happens:
-setuptools.setup(
-    name=NAME,
-    version=about["__version__"],
-    description=DESCRIPTION,
-    long_description=long_description,
-    long_description_content_type=LONG_DESCRIPTION_CONTENT_TYPE,
-    author=AUTHOR,
-    author_email=EMAIL,
-    python_requires=REQUIRES_PYTHON,
-    url=URL,
-    packages=setuptools.find_packages(exclude=("tests",)),
-    install_requires=base_packages,
-    extras_require={
-        "dev": dev_packages,
-        "compat": compat_packages,
-        "docs": docs_packages,
-        ":python_version == '3.6'": ["dataclasses"],
-    },
-    include_package_data=True,
-    license="BSD-3",
-    classifiers=[
-        # Trove classifiers
-        # Full list: https://pypi.python.org/pypi?%3Aaction=list_classifiers
-        "License :: OSI Approved :: BSD License",
-        "Programming Language :: Python",
-        "Programming Language :: Python :: 3",
-        "Programming Language :: Python :: 3.6",
-        "Programming Language :: Python :: 3.7",
-        "Programming Language :: Python :: 3.8",
-        "Programming Language :: Python :: Implementation :: CPython",
-        "Programming Language :: Python :: Implementation :: PyPy",
-    ],
-    ext_modules=cythonize(
-        module_list=[
-            setuptools.Extension(
-                "*",
-                sources=["**/*.pyx"],
-                include_dirs=[get_include()],
-                libraries=[] if platform.system() == "Windows" else ["m"],
-                define_macros=[("NPY_NO_DEPRECATED_API", "NPY_1_7_API_VERSION")],
-            )
-        ],
-        compiler_directives={
-            "language_level": 3,
-            "binding": True,
-            "embedsignature": True,
-        },
-    )
-    + [
-        setuptools.Extension(
-            "river.neighbors.libNearestNeighbor",
-            sources=[
-                os.path.join(
-                    "river",
-                    "neighbors",
-                    "src",
-                    "libNearestNeighbor",
-                    "nearestNeighbor.cpp",
-                )
-            ],
-            include_dirs=[get_include()],
-            libraries=[] if platform.system() == "Windows" else ["m"],
-            language="c++",
-        )
-    ],
-)
+import io
+import os
+import platform
+import subprocess
+import sys
+
+import setuptools
+
+try:
+    from numpy import get_include
+except ImportError:
+    subprocess.check_call([sys.executable, "-m", "pip", "install", "numpy"])
+    from numpy import get_include
+
+try:
+    from Cython.Build import cythonize
+except ImportError:
+    subprocess.check_call([sys.executable, "-m", "pip", "install", "Cython"])
+    from Cython.Build import cythonize
+
+
+# Package meta-data.
+NAME = "river"
+DESCRIPTION = "Online machine learning in Python"
+LONG_DESCRIPTION_CONTENT_TYPE = "text/markdown"
+URL = "https://github.com/online-ml/river"
+EMAIL = "maxhalford25@gmail.com"
+AUTHOR = "Max Halford"
+REQUIRES_PYTHON = ">=3.7.0"
+
+# Package requirements.
+base_packages = ["numpy>=1.18.1", "scipy>=1.4.1", "pandas>=1.0.1"]
+
+compat_packages = base_packages + [
+    "scikit-learn",
+    "scikit-surprise",
+    "sqlalchemy>=1.4",
+    "torch",
+    "vaex",
+]
+
+dev_packages = base_packages + [
+    "asv",
+    "graphviz>=0.10.1",
+    "matplotlib>=3.0.2",
+    "mypy>=0.761",
+    "pre-commit>=2.9.2",
+    "pytest>=4.5.0",
+    "pytest-cov>=2.6.1",
+    "scikit-learn>=1.0.1",
+    "sqlalchemy>=1.4",
+]
+
+docs_packages = dev_packages + [
+    "flask",
+    "ipykernel",
+    "jupyter-client",
+    "mike==0.5.3",
+    "mkdocs",
+    "mkdocs-awesome-pages-plugin",
+    "mkdocs-material",
+    "nbconvert",
+    "spacy",
+]
+
+here = os.path.abspath(os.path.dirname(__file__))
+
+# Import the README and use it as the long-description.
+with io.open(os.path.join(here, "README.md"), encoding="utf-8") as f:
+    long_description = "\n" + f.read()
+
+# Load the package's __version__.py module as a dictionary.
+about = {}
+with open(os.path.join(here, NAME, "__version__.py")) as f:
+    exec(f.read(), about)
+
+# Where the magic happens:
+setuptools.setup(
+    name=NAME,
+    version=about["__version__"],
+    description=DESCRIPTION,
+    long_description=long_description,
+    long_description_content_type=LONG_DESCRIPTION_CONTENT_TYPE,
+    author=AUTHOR,
+    author_email=EMAIL,
+    python_requires=REQUIRES_PYTHON,
+    url=URL,
+    packages=setuptools.find_packages(exclude=("tests",)),
+    install_requires=base_packages,
+    extras_require={
+        "dev": dev_packages,
+        "compat": compat_packages,
+        "docs": docs_packages,
+        "extra": [f"river_extra=={about['__version__']}"],
+        ":python_version == '3.6'": ["dataclasses"],
+    },
+    include_package_data=True,
+    license="BSD-3",
+    classifiers=[
+        # Trove classifiers
+        # Full list: https://pypi.python.org/pypi?%3Aaction=list_classifiers
+        "License :: OSI Approved :: BSD License",
+        "Programming Language :: Python",
+        "Programming Language :: Python :: 3",
+        "Programming Language :: Python :: 3.6",
+        "Programming Language :: Python :: 3.7",
+        "Programming Language :: Python :: 3.8",
+        "Programming Language :: Python :: Implementation :: CPython",
+        "Programming Language :: Python :: Implementation :: PyPy",
+    ],
+    ext_modules=cythonize(
+        module_list=[
+            setuptools.Extension(
+                "*",
+                sources=["**/*.pyx"],
+                include_dirs=[get_include()],
+                libraries=[] if platform.system() == "Windows" else ["m"],
+                define_macros=[("NPY_NO_DEPRECATED_API", "NPY_1_7_API_VERSION")],
+            )
+        ],
+        compiler_directives={
+            "language_level": 3,
+            "binding": True,
+            "embedsignature": True,
+        },
+    )
+    + [
+        setuptools.Extension(
+            "river.neighbors.libNearestNeighbor",
+            sources=[
+                os.path.join(
+                    "river",
+                    "neighbors",
+                    "src",
+                    "libNearestNeighbor",
+                    "nearestNeighbor.cpp",
+                )
+            ],
+            include_dirs=[get_include()],
+            libraries=[] if platform.system() == "Windows" else ["m"],
+            language="c++",
+        )
+    ],
+)
```

